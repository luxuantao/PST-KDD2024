<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-04-30">30 Apr. 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">DSI-Dipartimento di Scienze dell&apos;Informazione</orgName>
								<orgName type="institution">Universita`degli Studi di Milano</orgName>
								<address>
									<addrLine>Via Comelico 39</addrLine>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-04-30">30 Apr. 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">6B76F53B44E57C1E8100FB3732530E29</idno>
					<idno type="DOI">10.1109/TCBB.2010.38</idno>
					<note type="submission">received 19 Aug. 2009; revised 6 Dec. 2009; accepted 21 Dec. 2009;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>True Path Rule Hierarchical Ensembles for Genome-Wide Gene Function Prediction <ref type="bibr">Giorgio Valentini</ref> Abstract-Gene function prediction is a complex computational problem, characterized by several items: the number of functional classes is large, and a gene may belong to multiple classes; functional classes are structured according to a hierarchy; classes are usually unbalanced, with more negative than positive examples; class labels can be uncertain and the annotations largely incomplete; to improve the predictions, multiple sources of data need to be properly integrated. In this contribution, we focus on the first three items, and, in particular, on the development of a new method for the hierarchical genome-wide and ontology-wide gene function prediction. The proposed algorithm is inspired by the "true path rule" (TPR) that governs both the Gene Ontology and FunCat taxonomies. According to this rule, the proposed TPR ensemble method is characterized by a two-way asymmetric flow of information that traverses the graph-structured ensemble: positive predictions for a node influence in a recursive way its ancestors, while negative predictions influence its offsprings. Cross-validated results with the model organism S. Crevisiae, using seven different sources of biomolecular data, and a theoretical analysis of the the TPR algorithm show the effectiveness and the drawbacks of the proposed approach.</p><p>Index Terms-Gene function prediction, ensemble methods, hierarchical classification, Functional Catalogue (FunCat).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ç 1 INTRODUCTION</head><p>E XPLOITING the wealth of biomolecular data accumulated by novel high-throughput biotechnologies, "in silico" gene function prediction methods <ref type="bibr" target="#b0">[1]</ref> provide hypothetical annotations that can drive the biological validation and discovery of novel functions of genes and gene products, thus resulting in a relevant saving of experimental resources <ref type="bibr" target="#b1">[2]</ref>.</p><p>Gene function prediction is a multiclass, multilabel classification problem characterized by hundreds or thousands of functional classes structured according to a predefined hierarchy.</p><p>From a general standpoint, several approaches have been proposed for multilabel classification, with applications ranging from gene function prediction, to music categorization and semantic scene classification <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Classifications of hierarchically structured classes have been applied in several contexts, such as the automatic classification of World Wide Web documents <ref type="bibr" target="#b5">[6]</ref> or the prediction of enzyme classes according to the Enzyme Classification scheme <ref type="bibr" target="#b6">[7]</ref>.</p><p>In the framework of gene function prediction, the two main taxonomies of gene functional classes are represented by the Gene Ontology (GO) <ref type="bibr" target="#b7">[8]</ref>, and the Functional Catalogue (FunCat) <ref type="bibr" target="#b8">[9]</ref>. The GO is composed of thousands of functional classes structured according to a directed acyclic graph, and it is set out in three separated ontologies: "Biological Processes," "Molecular Function," and "Cellular Component." Indeed, a gene can participate in specific biological processes (e.g., cell cycle, metabolism, and nucleotide biosynthesis) and at the same time can perform specific molecular functions (e.g., catalytic or binding activities that occur at the molecular level) in specific cellular components (e.g., mitochondrion or rough endoplasmic reticulum). The FunCat represents a more simple and concise set of gene functional classes: it consists of 28 main functional categories (or branches) that covers general fields such as cellular transport, metabolism, and cellular communication/signal transduction. These main functional classes are divided into a set of subclasses with up to six levels of increasing specificity, according to a treelike structure that accounts for different functional characteristics of genes and gene products. Genes may belong at the same time to multiple functional classes, since several classes are subclasses of more general ones, and because a gene may participate in different biological processes and may perform different biological functions.</p><p>Several gene function prediction methods considered a relatively small set of functional classes <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, while others provided predictions extended to larger sets, using graph-based or machine learning methods such as functional linkage networks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, Bayesian Networks <ref type="bibr" target="#b10">[11]</ref>, Support Vector Machines and semidefinite programming <ref type="bibr" target="#b9">[10]</ref>, artificial neural networks <ref type="bibr" target="#b14">[15]</ref>, or methods that combine functional linkage networks with learning machines using a logistic regression model <ref type="bibr" target="#b15">[16]</ref> or simple algebraic operators <ref type="bibr" target="#b16">[17]</ref>. Other promising approaches are represented by structured output methods, based on the joint kernelization of both input variables and output labels, using, e.g., perceptron-like learning algorithms <ref type="bibr" target="#b17">[18]</ref> or maximum margin algorithms <ref type="bibr" target="#b18">[19]</ref>, and by methods that improve the prediction of GO annotations by extracting implicit semantic relationships between genes and functions <ref type="bibr" target="#b19">[20]</ref>. Several methods tried to take advantage of the intrinsic hierarchical nature of gene function prediction, explicitly considering the relationships between functional classes <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><p>Our proposed approach not only explicitly takes into account the hierarchical relationships between functional classes, but it is also directly inspired by the true path rule (TPR) that can be summarized as follows <ref type="bibr" target="#b25">[26]</ref>: "an annotation for a class in the hierarchy is automatically transferred to its ancestors, while genes unannotated for a class cannot be annotated for its descendants." According to this rule that governs the annotations of both GO and FunCat taxonomies, the proposed ensemble method is characterized by a two-way asymmetric flow of information that traverses the graph-structured ensemble: positive predictions for a node influence in a recursive way its ancestors, while negative predictions influence its offsprings. The resulting ensemble embeds the functional relationships between functional classes that characterize the hierarchical taxonomy. Our approach is related to two recently proposed methods <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. In <ref type="bibr" target="#b26">[27]</ref>, a probabilistic model that combines relational protein-protein interaction data and the hierarchical structure of GO to predict truepath-consistent function labels, obeys the TPR by setting the descendants of a node as negative whenever that node is set to negative. Nevertheless, this approach propagates information only toward the bottom of the graph, differing from our proposed method that propagates information also from bottom to top of the overall hierarchy.</p><p>The proposed TPR ensemble method is also related to the approach proposed by <ref type="bibr" target="#b27">[28]</ref>, by which predictions of an ensemble of SVMs, each specialized to identify genes belonging to a specific GO class, are hierarchically combined, extending a previous approach based on Bayesian networks <ref type="bibr" target="#b28">[29]</ref>. More precisely, in <ref type="bibr" target="#b27">[28]</ref>, two local strategies are proposed to take into account the relationships between GO nodes: the first one is based on the Markov blanket associated with each node (that is, the subgraph involving its parents, children, and children's parents), and the other one on a breadth-first search to recover all descendants up to a maximum of 30 GO nodes. Differently from <ref type="bibr" target="#b27">[28]</ref>, in our approach, the flow of information between nodes is not limited for each node to a specific subgraph of the taxonomy, but it traverses the graph involving a large set of classes of the hierarchy. This global strategy, even if, in principle, applicable with slight modifications to the GO, has been applied to FunCat, where graphs are simpler and structured according to a tree forest.</p><p>The TPR method can be applied to predict the annotations of genes at the level of the entire taxonomy or considering specific subsets of the hierarchical functional classes, and provides probabilistic and structured predictions of gene annotations. Moreover, by tuning a single global parameter, it allows us to regulate the trade-off between precision and recall that characterizes gene function prediction problems. We applied the TPR hierarchical ensemble method to the prediction of gene functions in yeast, using probabilistic SVMs as base learners <ref type="bibr" target="#b29">[30]</ref>, but the algorithm is general enough to be used with any probabilistic base learner and with other model organisms. Considering that data integration is crucial to improve prediction performances <ref type="bibr" target="#b30">[31]</ref>, TPR ensembles can be easily integrated with state-of-the-art biomolecular data integration methods <ref type="bibr" target="#b31">[32]</ref>, such as vector-space integration <ref type="bibr" target="#b32">[33]</ref>, kernel fusion <ref type="bibr" target="#b9">[10]</ref>, or ensembles of learning machines <ref type="bibr" target="#b33">[34]</ref>, without any modification of the algorithmic scheme.</p><p>A basic version of the TPR ensemble method has been recently presented in two conferences on ensemble methods and multilabel learning <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. In this paper, we present an extended and enhanced version in order to discuss in detail the characteristics and the theoretical properties of TPR ensembles, their application to the genome-wide gene function prediction in model organisms, and new possible research lines in the context of the hierarchical classification of gene functions. More precisely, this paper is organized as follows: in Section 2, the ensemble method inspired by the TPR is presented, and its properties are analyzed and discussed. Section 3 summarizes the experimental setup, and introduces the types of biomolecular data used in the experiments and the performance measures applied to properly evaluate the results in a multilabel hierarchical context. Section 4 shows genome-wide gene function prediction results obtained with the model organism S. cerevisiae using the proposed methods compared with hierarchical Top-down and Flat ensemble approaches, and discusses the pros and cons of the true-path-rule-based ensembles. The conclusions summarize the main contributions and depict possible future developments of this work. Detailed experimental results are available as supplementary information at: http://homes.dsi.unimi.it/~valenti/SupplInfo/TPR, which can also be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TCBB.2010.38. Source R code implementing the TPR algorithm is available upon request from the author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation and Basic Definitions</head><p>Genome-wide gene function prediction can be modeled as a hierarchical, multiclass, and multilabel classification problem. Indeed, a gene/gene product x can be assigned to one or more functional classes of the set C ¼ fc 1 ; c 2 ; . . . ; c m g. The assignments can be coded through a vector of multilabels y ¼ &lt;y 1 ; y 2 ; . . . ; y m &gt; 2 f0; 1g m , by which if x belongs to class c i , then y i ¼ 1, otherwise y i ¼ 0, where the variable i; 1 i m, refers to the indices corresponding to the m classes belonging to the set C.</p><p>In both the GO and FunCat taxonomies, the functional classes are structured according to a hierarchy and can be represented by a directed graph, where nodes correspond to classes, and arcs to relationships between classes. Hence, the node corresponding to the class c i can be simply denoted by i. We represent the set of children nodes of i by childðiÞ, and the set of its parents by parðiÞ. Moreover, y childðiÞ denotes the labels of the children classes of node i and analogously y parðiÞ denotes the labels of the parent classes of i. Note that in FunCat, only one parent is permitted, since the overall hierarchy is a tree forest, while in the GO, more parents are allowed, because the relationships are structured according to a directed acyclic graph. A classifier D : X ! f0; 1g m computes the multilabel associated with each gene x 2 X, and d i ðxÞ 2 f0; 1g is the label predicted by the classifier for class c i . For the sake of simplicity, if there is no ambiguity, we represent d i ðxÞ simply by d i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Gene Function Prediction Method Inspired by the TPR</head><p>The TPR enforces the consistency of gene function annotations in both GO and FunCat taxonomies.</p><p>"If the child term describes the gene product, then all its parent terms must also apply to that gene product."</p><p>In other words, if a gene is annotated with a specific functional term (functional class), then it is annotated with all the "parent" classes, and with all its ancestors in a recursive way. If a gene x is not annotated to a class c, the situation is slightly different in the GO and FunCat. Indeed, in FunCat, x cannot belong to any of the offsprings classes of c, because each node can have only one parent; in the GO, being structured according to a direct acyclic graph, x can be annotated to some of the offsprings of c, if there are one or more ancestors of the offsprings of c for which the gene x is annotated. Fig. <ref type="figure" target="#fig_0">1</ref> shows an example of the application of the TPR with the FunCat taxonomy. For a given example x, considering the parents of a given node i, a classifier that respects the TPR needs to obey the following rules:</p><formula xml:id="formula_0">d i ¼ 1 ) d parðiÞ ¼ 1; d i ¼ 0 6 ) d parðiÞ ¼ 0:<label>ð1Þ</label></formula><p>On the other hand, considering the children of a given node i, a classifier that respects the TPR needs to obey the following rules:</p><formula xml:id="formula_1">d i ¼ 1 6 ) d childðiÞ ¼ 1; d i ¼ 0 ) d childðiÞ ¼ 0:<label>ð2Þ</label></formula><p>From (1) and (2), we can observe an asymmetry in the rules that govern the assignments of positive and negative labels. Indeed, we have a propagation of positive predictions from bottom to top of the hierarchy (1) and a propagation of negative labels from top to bottom (2). On the contrary, negative labels cannot propagate from bottom to top, and positive predictions cannot propagate from top to bottom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The TPR Ensemble Algorithm</head><p>The proposed hierarchical ensemble algorithm puts together the predictions made at each node by local "base" classifiers to realize an ensemble that obeys the "true path rule."</p><p>The basic ideas behind the TPR ensemble algorithm can be summarized as follows:</p><p>1. Training of the base learners: for each node of the hierarchy, a suitable learning algorithm (e.g., a multilayer perceptron or a support vector machine) provides a classifier for the associated functional class. 2. In the evaluation phase, the trained classifiers associated with each class/node of the graph provide a local decision about the assignment of a given example to a given node. 3. Positive decisions may propagate from bottom to top across the graph: they influence the decisions of the parent nodes and of their ancestors in a recursive way, by traversing the graph toward higher level nodes/classes. On the contrary, negative decisions do not affect decisions of the parent node (that is, they do not propagate from bottom to top, (1)). 4. Negative predictions for a given node (taking into account the local decision of its descendants) are propagated to the descendants, to preserve the consistency of the hierarchy according to the TPR. On the contrary, positive decisions do not influence decisions of child nodes (2). The ensemble combines the local predictions of the base learners associated with each node with the positive decisions that come from the bottom of the hierarchy, and with the negative decisions that spring from the higher level nodes. More precisely, base classifiers estimate local probabilities pi ðxÞ that a given example x belongs to class c i , but the core of the algorithm is represented by the evaluation phase, where the ensemble provides an estimate of the "consensus" global probability p i ðxÞ.</p><p>Let us consider the set i ðxÞ of the children of node i for which we have a positive prediction for a given example x:</p><formula xml:id="formula_2">i ðxÞ ¼ fjjj 2 childðiÞ; d j ðxÞ ¼ 1g:<label>ð3Þ</label></formula><p>The global consensus probability p i ðxÞ of the ensemble depends both on the local prediction pi ðxÞ and on the prediction of the nodes belonging to i ðxÞ:</p><formula xml:id="formula_3">p i ðxÞ ¼ 1 1 þ j i ðxÞj pi ðxÞ þ X j2 i ðxÞ p j ðxÞ 0 @ 1 A :<label>ð4Þ</label></formula><p>The decision d i ðxÞ at node/class i is set to 1 if p i ðxÞ &gt; t, to 0 otherwise (a natural choice for t is 0.5). Note that the restriction to nodes belonging to i ðxÞ in the summation of (4) depends on the TPR: indeed only children nodes for which we have a positive prediction can influence their parent. In the leaf nodes, the sum of (4) disappears and (4) becomes p i ðxÞ ¼ pi ðxÞ. In this way, positive predictions propagate from bottom to top. On the contrary if for a given node d i ðxÞ ¼ 0, then this decision is propagated to its subtree.</p><p>A high-level representation of the evaluation phase of the algorithm for a tree-structured graph is given in Fig. <ref type="figure" target="#fig_1">2</ref>. The pseudocode of the algorithm is characterized by two main for loops: the external for (rows 1-18) handles a per level bottom-up traversal of the tree, while the internal (rows 2-17) scans the nodes at each level. If a node is a leaf (row 3), then the consensus probability p i is equal to the local probability pi ðxÞ. Note that a positive decision is taken if p i ðxÞ is larger than a threshold t (row 5). If a node is not a leaf (rows 7-16), at first the set i ðxÞ collects all the children nodes for which we have a positive prediction, and the consensus probability p i of the ensemble is computed by considering both the local estimate of the probability pi and the probabilities computed by the children nodes for which a positive decision has been taken (row 9). In the case of a negative decision for a node i, all the classes belonging to the subtree rooted at i are set to negative, and their probabilities are set to p i if larger than p i (rows 13-16). Indeed, according to the hierarchical structure of GO and FunCat taxonomies, if a gene does not belong to a class, it cannot belong to the descendants of that class. The algorithm provides both the multilabels associated with the example x and the probabilities p i that a given example belongs to the class i, 1 i m.</p><p>The bottom-up per level traversal of the tree assures that all the offsprings of a given node i are taken into account for the ensemble prediction. For the same reason, we can safely set the classes belonging to the subtree rooted at i to negative, when d i ðxÞ is set to 0. It is worth noting that we have a two-way asymmetric flow of information across the tree: positive predictions for a node influence its ancestors, while negative predictions influence its offsprings. This comes from the fact that the ensemble respects the TPR.</p><p>Note that in the TPR algorithm there is no way to explicitly balance the local prediction pi ðxÞ at node i with the positive predictions coming from its offsprings <ref type="bibr" target="#b3">(4)</ref>. By balancing the local predictions with the positive predictions coming from the ensemble, we can explicitly modulate the interplay between local and descendant predictors. To this end, we introduce a parent weight w, 0 w 1, such that if w ¼ 1, the decision at node i depends only by the local predictor; otherwise, the prediction is shared proportionally to w and 1 À w between, respectively, the local parent predictor and the set of its children:</p><formula xml:id="formula_4">p i ðxÞ ¼ w Á pi ðxÞ þ 1 À w j i ðxÞj X j2iðxÞ p j ðxÞ:<label>ð5Þ</label></formula><p>Hence, we can obtain a variant of the TPR algorithm, that we name weighted TPR (TPR-w) hierarchical ensemble algorithm by substituting rows 8 and 9 of the basic algorithm with the following pseudocode: i ðxÞ fjjj 2 childðiÞ; d j ðxÞ ¼ 1g if (j i ðxÞj &gt; 0) p i ðxÞ w Á pi ðxÞ þ 1Àw jiðxÞj P j2 i ðxÞ p j ðxÞ else p i ðxÞ pi ðxÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Analysis of the Propagation of Positive Decisions</head><p>While the propagation of negative decisions from top to bottom nodes is quite straightforward and common to the hierarchical Top-down algorithm, the propagation of positive decisions from bottom to top nodes of the hierarchy is specific to the TPR algorithm. In this section, we analyze the contribution of the descendants nodes/classifiers to the decision of the upper nodes, that is, we study the influence of the positive decisions of the descendants on the decision of their ancestors. This analysis aims at a better understanding of the characteristics of the TPR algorithm, and at suggesting new research lines based on variants of the proposed TPR-based algorithms.</p><p>We define nodes at level k as nodes with distance k from the root, that is nodes with a path to the root of length k. The root is at level 0, its children at level 1, its grandchildren at level 2, and so on. A generic node at level k is any node whose distance from the root is equal to k. The posterior probability computed by the ensemble for a generic node at level k is denoted by q k . Note that q k differs from p i defined in Section 2.3, since p i refers to the probability computed for the node i of the hierarchy, while q k to the probability computed for a generic node at level k of the hierarchy. More precisely, we denote by q k the probability computed by the ensemble and by qk the probability computed by the base learner local to a node at level k. Moreover, we define q j kþ1 as the probability of a child j of a node at level k, where the index j ! 1 refers to different children of a node at level k. Finally, define a node at level k positive when it is assigned to a node at level k by the ensemble, that is whenever q k ! t.</p><p>We start by considering a simplified setting, where for a node at level k, we have exactly one positive descendant for each of the m lower levels below. Considering that in this case, we have exactly one child for each level, to simplify the notation, we set q j k q k . Proposition 1. Influence of positive descendant nodes with one child. In a TPR-w ensemble, for a generic node at level k, with a given parameter w; 0 w 1, balancing the weight between parent and children predictors, and having exactly one positive descendant for each of the m lower levels below, that is such that q kþj &gt; t; 1 j m, the following equality holds for each m ! 1: Looking at the graph of fðwÞ for different values of m ! 1 (Fig. <ref type="figure" target="#fig_2">3</ref>), we can observe that the deeper the node is (the larger the value of m), the lesser is the influence of the node at level k þ m on the ancestor node at level k.</p><formula xml:id="formula_5">q k ¼ X mÀ1 j¼0 wð1 À wÞ j qkþj þ ð1 À wÞ m q kþm :<label>ð6Þ</label></formula><p>Remark 2. From Proposition 1, positive nodes above the deepest one, that is, nodes at level k þ j, j &lt; m, affect q k by the quantity wð1 À wÞ j qkþj ð6Þ. Considering the function gðwÞ ¼ wð1 À wÞ j , with w 2 ½0; 1, we have that arg max gðwÞ ¼ 1 jþ1 (see Appendix B for details). Fig. <ref type="figure" target="#fig_3">4</ref> shows, the plot of gðwÞ for different values of j: the nodes closer to the main node at level k have always larger influence independently of the value of w. Nevertheless, for large values of w (e.g., w ¼ 0:8), nodes below three levels (j ! 3) in practice are uninfluential on the node k. On the contrary, for small w values (e.g., w ¼ 0:1) also, nodes at deep levels can play a certain role. The vertical lines in Fig. <ref type="figure" target="#fig_3">4</ref> highlight the differences between the relative influence of the nodes at different levels j from k for, respectively, small, medium, and large values of the parameter w.</p><p>Remark 3. Proposition 1 and Remarks 1 and 2 show that the contribution of the positive descendant nodes decays exponentially with their level. The intensity of the decay depends on the parameter w, and the parameter w affects the contribution of the different levels (Figs. <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref>).</p><p>Proposition 1 can be easily extended to the more general case when we have a variable number of positive children for each descendant node. To this end, recalling that i is the set of children of i for which we have a positive prediction (3), we can extend its definition for a generic node at level k: k is the set of children of the generic node at level k for which we have a positive prediction, and (5) for a generic node at level k becomes:</p><formula xml:id="formula_6">q k ðxÞ ¼ w Á qk ðxÞ þ 1 À w j k ðxÞj X j2kðxÞ</formula><p>q j kþ1 ðxÞ: ð7Þ  We can observe that the quantity 1 j k ðxÞj P j2kðxÞ q j kþ1 ðxÞ in ( <ref type="formula">7</ref>) is the average of the probabilities computed by the positive children nodes of a generic node at level k. For brevity, we define this average as a kþ1 :</p><formula xml:id="formula_7">a kþ1 ¼ 1 j k j X j2 k qj kþ1 :<label>ð8Þ</label></formula><p>The average of the probability averages of the positive grandchildren of a node at level k is:</p><formula xml:id="formula_8">a kþ2 ¼ 1 j k j X j2k 1 j j kþ1 j X l2 j kþ1 ql kþ2 :<label>ð9Þ</label></formula><p>At a next level, the average of the averages of the probability averages of a node at level K is:</p><formula xml:id="formula_9">a kþ3 ¼ 1 j k j X j2 k 1 j j kþ1 j X l2 j kþ1 1 j l kþ2 j X r2 l kþ2 qr kþ3 :<label>ð10Þ</label></formula><p>By extending these definition across levels, we can obtain the following proposition that generalizes Proposition 1.</p><p>Proposition 2. Influence of positive descendant nodes with a variable number of children. In a TPR-w ensemble, for a generic node at level k, with a given parameter w; 0 w 1, balancing the weight between parent and children predictors, and having a variable number larger or equal than 1 of positive descendants for each of the m lower levels below, the following equality holds for each m ! 1:</p><formula xml:id="formula_10">q k ¼ wq k þ X mÀ1 j¼1</formula><p>wð1 À wÞ j a kþj þ ð1 À wÞ m a kþm :</p><formula xml:id="formula_11">Proof. See Appendix C. t u<label>ð11Þ</label></formula><p>Remark 4. Note that the form of Proposition 2 is very similar to that of Proposition 1, by substituting q k with a k . For this reason, Remarks 1, 2, and 3 can be extended to the general case when positive nodes have a variable number of positive children. Indeed, also in the general case, the contribution of the descendant nodes decays exponentially with their depth and depends critically on the choice of the w parameter.</p><p>To get a quantitative insight into the influence on a given node at level k of its positive descendants, using the results of Proposition 2, we can study the function:</p><formula xml:id="formula_12">hð qk ; a; w; mÞ ¼ wq k þ X mÀ1 j¼1</formula><p>wð1 À wÞ j a kþj þ ð1 À wÞ m a kþm ð12Þ by varying qk and w. We numerically simulated the results of the predictions of a generic node at level k in TPR-w by varying qk and w between 0 and 1; m is fixed to 5 (5 levels of positive descendants) and we considered the following cases for the averages a kþj , 1 j m:</p><p>Local rule:</p><p>No influence of descendant nodes. Constant rule: a kþj ¼ 0:75.</p><p>Decreasing rule: a kþj ¼ 1:1 À 0:1 Á j.</p><p>Increasing rule:</p><formula xml:id="formula_13">a kþj ¼ 0:5 þ 0:1 Á j.</formula><p>Of course these rules represent an oversimplification of real cases, because a kþj may vary with different and non monotone rules across levels. Nevertheless, this setting can be useful to shed light on the general behavior of the algorithm in simplified and well characterized situations. Fig. <ref type="figure" target="#fig_4">5</ref> summarizes the results. Fig. <ref type="figure" target="#fig_7">5a</ref> reports results when there is no influence of positive descendants (local rule). The decision depends only on the probability computed by the local predictor: a positive prediction is given when the probability of the local classifier is larger than 0.5. This situation happens when no positive descendants are available (that is, all the children of the node give a negative prediction). Note that this is equivalent to the situation of Flat ensembles that do not take into account the decisions of the other nodes. Figs. <ref type="figure" target="#fig_7">5a, 5b, 5c,</ref> and<ref type="figure" target="#fig_7">5d</ref> show that the decision of a node in a TPR-w ensemble is influenced by the positive decisions of its offsprings. Indeed, the pink and red areas (probability of the ensemble larger than 0.5) cross on the left-hand side the vertical dotted line representing the 0.5 probability of the local predictor, thus resulting in a modification of the negative decision of the local classifier to a positive one of the overall ensemble. It is worth noting that the value of the ensemble parameter w (abscissa of Fig. <ref type="figure" target="#fig_4">5</ref>) affects significantly the prediction of the ensembles: high values enforce the prediction of the parent classifiers, while low values increase the influence of the descendant nodes. The results depend of course also on the "strength" (i.e., the probability) of the predictions of the offsprings and by their level. Indeed, when the probabilities computed by the deeper nodes (decreasing rule, Fig. <ref type="figure" target="#fig_7">5c</ref>) are lower, even for low values of w (that is, giving more weight to the positive descendant nodes) the intensity of the modification toward a positive decision of the ensemble is significant, but lower with respect to the increasing rule case, where the probabilities computed by the deeper nodes are larger than those computed by high level nodes (Fig. <ref type="figure" target="#fig_7">5d</ref>). In Fig. <ref type="figure" target="#fig_7">5b</ref> (constant rule) we are in an intermediate case. These results are explained by Proposition 2, since the impact of lower level nodes on the decision of a node at level k decreases exponentially with their depth, but depends on the average of the probabilities computed by the positive descendants at each level, and on the global parameter w.</p><p>Summarizing, this section provides a quantitative analysis of the influence of the positive predictions of descendant nodes on a generic node at level k. This analysis shows that the posterior probability computed by a generic node of the ensemble depends nonlinearly on the weight parameter and on the nested average of the probabilities computed by the positive descendants nodes at each level. These results can also suggest some modifications of the TPR-w algorithm, as briefly discussed in Section 4.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL SETUP</head><p>We considered the functional classification of yeast genes at genome-wide level for a large number of classes structured according to the FunCat, a hierarchically tree-structured, controlled classification system enabling the functional description of proteins from any organism <ref type="bibr" target="#b8">[9]</ref>.</p><p>For each data set, we evaluated the performance of four different ensembles: the Flat ensemble, the hierarchical Topdown <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and the proposed TPR hierarchical ensemble and its weighted variant (TPR-w, Section 2.3).</p><p>The Flat ensemble does not take into account the hierarchical structure of the data and simply returns the predictions of each base classifier trained to recognize the genes belonging to a specific functional class. In other words, its output is the set of predictions made separately by the base learners without any correction due to the hierarchical relationships between the classes. The hierarchical Top-down algorithm classifies an example x, where d i ðxÞ is the classifier decision at node i and rootðT Þ denotes the set of nodes at the first level of the tree T , in the following way:</p><formula xml:id="formula_14">y i ¼ d i ðxÞ; if i 2 rootðT Þ; d i ðxÞ; if i 6 2 rootðT Þ AND y parðiÞ ¼ 1; 0;</formula><p>if i 6 2 rootðT Þ AND y parðiÞ ¼ 0:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">&lt; :</head><p>For each ensemble, we used two different types of SVM base learners (linear and gaussian). The probabilistic output of the SVMs composing TPR ensembles has been computed using the sigmoid fitting proposed in <ref type="bibr" target="#b29">[30]</ref>. The performance of the ensembles have been compared using 5-fold crossvalidation techniques. The selection of the w parameter in TPR-w ensembles has been performed by internal crossvalidation. The threshold t of TPR ensembles has been set to 0.5 in all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Sets</head><p>For the prediction of gene function in the yeast, we used seven biomolecular data sets. For each data set, we select only the genes annotated to FunCat, 1 using the HCgene R package <ref type="bibr" target="#b38">[39]</ref>. We also removed the genes annotated only with the "99" FunCat class ("unclassified proteins") and selected classes with at least 20 positive examples, in order to get a not too small set of positive examples for training. From the data sets we removed also uninformative features (e.g., features with the same value for all the available examples). At the end of these preprocessing steps, we obtained data whose characteristics are summarized in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 1 Biomolecular Data Sets Used in the Experiments</head><p>N. feat stands for the number of features.</p><p>1. We used the FunCat-2.1 scheme, and FunCat-2.1_data_20070316 data, available from the MIPS website (http://mips.gsf.de/projects/funcat). Pfam-1 data have been originally analyzed by Deng et al. <ref type="bibr" target="#b39">[40]</ref>: for each gene product the presence or absence of 4950 protein domains obtained from the Pfam (protein families) database <ref type="bibr" target="#b40">[41]</ref> is stored as a binary vector. Moreover, we used also an enriched representation of Pfam domains (Pfam-2), by replacing the binary scoring with log E-values obtained with the HMMER software tool kit <ref type="bibr" target="#b45">[46]</ref>.</p><p>Phylogenetic data (Phylo) are obtained through BLAST searches: each feature corresponds to the negative logarithm of the lowest E-value reported by BLAST version 2.0 in a search against a complete genome, with negative values (corresponding to E-values greater than 1) truncated to 0 <ref type="bibr" target="#b32">[33]</ref>.</p><p>We merged the experiments of Spellman et al. (gene expression measures relative to 77 conditions) <ref type="bibr" target="#b41">[42]</ref> with the transcriptional responses of yeast to environmental stress (173 conditions) by Gasch et al. <ref type="bibr" target="#b42">[43]</ref> to obtain the gene expression (Expr) data set.</p><p>Protein-protein interaction data (PPI-BG) have been downloaded from the BioGRID database, that collects PPI data from both high-throughput studies and conventional focused studies <ref type="bibr" target="#b43">[44]</ref>. BioGRID houses high-throughput twohybrid <ref type="bibr" target="#b46">[47]</ref>, mass spectrometric protein interaction data <ref type="bibr" target="#b47">[48]</ref>, and synthetic lethal genetic interactions obtained through synthetic genetic array and molecular barcode methods <ref type="bibr" target="#b48">[49]</ref>, as well as a vast collection of well-validated physical and genetic interactions from literature. Data are binary: they represent the presence or absence of proteinprotein interactions.</p><p>We also used another data set of protein-protein interactions (PPI-VM) that collects binary protein-protein interaction data from yeast two-hybrid assay, mass-spectrometry of purified complexes, correlated with mRNA expression, and genetic interactions <ref type="bibr" target="#b44">[45]</ref>. These data are binary too.</p><p>Finally, we considered pairwise similarities between yeast genes (SP-sim), by using data collected by William Noble and colleagues <ref type="bibr" target="#b49">[50]</ref>. They computed the Smith and Waterman log-E values between all pairs of yeast sequences, obtaining a symmetric matrix that expresses the pairwise similarities between yeast genes.</p><p>Different strategies can be chosen to select negative examples for each functional class <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In this work, negative examples for each class have been selected in such a way that they are not annotated for the class, but belong to the parent class (i.e., positive for the parent class). In this way, only negative examples that are not too dissimilar to the positive ones are selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Metrics</head><p>Considering the unbalance between positive and negative examples, we adopted the classical F-score to jointly take into account the precision and recall of the ensemble for each class of the hierarchy.</p><p>Nevertheless, the classical precision and recall measures, conceived for unstructured classification problems, appear to be inadequate to fully address the hierarchical nature of functional annotation. To this end, we used the hierarchical F-measure that represents a generalization of the classical F-measure <ref type="bibr" target="#b51">[52]</ref>.</p><p>Indeed, functional classes are structured according to a direct acyclic graph (GO) or to a tree (FunCat), and we need measures to accommodate not just "exact matches" but also "near misses" of different sorts. In other words, we need specific measures to estimate how far a predicted structured annotation is from a correct one. For instance, correctly predicting a parent or ancestor annotation, while failing to predict the most specific available annotation should be "partially correct", in the sense that we can gain information about the more general functional characteristics of a gene, missing only its most specific functions. To capture these characteristics of functional annotations, we should consider how much the entire path from the most specific upward to the more general annotation is correctly predicted or not. More precisely, given a general taxonomy G representing the graph of the functional classes, for a given gene/gene product x consider the graph P ðxÞ &amp; G of the predicted classes and the graph CðxÞ of the correct classes associated with x, and let be lðP Þ the set of the leaves (nodes without children) of the graph P . Given a leaf p 2 P ðxÞ, let be " p the set of ancestors of the node p that belong to P ðxÞ, and given a leaf c 2 CðxÞ, let be " c the set of ancestors of the node c that belong to CðxÞ, we can define the following definitions of hierarchical precision (HP), hierarchical recall (HR), and hierarchical F-score (HF) <ref type="bibr" target="#b51">[52]</ref>:</p><formula xml:id="formula_15">HP ¼ 1 jlðP ðxÞÞj X p2lðP ðxÞÞ max c2lðCðxÞÞ j"c \ "pj j "pj ; HR ¼ 1 jlðCðxÞÞj X c2lðCðxÞÞ max p2lðP ðxÞÞ j"c \ "pj j "cj ; HF ¼ 2 Á HP Á HR HP þ HR :<label>ð13Þ</label></formula><p>It is easy to see that in the case of the FunCat taxonomy, since it is structured as a tree, we can simplify HP , HR, and HF as follows:</p><formula xml:id="formula_16">HP ¼ 1 jlðP ðxÞÞj X p2lðP ðxÞÞ jCðxÞ \ "pj j "pj ; HR ¼ 1 jlðCðxÞÞj X c2lðCðxÞÞ j "c \ P ðxÞj j"cj ; HF ¼ 2 Á HP Á HR HP þ HR :<label>ð14Þ</label></formula><p>An overall high hierarchical precision is indicative of most predictions being ancestors of the correct predictions, or in other words that the predictor is able to detect the most general functions of genes/gene products. On the other hand, a high average hierarchical recall indicates that most predictions are successors of the actual, or that the predictors are able to detect the most specific functions of the genes. The hierarchical F-measure expresses the correctness of the structured prediction of the functional classes, taking into account also partially correct paths in the overall hierarchical taxonomy, thus providing in a synthetic way the effectiveness of the structured hierarchical prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND DISCUSSION</head><p>The performances of Flat, HTD, TPR, and TPR-w ensembles have been compared using fivefold cross-validation techniques, as explained in Section 3. We performed two nested cross-validation procedures with TPR-w ensembles . The "external" cross-validation has been applied to estimate the generalization capabilities of the ensemble, while the "internal" one to select the best w parameter. In this way, the selection of the best value for w was independent of the test data used in the external cross-validation.</p><p>We tested the ensembles using the "per-class" and hierarchical F-measures (Section 4.1), then we analyzed the performance of the ensembles at each level of the FunCat hierarchy (Section 4.2) and we also considered separated subtrees of the overall hierarchy related to the main functional categories of the taxonomy (Section 4.3). In Section 4.4, the precision/recall characteristics of TPR-w ensembles have been studied as a function of the global parameter w. We analyzed also whether the proper choice of the threshold parameter t (Section 2.3) can influence the performance of TPR-w ensembles (Section 4.5) and we investigated also the effectiveness of the TPR algorithms using different probabilistic base learners (Section 4.6). In Section 4.7, we evaluated whether local optimization techniques are able to improve the overall classification results, and finally, we discuss the main advantages and limitations of the proposed methods, considering both the theoretical analysis of the proposed algorithms (Section 2.4) and the experimental results presented in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">"Per-Class" and Hierarchical F-Measure Results</head><p>At first, we compared the performance of ensemble methods considering the "per-class" F-measure averaged across all FunCat classes for each data set. The results show that hierarchical methods largely outperform Flat ensembles (Fig. <ref type="figure" target="#fig_5">6</ref>). The first seven groups of barplots refer to the seven data sets used in the experiments (Table <ref type="table">1</ref>), while the last one reports the results averaged across all the data. Using linear SVM as base learners, Flat ensembles obtain an average Fmeasure across the seven data sets used in the experiments of 0.15 against, respectively, 0.22, 0.18, and 0.24 with HTD, TPR, and TPR-w ensembles. With gaussian SVMs Flat ensembles achieve an average F-measure of 0.14 against, respectively, 0.18, 0.19, and 0.23 with HTD, TPR, and TPR-w ensembles. The comparison of the algorithms using the Wilcoxon signed-ranks test <ref type="bibr" target="#b52">[53]</ref> shows that TPR-w ensembles outperform all other methods at 0.01 significance level with gaussian base learners, and at 0.02 significance level with linear SVMs, while no significant difference can be detected between HTD and TPR ensembles. The difference between hierarchical and Flat methods is always significant (Table <ref type="table">2</ref>).</p><p>As explained in the experimental set-up (Section 3), the Fhierarchical measure is a more appropriate performance metric for the hierarchical classification of gene functions. Note that we did not report the results obtained with Flat ensembles because in all cases they were very significantly worse than those achieved with hierarchical ensemble methods. Fig. <ref type="figure" target="#fig_6">7</ref> shows that on the average TPR-w achieves the best results using both linear and gaussian SVMs as base </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 2 Comparison between F-Scores of Ensemble Methods across the Data Sets</head><p>Each Cell Shows the P-values computed according to the Wilcoxon signed-ranks test; differences at 0.02 significance level are highlighted in bold. learners: 0.34 versus 0.25 and 0.29 with respect to TPR and HTD ensembles using linear SVMs, and 0.32 versus 0.27 and 0.25 with gaussian SVMs. <ref type="foot" target="#foot_1">2</ref> Note that TPR-w obtains equal or better results than HTD ensembles with respect to all the data sets. More precisely considering the three repetitions of fivefold cross validation results for each of the seven considered data sets (21 hierarchical classification tasks), TPR-w reported better results than HTD at 0.05 significance level on 14 tasks with linear SVMs and on 18 tasks with gaussian SVMs, according the 5-fold cross-validated paired ttest <ref type="bibr" target="#b53">[54]</ref>. Comparison between algorithms using the Wilcoxon signed-ranks test show that TPR-w ensembles significantly outperform all the other hierarchical methods, while the difference between Top-down and TPR is not significant (Table <ref type="table" target="#tab_1">3</ref>). These results show that we need the weighted version of TPR ensembles to enhance HTD predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Per-Level F-Measure Analysis</head><p>To get more insights into the reasons why TPR-w achieve better results, we performed a per-level analysis of the Fmeasure of the FunCat trees. Table <ref type="table" target="#tab_2">4</ref> shows the per level Fmeasure results with Pfam-1 protein domain data and pairwise sequence similarity data (SP-sim). Level 1 refers to the root nodes of the FunCat hierarchy, level i, to nodes at depth i, 2 i 5.</p><p>Looking at the results, we can observe that Flat ensembles tend to have the highest recall, Top-down the highest precision, while TPR-w tends to stay in the middle with respect to both the recall and precision, thus achieving the best F-measure at each level.</p><p>These results can be explained by analyzing the behavior of HTD and TPR-w algorithms. Indeed, for both hierarchical algorithms, we have a "negative" flow of information from top to bottom: when a node answers "no," its prediction is propagated to the subtree below, and the specificity and the precision are increased. Moreover, TPR-w shows also a positive flow of information in the opposite direction: "yes" answers of the descendants nodes influence the decisions of the parent nodes, thus improving the recall of the overall system. Indeed TPR-w recall tends to be larger than the corresponding Top-down recall at each level, and sometimes even larger than the recall of Flat ensembles. In other words, the two fluxes of information tend to balance precision and recall, thus resulting in an improved F-measure at each level of the FunCat hierarchy (Table <ref type="table" target="#tab_2">4</ref>).</p><p>Note that the accuracy is high at each level (at least with hierarchical ensemble methods), but these results are not significant, considering the large unbalance between positive and negative genes for each functional class (Table <ref type="table" target="#tab_2">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification of Specific Subtrees</head><p>Even if the main goal of this work consists in the development of a hierarchical algorithm that can be applied to the prediction of the overall taxonomy of a gene, we can restrict the analysis to specific subtrees of the taxonomy. For instance, with the tree rooted at the "protein fate" FunCat class (FunCat ID = 14), composed by 15 nodes (Fig. <ref type="figure">9</ref>), using Pfam-1 data we obtain an average precision equal to 0.79, an average recall of 0.48, and an average F-measure of 0.58 (Table <ref type="table" target="#tab_3">5</ref>), and for several classes we obtain an F-measure larger than 0.70, even if only one source of data is used for the hierarchical classification. Other results relative to specific subtrees of the FunCat taxonomy are available in the supplementary information. It is worth noting that for each subtree and more in general for each functional class the results largely depend on the choice of the data set. Indeed, each type of biomolecular data captures specific characteristics of genes that may correspond to different   functional features. For instance, gene expression data obtained from time series experiments can be informative to classify cell-cycle related to classes, while are less informative or unuseful to classify several other classes (e.g., classes related to signal transduction and interaction with the environment).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tuning Precision and Recall in TPR-w Ensembles</head><p>Another advantage of the TPR-w ensembles is the capability of tuning precision and recall rates, through the parameter parent weight w ð5Þ. Fig. <ref type="figure">8</ref> shows, the hierarchical precision, recall and F-measure as functions of the parameter w. For small values of w (w can vary from 0 to 1) the weight of the decision of the parent local predictor is small, and the ensemble decision depends mainly by the positive predictions of the offsprings nodes (classifiers): as a consequence we obtain a higher hierarchical recall for the TPR-w ensemble. On the contrary, higher values of w correspond to a higher weight of the parent predictor, with a resulting higher precision. The opposite trends of precision and recall are quite clear in all graphs of Fig. <ref type="figure">8</ref>. The best F-score is in "middle" values of the parameter parent weight: in practice in most of the analyzed data sets the best F-measure is achieved for w between 0.5 and 0.8, but if we need higher recall rates (at the expense of the precision) we can choose lower w values, and higher values of w are needed if precision is our first aim. Other results and graphs are available online in the supplementary information. The output of TPR ensembles is probabilistic, but the decision d i ðxÞ of the ensemble at node i for a given gene x depends on the threshold t (see lines 05 or 10 of the pseudocode of the algorithm, Fig. <ref type="figure" target="#fig_1">2</ref>). A reasonable choice for t is 0.5, but in this section, we analyze the behavior of the TPR algorithm by varying the value of the threshold t between 0 and 1, in order to understand whether a proper choice of t could improve the performance of the ensemble.</p><p>To this end, we studied the hierarchical precision, recall and F-score of two TPR-w ensembles trained, respectively, with linear and gaussian SVMs on the data sets Pfam-1 and PPI-VMs (Section 3.1), by varying the decision threshold between 0.05 and 0.95. Fig. <ref type="figure" target="#fig_10">10</ref> shows that the best results in terms of the F-hierarchical score (solid lines with crosses) are achieved when the threshold is close to 0.5: more precisely 0.45 with both Pfam-1 and PPI-VM data. It is worth noting that with values in the range [0.4, 0.6], we obtain in any case comparable results. As expected, the hierarchical recall decreases monotonically by increasing the threshold, while the precision at first increases, then reaches a quite "smooth" maximum and then decreases for the largest values of the threshold. The "smooth" maximum is due to the joint effect of the reduction of both false and true positives with the increment of the threshold, with opposite effects on the precision. Of course, when we have large values of the threshold, both precision and recall undergo a substantial reduction that leads to small values of the F-score. Summarizing, choosing a threshold close to 0.5 seems to be an acceptable choice, even if optimizing this parameter can lead to a slight improvement of the performance of the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Using Logistic Regression Classifiers as Base Learners</head><p>The proposed hierarchical ensembles can use different probabilistic classifiers as base learners. In this section, we compare the results obtained with Flat, Top-down, TPR, and TPR-w ensembles using probabilistic SVMs and logistic regression models as base classifiers. Logistic regression classifiers directly estimate the posterior probabilities of the classes <ref type="bibr" target="#b54">[55]</ref>, while probabilistic SVMs model posterior probabilities by fitting a sigmoid on the output of the discriminant function computed by the SVM algorithm <ref type="bibr" target="#b29">[30]</ref>.</p><p>The five-fold cross-validation results with Expr and PPI-VM data show that TPR-w ensembles achieve equal or better results than those obtained with the other methods, independently of the base learner used (Table <ref type="table" target="#tab_4">6</ref>). In particular, on these two data sets, logistic regression base classifiers behave better than probabilistic linear SVMs, and sometimes also better than probabilistic radial SVMs (e.g., with PPI-VM data, Table <ref type="table" target="#tab_4">6</ref>). Summarizing, these results show that the proposed TPR ensemble methods can be applied using different probabilistic base learners.   F-hierarchical measure with TPR-w ensembles and a slight decrement from 0.28 to 0.27 with TPR ensembles. With phylogenetic data the increment is from 0.22 to 0.24 with TPR-w and from 0.20 to 0.21 with TPR ensembles. These results (no influence of the C regularization factor and a certain impact of local cost-sensitive strategies) confirm previous findings observed with the classification of GO hierarchies <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Testing Local Optimization Techniques</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Advantages and Limitations of the TPR-Based</head><p>Algorithms F-hierarchical measures results show that TPR-w achieves equal or better results than the TPR and Top-down hierarchical strategy, and both hierarchical strategies achieve significantly better results than Flat classification methods, using the classical "per-class" F-measure, while no significant difference, according to the Wilcoxon signedranks test can be registered between Top-down and the basic TPR algorithm. The analysis of the per-level classification performances shows that TPR-w, by exploiting a global strategy of classification, is able to achieve a good compromise between precision and recall, enhancing the F-measure at each level of the taxonomy.</p><p>We can note for both hierarchical algorithms a degradation of both precision and recall (and as a consequence of the F-measure) by descending the levels of the trees. This fact could be at least in part due to the lack of annotations at the lowest levels of the hierarchy, where we may have several genes with unannotated specific functions. We also conjecture that experimenting with different strategies to assign negative examples in the training phase could improve the performance at the lower levels of the hierarchy <ref type="bibr" target="#b50">[51]</ref>.</p><p>Another advantage of TPR-w consists in the possibility of tuning precision and recall by using a global strategy: large values of the w parameter improve the precision, and small values the recall. The choice to favor precision or recall depends on the researcher's experimental objectives. In most data sets the best compromise between precision and recall is achieved for weights in the range between 0.5 and 0.8, that is giving a weight equal or larger to the parent predictor with respect to the predictions taken by its offsprings. In other words, we may obtain different precision-recall curves for different values of w: the weight is a global parameter that affects the general precision/ recall characteristics of the ensemble.</p><p>It is worth noting that we may vary the threshold t to obtain precision recall curves for a fixed value of w: this can slightly enhance the overall performance of TPR-w ensembles. Moreover, TPR and TPR-w ensembles provide also a probabilistic estimate of the prediction reliability for each functional class of the overall taxonomy.</p><p>In Section 2.4, we discussed how the decisions performed at each node of the hierarchical ensemble are influenced by the positive decisions of its descendants. The results of this analysis can be summarized as follows:</p><p>. Weights of descendants decrease exponentially w.r.t their depth. As a consequence the influence of descendant nodes decays quickly with their depth. . The parameter w plays a central role in balancing the weight of the parent classifier associated with a given node with the weights of its positive offsprings: small values of w increase the weight of descendant nodes, large values the weight of the local parent predictor associated with that node. . The effect on the overall probability predicted by the ensemble is the result of the choice of the w parameter , the strength of the prediction of the local learners and of its descendants. Different behaviors of the ensemble have been characterized according to some general rules by which probabilities are assigned to the positive offsprings (Section 2.4, Fig. <ref type="figure" target="#fig_4">5</ref>). These characteristics of TPR-w ensembles are well-suited for the hierarchical classification of gene functions, considering that annotations of deeper nodes are likely to have less experimental evidence than higher nodes. Moreover, by enforcing the strength of the descendant nodes through low w values, we can improve the recall characteristics of the overall system (at the expense of a possible reduction in precision).</p><p>Nevertheless, we can note that positive children of a node at level i of the hierarchy have the same weight, independently of the size of their hanging subtree. In some cases this could be useful, for the reasons discussed above, but in other cases it could be desirable to directly take into account the fact that a positive prediction is maintained along a path of the tree: indeed this witnesses for a positive annotation of the node at level i. The proposed algorithm weights the positive predictions of deeper nodes with an exponentially decrement with respect to their depth (Section 2.4), but other rules (e.g., linear or polynomial) could be considered as the basis for the development of new algorithms that put more weight on the decisions of deep nodes of the hierarchy.</p><p>Another possible research line could consist in the integration of the recently proposed hierarchical method based on isotonic regression <ref type="bibr" target="#b5">[6]</ref> with the TPR that governs both the GO and FunCat taxonomies. Indeed, this more general and principled approach to hierarchical classification could overcome the heuristic nature of the TPR algorithm, by adapting isotonic regression to the biological characteristics of gene function taxonomies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we proposed a new hierarchical strategy, inspired by the TPR, for gene function prediction extended to the overall functional taxonomy of genes.</p><p>TPR-w ensembles significantly outperform both the basic TPR and Top-down ensembles in the genome and ontologywide prediction of gene functions in S. cerevisiae. The analysis of the experimental results and a theoretical investigation of the flow of information that traverses the hierarchical ensemble show the reasons why TPR-w are well-suited to the prediction of gene functions, and suggest new research lines for the development of new hierarchyaware gene function prediction methods.</p><p>The overall results show that using a single source of evidence we can obtain a high precision and recall for specific trees of the FunCat forest. Nevertheless, we need to integrate multiple data sources to obtain methods to predict functions of hypothetical genes, or to discover or complete the functional annotation of genes whose function is incomplete or unknown. To this end, the proposed approach can be easily integrated with at least three different general strategies for biomolecular data integration: vector-space integration <ref type="bibr" target="#b32">[33]</ref>, kernel fusion <ref type="bibr" target="#b9">[10]</ref>, and ensemble methods <ref type="bibr" target="#b55">[56]</ref>. Indeed, for each node/class of the tree we may substitute a classifier trained on a specific type of biomolecular data with a classifier trained on concatenated vectors of different data, or trained on a (weighted) sum of kernels, or with an ensemble of learners each trained on a different type of data. This is the object of our planned future research.</p><formula xml:id="formula_17">q k ¼ wq k þ 1 À w j k j X j2k qj kþ1 ;<label>ð21Þ</label></formula><formula xml:id="formula_18">q j kþ1 ¼ wq j kþ1 þ 1 À w j j kþ1 j X l2 j kþ1 ql kþ2 :<label>ð22Þ</label></formula><p>By putting <ref type="bibr" target="#b21">(22)</ref> in <ref type="bibr" target="#b20">(21)</ref>:</p><formula xml:id="formula_19">q k ¼ wq k þ 1 À w j k j X j2k wq j kþ1 þ 1 À w j j kþ1 j X l2 j kþ1 ql kþ2 0 @ 1 A ¼ wq k þ wð1 À wÞ j k j X j2 k qj kþ1 þ ð1 À wÞ 2 j k j X j2 k 1 j j kþ1 j X l2 j kþ1 ql kþ2</formula><p>By using ( <ref type="formula" target="#formula_7">8</ref>) and ( <ref type="formula" target="#formula_8">9</ref>), we finally obtain:</p><formula xml:id="formula_20">q k ¼ wq k þ wð1 À wÞ a kþ1 þ ð1 À wÞ 2 a kþ2 :<label>ð23Þ</label></formula><p>We can repeat this procedure for m levels below a node at level k:</p><formula xml:id="formula_21">q k ¼ wq k þ wð1 À wÞ a kþ1 þ wð1 À wÞ 2 a kþ2 þ Á Á Á þ wð1 À wÞ mÀ1 a kþmÀ1 þ ð1 À wÞ m a kþm ¼ wq k þ X mÀ1 j¼1</formula><p>wð1 À wÞ j a kþj þ ð1 À wÞ m a kþm : u t</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. FunCat tree rooted at class 11 (Transcription): if example x belongs to class 11.06.03.01, then it belongs also to class 11.06.03, 11.06, and 11. On the contrary, if a gene x does not belong to class 11.02, it cannot belong, e.g., to class 11.02.03 or 11.02.03.01.</figDesc><graphic coords="3,29.65,51.17,244.12,170.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. TPR bottom-up hierarchical algorithm.</figDesc><graphic coords="4,29.48,51.17,244.46,414.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Plot of fðwÞ ¼ ð1 À wÞ m , while varying m from 1 to 10.</figDesc><graphic coords="5,33.45,51.17,236.52,217.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Plot of gðwÞ ¼ wð1 À wÞ j , while varying j from 1 to 10. The integers j refer to internal nodes at distance j from the reference node at level k.</figDesc><graphic coords="5,297.18,51.17,235.05,209.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Influence of positive descendants on the prediction of a node in TPR-w ensembles by varying the values of a kþj according to the local, constant, decreasing, and increasing rules. Abscissa: probability qk predicted by the parent local classifier; ordinate: value of the w parameter. Colors represent the probability computed by the ensemble (red: high probability; green: low probability). The vertical dotted line highlights the 0.5 probability computed by the local predictor for a node at level k. a) Local rule; b) constant rule; c) decreasing rule; and d) increasing rule.</figDesc><graphic coords="7,77.61,51.17,411.25,300.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Per-class F-measure comparison between Flat, Top-down, HTD, TPR, and TPR-w ensembles. (a) Linear SVMs. (b) Gaussian SVMs.</figDesc><graphic coords="9,36.57,51.17,230.40,260.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Hierarchical F-measures comparison between Top-down, HTD, TPR, and TPR-w ensembles. (a) Linear SVMs. (b) Gaussian SVMs.</figDesc><graphic coords="9,300.36,51.17,228.81,266.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4. 5</head><label>5</label><figDesc>Choice of the Decision Threshold and Its Impact on the Performance of TPR-w Ensembles</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Precision, recall, and F-measure as a function of the parent weight in TPR-w ensembles. PPI BioGRID data: (a) linear, (b) gaussian kernel; pairwise sequence similarity data, (c) linear, and (d) polynomial kernel.</figDesc><graphic coords="11,75.74,51.17,414.99,316.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Finally, we tested</head><label></label><figDesc>whether local optimization techniques can improve the overall performance of the TPR ensembles. We analyzed the results obtained through: a) computationally intensive "per node" model selection techniques, b) local cost-sensitive strategies. In both cases we used linear SVMs as base learners and only two data sets: PPI-VM and Phylo. Results obtained with 5-fold cross validation and internal threefold cross-validation for model selection did not result in an improvement of the overall performance of the hierarchical ensembles. This could seem quite surprising, but in this context, where examples can be at least partially mislabeled and data can be noisy (at least for certain data sets relatively to specific functional classes) a too intensive model selection can lead to overfitting. On the contrary, using local cost-sensitive strategies, we can achieve a certain improvement of the overall performances. More precisely, we imposed different costs for misclassifications of positive and negative examples for training linear SVMs, simply by setting the cost for misclassification of negative examples to one and the cost of misclassification of positive examples to the ratio between negative and positive examples in the training set. With PPI-VM data, we observed a slight increment from 0.40 to 0.41 of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Precision, recall, and F-measure as a function of the decision threshold t in TPR-w ensembles. (a) Pfam-1 data, (b) PPI-VM data.</figDesc><graphic coords="12,77.78,51.17,410.91,186.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 3</head><label>3</label><figDesc>Comparison between Hierarchical F-Scores of Ensemble Methods across the Data SetsEach cell shows the P-values computed according to the Wilcoxon signed-ranks test; differences at 0.05 significance level are highlighted in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 4</head><label>4</label><figDesc>Per-Level Precision, Recall, F-Measure, and Accuracy Comparison between Flat, Top-Down, TPR-w Ensembles Left: Pfam-1 protein domain data with gaussian SVMs as base learners. Right: SP-sim pairwise sequence similarity data with polynomial SVMs as base learners.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 5 Classification</head><label>5</label><figDesc>Results on the FunCat Tree Rooted at "Protein Fate"</figDesc><table /><note><p>Each row represents a functional class of the FunCat taxonomy. Prec. stands for precision, Rec. recall, Sp. specificity, F F-measure, and Acc. Accuracy.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 6</head><label>6</label><figDesc>Comparison of Hierarchical F-Measure Results Using Different Probabilistic Base Learners with Flat, Top-Down, TPR, and TPR-w Ensembles</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>VALENTINI: TRUE PATH RULE HIERARCHICAL ENSEMBLES FOR GENOME-WIDE GENE FUNCTION PREDICTION</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We used a polynomial kernel with SP-sim data, since the gaussian kernel showed convergence problems on this classification task.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The author would like to thank the anonymous reviewers for their comments and suggestions, and gratefully acknowledges partial support by the PASCAL2 Network of Excellence under EC grant no. 216886. This publication only reflects the author's views.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF PROPOSITION 1: INFLUENCE OF POSITIVE DESCENDANT NODES WITH ONE CHILD</head><p>In a TPR-w ensemble, for a generic node at level k, with a given parameter w; 0 w 1, balancing the weight between parent and children predictors, and having exactly one positive descendant for each of the m lower levels below, that is such that q kþj ! 0:5; 1 j m, the following equality holds for each m ! 1:</p><p>wð1 À wÞ j qkþj þ ð1 À wÞ m q kþm : ð15Þ</p><p>Proof. The proof is by induction. Considering that in TPR-w ensembles for a given node at level k, we have <ref type="bibr" target="#b6">(7)</ref>:</p><p>for m ¼ 1, having one positive child for a node at level k, we obtain:</p><p>and hence <ref type="bibr" target="#b14">(15)</ref> holds for m ¼ 1.</p><p>We suppose that ( <ref type="formula">15</ref>) is true for m ¼ n:</p><p>We show now that the equality holds also for m ¼ n þ 1. Indeed for the node at level k þ n, we have:</p><p>By substituting <ref type="bibr" target="#b17">(18)</ref> in <ref type="bibr" target="#b16">(17)</ref> we obtain:</p><p>wð1 À wÞ j qkþj þ wð1 À wÞ n qkþn þ ð1 À wÞ nþ1 q kþnþ1 ¼ X n j¼0 wð1 À wÞ j qkþj þ ð1 À wÞ nþ1 q kþnþ1 : u t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B COMPUTATION OF THE MAXIMUM OF gðwÞ</head><p>Considering the function gðwÞ ¼ wð1 À wÞ j , w 2 ½0; 1; j ! 1, we have that arg max gðwÞ ¼ 1 jþ1 . Indeed, dg dw ¼ ð1 À wÞ j À jwð1 À wÞ jÀ1 ¼ ð1 À wÞ jÀ1 ð1 À w À jwÞ; dg dw ¼ 0 () ðð1 À wÞ jÀ1 ¼ 0Þ _ ðð1 À w À jwÞ ¼ 0Þ;</p><p>Hence, we have:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>:</head><p>It is easy to see that w ¼ 1 is a minimum, w 6 ¼ 0; w 6 ¼ 1; j ! 1 is another minimum, while w ¼ 1 jþ1 is a maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C PROOF OF PROPOSITION 2: INFLUENCE OF POSITIVE DESCENDANT NODES WITH A VARIABLE NUMBER OF CHILDREN</head><p>In a TPR-w ensemble, for a generic node at level k, with a given parameter w; 0 w 1, balancing the weight between parent and children predictors, and having a variable number larger or equal than one of positive descendant for each of the m lower levels below, the following equality holds for each m ! 1:</p><p>wð1 À wÞ j a kþj þ ð1 À wÞ m a kþm :</p><p>Proof. If a generic node at level k has only one level of positive descendants (m ¼ 1), we have, by using (8):</p><p>If k has two levels of positive descendants (m ¼ 2), by using (9), we have:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated Protein Function Prediction-The Genomic Challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Friedberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="225" to="242" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Critical Assessment of Mus Musculus Gene Function Prediction Using Integrated Genomic Evidence</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pena-Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">S1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using Support Vector Machines for Classifying Large Sets of Multi-Represented Objects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kroger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pryakhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth SIAM Int&apos;l Conf. Data Mining</title>
		<meeting>Fourth SIAM Int&apos;l Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="102" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi Label Classification: An Overview</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Data Warehousing and Mining</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Empirical Study of Multi-Label Methods for Video Annotation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dimou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh Int&apos;l Workshop Content-Based Multimedia Indexing (CBMI &apos;09)</title>
		<meeting>Seventh Int&apos;l Workshop Content-Based Multimedia Indexing (CBMI &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhanced Hierarchical Classification via Isotonic Smoothing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Punera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int&apos;l Conf. World Wide Web</title>
		<meeting>17th Int&apos;l Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kernel-Based Learning of Hierarchical Multilabel Classification Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1601" to="1626" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gene Ontology: Tool for the Unification of Biology</title>
		<author>
			<persName><forename type="first">The</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ontology</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Genetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The FunCat, a Functional Annotation Scheme for Systematic Classification of Proteins from Whole Genomes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ruepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zollner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Albermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mokrejs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tetko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Guldener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mannhaupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Munsterkotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mewes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="5539" to="5545" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Statistical Framework for Genomic Data Fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>De Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2626" to="2635" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Bayesian Framework for Combining Heterogeneous Data Sources for Gene Function Prediction (in Saccharomices cerevisiae)</title>
		<author>
			<persName><forename type="first">O</forename><surname>Troyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat&apos;l Academy of Sciences USA</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="8348" to="8353" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast Protein Classification with Multiple Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Whole-Genome Annotation by Using Evidence Integration in Functional-Linkage Networks</title>
		<author>
			<persName><forename type="first">U</forename><surname>Karaoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat&apos;l Academy of Sciences USA</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="2888" to="2893" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An Efficient Strategy for Extensive Integration of Diverse Biological Data for Protein Function Prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="3364" to="3373" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Genome Wide Prediction of Gene Function via a Generic Knowledge Discovery Approach Based on Evidence Integration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining Guilt-by-Association and Guilt-by-Profiling to Predict Saccharomices cerevisiae Gene Function</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">S7</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inferring Mouse Gene Functions from Genomic-Scale Data Using a Combined Functional Network/Classification Strategy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Krumpelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marcotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">S5</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Structured-Outputs Method for Prediction of Protein Function</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second Int&apos;l Workshop Machine Learning in Systems Biology (MLSB &apos;08)</title>
		<meeting>Second Int&apos;l Workshop Machine Learning in Systems Biology (MLSB &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards Structured Output Prediction of Enzyme Function</title>
		<author>
			<persName><forename type="first">K</forename><surname>Astikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Holm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rousu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Proc</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">S2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting Novel Human Gene Ontology Annotations Using Semantic Analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Done</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khatri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Done</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Draghici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving Protein Prediction Using the Hierarchical Structure of the Gene Ontology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szafron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp. Computational Intelligence in Bioinformatics and Computational Biology</title>
		<meeting>IEEE Symp. Computational Intelligence in Bioinformatics and Computational Biology</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical Multilabel Classification Trees for Gene Function Prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic Modeling and Machine Learning in Structural and Systems Biology</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Rousu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Ukkonen</surname></persName>
		</editor>
		<imprint>
			<publisher>Helsinki Univ. Printing House</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gene Function Classification Using Bayesian Models with Hierarchy-Based Priors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shahbaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">448</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decision Trees for Hierarchical Multi-Label Classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="185" to="214" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Consistent Probabilistic Output for Protein Function Prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">S6</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Gene Ontology Consortium &quot;True Path Rule</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Integration of Relational and Hierarchical Network Information for Protein Function Prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nariai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steffen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kasif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kolaczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">350</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting Gene Function in a Hierarchical Context with an Ensemble of Classifiers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Barutcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Troyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">S2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical Multi-Label Prediction of Gene Function</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Barutcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Troyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="830" to="836" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Note on Platt&apos;s Probabilistic Outputs for Support Vector Machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic Annotation of Protein Function</title>
		<author>
			<persName><forename type="first">A</forename><surname>Valencia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Structural Biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="267" to="274" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Integrating Information for Protein Function Prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics-From Genomes to Therapies</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Lengauer</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1297" to="1314" />
			<date type="published" when="2007">2007</date>
			<publisher>Wiley-VCH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning Gene Functional Classification from Multiple Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Computational Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="401" to="411" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ensemble Based Data Fusion for Gene Function Prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eighth Int&apos;l Workshop Multiple Classifier Systems (MCS &apos;09)</title>
		<meeting>Eighth Int&apos;l Workshop Multiple Classifier Systems (MCS &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="448" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical Cost-Sensitive Algorithms for Genome-Wide Gene Function Prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Systems Biology</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="14" to="29" />
		</imprint>
	</monogr>
	<note>W&amp;C Proc.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weighted True Path Rule: A Multilabel Hierarchical Algorithm for Gene Function Prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Valentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First Int&apos;l Workshop Learning from Multi-Label Data (MLD &apos;09)</title>
		<meeting>First Int&apos;l Workshop Learning from Multi-Label Data (MLD &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="133" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Hierarchical Multi-Category Text Classification Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szdemak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int&apos;l Conf. Machine Learning</title>
		<meeting>22nd Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="745" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Incremental Algorithms for Hierarchical Classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tironi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zaniboni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="233" to="240" />
			<date type="published" when="2005">2005</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hcgene: A Software Tool to Support the Hierarchical Classification of Genes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Valentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="729" to="731" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An Integrated Probabilistic Model for Functional Prediction of Proteins</title>
		<author>
			<persName><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Seventh Int&apos;l Conf. Computational Molecular Biology</title>
		<imprint>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Pfam Protein Families Database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coggill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sammut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ceric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Forslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sonnhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bateman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="281" to="D288" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Comprehensive Identification of Cell Cycle-Regulated Genes of the Yeast Saccharomices cerevisiae by Microarray Hybridization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Biology of the Cell</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3273" to="3297" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Genomic Expression Programs in the Response of Yeast Cells to Environmental Changes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Biology of the Cell</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="4241" to="4257" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">BioGRID: A General Repository for Interaction Datasets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="D535" to="D539" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comparative Assessment of Large-Scale Data Sets of Protein-Protein Interactions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Snel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">417</biblScope>
			<biblScope unit="page" from="399" to="403" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Profile Hidden Markov Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="755" to="763" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Comprehensive Analysis of Protein-Protein Interactions in Saccharomyces cerevisiae</title>
		<author>
			<persName><forename type="first">P</forename><surname>Uetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Giot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cagney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Judson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lockshon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pochart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="page" from="623" to="627" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Systematic Identification of Protein Complexes in Saccharomyces cerevisiae by Mass Spectrometry</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gruhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heilbut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Millar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="180" to="183" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The Synthetic Genetic Interaction Spectrum of Essential Genes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davierwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mnaimneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Genetics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1147" to="1152" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Kernel-Based Data Fusion and Its Application to Protein Function Prediction in Yeast</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pacific Symp. Biocomputing</title>
		<meeting>Pacific Symp. Biocomputing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="300" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Choosing Negative Examples for the Prediction of Protein-Protein Interactions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A Categorization Approach to Automated Ontological Function Annotation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mnizewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Joslyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1544" to="1549" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Statistical Comparisons of Classifiers over Multiple Data Sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Approximate Statistical Test for Comparing Supervised Classification Learning Algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1895" to="1924" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Giorgio Valentini received the laurea degree in biological science and in computer science from the University of Genova, and the PhD degree in computer science from the DISI, Computer Science Department of the same university. He is currently an assistant professor at the DSI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Systems Biology</title>
		<imprint>
			<publisher>International Society of Computational Biology</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="98" to="111" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department of the University of Milano,</orgName>
		</respStmt>
	</monogr>
	<note>where he attends to both teaching and research. His main research interests include bioinformatics and machine learning. He is the author of about 80 papers published in international peer-reviewed journals, books, and conference proceedings. He is a member of the International Neural Network Society. For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
