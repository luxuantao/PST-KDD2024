<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analysis of Branch Prediction via Data Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">I-Cheng</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
							<email>icheng@eecs.umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">EECS Department</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<addrLine>1301 Beal Ave</addrLine>
									<postCode>48109-2122</postCode>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">T</forename><surname>Coffey</surname></persName>
							<email>scoffey@eecs.umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">EECS Department</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<addrLine>1301 Beal Ave</addrLine>
									<postCode>48109-2122</postCode>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Trevor</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EECS Department</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<addrLine>1301 Beal Ave</addrLine>
									<postCode>48109-2122</postCode>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analysis of Branch Prediction via Data Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Branch prediction is an important mechanism in modern microprocessor design. The focus of research in this area has been on designing new branch prediction schemes. In contrast, very few studies address the theoretical basis behind these prediction schemes. Knowing this theoretical basis helps us to evaluate how good a prediction scheme is and how much we can expect to improve its accuracy.</p><p>In this paper, we apply techniques from data compression to establish a theoretical basis for branch prediction, and to illustrate alternatives for further improvement. To establish a theoretical basis, we first introduce a conceptual model to characterize each component in a branch prediction process. Then we show that current "two-level" or correlation based predictors are, in fact, simplifications of an optimal predictor in data compression, Prediction by Partial Matching (PPM).</p><p>If the information provided to the predictor remains the same, it is unlikely that significant improvements can be expected (asymptotically) from two-level predictors, since PPM is optimal. However, there are a rich set of predictors available from data compression, several of which can still yield some improvement in cases where resources are limited. To illustrate this, we conduct trace-driven simulation running the Instruction Benchmark Suite and the SPEC CINT95 benchmarks. The results show that PPM can outperform a two-level predictor for modest sized branch target buffers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As the design trends of modern superscalar microprocessors move toward wider issue and deeper super-pipelines, effective branch prediction becomes essential to exploring the full performance of microprocessors. A good branch prediction scheme can increase the performance of a microprocessor by eliminating the instruction fetch stalls in the pipelines. As a result, numerous branch prediction schemes have been proposed and implemented on new microprocessors <ref type="bibr" target="#b11">[MReport95]</ref>.</p><p>Many researchers focus on designing new branch prediction schemes solely based on comparing simulation results. However, very few studies address the theoretical basis behind these prediction schemes. Knowing the theoretical basis helps us to assess how good a prediction scheme is as well as how much more we can improve the existing predictors.</p><p>To establish a theoretical basis, we first introduce a conceptual system model to characterize components in a branch prediction process. Using this model, we notice that many of the best prediction schemes <ref type="bibr" target="#b15">[Pan92,</ref><ref type="bibr" target="#b22">Yeh92,</ref><ref type="bibr" target="#b23">Yeh93,</ref><ref type="bibr" target="#b9">McFarling92,</ref><ref type="bibr" target="#b2">Chang94,</ref><ref type="bibr" target="#b14">Nair95b]</ref> use predictors similar to a "two-level" adaptive branch predictor <ref type="bibr" target="#b21">[Yeh91]</ref>. Then, we demonstrate that these "twolevel like" predictors are, in fact, simplified implementations of an optimal predictor in data compression, Prediction by Partial Matching (PPM) <ref type="bibr" target="#b3">[Cleary84,</ref><ref type="bibr" target="#b10">Moffat90]</ref>. This establishes a theoretical basis for current two-level predictors that can draw on the relatively mature field of data compression.</p><p>In particular, the potential benefit of applying data compression techniques to branch prediction is readily apparent in the similarity of predictors used in both methods. In practice, the predictors used in branch prediction are only a very small subset of predictors developed in data compression. To illustrate the potential improvement using data compression techniques, we conduct trace-driven simulations. The results show that PPM outperforms equivalent two-level predictor in the Instruction Benchmark Suite (IBS) <ref type="bibr" target="#b19">[Uhlig95]</ref> and the SPEC CINT95 [SPEC95] benchmarks. However, the improvement is not great, because two-level predictors are near optimal. In the case of modest size systems, the improvement is more significant. This paper is organized into seven sections. In section 2 we introduce a conceptual system model to describe the process and components of branch prediction. We then use this model to summarize current popular branch prediction schemes. In section 3, we show that data compression is relevant to branch prediction because it also requires prediction.</p><p>In section 4, we develop a theoretical basis for two-level predictors by demonstrating that they are simplified versions of an optimal predictor, PPM. Section 5 considers the implication of the availability of optimal predictors, and shows that, in some cases, branch prediction can still benefit from data compression. We verify this with trace-driven simulation running the IBS and the SPEC CINT95 benchmarks. Section 6 discusses the potential benefits from data compression and further improvement in each component of our conceptual prediction model. Finally, we present conclusions and further work in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Model and Overview of Branch Prediction</head><p>To explain branch prediction schemes, a conceptual view of a branch prediction scheme is introduced. This conceptual view allows us to compare various branch prediction schemes. It also enables us to focus and improve each component by clearly defining its function. This conceptual model elaborates on the one in <ref type="bibr">[Young95]</ref>. Our model extends it to accommodate most popular branch prediction schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A general conceptual system model for branch prediction</head><p>The general conceptual model we introduce for branch prediction consists of three major components: a source, an information processor, and a predictor, as illustrated in Figure <ref type="figure">1</ref>. Although some components are often combined in a hardware implementation, this three-part model is useful in explaining the principles behind different prediction schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Source</head><p>The source is simply the machine code of the programs we are running. The source contains program semantics and algorithmic information. To aid branch prediction, this information can be explored and extracted during the compile-time. It can be stored and passed on to be used during execution. A hint bit in branch instructions is one means of passing this information. In addition, the source can be modified to produce more predictable branches using statistics from previous test-runs. This is how code restructuring and code profiling work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1.2</head><p>Information processor In a hardware implementation, the information processor is often combined with predictors and, hence, overlooked. However, the information processor plays a key role in the prediction process and thus deserves a close study. Conceptually, it can be subdivided into two components: selector and dispatcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.1">Selector</head><p>The selector selects which run-time information should be used for branch prediction and encode it. This information can be branch address, operation code, branch outcome, target address, hint bits, or statistics from test-runs. Prediction accuracy depends heavily on the mix of run-time information that is employed.</p><p>Once the information is determined, the selector decides what formats to represent the information. For example, suppose branch outcomes and branch addresses are selected as information, the selector can combine the outcomes with addresses into one sin-gle stream or keep outcomes as individual streams classified by branch addresses. Good encoding can extract the essence of information, producing a concise and efficient representation to help prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.2">Dispatcher</head><p>The dispatcher determines how the information is mapped (fed) to the various predictors, since multiple information streams and predictors may exist in a prediction scheme. The mapping can be one-to-one, many-to-one, one-to-many, dedicated, or multiplexed (time-shared). Different mappings often have great influence on the final prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Predictor</head><p>A predictor is simply a finite-state machine that takes input and produces a prediction. It does not need to know the meaning of the input. Common examples are a constant or static predictor, a 1bit counter, a 2-bit up-down saturating counter <ref type="bibr" target="#b18">[Smith81]</ref>, and a Markov predictor. A Markov predictor forms the basis of recent two-level prediction schemes and is discussed in detail in Section 3. For the moment, a Markov predictor is simply a finitestate machine that generates predictions based on a finite number of previous inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Overview of current branch prediction schemes</head><p>Using the conceptual model just introduced, we can summarize current popular branch prediction schemes in Table <ref type="table" target="#tab_0">1</ref>. This table describes the basic components used in each prediction scheme. It lists whether source modification or profiling are used for each prediction scheme. For the information processor, it describes both the information used by the selector and the way dispatcher maps information. Finally, the predictor used in each prediction scheme is also listed. From this table, we notice that many of the best prediction schemes <ref type="bibr" target="#b15">[Pan92,</ref><ref type="bibr" target="#b22">Yeh92,</ref><ref type="bibr" target="#b23">Yeh93,</ref><ref type="bibr" target="#b9">McFarling92,</ref><ref type="bibr" target="#b2">Chang94,</ref><ref type="bibr" target="#b14">Nair95b]</ref> use Markov predictors. We will explain this further in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Compression and Prediction</head><p>Like branch prediction, data compression relies on prediction. In data compression, the goal is to represent the original data with fewer bits. The basic principle of data compression is to use fewer bits to represent frequent symbols, while using more bits to represent infrequent symbols. Thus, the net effect is to reduce the overall number of bits needed to represent the original data. In order to perform this compression effectively, a compression algorithm has to predict future data accurately to build a good probabilistic model for the next symbol <ref type="bibr" target="#b0">[Bell90]</ref>. Then, as shown in Figure <ref type="figure" target="#fig_1">2</ref>, the algorithm encodes the next symbol with a coder tuned to the probability distribution. Current coders can encode data so effectively that the number of bits used is very close to optimal and, consequently, the design of good compression relies on an accurate predictor. The problem of designing efficient and general universal compressors/predictors has been extensively examined. In our experiments we draw on these techniques, adapting them to the new context of branch prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Prediction by Partial Matching</head><p>Prediction by partial matching (PPM) is a universal compression/prediction algorithm that has been theoretically proven optimal and has been applied in data compression and prefetching <ref type="bibr" target="#b3">[Cleary84,</ref><ref type="bibr" target="#b6">Krishnan94,</ref><ref type="bibr" target="#b7">Kroeger96,</ref><ref type="bibr" target="#b10">Moffat90,</ref><ref type="bibr" target="#b20">Vitter91]</ref>. Indeed, it usually outperforms the Lempel-Ziv algorithm (found in Unix compress ) due to implementation considerations and a faster convergence rate [Curewitz93, Bell90, Witten94]. As described above, the PPM algorithm for text compression consists of a predictor to estimate probabilities for characters and an arithmetic coder. We only make use of the predictor. We encode the outcomes of a branch, taken or not taken, as 1 or 0 respectively. Then the PPM predictor is used to predict the value of the next bit given the prior sequence of bits that have already been observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Markov predictors</head><p>The basis of the PPM algorithm of order m is a set of ( m + 1) Markov predictors. A Markov predictor of order j predicts the next bit based upon the j immediately preceding bits-it is a simple Markov chain <ref type="bibr" target="#b16">[Ross85]</ref>. The states are the 2 j possible patterns of j bits. The transition probabilities are proportional to the observed frequencies of a 1 or a 0 that occur given that the predictor is in a particular state (has seen the bit pattern associated with that state). The predictor builds the transition frequency by recording the number of times a 1 or a 0 occurs in the ( j + 1)-th bit that follows the j -bit pattern. The chain is built at the same time that it is used for prediction and thus parts of the chain are often incomplete. To predict a branch outcome the predictor simply uses the j immediately preceding bits (outcomes of branches) to index a state and predicts the next bit to correspond to the most frequent transition  Figure <ref type="figure" target="#fig_2">3</ref> illustrates how a Markov predictor works. Let the input sequence seen so far be 010101101, and the order of Markov predictor be 2. The next bit is predicted based on the two immediately preceding bits, that is, 01. The pattern 01 occurs 3 times previously in the input sequence. The frequency counts of the bit following 01 are: 0 follows 01 twice, and 1 follows 01 once. Therefore, the predictor predicts the next bit to be 0 with a probability of 2/3. The (incomplete) 4-state Markov chain is shown at the left of the figure. Note that a 0-th order Markov predictor simply predicts the next bit based on the relative frequency in the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Combining Markov predictors to perform PPM</head><p>We noted earlier that the basis of a PPM algorithm of order m is a set of ( m + 1) Markov predictors. The algorithm is illustrated in Figure <ref type="figure" target="#fig_3">4</ref>. PPM uses the m immediately preceding bits to search a pattern in the highest order Markov model, in this case m . If the search succeeds, which means the pattern appears in the input sequence seen so far (the pattern has a non-zero frequency count), PPM predicts the next bit using this m th-order Markov predictor as described in the previous subsection. However, if the pattern is not found, PPM uses the ( m -1) immediately preceding bits to search the next lower order ( m -1)-th order Markov predictor. Whenever a search misses, PPM reduces the pattern by one bit and uses it to search in the next lower order Markov predictor. This process continues until a match is found and the corresponding prediction can be made.</p><p>There are a number of variations on how the frequency The Markov chain at left corresponds to the information collected from the input sequence in the table at right. Note that the chain is incomplete, because of 0 frequency count transitions.</p><p>0 1 0 1 0 1 1 0 1 ?</p><p>Input sequence: . . . ... information in the individual Markov predictors can be updated as the PPM process proceeds. In our experiments we use update exclusion . This means that we only update the frequency counters for the predictor that makes the prediction and the predictors with higher order. Lower order predictors are not updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Two-level Branch Prediction as an Approximation of PPM</head><p>In this section, we show that recently proposed two-level or correlation based predictors are approximations of, PPM, an optimal prediction algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Description of two-level predictor</head><p>Among the various branch prediction schemes, two-level or correlation based predictors are among the best. In addition, these predictors all share very similar hardware components. As Figure <ref type="figure" target="#fig_4">5</ref> shows, they have one or more shift-registers (branch history registers) to store history information in the first level and have one or more tables of 2-bit counters (pattern history tables) in their second level <ref type="bibr" target="#b21">[Yeh91]</ref>. The contents of the first level shift-registers are typically used to select a 2-bit counter in one of the second-level tables. Predictions are made based on the value of the 2-bit counter selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Two-level branch predictors as Markov predictors</head><p>From the above discussion on two-level adaptive branch predictors and the one on Markov predictors in Section 3.1.1, it can be seen that there are strong similarities. Though different schemes of two-level branch predictors exist, they differ only in what information is used for history and what subsets of branch outcomes are used to index and update the counters. As a result, there exists a corresponding Markov predictor for each scheme.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows the similarity between a two-level predictor and a Markov predictor. Both predictors behave exactly the same in the first level. They both use the last m bits of branch outcome to search the corresponding data structure. Note that an m -bit shift register serves two functions: first, it limits the information used for prediction to m previous outcomes and, second, it uniquely defines a finite-state machine in which each state has exactly two predefined next states. In the second level, the Markov predictor uses a frequency counter for each outcome, while the two-level predictor uses a saturating up-down 2-bit counter <ref type="bibr" target="#b18">[Smith81]</ref>. Whenever a branch is taken/not taken, the 2-bit counter increments/decrements. The decision for a two-level predictor depends on whether the value of the counter falls in the positive half or the negative half. Similarly, a Markov predictor simply predicts the next branch to be the most frequent outcome based on two frequency counters. Both predictors are utilizing a majority vote via different implementations. The saturating counter is an approximation to this that can be realized in hardware efficiently.</p><p>An interesting illustration is to see how a two-level predictor, the per-address branch history register with global pattern history table (PAg), corresponds to a Markov predictor. This peraddress scheme uses one table of 2-bit counters and multiple shift registers where each register records only outcomes of a particular branch. Although multiple shift registers exist, all shift registers operate the same and correspond to the same transition rule for a finite-state machine (state diagram). In addition, all shift registers share the same global table of 2-bit counters and, hence, share the same value (counter) in each state. Therefore, this per-address scheme uses one Markov predictor that is time-shared and updated among various branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Approximation to optimal predictors</head><p>Given the arguments in the previous sections, we now show that a two-level predictor is an approximation of an optimal predictor. As mentioned in Section 3.1, PPM is a theoretically proven optimal predictor consisting of a set of Markov predictors. Though performance is inferior at the beginning, a single Markov predictor can approach the performance of PPM in the long run (asymptotically) <ref type="bibr" target="#b0">[Bell90]</ref>. Furthermore, we have shown that a two-level predictor is a simplified Markov predictor. Therefore, we can see that a two-level predictor is an approximation of an optimal predictor, PPM.  Under hardware implementation constraints, we think that a two-level predictor is a reasonable simplification of PPM. The complete PPM predictor can be viewed as a set of two-level predictors, having not one size of predictor ( m ) but a set that spans m down to 0 (a simple two-bit counter-equivalent to a per-address predictor with zero history length). These extra small predictors help to reduce "cold starts," i.e., lack of information at the training period. Although two-level predictors do not include small predictors, they still can perform well since cold starts are far less severe in branch prediction than in text compression. To see how cold starts differ in the two fields, we consider the number of all possible combinations of m outcomes. In branch prediction, there are possible combinations since a branch has only two possible outcomes (taken or not taken). In text compression, on the other hand, there are roughly possible combinations where 128 is the number of printable ASCII symbols. Compared to the large number of branches executed in typical programs, these cold starts are negligibly small and hardly decrease the overall prediction accuracy. Another simplification made by two-level predictors is the use of a 2-bit counter instead of an n -bit counter. This is a costeffective choice, since two bits is the minimal number needed so that the direction of the predictor is not changed by the single exit in a loop statement <ref type="bibr" target="#b8">[Lee84,</ref><ref type="bibr" target="#b18">Smith81]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAg</head><p>As an aside, note that it is not coincidental that a 2-bit saturating up-down counter is the best among 4-state predictors <ref type="bibr" target="#b13">[Nair95a]</ref>. This is because, with four states, one 2-bit saturating updown counter is the best way to mimic the majority vote used in the Markov predictor. In the original Markov predictor, this voting prediction is done with two frequency counters (one for each outcome).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Impact of Optimal Predictors and Further Improvement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implication of optimal predictors</head><p>Having shown that a two-level predictor is an approximation of an optimal predictor, we have established a theoretical basis for this type of branch predictor. Rather than just comparing simulation results, which does not tell us how well these predictors perform in general, we can now have a reasonable degree of confidence in the performance of two-level predictors. It is unlikely that, by improving the predictor component alone, we can generate significant improvement in prediction accuracy excepting in special cases discussed below. This is because two-level predictors already perform close to optimal under the constraints imposed by the information they are given. Of course, the inclusion of more information (e.g., knowledge about the program executing) can always be used to improve the prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Illustration of modest improvements using PPM techniques</head><p>In this section, we illustrate that techniques from data compression can still, in some cases, yield modest improvements to branch prediction. To assess and confirm the potential improvement, we conduct trace-driven simulations. As input for the simulation, we use the Instruction Benchmark Suite (IBS) benchmarks <ref type="bibr" target="#b19">[Uhlig95]</ref> and the SPEC CINT95 benchmark suite [SPEC95] for our simulation.</p><p>The IBS benchmarks are a set of applications designed to reflect realistic workloads. The traces of these benchmarks are generated through hardware monitoring of a MIPS R2000-based workstation. We use the traces collected under the operating system Ultrix 3.1, which include both kernel-level and user-level instructions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IBS average</head><p>Average BTB miss rate = 29.9% size in bits of storage (K bytes) size in bits of storage (K bytes) misprediction rate (%) misprediction rate (%)</p><formula xml:id="formula_0">2 m 2 m 1 - 2 m 2 - ... 2 0 + + + +</formula><p>more features of optimal predictors can be added to achieve higher prediction accuracy. Examples would be the use of n -bit counters instead of 2-bit counters and the use of a variable length history instead of a fixed length. In the meantime, the design of predictors is an optimization under hardware constraints. Economic design and careful handling of details are required since every percent of improvement is important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Design of other optimal predictors</head><p>There are several optimal predictors with different costs and levels of efficiency. Furthermore, depending on the type of application, an optimal predictor may have different levels of efficiency <ref type="bibr" target="#b0">[Bell90]</ref>. For example, the Lempel-Ziv predictor (found in Unix compress ) and PPM are both optimal predictors. While the Lempel-Ziv predictor has a faster prediction speed, PPM has higher accuracy in general. Yet in the long run, they can both achieve optimal accuracy. Therefore, depending on the application and the speed constraint, we may prefer one to the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Design of efficient non-optimal predictors</head><p>A non-optimal, yet efficient, predictor may have higher accuracy than an optimal predictor in some cases. This happens when programs end or change behavior too soon before an optimal predictor can reach optimal accuracy. Therefore, though an efficient non-optimal predictor can never reach optimal accuracy, it may achieve higher accuracy in short or fast-changing programs. An example in data compression would be Dynamic Markov Compression (DMC) <ref type="bibr" target="#b0">[Bell90]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Improvement of the information processor</head><p>Even with optimal predictors, we can still increase accuracy by improving the information processor. Good information selection, encoding, and dispatching can extract the essence of branch behavior and, hence, improve prediction accuracy. In particular, this information processing is important since the predictor does not know the meaning of its input. Even using the same predictor, different information processing can result in prediction schemes with varied accuracy. Information describing branch behavior includes: branch address, branch outcome, operation code, target address, hint bits, and statistics from previous runs. How to best exploit and represent this information still remains to be studied. Examples of prediction schemes attempting to improve the information processor are the gshare scheme <ref type="bibr" target="#b9">[McFarling92]</ref> and the path correlation scheme <ref type="bibr" target="#b14">[Nair95b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Improvement of the source</head><p>We can fundamentally improve the predictability of the branches by changing the source and, thereby, their behavior. A more predictable source can be derived by adding algorithmic knowledge and run-time statistics from test-runs. The goal is to decrease the entropy of the source by making the outcomes of branches more unevenly distributed. An example is code restructuring with profiling information <ref type="bibr" target="#b1">[Calder94,</ref><ref type="bibr" target="#b24">Young94]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Further Work</head><p>In this paper, we establish the connection between data compression and branch prediction. This allows us to draw techniques from data compression to form a theoretical basis for branch prediction. In particular, we show that current two-level adaptive branch predictors are approximations of an optimal predictor, PPM. Based upon this theoretical basis rather than just simulation results, we can now have a reasonable degree of confidence in the performance of two-level predictors. Although two-level predictors are close to optimal if unlimited resources are available, PPM can still outperform two-level predictors when branch-target buffers are small. This is because PPM has better mechanisms for handling misses.</p><p>To illustrate directions for further improvement, we introduce a conceptual model, which consists of three components: a predictor, an information processor, and a source. For the predictor, we can borrow the rich set of predictors developed in data compression and apply them to branch prediction. However, since PPM is optimal, it is unlikely that significant improvement can be made by improving the predictor alone, except for the cases noted. Therefore, to further increase branch prediction accuracy, the focus should be on improving the information processor and the source.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 1: A conceptual system model for branch prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A two-step model for data compression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of a Markov predictor of order 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Prediction flowchart of a PPM predictor of order m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Popular variations of two-level predictors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A two-level branch predictor vs. a Markov predictor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Branch predictors as a subset of predictors used in data compression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Summary of current popular prediction schemes Predictor Coder</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Source</cell><cell cols="2">Information processor</cell><cell></cell></row><row><cell>Prediction scheme</cell><cell>modification</cell><cell></cell><cell></cell><cell>predictor</cell></row><row><cell></cell><cell>or profiling</cell><cell>selector</cell><cell>dispatcher</cell><cell></cell></row><row><cell>forward not-taken, backward taken</cell><cell>no</cell><cell>(address -target)</cell><cell>many-to-one</cell><cell>constant</cell></row><row><cell>2-bit counter</cell><cell>no</cell><cell>outcome, classified by address</cell><cell>one-to-one, mapped with address</cell><cell>2-bit counters</cell></row><row><cell>path correlation</cell><cell>no</cell><cell>target, in execution order</cell><cell>one-to-many, mapped with address</cell><cell>several Markov predictors</cell></row><row><cell>gshare</cell><cell>no</cell><cell>address, outcome, XOR together</cell><cell>one-to-one</cell><cell>a Markov predictor</cell></row><row><cell>GAg</cell><cell>no</cell><cell>outcome, in execution order</cell><cell>one-to-one</cell><cell>a Markov predictor</cell></row><row><cell>GAs</cell><cell>no</cell><cell>outcome, in execution order</cell><cell>one-to-many, mapped with address</cell><cell>several Markov predictors</cell></row><row><cell>PAg</cell><cell>no</cell><cell cols="2">outcome, classified by address many-to-one, multiplexed</cell><cell>a Markov predictor</cell></row><row><cell>PAs</cell><cell>no</cell><cell>outcome, classified by address</cell><cell>many-to-many, mapped with address</cell><cell>several Markov predictors</cell></row><row><cell>PSg</cell><cell>no</cell><cell cols="2">outcome, classified by address many-to-one, multiplexed</cell><cell>constant</cell></row><row><cell>branch correlation</cell><cell>yes</cell><cell>statistics from previous runs, hint bit</cell><cell>one-to-one, mapped with address</cell><cell>constant</cell></row><row><cell>hybrid predictor</cell><cell>yes</cell><cell>combinations of above</cell><cell>combinations of above</cell><cell>combinations of above</cell></row><row><cell>original</cell><cell></cell><cell>probability</cell><cell></cell><cell>compressed</cell></row><row><cell>data</cell><cell></cell><cell>model</cell><cell></cell><cell>data</cell></row><row><cell cols="3">Predicts the frequency</cell><cell cols="2">Based on the model,</cell></row><row><cell cols="2">of each symbol and</cell><cell></cell><cell cols="2">produces compressed</cell></row><row><cell cols="2">forms a model</cell><cell></cell><cell>encoded output</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Static and dynamic conditional branch counts in the IBS and SPEC CINT95 programs</head><label>2</label><figDesc></figDesc><table><row><cell>Input to the SPEC CINT95 benchmarks was a reduced input data set;</cell></row><row><cell>each benchmark was run to completion.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by Advanced Research Projects Agency under ARPA/ARO Contract Number DAAH 04-94-G-0327.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Text Compression</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reducing branch costs via branch alignment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 6th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="242" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Branch classification: a new mechanism for improving branch predictor performance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International Symposium on Microarchitecture</title>
				<meeting>the 27th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1994-11">November 1994</date>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data compression using adaptive coding and partial string matching</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="396" to="402" />
			<date type="published" when="1984-04">April 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Practical prefetching via data compression</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Curewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data</title>
				<meeting>the 1993 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ATOM: A flexible interface for building high performance program analysis tools</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eustace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Winter 1995 USENIX Technical Conference on UNIX and Advanced Computing Systems</title>
				<meeting>the Winter 1995 USENIX Technical Conference on UNIX and Advanced Computing Systems</meeting>
		<imprint>
			<date type="published" when="1995-01">January 1995</date>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal prediction for prefetching in the worst case</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
				<meeting>the 5th Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1994-01">January 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting file system actions from prior events</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D E</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX Winter Technical Conference</title>
				<meeting>USENIX Winter Technical Conference</meeting>
		<imprint>
			<date type="published" when="1996-01">January 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Branch prediction strategies and branch target buffer design</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="6" to="22" />
			<date type="published" when="1984-01">January 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Combining branch predictors. WRL Technical Note TN-36</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcfarling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Implementing the PPM data compression scheme</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1917">1917-1921, November 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m">Microprocessor Report</title>
				<meeting><address><addrLine>Sebastopol, CA</addrLine></address></meeting>
		<imprint>
			<publisher>MicroDesign Resources</publisher>
			<date type="published" when="1995-03">March 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Limits to branch prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-C</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Coffey</surname></persName>
		</author>
		<idno>CSE-TR-282-96</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal 2-bit branch predictors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="698" to="702" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic path-based branch correlation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on Microarchitecture</title>
				<meeting>the 28th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1995-11">November 1995</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving the accuracy of dynamic branch prediction using branch correlation</title>
		<author>
			<persName><forename type="first">S-T</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Rahmeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 5th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Introduction to probability models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Academic press</publisher>
			<pubPlace>London, United Kingdom</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Correlation and aliasing in dynamic branch predictors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sechrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Symposium on Computer Architecture</title>
				<meeting>the 23rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A study of branch prediction strategies</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<idno>SPEC95] SPEC CPU&apos;95</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Symposium on Computer Architecture</title>
				<meeting>the 8th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1981-05">May 1981. August 1995</date>
			<biblScope unit="page" from="135" to="148" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Manual</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Instruction Fetching: Coping with Code Bloat</title>
		<author>
			<persName><forename type="first">R</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sechrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Symposium on Computer Architecture</title>
				<meeting>the 22nd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="345" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimal prefetching via data compression</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science</title>
				<editor>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Moffat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Bell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">C</forename></persName>
		</editor>
		<meeting>the 32nd Annual IEEE Symposium on Foundations of Computer Science<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Van Nostrand Reinhold</publisher>
			<date type="published" when="1991-10">October 1991. 1994</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
	<note>Managing Gigabytes</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-level adaptive training branch prediction</title>
		<author>
			<persName><forename type="first">T-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Symposium on Microarchitecture</title>
				<meeting>the 24th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1991-11">November 1991</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alternative implementation of Two-Level Adaptive Branch Prediction</title>
		<author>
			<persName><forename type="first">T-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Symposium on Computer Architecture</title>
				<meeting>the 19th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1992-05">May 1992</date>
			<biblScope unit="page" from="124" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comparison of dynamic branch predictors that use two levels of branch history</title>
		<author>
			<persName><forename type="first">T-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Symposium on Computer Architecture</title>
				<meeting>the 20th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving the accuracy of static branch prediction using branch correlation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 6th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1994-06">1994. June 1995</date>
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
	<note>Proceedings of the 22nd International Symposium on Computer Architecture</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
