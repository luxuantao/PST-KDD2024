<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hallucinating face by position-patch</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics and Information Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shaanxi</region>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chun</forename><surname>Qi</surname></persName>
							<email>qichun@mail.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics and Information Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shaanxi</region>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hallucinating face by position-patch</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C4E67A3D706DB3F5826068DD3D38011A</idno>
					<idno type="DOI">10.1016/j.patcog.2009.12.019</idno>
					<note type="submission">Received 19 December 2008 Received in revised form 2 October 2009 Accepted 31 December 2009</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Face hallucination Super-resolution Position patch Training image pairs</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel face hallucination method is proposed in this paper for the reconstruction of a high-resolution face image from a low-resolution observation based on a set of high-and low-resolution training image pairs. Different from most of the established methods based on probabilistic or manifold learning models, the proposed method hallucinates the high-resolution image patch using the same position image patches of each training image. The optimal weights of the training image position-patches are estimated and the hallucinated patches are reconstructed using the same weights. The final highresolution facial image is formed by integrating the hallucinated patches. The necessity of two-step framework or residue compensation and the differences between hallucination based on patch and global image are discussed. Experiments show that the proposed method without residue compensation generates higher-quality images and costs less computational time than some recent face image super-resolution (hallucination) techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The details of facial features are crucial for identifying an individual from surveillance video, but the resolution of a face image is normally low in surveillance video, causing some losses of facial features. Therefore, in order to obtain detailed facial features for the purpose of recognition, it is necessary to infer a high-resolution face image from the low-resolution one and this technique is called face hallucination or face super-resolution <ref type="bibr" target="#b8">[9]</ref>. Super-resolution addresses various applications in a variety of important sectors, as diverse as medical imaging, satellite imaging, surveillance system, image enlarging in web pages, and restoration of old historic photographs.</p><p>A number of super-resolution techniques have been proposed in the past 10 years and are categorized into two classes. The approach to generate a high-resolution image based on multiple low resolution images of the same scene, which is called multipleframe super-resolution <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Super resolution reconstruction of image sequences is highly dependent on quality of the motion estimation between successive frames <ref type="bibr" target="#b5">[6]</ref>. These approaches are mainly applied to generic images without special consideration of the properties of face images. The approach to generate a high-resolution image from a single low-resolution image, with a set of training images from the same scene is called single-frame super-resolution <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">30]</ref>. For face hallucination, the utilization of the special properties of the faces is conductive to generate the high-resolution face images <ref type="bibr" target="#b16">[17]</ref>. In this work, we focus on single-frame face image super-resolution. Besides the classification mentioned previously, super-resolution can also be classified into general super-resolution and domain specific super-resolution according to the type of applied low-resolution images <ref type="bibr" target="#b30">[30]</ref>.</p><p>In recent years, the common learning-based face super-resolution algorithms usually involve two steps. The first step generates global face image keeping the main characteristics of the ground truth image using probabilistic method in Maximum a Posteriori (MAP) frame or manifold learning method such as Locally Linear Embedding (LLE); the second step produces residual image to compensate the results of the first step. However, the two-step framework or residue compensation may not be indispensable in all circumstances. Since the local model has higher reconstruction precision than global model, we can achieve face hallucination only using local model.</p><p>In learning-based super-resolution method, we need to make use of prior information from image training set as much as possible to hallucinate facial details. However, common algorithms result in the loss of non-feature information, for they usually incorporate dimensionality reduction methods such as principal component analysis (PCA), LLE, and locality preserving -projection (LPP) into their own operation. Non-feature information contributes to super-resolution. The reconstruction coefficients obtained through those algorithms do not contain non-feature information. Therefore, the loss of detailed facial information inevitably arises in the first step and additional residue compensation is indispensable in the second step. Different from the usual face hallucination, the proposed approach does not incorporate dimensionality reduction methods into process and the residue compensation is thus no longer necessary because the non-feature information is reserved and the reconstruction coefficients contain both feature information and non-feature information. Patch position in the face image is used as well as image features to synthesize a high-resolution face image from a low resolution one in our method. The image position-patches are used to hallucinate the high-resolution image, and position-patches need not be searched in the training set. Compared with neighbor patches that are widely used in face hallucination <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref> where neighborhood preservation for low-and high-resolution patches rarely holds, positionpatches hold the same position preservation between low-and high-resolution patches, leading to more satisfactory results than neighbor patches. In addition, most face hallucination algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">30]</ref> used global face model that results in low reconstruction precision, making additional residue compensation step indispensable in these algorithms.</p><p>In this study, we propose a one-step face hallucination based on position-patch instead of on a complicated probabilistic or manifold learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Baker et al. <ref type="bibr" target="#b0">[1]</ref> were the first to develop a hallucination method under a Bayesian formulation and proposed the term ''face hallucination''. In this method, it generates the high frequency details from a parent structure with the assistance of training samples. Liu et al. <ref type="bibr" target="#b6">[7]</ref> developed a two-step approach integrating a global parametric model with Gaussian assumption and a local non-parametric model based on Markov random field (MRF). Both of the methods incorporated the degradation function into the formulation to solve the final hallucinated result. Also, probabilistic model based face hallucination algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref> need large number of training images. Su et al. <ref type="bibr" target="#b20">[21]</ref> extracted multiorientation and multi-scale information of low-level facial features from the input low-resolution and high-resolution faces using a steerable pyramid. They adopted the pyramid parent structure and local best match to optimize the prior information in order to solve a Bayesian MAP problem. Enlightened by Liu's work <ref type="bibr" target="#b6">[7]</ref>, a number of existing works treated face hallucination as a two-step problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref>. Firstly, a global face image was generated which held the main characteristics of the original high-resolution face but lacked some detailed features. Secondly, a residue face image containing the high-frequency image information was synthesized and added to the global face image to obtain the final results. For example, in a two-phase face hallucination model developed by Zhuang et al. <ref type="bibr" target="#b9">[10]</ref>, the locality preserving hallucination algorithm combined LPP and radial basis function (RBF) regression together to hallucinate a global highresolution face. Details of the synthesized high-resolution face were further improved by residue compensation based on Neighbor Embedding <ref type="bibr" target="#b11">[12]</ref>. Wang et al. <ref type="bibr" target="#b8">[9]</ref> suggested the use of PCA to represent the structural similarity of face images; however, this can hardly maintain the global smoothness and visual rationality, especially at locations around the face contour and margin of the mouth. Inspired by LLE, a well-known manifold learning method, Chang et al. <ref type="bibr" target="#b11">[12]</ref> developed the Neighbor Embedding algorithm based on the assumption that the training low-and high-resolution images form manifolds with similar local geometry in two distinct feature spaces. Motivated by Chang's work <ref type="bibr" target="#b11">[12]</ref>, a number of face hallucinations methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> were developed based on Neighbor Embedding or using neighbor patch. However, Su's experiments <ref type="bibr" target="#b14">[15]</ref> showed that neighborhood preservation for low-and highresolution patches rarely holds. Jia et al. <ref type="bibr" target="#b19">[20]</ref> applied hierarchical tensor (multilinear) algebra to face hallucination to solve different facial modalities. Multilinear analysis <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> can be employed to model the correlation between two spaces with different modalities. Multilinear analysis is a general extension of traditional linear methods and two-dimensional tensor is similar to PCA. Although Jia provided a novel way to represent different facial modalities, his super-resolution approach still follows a two-step framework, integrating parametric global models with Gaussian assumption. Yang et al. <ref type="bibr" target="#b29">[29]</ref> applied the perspective of compressed sensing <ref type="bibr" target="#b23">[24]</ref> to super-resolution. The high-resolution image produced by the sparse representation approach might not satisfy the acquisition process assumed. The final result was generated by additional residue compensation step using backprojection method. Park et al. <ref type="bibr" target="#b30">[30]</ref> proposed a novel examplebased face hallucination method with an extended morphable face model. Since the global way in the first step of Park's method <ref type="bibr" target="#b30">[30]</ref> usually results in low reconstruction precision, the algorithm proposed additional methods to compensate for residual errors and preserve characteristics. This method has advantages in enhancing the resolution of face image and improving the performance of face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed face hallucination method</head><p>A two-dimensional face image is represented as a column vector of all pixels. Face image can be reconstructed from the optimal linear combination of the training face images because of the structural similarity <ref type="bibr" target="#b8">[9]</ref>. However, because facial image contains abundant information and each has its own characteristics, global reconstruction tends to result in low reconstruction precision and unsatisfactory results. We propose a reconstruction based on position-patch instead of a complicated model. Position in the face image is used as well as image features to reconstruct new image. Position-patches are defined as the patches in all training images that have the same position with certain patch in the face image input.</p><p>Each low-or high-resolution image is represented as a set of small image patches with overlap; each patch matrix is separated We consider each face image as a patch matrix fY P ði; jÞg</p><formula xml:id="formula_0">N P ¼ 1</formula><p>composed of overlapping square patches, where N is the number of patches in image Y. The patch located at row i and column j in the patch matrix is denoted as Y P ði; jÞ. Suppose that the square patch covers n Â n pixels. For low-resolution image fY P L ði; jÞg N p ¼ 1 : if n is an odd number, the patch Y P L ði; jÞ overlaps with its adjacent patches by size n Â ½ðnÀ1Þ=2, and its corresponding highresolution patch Y P H ði; jÞ covering qn Â qn pixels overlaps with its adjacent patches by size ðqnÞ Â ½qðnÀ1Þ=2. If n is an even number, the patch Y P L ði; jÞ overlaps with its adjacent patches by size n Â ðn=2Þ, and its corresponding high-resolution patch Y P H ði; jÞ covering qn Â qn pixels overlaps with its adjacent patches by size ðqnÞ Â ðqn=2Þ. The high-and low-resolution patch pair at the position (i, j) and their relationships with adjacent patches are shown in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reconstruction based on position-patch</head><p>Let Y m represent the training face images, m ¼ 1; 2; . . . ; M, where M is the number of training images. Each training face image is denoted in patches as fY mP ði; jÞg N p ¼ 1 , and its reconstruction weight matrix is w m ði; jÞ, where w m ði; jÞ represents the contribution of each training image patch located at position ði; jÞ to the reconstruction of the input face image patch located at the same position. All weights at the position ði; jÞ are constrained to have a sum of one.</p><p>Let X represent the testing face image and X is also denoted in patches as fX P ði;</p><formula xml:id="formula_1">jÞg N p ¼ 1 .</formula><p>For training images, the image patches located at position ði; jÞ such as Y mP ði; jÞ are defined as positionpatches of the image patch X P ði; jÞ.</p><p>We expect each patch X P ði; jÞ in the face image fX P ði; jÞg N p ¼ 1 to be represented by</p><formula xml:id="formula_2">X P ði; jÞ ¼ X M m ¼ 1 w m ði; jÞY mP ði; jÞþe<label>ð1Þ</label></formula><p>where e is the reconstruction error.</p><p>From (1), the optimal reconstruction weights are based on the minimization of the reconstruction error e: wði; jÞ ¼ arg min wmði;jÞ</p><formula xml:id="formula_3">:X P ði; jÞÀ X M m ¼ 1 w m ði; jÞY mP ði; jÞ: 2<label>ð2Þ</label></formula><p>where wði; jÞ is a M-dimensional weight vector of each reconstruction weight w m ði; jÞ, for m =1,y,M. Let</p><formula xml:id="formula_4">S ¼ X P ði; jÞ Á C T ÀY<label>ð3Þ</label></formula><p>where C is a column vector of ones and Y is a matrix with its columns being the training patches Y mP ði; jÞ. The local covariance matrix Z can be obtained by Z ¼ S T S. Eq. ( <ref type="formula" target="#formula_3">2</ref>) is a constrained least squares problem that has the following solution:</p><formula xml:id="formula_5">wði; jÞ ¼ ðZ À1 CÞ=ðC T Z À1 CÞ ð<label>4Þ</label></formula><p>A more efficient way to obtain wði; jÞ is to solve the linear system Z Á wði; jÞ ¼ C and then rescale the weights so that they sum up to one.</p><p>wði; jÞ are used to reconstruct the new image patch X P R ði; jÞ:</p><formula xml:id="formula_6">X P R ði; jÞ ¼ X M m ¼ 1</formula><p>w m ði; jÞY mP ði; jÞffiX P ði; jÞ ð 5Þ where X P R ði; jÞ is a vector and converted into a matrix to integrate the global image.</p><p>The global image can be synthesized by integrating the reconstructed patches according to the original position. Pixels of the overlapping regions in the reconstructed global image are obtained by averaging the pixels value in the overlapping regions between two adjacent patches reconstructed. Fig. <ref type="figure" target="#fig_1">3</ref> compares our reconstruction results with that from PCA and Wang's eigentransformation method <ref type="bibr" target="#b8">[9]</ref>. According to this figure, our reconstruction method is able to maintain detailed local features and generates better result compared to traditional reconstruction methods in the same resolution space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Super-resolution based on position-patch</head><p>Given the low-resolution image X L and the original high-resolution image X H that is q 2 times larger than X L , the degradation process can be expressed as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> </p><formula xml:id="formula_7">X L ¼ 1 q 2 X qÀ1 k ¼ 0 X qÀ1 l ¼ 0 X H ðqi þk; qj þlÞþnði; jÞ ð<label>6Þ</label></formula><p>where q is a positive integer and nði; jÞ represents random noise. If X H , X L and n are, respectively, L Â 1, K Â 1 and K Â 1 vectors, Eq. ( <ref type="formula" target="#formula_7">6</ref>) can be simplified as</p><formula xml:id="formula_8">X L ¼ HX H þ n<label>ð7Þ</label></formula><p>where H is a K Â L matrix. Eq. ( <ref type="formula" target="#formula_8">7</ref>) combines a smoothing and a down-sample step and can be rewritten in image patch as</p><formula xml:id="formula_9">fX P L ði; jÞg N p ¼ 1 ¼ HfX P H ði; jÞg N p ¼ 1 þ n<label>ð8Þ</label></formula><formula xml:id="formula_10">Smoothed &amp; Downsampled ( 1, ) Y i j - ( , 1) Y i j- ( , 1) Y i j+ qn ( 1, 1) Y i j - - n ( , ) Y i j ( 1, 1) Y i j - + ( 1, 1) Y i j + + ( 1, 1) Y i j + - ( 1, ) Y i j +</formula><p>Fig. <ref type="figure">2</ref>. The high-and low-resolution patch pair at the position (i, j) and their relationships with adjacent patches. For each patch X P L ði; jÞ in the input low-resolution image, the weight w m ði; jÞ is obtained from (1) to (4), satisfying the following equation:</p><formula xml:id="formula_11">X p L ði; jÞffi X M m ¼ 1 Y mP L ði; jÞw m ði; jÞ ð<label>9Þ</label></formula><p>Replacing each low-resolution image patch Y mP L ði; jÞ by its corresponding high-resolution sample Y mP H ði; jÞ, the result is denoted as</p><formula xml:id="formula_12">X M m ¼ 1 Y mP H ði; jÞw m ði; jÞ ¼ X p H ði; jÞ ð<label>10Þ</label></formula><p>From ( <ref type="formula" target="#formula_8">7</ref>) and ( <ref type="formula" target="#formula_12">10</ref>), without considering noise, we have</p><formula xml:id="formula_13">H Á X p H ði; jÞ ¼ X M m ¼ 1 H Á Y mP H ði; jÞw m ði; jÞ ¼ X M m ¼ 1 Y mP L ði; jÞw m ði; jÞ ð<label>11Þ</label></formula><p>From ( <ref type="formula" target="#formula_11">9</ref>) and ( <ref type="formula" target="#formula_13">11</ref>), we have</p><formula xml:id="formula_14">X p L ði; jÞffiH Á X p H ði; jÞ ð<label>12Þ</label></formula><p>From <ref type="bibr" target="#b11">(12)</ref>, we can see that the degradation of X p H ði; jÞ is close to the low-resolution input X P L ði; jÞ. All the high-resolution patches X p H ði; jÞ are integrated to form the final global high-resolution image fX P H ði; jÞg N p ¼ 1 according to the original position. Pixels of the overlapping regions in the final result are obtained by averaging the pixels value in the overlapping regions between two adjacent patches hallucinated.</p><p>Outline of our method is illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>. The proposed face hallucination algorithm is summarized as follows:</p><p>Step respectively, for m ¼ 1; 2; . . . ; M.</p><p>Step 2: For each patch X P L ði; jÞ: (a) Compute the reconstruction weights wði; jÞ. (b) Synthesize the high-resolution patch X P H ði; jÞ.</p><p>Step 3: Concatenate and integrate the hallucinated highresolution patches to form a facial image, which is the target high-resolution facial image fX P H ði; jÞg N p ¼ 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussions on the necessity of residue compensation</head><p>The common face hallucination methods usually follow a twostep framework. The first step generates a global image, and the second step compensates the image with details. We propose that the residue compensation is not indispensable if the face hallucination algorithm does not incorporate with dimension reduction methods such as PCA, LPP, which loss non-feature information. And in general, the face hallucination method using image patches produces less reconstruction errors and higherquality images than using global images.</p><p>Does our method need residue compensation? Motivated by Zhuang's method <ref type="bibr" target="#b9">[10]</ref> of residue compensation, we developed a residue compensation algorithm based on position-patch.</p><p>The pair-wise training set for residue compensation is obtained as follows: the low-resolution training image (1,1)  Under what circumstances is residue compensation indispensable?</p><formula xml:id="formula_15">R L ¼ Y m L ÀDðX m H1 Þ,</formula><formula xml:id="formula_16">X (1, 2) X ( , ) X i j (1,1) Y (1,1) Y (1,1) Y ( , ) Y i j ( , ) Y i j ( , ) Y i j (1,1) Y (1,1) Y (1,1) Y ( , ) Y i j ( , ) Y i j ( , ) Y i j (1,1) X (1, 2) X ( , ) X i j ( ) 1, 1 w ( ) 1, 1 w ( ) 1, 1 w . . .</formula><p>We changed our patch-based algorithm to global image based algorithm (viewing the whole image as one position patch).</p><p>Eq. ( <ref type="formula" target="#formula_2">1</ref>) can be rewritten as follows:</p><formula xml:id="formula_17">X L ¼ X M m ¼ 1 w m Y m L þ e<label>ð13Þ</label></formula><p>where X L is the input low-resolution image and Y m L represents the low-resolution training image.</p><p>From ( <ref type="formula" target="#formula_2">1</ref>)-( <ref type="formula" target="#formula_5">4</ref>), we have</p><formula xml:id="formula_18">w ¼ ðZ À1 CÞ=ðC T Z À1 CÞ ð 14Þ Z ¼ ðXÀYÞ T ðXÀYÞ ð<label>15Þ</label></formula><p>where X ¼ X L Á C T , C denotes a column vector of ones and Y is a matrix with its columns being the training patches Y m L . w is a M-dimensional weight vector by stacking each reconstruction weights w m . The estimated weight w maintains the minimization of the reconstruction error.</p><p>From <ref type="bibr" target="#b9">(10)</ref>, the step-one result X H1 is obtained by the following equation:</p><formula xml:id="formula_19">X H1 ¼ X M m ¼ 1 w m Y m H<label>ð16Þ</label></formula><p>Step-one results are shown in Fig. <ref type="figure" target="#fig_7">6</ref>(b). It can be seen that high-resolution faces hallucinated using the global image approach are more blurred and need further step to enhance the results.</p><p>The residue compensation algorithm based on position-patch depicted in the previous section is performed to the step-one result:</p><formula xml:id="formula_20">X H ¼ X H1 þ R<label>ð17Þ</label></formula><p>where R represents the residue image and X H is the final result with residue compensation. Results from the two steps are shown separately in Fig. <ref type="figure" target="#fig_7">6</ref>. Figs. <ref type="figure" target="#fig_7">6</ref> and<ref type="figure" target="#fig_5">5</ref> show that our algorithm in global image way results in more reconstruction errors than patch way. Thus, residue compensation is indispensable to the hallucinated faces acquired by the approach based on global image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results and discussion</head><p>Our face hallucination algorithm was performed on the CAS-PEAL Face Database <ref type="bibr" target="#b15">[16]</ref>, FERET database <ref type="bibr" target="#b25">[26]</ref>, CMU database <ref type="bibr" target="#b26">[27]</ref> and Stereo-pair database <ref type="bibr" target="#b27">[28]</ref>. We randomly selected 300 normal expression images of different people under the same light condition in CAS-PEAL Face Database <ref type="bibr" target="#b15">[16]</ref>. These face images were aligned manually using the locations of three points: centers of left and right eyeballs and center of the mouth. According to interesting, we cut out the region of the faces and all faces are standardized to the size of 128 Â 96 or 119 Â 92.   set is too small, a lot of the individual characteristics fail to be rendered.</p><p>Our performance was also quantified by evaluating the peak signal-to-noise ration (PSNR) between the ground truth face images and the hallucinated images based on a different number of training samples as shown in Fig. <ref type="figure" target="#fig_9">8</ref> (2) Hallucination of face with glasses or expression: We studied the hallucination performance with test set containing face images with glasses or expression and training set containing face images without glasses or expression. Two examples are shown in Fig. <ref type="figure" target="#fig_10">9</ref>.</p><p>Although the training set contain samples without glasses or expression, Fig. <ref type="figure" target="#fig_9">8</ref> indicates that our hallucination algorithm can achieve results with glasses or expression as same with the input image. Our results preserve the characteristics of the input image since the input image is approximated by position-patches in algorithm, while some global image based hallucination methods, such as <ref type="bibr" target="#b8">[9]</ref>, preserves the training set image's characteristics.</p><p>(3) Impact of the size of position-patch: Fig. <ref type="figure" target="#fig_11">10</ref> illustrates the hallucination performance using position-patch with different sizes. It can be seen that the quality of the reconstructed image degrades as the size of the position patch increases; also position-patches with the size 3 Â 3 and 4Â 4 are associated with reconstructed images of the best quality. We also quantified our performance by evaluating the PSNR between the ground truth face images and the hallucinated images using position patch of different sizes. As shown in Fig. <ref type="figure" target="#fig_12">11</ref>, the PSNR value of the reconstructed image gradually decreases as the size of the position-patch increases; also position patches with the size of 3 Â 3 and 4 Â 4 have the highest PSNR values. Based on this experiment, the size of 3 Â 3 or 4Â 4 is the ideal choice for position patch.</p><p>(4) Impact of variations in face pose: Since the input image is approximated by position-patches in our algorithm, our method preserves the characteristics of the input image. Fig. <ref type="figure" target="#fig_6">12</ref> indicates that our method is also applicable to face image input with pose variations.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The number of training image pairs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Impact of noise</head><p>Zero mean Gaussian noise is added to the low-resolution face images in Fig. <ref type="figure" target="#fig_6">13</ref>.</p><p>As shown in Fig. <ref type="figure" target="#fig_6">13</ref> the hallucination results retain most of the facial characteristics, but the noise distortion cannot be removed. Because the input image is approximated by our algorithm and the input image's characters are retained, the noise in input lowresolution face is also preserved. Some hallucination methods based on global image preserve the characteristics of the training set, such as <ref type="bibr" target="#b8">[9]</ref>, can remove the noise well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion: neighbor-patch vs. position-patch</head><p>In 2004, Chang et al. <ref type="bibr" target="#b11">[12]</ref> was the first to apply LLE to superresolution and developed the Neighbor Embedding algorithm. Motivated by LLE, they selected neighbor-patches in low-resolution training sets and computed the reconstruction weights. Based on the assumption that low-and high-resolution spaces share similar local distribution structure, the reconstruction weights obtained in low-resolution space are used directly for synthesizing high-resolution images. Since then, face hallucination algorithms based on image patch have begun to use neighbor-patches to reconstruct high-resolution image. For example, Park et al. <ref type="bibr" target="#b12">[13]</ref> proposed a method to update the test image iteratively in low-and high-resolution spaces and compensate the considerable difference of neighborhood relationships between low-and high-resolution images, Liu <ref type="bibr" target="#b10">[11]</ref> proposed another model that effectively learns neighbor transformations between low-and high-resolution image patch residuals to compensate Neighbor Embedding results.</p><p>Which reconstruction is better, neighbor or position? We compare the neighbor-patch based Neighbor Embedding with our position-patch based method. The number of neighbor-patches in Neighbor Embedding is denoted as K. The image pairs of 150 individuals are used for training. The neighbor-and positionpatch are of equal size, 3 Â 3. Some representative results are shown in Fig. <ref type="figure" target="#fig_6">14</ref>.</p><p>As indicated by Fig. <ref type="figure" target="#fig_6">14</ref>, the hallucinated results from the position-patch keep more detailed facial features than neighborpatch does.</p><p>When a patch is too small, it loses the geometrical information of a human face, so the super-resolution reconstruction image becomes blurry <ref type="bibr" target="#b12">[13]</ref>. On the other hand, as a patch becomes larger, it needs much more training images to extract reliable generalized basis. To make further comparison with Neighbor Embedding, we choose the optimal size of 32 Â 32 <ref type="bibr" target="#b12">[13]</ref> whose corresponding low-resolution size is 8 Â 8. Our method still uses the size of 3 Â 3. The image pairs of 150 individuals are used for training. Some representative results are shown in Fig. <ref type="figure" target="#fig_6">15</ref>.</p><p>As shown by Figs. 14 and 15, Neighbor Embedding using 8 Â 8 patch generates high-resolution face images with more detailed facial features than the 3 Â 3 patch do, but the results using position-patch are still more similar to the original image with good image quality. Neighbor patch based on LLE is not the optimal reconstruction patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparative experiment</head><p>We compared our approach with recent methods based on the same training set. These methods are Cubic-B-Spine, Wang's eigentransformation method <ref type="bibr" target="#b8">[9]</ref>, Chang's Neighbor Embedding <ref type="bibr" target="#b11">[12]</ref> and Zhuang's Locality Preserving method <ref type="bibr" target="#b9">[10]</ref>. The optimal patch size of 32 Â 32 <ref type="bibr" target="#b12">[13]</ref> was chosen in Neighbor Embedding method whose corresponding low-resolution size is 8 Â 8. The number of neighbor-patches for reconstruction in Neighbor Embedding is 150. Image pairs of 150 people were used for training in Zhuang's method <ref type="bibr" target="#b9">[10]</ref>, Neighbor Embedding and our (c) Wang's method <ref type="bibr" target="#b8">[9]</ref> 128 Â 96. s is the standard deviation of Gaussian noise. method. In order to achieve the optimal results in Wang's method <ref type="bibr" target="#b8">[9]</ref>, we used image pairs of 270 people for training and let variance contribution rate of PCA be 0.9999. We selected h ¼ 135, K 1 ¼ 8, K 2 ¼ 5 mentioned in <ref type="bibr" target="#b9">[10]</ref> to optimize the results of Zhuang's method <ref type="bibr" target="#b9">[10]</ref>. The size of the image patch used in our method was 3 Â 3. Images of 20 people were used as test images. These images were blurred using a 7 Â 7 Gaussian filter with s=0.85, and down-sampled to 32 Â 24.</p><p>Some representative hallucinated results are shown in Fig. <ref type="figure" target="#fig_15">16</ref>.</p><p>The hallucinated results of Wang's method <ref type="bibr" target="#b8">[9]</ref> can hardly maintain global smoothness and visual rationality, especially on locations around the face contour and margin of the mouth. Wang's method <ref type="bibr" target="#b8">[9]</ref> is on the basis of statistical model. If the training number of Wang's method <ref type="bibr" target="#b8">[9]</ref> is reduced to 150 as same with others, the quality of the results will become worse. Zhuang's method <ref type="bibr" target="#b9">[10]</ref> generates more detailed facial features than Wang's, but there is more noise at the local facial features such as the face contour, nostril and the eyebrow, because LPP adopted in the first step resulted in the loss of non-feature information. Also, first step based on global image usually give rise to low reconstruction precision. To improve results from the first step, Zhuang inevitably developed the residue compensation based on patch in the second step and qualities of the final results rely on the residue compensation. Our method does not incorporate dimensionality reduction methods into the hallucination and instead based on patches, which brings about the best results without residue compensation.</p><p>To quantify the results from the ground truth data, we also computed the PSNR for each method. All the results are shown in   With regard to the computational complexity and the execution time of the proposed approach, Zhuang's method <ref type="bibr" target="#b9">[10]</ref> takes more than 10 min using PC with 3.0 G CPU to compute an image, this includes the generation of the residue training set, Neighbor Embedding takes more than 4 min because it requires the search of neighbor patches in image training. In comparison, our method only takes around 1 min. In addition, Wang's method <ref type="bibr" target="#b8">[9]</ref> maintains high computational speed because it performs in the global image way.</p><p>We also performed our comparison on FERET database <ref type="bibr" target="#b25">[26]</ref> and Stereo-pair database <ref type="bibr" target="#b27">[28]</ref>. Some representative results are shown in Fig. <ref type="figure" target="#fig_18">18</ref>.</p><p>Fig. <ref type="figure" target="#fig_18">18</ref> shows that our method has the same performance in spite of the different face databases used. Our method preserves the characteristics of the input low-resolution image and is adaptive to diverse inputs.</p><p>From the comparison above, it can be concluded that the proposed method generates images more similar to the original images and uses less computational time than some of the other face hallucination methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments with real world image</head><p>Two real world images from CMU database <ref type="bibr" target="#b26">[27]</ref> and a camera are used to further testify our method.</p><p>We used face images from a subset of FERET database <ref type="bibr" target="#b25">[26]</ref>, CAS-PEAL Face Database <ref type="bibr" target="#b15">[16]</ref> and Stereo-pair database <ref type="bibr" target="#b27">[28]</ref> to form an experimental dataset consisting of 150 face images of 150 different individuals. These face images were standardized to the size of 128 Â 96 as high-resolution training images using the same process mentioned above. The 150 high-resolution images were downsampled to low-resolution 32 Â 24 images, and the training set is composed by these 150 high-and low-resolution image pairs. The low-resolution faces in Figs. <ref type="figure" target="#fig_19">19(a</ref>) and 20(a) were aligned, extracted manually and standardized to the size of 32 Â 24 as inputs. Several Figs. <ref type="bibr" target="#b18">19</ref> and 20 show that our method is applicable to the real world pictures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this study, we designed a face hallucination method based on position-patch. Experiments show that the proposed method costs less computational time and generates results with the best image quality compared to some recent algorithms. Since the input image is approximated by position-patches in algorithm, our method preserves the characteristics of the low-resolution image input. If the input face images have noise, our method will enlarge the noise instead of removing it. And global image based face hallucinations are able to retain the characteristics of the training set. In this case, if the input face images have noise or glasses but the training images do not, these hallucination methods can remove the noise or glasses. Non-feature information contributes to super-resolution, and the reconstruction coefficients obtained by common algorithms do not contain the non-feature information. Thus, the loss of some detailed facial information inevitably occurs in the first step and additional residue compensation is indispensable in such circumstances. In addition, global reconstruction tends to result in low reconstruction precision. That is the reason why common face hallucination usually follows a two-step framework and requires residue compensation. In comparison to the complicated probabilistic model or manifold learning based face hallucination methods, our method is much simpler and easy to implement. Because the degradation process is diverse in real world but the degradation matrix assumed in algorithm is fixed, the hallucination results may not be satisfactory to all the real world images, which is also a limitation of other face hallucinations. One possible direction for future research may be to broaden the applications of face hallucination in the real world setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The definition of image patch matrix-vector conversion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparative results of reconstruction: (a) PCA; (b) Wang's method [9]; (c) our method; and (d) original image.</figDesc><graphic coords="3,310.25,58.64,234.00,85.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>Denote the input low resolution image, low resolution training image and high resolution training image in overlapping patches as fX P L ði; jÞg N p ¼ 1 , fY mP L ði; jÞg N p ¼ 1 and fY mP H ði; jÞg N p ¼ 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where D is the down-sample function and X m H1 is Y m L 's corresponding high-resolution image produced by our face hallucination based on position-patch. The high-resolution training image R H can be obtained from R H ¼ Y m H ÀX m H1 . We consider each low-and high-resolution residue face as a position-patch matrix denoted in patches, respectively, as fR mP L ði; jÞg N p ¼ 1 and fR mP H ði; jÞg N p ¼ 1 . The residue compensation algorithm based on position-patch can be summarized as follows: Step 1: Compute the input low-resolution face residue fR P XL ði; jÞg N p ¼ 1 by subtracting the down-sampled version of the hallucinated high-resolution face from the original lowresolution input face. Step 2: For each residue patch R P XL ði; jÞ: (a) Compute the reconstruction weights wði; jÞ based on the position patches introduced in the previous subsection. (b) Synthesize the high-resolution residue patch R P XH ði; jÞ.Step 3: Concatenate and integrate the hallucinated highresolution patches to form the target high-resolution residue image.We add the inferred residue image fR P XH ði; jÞg N p ¼ 1 to the superresolution image fX P H ði; jÞg N p ¼ 1 hallucinated by position-patch. The result X Ã H is shown in Fig. 5, where X Ã H ¼ fX P H ði; jÞg N p ¼ 1 þfR P XH ði; jÞg N p ¼ 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Architecture of the proposed algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The hallucinated results: (a) input low-resolution 32 Â 24 image; (b) our method; and (c) our method with residue compensation X</figDesc><graphic coords="4,320.07,506.41,234.00,215.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 1 .</head><label>1</label><figDesc>Impact of training set (1) Impact of training set size: Fig. 7 illustrates one example of hallucinating face images using different numbers of training samples. The results are not much different if the pair-wise training number is above 100, suggesting that our hallucination algorithm is capable of achieving satisfactory results based on even a relatively small training set. However, when the training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The hallucinated results: (a) input 32 Â 24 low-resolution image; (b) the step-one hallucinating result based on global image; (c) the final result with position-based residue compensation; and (d) the original high-resolution 128 Â 96 image.</figDesc><graphic coords="5,41.26,382.72,234.00,322.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results from our method using training set of different sizes: (a) 20; (b) 50; (c) 100; (d) 150; (e) 200; and (f) 270.</figDesc><graphic coords="5,310.25,471.85,234.00,250.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. PSNR values of results from our method using training set of different sizes.</figDesc><graphic coords="6,320.07,264.76,234.00,216.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Face hallucination with glasses or expression: (a) input 32 Â 24 lowresolution image; (b) results using our method; and (c) original high-resolution 128 Â 96 images.</figDesc><graphic coords="6,51.08,496.90,234.00,216.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Face hallucination results using different size of position-patch: (a) 2 Â 2; (b) 3 Â 3; (c) 4 Â 4; (d) 5 Â 5; (e) 6 Â 6; and (f) 8 Â 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. PSNR values of the hallucinated results using different size of positionpatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. Face hallucination with pose variations: (a) input 32 Â 24 low-resolution image; (b) our method results using all frontal training images; (c) our method results using training set containing the face images with the same pose as the input; and (d) original high-resolution 128 Â 96 images.</figDesc><graphic coords="7,41.26,58.64,234.00,161.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Fig. 14. Comparative results of Neighbor Embedding based on neighbor-patch and our method based on position-patch: (a) input 29 Â 23 low-resolution face image; (b) K=5; (c) K=150; (d) K= 250; (e) our method; and (f) original 116 Â 92 high-resolution images.</figDesc><graphic coords="8,134.58,58.64,336.24,290.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. The hallucination results. (a) The input 32 Â 24 low-resolution faces. (b) Cubic B-Spline. (c) Neighbor Embedding [12]. (d) Wang's method [9]. (e) Zhuang's method [10]. (f) Our method. (g) The original 128 Â 96 high-resolution faces.</figDesc><graphic coords="9,124.76,58.64,336.24,388.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. PSNR values of the hallucinated results from four different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. It can see that our method has the highest PSNR values compared to the values from other methods for all test faces.With regard to the computational complexity and the execution time of the proposed approach, Zhuang's method<ref type="bibr" target="#b9">[10]</ref> takes more than 10 min using PC with 3.0 G CPU to compute an image, this includes the generation of the residue training set, Neighbor Embedding takes more than 4 min because it requires the search of neighbor patches in image training. In comparison, our method only takes around 1 min. In addition, Wang's method<ref type="bibr" target="#b8">[9]</ref> maintains high computational speed because it performs in the global image way.</figDesc><graphic coords="10,134.60,197.12,336.24,525.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. The hallucination results: (a) input 32 Â 24 low-resolution faces; (b) Wang's method; (c) Neighbor Embedding; (d) Zhuang's method; (e) our method; and (f) original 128 Â 96 high-resolution faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. The hallucination results: (a) a real world picture from a camera; (b) our method; (c) Zhuang's method; (d) Neighbor Embedding; and (e) Wang's method.</figDesc><graphic coords="11,125.34,150.86,334.80,579.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. (a) A real world picture from CMU database; and (b) the hallucinated results of our method.</figDesc><graphic coords="12,134.58,192.60,336.24,537.84" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>X. Ma et al. / Pattern Recognition 43 (2010) 2224-2236</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported in part by the National Natural Science Foundation of China (no. 60972124, no. 60703003), in part by the National High-Tech Research and Development Program (''863'' program) of China (no. 2007AA01Z176) and in part by the National Basic Research Program (''973'' program) of China (no. 2010CB327900).</p><p>The research in this work used the CAS-PEAL-R1 face database <ref type="bibr" target="#b15">[16]</ref> collected under the sponsorship of the Chinese National Hi-Tech Program and ISVISION Tech. Co. Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hallucinating faces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the International Conference on Automatic Face and Gesture Recognition<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Limits on super-resolution and how to break them</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1167" to="1183" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Super-resolution reconstruction of image sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feuer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="817" to="834" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint MAP registration and highresolution image estimation using a sequence of undersampled images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1621" to="1633" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An image super-resolution algorithm for different error levels per frame</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kondi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="592" to="603" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical analysis of the LMS algorithm applied to super-resolution image reconstruction</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C M</forename><surname>Bermudez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2084" to="2095" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A two-step approach to hallucinating faces: global parametric model and local nonparametric model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR&apos;01</title>
		<meeting>the CVPR&apos;01<address><addrLine>Kauai Marriott, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="192" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An improved two-step approach to hallucinating faces</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image and Graphics</title>
		<meeting>the International Conference on Image and Graphics<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="298" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hallucinating face by eigentransformation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">O</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems Man and Cybernetics (Part C)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="434" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hallucinating faces: LPH super-resolution and neighbor reconstruction for residue compensation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3178" to="3194" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neighbor combination and transformation for hallucinating faces</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">O</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo<address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="478" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Super-resolution through Neighbor Embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1275" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust super-resolution of face images by iterative compensating neighborhood relationships</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Biometrics Symposium</title>
		<meeting>the Biometrics Symposium<address><addrLine>United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">4430531</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image hallucination using Neighbor Embedding over visual primitive manifolds</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">4270026</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neighborhood issue in single-frame image super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo<address><addrLine>Netherlands, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1122" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The CAS-PEAL large-scale Chinese face database and baseline evaluations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on System Man, and Cybernetics (Part A</title>
		<imprint>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="149" to="161" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hallucinating faces: TensorPatch super-resolution and coupled residue compensation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">O</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="478" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Breaking the limitation of manifold analysis for superresolution of facial images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="573" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image Hallucination using Neighbor Embedding over visual primitive manifolds</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007-06">2007. June 2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized face super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="873" to="886" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Steerable pyramid-based face hallucination</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="813" to="824" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multilinear image analysis for facial recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A O</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="511" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilinear analysis of image ensembles: TensorFaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A O</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference Computer Vision</title>
		<meeting>the European Conference Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="447" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face hallucination: theory and practice</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The FERET evaluation methodology for face recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Philips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rivzvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="137" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="143" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Fransens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">Parametric Stereo for Multi-Pose Face Recognition and 3D-Face Modeling</title>
		<imprint/>
	</monogr>
	<note>Proceedings of the ICCV 2005</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Analysis and Modeling of Faces and Gestures</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="109" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR&apos;08</title>
		<meeting>the CVPR&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1806" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An example-based face hallucination method for singleframe, low-resolution facial images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="1806" to="1816" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">About the Author-CHUN QI is currently a professor and Ph.D. supervisor at school of Electronics and Information Engineering, Xi&apos;an Jiaotong University. His current research interests mainly include image processing, pattern recognition and signal processing</title>
	</analytic>
	<monogr>
		<title level="m">About the Author-JUNPING ZHANG received the Ph.D. degree from Institute of Automation Chinese Academy of Science</title>
		<meeting><address><addrLine>Xi&apos;an, China; Beijing, China; Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2007. 2003</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science and Engineering, Fudan University</orgName>
		</respStmt>
	</monogr>
	<note>He is currently working toward the Ph.D. degree at the school of Electronics and Information Engineering, Xi&apos;an Jiaotong University</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
