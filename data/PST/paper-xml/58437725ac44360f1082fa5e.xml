<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WAVENET: A GENERATIVE MODEL FOR RAW AUDIO Aäron van den Oord</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-09-12">12 Sep 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
							<email>sedielem@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
							<email>heigazen@google.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Google, London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
							<email>simonyan@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
							<email>nalk@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
							<email>andrewsenior@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
							<email>korayk@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName><roleName>UK</roleName><surname>London</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Oriol Vinyals Alex Graves</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WAVENET: A GENERATIVE MODEL FOR RAW AUDIO Aäron van den Oord</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-09-12">12 Sep 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1609.03499v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.</p><p>• We show that WaveNets can generate raw speech signals with subjective naturalness never before reported in the field of text-to-speech (TTS), as assessed by human raters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>This work explores raw audio generation techniques, inspired by recent advances in neural autoregressive generative models that model complex distributions such as images <ref type="bibr" target="#b49">(van den Oord et al., 2016a;</ref><ref type="bibr" target="#b34">b)</ref> and text <ref type="bibr" target="#b19">(Józefowicz et al., 2016)</ref>. Modeling joint probabilities over pixels or words using neural architectures as products of conditional distributions yields state-of-the-art generation.</p><p>Remarkably, these architectures are able to model distributions over thousands of random variables (e.g. 64×64 pixels as in <ref type="bibr">PixelRNN (van den Oord et al., 2016a)</ref>). The question this paper addresses is whether similar approaches can succeed in generating wideband raw audio waveforms, which are signals with very high temporal resolution, at least 16,000 samples per second (see Fig. <ref type="figure" target="#fig_0">1</ref>). • In order to deal with long-range temporal dependencies needed for raw audio generation, we develop new architectures based on dilated causal convolutions, which exhibit very large receptive fields.</p><p>• We show that when conditioned on a speaker identity, a single model can be used to generate different voices.</p><p>• The same architecture shows strong results when tested on a small speech recognition dataset, and is promising when used to generate other audio modalities such as music.</p><p>We believe that WaveNets provide a generic and flexible framework for tackling many applications that rely on audio generation (e.g. TTS, music, speech enhancement, voice conversion, source separation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">WAVENET</head><p>In this paper we introduce a new generative model operating directly on the raw audio waveform.</p><p>The joint probability of a waveform x = {x 1 , . . . , x T } is factorised as a product of conditional probabilities as follows:</p><formula xml:id="formula_0">p (x) = T t=1 p (x t | x 1 , . . . , x t−1 )<label>(1)</label></formula><p>Each audio sample x t is therefore conditioned on the samples at all previous timesteps.</p><p>Similarly to PixelCNNs (van den <ref type="bibr" target="#b49">Oord et al., 2016a;</ref><ref type="bibr" target="#b34">b)</ref>, the conditional probability distribution is modelled by a stack of convolutional layers. There are no pooling layers in the network, and the output of the model has the same time dimensionality as the input. The model outputs a categorical distribution over the next value x t with a softmax layer and it is optimized to maximize the loglikelihood of the data w.r.t. the parameters. Because log-likelihoods are tractable, we tune hyperparameters on a validation set and can easily measure if the model is overfitting or underfitting. The main ingredient of WaveNet are causal convolutions. By using causal convolutions, we make sure the model cannot violate the ordering in which we model the data: the prediction p (x t+1 | x 1 , ..., x t ) emitted by the model at timestep t cannot depend on any of the future timesteps x t+1 , x t+2 , . . . , x T as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. For images, the equivalent of a causal convolution is a masked convolution <ref type="bibr" target="#b49">(van den Oord et al., 2016a)</ref> which can be implemented by constructing a mask tensor and doing an elementwise multiplication of this mask with the convolution kernel before applying it. For 1-D data such as audio one can more easily implement this by shifting the output of a normal convolution by a few timesteps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DILATED CAUSAL CONVOLUTIONS</head><p>At training time, the conditional predictions for all timesteps can be made in parallel because all timesteps of ground truth x are known. When generating with the model, the predictions are sequential: after each sample is predicted, it is fed back into the network to predict the next sample.</p><p>Because models with causal convolutions do not have recurrent connections, they are typically faster to train than RNNs, especially when applied to very long sequences. One of the problems of causal convolutions is that they require many layers, or large filters to increase the receptive field. For example, in Fig. <ref type="figure" target="#fig_1">2</ref> the receptive field is only 5 (= #layers + filter length -1). In this paper we use dilated convolutions to increase the receptive field by orders of magnitude, without greatly increasing computational cost.</p><p>A dilated convolution (also called à trous, or convolution with holes) is a convolution where the filter is applied over an area larger than its length by skipping input values with a certain step. It is equivalent to a convolution with a larger filter derived from the original filter by dilating it with zeros, but is significantly more efficient. A dilated convolution effectively allows the network to operate on a coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but here the output has the same size as the input. As a special case, dilated convolution with dilation 1 yields the standard convolution. Fig. <ref type="figure" target="#fig_2">3</ref> depicts dilated causal convolutions for dilations 1, 2, 4, and 8. Dilated convolutions have previously been used in various contexts, e.g. signal processing <ref type="bibr" target="#b12">(Holschneider et al., 1989;</ref><ref type="bibr" target="#b5">Dutilleux, 1989)</ref>, and image segmentation <ref type="bibr" target="#b2">(Chen et al., 2015;</ref><ref type="bibr" target="#b54">Yu &amp; Koltun, 2016)</ref>. Stacked dilated convolutions enable networks to have very large receptive fields with just a few layers, while preserving the input resolution throughout the network as well as computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>In this paper, the dilation is doubled for every layer up to a limit and then repeated: e.g. 1, 2, 4, . . . , 512, 1, 2, 4, . . . , 512, 1, 2, 4, . . . , 512.</p><p>The intuition behind this configuration is two-fold. First, exponentially increasing the dilation factor results in exponential receptive field growth with depth <ref type="bibr" target="#b54">(Yu &amp; Koltun, 2016)</ref>. For example each 1, 2, 4, . . . , 512 block has receptive field of size 1024, and can be seen as a more efficient and discriminative (non-linear) counterpart of a 1×1024 convolution. Second, stacking these blocks further increases the model capacity and the receptive field size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SOFTMAX DISTRIBUTIONS</head><p>One approach to modeling the conditional distributions p (x t | x 1 , . . . , x t−1 ) over the individual audio samples would be to use a mixture model such as a mixture density network <ref type="bibr" target="#b1">(Bishop, 1994)</ref> or mixture of conditional Gaussian scale mixtures (MCGSM) <ref type="bibr" target="#b40">(Theis &amp; Bethge, 2015)</ref>. However, <ref type="bibr" target="#b49">van den Oord et al. (2016a)</ref> showed that a softmax distribution tends to work better, even when the data is implicitly continuous (as is the case for image pixel intensities or audio sample values). One of the reasons is that a categorical distribution is more flexible and can more easily model arbitrary distributions because it makes no assumptions about their shape.</p><p>Because raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a softmax layer would need to output 65,536 probabilities per timestep to model all possible values. To make this more tractable, we first apply a µ-law companding transformation <ref type="bibr" target="#b18">(ITU-T, 1988)</ref> to the data, and then quantize it to 256 possible values:</p><formula xml:id="formula_1">f (x t ) = sign(x t ) ln (1 + µ |x t |) ln (1 + µ) ,</formula><p>where −1 &lt; x t &lt; 1 and µ = 255. This non-linear quantization produces a significantly better reconstruction than a simple linear quantization scheme. Especially for speech, we found that the reconstructed signal after quantization sounded very similar to the original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GATED ACTIVATION UNITS</head><p>We use the same gated activation unit as used in the gated PixelCNN <ref type="bibr" target="#b50">(van den Oord et al., 2016b)</ref>:</p><formula xml:id="formula_2">z = tanh (W f,k * x) σ (W g,k * x) ,<label>(2)</label></formula><p>where * denotes a convolution operator, denotes an element-wise multiplication operator, σ(•) is a sigmoid function, k is the layer index, f and g denote filter and gate, respectively, and W is a learnable convolution filter. In our initial experiments, we observed that this non-linearity worked significantly better than the rectified linear activation function <ref type="bibr" target="#b30">(Nair &amp; Hinton, 2010)</ref> for modeling audio signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">RESIDUAL AND SKIP CONNECTIONS</head><formula xml:id="formula_3">1 ⇥ 1 ReLU ReLU 1 ⇥ 1 Dilated Conv tanh ⇥ + 1 ⇥ 1 + Softmax Residual Skip-connections k Layers Output Causal Conv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Figure <ref type="figure">4</ref>: Overview of the residual block and the entire architecture.</p><p>Both residual <ref type="bibr" target="#b10">(He et al., 2015)</ref> and parameterised skip connections are used throughout the network, to speed up convergence and enable training of much deeper models. In Fig. <ref type="figure">4</ref> we show a residual block of our model, which is stacked many times in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">CONDITIONAL WAVENETS</head><p>Given an additional input h, WaveNets can model the conditional distribution p (x | h) of the audio given this input. Eq. ( <ref type="formula" target="#formula_0">1</ref>) now becomes</p><formula xml:id="formula_4">p (x | h) = T t=1 p (x t | x 1 , . . . , x t−1 , h) .<label>(3)</label></formula><p>By conditioning the model on other input variables, we can guide WaveNet's generation to produce audio with the required characteristics. For example, in a multi-speaker setting we can choose the speaker by feeding the speaker identity to the model as an extra input. Similarly, for TTS we need to feed information about the text as an extra input.</p><p>We condition the model on other inputs in two different ways: global conditioning and local conditioning. Global conditioning is characterised by a single latent representation h that influences the output distribution across all timesteps, e.g. a speaker embedding in a TTS model. The activation function from Eq. ( <ref type="formula" target="#formula_2">2</ref>) now becomes:</p><formula xml:id="formula_5">z = tanh W f,k * x + V T f,k h σ W g,k * x + V T g,k h .</formula><p>where V * ,k is a learnable linear projection, and the vector V T * ,k h is broadcast over the time dimension.</p><p>For local conditioning we have a second timeseries h t , possibly with a lower sampling frequency than the audio signal, e.g. linguistic features in a TTS model. We first transform this time series using a transposed convolutional network (learned upsampling) that maps it to a new time series y = f (h) with the same resolution as the audio signal, which is then used in the activation unit as follows:</p><formula xml:id="formula_6">z = tanh (W f,k * x + V f,k * y) σ (W g,k * x + V g,k * y) ,</formula><p>where V f,k * y is now a 1×1 convolution. As an alternative to the transposed convolutional network, it is also possible to use V f,k * h and repeat these values across time. We saw that this worked slightly worse in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">CONTEXT STACKS</head><p>We have already mentioned several different ways to increase the receptive field size of a WaveNet: increasing the number of dilation stages, using more layers, larger filters, greater dilation factors, or a combination thereof. A complementary approach is to use a separate, smaller context stack that processes a long part of the audio signal and locally conditions a larger WaveNet that processes only a smaller part of the audio signal (cropped at the end). One can use multiple context stacks with varying lengths and numbers of hidden units. Stacks with larger receptive fields have fewer units per layer. Context stacks can also have pooling layers to run at a lower frequency. This keeps the computational requirements at a reasonable level and is consistent with the intuition that less capacity is required to model temporal correlations at longer timescales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>To measure WaveNet's audio modelling performance, we evaluate it on three different tasks: multispeaker speech generation (not conditioned on text), TTS, and music audio modelling. We provide samples drawn from WaveNet for these experiments on the accompanying webpage: https://www.deepmind.com/blog/wavenet-generative-model-raw-audio/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MULTI-SPEAKER SPEECH GENERATION</head><p>For the first experiment we looked at free-form speech generation (not conditioned on text). We used the English multi-speaker corpus from CSTR voice cloning toolkit (VCTK) <ref type="bibr" target="#b52">(Yamagishi, 2012)</ref> and conditioned WaveNet only on the speaker. The conditioning was applied by feeding the speaker ID to the model in the form of a one-hot vector. The dataset consisted of 44 hours of data from 109 different speakers.</p><p>Because the model is not conditioned on text, it generates non-existent but human language-like words in a smooth way with realistic sounding intonations. This is similar to generative models of language or images, where samples look realistic at first glance, but are clearly unnatural upon closer inspection. The lack of long range coherence is partly due to the limited size of the model's receptive field (about 300 milliseconds), which means it can only remember the last 2-3 phonemes it produced.</p><p>A single WaveNet was able to model speech from any of the speakers by conditioning it on a onehot encoding of a speaker. This confirms that it is powerful enough to capture the characteristics of all 109 speakers from the dataset in a single model. We observed that adding speakers resulted in better validation set performance compared to training solely on a single speaker. This suggests that WaveNet's internal representation was shared among multiple speakers.</p><p>Finally, we observed that the model also picked up on other characteristics in the audio apart from the voice itself. For instance, it also mimicked the acoustics and recording quality, as well as the breathing and mouth movements of the speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TEXT-TO-SPEECH</head><p>For the second experiment we looked at TTS. We used the same single-speaker speech databases from which Google's North American English and Mandarin Chinese TTS systems are built. The North American English dataset contains 24.6 hours of speech data, and the Mandarin Chinese dataset contains 34.8 hours; both were spoken by professional female speakers.</p><p>WaveNets for the TTS task were locally conditioned on linguistic features which were derived from input texts. We also trained WaveNets conditioned on the logarithmic fundamental frequency (log F 0 ) values in addition to the linguistic features. External models predicting log F 0 values and phone durations from linguistic features were also trained for each language. The receptive field size of the WaveNets was 240 milliseconds. As example-based and model-based speech synthesis baselines, hidden Markov model (HMM)-driven unit selection concatenative <ref type="bibr" target="#b9">(Gonzalvo et al., 2016)</ref> and long short-term memory recurrent neural network (LSTM-RNN)-based statistical parametric <ref type="bibr" target="#b45">(Zen et al., 2016)</ref> speech synthesizers were built. Since the same datasets and linguistic features were used to train both the baselines and WaveNets, these speech synthesizers could be fairly compared.</p><p>To evaluate the performance of WaveNets for the TTS task, subjective paired comparison tests and mean opinion score (MOS) tests were conducted. In the paired comparison tests, after listening to each pair of samples, the subjects were asked to choose which they preferred, though they could choose "neutral" if they did not have any preference. In the MOS tests, after listening to each stimulus, the subjects were asked to rate the naturalness of the stimulus in a five-point Likert scale score (1: Bad, 2: Poor, 3: Fair, 4: Good, 5: Excellent). Please refer to Appendix B for details. Fig. <ref type="figure" target="#fig_3">5</ref> shows a selection of the subjective paired comparison test results (see Appendix B for the complete table). It can be seen from the results that WaveNet outperformed the baseline statistical parametric and concatenative speech synthesizers in both languages. We found that WaveNet conditioned on linguistic features could synthesize speech samples with natural segmental quality but sometimes it had unnatural prosody by stressing wrong words in a sentence. This could be due to the long-term dependency of F 0 contours: the size of the receptive field of the WaveNet, 240 milliseconds, was not long enough to capture such long-term dependency. WaveNet conditioned on both linguistic features and F 0 values did not have this problem: the external F 0 prediction model runs at a lower frequency (200 Hz) so it can learn long-range dependencies that exist in F 0 contours.</p><p>Table <ref type="table" target="#tab_0">1</ref> show the MOS test results. It can be seen from the table that WaveNets achieved 5-scale MOSs in naturalness above 4.0, which were significantly better than those from the baseline systems.</p><p>They were the highest ever reported MOS values with these training datasets and test sentences. The gap in the MOSs from the best synthetic speech to the natural ones decreased from 0.69 to 0.34 (51%) in US English and 0.42 to 0.13 (69%) in Mandarin Chinese.  • the MagnaTagATune dataset <ref type="bibr" target="#b25">(Law &amp; Von Ahn, 2009)</ref>, which consists of about 200 hours of music audio. Each 29-second clip is annotated with tags from a set of 188, which describe the genre, instrumentation, tempo, volume and mood of the music. • the YouTube piano dataset, which consists of about 60 hours of solo piano music obtained from YouTube videos. Because it is constrained to a single instrument, it is considerably easier to model.</p><p>Although it is difficult to quantitatively evaluate these models, a subjective evaluation is possible by listening to the samples they produce. We found that enlarging the receptive field was crucial to obtain samples that sounded musical. Even with a receptive field of several seconds, the models did not enforce long-range consistency which resulted in second-to-second variations in genre, instrumentation, volume and sound quality. Nevertheless, the samples were often harmonic and aesthetically pleasing, even when produced by unconditional models.</p><p>Of particular interest are conditional music models, which can generate music given a set of tags specifying e.g. genre or instruments. Similarly to conditional speech models, we insert biases that depend on a binary vector representation of the tags associated with each training clip. This makes it possible to control various aspects of the output of the model when sampling, by feeding in a binary vector that encodes the desired properties of the samples. We have trained such models on the MagnaTagATune dataset; although the tag data bundled with the dataset was relatively noisy and had many omissions, after cleaning it up by merging similar tags and removing those with too few associated clips, we found this approach to work reasonably well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SPEECH RECOGNITION</head><p>Although WaveNet was designed as a generative model, it can straightforwardly be adapted to discriminative audio tasks such as speech recognition.</p><p>Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently <ref type="bibr" target="#b32">(Palaz et al., 2013;</ref><ref type="bibr" target="#b47">Tüske et al., 2014;</ref><ref type="bibr" target="#b13">Hoshen et al., 2015;</ref><ref type="bibr" target="#b37">Sainath et al., 2015)</ref>. Recurrent neural networks such as LSTM-RNNs <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997)</ref> have been a key component in these new speech classification pipelines, because they allow for building models with long range contexts. With WaveNets we have shown that layers of dilated convolutions allow the receptive field to grow longer in a much cheaper way than using LSTM units.</p><p>As a last experiment we looked at speech recognition with WaveNets on the TIMIT <ref type="bibr" target="#b8">(Garofolo et al., 1993)</ref> dataset. For this task we added a mean-pooling layer after the dilated convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160× downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>This paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals. We have shown how WaveNets can be conditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition.</p><p>Gaffney and Steve Crossan for helping to manage the project, Faith Mackinder for help with preparing the blogpost, James Besley for legal support and Demis Hassabis for managing the project and his inputs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TEXT-TO-SPEECH BACKGROUND</head><p>The goal of TTS synthesis is to render naturally sounding speech signals given a text to be synthesized. Human speech production process first translates a text (or concept) into movements of muscles associated with articulators and speech production-related organs. Then using air-flow from lung, vocal source excitation signals, which contain both periodic (by vocal cord vibration) and aperiodic (by turbulent noise) components, are generated. By filtering the vocal source excitation signals by time-varying vocal tract transfer functions controlled by the articulators, their frequency characteristics are modulated. Finally, the generated speech signals are emitted. The aim of TTS is to mimic this process by computers in some way.</p><p>TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform. This part typically includes prosody prediction and speech waveform generation.</p><p>There are two main approaches to realize the speech synthesis part; non-parametric, example-based approach known as concatenative speech synthesis <ref type="bibr" target="#b28">(Moulines &amp; Charpentier, 1990;</ref><ref type="bibr" target="#b36">Sagisaka et al., 1992;</ref><ref type="bibr" target="#b14">Hunt &amp; Black, 1996)</ref>, and parametric, model-based approach known as statistical parametric speech synthesis <ref type="bibr" target="#b53">(Yoshimura, 2002;</ref><ref type="bibr">Zen et al., 2009)</ref>. The concatenative approach builds up the utterance from units of recorded speech, whereas the statistical parametric approach uses a generative model to synthesize the speech. The statistical parametric approach first extracts a sequence of vocoder parameters <ref type="bibr" target="#b4">(Dudley, 1939)</ref> </p><formula xml:id="formula_7">o = {o 1 , . . . , o N } from speech signals x = {x 1 , . . . , x T }</formula><p>and linguistic features l from the text W , where N and T correspond to the numbers of vocoder parameter vectors and speech signals. Typically a vocoder parameter vector o n is extracted at every 5 milliseconds. It often includes cepstra <ref type="bibr" target="#b15">(Imai &amp; Furuichi, 1988)</ref> or line spectral pairs <ref type="bibr" target="#b16">(Itakura, 1975)</ref>, which represent vocal tract transfer function, and fundamental frequency (F 0 ) and aperiodicity <ref type="bibr" target="#b24">(Kawahara et al., 2001)</ref>, which represent characteristics of vocal source excitation signals. Then a set of generative models, such as hidden Markov models (HMMs) <ref type="bibr" target="#b53">(Yoshimura, 2002)</ref>, feed-forward neural networks <ref type="bibr">(Zen et al., 2013)</ref>, and recurrent neural networks <ref type="bibr" target="#b46">(Tuerk &amp; Robinson, 1993;</ref><ref type="bibr" target="#b22">Karaali et al., 1997;</ref><ref type="bibr" target="#b6">Fan et al., 2014)</ref>, is trained from the extracted vocoder parameters and linguistic features as</p><formula xml:id="formula_8">Λ = arg max Λ p (o | l, Λ) ,<label>(4)</label></formula><p>where Λ denotes the set of parameters of the generative model. At the synthesis stage, the most probable vocoder parameters are generated given linguistic features extracted from a text to be synthesized as ô = arg max o p(o | l, Λ).</p><p>(5)</p><p>Then a speech waveform is reconstructed from ô using a vocoder. The statistical parametric approach offers various advantages over the concatenative one such as small footprint and flexibility to change its voice characteristics. However, its subjective naturalness is often significantly worse than that of the concatenative approach; synthesized speech often sounds muffled and has artifacts. <ref type="bibr">Zen et al. (2009)</ref> reported three major factors that can degrade the subjective naturalness; quality of vocoders, accuracy of generative models, and effect of oversmoothing. The first factor causes the artifacts and the second and third factors lead to the muffleness in the synthesized speech. There have been a number of attempts to address these issues individually, such as developing high-quality vocoders <ref type="bibr" target="#b23">(Kawahara et al., 1999;</ref><ref type="bibr" target="#b0">Agiomyrgiannakis, 2015;</ref><ref type="bibr" target="#b27">Morise et al., 2016)</ref>, improving the accuracy of generative models <ref type="bibr">(Zen et al., 2007;</ref><ref type="bibr" target="#b32">2013;</ref><ref type="bibr" target="#b6">Fan et al., 2014;</ref><ref type="bibr" target="#b48">Uria et al., 2015)</ref>, and compensating the oversmoothing effect <ref type="bibr" target="#b41">(Toda &amp; Tokuda, 2007;</ref><ref type="bibr" target="#b39">Takamichi et al., 2016)</ref>. <ref type="bibr" target="#b45">Zen et al. (2016)</ref> showed that state-of-the-art statistical parametric speech syntheziers matched state-of-the-art concatenative ones in some languages. However, its vocoded sound quality is still a major issue.</p><p>Extracting vocoder parameters can be viewed as estimation of a generative model parameters given speech signals <ref type="bibr" target="#b17">(Itakura &amp; Saito, 1970;</ref><ref type="bibr" target="#b15">Imai &amp; Furuichi, 1988)</ref>. For example, linear predictive analysis <ref type="bibr" target="#b17">(Itakura &amp; Saito, 1970)</ref>, which has been used in speech coding, assumes that the generative model of speech signals is a linear auto-regressive (AR) zero-mean Gaussian process;</p><formula xml:id="formula_9">x t = P p=1 a p x t−p + t (6) t ∼ N (0, G 2 ) (7)</formula><p>where a p is a p-th order linear predictive coefficient (LPC) and G 2 is a variance of modeling error. These parameters are estimated based on the maximum likelihood (ML) criterion. In this sense, the training part of the statistical parametric approach can be viewed as a two-step optimization and sub-optimal: extract vocoder parameters by fitting a generative model of speech signals then model trajectories of the extracted vocoder parameters by a separate generative model for time series <ref type="bibr" target="#b43">(Tokuda, 2011)</ref>. There have been attempts to integrate these two steps into a single one <ref type="bibr" target="#b42">(Toda &amp; Tokuda, 2008;</ref><ref type="bibr" target="#b51">Wu &amp; Tokuda, 2008;</ref><ref type="bibr" target="#b26">Maia et al., 2010;</ref><ref type="bibr" target="#b31">Nakamura et al., 2014;</ref><ref type="bibr" target="#b29">Muthukumar &amp; Black, 2014;</ref><ref type="bibr" target="#b44">Tokuda &amp; Zen, 2015;</ref><ref type="bibr">2016;</ref><ref type="bibr" target="#b38">Takaki &amp; Yamagishi, 2016)</ref>. For example, <ref type="bibr" target="#b45">Tokuda &amp; Zen (2016)</ref>  The conventional generative models of raw audio signals have a number of assumptions which are inspired from the speech production, such as</p><p>• Use of fixed-length analysis window; They are typically based on a stationary stochastic process <ref type="bibr" target="#b17">(Itakura &amp; Saito, 1970;</ref><ref type="bibr" target="#b15">Imai &amp; Furuichi, 1988;</ref><ref type="bibr" target="#b34">Poritz, 1982;</ref><ref type="bibr" target="#b20">Juang &amp; Rabiner, 1985;</ref><ref type="bibr" target="#b21">Kameoka et al., 2010)</ref>. To model time-varying speech signals by a stationary stochastic process, parameters of these generative models are estimated within a fixed-length, overlapping and shifting analysis window (typically its length is 20 to 30 milliseconds, and shift is 5 to 10 milliseconds). However, some phones such as stops are time-limited by less than 20 milliseconds <ref type="bibr" target="#b35">(Rabiner &amp; Juang, 1993)</ref>. Therefore, using such fixed-size analysis window has limitations. • Linear filter; These generative models are typically realized as a linear time-invariant filter <ref type="bibr" target="#b17">(Itakura &amp; Saito, 1970;</ref><ref type="bibr" target="#b15">Imai &amp; Furuichi, 1988;</ref><ref type="bibr" target="#b34">Poritz, 1982;</ref><ref type="bibr" target="#b20">Juang &amp; Rabiner, 1985;</ref><ref type="bibr" target="#b21">Kameoka et al., 2010)</ref> within a windowed frame. However, the relationship between successive audio samples can be highly non-linear.</p><p>• Gaussian process assumption; The conventional generative models are based on Gaussian process <ref type="bibr" target="#b17">(Itakura &amp; Saito, 1970;</ref><ref type="bibr" target="#b15">Imai &amp; Furuichi, 1988;</ref><ref type="bibr" target="#b34">Poritz, 1982;</ref><ref type="bibr" target="#b20">Juang &amp; Rabiner, 1985;</ref><ref type="bibr" target="#b21">Kameoka et al., 2010;</ref><ref type="bibr" target="#b44">Tokuda &amp; Zen, 2015;</ref><ref type="bibr">2016)</ref>. From the source-filter model of speech production <ref type="bibr" target="#b3">(Chiba &amp; Kajiyama, 1942;</ref><ref type="bibr" target="#b7">Fant, 1970)</ref> point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution <ref type="bibr" target="#b17">(Itakura &amp; Saito, 1970;</ref><ref type="bibr" target="#b15">Imai &amp; Furuichi, 1988;</ref><ref type="bibr" target="#b34">Poritz, 1982;</ref><ref type="bibr" target="#b20">Juang &amp; Rabiner, 1985;</ref><ref type="bibr" target="#b44">Tokuda &amp; Zen, 2015;</ref><ref type="bibr" target="#b21">Kameoka et al., 2010;</ref><ref type="bibr" target="#b45">Tokuda &amp; Zen, 2016)</ref>. Together with the linear assumption above, it results in assuming that speech signals are normally distributed. However, distributions of real speech signals can be significantly different from Gaussian.</p><p>Although these assumptions are convenient, samples from these generative models tend to be noisy and lose important details to make these audio signals sounding natural.</p><p>WaveNet, which was described in Section 2, has none the above-mentioned assumptions. It incorporates almost no prior knowledge about audio signals, except the choice of the receptive field and µ-law encoding of the signal. It can also be viewed as a non-linear causal filter for quantized signals. Although such non-linear filter can represent complicated signals while preserving the details, designing such filters is usually difficult <ref type="bibr" target="#b33">(Peltonen et al., 2001)</ref>. WaveNets give a way to train them from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DETAILS OF TTS EXPERIMENT</head><p>The HMM-driven unit selection and WaveNet TTS systems were built from speech at 16 kHz sampling. Although LSTM-RNNs were trained from speech at 22.05 kHz sampling, speech at 16 kHz sampling was synthesized at runtime using a resampling functionality in the Vocaine vocoder <ref type="bibr" target="#b0">(Agiomyrgiannakis, 2015)</ref>. Both the LSTM-RNN-based statistical parametric and HMM-driven unit selection speech synthesizers were built from the speech datasets in the 16-bit linear PCM, whereas the WaveNet-based ones were trained from the same speech datasets in the 8-bit µ-law encoding.</p><p>The linguistic features include phone, syllable, word, phrase, and utterance-level features <ref type="bibr" target="#b55">(Zen, 2006</ref>) (e.g. phone identities, syllable stress, the number of syllables in a word, and position of the current syllable in a phrase) with additional frame position and phone duration features <ref type="bibr">(Zen et al., 2013)</ref>. These features were derived and associated with speech every 5 milliseconds by phone-level forced alignment at the training stage. We used LSTM-RNN-based phone duration and autoregressive CNN-based log F 0 prediction models. They were trained so as to minimize the mean squared errors (MSE). It is important to note that no post-processing was applied to the audio signals generated from the WaveNets.</p><p>The subjective listening tests were blind and crowdsourced. 100 sentences not included in the training data were used for evaluation. Each subject could evaluate up to 8 and 63 stimuli for North American English and Mandarin Chinese, respectively. Test stimuli were randomly chosen and presented for each subject. In the paired comparison test, each pair of speech samples was the same text synthesized by the different models. In the MOS test, each stimulus was presented to subjects in isolation. Each pair was evaluated by eight subjects in the paired comparison test, and each stimulus was evaluated by eight subjects in the MOS test. The subjects were paid and native speakers performing the task. Those ratings (about 40%) where headphones were not used were excluded when computing the preference and mean opinion scores. Table <ref type="table">2</ref> shows the full details of the paired comparison test shown in Fig. <ref type="figure" target="#fig_3">5</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A second of generated speech. This paper introduces WaveNet, an audio generative model based on the PixelCNN (van den Oord et al., 2016a;b) architecture. The main contributions of this work are as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of a stack of causal convolutional layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of a stack of dilated causal convolutional layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Subjective preference scores (%) of speech samples between (top) two baselines, (middle) two WaveNets, and (bottom) the best baseline and WaveNet. Note that LSTM and Concat correspond to LSTM-RNN-based statistical parametric and HMM-driven unit selection concatenative baseline synthesizers, and WaveNet (L) and WaveNet (L+F) correspond to the WaveNet conditioned on linguistic features only and that conditioned on both linguistic features and log F 0 values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>Figure Outline of statistical parametric speech synthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Subjective 5-scale mean opinion scores of speech samples from LSTM-RNN-based statistical parametric, HMM-driven unit selection concatenative, and proposed WaveNet-based speech synthesizers, 8-bit µ-law encoded natural speech, and 16-bit linear pulse-code modulation (PCM) natural speech. WaveNet improved the previous state of the art significantly, reducing the gap between natural speech and best previous model by more than 50%.</figDesc><table><row><cell></cell><cell cols="2">Subjective 5-scale MOS in naturalness</cell></row><row><cell>Speech samples</cell><cell>North American English</cell><cell>Mandarin Chinese</cell></row><row><cell>LSTM-RNN parametric</cell><cell>3.67 ± 0.098</cell><cell>3.79 ± 0.084</cell></row><row><cell>HMM-driven concatenative</cell><cell>3.86 ± 0.137</cell><cell>3.47 ± 0.108</cell></row><row><cell>WaveNet (L+F)</cell><cell>4.21 ± 0.081</cell><cell>4.08 ± 0.085</cell></row><row><cell>Natural (8-bit µ-law)</cell><cell>4.46 ± 0.067</cell><cell>4.25 ± 0.082</cell></row><row><cell>Natural (16-bit linear PCM)</cell><cell>4.55 ± 0.075</cell><cell>4.21 ± 0.071</cell></row><row><cell>3.3 MUSIC</cell><cell></cell><cell></cell></row><row><cell cols="3">For out third set of experiments we trained WaveNets to model two music datasets:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Senior, Andrew, and Schuster, Mike. Statistical parametric speech synthesis using deep neural networks. In Proc. ICASSP, pp. 7962-7966, 2013. Zen, Heiga, Agiomyrgiannakis, Yannis, Egberts, Niels, Henderson, Fergus, and Szczepaniak, Przemysław. Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices. In Interspeech, 2016. URL https://arxiv.org/abs/1606. 06061.</figDesc><table><row><cell>Zen, Heiga, Tokuda, Keiichi, and Kitamura, Tadashi. Reformulating the HMM as a trajectory model</cell></row><row><cell>by imposing explicit relationships between static and dynamic features. Comput. Speech Lang.,</cell></row><row><cell>21(1):153-173, 2007.</cell></row><row><cell>Zen, Heiga, Tokuda, Keiichi, and Black, Alan W. Statistical parametric speech synthesis. Speech</cell></row><row><cell>Commn., 51(11):1039-1064, 2009.</cell></row><row><cell>Zen, Heiga,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>integrated non-stationary, nonzero-mean Gaussian process generative model of speech signals and LSTM-RNN-based sequence generative model to a single one and jointly optimized them by back-propagation. Although they showed that this model could approximate natural speech signals, its segmental naturalness was significantly worse than the non-integrated model due to overgeneralization and over-estimation of noise components in speech signals.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank Lasse Espeholt, Jeffrey De Fauw and Grzegorz Swirszcz for their inputs, Adam Cain, Max Cant and Adrian Bolton for their help with artwork, Helen King, Steven</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vocaine the vocoder and applications is speech synthesis</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4230" to="4234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<idno>NCRG/94/004</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Neural Computing Research Group, Aston University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Liang-Chieh</surname></persName>
		</author>
		<author>
			<persName><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><surname>George</surname></persName>
		</author>
		<author>
			<persName><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><surname>Iasonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.7062" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Vowel: Its Nature and Structure</title>
		<author>
			<persName><forename type="first">Tsutomu</forename><surname>Chiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masato</forename><surname>Kajiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1942">1942</date>
			<pubPlace>Tokyo-Kaiseikan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Remaking speech</title>
		<author>
			<persName><forename type="first">Homer</forename><surname>Dudley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="177" />
			<date type="published" when="1939">1939</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An implementation of the &quot;algorithme à trous&quot; to compute the wavelet transform</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Dutilleux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets: Time-Frequency Methods and Phase Space</title>
				<editor>
			<persName><forename type="first">Jean</forename><forename type="middle">-</forename><surname>Combes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Grossmann</forename><surname>Michel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Tchamitchian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philippe</forename></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="298" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">TTS synthesis with bidirectional LSTM based recurrent neural networks</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng-</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1964" to="1968" />
		</imprint>
	</monogr>
	<note type="report_type">In Interspeech</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Acoustic Theory of Speech Production</title>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Fant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
			<publisher>Mouton De Gruyter</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIST speech disc 1-1.1. NASA STI/Recon technical report</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recent advances in Google real-time HMM-driven unit selection synthesizer</title>
		<author>
			<persName><forename type="first">Xavi</forename><surname>Gonzalvo</surname></persName>
		</author>
		<author>
			<persName><surname>Tazari</surname></persName>
		</author>
		<author>
			<persName><surname>Siamak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-An</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><surname>Markus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gutkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Silen</surname></persName>
		</author>
		<ptr target="http://research.google.com/pubs/pub45564.html" />
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tchamitchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets: Time-Frequency Methods and Phase Space</title>
				<editor>
			<persName><forename type="first">Jean</forename><forename type="middle">-</forename><surname>Combes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Grossmann</forename><surname>Michel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Tchamitchian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philippe</forename></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech acoustic modeling from raw multichannel waveforms</title>
		<author>
			<persName><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4624" to="4628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unit selection in a concatenative speech synthesis system using a large speech database</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="373" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unbiased estimation of log spectrum</title>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieko</forename><surname>Furuichi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EURASIP</title>
				<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="203" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Line spectrum representation of linear predictor coefficients of speech signals</title>
		<author>
			<persName><forename type="first">Fumitada</forename><surname>Itakura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoust. Society of America</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">S1</biblScope>
			<biblScope unit="page" from="S35" to="S35" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A statistical method for estimation of speech spectral density and formant frequencies</title>
		<author>
			<persName><forename type="first">Fumitada</forename><surname>Itakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzo</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. IEICE</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="35" to="42" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pulse Code Modulation (PCM) of voice frequencies</title>
		<author>
			<persName><surname>Itu-T</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">711</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><surname>Mike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>CoRR, abs/1602.02410</idno>
		<ptr target="http://arxiv.org/abs/1602.02410" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mixture autoregressive hidden Markov models for speech signals</title>
		<author>
			<persName><forename type="first">Biing-Hwang And</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="1404" to="1413" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speech analysis with multi-kernel linear prediction</title>
		<author>
			<persName><surname>Kameoka</surname></persName>
		</author>
		<author>
			<persName><surname>Hirokazu</surname></persName>
		</author>
		<author>
			<persName><surname>Ohishi</surname></persName>
		</author>
		<author>
			<persName><surname>Yasunori</surname></persName>
		</author>
		<author>
			<persName><surname>Mochihashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Daichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spring Conference of ASJ</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="499" to="502" />
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text-to-speech conversion with neural networks: A recurrent TDNN approach</title>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Karaali</surname></persName>
		</author>
		<author>
			<persName><surname>Corrigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerson</forename><surname>Gerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Massey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurospeech</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="561" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequencybased f 0 extraction: possible role of a repetitive structure in sounds</title>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ikuyo</forename><surname>Masuda-Katsuse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>De Cheveigné</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="187" to="207" />
		</imprint>
	</monogr>
	<note>Speech Commn.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modification and synthesis system STRAIGHT</title>
		<author>
			<persName><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><surname>Hideki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo</forename><surname>Estill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osamu</forename><surname>Fujimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MAVEBA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Input-agreement: a new mechanism for collecting data using human computation games</title>
		<author>
			<persName><forename type="first">Edith</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Statistical parametric speech synthesis with joint estimation of acoustic and excitation model parameters</title>
		<author>
			<persName><forename type="first">Ranniery</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA SSW7</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">WORLD: A vocoder-based high-quality speech synthesis system for real-time applications</title>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Morise</surname></persName>
		</author>
		<author>
			<persName><surname>Yokomori</surname></persName>
		</author>
		<author>
			<persName><surname>Fumiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Ozawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1877" to="1884" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pitch synchronous waveform processing techniques for text-to-speech synthesis using diphones</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Moulines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Charpentier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commn</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="453" to="467" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A deep learning approach to data-driven parameterizations for statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Muthukumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.8558</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Integration of spectral feature extraction and modeling for HMM-based speech synthesis</title>
		<author>
			<persName><forename type="first">Kazuhiro</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><surname>Kei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshihiko</forename><surname>Nankaku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1438" to="1448" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Dimitri</forename><surname>Palaz</surname></persName>
		</author>
		<author>
			<persName><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Magimai-Doss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1766" to="1770" />
		</imprint>
	</monogr>
	<note type="report_type">In Interspeech</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Nonlinear filter design: methodologies and challenges</title>
		<author>
			<persName><forename type="first">Sari</forename><surname>Peltonen</surname></persName>
		</author>
		<author>
			<persName><surname>Gabbouj</surname></persName>
		</author>
		<author>
			<persName><surname>Moncef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Astola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ISPA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="102" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linear predictive hidden Markov models and the speech signal</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">B</forename><surname>Poritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="1291" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biing-</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><surname>Hwang</surname></persName>
		</author>
		<title level="m">Fundamentals of Speech Recognition</title>
				<imprint>
			<publisher>PrenticeHall</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ATR ν-talk speech synthesis system</title>
		<author>
			<persName><forename type="first">Yoshinori</forename><surname>Sagisaka</surname></persName>
		</author>
		<author>
			<persName><surname>Kaiki</surname></persName>
		</author>
		<author>
			<persName><surname>Nobuyoshi</surname></persName>
		</author>
		<author>
			<persName><surname>Iwahashi</surname></persName>
		</author>
		<author>
			<persName><surname>Naoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuhiko</forename><surname>Mimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSLP</title>
				<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="483" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning the speech front-end with raw waveform CLDNNs</title>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5535" to="5539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Postfilters to modify the modulation spectrum for statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">Shinnosuke</forename><surname>Takamichi</surname></persName>
		</author>
		<author>
			<persName><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><surname>Tomoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><surname>Sakriani</surname></persName>
		</author>
		<author>
			<persName><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="755" to="767" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generative image modeling using spatial LSTMs</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1927" to="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A speech parameter generation algorithm considering global variance for HMM-based speech synthesis</title>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="816" to="824" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Statistical approach to vocal tract transfer function estimation based on factor analyzed trajectory hmm</title>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3925" to="3928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Speech synthesis as a statistical machine learning problem</title>
		<author>
			<persName><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
		<ptr target="http://www.sp.nitech.ac.jp/˜tokuda/tokuda_asru2011_for_pdf.pdf" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Invited talk given at ASRU</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Directly modeling speech waveforms by neural networks for statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4215" to="4219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Directly modeling voiced and unvoiced components in speech waveforms by neural networks</title>
		<author>
			<persName><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5640" to="5644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Speech synthesis using artificial neural networks trained on cepstral coefficients</title>
		<author>
			<persName><forename type="first">Christine</forename><surname>Tuerk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
				<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1713" to="1716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Acoustic modeling with deep neural networks using raw time signal for LVCSR</title>
		<author>
			<persName><forename type="first">Zoltán</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName><surname>Golik</surname></persName>
		</author>
		<author>
			<persName><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="890" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Modelling acoustic feature dependencies with artificial neural networks: Trajectory-RNADE</title>
		<author>
			<persName><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><surname>Benigno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassia</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bridle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4465" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Aäron</surname></persName>
		</author>
		<author>
			<persName><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Aäron</surname></persName>
		</author>
		<author>
			<persName><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><surname>Nal</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><surname>Lasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>CoRR, abs/1606.05328</idno>
		<ptr target="http://arxiv.org/abs/1606.05328" />
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Minimum generation error training with direct log spectral distortion on LSPs for HMM-based speech synthesis</title>
		<author>
			<persName><forename type="first">Yi-Jian And</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="577" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">English multi-speaker corpus for CSTR voice cloning toolkit</title>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<ptr target="http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Simultaneous modeling of phonetic and prosodic parameters, and characteristic conversion for HMM-based text-to-speech systems</title>
		<author>
			<persName><forename type="first">Takayoshi</forename><surname>Yoshimura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Nagoya Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.07122" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">An example of context-dependent label format for HMM-based speech synthesis in English</title>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<ptr target="http://hts.sp.nitech.ac.jp/?Download" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
