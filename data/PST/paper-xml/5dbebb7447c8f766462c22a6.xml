<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Convolutional Networks with Motif-based Attention</title>
				<funder ref="#_CaPbF4V">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">Boaz</forename><surname>Lee</surname></persName>
							<email>jtlee@wpi.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
							<email>rrossi@adobe.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
							<email>xkong@wpi.edu</email>
						</author>
						<author>
							<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
							<email>sukim@adobe.com</email>
						</author>
						<author>
							<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
							<email>anuprao@adobe.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Convolutional Networks with Motif-based Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357384.3357880</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph attention</term>
					<term>motifs</term>
					<term>graph convolution</term>
					<term>higher-order proximity</term>
					<term>structural role</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of deep convolutional neural networks in the domains of computer vision and speech recognition has led researchers to investigate generalizations of the said architecture to graphstructured data. A recently-proposed method called Graph Convolutional Networks has been able to achieve state-of-the-art results in the task of node classification. However, since the proposed method relies on localized first-order approximations of spectral graph convolutions, it is unable to capture higher-order interactions between nodes in the graph. In this work, we propose a motif-based graph attention model, called Motif Convolutional Networks, which generalizes past approaches by using weighted multi-hop motif adjacency matrices to capture higher-order neighborhoods. A novel attention mechanism is used to allow each individual node to select the most relevant neighborhood to apply its filter. We evaluate our approach on graphs from different domains (social networks and bioinformatics) with results showing that it is able to outperform a set of competitive baselines on the semi-supervised node classification task. Additional results demonstrate the usefulness of attention, showing that different higher-order neighborhoods are prioritized by different kinds of nodes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HONE: Higher-Order Network Embeddings</head><p>? Weighted Motif Graph: Given a network G and a network motif H t 2 H , form the weighted motif adjacency matrix W t whose entries (i, j) are the co-occurrence counts of nodes i and j in the motif H t : (W t ) i j = number of instances of H t that contain nodes i and j. In the case of using HONE directly with a weighted motif adjacency matrix W, then</p><formula xml:id="formula_0">: W ! IW<label>(3)</label></formula><p>The number of paths weighted by motif counts from node i to node j in k-steps is given by</p><formula xml:id="formula_1">(W k ) i j = W ? ? ? W | {z } k i j<label>(4)</label></formula><p>? Motif Transition Matrix: The random walk on a graph W weighted by motif counts has transition probabilities</p><formula xml:id="formula_2">P i j = W i j w i<label>(5)</label></formula><p>where w i = ? j W i j is the motif degree of node i. The random walk motif transition matrix P for an arbitrary weighted motif graph W is dened as:</p><formula xml:id="formula_3">P = D 1 W<label>(6)</label></formula><p>where D = diag(We) = diag(w 1 , w 2 , . . . ,w N ) is a N ? N diagonal matrix with the motif degree w i = ? j W i j of each node on the diagonal called the diagonal motif degree matrix and ? ? T  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, deep learning has made a significant impact on the field of computer vision. Various deep learning models have achieved state-of-the-art results on a number of vision-related benchmarks. In most cases, the preferred architecture is a Convolutional Neural Network (CNN). CNN-based models have been applied successfully to the tasks of image classification <ref type="bibr" target="#b20">[21]</ref>, image super-resolution <ref type="bibr" target="#b18">[19]</ref>, and video action recognition <ref type="bibr" target="#b9">[10]</ref>, among many others. CNNs, however, are designed to work for data that can be represented as grids (e.g., videos, images, or audio clips) and do not generalize well to graphs -which have more irregular structure. Due to this limitation, it cannot be applied directly to many realworld problems whose data come in the form of graphs -social networks <ref type="bibr" target="#b30">[31]</ref> or collaboration/citation networks <ref type="bibr" target="#b23">[24]</ref> in social network analysis, for instance.</p><p>A recent deep learning architecture, called Graph Convolutional Networks (GCN) <ref type="bibr" target="#b19">[20]</ref> approximates the spectral convolution operation on graphs by defining a layer-wise propagation that is based on the one-hop neighborhood of nodes. The first-order filters used by GCNs were found to be useful and have allowed the model to beat many established baselines in the semi-supervised node classification task <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>However, in many cases, it has been shown that it may be beneficial to consider the higher-order structure in graphs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref>. In this work, we introduce a general class of graph convolution networks which utilize weighted multi-hop motif adjacency matrices <ref type="bibr" target="#b32">[33]</ref> to capture higher-order neighborhoods in graphs. The weighted adjacency matrices are computed using various network motifs <ref type="bibr" target="#b32">[33]</ref>. Fig. <ref type="figure" target="#fig_3">1</ref> shows an example of the node neighborhoods that are induced when we consider two different kinds of motifs, showing that the choice of motif can significantly alter the neighborhood structure of nodes.</p><p>Our proposed method, which we call Motif Convolutional Networks (MCN), uses a novel attention mechanism to allow each node to select the most relevant motif-induced neighborhood to integrate information from. Intuitively, this allows a node to select its one-hop neighborhood (as in classical GCN) when its immediate neighborhood contains enough information for the model to classify the node correctly but gives it the additional flexibility to select an alternative neighborhood (defined by higher-order structures) when the information in its immediate vicinity is too sparse and/or noisy for good classification.</p><p>The aforementioned attention mechanism is trained using reinforcement learning which rewards choices (i.e, actions) that consistently result in a correct classification.</p><p>The main contributions of this paper are summarized as follows:</p><p>? We propose a model that generalizes GCNs by introducing multiple weighted motif-induced adjacencies that capture various higher-order neighborhoods.</p><p>? We introduce a novel attention mechanism that allows the model to choose the best neighborhood for each node to integrate information from.</p><p>? We demonstrate the superiority of the proposed method by comparing against strong baselines on graphs from two different domains (social network and bioinformatics). In particular, we observed a gain of up to 5.6% over the next best method on graphs which did not exhibit homophily.</p><p>? We demonstrate the usefulness of attention by showing how different nodes prioritize different neighborhoods.</p><p>The rest of the paper is organized as follows. In Section 2, we provide a review of related literature. We then introduce the details of our proposed approach in Section 3. We discuss important experimental results in Section 4. Finally, we conclude the paper in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED LITERATURE</head><p>Neural Networks for Graphs Initial attempts to adapt neural network models to work with graph-structured data started with recursive models that treated the data as directed acyclic graphs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>. Later on, more generalized models called Graph Neural Networks (GNN) were introduced to process arbitrary graph-structured data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Recently, with the rise of deep learning and the success of models such as recursive neural networks (RNN) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b47">48]</ref> for sequential data and CNNs for grid-shaped data, there has been a renewed interest in adapting some of these approaches to more general graph-structured data.</p><p>Some work introduced architectures tailored for more specific problem domains <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref> -like NeuralFPS <ref type="bibr" target="#b8">[9]</ref> which is an end-to-end differentiable deep architecture which generalizes the well-known Weisfeiler-Lehman algorithm for molecular graphs -while others defined graph convolutions based on spectral graph theory <ref type="bibr" target="#b16">[17]</ref>. Another group of methods attempt to substitute principled-yetexpensive graph convolutions using spectral approaches by using approximations of such. For instance, Defferrard et al. <ref type="bibr" target="#b7">[8]</ref> used Chebyshev polynomials to approximate a smooth filter in the spectral domain while GCNs <ref type="bibr" target="#b19">[20]</ref> further simplified the process by using simple first-order filters.</p><p>The model introduced by Kipf and Welling <ref type="bibr" target="#b19">[20]</ref> has been shown to work well on a variety of graph-based tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45]</ref> and has spawned variants including <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref>. We introduce a generalization of GCN <ref type="bibr" target="#b19">[20]</ref> in this work but we differ from past approaches in two main points: first, we use weighted motif-induced adjacencies to expand the possible kinds of node neighborhoods available to nodes, and secondly, we introduce a novel attention mechanism that allows each node to select the most relevant neighborhood to diffuse (or integrate) information. Higher-order Structures with Network Motifs Network motifs <ref type="bibr" target="#b24">[25]</ref> are fundamental building blocks of complex networks; investigation of such patterns usually lead to the discovery of crucial information about the structure and the function of many complex systems that are represented as graphs. Prill et al. <ref type="bibr" target="#b31">[32]</ref> studied motifs in biological networks showing that the dynamical property of robustness to perturbations correlated highly to the appearance of certain motif patterns while Paranjape et al. <ref type="bibr" target="#b29">[30]</ref> looked at motifs in temporal networks showing that graphs from different domains tend to exhibit very different organizational structures as evidenced by the type of motifs present.</p><p>Multiple work have demonstrated that it is useful to account for higher-order structures in different graph-based ML tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref>. DeepGL <ref type="bibr" target="#b33">[34]</ref> uses motifs as a basis to learn deep inductive relational functions that represent compositions of relational operators applied to a base graph function such as triangle counts. Rossi et al. <ref type="bibr" target="#b32">[33]</ref> proposed the notion of higher-order network embeddings and demonstrated that one can learn better embeddings when various motif-based matrix formulations are considered.</p><p>Yang et al. <ref type="bibr" target="#b45">[46]</ref> defined a hierarchical motif convolution for the task of subgraph identification for graph classification. Sankar et al. <ref type="bibr" target="#b35">[36]</ref>, on the other hand, proposes a graph convolution method designed primarily for heterogeneous graphs which utilizes motifbased connectivities. In a recent work, Morris et al. <ref type="bibr" target="#b27">[28]</ref> has shown that standard GNN architectures such as GCN have the same expressiveness as the 1-dimensional WL graph isomorphism heuristic and hence both approaches suffer from similar shortcomings. They propose a generalization using higher-order structures for the task of graph classification. Node feature embedding inputted at layer l; H (1) = X.</p><p>W (l )  Trainable embedding matrix at layer l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N ( ?) i</head><p>The set of neighbors of node i with respect to adjacency matrix ?, i.e., { j | ?i, j 0, for 1 ? j ? N }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R i</head><p>Reinforcement learning reward corresponding to training sample i. If we classify node i correctly then</p><formula xml:id="formula_4">R i = 1, otherwise R i = -1.</formula><p>Our work differs from previous approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46</ref>] in several key points. Specifically, in contrast to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46]</ref>, we propose a new class of higher-order network embedding methods which utilizes a novel motif-based attention for the task of semisupervised node classification. The proposed method generalizes previous graph convolutional approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref>. Also, unlike <ref type="bibr" target="#b35">[36]</ref>, we focus primarily on homogeneous graphs. Attention Models Attention was popularized in the deep learning community as a way for models to attend to important parts of the data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. The technique has been successfully adopted by models solving a variety of tasks. For instance, it was used by Mnih et al. <ref type="bibr" target="#b25">[26]</ref> to take glimpses of relevant parts of an input image for image classification; on the other hand, Xu et al. <ref type="bibr" target="#b43">[44]</ref> used attention to focus on task-relevant parts of an image for the image captioning task. Meanwhile, Bahdanau et al. <ref type="bibr" target="#b3">[4]</ref> utilized attention for the task of machine translation by fixing the model attention on specific parts of the input when generating the corresponding output words.</p><p>There has also been a surge in interest at applying attention to deep learning models for graphs. The work of Velickovic et al. <ref type="bibr" target="#b41">[42]</ref> used a node self-attention mechanism to allow each node to focus on features in its neighborhood that were more relevant while Lee et al. <ref type="bibr" target="#b21">[22]</ref> used attention to guide a walk in the graph to learn an embedding for the graph. More specialized methods of graph attention models include <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> with Choi et al. <ref type="bibr" target="#b6">[7]</ref> using attention on a medical ontology graph for medical diagnosis and Han et al. <ref type="bibr" target="#b13">[14]</ref> using attention on a knowledge graph for the task of entity link prediction. Our approach differs significantly, however, from previous approach in that we use attention to allow our model to select task relevant neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>We begin this section by introducing the foundational layer that is used to construct arbitrarily deep motif convolutional networks. When certain constraints are imposed on our model's architecture, the model degenerates into a Graph Attention Network (GAT) <ref type="bibr" target="#b41">[42]</ref> which, in turn, generalizes a GCN <ref type="bibr" target="#b19">[20]</ref>. Because of this, we briefly introduce a few necessary concepts from <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref> before defining the actual neural architecture we employ -including the reinforcement learning strategy we use to train our attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>We use upper-case bold letters to denote matrices, lower-case bold letters to represent vectors, and non-bold italicized letters for scalars. Frequently used notation is summarized in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Self-Attention Layer</head><p>A multi-layer GCN <ref type="bibr" target="#b19">[20]</ref> is constructed using the following layerwise propagation:</p><formula xml:id="formula_5">H (l +1) = ? ( D-1 2 ? D-1 2 H (l ) W (l ) ).<label>(1)</label></formula><p>Here, ? = A + I N is the modified adjacency matrix of the input graph with added self-loops -A is the original adjacency matrix of the input undirected graph with N nodes while I N represents an identity matrix of size N . The matrix D, on the other hand, is the diagonal degree matrix of ? (i.e., Di,i = j ?i, j ). Finally, H (l ) is the matrix of node features inputted to layer l while W (l ) is a trainable embedding matrix used to embed the given inputs (typically to a lower dimension) and ? is a non-linearity. The term D-1 2 ? D-1 2 in Eq. 1 produces a symmetric normalized matrix which updates each nodes representation via a weighted sum of the features in a node's one-hop neighborhood (the added self-loop allows the model to include the node's own features). Each link's strength (i.e., weight) is normalized by considering the degrees of the corresponding pair of nodes. Formally, at each layer l, node i integrates neighboring features to obtain a new feature/embedding via:</p><formula xml:id="formula_6">? h (l +1) i = ? j ?N ( ?) i ? i, j ? h (l ) j W (l ) ,<label>(2)</label></formula><p>where ? h (l ) i is the feature vector of node i at layer l, with fixed weights</p><formula xml:id="formula_7">? i, j = 1 ? deg(i) deg(j)</formula><p>, and N ( ?) i is the set of i's neighbors defined by the matrix ? -which includes itself.</p><p>In GAT <ref type="bibr" target="#b41">[42]</ref>, Eq. 2 is modified with weights ? that are differentiable or trainable and this can be viewed as follows,</p><formula xml:id="formula_8">? i, j = exp LeakyReLU a[ ? h i W ? h j W] k ?N ( ?) i exp LeakyReLU a[ ? h i W ? h k W] .<label>(3)</label></formula><p>The attention vector a in Eq. 3 is a trainable weight vector that assigns importance to the different neighbors of i allowing the model to highlight particular neighboring node features that are more task-relevant.  We can view a GCN <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref> as a message-passing algorithm. Each additional layer in a GCN allows the model to integrate information from a wider neighborhood. We illustrate this from the perspective of a target node (in gray). The target node integrates information from its one-hop neighbors (in orange) in layer 3. Previously, in layer 2, the orange nodes integrated information from their own one-hop neighborhood. Thus the target node also receives information from its two-hop neighbors (in blue). Similarly, in layer 1, the blue nodes integrated information from their immediate neighbors which results in the target node receiving information from its three-hop neighbors (in green). Image best viewed in color. Using the formulation in Eq. 3 with Eqs. 1 and 2, we can now define multiple layers which can be stacked together to form a deep GCN (with self-attention) that is end-to-end differentiable. The initial input to the model can be set as H (1) = X, where X ? R N ?D is the initial node attribute matrix with D attributes. The final layer's weight matrix can also be set accordingly to output node embeddings at the desired output dimensions.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates how an L-layer GCN (or GAT) enables a node to integrate information from its L-hop neighborhood. We see that this is done via repeated propagation via each nodes' one-hop neighborhood, layer by layer. Also, the size of the final neighborhood that information is propagated through is equivalent to the depth of the model.  ? Weighted Motif Graph: Given a network G and a network motif H t 2 H , form the weighted motif adjacency matrix W t whose entries (i, j) are the co-occurrence counts of nodes i and j in the motif H t : (W t ) i j = number of instances of H t that contain nodes i and j. In the case of using HONE directly with a weighted motif adjacency matrix W, then</p><formula xml:id="formula_9">: W ! IW<label>(3)</label></formula><p>The number of paths weighted by motif counts from node i to node j in k-steps is given by</p><formula xml:id="formula_10">(W k ) i j = W ? ? ? W | {z } k i j<label>(4)</label></formula><p>? Motif Transition Matrix: The random walk on a graph W weighted by motif counts has transition probabilities</p><formula xml:id="formula_11">P i j = W i j w i<label>(5)</label></formula><p>where w i = ? j W i j is the motif degree of node i. The random walk motif transition matrix P for an arbitrary weighted motif graph W is dened as:</p><formula xml:id="formula_12">P = D 1 W<label>(6)</label></formula><p>where D = diag(We) = diag(w 1 , w 2 , . . . , w N ) is a N ? N diagonal matrix with the motif degree w i = ? j W i j of each node on the diagonal called the diagonal motif degree matrix and</p><formula xml:id="formula_13">e = ? 1 1 ? ? ? 1 ? T</formula><p>is the vector of all ones. P is a row-stochastic matrix with ? j P i j = p T i e = 1 where p i 2 R N is a column vector corresponding to the i-th row of P. For directed graphs, the motif out-degree is used. However, one can also leverage the motif indegree or total motif degree (among other quantities). The motif transition matrix P represents the transition probabilities of a non-uniform random walk on the graph that selects subsequent </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Convolutional Layer with Motif Attention</head><p>We observe that both GCN and GAT rely on the edge-defined onehop neighborhood of nodes (i.e., ? in Eq. 1) to propagate information. However, it may not always be suitable to apply a single uniform definition of node neighborhood for all nodes. For instance, we show an example in Fig. <ref type="figure" target="#fig_6">3</ref> where a node can benefit from using a neighborhood defined using triangle motifs to keep only neighbors connected via a stronger bond which is a well-known concept from social theory allowing us to distinguish between weaker ties and strong ones via the triadic closure <ref type="bibr" target="#b11">[12]</ref>.</p><formula xml:id="formula_14">3.3.1 Weighted Motif-Induced Adjacencies. Given a network G = (V, E) with N = |V | nodes, M = |E | edges, as well as a set of T network motifs H = {H 1 , ? ? ? , H T }, we can construct a set of T different motif-induced adjacency matrices A = {A 1 , ? ? ? , A T }</formula><p>where A t is defined as follows:</p><p>(A t ) i, j = # of motifs of type H t which contains both i and j. In this paper, we use a loose definition for motifs and it can also mean induced subgraphs (e.g., graphlets or orbits <ref type="bibr" target="#b1">[2]</ref>). Motifs of sizes 2-4 are shown in Fig. <ref type="figure">4</ref>. As shown in Fig. <ref type="figure" target="#fig_3">1</ref>, neighborhoods defined by different motifs can vary significantly. Furthermore, the weights in a motif-induced adjacency A t can also vary as motifs can appear in varying degrees of frequency between different pairs of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Motif Matrix</head><p>Functions. Each of the calculated motif adjacencies A t ? A can now be potentially used to define motif-induced neighborhoods N (A t ) i with respect to a node i. While Eq. 3 defines self-attention weights over a node's neighborhood, the initial weights in A t can still be used as reasonable initial estimates of each neighbor's "importance. "</p><p>Hence, we introduce a motif-based matrix formulation as a function ? : R N ?N ? R N ?N over a motif adjacency A t ? A similar to <ref type="bibr" target="#b32">[33]</ref>. Given a function ?, we can obtain motif-based matrices ?t = ?(A t ), for t = 1, ? ? ? ,T . Below, we summarize the different variants of ? that we chose to investigate.</p><p>? Unweighted Motif Adjacency w/ Self-loops: In the simplest case, we can construct ? (here on, we omit the subscripts t for brevity) from A by simply ignoring the weights:</p><formula xml:id="formula_15">?i, j = ? ? ? ? ? ? ? ? ? 1 i = j 1 A i, j &gt; 0 0 otherwise.<label>(4)</label></formula><p>But, as mentioned above, we lose the initial benefit of leveraging the weights in the motif-induced adjacency A.</p><p>? Weighted Motif Adjacency w/ Row-wise Max: We can also choose to retain the weighted motif adjacency A without modification save for added row-wise maximum self-loops. This is defined as follows:</p><formula xml:id="formula_16">? = A + M,<label>(5)</label></formula><p>where M is a diagonal square matrix with M i,i = max 1?j ?N A i, j .</p><p>Intuitively, this allows us to assign an equal amount of importance to a self-loop consistent with that given to each node's most important neighbor.</p><p>? Motif Transition w/ Row-wise Max: The random walk on the weighted graph with added row-wise maximum self-loops has transition probabilities P i, j = A i, j ( k A i,k )+(max 1?k ?N A i, k ) . Our random walk motif transition matrix can thus be calculated by</p><formula xml:id="formula_17">? = D -1 (A + M),<label>(6)</label></formula><p>where, in this context, the matrix D is the diagonal square degree matrix of A + M (i.e.,</p><formula xml:id="formula_18">D i,i = ( k A i,k ) + (max 1?k ?N A i,k )) while</formula><p>M is defined as above. Here, ?i, j = P i, j or the transition probability from node i to j is proportional to the motif count between nodes i and j relative to the total motif count between i and all its neighbors.</p><p>? Absolute Motif Laplacian: The absolute Laplacian matrix can be constructed as follows:</p><formula xml:id="formula_19">? = D + A.<label>(7)</label></formula><p>Here, the matrix D is the degree matrix of A. Note that because the self-loop is a sum of all the weights to a node's neighbors, the initial importance of the node itself can be disproportionately large.</p><p>? Symmetric Normalized Matrix w/ Row-wise Max: Finally, we calculate a symmetric normalized matrix (similar to the normalized Laplacian) via:</p><formula xml:id="formula_20">? = D -1 2 (A + M)D -1 2 .<label>(8)</label></formula><p>Here, based on the context, the matrix D is the diagonal degree matrix of A + M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">K-</head><p>Step Motif Matrices. Given a step-size K, we further define K different k-step motif-based matrices for each of the T motifs which gives a total of K ? T adjacency matrices. Formally, this is formulated as follows:</p><formula xml:id="formula_21">?(k) t = ?(A k t ), for k = 1, ? ? ? , K and t = 1, ? ? ? ,T<label>(9)</label></formula><p>where</p><formula xml:id="formula_22">?(A k t ) = ?(A t ? ? ? A t k )<label>(10)</label></formula><p>When we set K &gt; 1, we allow nodes to accumulate information from a wider neighborhood. For instance if we choose to use Eq. 4 (for ?) and use an edge as our motif, ?(k) (we omit the motif-type subscript here) then captures k-hop neighborhoods of each node. While, in theory, using ?(k) is equivalent to using a k-layer GCN or GAT model, extensive experiments by Abu-El-Haija et al. <ref type="bibr" target="#b0">[1]</ref> have shown that GCNs don't necessarily benefit from a wider receptive field as a result of increased model depth. This may be for reasons similar as to why skip-connections are needed in deep architectures since the signal starts to degrade as the model gets deeper <ref type="bibr" target="#b15">[16]</ref>.</p><p>As another example, we set ? to Eq. 6. Now for an arbitrary motif, we see that ( ?(k) ) i, j encodes the probability of transitioning from node i to node j in k steps.</p><p>While the K-step motif-based adjacencies defined here share some similarity to that of Rossi et al. <ref type="bibr" target="#b32">[33]</ref> we would like to point out that there is an important distinction with our formulation. In particular, since graph convolutions integrate a node's own features via a self-loop we needed to define reasonable weights for the selfloops in the weighted adjacencies (i.e., the diagonal) so that a node's information is not "overpowered" by its neighbors' features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Motif Matrix Selection via Attention.</head><p>Given T different motifs and a step-size of K, we now have K ?T motif matrices we could use with Eq. 1 to define layer-wise propagations. A simple approach would be to implement K ? T independent GCN instances and concatenate the final node outputs before classification. However, this approach may have problems scaling when T and/or K is large which makes it unfeasible.</p><p>Instead, we propose to use an attention mechanism, at each layer, to allow each node to select a single most relevant neighborhood to integrate or accumulate information from. For a layer l, this can be defined by two functions f l : R S l ? R T and f ? l : R S l ? R T ? R K , where S l is the dimension of the state-space for layer l. The functions' outputs are softmaxed to form probability distributions over {1, ? ? ? ,T } and {1, ? ? ? , K }, respectively. Essentially, what this means is that given a node i's state, the functions recommend the most relevant motif t and step size k for node i to integrate information from.</p><p>Specifically, we define the state matrix encoding node states at layer l as a concatenation of two matrices:</p><formula xml:id="formula_23">S l = ?(A)H (l ) W (l ) C ,<label>(11)</label></formula><p>where W (l ) ? R N ?D l is the weight matrix that embeds the inputs to dimension D l , ?(A)H (l ) W (l ) is the matrix containing local information obtained by doing a weighted sum of the features in the simple one-hop neighborhood for each node (from the original adjacency A), and C ? R N ?C is a motif count matrix that gives us basic local structural information about each node by counting the number of C different motifs that each node belongs to. We note here that C is not appended to the node attribute matrix X and is not used for prediction. Its only purpose is to capture the local structural information of each node. C is computed once. Let us consider an arbitrary layer. Recall that f (for brevity, we omit subscripts l) produces a probability vector specifying the importance of the various motifs, let ? f i = f (? s i ) be the motif probabilities for node i. Similarly, let ? f ? i = f ? (? s i , ? f i ) be the probability vector recommending the step size. Now let t i be the index of the largest value in ? f i and similarly, let k i be the index of the largest value in ? f ? i . In other words, t i is the recommended motif for i while k i is the recommended step-size. Attention can now be used to define an N ? N propagation matrix as follows:</p><formula xml:id="formula_24">? = ? ? ? ? ? ? ? ? ? ?(k 1 ) t 1 1,:</formula><p>. . .</p><formula xml:id="formula_25">?(k N ) t N N ,: ? ? ? ? ? ? ? ? ? . (<label>12</label></formula><formula xml:id="formula_26">)</formula><p>This layer-specific matrix ? can now be plugged into Eq. 1 to replace ?. What this does is it gives each node the flexibility to select the most appropriate motif t and step-size k to integrate information from. which maps each node to one of J class labels in J = {1, ? ? ? , J }, our goal is to train a classifier that can predict the label of all the nodes. Given a subset T ? V, or the training set of nodes, we can train an L-layer MCN (the classifier) using standard cross-entropy loss as follows:</p><formula xml:id="formula_27">L C = - v ? T J j=1 Y v j log ? (H (L+1) i, j ),<label>(13)</label></formula><p>where Y v j is a binary value indicating node v's true label (i.e., Y v j = 1 if ?(v) = j, zero otherwise), and H (L+1) ? R N ?L is the softmaxed output of the MCN's last layer. While Eq. 13 is sufficient for training the MCN to classify inputs it does not tell us how we can train the attention mechanism that selects the best motif and step-size for each node at each layer. We define a second loss function based on the REINFORCE rule:</p><formula xml:id="formula_28">L A = -n L ? T R v log ? ? f (L) n L t (L) n L + log ? ? f (L) n L k (L) n L + nL ?T nL-1 ?N ( ?(L) ) n L R v log ? ? f (L-1) nL-1 t (L-1) n L-1 + log ? ? f (L-1) nL-1 k (L-1) n L-1 + ? ? ? + nL ? T ? ? ? n1 ?N ( ?(2) ) n 2 R v log ? ? f (1) n1 t (1) n 1 + log ? ? f (1) n1 k (1) n 1 (14)</formula><p>Here, R v is the reward we give to the system (R v = 1 if we classify v correctly, R v = -1 otherwise). The intuition here is this: at the last layer we reward the actions of the classified nodes; we then go to the previous layer (if there is one) and reward the actions of the neighbors of the classified nodes since their actions affect the outcome, we continue this process until we reach the first layer. Please refer to <ref type="bibr" target="#b25">[26]</ref> for a more detailed explanation of the REINFORCE rule for reinforcement learning.  There are a few important things to point out. In practice, we use an ?-greedy strategy when selecting a motif and step-size during training. Specifically, we pick the action with highest probability most of the time but during 1? instances we select a random action. During testing, we choose the action with highest probability. Also, in practice, we use dropout to train the network as in GAT <ref type="bibr" target="#b41">[42]</ref> which is a good regularization technique but also has the added advantage of being a way to sample the neighborhood during training to keep the receptive field from growing too large during training. Finally, to reduce model variance we can also include an advantage term (see Eq. 2 in <ref type="bibr" target="#b21">[22]</ref>, for instance). Our final loss can then be written as:</p><formula xml:id="formula_29">L = L C + L A . (<label>15</label></formula><formula xml:id="formula_30">)</formula><p>We show a simple (2-layer) example of the proposed MCN model in Fig. <ref type="figure">5</ref>. As mentioned, MCN generalizes both GCN and GAT. We list settings of these methods in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semi-supervised node classification</head><p>We first compare our proposed approach against a set of strong baselines (including methods that are considered the current stateof-the-art) on three well-known graph benchmark datasets for semi-supervised node classification. We show that the proposed method is able to achieve state-of-the-art results on all compared datasets. The compared baselines are as follows:</p><p>? MLP: Standard fully-connected multi-layer perceptron. The model does not take into account graph structure and takes directly as input node features.</p><p>? LP <ref type="bibr" target="#b48">[49]</ref>: Semi-supervised method based on Gaussian random fields which places both labeled and unlabeled samples on a weighted graph with weights representing pair-wise similarity.</p><p>? ICA <ref type="bibr" target="#b23">[24]</ref>: A structured logistic regression model which leverages links between objects.</p><p>? ManiReg <ref type="bibr" target="#b4">[5]</ref>: A framework that can be used for semi-supervised classification which uses a manifold-based regularization.</p><p>Table <ref type="table">4</ref>: Summary of experimental results: "average accuracy ? SD (rank)". The "Avg. Rank" column shows the average rank of each method. The lower the average rank, the better the overall performance of the method. ? SemiEmb <ref type="bibr" target="#b42">[43]</ref>: A model which integrates an unsupervised dimension reduction technique into a deep architecture to boost performance of semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>? DeepWalk <ref type="bibr" target="#b30">[31]</ref>: An unsupervised network embedding approach which uses the skip-gram algorithm to learn node embeddings that are similar for nodes that share a lot of links.</p><p>? Chebyshev <ref type="bibr" target="#b7">[8]</ref>: A graph convolution approach which uses Chebyshev polynomials to approximate a smooth filter in the spectral domain.</p><p>? Planetoid <ref type="bibr" target="#b46">[47]</ref>: A method which integrates graph embedding techniques into graph-based semi-supervised learning.</p><p>? MoNet <ref type="bibr" target="#b26">[27]</ref>: A geometric deep learning approach that generalizes CNNs to graph-structured data.</p><p>? GCN <ref type="bibr" target="#b19">[20]</ref>: A method which approximates spectral graph convolutions using first-order filters.</p><p>? GAT <ref type="bibr" target="#b41">[42]</ref>: Generalization of GCNs with added node-level self-attention.</p><p>? MCN (this paper): Our proposed graph attention model with motif-based attention.</p><p>4.1.1 Datasets. We compare all baselines using three established benchmark datasets, these are: Cora, Citeseer, and Pubmed. Specifically, we use the pre-processed versions made available by Yang et al. <ref type="bibr" target="#b46">[47]</ref>. The aforementioned graphs are undirected citation networks where nodes represent documents and edges denote citation; furthermore, a bag-of-words vector capturing word counts in each document serves as each node's feature. Each document is assigned a unique class label. Following the procedure established in previous work, we use only 20 nodes per class for training <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>. Again, following previous work, we take 1,000 nodes per dataset for testing and utilize an additional 500 for validation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42]</ref>. We use the same train/test/validation splits as defined in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref>. Statistics for the datasets is shown in Tab. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Setup.</head><p>For Cora and Citeseer, we used the same 2-layer model architecture as that of GAT consisting of 8 self-attention heads each with a total of 8 hidden nodes (for a total of 64 hidden nodes) in the first layer, followed by a single softmax layer for classification <ref type="bibr" target="#b41">[42]</ref>. Similarly, we fixed early-stopping patience at 100 and ? 2 -regularization at 0.0005. For Pubmed, we also used the same architecture as that of GAT (first layer remains the same but the output layer has 8 attention heads to deal with sparsity in the training data). Patience remains the same and similar to GAT, we use a strong ? 2 -regularization at 0.001.</p><p>We further optimized all models by testing dropout values of {0.50, 0.55, 0.60, 0.65}, learning rates of {0.05, 0.005}, step-sizes K ? {1, 2, 3}, and motif adjacencies formed using combinations of the following motifs: edge, 2-star, triangle, 3-star, and 4-clique (please refer to Fig. <ref type="figure">4</ref> for illustration of motifs). Self-attention learns to prioritize neighboring features that are more relevant and the motif-based adjacencies derived from ? (Eqs. 4-8) can be viewed as reasonable initial estimates of selfattention. We select the initialization that yields the best result. Finally, we adopt an ?-greedy strategy (? = 0.1).</p><p>We note that for classification, our model uses exactly the same amount of information and the same number of model parameters as GAT <ref type="bibr" target="#b41">[42]</ref> for fairness of comparison. The motif attention mechanism uses some additional trainable parameters to allow each node to select motifs but these parameters are separate from that of the classification network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparison.</head><p>For all three datasets, we report the classification accuracy averaged over 15 runs on random seeds (including standard deviation for methods that report these). A summary of the results is shown in Table <ref type="table">4</ref>. We see that our proposed method achieves superior performance against all compared baselines on all three benchmarks. On the Cora dataset, the best model used a learning rate of 0.005, dropout of 0.6, and both the edge and triangle motifs with step-size K = 1. For Citeseer, the learning rate was 0.05 and dropout was still 0.6 while the only motif used was the edge motif with step-size K = 2. However, the second best model for Citeseer -which had comparable performance -utilized the following motifs: edge, 2-star, and triangle. Finally, on Pubmed, the best model used learning rate 0.05 and dropout of 0.5. Once again, the best motifs were the edge and triangle motifs on K = 1.</p><p>One interesting observation that can be made is the fact that the triangle motif is consistently used by the top models on all the datasets. This highlights an important advantage of MCN over past approaches (e.g., GCN &amp; GAT) which are not able to handle neighborhoods based on higher-order structures such as triangles.</p><p>The results indicate that it can be beneficial to consider stronger bonds (friends that are friends themselves) when selecting a neighborhood.</p><p>Our experimental results show that we can improve model performance simply by relaxing the notion of node neighborhoods by allowing the model to choose attention-guided motif-based neighborhoods. We argue that the performance gain from this subtle but important change is significant especially since both MCN and GAT use an equal number of parameters for classification.</p><p>We also conducted some experiments on a random version of MCN which does not use attention to select motif-based neighborhoods. From our tests, we find that the method cannot outperform MCN with attention and the performance drops especially if there is a large number of motifs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison on Networks with Heterophily</head><p>The benchmark datasets (Cora, Citeseer, and Pubmed) that we initially tested our method on exhibited strong homophily where nodes that share the same labels tend to form densely connected communities. Under these circumstances, methods like GAT or GCN that use a first-order propagation rule will perform reasonably well. However, not all real-world graphs share this characteristic and in some cases the node labels are more spread out. In this latter case, there is reason to believe that neighborhoods constructed using different motifs -other than just edges and triangles -may be beneficial.</p><p>We test this hypothesis by comparing GAT and GCN against MCN on two graphs from the DD dataset <ref type="bibr" target="#b17">[18]</ref>. Specifically, we chose two of the largest graphs in the dataset: DD-6 and DD-7with a total of 4, 152 and 1, 396 nodes, respectively. Both graphs had twenty different node labels with the labels being quite imbalanced.</p><p>We stick to the semi-supervised training regime, using only 15 nodes per class for training with the rest of the nodes split evenly between testing and validation. This makes the problem highly challenging since the graphs do not exhibit homophily. Since the nodes do not have any attributes, we use the WL algorithm (we initialize node attributes to a single value and run the algorithm for 3 iterations) to generate node attributes that capture each node's neighborhood structure <ref type="bibr" target="#b37">[38]</ref> as in previous work <ref type="bibr" target="#b40">[41]</ref>.</p><p>For the three approaches (GCN, GAT, and MCN), we fix earlystop patience at 50 and use a two-layer architecture with 32 hidden   <ref type="table" target="#tab_4">5</ref>. These results demonstrate the effectiveness of MCN for realistic graphs that lack strong homophily. In particular, motif attention is shown to be extremely valuable as MCN achieves a 5.6% gain over the next best method for DD-7.</p><p>For DD-6, the best method utilized all motifs except for the 4path-edge with K = 1 while in DD-7 the best approach used the edge, triangle, and 4-clique motifs with K = 4. In both cases, the model utilized multiple motifs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualizing Motif Attention</head><p>We ran an instance of MCN (K = 1) on the Cora dataset with the following motifs: edge, 4-path, and triangle. Fig. <ref type="figure" target="#fig_12">7</ref> shows the nodes from two of the larger classes (class 3 and class 4) with each node colored by the motif that was selected by the attention mechanism.</p><p>Three important and interesting observations can be made here, we summarize them below.</p><p>? First, we find evidence of the model taking advantage of the flexibility provided by the attention mechanism to select a different motif-induced neighborhood for each node. We We observe that the nodes near the fringe of the cluster -particularly in (b) -tend to select the 4-path allowing them to aggregate information from a wider neighborhood. On the other hand, nodes that choose the triangle motif are fewer in number and can be found in the denser regions where it may be helpful to take advantage of stronger bonds. Image best viewed in color.</p><p>observe that all three types of motifs are selected and the model is not simply "defaulting" to a single type. Since our model can generalize to GAT <ref type="bibr" target="#b41">[42]</ref>, it can very well choose to just utilize the edge-based connections for every node if the other motif-based neighborhoods were not necessary.</p><p>? Second, we note that nodes that chose the triangle motif appear predominantly in denser parts of the cluster. This shows that it can be beneficial in these cases to consider the many strong bonds in the dense parts (especially if these nodes also share connections with nodes from other classes, e.g., there is noise). For class 3, we observe 3 nodes selecting the neighborhood based on the triangle motif while more than 20 nodes chose the triangle motif for class 4.</p><p>? Lastly, we notice that nodes at the fringe of the cluster often prioritized the 4-path motif. This is quite intuitive since this allows the fringe nodes to aggregate information from a wider (4-hop) neighborhood which is useful since they are more separated from the other nodes in the same class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Runtime on Large-scale Datasets</head><p>In the paper, we report semi-supervised classification results for smaller datasets as these are the standard graph benchmarks used by previous work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref> and also because these datasets have ground-truth node labels. However, the approach is fast and scalable for larger graph data. We demonstrate this in experiments on several large real-world social networks. We benchmark a sparse implementation of our proposed method on three large real-world social networks: Delicious, Youtube-Snap, and LastFM 1 . For reference, we also include Cora. The statistics for these datasets are shown in the Tab. <ref type="bibr" target="#b5">6</ref>.</p><p>In our tests, we used the architecture of the model which performed the best in previous experiments. Specifically, we used a two-layer MCN with 8 self-attention heads (each with 8 hidden nodes) in the first layer and a softmax binary classification layer in the second layer. We tested the model with the following motifs: edge, triangle, and 4-clique. These were shown to give good performance in all our previous tests with K = 1 and weighted motif adjacencies. Finally, we used 5% of the total number of nodes for training and used an equal number for validation and testing. Since the graphs do not have corresponding node attributes, we randomly generated 50-dimensional node features for each node. Likewise we also assigned random class labels to the nodes.</p><p>We report the average one-time training runtime (over five runs) of our model when run for 400 epochs -which we have found in previous experiments to be sufficient in most cases for convergence. All experiments were performed on a MacBook Pro with 2.2 GHz Intel Core i7 processors and 16GB of RAM.</p><p>The plot in Fig. <ref type="figure" target="#fig_11">6</ref> shows the one-time training cost for the model on four large real-world datasets. Once the model is trained, the parameters can be loaded and prediction can be performed in O(1) or constant time. We observe that training time does not exceed 21 hours for any of the datasets which is reasonable especially since the experiments were conducted on a standard work laptop. Also, the increase in runtime seems to be roughly linear with respect 1 These are available at http://networkrepository.com to the number of edges in the graph which is helpful since many real-world graphs are quite sparse <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we introduced a new class of higher-order network embedding methods which generalizes both GCN and GAT. The proposed model utilizes a novel motif-based attention for the task of semi-supervised node classification. Attention is used to allow different nodes to select the most task-relevant neighborhood to integrate information from.</p><p>Experiments on three citation (Cora, Citeseer, &amp; Pubmed) and two bioinformatic (DD-6 &amp; DD-7) benchmark graphs show the advantage of the proposed approach over previous work. We also show experimentally that different nodes do utilize attention to select different neighborhoods, indicating that it may be useful to consider various motif-defined neighborhoods. In particular, we found that neighborhoods defined by the triangle motif seemed to be especially useful. Finally, we benchmark a sparse implementation of MCN on several large real-world graphs and showed that the method can be run reasonably fast on large-scale networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Initial graph (b) Weighted 4-clique graph (c) Weighted 4-path graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Motif graphs dier in structure and weight. Size (weight) of nodes and edges in the 4-clique and 4-path motif graphs correspond to the frequency of 4-node cliques and 4-node paths, respectively.</figDesc><graphic url="image-3.png" coords="1,471.49,307.38,80.24,85.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4-cycle tailed-triangle chordal-cycle 4-clique (d) Various graph motifs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Node neighborhoods can differ significantly when we define adjacency based on higher-order structures or motifs. The size (weight) of nodes and edges in (b) and (c) correspond to the frequency of 4-node cliques and 4-node paths between nodes, respectively.</figDesc><graphic url="image-1.png" coords="1,315.88,227.39,137.58,138.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: We can view a GCN<ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref> as a message-passing algorithm. Each additional layer in a GCN allows the model to integrate information from a wider neighborhood. We illustrate this from the perspective of a target node (in gray). The target node integrates information from its one-hop neighbors (in orange) in layer 3. Previously, in layer 2, the orange nodes integrated information from their own one-hop neighborhood. Thus the target node also receives information from its two-hop neighbors (in blue). Similarly, in layer 1, the blue nodes integrated information from their immediate neighbors which results in the target node receiving information from its three-hop neighbors (in green). Image best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: A researcher (target node) may have collaborated on various projects in visualization and theory. However, his main research focus is ML and he collaborates closely with lab members who also work among themselves. (a) If we simply use the target node's one-hop neighborhood, we may incorrectly infer his research area; however, (b) when we limit his neighborhood using the triangle motif, we reveal neighbors connected via stronger bonds giving us a better chance at inferring the correct research area. This observation is empirically shown in our experimental results. Illustration best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Initial graph (c) Weighted 4-path graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Motif graphs dier in structure and weight. Size (weight) of nodes and edges in the 4-clique and 4-path motif graphs correspond to the frequency of 4-node cliques and 4-node paths, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Initial graph (c) Weighted 4-path graph edge 2-star triangle 3-star 4-path 4-cycle tailed-triangle chordal-cycle 4-clique (d) Various graph motifs Figure 4: Network motifs or graphlets of sizes 2-4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>3. 3 . 5 Figure 5 :</head><label>355</label><figDesc>Figure 5: An example of a 2-layer MCN with N = 11 nodes, step-size K = 2, and T motifs. Attention allows each node to select a different motif-induced neighborhood to accumulate information from for each layer. For instance, in layer 1, the node v N considers neighbors (up to 2-hops) that share a stronger bond (in this case, triangles) with it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Runtime of proposed method on large real-world graphs. Percent values above the bars indicate the ratio of the dataset's edges compared to the number of edges in the largest dataset (LastFM).nodes in the first layer followed by the softmax output. We optimized the hyperparameters by searching over learning rate in {0.05, 0.005}, ? 2 regularization in {0.01, 0.001, 0.0001, 0.00001}, and dropout at {0.2, 0.3, 0.4, 0.5, 0.6}. Furthermore, for MCN, we considered combinations of the following motifs {edge, 2-star, triangle, 4-path-edge, 3-star, 4-cycle, 4-clique} and considered K-steps from 1, ? ? ? , 4. Since there are multiple classes and they are highly imbalanced, we report the Micro-F1 score averaged over 10 runs.A summary of the results are shown in Table5. These results demonstrate the effectiveness of MCN for realistic graphs that lack strong homophily. In particular, motif attention is shown to be extremely valuable as MCN achieves a 5.6% gain over the next best method for DD-7.For DD-6, the best method utilized all motifs except for the 4path-edge with K = 1 while in DD-7 the best approach used the edge, triangle, and 4-clique motifs with K = 4. In both cases, the model utilized multiple motifs.</figDesc><graphic url="image-7.png" coords="8,334.85,191.66,204.20,151.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The largest connected components taken from the two induced subgraphs in Cora of nodes from (a) class 3 and (b) class 4, respectively. Nodes are colored to indicate the motif selected by the motif attention mechanism in the first layer. The motifs are: edge (blue), 4-path (red), and triangle (green).We observe that the nodes near the fringe of the cluster -particularly in (b) -tend to select the 4-path allowing them to aggregate information from a wider neighborhood. On the other hand, nodes that choose the triangle motif are fewer in number and can be found in the denser regions where it may be helpful to take advantage of stronger bonds. Image best viewed in color.</figDesc><graphic url="image-8.png" coords="9,53.80,83.68,504.37,217.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Table of notations.</figDesc><table><row><cell cols="2">Symbol Definition</cell></row><row><cell>G N H A t ?t Dt D X H (l )</cell><cell>Undirected graph with vertex set V and edge set E. Number of nodes in G, i.e., |V | = N . The set {H 1 , ? ? ? , H T } of T network motifs (i.e., in-duced subgraphs). N ?N motif-induced adjacency matrix corresponding to motif H t . (A t ) i, j encodes the number of motifs of type H t which contain (i, j) ? E. When the subscript t is ommitted, this refers to the default edge-defined adjacency matrix. N ? N adjacency matrix A t with added self-loops. N ? N diagonal degree matrix of ?t . Number of features or attributes per node. N ? D attribute matrix.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Space of methods expressed by MCN. GCN and GAT are shown below to be special cases of MCN.</figDesc><table><row><cell cols="2">Method Motif</cell><cell>Adj.</cell><cell>K</cell><cell cols="2">Self-attention Motif-attention</cell></row><row><cell>GCN</cell><cell>edge</cell><cell>Eq. 4</cell><cell>K = 1</cell><cell>no</cell><cell>no</cell></row><row><cell>GAT</cell><cell>edge</cell><cell>Eq. 4</cell><cell>K = 1</cell><cell>yes</cell><cell>no</cell></row><row><cell>MCN-*</cell><cell>any</cell><cell cols="2">Eqs. 4-8 K = {1, ? ? ? }</cell><cell>yes</cell><cell>yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Dataset statistics. Value shown in brackets is the percentage of the nodes used for training.</figDesc><table><row><cell>Cora</cell><cell>Citeseer Pubmed</cell></row><row><cell cols="2"># of Nodes # of Edges # of Features/Node # of Classes # of Training Nodes 140 (5%) 120 (4%) 60 (&lt;1%) 2,708 3,327 19,717 5,429 4,732 44,338 1,433 3,703 500 7 6 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Micro-F1 scores of compared methods on DD.</figDesc><table><row><cell>Method GCN GAT MCN</cell><cell>Dataset 11.9 ? 0.6% 12.4 ? 0.8% DD-6 DD-7 11.8 ? 0.5% 11.8 ? 1.1% 12.4 ? 0.5% 13.1 ? 0.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Statistics of large benchmark graphs. 'Edge %' denotes the ratio of the graph's edges versus the total number of edges in the largest dataset (LastFM).</figDesc><table><row><cell>Dataset</cell><cell cols="5"># of Nodes # of Edges Max Degree Avg. Degree Edge %</cell></row><row><cell>Cora Delicious YouTube-Snap LastFM</cell><cell>2,708 ?536K ?1.1M ?1.2M</cell><cell>5,429 ?1.4M ?3M ?4.5M</cell><cell>168 ?3K ?29K ?5K</cell><cell>?4 ?5 ?5 ?7</cell><cell>&lt;1.0% 31.1% 66.7% 100.0%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work is supported in part by <rs type="funder">National Science Foundation</rs> through grant <rs type="grantNumber">IIS-1718310</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CaPbF4V">
					<idno type="grant-number">IIS-1718310</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">N-GCN: Multi-scale Graph Convolution for Semi-supervised Node Classification</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08888v1</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graphlet Decomposition: Framework, Algorithms, and Applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nesreen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><forename type="middle">L</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><surname>Willke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KAIS</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="689" to="722" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Role-based Graph Embeddings</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nesreen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Boaz Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><forename type="middle">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoda</forename><surname>Willke</surname></persName>
		</author>
		<author>
			<persName><surname>Eldardiry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">StarAI @ IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Higher-order Spectral Clustering for Heterogeneous Graphs</title>
		<author>
			<persName><forename type="first">Aldo</forename><forename type="middle">G</forename><surname>Carranza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunyee</forename><surname>Koh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02959</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GRAM: Graph-based Attention Model for Healthcare Representation Learning</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Taha</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="787" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional Two-Stream Network Fusion for Video Action Recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A general framework for adaptive processing of data structures</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="768" to="786" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Triangles to Capture Social Cohesion</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Friggeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Chelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Fleury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SocialCom/PASSAT</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="258" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<idno>IJCNN. 729-734</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural Knowledge Acquisition via Mutual Attention Between Knowledge Graph and Text</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Recurrent Q-Learning for Partially Observable MDPs</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Fall Symposium</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep Convolutional Networks on Graph-Structured Data</title>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163v1</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Benchmark Data Sets for Graph Kernels</title>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>Petra Mutzel, and Marion Neumann</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeply-Recursive Convolutional Network for Image Super-Resolution</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph Classification using Structural Attention</title>
		<author>
			<persName><forename type="first">John</forename><surname>Boaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network Motifs: Simple Building Blocks of Complex Networks</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="824" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent Models of Visual Attention</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep Convolutional Networks on Graph-Structured Data</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08402</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph Convolutional Networks with Argument-Aware Pooling for Event Detection</title>
		<author>
			<persName><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5900" to="5907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Motifs in Temporal Networks</title>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic Properties of Network Motifs Contribute to Biological Network Organization</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Prill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Levchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1881" to="1892" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Higher-order Network Representation Learning</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunyee</forename><surname>Koh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Inductive Network Representation Learning</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigNet @ WWW</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimation of Graphlet Counts in Massive Networks</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TNNLS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Motifbased Convolutional Neural Network on Graphs</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05697v3.1-7</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman Graph Kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jan Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonina</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="735" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Less is More: Compact Matrix Decomposition for Large Sparse Graphs</title>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="366" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Graph Representations with Recurrent Neural Network Autoencoders</title>
		<author>
			<persName><forename type="first">Aynaz</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Berger-Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Day @ KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep Learning via Semi-supervised Embedding</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?d?ric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3482" to="3489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Node, Motif and Subgraph: Leveraging Network Functional Blocks Through Structural Convolution</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>ASONAM. 1-8</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Revisiting Semi-Supervised Learning with Graph Embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Minimal gated unit for recurrent neural networks</title>
		<author>
			<persName><forename type="first">Guo-Bing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJAC</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="226" to="234" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
