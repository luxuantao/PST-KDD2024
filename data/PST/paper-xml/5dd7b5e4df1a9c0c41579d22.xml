<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Journal Pre-proof Diabetic retinopathy detection using red lesion localization and convolutional neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gabriel</forename><forename type="middle">Tozatto</forename><surname>Zago</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rodrigo</forename><forename type="middle">Varejão</forename><surname>Andreão</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Instituto Federal do Espírito Santo</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernadette</forename><surname>Dorizzi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">SAMOVAR</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Télécom SudParis</orgName>
								<address>
									<addrLine>9 rue Charles Fourier</addrLine>
									<postCode>91011 EVRY</postCode>
									<settlement>Paris</settlement>
									<region>IP</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Evandro</forename><surname>Ottoni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Teatini</forename><surname>Salles</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Universidade Federal do Espírito Santo</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Tozatto Zago</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Control and Automation Engineering</orgName>
								<orgName type="institution">Instituto Federal do Espírito Santo</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Journal Pre-proof Diabetic retinopathy detection using red lesion localization and convolutional neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FA77884565A5C03882008330FBFA1E9A</idno>
					<idno type="DOI">10.1016/j.compbiomed.2019.103537</idno>
					<note type="submission">Received Date: 17 June 2019 Revised Date: 8 November 2019 Accepted Date: 10 November 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>retinal images</term>
					<term>deep learning</term>
					<term>diabetic retinopathy</term>
					<term>convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Diabetic retinopathy (DR) is a common complication of diabetes and involves variations in retinal blood vessels. These variations can cause the blood vessels to bleed or leak fluid, distorting vision. It is the most prevalent cause of vision loss among individuals with diabetes and a significant cause of blindness among working-age adults. Fortunately, early detection, timely therapy, and adequate diabetic eye disease follow-up care can safeguard against the loss of vision.</p><p>Therefore, it is crucial to offer easy methods of detection of this disease on a large scale. Several devices enable the acquisition of retinal images, but manual diagnosis and evaluation of images requires significant effort. Therefore, automatic systems can reduce time, cost, and effort significantly and be a valuable tool for practitioners, especially considering the increasing number of diabetes cases.</p><p>The first attempts at automatic systems involved the use of classical image processing techniques, but quite recently, the introduction of deep networks and, in particular, convolutional neural networks (CNNs) has had a significant effect on medical image analysis. Indeed, such approaches are producing impressive results in the classification of many types of diseases, as well as in the localization and segmentation of regions of interest <ref type="bibr" target="#b1">[1]</ref>. These results are obtained because of the availability of a vast quantity of labeled data. DR detection, which is the subject of the present study, is no exception, as it is generally performed through the analysis of retinal images.</p><p>In contrast to classical image processing systems, which use predefined features as an intermediate stage for classification, CNN models can directly propose a classification from raw pixel images and independently extract the appropriate representation of the images.</p><p>Following this line of thinking, several authors have proposed CNN models for indicating the degree of DR or the presence of a disease in a given image. These models are very efficient, but, if learned from scratch, they require a large quantity of labeled data, which are not always available. Moreover, the performance is very dependent on the statistics of the data used for learning. Therefore, the resulting system may not be robust when used with a different type of data from a different acquisition environment.</p><p>In practice, these models, which do not explicitly explain their method of decision-making, are not suited for interaction with clinicians. Nevertheless, it is important for clinicians to understand why the model made its decision and in particular, which region in the images influenced the DR diagnosis.</p><p>For this reason, we decided to focus the present work on building an automatic DR system based on a CNN approach, but specifically, one that explicitly relies on a localization of lesions that can be assessed by an expert. We do not aim to precisely segment the lesions, but to localize them, and to produce some probability that the lesion may induce DR or not.</p><p>The chosen method is inspired by the work of Ciresan et al. <ref type="bibr" target="#b2">[2]</ref> in cancerology, and it uses a deep CNN to classify patches of an image as lesion or non-lesion. Therefore, the system is able to localize the regions with lesions for further DR detection.</p><p>In practice, the CNN takes patches of the initial retinal image as inputs and produces a probability of a lesion being present in each patch. The resulting probability map is post-processed to ultimately decide whether DR is present or not. This model can be seen in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>As the first signs of DR in the retina are generally microaneurysms (MAs) and hemorrhages, we focus on localizing these lesions, called "red lesions," instead of bright lesions or neovascularizations (which are later signs of DR). Training such a CNN requires the availability of a database with explicit labeling of the regions by an expert. In contrast to what occurs when a CNN is trained on a global image, even if the training database is of limited size, we obtain a large number of input data (as one image of size 512 pixels x 512 pixels gives rise to 1,310,720 patches). Therefore, finding efficient protocols for choosing adequate samples has a significant impact on the final quality of the system.</p><p>We propose a novel strategy for selecting challenging samples by relying on a two-stage construction: a simple classifier allows for the detection of misclassified samples, which are used to enrich the initial training database for the final classifier in a second stage. In this way, the performance of the classifier is increased. This is one of the solutions which makes this study original.</p><p>The high number of patches also raises a problem in terms of complexity in the production phase (once the learning phase is completed). The modification that we introduced relies on a subsampling of the image. Indeed, as we search for only a raw localization of the lesion and not a precise segmentation, we can downsample the image, and consider only a sampling of the patches for analysis. The use of strides accelerates the process by up to a factor of 25 as compared to other CNNs that do not use strides.</p><p>We also expect such an approach to be more generic than global models working globally on an image, as the final decision will result from a focused local analysis. We will show in the experimental part of the study that this is indeed the case, in particular, as compared to other state-of-the-art models.</p><p>As no large databases of labeled retinal lesions are available, we propose, like other authors, to use a pre-trained CNN with transfer learning to classify patches of retinal images as lesion or non-lesion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>State-of-the-art automatic DR detection techniques can be roughly classified into two types: older studies that rely on classical image processing techniques for detecting, segmenting, and analyzing lesions in images depending on their precise characteristics, and more recent studies, that rely on CNNs to perform both feature extraction and classification.</p><p>Because MAs and hemorrhages are generally the first signs of DR, several works have focused on these diseases, particularly for the early detection of DR. Hence, we focus our review on those works.</p><p>There are many studies that use the classical image processing pipeline, and some of them are summarized in Table <ref type="table" target="#tab_0">1</ref>. We have limited ourselves to the studies that detect MAs and hemorrhage lesions, as their presence is significant for the early detection of DR.</p><p>Most works on this subject use the following pipeline.</p><p>• Preprocessing: enhancing the retinal image to make the lesions more visible.</p><p>• Lesion candidate selection: filtering, morphological transformations, and thresholding are the techniques commonly used to select candidates. • Feature extraction: features such as perimeter, area, and average intensity are obtained from the lesion candidates. Commonly-used features include perimeter, area, and average intensity.</p><p>• Classification: the candidates are classified into lesion or non-lesion based on the characteristics of the extracted features. The second period corresponds to the wider, more recent one, which started in the domain of DR detection, with the use of CNN-based systems. Most of the studies use CNNs globally on the images without any detection of regions <ref type="bibr" target="#b16">[16]</ref>- <ref type="bibr" target="#b20">[20]</ref>, and only few approaches consider lesion detection before DR classification <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>. To that end, the retinal image is split into patches that constitute the input to the network.</p><p>In that regard, a few papers have been published on lesion detection using deep learning <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>. The studies that employed CNNs are summarized in Table <ref type="table" target="#tab_1">2</ref>.</p><p>In particular, two studies are related to the present one, as they also employ patch-based CNN. Orlando et al. <ref type="bibr" target="#b21">[21]</ref> selected lesion candidates using traditional image processing, and used manually-designed features together with CNN features for classification by a random forest classifier.</p><p>A lesion probability map is built by assembling the outputs of the classifier in the position of each candidate analyzed. The authors conducted several experiments, but two of them are more important in the context of this study: one experiment was conducted to evaluate the lesion localization and another to assess the DR detection capability of the proposed pipeline. In contrast to <ref type="bibr" target="#b21">[21]</ref>, our approach is entirely based on deep learning, which increases the genericity concerning different datasets.</p><p>A patch-based CNN MA detection method was developed by Chudzik et al. <ref type="bibr" target="#b22">[22]</ref>. They aimed to segment the regions rather precisely to detect MA. Their processing pipeline was simple and composed of preprocessing, patch extraction, and classification. A 24-layer CNN was used to segment the potential lesions, and a voting method was employed to generate the final probability map. A Dice coefficient loss function was used to solve the problem of imbalanced data. Our work differs from <ref type="bibr" target="#b22">[22]</ref> in several aspects. First, our goal is to localize the lesions for further DR detection instead of segmenting them, and this allows us to select only some patches of the image, thereby speeding up the prediction process. We believe that a rough segmentation is sufficient to produce a discriminant marker of a lesion. In addition, our approach does not require any retraining when the model is applied to a new dataset. The model learns to detect lesions (localization) on a given database and is tested in terms of DR on other databases.</p><p>To summarize, the main contributions and novelties brought by the present study are the following.</p><p>-We designed a fully automatic CNN method for lesion localization (without any manually-designed features), acting on patches extracted from an image that are ultimately used to provide a DR diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-</head><p>The usage of strides (subsampling of patches) accelerates the processing time by up to a factor of 25 as compared to other CNN methods that do not use strides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-</head><p>During the learning stage, the sample selection method helps the final model focus on challenging samples, thereby increasing the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-</head><p>The designed model is shown to be robust to cross-validation over different databases, which is promising for practical applications. Section 2 is devoted to a description of the databases considered in this work and a precise explanation of the main stages of the model. It includes a description of the CNN, procedure for patch labeling, challenging patches selection for the training, subsampling method for accelerating the lesion localization, and process of DR detection from the global image. In Section 3, we present our results by using cross-dataset validation to prove the robustness of our method and compare them with those from recent literature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Databases</head><p>Seven datasets were used in our experiments: the Standard Diabetic Retinopathy Database. Calibration level 0 (DIARETDB0), Standard Diabetic Retinopathy Database. Calibration level 1 (DIARETDB1) <ref type="bibr" target="#b23">[23]</ref>, Kaggle, Messidor <ref type="bibr" target="#b24">[24]</ref>, Messidor-2, Indian Diabetic Retinopathy Image Dataset (IDRiD) <ref type="bibr" target="#b25">[25]</ref>, and DDR <ref type="bibr" target="#b26">[26]</ref>.</p><p>DIARETDB0 <ref type="bibr" target="#b27">[27]</ref> consists of 130 fundus images; 20 are normal, and 110 contain signs of DR. The dataset has annotations concerning the presence of red small dots, hemorrhages, hard exudates, soft exudates, and neovascularization on the image level.</p><p>DIARETDB1 <ref type="bibr" target="#b23">[23]</ref> is composed of 84 retinal images with signs of DR, and 5 considered normal. It contains 28 images in the training set, and 61 in the test set. The entire dataset was analyzed by four experts who delineated the lesions by type: soft exudates, hard exudates, red small dots, and hemorrhages. Five levels of lesion agreements are possible, 0, 0.25, 0.5, 0.75, and 1, indicating how many experts labeled each pixel as a lesion. Figure <ref type="figure" target="#fig_1">2</ref> shows a retinal image and its ground truths from this dataset. The different gray levels in the ground-truth images indicate the level of agreement. Each pixel is associated with one of 5 values (0 means no lesion, whereas the others indicate a lesion with a certain degree of confidence). The Kaggle retinal dataset is available in a competition platform website and was proposed for a DR detection competition in 2015 1 . It consists of 88,702 color fundus images, including 35,126 for training and 53,576 for testing. There are two images per subject, one per eye. An expert evaluated each image for the presence of DR with a scale of 0-4, according to the Early Treatment Diabetic Retinopathy Study (ETDRS) scale <ref type="bibr" target="#b28">[28]</ref>.</p><p>Messidor contains only image-level labels and indicates DR presence through macular edema grades. It comprises 1,200 fundus images acquired from different ophthalmic institutions in France <ref type="bibr" target="#b24">[24]</ref>. There are four levels of DR in this dataset: R0, composed of images indicating a healthy retinal image; R1, composed of images with very mild signs of DR; R2, composed of images with signs of DR that require the attention of an ophthalmologist; and R3, composed of images indicating a proliferative DR retinal image. The public Messidor-2 dataset contains 1,748 fundus images from 874 subjects in a region of France. Although the publishers do not provide public labels, referable DR annotations are available from another research group 2 .</p><p>Another dataset used in this work is IDRiD, which contains 516 images annotated according to a DR severity level (0-4). The dataset is divided into a training (413) and test set (103).</p><p>The last dataset used in the present work is the DDR dataset, acquired from 147 hospitals in China. It consists of 13,673 fundus images, annotated with the severity of DR as determined by multiple experts. The test set is composed of 4,105 images.</p><p>These datasets are summarized in Table <ref type="table" target="#tab_2">3</ref>. In our work, DIARETDB1 was used to train the models, as it labels lesions at the pixel level. In that regard, all of the remaining datasets were used to validate the models for DR classification, as they provide DR-level annotations. When possible, a comparison with other approaches in the literature on the same dataset is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Preprocessing</head><p>The retinal images ( ! ) are enhanced as proposed in <ref type="bibr" target="#b29">[29]</ref>, to make the lesions more visible: " #, %; ' = ( ⋅ ! #, % + + ⋅ ,-.//#-0 #, %; ' * ! #, % + 2</p><p>(1)</p><p>Here, ( = 4, + = -4, ' = 3 30 ⁄ , 2 = 128, and 3 is the width of the image. The result of the preprocessing can be observed in Figure <ref type="figure">3</ref>. This preprocessing is performed on all datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Patch labeling</head><p>The patch-based method is inspired by Cireşan et al.'s work <ref type="bibr" target="#b2">[2]</ref> on cancer detection. A patch is a × region centered on a particular pixel.</p><p>We aim to provide a classification of each patch extracted from a given retinal image into two clusters: lesion or non-lesion. (We could imagine more classes, as DIARETDB1 provides finer labeling, but we want to make the entire process as simple as possible.)</p><p>As DIARETDB1 provides labels for several types of lesions, the first step is to combine the labels of the types of lesions we are interested in -hemorrhages ( 678 6 9 : ), and red small dots ( 678 6 ;&lt; ). To achieve that, each label image (Figure <ref type="figure" target="#fig_1">2</ref>) is binarized by a hard threshold -any pixel with a label larger than zero is considered positive. Next, the union of the binarized individual labels is taken as the final pixel-level label: 678 6 = = 678 6 9 : &gt; 0? ∪ = 678 6 ;&lt; &gt; 0?</p><p>As the database is labeled at the pixel-level, we need a protocol to combine the outputs of the pixels in the patch, thereby producing a single output for each patch. Thus, a patch is considered as a lesion if any pixel in a radius of A pixels around its center is labeled as a lesion. In this work, the values of = 65 and A = 16 were used, considering images that are 512 pixels wide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Selecting the patches for fitting the final model</head><p>The DIARETDB1 dataset is used as the training set, and its images are divided into training and test sets, as proposed in the database itself. Instead of training and validating the model using all available patches (which would result in more than 23 million samples, considering that the preprocessed images are 512 × 512 and that there are 262,144 patches per image), a sample selection strategy was used.</p><p>We operate in two stages, using two different networks.</p><p>First, for each image of the training set, all lesion pixels and the same number of random non-lesion pixels are selected to fit a five-layer CNN (Figure <ref type="figure" target="#fig_6">6</ref>) called the selection model, for a small number of epochs. This model quickly converges, because the dataset with pre-selected patches is balanced in terms of lesion and non-lesion pixels.</p><p>However, this model performs poorly. Indeed, it tends to misclassify some non-lesion patches, as it has not been trained with enough patches with anatomic structures, which may be confused with lesions. We, therefore, use this model to select more challenging non-lesion patches (ones that are misclassified by the selection model) for use in the final model, for performing the last step of transfer learning retraining.</p><p>More precisely, we classify all patches of the training set with this selection model. Then, we select all lesion patches and 50,000 non-lesion patches. However, instead of choosing them randomly, they are chosen as follows:</p><p>• All non-lesion patches of a retinal image are sorted according to the prediction error of the selection model. • 50,000 patches with greater prediction errors are used in the final training process.</p><p>• The prediction error of each patch is used as a weight in a back-propagation algorithm (sample weight), to give more importance to patches that have been challenging the selection model (where the error was greater) <ref type="bibr" target="#b2">[2]</ref>. Using this approach, the final model will be more robust, as it is trained with samples that confused the selection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">Transfer learning</head><p>For the final model, we used the Visual Geometry Group (VGG)16 model <ref type="bibr" target="#b30">[30]</ref> because of its high generalization capacity, and because we already have ImageNet pre-trained weights available. The model is initialized with the weights trained on the ImageNet dataset, and is tuned using the patches selected by the selection model as described earlier.</p><p>The entire training process is patch-based, as the model is trained using a pair (B C , D C ) where B C is an input patch of a retinal image, and D C is the output label of this patch (as defined in Section 2.3.1).</p><p>The classification of a pixel is slightly different and is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Calculation of the Lesion Probability Map</head><p>It is well known that data augmentation improves the performance of CNNs. We therefore average the model's output of five rotations of the patch, as proposed by Ciresan et al. <ref type="bibr" target="#b2">[2]</ref>. This process is illustrated in Figure <ref type="figure" target="#fig_4">4</ref>. The problem with this approach is that the segmentation of each 512 × 512 image would require the model to predict 1,310,720 patches, which, using a GPU GeForce GTX 1050Ti, would take 20 minutes. A total of 16 days would be required to segment the entire Messidor database.</p><p>Considering that the goal is to detect the presence of lesions rather than to provide a precise segmentation, we propose to use a reduced number of patches per image, by considering only part of the image's pixels, as illustrated in Figure <ref type="figure" target="#fig_5">5</ref>. Instead of considering all pixels of the image, we select pixels spaced by pixels from each other, horizontally and vertically. In this way, the number of patches that must be analyzed in an image is reduced by E . Subsequently, the resulting segmented image is resized back to the original dimensions, using extrapolation.</p><p>When the size of the stride increases, the number of patches to be predicted decreases. A 512 × 512 image that initially required 1,310,720 predictions to be segmented, using stride 5, for example, would only require 52,428predictions, taking only 48 seconds to be processed by the same hardware mentioned before.</p><p>The probability map is an image with the same dimensions as the input image with pixel intensity values varying from 0 to 1, representing the probability of a particular pixel being a lesion.</p><p>In summary, to obtain the lesion probability map from a retinal image using the trained model, (i) pixels are selected using strides, (ii) the model evaluates patches centered on the chosen pixels, (iii) the output map is resized to the original retinal image size through linear extrapolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Inferring the Level of Diabetic Retinopathy (DR) from a Lesion Probability Map</head><p>To classify a retinal image ( ! ) as indicating DR or not indicating DR, we must infer a single prediction value from its probability map.</p><p>Several approaches could be considered for this objective. For example, it would be possible to extract several features from the probability map generated by the model, such as the number of lesions, maximum value, minimum value, the average area of the lesions, and others and to build a new classifier to generate a class (or probability) from those features.</p><p>As we want the system to remain generic and straightforward, we propose to represent the image by the maximum value of the probability map, as used in <ref type="bibr" target="#b15">[15]</ref>:</p><formula xml:id="formula_0">F G /#H0 ∨ ! = J-B= ! ?<label>(2)</label></formula><p>Here, ! is the red lesion probability map described above. Indeed, this quantity:</p><p>• tends to increase with the severity of DR; and</p><p>• goes from 0 to 1 (or can be monotonically transformed to this range).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Performance Indicators for Diabetic Retinopathy (DR) Detection</head><p>The classifier's performances were described using the area under the receiver's operating characteristic curve (AUC), sensitivity (Se), and specificity (Sp).</p><p>Contrary to many authors, who present the average level of the performance indicator calculated over the entire dataset, we calculate the confidence intervals (CIs) using the following bootstrap method <ref type="bibr" target="#b31">[31]</ref> ´ is calculated in the original sample. 6. For a 95% confidence level, the percentiles 2.5 ( .UEV</p><p>) and 97.5 ( .WXV ) are taken from all S Q * .</p><p>7. Finally, the CI for the mean of the performance indicator P using the classifier is calculated by</p><formula xml:id="formula_1">υ Z RRR -p .WXV δ Z * RRR , υ Z RRR -p .UEV δ Z * RRR .</formula><p>This technique can also be used to conduct paired tests to compare K and E ,as follows.</p><p>1. M 8NN! bootstrap empirical samples (O * ) are created from the original data.</p><p>2. The classifier performance indicator P is calculated for each O C * bootstrap empirical sample, for each classifier K and E . 3. As there is no guarantee that the distributions are normal, a one-sided Wilcoxon rank test is conducted with the sets P Q ] and P Q ^ to compare the classifiers. As a result, the hypothesis test is accepted or rejected, and the -_-G. is returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>The model was trained on the DIARETDB1 database, and was tested for DR detection on the datasets described in Section 2.1. These datasets contain DR labels with different grades, and are also frequently used in the literature for automatic DR detection, thereby allowing comparison with other works.</p><p>Two experiments are proposed to assess the model's capacity for detecting DR. As the first grades of DR correspond to images with a very mild level of DR that would not require any ophthalmic intervention, researchers have defined two different classification problems.</p><p>1. DR screening: in this experiment, the objective is to detect any level of DR, i.e., to discriminate between normal images and any level of DR; this is challenging, as images with mild DR have very few signs of DR. 2. DR need for referral: the goal in this problem is to separate images with a high level of DR, namely to cluster normal and mild DR versus all others. We use a stride of 5 pixels in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Training on standard diabetic retinopathy database, calibration level 1 (DIARETDB1)</head><p>Following the method described in Section 2.3, the model was trained on the DIARETDB1 training set, which is composed of 28 retinal images with pixel-level annotations.</p><p>The choice of the patch size was made empirically, by observing the lesions of the training set. The patch should be large enough to contain the largest lesions (such as hemorrhages), but not too large; otherwise, the small lesions would occupy a very small area of the patch. Given that 80% of the hemorrhages have a size smaller than 52 pixels and 80% of the red small dots have a size smaller than 22 pixels, we used patches of a size = 65 pixels, and a radius A = 16 pixels.</p><p>The selection model is a deep convolution network <ref type="bibr" target="#b32">[32]</ref>. The details of the different layers are given in Figure <ref type="figure" target="#fig_6">6</ref>. The model was trained for 50 epochs with 25,920 lesion patches, and with the same number of random non-lesion patches. Both sets of patches were augmented by random rotations between 0 and 360 degrees, and with horizontal and vertical flips. This model is used to select the 50,000 most challenging non-lesion patches that will be used in the second stage, i.e., the ones leading to higher error. In the second stage, a model is built based on the VGG16 architecture <ref type="bibr" target="#b30">[30]</ref>, where the last layers are replaced by three dense layers of size 512, 256, and 2, respectively. The model's weights were initialized through the training of the network for another task (object detection on ImageNet dataset) and were tuned using the DIARETDB1 training set for 1,000 epochs and using the stochastic gradient descent (SGD) algorithm with batches of 128 patches.</p><p>In the experiments, we used an initial learning rate of 0.01, with a momentum of 0.9. We implemented early stopping after 15 epochs without improvement on the MSE loss, together with a learning rate reduction by a factor of 0.5 after ten epochs without improvement on the validation set (20% randomly-selected patches out of the training set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Tests on DIARETDB1</head><p>The DIARETDB1 test set was used to test the model's ability to detect red lesions. In Section 2.3.1, we saw that labeling the lesion requires the choice of a threshold. We selected a threshold that maximized the `K score, which is the harmonic average between recall and precision (`</p><formula xml:id="formula_2">K = 2 • A b#/#H0 • A b-GG A b#/#H0 + A b-GG ⁄</formula><p>). The selected threshold was 0.39.</p><p>Next, the bootstrap method was applied to calculate the distribution and CIs for recall, precision, and `K score on the entire test set of DIARETDB1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Tests on standard diabetic retinopathy database, calibration level 0 (DIARETDB0)</head><p>DIARETDB0 has only binary annotations on the presence of lesions. To use it for DR detection, a simple protocol was used: images containing labels for red small dots or hemorrhages were considered as indicating DR. Therefore, the only experiment tested on this dataset is the DR screening experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Tests on Messidor</head><p>Messidor has been extensively used in the literature to evaluate DR detection approaches. As already described, the images are graded with DR levels from R0 (no DR) to R3. The experiments are designed in the following way:</p><p>1. DR screening: {R0} versus {R1, R2, R3}; and 2. DR need for referral: {R0, R1} versus {R2, R3}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5.">Tests on Indian diabetic retinopathy image dataset (IDRiD)</head><p>The IDRiD dataset separates images in groups ranging from 0 (No apparent DR) to 4 (Severe DR), according to the International Clinical Diabetic Retinopathy Scale <ref type="bibr" target="#b33">[33]</ref>. In the experiments, we divided the dataset into {0} versus {1,2,3,4} in the DR screening experiment, and {0,1} versus {2,3,4} in the DR need for referral experiment. Even if this database provides training and test sets, we decided to use only the test set for comparison with other works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6.">Tests on DDR</head><p>The DDR dataset uses the DR grades from the International Classification of DR <ref type="bibr" target="#b34">[34]</ref> (from 0 to 4), and a specific label for poor-quality images. Those poor-quality images were discarded from the analysis. We divided the dataset using the same groups as with the IDRiD dataset and tested our model on the test set only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.7.">Tests on Kaggle</head><p>The Kaggle dataset uses the same grades as IDRiD, with the difference being that Kaggle contains two images per subject, i.e., one per eye. We made use of this extra information and carried out the two experiments described above using per-subject analysis as in other datasets. For each subject, we obtained two prediction values, and the maximum one was selected for classification. As the Kaggle dataset was proposed in a competition context, it only provides grades for the training set. For this reason, we applied our approach on the training set only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.8.">Tests on Messidor-2</head><p>The Messidor-2 dataset also has two images per subject, and the same protocol as in Section 3.1.7 was employed. Only two grade levels are available for this dataset 5 : referable subjects, and non-referable subjects. For that reason, only the DR need for referral experiment is applied to this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Results</head><p>In the state of the art, most of the works that apply deep learning for DR detection use hundreds of thousands of images to train the model, meaning a huge burden for the experts to label the images accordingly. As a difference from those works, our model was trained using only 28 images from the DIARETDB1 dataset, with pixel-level annotations.</p><p>The results for these datasets are shown in Table <ref type="table" target="#tab_4">4</ref> for the DR screening experiment, and in Table <ref type="table" target="#tab_5">5</ref> for the DR need for referral experiment. The results are presented in terms of AUC and Se at a fixed Sp of 50%, as performed by other authors <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b21">[21]</ref>. The metrics distributions can be seen in Figure <ref type="figure" target="#fig_7">7</ref>. From Table <ref type="table" target="#tab_5">5</ref>, we observe that our model provides a Se superior to 80% in all of the databases. Given that the international guidelines recommend a Se ranging from 60% to 80% for DR screening <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, our results suggest that our approach can be used in a real scenario for DR screening, with the ability to significantly reduce the burden on experts in triage programs.   From Figure <ref type="figure" target="#fig_8">8</ref>, we observe rather small CIs for all databases, except IDRiD and DIARETDB0. This is owing to their small number of test images, as described in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with Other Works</head><p>To compare our approach with other works, two datasets were employed: Messidor, and Messidor-2. We essentially considered works that tested their results on Messidor or Messidor-2 datasets while training their models on a different dataset (cross-dataset validation), as it corresponds to our protocol. We also considered the performance of 2 experts. The results are shown in Table <ref type="table" target="#tab_6">6</ref>.</p><p>For both experiments on the Messidor dataset, our method resulted in AUC and Se values comparable or superior to those in the state-of-the-art, and even to specialists, indicating that our patch-based CNN approach can be successfully employed both to detect DR and to localize its signs.</p><p>We infer that this level of success is owing to several factors. First, the patch selection approach tends to reduce the number of false positives of the classifier, because it enhances the importance of challenging samples.</p><p>Second, when compared to Orlando's method <ref type="bibr" target="#b21">[21]</ref>, we note the superiority of learning characteristics adapted to the classification task (rather than using hand-crafted ones), which is also the conclusion of other works regarding medical images. In this study, we used a single database (DIARETDB1) for localizing the lesion. A question may arise whether using another database would improve the generalization ability, by increasing the diversity of lesion appearance.</p><p>However, a careful analysis of the few datasets which present lesion annotations led us not to pursue this direction. Indeed, the Retinopathy Online Challenge (ROC) dataset <ref type="bibr" target="#b38">[38]</ref> labels are not entirely trustworthy, because they result from the union of four specialists' annotations (and not in a grade, as done in DIABRETDB1). From the results of specialists <ref type="bibr" target="#b11">[11]</ref>, it is reasonable to affirm that because they make mistakes such as confusing artifacts with lesions, for example, and using the union of several specialists' annotations tends to increase the number of falsepositive labels, leading a model to learn incorrect features.</p><p>Another possible public dataset is e-ophta <ref type="bibr" target="#b39">[39]</ref>, but we did not use it because it does not provide hemorrhage labels, which are important markers of DR.</p><p>Finally, it seems that the choice of using only the DIARETDB1 dataset for learning was sufficient to ensure a good generalization ability with fast network convergence.</p><p>The appropriate statistical method to make a proper comparison between classifiers is a paired test <ref type="bibr" target="#b40">[40]</ref>. Understandably, making a paired test is not always possible, because the results from other approaches must be available. We could do this only with Orlando's et al. model <ref type="bibr" target="#b21">[21]</ref>, as they kindly provided their results for all images of the Messidor database.</p><p>As the distributions of the result metrics are not normal, a Wilcoxon rank-sum test was applied to the results of both classifiers on the bootstrap sample, and the results can be seen in Figure <ref type="figure" target="#fig_7">7</ref>.</p><p>Concerning the DR screening experiment, which is the most challenging one, the Wilcoxon rank test <ref type="bibr" target="#b41">[41]</ref> for AUC indicated that our approach resulted in an AUC similar to that of Orlando et al. <ref type="bibr" target="#b21">[21]</ref> ( &gt; 0.05) and a Se value greater than Orlando et al. ( ≪ 0.01). This is probably because of the patch selection procedure that we used for the final training. In the Messidor-2 dataset, our approach resulted in an AUC value of 0.944 (95% CI: 0.925 -0.966) and an Se value of 0.900 (95% CI: 0.860 -0.961) for a fixed Sp of 87%, as shown in Table <ref type="table" target="#tab_7">7</ref>. This fixed Sp point was chosen to compare our work with that of Gargeya et al. <ref type="bibr" target="#b18">[18]</ref>. Our results are comparable to Gargeya et al. <ref type="bibr" target="#b18">[18]</ref>, with a lighter training phase. Abramoff et al. <ref type="bibr" target="#b16">[16]</ref> trained with many more images (1.25 million images against 28 images in our training set), but their results in terms of Se and Sp are comparable to ours. In contrast, Gulshan et al. <ref type="bibr" target="#b42">[42]</ref> employed both a larger training set (128,175 images) and a different labeling protocol, which in turn produced better results in the Messidor-2 dataset.</p><p>Indeed, the work of Gulshan et al. explained through a performance curve that when taking only the annotations of two ophthalmologists, as we did, their performance drops by approximately 30% in terms of Se, which is smaller than ours.</p><p>From Table <ref type="table" target="#tab_7">7</ref>, we can observe that one of the advantages of the patch-based approach is that it allows for training of the CNN with only a few images with pixel-level annotations. Therefore, for DR classification, we obtain results similar to those of other works that require a massive number of graded images.</p><p>Apart from DR detection, our approach also performs lesion localization. We, therefore, designed another experiment on the DIARETDB1 test set to evaluate the performance of our lesion localization. The results are presented in Figure <ref type="figure" target="#fig_8">8</ref>. We observe that the recall is relatively low. However, because of the redundancy of lesions (there is generally more than one lesion per image), the DR screening results are good, as shown in Table <ref type="table" target="#tab_4">4</ref>.  We can observe that the localization of red lesions produced by the system can be useful for specialists to focus their attention on a limited region of the image, thereby reducing the burden of evaluating the retinal image globally. Apart from using the complete model for DR detection, the system can also be used for potential lesion detection. In that case, a lower threshold can be used to increase the model's recall of image regions that might contain signs of DR. It is important to emphasize that manual image assessment is a difficult task even for experts, as it depends on small lesion localization in retinal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>The present study is devoted to the early detection of DR in retinal images using a CNN deep network approach. In contrast to many authors who use CNN globally on the pixels of the image to produce a label, our model deals with patches and can localize potential regions of lesions, providing a powerful tool for a specialist in retinal red lesion detection, and leading to further DR detection.</p><p>Our main concern was to reduce the complexity of the model, while improving its performance. To this end, we designed an efficient procedure for selecting training patches, so that challenging examples would be given special attention during the training process. Moreover, as our aim was a crude localization of the regions rather than their precise segmentation, we under-sampled the images, leading to a drastic reduction in the processing time of generalization, which is important in a real application scenario.</p><p>The classification decision is based on a probability map indicating the level of DR of each pixel. We used a simple procedure to deduce a label function for the complete image, with the advantage of not requesting any retraining step. More precisely, our approach separates the phase of lesion localization (learned by a CNN) and DR labeling as a direct outcome of the outputs of the CNN. In this way, our model can be used immediately on any database, without any further adaptation.</p><p>Extensive experiments on several databases have shown that our approach outperforms other models tested under the same experimental conditions, namely, in a cross-validation context where the test and training databases are different.</p><p>Moreover, in contrast to the state-of-the-art, training our model requires only a small number of images, which is of high value considering the burden of labeling large quantities of images.</p><p>Future research could investigate building separate models for each type of lesion such as MAs, hemorrhages, and even bright lesions such as exudates. This would allow other databases to be used, such as e-ophta or DDR, and would allow us to make use of their large number of images. Another direction would be to consider other types of data (such as clinical data) to complement the data image to improve the recognition performance, rather than using costly specialist labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conflict of Interest</head><p>None declared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>This work was supported by FAPES (grant numbers TO041/2016 and TO093/2017), CNPq, STIC AMSUD program (grant number 88881.117609/2016-01), and Prodif-Ifes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 -</head><label>1</label><figDesc>Figure 1-Localization model based on patches. The convolutional neural network (CNN) input is a single patch of the retinal image, and, for each patch, the output is the probability of a lesion being in the analyzed patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 -</head><label>2</label><figDesc>Figure 2 -Standard Diabetic Retinopathy Database. Calibration Level 1 (DIARETDB1) For each type of evaluated lesion, the database provides a ground truth image composed by the superposition of each expert's annotation. (a) shows retinal image and ground truths, (b) shows the hemorrhages' ground truth, and (c) shows the red small dots' ground truth. The brighter the dots appear, the more the experts agree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1 https://www.kaggle.com/c/diabetic-retinopathy-detection/ 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2</head><label></label><figDesc>Figure 3 -(a) Initial retinal image of the DIARETDB1 dataset and (b) corresponding pre-processed retinal image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 -</head><label>4</label><figDesc>Figure 4-Model predicts the lesions by averaging several rotations of the patch. To generate the rotated patches, the original image is rotated around the center pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 -</head><label>5</label><figDesc>Figure 5 -Use of strides to speed up the segmentation process. Each white square in the images represents an evaluated pixel. The output map is further re-sampled to the input retinal image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 -</head><label>6</label><figDesc>Figure 6-Selection model description. The input is a retinal image patch, and the output is the probability that the input patch contains a red lesion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 -</head><label>7</label><figDesc>Figure 7-Distributions of area under the receiver's operating characteristic curve (AUC) and sensitivity for both experiments on several datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 -</head><label>8</label><figDesc>Figure 8-Performance metric of the lesion detection in the DIARETDB1 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9</head><label>9</label><figDesc>Figure 9 shows a qualitative result of lesion localization on an image from the Messidor database, i.e., a falsepositive (left) detection, and a true-positive (right) detection, as obtained by our model.We can observe that the localization of red lesions produced by the system can be useful for specialists to focus their attention on a limited region of the image, thereby reducing the burden of evaluating the retinal image globally. Apart from using the complete model for DR detection, the system can also be used for potential lesion detection. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 -</head><label>9</label><figDesc>Figure 9-Qualitative results of the red lesion localization. Both images are composed of the original retinal image with the output of our approach superimposed and the localized lesions highlighted. The regions delimited by red squares are shown in detail at the right side of the retinal image.Figure (a) shows a false-positive example, which is an intersection between blood vessels and (b) three true-positive examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,61.92,79.44,226.80,226.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,294.60,79.44,226.80,226.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 -</head><label>1</label><figDesc>Works that employ a classical image processing pipeline with different database configurations for lesion detection.</figDesc><table><row><cell>Authors</cell><cell>Year</cell><cell>Approach</cell><cell>Databases</cell><cell></cell><cell>Results</cell></row><row><cell>Larsen et al. [3]</cell><cell>2003</cell><cell>Commercial system</cell><cell>Welsh Community</cell><cell></cell><cell>Image level</cell></row><row><cell></cell><cell></cell><cell>against experts</cell><cell>Diabetic Retinopathy</cell><cell cols="2">lesion detection</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Study (WCSDR)</cell><cell></cell><cell>Specificity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(</cell><cell>= 0.714</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sensitivity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(</cell><cell>= 0.967</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Area under the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">receiver's operating</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">characteristic curve</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(</cell><cell>= 0.903</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>= 0.659</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 -</head><label>2</label><figDesc>Summary of the papers that employed deep learning for DR or lesion detection.</figDesc><table><row><cell>Authors</cell><cell cols="2">Year Approach</cell><cell>Databases</cell><cell>Results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Need for referral DR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>= 0.968 95% = 0.933 -0.988</cell></row><row><cell>Abràmoff et al. [16]</cell><cell>2016</cell><cell>Set of image-level convolutional neural networks (CNNs) followed by random forest</cell><cell>tested on Messidor2 Trained on up to 1,250,000 images, manually annotated,</cell><cell>= 0.870 95% = 0.842 -0.894</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>= 0.980 = 0.968 -0.992 95%</cell></row><row><cell>Gulshan et al. [17]</cell><cell>2016</cell><cell>An ensemble of InceptionV3 networks, trained on 100,000+ images, multiple grades per image, two images per subject</cell><cell>Private, Eye Picture Archive Communication System (EyePACS)-1, and Messidor-2</cell><cell>Need for referral DR on Messidor-2 = 0.999 95% = 0.986 -0.995</cell></row><row><cell>Gargeya et al. [18]</cell><cell>2017</cell><cell>Small image-level CNN, data augmentation using rotation, brightness, and contrast, trained on 75,000+ images</cell><cell>EyePACS-1 (train) and Messidor-2 (test)</cell><cell>Need for referral DR on Messidor-2 = 0.94 = 0.93 = 0.87</cell></row><row><cell>Ting et al.</cell><cell cols="2">2017 Ensembles of eight image-level</cell><cell>Private dataset</cell><cell>DR Screening</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 -</head><label>3</label><figDesc>Description of the datasets used in this work.</figDesc><table><row><cell>Dataset</cell><cell>Number of images/subjects</cell><cell>No DR (%)</cell><cell>DR at any level</cell><cell>Camera</cell><cell>Number of referees</cell></row><row><cell>DIARETDB0 [23]</cell><cell>130</cell><cell>20 (15.4)</cell><cell>110 (84.6)</cell><cell>Unknown</cell><cell>not informed</cell></row><row><cell>DIARETDB1 [23]</cell><cell>89</cell><cell>5 (5.6)</cell><cell>84 (94.4)</cell><cell cols="2">Unknown 4</cell></row><row><cell>Kaggle training set * 3</cell><cell>15,919</cell><cell>11,583 (73.4)</cell><cell>4,186 (26.5)</cell><cell>Various</cell><cell>1</cell></row><row><cell>Messidor [24]</cell><cell>1,200</cell><cell>546 (45.5)</cell><cell>654 (54.5)</cell><cell>Topcon</cell><cell>not informed</cell></row><row><cell>Messidor-2 * 4</cell><cell>874</cell><cell>684 (78.2)</cell><cell>190 (21.7)</cell><cell>Topcon</cell><cell>3</cell></row><row><cell>Indian Diabetic Retinopathy Image Dataset (IDRiD) test set [25]</cell><cell>103</cell><cell>34 (33)</cell><cell>69 (67)</cell><cell>Kowa</cell><cell>not informed</cell></row><row><cell>DDR [26]</cell><cell>4,105</cell><cell>1,880 (45.8)</cell><cell>2,225 (54.2)</cell><cell>Topcon</cell><cell>4</cell></row></table><note><p>* Two images per subject</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>: 1. Given the data B K , D K , B E , D E , ⋯ , B " , D " , where B C represents a retinal image, and D C the corresponding class (0 or 1): 2. M 8NN! bootstrap empirical samples (O * ) are created from the original data. 3. The classifier ( ) performance indicator P Q (AUC and mean squared error (MSE)) is calculated for each</figDesc><table><row><cell>O C  *  bootstrap empirical sample. 4. The average P Q  *  RRRR is calculated for each O  *  . 5. As the bootstrap works by approximating the variation, for each bootstrap sample, the variation S Q  *  = P Q  *  RRR -P Q RRR is calculated, where P Q</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 -</head><label>4</label><figDesc>Results for the DR screening experiment on several datasets.</figDesc><table><row><cell>DR screening</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 -</head><label>5</label><figDesc>Results for the DR need for referral experiments on several datasets.</figDesc><table><row><cell>DR need for referral</cell></row></table><note><p>* Per subject evaluation</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 -</head><label>6</label><figDesc>Comparison of DR screening and need for referral using the Messidor dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">DR screening</cell><cell cols="2">DR need for referral</cell></row><row><cell></cell><cell>AUC (95% CI)</cell><cell>Se (95% CI)</cell><cell>AUC (95% CI)</cell><cell>Se (95% CI)</cell></row><row><cell>Experts</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Expert A [11]</cell><cell>.922 (.902-.936)</cell><cell>.945</cell><cell>.940</cell><cell>.982</cell></row><row><cell>Expert B [11]</cell><cell>.865 (.789-.925)</cell><cell>.912</cell><cell>.920</cell><cell>.976</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 -</head><label>7</label><figDesc>Comparison of referable DR detection using the Messidor-2 dataset. Our approach is the only one that employs a patch-based CNN.</figDesc><table><row><cell>Method</cell><cell cols="2">AUC (95% CI) Se (95% CI)</cell><cell>Sp (95% CI)</cell><cell>Number of training images</cell></row><row><cell>Abràmoff, et al. [16]</cell><cell cols="4">.980 (.968-.992) .968 (.933-.988) .870 (.842-.894) 1,250,000</cell></row><row><cell>Gulshan, et al. [42]</cell><cell cols="4">.990 (.986-.995) .961 (.924-.983) .939 (.924-.953) 128,175</cell></row><row><cell>Gargeya, et al. [18]</cell><cell>.940</cell><cell>.930</cell><cell>.87</cell><cell>75,137</cell></row><row><cell>This work</cell><cell cols="3">.944 (.925-.966) .900 (.860-.961) .87 (.863-.871)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>https://medicine.uiowa.edu/eye/abramoff</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>* Per subject evaluation</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HIGHLIGHTS HIGHLIGHTS HIGHLIGHTS HIGHLIGHTS</head><p>3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.</p><p>• We design a fully patch-based CNN method for retinal lesion localization.</p><p>• The strides usage accelerates the lesion localization up to a factor of 25.</p><p>• The sample selection method helps the model to focus on challenging samples.</p><p>• The model has been trained using only 28 retinal images with pixel-level labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evandro Ottoni Teatini Salles</head><p>Bernadette Dorizzi</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Comput. Comput. Assist. Interv</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="411" to="418" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Pt 2</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated detection of fundus photographic red lesions in diabetic retinopathy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investig. Ophthalmol. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="761" to="766" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic detection of red lesions in digital color fundus photographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S A M S A M S A</forename><surname>Suttorp-Schulten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="584" to="592" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Red lesions detection in digital fundus images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 15th IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2932" to="2935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical detection of red lesions in retinal images by multiscale correlation filtering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
		<idno>72601L-72601L-12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2009-02">Feb. 2009</date>
			<biblScope unit="volume">7260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detection of red lesions in digital fundus images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Kande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Savithri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Subbaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R M</forename><surname>Tagore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="558" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic Detection of Microaneurysms and Hemorrhages in Digital Fundus Images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Kande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Savithri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Subbaiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="430" to="437" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detection of microaneurysms using multi-scale correlation coefficients</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2237" to="2248" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal filter framework for automated, instantaneous detection of lesions in retinal images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="523" to="533" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluation of a Computer-Aided Diagnosis System for Diabetic Retinopathy Screening on Public Data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dumitrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S A A</forename><surname>Suttorp-Schulten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investig. Opthalmology Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">4866</biblScope>
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Points of Interest and Visual Dictionaries for Automatic Retinal Lesion Detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2244" to="2253" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving microaneurysm detection using an optimally selected subset of candidate extractors and preprocessing methods</title>
		<author>
			<persName><forename type="first">B</forename><surname>Antal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hajdu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="264" to="270" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exudate detection in color retinal images for mass screening of diabetic retinopathy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1026" to="1043" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Red Lesion Detection Using Dynamic Shape Features for Diabetic Retinopathy Screening</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hurtut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chelbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M P</forename><surname>Langlois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1116" to="1126" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved Automated Detection of Diabetic Retinopathy on a Publicly Available Dataset Through Integration of Deep Learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D M D</forename><surname>Abràmoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investig. Opthalmology Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">5200</biblScope>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page">2402</biblScope>
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated Identification of Diabetic Retinopathy Using Deep Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gargeya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="962" to="969" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S W</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA -J. Am. Med. Assoc</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2211" to="2223" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep image mining for diabetic retinopathy screening</title>
		<author>
			<persName><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Charrière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="178" to="193" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An ensemble deep learning based approach for red lesion detection in fundus images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Prokofyeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Del Fresno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="115" to="127" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microaneurysm detection using fully convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chudzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Calivá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Al-Diri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="185" to="192" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">the DIARETDB1 diabetic retinopathy database and evaluation protocol</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kauppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procedings of the British Machine Vision Conference</title>
		<meeting>edings of the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="15" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feedback on a publicly distributed image database: The Messidor database</title>
		<author>
			<persName><forename type="first">E</forename><surname>Decencière</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Anal. Stereol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="234" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Indian Diabetic Retinopathy Image Dataset (IDRiD): A Database for Diabetic Retinopathy Screening Research</title>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2018-07">Jul. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diagnostic assessment of deep learning algorithms for diabetic retinopathy screening</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci. (Ny)</title>
		<imprint>
			<biblScope unit="volume">501</biblScope>
			<biblScope unit="page" from="511" to="522" />
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DIARETDB0 : Evaluation Database and Methodology for Diabetic Retinopathy Algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kauppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kalesnykiene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lensu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Pattern Recognit. Res. Group, Lappeenranta Univ. Technol. Finland</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Proposed international clinical diabetic retinopathy and diabetic macular edema disease severity scales</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast Convolutional Neural Network Training Using Selective Data Sampling: Application to Hemorrhage Detection in Color Fundus Images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J J P J P</forename><surname>Van Grinsven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Hoyng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1273" to="1284" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Comparing the Areas under Two or More Correlated Receiver Operating Characteristic Curves: A Nonparametric Approach</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Clarke-Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">837</biblScope>
			<date type="published" when="1988-09">Sep. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Backpropagation Applied to Handwritten Zip Code Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Classification of diabetic retinopathy and diabetic macular edema</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fernandez-Loaiza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sauma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hernandez-Bogantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Masis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World J. Diabetes</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">290</biblScope>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">International Council of Ophthalmology Guidelines for Glaucoma Eye Care</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>International Council of Ophthalmology</publisher>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Diabetic retinopathy management guidelines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Keeffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Rev. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="417" to="439" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Diabetic eye screening: commission and provide</title>
		<author>
			<persName><forename type="first">Royal</forename><surname>College</surname></persName>
		</author>
		<author>
			<persName><surname>Opthalmology</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Validation of microaneurysmbased diabetic retinopathy screening across retina fundus datasets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Giancardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Karnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th IEEE International Symposium on Computer-Based Medical Systems</title>
		<meeting>the 26th IEEE International Symposium on Computer-Based Medical Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="125" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Retinopathy Online Challenge: Automatic Detection of Microaneurysms in Digital Color Fundus Photographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="195" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">TeleOphta: Machine learning and image processing methods for teleophthalmology</title>
		<author>
			<persName><forename type="first">E</forename><surname>Decencière</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRBM</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="203" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evaluating and comparing classifiers: Review, some recommendations and limitations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Stapor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Systems and Computing</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Kurzynski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Wozniak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Burduk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">578</biblScope>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Individual Comparisons by Ranking Methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics Bull</title>
		<imprint>
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA -J. Am. Med. Assoc</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
