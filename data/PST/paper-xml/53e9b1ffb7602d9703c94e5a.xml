<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A color image segmentation approach for content-based image retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mustafa</forename><surname>Ozden</surname></persName>
							<email>mozden@kku.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Electronics Engineering Department</orgName>
								<orgName type="institution">Kirikkale University</orgName>
								<address>
									<postCode>71450</postCode>
									<settlement>Kirikkale</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ediz</forename><surname>Polat</surname></persName>
							<email>polat@kku.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Electronics Engineering Department</orgName>
								<orgName type="institution">Kirikkale University</orgName>
								<address>
									<postCode>71450</postCode>
									<settlement>Kirikkale</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A color image segmentation approach for content-based image retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FF6E05D84D17C9A8BE9FDFF389E0DBE2</idno>
					<idno type="DOI">10.1016/j.patcog.2006.08.013</idno>
					<note type="submission">Received 28 October 2005; received in revised form 11 July 2006; accepted 15 August 2006</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image segmentation</term>
					<term>Mean-shift algorithm</term>
					<term>Discrete wavelet frames</term>
					<term>Texture segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a new color image segmentation method based on low-level features including color, texture and spatial information. The mean-shift algorithm with color and spatial information in color image segmentation is in general successful, however, in some cases, the color and spatial information are not sufficient for superior segmentation. The proposed method addresses this problem and employs texture descriptors as an additional feature. The method uses wavelet frames that provide translation invariant texture analysis. The method integrates additional texture feature to the color and spatial space of standard mean-shift segmentation algorithm. The new algorithm with high dimensional extended feature space provides better results than standard mean-shift segmentation algorithm as shown in experimental results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image segmentation is an important step for many image processing and computer vision algorithms. The interest is motivated by applications over a wide spectrum of topics. For example, analyzing different regions of an aerial photo is useful for understanding plant/land distribution. Extracting an object of interest from background of an image is important for building intelligent machines for factory automation systems. Segmenting and counting blood cells from cell images can help hematologists to improve diagnosis of diseases. Scene segmentation is also helpful to retrieve images from large image databases for content-based image retrieval systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Most of the methods require using image features that characterize the regions to be segmented. Particularly, texture and color have been independently and extensively used in the area. Similar image segmentation algorithms have been developed in Refs. <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, where texture is used as a main descriptor and wavelet frames are employed for feature extraction. However, it is also mentioned that using only texture descriptor cannot be sufficient for successful texture segmentation. In Ref. <ref type="bibr" target="#b5">[6]</ref>, color and spatial information are used in a nonparametric probabilistic framework where the mean-shift algorithm is employed for feature space analysis. The methods combining multiple image features in a probabilistic framework remain limited and active. Some of the work that employs different image features can be found in Refs. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> where color and texture features are used together for segmentation. For texture segmentation, Bayes network-based algorithms provide better results comparing to other algorithms especially in case of having unknown texture types <ref type="bibr" target="#b10">[11]</ref>. Other similar texture segmentation algorithms that employ multiple features can be found in Refs. <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. All of the algorithms developed in these works consider relatively low-dimensional feature spaces which are computationally inexpensive, however, relatively less robust.</p><p>This paper considers the segmentation problem of image regions based on color, texture and spatial information in a nonparametric framework. The proposed method uses discrete wavelet frames (DWF) <ref type="bibr" target="#b2">[3]</ref> to characterize textured regions in images. DWF decomposition of a textured region provides a translation invariant texture description which results in better estimation and more detailed texture characterization at region boundaries. The color and spatial feature space of the mean-shift algorithm <ref type="bibr" target="#b5">[6]</ref> is then extended using these texture characteristics to create higher dimensional feature space which results in improved segmentation.</p><p>The rest of the paper is organized as follows. In Section 2, the brief description of texture characterization using wavelet frames are presented. Subsequently in Section 3, building the higher dimensional feature space using spatial, color and texture information and the mean-shift filtering based on this feature space is described. The experimental results are given in Section 4. Finally Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Wavelet frames for texture characterization</head><p>In this paper, the DWF decomposition, a variation of the discrete wavelet transform, is used for texture characterization. A filter bank is employed for decomposing the image into orthogonal components which simplify the classification problem. Unlike other decompositions, DWF is computationally inexpensive for the evaluation of low-frequency components. Dissimilar to other wavelet-based approaches, the output of the filter banks is not sub-sampled in DWF decomposition between levels. This provides translation invariant texture description of input signal. This property yields a better estimation of texture statistics and more detailed characterization at region boundaries. DWF decomposition can be calculated by successive 1-D processing along the rows and columns of the image using initial low-pass (h i ) and high-pass (g i ) filters which are expanded in every iteration i. The expansion can be achieved iteratively in the z and signal domains as follows <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_0">H i+1 (z) = H (z 2 i )H i (z), h i+1 (k) = [h] ↑2 i * h i (k), G i+1 (z) = G(z 2 i )H i (z), g i+1 (k) = [g] ↑2 i * h i (k),<label>(1)</label></formula><p>where the notation [.] ↑m indicates the up-sampling by a factor of m. These expanded filters can be used to decompose a signal in sub-bands approximately one octave each. In order to construct orthogonal wavelet decomposition, the following discrete normalized wavelet basis functions can be used:</p><formula xml:id="formula_1">i,t (k) = 2 i/2 h i (k -2 i t), i,t (k) = 2 i/2 g i (k -2 i t),<label>(2)</label></formula><p>where i and t are the scale and translation indices, respectively. A fast and iterative implementation of the decomposition can be achieved as follows:</p><formula xml:id="formula_2">s i+1 (k) = [h] ↑2 i * s i (k), d i+1 (k) = [g] ↑2 i * s i (k), (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where s i and d i are the signal expansion coefficients and wavelet coefficients, respectively. A block diagram of iterative DWF decomposition of a 1-D signal is presented in Fig. <ref type="figure" target="#fig_1">1</ref> A texture is characterized by a set of median values of energy estimated in a local window at the output of the corresponding filter bank. The energy in a local window can be calculated using coefficients of DWF decompositions (LL, LH, HL, and HH) where the energy is defined as the square of the coefficients. The advantage of using median filter is that it preserves the energy associated with texture between regions. The sub-bands at the output of filter bank in Fig. <ref type="figure" target="#fig_1">1</ref>(b) correspond to approximate, horizontal, vertical and diagonal components of the input image signal <ref type="bibr" target="#b6">[7]</ref>. Due to the fact that most of the texture information are contained in LH and HL sub-bands, we used only these decomposition coefficients to obtain texture features. A pixel in textured region can be classified into one of four texture categories based on texture orientation. These are smooth (not enough energy in any orientation), vertical (dominant energy in vertical direction), horizontal (dominant energy in horizontal direction), and complex (no dominant orientation). Texture feature extraction consists of two steps. First, the energy of LH and HL sub-bands are classified into two categories (0 and 1) using K-means clustering algorithm. Second, a further classification is made using combination of two categories in each sub-band LH and HL. A pixel is classified as smooth if its category is 0 in both LH and HL sub-bands. A pixel is classified vertical if its category is 0 in LH, and 1 in HL sub-bands. Similarly, a pixel is classified horizontal if its category is 1 in LH, and 0 in HL sub-bands. Finally, a pixel is classified as complex if its category is 1 in both LH and HL sub-bands. An input image with different texture regions as well as the classification results is illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. In Fig. <ref type="figure" target="#fig_2">2(a)</ref>, four regions with different Brodatz textures are shown. In Fig. <ref type="figure" target="#fig_2">2(b</ref>), the regions are classified based on their energy and orientation. The leftmost region in the original image is classified as horizontal, the rightmost region is classified as vertical, and the middle top region is classified as smooth, finally the middle bottom region is classified as complex based on their texture characteristics. The spatial accuracy in region boundaries depends on the texture window size. The window size should be large enough to capture texture characteristics. On the other hand, excessively large window size can cause border effects. The goal is to characterize image pixels using these texture features and use them to extent the mean-shift feature space to obtain better segmentation. The details of  extending mean shift feature space is given in the following section.</p><formula xml:id="formula_4">G(z 2 ) G(z 4 ) H(z) H(z 2 ) G(z) G(z 2 I ) H(z 2 I ) d 1 (k) d 2 (k) d 3 (k) d i (k) s i (k) x(k)=s 0 (k) L H L H L H LL LH HL HH I Rows Columns s 1 (k) s 2 (k) (a) (b) s i-1 (k)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mean-shift filtering in higher dimensional space and segmentation</head><p>Given n data points x i , i = 1, . . . , n in the d-dimensional space R d , the kernel density estimation at the location x can be calculated by</p><formula xml:id="formula_5">fK (x) = 1 n n i=1 1 h d i k x -x i h i 2 ,<label>(4)</label></formula><p>with the bandwidth parameter h i &gt; 0. The kernel K is a spherically symmetric kernel with bounded support satisfying <ref type="bibr" target="#b15">[16]</ref> </p><formula xml:id="formula_6">K(x) = c k,d k( x 2 ) &gt; 0, x 1,<label>(5)</label></formula><p>where the normalization constant c k,d assures that K(x) integrates to one. The function k(x) is called the profile of the kernel. Assuming derivative of the kernel profile k(x) exists, using g(x) = -k (x) as the profile, the kernel G(x) is defined as</p><formula xml:id="formula_7">G(x) = c g,d g( x 2</formula><p>). The following property can be proven by taking the gradient of Eq. ( <ref type="formula" target="#formula_5">4</ref>) as follows:</p><formula xml:id="formula_8">m G (x) = C ∇f K (x) fG (x) , (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where C is a positive constant and, it shows that, at location x, the mean-shift vector computed with kernel G is proportional to the normalized density gradient estimate obtained with kernel K. The mean-shift vector is defined as follows:</p><formula xml:id="formula_10">m G (x) = n i=1 1 h d+2 i x i g x -x i h i 2 n i=1 1 h d+2 i g x -x i h i 2 -x. (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>The mean-shift vector thus points toward the direction of maximum increase in the density. The mean-shift procedure is obtained by successive computation of the mean-shift vector and translation of the kernel G(x) by the mean-shift vector. At the end of the procedure, it is guaranteed to converge at a nearby point where the estimate has zero gradient <ref type="bibr" target="#b16">[17]</ref>.</p><p>In other words, it is a hill climbing technique to the nearest stationary point of the density. The iterative equation is  given by</p><formula xml:id="formula_12">y j +1 = n i=1 x i h d+2 i g y j -x i h i 2 n i=1 1 h d+2 i g y j -x i h i 2 , j = 1, 2, . . . . (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>The initial position of the kernel (starting point to calculate y 1 ) can be chosen as one of the data points x i . Usually, the modes (local maxima) of the density are the convergence points of the iterative procedure. The mean-shift image segmentation algorithm <ref type="bibr" target="#b5">[6]</ref> considers a joint domain representation that includes spatial and range (color) domains. An image is represented as a twodimensional lattice where the space of the lattice is known as spatial domain, and the gray-level or color information is represented in the range domain. Every pixel in the image can be considered as a p-dimensional vectors where p = 1 in gray-level case and p = 3 for color images. The dimension of joint domain representation becomes d = p + 2. Using this representation, the mean-shift filtering is performed to smooth the image and to preserve the region boundaries based on color and spatial information. However, in cases where colors in region boundaries are similar, this representation will not be sufficient and additional features are needed for more robust segmentation. In this paper, we addressed this problem and thus we extended the mean-shift feature space by integrating texture descriptors to improve segmentation. The details of obtaining texture descriptors are explained in the previous section. The block diagram of proposed method is shown in Fig. <ref type="figure" target="#fig_3">3</ref>. The steps can be explained as follows:</p><p>1. Use wavelet transformation to decompose the image into sub-bands (LL, LH, HL, and HH). The DWF decompositions are the same as these sub-bands except that there is no sub-sampling. Most of the texture information are in the LH and HL sub-bands.</p><p>2. Calculate the median energy using coefficients of LH and HL sub-bands in a local window. The size of the window should be large enough to capture the local texture characteristics. The energy is defined as the square of the coefficients, and used for texture characterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Use K-means clustering algorithm to classify the energy</head><p>values in two classes for each sub-band. There will be four texture classes based on energy: smooth (not enough energy in any orientation), vertical (dominant energy in vertical orientation), horizontal (dominant energy in horizontal orientation), and complex (no dominant orientation). 4. Generate the feature vector such that every pixel in the image has p-dimensional feature vector which includes spatial (x, y), color (gray-level or L * u * v values) and texture (smooth, vertical, horizontal or complex) information. 5. Filter the image using mean-shift algorithm in higher dimensional feature space which includes spatial, color and texture information. The filtering operation can be controlled by setting the spatial window radius (h s ) and color range (h r ). The filter output (convergence point in mean shift algorithm) is determined by color as well as texture information unlike in standard meanshift filtering. This provides better discrimination between regions where colors are similar but texture is different. 6. The output image can be segmented using K-means clustering algorithm.</p><p>The results will be given in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The objective of the proposed algorithm is to achieve robust and accurate segmentation in images. The standard mean-shift filtering algorithm with color and spatial information are not sufficient for superior segmentation in cases  where only color feature does not provide essential discriminative information. The proposed method addresses this problem and employs texture descriptors as an addi-tional feature in a higher-dimensional feature space using mean-shift framework. The method uses wavelet frames that provide translation invariant texture analysis. This property Fig. <ref type="figure" target="#fig_7">6</ref>. Image segmentation results using proposed approach. Left images: original images. Middle images: filtered images. Right images: edges are superimposed on original images. yields a better estimation of texture statistics and more detailed characterization at region boundaries. The method integrates additional texture feature to the color and spatial space of standard mean-shift filtering algorithm. The new algorithm with high dimensional extended feature space provides better results than standard algorithm as shown in experimental results. To demonstrate the performance of the algorithm, we experimented with a number of natural images. To illustrate the accuracy of the proposed algorithm, an image with trees, grass, river and a house is used as shown in  the results, a three-dimensional graphical demonstration is also shown in Fig. <ref type="figure" target="#fig_5">5</ref>. The first row shows the original images marked with two different region of interest. The data in these windows are represented with (x, y) image coordinates and gray-level values in three-dimension as shown in second and third rows of Fig. <ref type="figure" target="#fig_5">5</ref>. The integration of texture information to the mean-shift feature space provided more improved results particularly between regions due to the fact that the proposed approach contributes better estimation and more detailed texture characterization at region boundaries. More results are shown in Fig. <ref type="figure" target="#fig_7">6</ref>. The original images are shown on the left, filtered images are shown in the middle and the segmented images are shown on the right columns. The regions smaller then a threshold were removed for better visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative analysis of results</head><p>In order to provide quantitative support and evaluate the performance of the proposed algorithm, the number of misclassified pixels in a region of interest is used. A misclassified pixel in a region can be defined as a pixel which does not belong to region of interest. The number of such pixels provides a performance measure for the algorithm. The segmentation/classification process depends on both the quality of the filtered image and the segmenta- </p><p>The quantitative results indicate that the inclusion of texture feature to the mean-shift filtering algorithm reduces the number of misclassified pixels and thus helps to improve the segmentation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we presented a new approach for image segmentation based on low-level features including color, texture and spatial information. The proposed approach is based on extending the feature space for filtering in meanshift algorithm. The proposed method uses discrete wavelet frames (DWF) to characterize textured regions into predefined texture classes based on texture energy in different orientations. DWF decomposition of a textured region provides a translation invariant texture description which results in better estimation and more detailed texture characterization at region boundaries. This texture description is then fused into the mean-shift filtering framework to create higher dimensional feature space. The performance of the proposed approach has been demonstrated using natural images where color and texture information are available. The filtering with extended feature space provides satisfactory results particularly between regions. This results in more accurate segmentation process. The algorithm has also been compared to the standard mean-shift filtering. The results indicate that the proposed approach is more robust and accurate than standard mean-shift filtering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a). A block diagram of one-level DWF decomposition of 2-D image is also shown in Fig.1(b)where L and H correspond to low-pass and high-pass filters, respectively. Rows and columns of the image are processed separately using filters. The output of the filter bank is organized into the N-component vector where N is the number of sub-bands (N = 4 in Fig.1(b) for one-level decomposition). Each vector contains coefficients which represent approximate, horizontal, vertical and diagonal characteristics of input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of discrete wavelet frame decomposition: (a) iterative implementation for 1-D signal; (b) one-level decomposition of 2-D image using low-pass (L) and high-pass (H) filters. The input image is decomposed into four sub-bands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of classifying different textured regions: (a) a textured image containing vertical, horizontal, smooth and complex textures created using Brodatz textures; (b) classification result using wavelet frame decomposition with median energy in a local window.</figDesc><graphic coords="3,61.94,303.34,477.72,63.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The comparison of filtering results: (a) original image; (b) mean shift filtering result. Note that the region transitions are blurred; (c) texture supported filtering result. Note that the region boundaries are well-separated.</figDesc><graphic coords="5,121.94,70.98,357.84,100.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Three-dimensional visualization of results. First row: original images with two different windows that the both standard mean shift and proposed filtering are applied. Second and third row: the proposed method provides well-separated regions which result in better segmentation.</figDesc><graphic coords="5,49.94,223.20,501.84,423.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 (</head><label>4</label><figDesc>Fig. 4(a). The standard (color-based) mean-shift filtering result is shown in Fig. 4(b) as a comparison. The filtering result using proposed algorithm is also shown in Fig. 4(c). It can be easily seen in Fig. 4(b) that the region transitions between trees and grass are smoothed and blurred which negatively affect robust segmentation of regions. More improved and separated regions can be obtained using proposed algorithm as shown in Fig. 4(c). The accuracy of the filtering output can be adjusted by changing the size of the texture window, color range and spatial window. For better visualization of</figDesc><graphic coords="6,115.87,71.58,357.12,492.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (continued).</figDesc><graphic coords="7,122.44,71.66,357.48,308.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Comparison of number of misclassified pixels using mean shift and proposed algorithms On the other hand, instead of using well-known data classification methods, it is also possible to develop a specific segmentation algorithm for this purpose. Table1shows the quantitative comparison of results between mean shift and proposed algorithm in terms of number of misclassified pixels. The error is calculated using the following</figDesc><table><row><cell>Image</cell><cell cols="2">Number of misclassified pixels</cell><cell>Error (%)</cell><cell></cell></row><row><cell></cell><cell cols="2">Mean shift Proposed</cell><cell cols="2">Mean shift Proposed</cell></row><row><cell></cell><cell></cell><cell>algorithm</cell><cell></cell><cell>algorithm</cell></row><row><cell cols="2">Fig. 2(a) 3988</cell><cell>2891</cell><cell>13.00</cell><cell>9.60</cell></row><row><cell>Fig. 6(a)</cell><cell>542</cell><cell>340</cell><cell>9.40</cell><cell>5.90</cell></row><row><cell>Fig. 6(b)</cell><cell>250</cell><cell>123</cell><cell>9.16</cell><cell>4.50</cell></row><row><cell>Fig. 6(c)</cell><cell>360</cell><cell>270</cell><cell>17.02</cell><cell>12.70</cell></row><row><cell>Fig. 6(d)</cell><cell>55</cell><cell>50</cell><cell>0.85</cell><cell>0.77</cell></row><row><cell cols="2">Fig. 6(e) 5548</cell><cell>1360</cell><cell>33.40</cell><cell>8.20</cell></row><row><cell>Fig. 6(g)</cell><cell>155</cell><cell>155</cell><cell>3.67</cell><cell>3.67</cell></row><row><cell>Fig. 6(h)</cell><cell>592</cell><cell>30</cell><cell>7.50</cell><cell>0.68</cell></row><row><cell cols="5">tion/classification algorithm itself. In this paper, the goal is</cell></row><row><cell cols="5">to improve the filtered image such that better segmentation</cell></row><row><cell cols="5">results can be obtained using well-known classification al-</cell></row><row><cell>gorithms.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image retrieval: current techniques, promising directions and open issues</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Commun. Image Representation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="39" to="62" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contentbased image retrieval at the end of early years</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1349" to="1379" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Texture classification and segmentation using wavelet frames</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1549" to="1560" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Texture segmentation using filters with optimized energy separation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Randen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Husoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="571" to="582" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical texture characterization from discrete wavelet representations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Wouver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scheunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Dyck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="592" to="598" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mean shift: a robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comanicui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image segmentation by spatially adaptive color and texture features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mojsilovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rogowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing (ICIP&apos;03)</title>
		<meeting>IEEE International Conference on Image Processing (ICIP&apos;03)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of color texture regions in images and video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="800" to="810" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Color and texture based image segmentation using EM and its application to content based image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="675" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Color and texture image retrieval using chromaticity histograms and wavelet frames</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liapis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tziritas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Haindl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Pattern Recognition</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1451</biblScope>
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Markov random field models for unsupervised segmentation of textured color images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Panjwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="939" to="954" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic relaxation on partitions with connected components and its applications to image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="619" to="639" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic watershed segmentation of randomly textured color images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shafarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1530" to="1544" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kernel</forename><surname>Smoothing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mean shift, mode seeking and clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">He was a member of the Penn State Computer Vision group where he worked in developing vision-based human-computer interaction systems funded by National Science Foundation and Department of Defence. He is currently an Assistant Professor in Electrical and Electronics Engineering Department at Kirikkale University, Kirikkale, Turkey. His research interests include computer vision particularly detection and visual tracking of objects in image sequences</title>
	</analytic>
	<monogr>
		<title level="m">About the Author-MUSTAFA OZDEN received the B.S. and M.S. degree from Electrical and Electronics Engineering Department at Kirikkale University, Turkey in 2001 and in 2005, respectively. He is currently a Ph.D. student in</title>
		<imprint/>
		<respStmt>
			<orgName>D. degree from Pennsylvania State University, University Park ; Electrical and Electronics Engineering Department at Gazi University</orgName>
		</respStmt>
	</monogr>
	<note>His main research area consists of computer vision and image processing particularly segmentation and tracking of objects in video sequences</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
