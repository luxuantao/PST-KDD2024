<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Clustering aggregation by probability accumulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunyu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Clustering aggregation by probability accumulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BCE5EC83484E6501B493CD62FFA38D73</idno>
					<idno type="DOI">10.1016/j.patcog.2008.09.013</idno>
					<note type="submission">Received 11 May 2007 Received in revised form 15 September 2008 Accepted 21 September 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Clustering aggregation Evidence accumulation Probability accumulation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since a large number of clustering algorithms exist, aggregating different clustered partitions into a single consolidated one to obtain better results has become an important problem. In Fred and Jain's evidence accumulation algorithm, they construct a co-association matrix on original partition labels, and then apply minimum spanning tree to this matrix for the combined clustering. In this paper, we will propose a novel clustering aggregation scheme, probability accumulation. In this algorithm, the construction of correlation matrices takes the cluster sizes of original clusterings into consideration. An alternate improved algorithm with additional pre-and post-processing is also proposed. Experimental results on both synthetic and real data-sets show that the proposed algorithms perform better than evidence accumulation, as well as some other methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As an important approach of unsupervised learning, clustering has been playing a more and more indispensable role in knowledge discovery, with application to numerous fields. These include data mining [1], information retrieval <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b4">3]</ref>, image segmentation <ref type="bibr" target="#b5">[4]</ref>, machine learning, and so on. Informally, clustering can be defined as the problem of partitioning samples into different clusters, so that the samples in the same cluster are more similar than those in different clusters.</p><p>A large number of clustering algorithms exist <ref type="bibr" target="#b6">[5]</ref><ref type="bibr" target="#b7">[6]</ref><ref type="bibr" target="#b8">[7]</ref>, yet no single one can handle all types of cluster shapes and structures. Additionally, there are still some problems with conventional algorithms: on one hand, results depend much on the parameters and initializations, such as the times of iterations in K-means and the threshold in hierarchical clustering; on the other hand, most algorithms cannot automatically determine how many clusters the data-set should be partitioned into. For these reasons, partitions given by different algorithms might be greatly different. Determining which one of them should be trusted becomes a difficult problem.</p><p>Inspired by sensor fusion and classifier combination <ref type="bibr" target="#b9">[8]</ref>, and for the sake of improving the robustness and quality of clustering solutions, recent research is increasingly focusing on combining multiple partitions into a single one, i.e., clustering aggregation. This process can be simply described as given a data-set with multiple partitions, the manipulation will combine the inputs to get a final partition.</p><p>0031-3203/$ -see front matter © 2008 Elsevier Ltd. All rights reserved. doi:10.1016/j.patcog.2008.09.013 Some work has been done in this new field: Fred and Jain introduced the so-called evidence accumulation (EA), which is based on a co-association matrix <ref type="bibr" target="#b10">[9]</ref><ref type="bibr" target="#b11">[10]</ref><ref type="bibr" target="#b12">[11]</ref>. Strehl and Ghosh's hypergraph partitioning proposed a new hypergraph cuts approach <ref type="bibr" target="#b13">[12]</ref>, and Fern and Brodley developed this algorithm further, designing their hybrid bipartite graph formulation (HBGF) <ref type="bibr" target="#b14">[13]</ref>. Mutual information algorithm (QMI) <ref type="bibr" target="#b15">[14]</ref> and voting approach <ref type="bibr" target="#b16">[15]</ref> have also been put forward recently to solve the problem of clustering aggregation. Among these algorithms, EA has proven to be most effective <ref type="bibr" target="#b10">[9]</ref>.</p><p>In EA, each given partition is mapped into a n × n 0-1 symmetric matrix, where n is the number of samples in the data-set. These matrices can be regarded as component matrices. In these matrices, the value 1 denotes that the corresponding data pair are partitioned into the same cluster, while 0 denotes that they are divided into different clusters. The mean of all the component matrices is defined as the co-association matrix. Then, minimum spanning tree (MST) <ref type="bibr" target="#b17">[16]</ref> with single link (SL) or average link (AL) <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b7">6]</ref> is applied to the coassociation matrix to obtain the combined final partition. Moreover, Fred and Jain proposed a highest lifetime criterion to determine the number of clusters. As a result, this algorithm can determine a sole combined clustering out of the original partitions labels.</p><p>The co-association matrix provides the pairwise correlations by simple counting. This kind of matrix refers only to the partition labels, and it identically define the correlation of every pair from the same cluster as 1. Thus it does not take clusters' other characteristics into account, such as their sizes.</p><p>In this paper, we propose a novel clustering aggregation algorithm: probability accumulation (PA). For each given partition, PA takes the size of each cluster, as well as the dimension of samples into consideration, so that elements in the component matrix are continuous values rather than binary values. As a result, the proposed algorithm can measure the pairwise correlations in higher resolution. The overall procedure of our algorithm is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, in which we name the mean of all new component matrices as p-association matrix instead of co-association matrix in EA. Both theoretical analysis and experimental results illustrate that our algorithm outperforms (EA).</p><p>The rest of this paper is structured as follows: In Section 2 we will introduce the proposed PA. Starting from an ideal formula, the algorithm is simplified on two different levels to deduce its practical form. Moreover, we also propose a pre/post-processing step to deal with data-sets that do not satisfy the assumption. In Section 3, experiments on synthetic and real data-sets are presented and analyzed. Finally, in Section 4, a conclusion is drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Probability accumulation</head><p>We begin our discussion about PA by introducing our notations: let X={x 1 , x 2 , ... , x n } be a data-set, with |X|=n, and x i is an m×1 vector. C (p) = {c (1) (x 1 ), c (2) (x 2 ), ... , c (p) (x n )} denotes the partition label from the p-th clustering method, where p=1, 2, . . . , d. There, c (p) (x i ) means that in the p-th clustering x i is partitioned into the c (p) (x i )-th cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X (p)</head><p>k denotes samples which are partitioned into the k-th cluster by C (p) . In addition, A (p) = (a ij ) n×n represents the p-component matrix corresponding to partition C (p) , where a ij denotes the correlation of the couple (x i , x j ), which will be explained in details in the rest of this section.</p><p>The process of clustering aggregation can be described as follows:</p><p>given a set of partitions C = {C (1) (X), C (2) (X), ... , C (d) (X)}, the combined partition of the data-set, P = {p 1 , p 2 , ... , p n }, is determined and output.</p><p>In this section, the deduction of PA is structured as follows: (1) We firstly assume the pairwise distances are available, and give out a general framework of PA which takes all pairwise distances of the data-set into consideration. (2) Then we relax such strict assumption, and use the average pairwise distance of a cluster to represent the distance of all pairs belong to that cluster. Here, we introduce the cluster conditional probability densities to calculate this average distance. (3) Furthermore, when only partition labels are available, we make the uniform distribution assumption on basis of maximum entropy theory and the spherical distribution assumption regarding the characters of K-means. Then we get the practical form of PA algorithm. Aside from the theoretic deduction, the pre/post-processing is introduced at the end of this section, which could help to deal with the data-sets which do not satisfy the assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">A new measurement of correlation</head><p>We mark the sample feature space as . Since all the samples have been mapped into , we can calculate any pair's distance, and use D(x i , x j ) to denote the distance between x i and x j .</p><p>The basic idea of our construction of p-component and passociation matrix is that the larger two samples' distance is, the weaker their correlation usually becomes. As a result, we define the correlation of any couple (x i , x j ) as</p><formula xml:id="formula_0">A(x i , x j ) = + D(x i , x j ) ,<label>( 1 )</label></formula><p>where is a coefficient to be determined. This kind of correlation lies in the range [0, 1]. On one extreme, when two samples are too far away from each other, we set their distance as infinite, and then their correlation becomes extremely weak, essentially 0. On the other extreme, the correlation between any individual sample and itself is the strongest, where D(x i , x i ) becomes 0, and A(x i , x i ) becomes 1. This kind of p-component matrix has given out a general framework for the description of pairwise correlations. When the details of samples or the distance matrix are available, we can apply this definition directly. If not, according to different prior information, we can simplify the formula for the p-component matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Simplified form on probability densities</head><p>Since neither samples' details nor the distance matrix are usually available, we have to relax our algorithm's requirement. In this part, our work is based on the hypothesis that in each original partition, the cluster conditional probability densities of samples in all clusters can be obtained.</p><p>Firstly, we consider the case that two samples are partitioned into the same cluster k by partition C (p) , or rather x i , x j ∈ X (p) k . We do not know the accurate value of each D(x i , x j ). However, the average distance of all the pairs can be used instead approximately. Given the sub-data-set's probability density function is given, this average distance can be calculated easily. And then, according to Eq. (1), we can get the corresponding average correlation of X (p) k . We use this value to represent the correlation between each of the two samples that both belong to</p><formula xml:id="formula_1">X (p) k . There, we let (p) k be the sub-region of supporting X (p) k . Q is set to denote a point in . The cluster conditional probability density function of X (p) k is denoted as f (p) k (Q) = p(x|x ∈ (p) k ). Assume that Q 1 and Q 2 are two points in (p) k and D(Q 1 , Q 2 ) denotes their actual distance. Additionally, let D (p) k be the average distance of all pairs in X (p)</formula><p>k . Since we have assumed that f (p) k (Q) is already known, the average distance can be calculated as below:</p><formula xml:id="formula_2">D (p) k = Q 1 ,Q 2 ∈ p k f (p) k (Q 1 )f (p) k (Q 2 )D(Q 1 , Q 2 ) dQ 1 dQ 2 .</formula><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>We replace D(x i , x j ) by</p><formula xml:id="formula_4">D (p)</formula><p>k in Eq. (1), which can then be rewritten as</p><formula xml:id="formula_5">A (p) (x i , x j ) = + D (p) k . (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>Considering that the average distance can be any continuous value, the correlation A (p) (x i , x j ) would also be a continuous value.</p><p>This definition of correlation is also applicable to other two situations: (1) As to a couple belong to two different clusters, since we have no information of their real distance, we set it to be infinite. As a result, their correlation becomes 0 now. <ref type="bibr" target="#b3">(2)</ref> As to the correlation between any sample and itself, or a cluster composed of a singular sample, the distance becomes 0 and thus the correlation becomes 1. This describes the strongest correlation in our p-component matrix. In consequence, all situations have been taken into consideration and can be unanimously measured by Eq. ( <ref type="formula" target="#formula_5">3</ref>). For clearer description about our entire construction of PAs p-component matrix, we give it out here according to different conditions</p><formula xml:id="formula_7">A (p) (x i , x j ) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ 1, i = j, 0, i j and c (p) (x i ) c (p) (x j ), + D (p) k , i j and c (p) (x i ) = c (p) (x j ) = k. (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>When the probability distributions for all the clusters in partition C (p) are given, we can calculate their corresponding p-component matrix according to Eq. ( <ref type="formula" target="#formula_7">4</ref>), and we get a simplified form of PA. All these correlations are continuous values in the area [0, 1]. Compared with the 0-1 component matrices in EA, our method can introduce more information, and therefore better performance can be achieved.</p><p>Each partition determines the sole p-component matrix for PA. Subsequently, we calculate the mean of all these matrices, similarly to EA. In EA, this average matrix is called co-association matrix. In our approach, we name it after p-association matrix. Subsequently, we apply MST to this p-association matrix with the highest lifetime criterion as Fred and Jain do in Ref. <ref type="bibr" target="#b10">[9]</ref>, and the combined partition can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Simplified form on partition labels</head><p>By introducing the cluster probability density functions instead of samples' details, we have simplified our PA algorithm. Yet in most situations, only the partition labels are available, just like the assumption in Ref. <ref type="bibr" target="#b10">[9]</ref>. In order to address this circumstance, we deduce a further simplified form of PA. Since we cannot get the real distributions, we have to estimate them. Maximum entropy theory helps us to draw the uniform distribution, and subsequently the more simplified and practical form of PA. Moreover, under a special circumstance, when each cluster is of the same size, our PA algorithm would degenerate to EA. This relationship is also discussed theoretically at the end of this part.</p><p>Uniform distribution means that the probability density through-</p><formula xml:id="formula_9">out each sub-area (p)</formula><p>k is constant. In another word, the probability of every sample x of</p><formula xml:id="formula_10">X (p) k in (p)</formula><p>k is the same. This hypothesis is ensured by maximum entropy theory.</p><p>Maximum entropy theory aims at the problem of estimating a system's model out of insufficient limitations on the system <ref type="bibr" target="#b18">[17]</ref>. It is based on the concept of information entropy, which is defined as</p><formula xml:id="formula_11">h(Y) = -p(y) log(p(y)) dy, (<label>5</label></formula><formula xml:id="formula_12">)</formula><p>where Y is a random variable and p(y) denotes its probability density function. According to maximum entropy theory, the probability density function of the random variable Y we seek is the one to maximize the value of function <ref type="bibr" target="#b6">(5)</ref>. This theory describes an approach to estimate a variable's distribution when prior knowledge is not sufficient to determine it completely. Since we hope for different clusters' distributions in the m-dimensional space while we have not any prior information rather than partition labels, we resort to maximum entropy theory.</p><p>After simple calculation and deduction, we can easily optimize the solution under such circumstance: p(y)=e -0 . As a result, we can get the conclusion: when no information about the probability density is provided, the best estimation should be the uniform distribution. In another word, the probability each sample locates at any point in the area is identical.</p><p>Under the premise we propose above, all the probability density functions have thus become constant, or rather f</p><formula xml:id="formula_13">(p) k (Q) ≡ 1/V (p) k , where V (p) k denotes the volume of the area (p) k .</formula><p>After that, another factor to be determined for a cluster's average distance is the shape of (p) k . There we make the hypothesis that every (p) k is a sphere in the space. The reason lies in the following two aspects: On one hand, when carrying out clustering aggregation, we have not any prior knowledge about each cluster's details. Under this circumstance, the sphere distribution is the most common hypothesis. On the other hand, K-means is one of the simplest and most commonly used clustering algorithm. It is oriented to minimize the squared error and therefore tend to divide samples into a series of spheres in the corresponding space <ref type="bibr" target="#b6">[5]</ref>. In Ref. <ref type="bibr" target="#b10">[9]</ref>, Fred and Jain use K-means to generate original partitions. Thus, we also decide to use K-means to produce partition labels in this study. With respect to these two reasons, we assume each (p) k to be a sphere, and set R (p) k to denote its radium.</p><p>In spheres, the distance</p><formula xml:id="formula_14">D(Q 1 , Q 2 ) is in proportion to R (p)</formula><p>k . According to Eq. ( <ref type="formula" target="#formula_3">2</ref>), the average distance D k is the dimension m, which determines the proportion and this coefficient is marked as (m). Thus, we have</p><formula xml:id="formula_15">D (p) k = (m)R (p) k .<label>( 6 )</label></formula><p>In addition, we introduce the concept of sample proportional density to describe the density of samples in each sub-area (p)</p><formula xml:id="formula_16">k . It is defined as (p) k = |X (p) k | nV (p) k .<label>( 7 )</label></formula><p>There, |X</p><p>k | denotes the number of samples that X (p) k contains. And this density describes how many percents of all the samples that unit volume of each sub-area contains on average. Since all the (p) k are spheres in m-dimensional space, according to sphere's volume formula, we can get the correlation between</p><formula xml:id="formula_18">V (p) k and R (p) k : V(R, m) = m/2 R m (1 + m/2) ,<label>( 8 )</label></formula><p>where (•) denotes the gamma function. Moreover, Eq. ( <ref type="formula" target="#formula_16">7</ref>) has described the proportional correlation between R  <ref type="formula" target="#formula_5">3</ref>), <ref type="bibr" target="#b8">(7)</ref>, and (8), we can get the expression for calculating the corresponding correlation value as</p><formula xml:id="formula_19">A (p) (x i , x j ) = + D (p) k = m n (m) (p) k m n (m) (p) k + m |X (p) k | . (<label>9</label></formula><formula xml:id="formula_20">)</formula><p>There, (m) = m/2 / (1 + m/2) denotes the coefficient completely determined by the dimension m. From this formula we can easily see that, once the data-set is given, the coefficient n, m, and (m) are determined and therefore can be regarded as constant. is constant, too. Thus, aside from these factors above, this correlation value is only subject to the number of samples, i.e., |X p k | and the sample proportional density (p) k . The former shows the contents of different sub-data-sets while the latter shows their respective densities. That means, when we consider two clusters partitioned by a clustering, if they have the same density, the one containing more samples would construct a bigger sphere in the space. Thus, the average correlation of any two samples would be weaker. Similarly, if they both contained the same number of samples and constructed two spheres of different volumes, in the smaller sphere, samples' average correlation would be stronger. The result of Eq. ( <ref type="formula" target="#formula_19">9</ref>) shows the same tendency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The |X (p)</head><p>k | can be directly extracted from the partition label C (p) , but (p) k must be given previously. When the probability densities or (p) k are available, we can apply Eq. ( <ref type="formula" target="#formula_19">9</ref>) directly. However, when they are unknown, we have to make the hypothesis that all the clusters' uniform distributions have the same probability densities, and it means that all the (p) k are of the same value (p) 0 . Additionally, for simplification, we set the coefficient by = (n (m) (p) 0 ) -1/m , which makes the numerator of Eq. ( <ref type="formula" target="#formula_19">9</ref>) equal 1. As a result, |X p k | and m become the sole factors that would affect the correlation A (p) (x i , x j ), and we get the simplified form of PA algorithm when only partition labels and basic knowledge about the data-set (dimension m) are available.</p><p>To sum up, we combine all the three situations and therefore get the entire formula to construct p-component matrix for PA:</p><formula xml:id="formula_21">A (p) (x i , x j ) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ 1, i = j, 0, i j and c (p) (x i ) c (p) (x j ), 1 1 + m |X (p) k | , i j and c (p) (x i ) = c (p) (x j ) = k. (<label>10</label></formula><formula xml:id="formula_22">)</formula><p>There, x i , x j ∈ X, i, j = 1, 2, . . . , n. And the matrix A (p) (a ij ) n×n is pcomponent matrix we calculate from the partition label C (p) . As introduced previously, this is the most important difference between EA and PA.</p><p>One special situation we want to talk about here is when a clustering partitions whose samples are divided into clusters of the same size. Under such circumstance, all the |X p k | become the same. When two samples are in the same cluster, their correlation is (1</p><formula xml:id="formula_23">+ |X p k |) -1 ;</formula><p>otherwise, it is 0. Now, our p-association matrix has only three different values: 0, 1, and (1 + |X p k |) -1 . Since the diagonal of the matrix would not affect the result of MST, our p-association matrix actually becomes a binary one. It means that, in this special situation, our PA algorithm degenerates to EA. Our experimental results in the next section also verify this relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">The overall procedure</head><p>After calculating all the p-component matrices from the given partition labels, we get p-association matrix by calculating their mean, as EA does to obtain the co-association matrix. Apply MST to this p-association matrix with Fred and Jain's highest lifetime criterion <ref type="bibr" target="#b10">[9]</ref>, we can get the combined clustering in the end. The overall procedure of PA summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>This procedure has represented the basic framework of our algorithm. Both EA and PA demand for partition labels and try to deduce the combined result based only on them. Compared with the former, our algorithm needs only one more input, the data-set's dimension. This factor is easy to obtain in real applications. Moreover, when </p><formula xml:id="formula_24">C (p) → A (p) , p = 1, 2, . . . , d A (p) (xi, xj) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ 1, i = j 0, i j and c (p) (xi) c (p) (xj) 1 1 + m |X (p) k | , i j and c (p) (xi) = c (p) (xj) = k 2.</formula><p>Calculate the p_association matrix:</p><formula xml:id="formula_25">p_association = d p=1 A (p)</formula><p>3. Apply MST with highest lifetime to p_association Output: P-the combined clustering constructing the p-component matrices, what we can exploit from the labels is not only whether any two samples are in the same cluster, but also the number of samples in each cluster and the whole data-set. However, EA only makes use of the former information. Since PA has utilized more information, better performance can be expected. And the experimental results in next section have also shown its capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Improved scheme with pre/post-processing strategy</head><p>As introduced previously, our PA algorithm is based on the hypothesis uniform distribution supported by maximum entropy theory, and the densities of all the clusters are identical, when no other information is available. As a result, this approach is more applicable to the data-sets which satisfy, or mostly satisfy this distribution. For those which do not, our algorithm may not perform well enough. To make the algorithm more effective, an improved scheme with a pre/post-processing to original data set is proposed as below:</p><p>(1) Pre-process: we define a reflection (X, ) = X here, which acts on the original data-set X. This reflection firstly divides the highdimensional space into grids of the same size. Samples in the same grid would then be reflected to the same point in the grid's space. There, we use one of the grid's vertexes. The threshold determine the size of each grid. And all these reflections compose the new data-set X , which now satisfy the uniform distribution. (2) Run K-means X and apply PA to the original partition labels to get the combined clustering P , just the same as the procedure described in Table <ref type="table" target="#tab_0">1</ref>. (3) Post-process: we give the partition label back to the original data-set for the real combined clustering P. It means that each sample in X get the same clustering index as its reflection in X , i.e., P(x i ) = P ( (x i , )).</p><p>Considering that the threshold depends much on the size of the original data-set, we also suggest normalizing the data-set to zero mean and unitary variance before the pre-processing step. Thus, a rational threshold would be versatile to all data-sets. The procedure for all these extra processes is summarized in Table <ref type="table">2</ref>.</p><p>We can find that the proposed pre-processing step serves to convert the original data-set into one, i.e., X , that satisfies PAs assumption. Thus, PA would be more applicable and effective. Since PA here is actually carried out on X rather X, the post-processing step is designed to get the combined clustering for X on basis of that for X . Experimental results in next section reveal that these extra processes do improve the performance of our PA algorithm, especially on the data-sets which do not satisfy our hypothesis. Considering we rarely know data-sets' distributions beforehand, and this extra processes do not increase much computation expenditure, we suggest implementing them with PA all the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results</head><p>We have conducted extensive experiments to test the quality of PA on both synthetic and real data-sets. Parts of these data-sets are used by Fred and Jain in Ref. <ref type="bibr" target="#b10">[9]</ref>, while others are downloaded from the UCI Machine Learning Repository. In this section, we firstly introduce the data-sets we have run our experiments on. Subsequently, we test the performance of EA, Meta-Clustering Algorithm (MCLA) <ref type="bibr" target="#b13">[12]</ref>, HBGF <ref type="bibr" target="#b14">[13]</ref>, and PA on these data-sets, and compare their capability for clustering aggregation. Finally, some additional experiments are carried out: we do the pre-process and construct data-sets with constant probability density from the original ones. Clustering aggregation on these data-sets helps to show PAs capability when our hypothesis of uniform distribution exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Description of data-sets</head><p>Fred and Jain used nine data-sets altogether in Ref. <ref type="bibr" target="#b10">[9]</ref> to test their EAs capability on. Some of them are synthetic 2D data-sets, while other real data-sets are mainly downloaded from UCI Machine Learning Repository. These data-sets include:</p><p>(1) Half-rings-two clusters (see Fig. <ref type="figure">2a</ref>);</p><p>(2) Three-rings-three clusters (see Fig. <ref type="figure">2b</ref>);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The procedure of pre and post-processing steps Pre/post Processing Inputs: X-original data-set -threshold for pre-process 1. Normalizing the data-set X to X satisfying: E(X = 0) and D(X) = 1 2. Run pre-process on X get the reflected data-set: (X, ) = X 3. Run K-means on X get partition labels: (3) Cigar data-four clusters (see Fig. <ref type="figure">2c</ref>); (4) Iris data-three clusters, from UCI;</p><formula xml:id="formula_26">{C (p) } d</formula><p>(5) Wisconsin breast-cancer-two clusters, from UCI; (6) Optical-digits-10 clusters, from UCI. Following the experiments in Ref. <ref type="bibr" target="#b10">[9]</ref>, we only use a subset composed of its first 100 samples; (7) Log yeast, and (8) Std yeast consist of the logarithm and the standardized version (normalization to zero mean and unitary variance), respectively, of gene expression levels of 384 genes over two cell cycles of yeast cell data.</p><p>The first three 2D data-sets are shown in Fig. <ref type="figure">2</ref>.</p><p>Aside from these data-sets above, for further comparison, we additionally use other five data-sets, which are all widely used in the research of data mining and machine learning, and downloaded from the UCI Machine Learning Repository. These include:</p><p>(1) Wine data-178 samples in three clusters;</p><p>(2) Glass data-216 samples in six clusters;</p><p>(3) Waveform data-three clusters. There, we use a subset composed of its first 300 samples; (4) Waveform with noise-three clusters with 40 dimensions. Actually this data-set is the original form of "waveform". Compared with the 21 attributes of Waveform, the 19 addition attributes are actually noises. We also carry out our experiment on its first 300 samples; (5) Credit-screening-the original data-set consists of 690 samples. After removing those with missing attribute values, we have 652 complete samples.</p><p>The characters of all these data-sets are summarized in Table <ref type="table" target="#tab_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance of algorithms</head><p>For each data-set, we run K-means d times with given numbers of clusters, K, as initialization, and therefore we obtain d partition labels for clustering aggregation. We then transport these original clustering results to all four approaches, respectively, to get the combined partition. In addition, we also run these algorithms with the pre/post-process, following the steps described in Table <ref type="table">2</ref>, to test its capability. Since we have all the real labels of these data-sets, we can calculate error rates under the criterion used by Fred and Jain in Ref. <ref type="bibr" target="#b10">[9]</ref>. For EA and PA, we apply SL to the step of MST. In addition, considering that MCLA and HBGF cannot automatically determine the right number of clusters for their following re-partition by METIS, we input the cluster numbers PA detected for them to use.</p><p>This experiment has two parameters to be designed previously: d, the number of clusterings to combine, and K, the number of clusters for K-means. For the parameter d, we set it as 10 and 50, and conduct experiments, respectively. For the parameter K, Fred and Jain suggested that it could be randomly generated for each K-means, but can neither be too large nor too small <ref type="bibr" target="#b10">[9]</ref>. Small values of K are not able to capture the complexity of the data, while large values may produce an over fragmentation of the data. Besides, they also suggested that the initial K could be set as the square root of the data-set's sample numbers <ref type="bibr" target="#b11">[10]</ref>. In another word, if the data-set contains N samples, K should be set as √ N. In our experiments, of all the data-sets, the maximum number of clusters is 10, and therefore we set K min as 10. On the other hand, of all the data-sets in our experiment, the maximum number of samples is 683, while the minimal number is only 100. Besides, the average number is 348.8, and its square root is 18.7. Thus, we set 20 to be the medium value of the range. As a result, we decide to set the range <ref type="bibr" target="#b11">[10,</ref><ref type="bibr">30]</ref> for random initialization of K.</p><p>Similar to Ref. <ref type="bibr" target="#b10">[9]</ref>, for each data-set, we repeat the experiment for 50 times and get 50 error rates, respectively, for each algorithm. We record the minimum one of all these 50 error rates, and also calculate the mean of them. All these minimum error rates and average error rates for d = 10 and 50 are listed in Tables <ref type="table">4</ref><ref type="table">5</ref><ref type="table">6</ref><ref type="table">7</ref>. Additionally, we also calculate each algorithm's average error rates on all data-sets, and list them at the bottom of the tables. There, algorithms with pre/post-process are, respectively, referred to as P-EA, P-MCLA, P-HBGF, and P-PA for short.</p><p>As listed in Table <ref type="table">4</ref>, PA has the best performance of all these methods. Its minimum error rates and average error rates of 50 runs are mostly lower than those of its three peers. As illustrated by the average performance on all data-sets, using PA instead of EA can help reduce error rates by more than 3%. It can reduce error rates by around 10% compared with using MCLA and HBGF under the same circumstance. It means that our algorithm can produce more accurate and robust consolidated partitions, on basis of the same original clustering ensemble. In addition, we can easily find the proposed pre/post-process can improve the performance of both EA and PA evidently, by reducing the average error rates by around 10%. Aside from that, it can also enhance the performance of MCLA and HBGF. This result verifies the effectiveness and efficiency of pre/postprocess.</p><p>As to the Half-rings and Cigar data-sets, PA performed slightly worse than EA when carried out directly. This is due to that samples in these data-sets do not obey a uniform distribution, which can be easily found from Fig. <ref type="figure">2</ref>. In Fig. <ref type="figure">2a</ref>, two clusters are, respectively, composed of 400 and 100 samples. However, the areas they expanded are of a similar size. It means that they have much different densities, and therefore PA is not applicable to this situation. Cigar also suffers from this problem. Although PA performs worse than EA on these two data-sets, P-PA has lower error rates than both EA and P-EA. It means that the pre/post-process can strengthen both these two algorithms, especially for PA. The data-sets of Half-rings, Three-rings and Cigar after the pre-processing step of normalization are shown in Fig. <ref type="figure" target="#fig_4">3</ref>, in which we can easily find that these samples distribute more uniformly.</p><p>It is also interesting to analyze the experimental results on Iris. From results in Table <ref type="table" target="#tab_1">3</ref>, we can see that results of EA and PA are same. This phenomenon is due to the fact that Iris contains three clusters, and each of them consists of 50 samples. As described in last section, when data-sets are composed of clusters of the same size, the p-association matrix would degenerate to co-association matrix, and therefore PA and EA would produce the same combined clustering.</p><p>Statistic results of combining 50 clusterings in Tables 6 and 7 also reveal the same tendency as addressed above, and can further validate that the proposed PA algorithm outperforms other three algorithms, i.e., EA MCLA and HBGF, in clustering aggregation, and the pre/post-processing can help to further improve capability of all these four methods, especially on EA and PA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we have proposed a new approach for clustering aggregation called probability accumulation, which has provided a general framework for the aggregation of partitions. Based on it, we deduce two simplified form of our algorithm: one is for the situation that probability densities can be obtained, while another is applicable to situations when only partition labels are provided. Moreover, we propose the pre-processing and post-processing methods to deal with data-sets that do not satisfy the uniform distribution. Finally, we compare our algorithms' behaviors with three other state-of-the-art approaches for clustering aggregation, and the experimental results illustrated our algorithm's effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall procedure of clustering aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>k</head><label></label><figDesc>is actually a weighed average of all the D(Q 1 , Q 2 ) with their corresponding weights and therefore in proportion to R (p) k as well. Other factors that would affect D (p)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>combining Eqs. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>p=1 4 .Fig. 2 .</head><label>42</label><figDesc>Fig. 2. Test data-sets. (a) Half-rings shape clusters. Left cluster has 100 patterns and the right cluster has 400 patterns. (b) Three-rings data-set. Number of patterns in three rings are, respectively, 50, 250, and 200. (c) Cigar data-set. The top-left cluster has 50 patterns, and the top-right cluster has 100 patterns. Those two bottom clusters have 20 patterns, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Data-sets after the pre-processing step of normalization. (a) Half-rings, (b) Three-rings, and (c) Cigar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>The Procedure of Algorithm</figDesc><table><row><cell>Probability Accumulation</cell></row><row><cell>Inputs: n-number of samples</cell></row><row><cell>m-dimension of samples</cell></row><row><cell>{C (p) } d p=1 -d partition labels</cell></row><row><cell>1. Calculate all the component matrices</cell></row><row><cell>repeat d times:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Overview of data-sets</figDesc><table><row><cell>Data-set</cell><cell>Description of data-set</cell><cell></cell><cell></cell></row><row><cell></cell><cell>#Sample</cell><cell>#Attribute</cell><cell>#Class</cell></row><row><cell>Half-rings</cell><cell>500</cell><cell>2</cell><cell>2</cell></row><row><cell>Three-rings</cell><cell>500</cell><cell>2</cell><cell>3</cell></row><row><cell>Cigar</cell><cell>190</cell><cell>2</cell><cell>4</cell></row><row><cell>Iris</cell><cell>150</cell><cell>3</cell><cell>3</cell></row><row><cell>Optical-digit</cell><cell>100</cell><cell>64</cell><cell>10</cell></row><row><cell>Breast-cancer</cell><cell>683</cell><cell>9</cell><cell>2</cell></row><row><cell>Log yeast</cell><cell>384</cell><cell>76</cell><cell>5</cell></row><row><cell>Std yeast</cell><cell>384</cell><cell>76</cell><cell>5</cell></row><row><cell>Wine</cell><cell>178</cell><cell>13</cell><cell>3</cell></row><row><cell>Glass</cell><cell>214</cell><cell>9</cell><cell>6</cell></row><row><cell>Waveform</cell><cell>300</cell><cell>21</cell><cell>3</cell></row><row><cell>Wave + noise</cell><cell>300</cell><cell>40</cell><cell>3</cell></row><row><cell>Credit</cell><cell>652</cell><cell>15</cell><cell>2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is under the Services Science joint research project between Tsinghua University and IBM China Research Lab. It is also supported by Natural Science Foundation of China under Grants 60573062, 60673106 and 60721003, and the Specialized Research Fund for the Doctoral Program of Higher Education.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Average error rates</title>
		<imprint/>
	</monogr>
	<note>in percentage. of combining 10 clusterings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ea Mcla Hbgf Pa P-Ea P-Mcla P-Hbgf P-Pa</forename><surname>Half</surname></persName>
		</author>
		<idno>rings 2.88 28.10</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Data-set Without pre/post-process With pre/post-process</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale parallel data clustering</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M D</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="158" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conceptual clustering in information retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deogun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="427" to="536" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A lattice conceptual clustering system and its application to browsing retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="122" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A robust competitive clustering algorithm with applications in computer vision</title>
		<author>
			<persName><forename type="first">H</forename><surname>Frigui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnapuram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="450" to="466" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stock</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Pattern Recognition</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Tsinghua University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining multiple clusterings using evidence accumulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="835" to="850" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data clustering using evidence accumulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Pattern Recognition (ICPR)</title>
		<meeting>the 16th International Conference on Pattern Recognition (ICPR)<address><addrLine>Quebec City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="276" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evidence accumulation clustering based on the k-means algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshops on Structural and Syntactic Pattern Recognition (SSPR)</title>
		<meeting>the International Workshops on Structural and Syntactic Pattern Recognition (SSPR)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="442" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cluster ensembles-a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Solving cluster ensemble problems by bipartite graph partitioning</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning (ICML)</title>
		<meeting>the 21st International Conference on Machine Learning (ICML)<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A mixture model of clustering ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining</title>
		<meeting>the SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bagging to improve the accuracy of a clustering procedure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dudoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridlyand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1090" to="1099" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Fundamentals of Applied Information Theory</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Tsinghua University Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>first ed.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Currently, he is a Full Professor and Assistant Chair with the Department of Automation, Tsinghua University. His research area includes pattern recognition, image processing, computer vision, and information fusion. Recently, he has authored more than 10 papers in international journals and more than 40 papers in international conferences</title>
	</analytic>
	<monogr>
		<title level="m">About the Author-JIE ZHOU was born in 1968. He received the B.S. and M.S. degrees from the Department of Mathematics</title>
		<meeting><address><addrLine>Beijing, China; Beijing, China; Tianjin, China; Wuhan, China; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">2006. 2004. 1990 and 1992. 1995</date>
		</imprint>
		<respStmt>
			<orgName>Tsinghua University ; Tsinghua University ; Nankai University ; Huazhong University of Science and Technology (HUST) ; Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>where he is currently pursuing the Ph.D. degree. His research interests are in pattern recognition, machine learning, data mining and intelligent information processing. He is an associate editor for the International Journal of Robotics and Automation. Dr. Zhou received the Best Doctoral Thesis Award from HUST, the First Class Science and Technology Progress Award from the Ministry of Education, China, and the Excellent Young Faculty Award from Tsinghua University in 1995, 1998, and 2003, respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
