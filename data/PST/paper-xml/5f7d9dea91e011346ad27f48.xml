<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIRECTIONAL GRAPH NETWORKS ANISOTROPIC AGGREGATION IN GRAPH NEURAL NETWORKS VIA DIRECTIONAL VECTOR FIELDS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-06">6 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
							<email>dominique@invivoai.com</email>
						</author>
						<author>
							<persName><forename type="first">Saro</forename><surname>Passaro</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
							<email>vincent@invivoai.com</email>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
							<email>pietro.lio@cst.cam.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">InVivo AI Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">InVivo AI Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>MILA Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DIRECTIONAL GRAPH NETWORKS ANISOTROPIC AGGREGATION IN GRAPH NEURAL NETWORKS VIA DIRECTIONAL VECTOR FIELDS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-06">6 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.02863v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In order to overcome the expressive limitations of graph neural networks (GNNs), we propose the first method that exploits vector flows over graphs to develop globally consistent directional and asymmetric aggregation functions. We show that our directional graph networks (DGNs) generalize convolutional neural networks (CNNs) when applied on a grid. Whereas recent theoretical works focus on understanding local neighbourhoods, local structures and local isomorphism with no global information flow, our novel theoretical framework allows directional convolutional kernels in any graph. First, by defining a vector field in the graph, we develop a method of applying directional derivatives and smoothing by projecting node-specific messages into the field. Then we propose the use of the Laplacian eigenvectors as such vector field, and we show that the method generalizes CNNs on an n-dimensional grid. Finally, we bring the power of CNN data augmentation to graphs by providing a means of doing reflection, rotation and distortion on the underlying directional field. We evaluate our method on different standard benchmarks and see a relative error reduction of 8% on the CIFAR10 graph dataset and 11% to 32% on the molecular ZINC dataset. An important outcome of this work is that it enables to translate any physical or biological problems with intrinsic directional axes into a graph network formalism with an embedded directional field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the most important distinctions between convolutional neural networks (CNNs) and graph neural networks (GNNs) is that CNNs allow for any convolutional kernel, while most GNN methods are limited to symmetric kernels (also called isotropic kernels in the literature) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11]</ref>. There are some implementation of asymmetric kernels using gated mechanisms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref>, motif attention <ref type="bibr" target="#b29">[30]</ref>, edge features <ref type="bibr" target="#b10">[11]</ref> or by using the 3D structure of molecules for message passing <ref type="bibr" target="#b18">[19]</ref>.</p><p>However, to the best of our knowledge, there are currently no methods that allow asymmetric graph kernels that are dependent on the full graph structure or on directional flows. They either depend on local structures or local features. This is in opposition to images which exhibit canonical directions: the horizontal and vertical axes. The absence of an analogous concept in graphs makes it difficult to define directional message passing and to produce an analogue of the directional frequency filters (or Gabor filters) widely present in image processing <ref type="bibr" target="#b28">[29]</ref>.</p><p>We propose a novel idea for GNNs: use vector fields in the graph to define directions for the propagation of information. Hence, the aggregation or message passing will be projected onto these directions so that the contribution of each neighbouring node n v will be weighted by its alignment with the vector fields at the receiving node n u . This enables our method to propagate information via directional derivatives or smoothing of the features.</p><p>We also explore using the gradients of the low-frequency eigenvectors of the Laplacian of the graph ? k , since they exhibit interesting properties <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. In particular, they can be used to define optimal partitions of the nodes in a graph, to give a natural ordering <ref type="bibr" target="#b24">[25]</ref>, and to find the dominant directions of the graph diffusion process <ref type="bibr" target="#b4">[5]</ref>. Further, we show that they generalize the horizontal and vertical directional flows in a grid (see figure <ref type="figure" target="#fig_0">1</ref>), allowing them to guide the aggregation and mimic the asymmetric and directional kernels present in computer vision. In fact, we demonstrate mathematically that our work generalizes CNNs by reproducing all convolutional kernels of radius R in an n-dimensional grid, while also bringing the powerful data augmentation capabilities of reflection, rotation or distortion of the directions.</p><p>We further show that our directional graph network (DGN) model theoretically and empirically allows for efficent message passing across distant communities, which reduces the well known problem of over-smoothing, and aligns well with the need of independent aggregation rules <ref type="bibr" target="#b6">[7]</ref>. Alternative methods reduce the impact of over-smoothing by using skip connections <ref type="bibr" target="#b25">[26]</ref> or global pooling <ref type="bibr" target="#b0">[1]</ref>, but without solving the underlying problem.</p><p>Our method distinguishes itself from other spectral GNNs since the literature usually uses the low frequencies to estimate local Fourier transforms in the graph <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>. Instead, we do not try to approximate the Fourier transform, but only to define a directional flow at each node and guide the aggregation.</p><p>2 Theoretical development</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Intuitive overview</head><p>One of the biggest limitations of current GNN methods compared to CNNs is the inability to do message passing in a specific direction such as the horizontal one in a grid graph. In fact, it is difficult to define directions or coordinates based solely on the shape of the graph.</p><p>The lack of directions strongly limits the discriminative abilities of GNNs to understand local structures and simple feature transformations. Most GNNs are invariant to the permutation of the neighbours' features, so the nodes' received signal is not influenced by swapping the features of 2 neighbours. Therefore, several layers in a deep network will be employed to understand these simple changes instead of being used for higher level features, thus over-squashing the message sent between 2 distant nodes <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this work, one of the main contribution is the realisation that low frequency eigenvectors of the Laplacian can overcome this limitation by providing a variety of intuitive directional flows. As a first example, taking a grid-shaped graph of size N ? M with N 2 &lt; M &lt; N , we find that the eigenvector associated to the smallest non-zero eigenvalue increases in the direction of the width N and the second one increases in the direction of the height M . This property generalizes to n-dimensional grids and motivated the use of gradients of eigenvectors as preferred directions for general graphs.</p><p>We validated this intuition by looking at the flow of the gradient of the eigenvectors for a variety of graphs, as shown in figure <ref type="figure" target="#fig_0">1</ref>. For example, in the Minnesota map, the first 3 non-constant eigenvectors produce logical directions, namely South/North, suburb/city, and West/East. Another important contribution also noted in figure <ref type="figure" target="#fig_0">1</ref> is the ability to define any kind of direction based on a prior knowledge of the problem. Hence, instead of relying on eigenvectors to find directions in a map, we can simply use the cardinal directions or the rush-hour traffic flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vector fields in a graph</head><p>Based on a recent review from <ref type="bibr" target="#b2">[3]</ref>, this section presents the ideas of differential geometry applied to graphs, with the goal of finding proper definitions of scalar products, gradients and directional derivatives.</p><p>Let G = (V, E) be a graph with V the set of vertices and E ? V ? V the set of edges. The graph is undirected meaning that (i, j) ? E iff (j, i) ? E. Define the vector spaces L 2 (V ) and L 2 (E) as the set of maps V ? R and E ? R with x, y ? L 2 (V ) and F , H ? L 2 (E) and scalar products</p><formula xml:id="formula_0">x, y L 2 (V ) := i?V x i y i , F , H L 2 (E) := (i,j)?E F (i,j) H (i,j)<label>(1)</label></formula><p>Think of E as the "tangent space" to V and of L 2 (E) as the set of "vector fields" on the space V with each row F i,: representing a vector at the i-th node. Define the pointwise scalar product as the map L 2 (E) ? L 2 (E) ? L 2 (V ) taking 2 vector fields and returning their inner product at each point of V , at the node i is defined by equation 2. F ,</p><formula xml:id="formula_1">H i := j:(i,j)?E F i,j H i,j<label>(2)</label></formula><p>In equation 3, we define the gradient ? as a mapping L 2 (V ) ? L 2 (E) and the divergence div as a mapping L 2 (E) ? L 2 (V ), thus leading to an analogue of the directional derivative in equation 4.</p><p>(?x</p><formula xml:id="formula_2">) (i,j) := x(j) -x(i) , (<label>div</label></formula><formula xml:id="formula_3">F ) i := j:(i,j)?E F (i,j)<label>(3)</label></formula><p>Definition 1. The directional derivative of the function x on the graph G in the direction of the vector field F where each vector is of unit-norm is</p><formula xml:id="formula_4">D F x(i) := ?x, F i = j:(i,j)?E (x(j) -x(i)) Fi,j<label>(4)</label></formula><p>|F | will denote the absolute value of F and ||F i,: || L p the L p -norm of the i-th row of F . We also define the forward/backward directions as the positive/negative parts of the field F ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Directional smoothing and derivatives</head><p>Next, we show how the vector field F is used to guide the graph aggregation by projecting the incoming messages. Specifically, we define the weighted aggregation matrices B av and B dx that allow to compute the directional smoothing and directional derivative of the node features.</p><p>The directional average matrix B av is the weighted aggregation matrix such that all weights are positives and all rows have an L 1 -norm equal to 1, as shown in equation 5 and theorem 2.1, with a proof in the appendix C.1.</p><formula xml:id="formula_5">B av (F ) i,: = |F i,: | ||F i,: || L 1 + (5)</formula><p>The variable is an arbitrarily small positive number used to avoid floating point errors. The L 1 -norm denominator is a local row-wise normalization. The aggregator works by assigning a large weight to the elements in the forward or backward direction of the field, while assigning a small weight to the other elements, with a total weight of 1. Theorem 2.1 (Directional smoothing). The operation y = B av x is the directional average of x, in the sense that y u is the mean of x v , weighted by the direction and amplitude of F .</p><p>The directional derivative matrix B dx is defined in (6) and theorem 2.2, with the proof in appendix C.2. Again, the denominator is a local row-wise normalization but can be replaced by a global normalization. diag(a) is a square, diagonal matrix with diagonal entries given by a. The aggregator works by subtracting the projected forward message by the backward message (similar to a center derivative), with an additional diagonal term to balance both directions.</p><p>B dx (F ) i,: = Fi,:diag j F:,j i,:</p><p>Fi,: = F i,:</p><formula xml:id="formula_6">||F i,: || L 1 + (6)</formula><p>Theorem 2.2 (Directional derivative). Suppose F have rows of unit L 1 norm. The operation y = B dx ( F )x is the centered directional derivative of x in the direction of F , in the sense of equation 4, i.e.</p><formula xml:id="formula_7">y = D F x = F -diag j F:,j x</formula><p>These aggregators are directional, interpretable and complementary, making them ideal choices for GNNs. We discuss the choice of aggregators in more details in appendix A, while also providing alternative aggregation matrices such as the center-balanced smoothing, the forward-copy, the phantom zero-padding, and the hardening of the aggregators using softmax/argmax on the field. We further provide a visual interpretation of the B av and B dx aggregators in figure <ref type="figure" target="#fig_1">2</ref>. Interestingly, we also note in appendix A.1 that B av and B dx yield respectively the mean and Laplacian aggregations when F is a vector field such that all entries are constant F ij = ?C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Gradient of the eigenvectors as interpretable vector fields</head><p>In this section we give theoretical support for the choice of gradients of the eigenfunctions of the Laplacian as sensible vectors along which to do directional message passing since they are interpretable and allow to reduce the over-smoothing.</p><p>As usual the combinatorial, degree-normalized and symmetric normalized Laplacian are defined as</p><formula xml:id="formula_8">L = D -A , L norm = D -1 L , L sym = D -1 2 LD -1 2<label>(7)</label></formula><p>The problems of over-smoothing and over-squashing are critical issues in GNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>. In most GNN models, node representations become over-smoothed after several rounds of message passing (i.e., convolutions), as the representations tend to reach a mean-field equilibrium equivalent to the stationary distribution of a random walk <ref type="bibr" target="#b12">[13]</ref>. Over-smoothing is also related to the problem of over-squashing, which reflects the inability for GNNs to propagate informative signals between distant nodes <ref type="bibr" target="#b0">[1]</ref> and is a major bottleneck to training deep GNN models <ref type="bibr" target="#b31">[32]</ref>. Both problems are related to the fact that the influence of one node's input on the final representation of another node in a GNN is determined by the likelihood of the two nodes co-occurring on a truncated random walk <ref type="bibr" target="#b33">[34]</ref>.</p><p>We show in theorem 2.3 (proved in appendix C.3) that by passing information in the direction of ? 1 , the eigenvector associated to the lowest non-trivial frequency of L norm , DGNs can efficiently share information between the farthest nodes of the graph, when using the K-walk distance to measure the difficulty of passing information. Thus, DGNs provide a natural way to address both the over-smoothing and over-squashing problems: they can efficiently propagate messages between distant nodes and in a direction that counteracts over-smoothing. Definition 2 (K-walk distance). The K-walk distance d K (v i , v j ) on a graph is the average number of times v i is hit in a K step random walk starting from v j . Theorem 2.3 (K-Gradient of the low frequency eigenvectors). Let ? i and ? i be the eigenvalues and eigenvectors of the normalized Laplacian of a connected graph L norm and let a, b be the nodes that have a farthest K-walk distance. Suppose that ? 2 ? 1 , then the optima of ? 1 approximates a, b.</p><p>As another point of view into the problem of oversmoothing, consider the hitting time Q(x, y) defined as the expected number of steps in a random walk starting from node x ending in node y with the probability transition P (x, y) = 1 dx . In appendix C.4 we give an informal argument supporting the following conjecture. Conjecture 2.4 (Gradient steps reduce expected hitting time). Suppose that x, y are uniformly distributed random nodes such that ? i (x) &lt; ? i (y). Let z be the node obtained from x by taking one step in the direction of ?? i , then the expected hitting time is decreased proportionally to ? -1</p><p>i and</p><formula xml:id="formula_9">E x,y [Q(z, y)] ? E x,y [Q(x, y)]</formula><p>These results from theorem 2.3 and conjecture 2.4 have the following immediate corollaries. Corollary 2.5 (Reduces over-squashing). Following the direction of ?? 1 is an efficient way of passing information between the farthest nodes of the graph (in terms of the K-walk distance). Corollary 2.6 (Reduces over-smoothing). Following the direction of ?? 1 allows the influence distribution between node representations to be decorrelated from random-walk hitting times (assuming the definition of influence introduced in <ref type="bibr" target="#b33">[34]</ref>).</p><p>Our method also aligns perfectly with a recent proof that multiple independent aggregators are needed to distinguish neighbourhoods of nodes with continuous features <ref type="bibr" target="#b6">[7]</ref>.</p><p>When using eigenvectors of the Laplacian ? i to define directions in a graph, we need to keep in mind that there is never a single eigenvector associated to an eigenvalue, but a whole eigenspace. For instance, a pair of eigenvalues can have a multiplicity of 2 meaning that they can be generated by different pairs of orthogonal eigenvectors. For an eigenvalue of multiplicity 1, there are always 2 norm 1 eigenvectors of opposite sign, which poses a problem during the directional aggregation. We can make a choice of sign and later take the absolute value (i.e. B av in equation <ref type="formula">5</ref>). An alternative is to take a sample of orthonormal basis of the eigenspace and use each choice to augment the training (see section 2.7). Note that although eigenvalues with multiplicity higher than one do happen (e.g. square grids have a multiplicity 2 for ? 1 ) this is not common for "real-world graphs"; we found no ? 1 multiplicity greater than 1 on the ZINC and PATTERN datasets. Further, although all ? are orthogonal, their gradients, used to define directions, are not always locally orthogonal (e.g. there are many horizontal flows in the grid). This last concern is left to be addressed in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Generalization of the convolution on a grid</head><p>In this section we show that our method generalizes CNNs by allowing to define any radius-R convolutional kernels in grid-shaped graphs. The radius-R kernel at node u is a convolutional kernel that takes the weighted sum of all nodes v at a distance d(u, v) ? R.</p><p>Consider the lattice graph ? of size N 1 ? N 2 ? ... ? N n where each vertices are connected to their direct non-diagonal neighbour. We know from Lemma C.1 that, for each dimension, there is an eigenvector that is only a function of this specific dimension. For example, the lowest frequency eigenvector ? 1 always flows in the direction of the longest length. Hence, the Laplacian eigenvectors of the grid can play a role analogous to the axes in Euclidean space, as shown in figure <ref type="figure" target="#fig_0">1</ref>.</p><p>With this knowledge, we show in theorem 2.7 (proven in C.7), that we can generalize all convolutional kernels in an n-dimensional grid. This is a strong result since it demonstrates that our DGN framework generalizes CNNs when applied on a grid, thus closing the gap between GNNs and the highly successful CNNs on image tasks. Theorem 2.7 (Generalization radius-R convolutional kernel in a lattice). For an n-dimensional lattice, any convolutional kernel of radius R can be realized by a linear combination of directional aggregation matrices and their compositions.</p><p>As an example, figure <ref type="figure" target="#fig_1">2</ref>.5 shows how a linear combination of the first and m-th aggregators B(?? 1,m ) realize a kernel on an N ? M grid, where m = N/M and N &gt; M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Extending the radius of the aggregation kernel</head><p>Having aggregation kernels for neighbours of distance 2 or 3 is important to improve the expressiveness of GNNs, their ability to understand patterns, and to reduce the number of layers required. However, the lack of directions in GNNs strongly limits the radius of the kernels since, given a graph of regular degree d, a mean/sum aggregation at a radius-R will result in a heavy over-squashing of d R messages. Using the directional fields, we can enumerate different paths, thus assigning a different weight for different R-distant neighbours. This method, proposed in appendix A.7, avoids the over-squashing, but empirical results are left for future work. </p><formula xml:id="formula_10">? = 2? ?? 1 ? 1 -1 ? = 2? ?? 1 ? 1 1 ? = 2? ?? ? ? 1 -1 ? = 2? ?? ? ? ? 1 ? 2 + ? 3 ? 4 + ? 5 ? 4 -? 5 ? 2 -? 3</formula><p>Figure <ref type="figure">3</ref>: Realization of a radius-1 convolution using the proposed aggregators. I x is the input feature map, * the convolutional operator, I y the convolution result, and B i = B(?? i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Data augmentation</head><p>Another important result is that the directions in the graph allow to replicate some of the most common data augmentation techniques used in computer vision, namely reflection, rotation and distortion. The main difference is that, instead of modifying the image (such as a 5 ? rotation), the proposed transformation is applied on the vector field defining the aggregation kernel (thus rotating the kernel by -5 ? without changing the image). This offers the advantage of avoiding to pre-process the data since the augmentation is done directly on the kernel at each iteration of the training.</p><p>The simplest augmentation is the vector field flipping, which is done changing the sign of the field F , as stated in definition 3. This changes the sign of B dx , but leaves B av unchanged. Definition 3 (Reflection of the vector field). For a vector field F , the reflected field is -F .</p><p>Let F 1 , F 2 be vector field in a graph. Let ? be the angle between the vector fields such that</p><formula xml:id="formula_11">F 1 , F 2 = F 1 F 2 cos(?). The vector field F ? 2 is the component of F 2 perpendicular to F 1 : (F ? 2 ) i,: = (F 2 -F 1 , F 2 F 1 ) i,: ||F 2 -F 1 , F 2 F 1 || i,:<label>(8)</label></formula><p>Definition 4 (Rotation of the vector fields). For F 1 and F 2 non-colinear vector fields, their rotation by the angle ? in the plane formed by {F 1 , F 2 } is</p><formula xml:id="formula_12">F 1 = F 1 cos ? + F ? 2 sin ? , F 2 = F 1 cos(? + ?) + F ? 2 sin(? + ?)<label>(9)</label></formula><p>Finally, the following augmentation has similar effect to a wave distortion applied on images. Definition 5 (Random distortion of the vector field). For vector field F and anti-symmetric random noise matrix R, its randomly distorted field is</p><formula xml:id="formula_13">F = F + R ? A.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation</head><p>We implemented the models using the DGL and PyTorch libraries and we provide the code at the address https://github.com/Saro00/DGN. We test our method on standard benchmarks from <ref type="bibr" target="#b8">[9]</ref>, namely ZINC, CIFAR10 and PATTERN, with more details on the datasets and how we enforce a fair comparison in appendix B.1.</p><p>For the empirical experiments we inserted our proposed aggregation method in two different type of message passing architecture used in the literature: a simple one similar to the one present in GCN (equation 10a) <ref type="bibr" target="#b17">[18]</ref> and a more complex and general one typical of MPNN (10b) <ref type="bibr" target="#b10">[11]</ref> with or without edge features e ji . Hence, the time complexity O(Em) is identical to the PNA <ref type="bibr" target="#b6">[7]</ref>, where E is the number of edges and m the number of aggregators, with an additional O(Ek) to pre-compute the k-first eigenvectors, as explained in the appendix B.2.</p><formula xml:id="formula_14">X (t+1) i = U (j,i)?E X (t) j (10a) X (t+1) i = U X (t) i , (j,i)?E M X (t) i , X (t) j , e ji optional (<label>10b</label></formula><formula xml:id="formula_15">)</formula><p>where is an operator which concatenates the results of multiple aggregators, X is the node features, M is a linear transformation and U a multiple layer perceptron.</p><p>We tested the directional aggregators across the datasets using the gradient of the first k eigenvectors ?? 1,...,k as the underlying vector fields. To deal with the arbitrary sign of the eigenvectors, we take the absolute value of the result of equation 6, making it invariant to a reflection of the field. In case of a disconnected graph, ? i is the i-th eigenvector of each connected component. Despite the numerous aggregators proposed in appendix A, only B dx and B av are tested empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and discussion</head><p>Directional aggregation Using the benchmarks introduced in section 3, we present in figure 4 a fair comparison of various aggregation strategies using the same parameter budget and hyperparameters. We see a consistent boost in the performance for simple, complex and complex with edges models using directional aggregators compared to the mean-aggregator baseline. For brevity, we denote dx i and av i as the directional derivative B i dx and smoothing B i av aggregators. : Test set results using a parameter budget of 100k, with the same hyperparameters as <ref type="bibr" target="#b6">[7]</ref>. The low-frequency Laplacian eigenvectors are used to define the directions, except for CIFAR10 that uses the coordinates of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZINC</head><p>In particular, we see a significant improvement on ZINC using the derivative aggregator. We believe this is due to the capacity to move efficiently messages across opposite parts of the molecule and to better understand the role of atom pairs. Further, the thesis that DGNs can bridge the gap between CNNs and GNNs is supported by the clear improvements on CIFAR10 over the baselines.</p><p>With our theoretical analysis in mind, we expected to perform well on PATTERN since the flow of the first eigenvectors are meaningful directions in a stochastic block model and passing messages using those directions allows the network to efficiently detect the two communities. The results match our expectations, outperforming all the previous models.</p><p>Comparison to the literature In order to compare our model with the literature, we fine tuned it on the various datasets and we report its performance in figure <ref type="figure" target="#fig_3">5</ref>. We observe that DGN provides significant improvement across all benchmarks, highlighting the importance of anisotropic kernels. In the work by <ref type="bibr" target="#b8">[9]</ref>, they proposed the use of positional encoding of the eigenvectors in node features, but these bring significant improvement when many eigenvectors and high network depths are used. Our results outperform them with fewer parameters, less depth, and only 1-2 eigenvectors, further motivating their use as directional flows instead of positional encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZINC PATTERN CIFAR10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>No  : GCN <ref type="bibr" target="#b17">[18]</ref>, GraphSage <ref type="bibr" target="#b11">[12]</ref>, GIN <ref type="bibr" target="#b32">[33]</ref>, GAT <ref type="bibr" target="#b30">[31]</ref>, MoNet <ref type="bibr" target="#b27">[28]</ref>, GatedGCN <ref type="bibr" target="#b1">[2]</ref> and PNA <ref type="bibr" target="#b6">[7]</ref>. All the models are given a parameter budget ? 100k. In ZINC we used aggregators {mean, dx 1 , max, min}, in PATTERN {mean, dx 1 , av 1 } and in CIFAR10 {mean, dx 1 , dx 2 , max}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data augmentation</head><p>To evaluate the effectiveness of the proposed augmentation, we trained the models on a reduced version of the CIFAR10 dataset. The results in figure <ref type="figure" target="#fig_4">6</ref> show clearly a higher expressive power of the dx aggregator, enabling it to fit well the training data. For a small dataset, this comes at the cost of overfitting and reduced performance on the test set. However, we observe that randomly rotating the kernels counteract the overfitting and allows the model to better generalize. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The proposed DGN method allows to solve many problems of GNNs, including the lack of anisotropy, the low expressiveness, the over-smoothing and over-squashing. For the first time in graph networks, we generalize the directional properties of CNNs and their data augmentation capabilities. Based on an intuitive idea and backed by a set of strong theoretical and empirical results, we believe this work will give rise to a new family of directional GNNs. Future work can focus on the implementation of radius-R kernels and improving the choice of multiple orthogonal directions.</p><p>Broader Impact This work will extend the usability of graph networks to all problems with physically defined directions, thus making GNN a new laboratory for physics, material science and biology. In fact, the anisotropy present in a wide variety of systems could be expressed as vector field (spinor, tensor) compatible with the DGN framework, without the need of eigenvectors. One example is magnetic anisotropicity in metals, alloys and also in molecules such as benzene ring, alkene, carbonyl, alkyne that are easier or harder to magnetise depending on the directions or which way the object is rotated. Other examples are the response of material to high electromagnetic fields (e.g. to study material responses at terahertz frequency); all kind of field propagation in crystals lattices (vibrations, heat, shear and frictional force, young modulus, light refraction, birefringence); multi-body or liquid motion; traffic modeling; and design of novel materials and constrained structures. This also enables GNNs to be used for virtual prototyping systems since the added directional constraints could improve the analysis of a product's functionality, manufacturing and behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix -Choices of directional aggregators</head><p>This appendix helps understand the choice of B av and B dx in section 2.3 and presents different directional aggregators that can be used as an alternative to the ones proposed.</p><p>A simple alternative to the directional smoothing and directional derivative operator is to simply take the forward/backward values according to the underlying positive/negative parts of the field F , since it can effectively replicate them. However, there are many advantage of using B av,dx . First, one can decide to use either of them and still have an interpretable aggregation with half the parameters. Then, we also notice that B av,dx regularize the parameter by forcing the network to take both forward and backward neighbours into account at each time, and avoids one of the neighbours becoming too important. Lastly, they are robust to a change of sign of the eigenvectors since B av is sign invariant and B dx will only change the sign of the results, which is not the case for forward/backward aggregations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Retrieving the mean and Laplacian aggregations</head><p>It is interesting to note that we can recover simple aggregators from the aggregation matrices B av (F ) and B dx (F ). Let F be a vector field such that all edges are equally weighted F ij = ?C for all edges (i, j). Then, the aggregator B av is equivalent to a mean aggregation:</p><formula xml:id="formula_16">B av (F )x = D -1 Ax</formula><p>Under the condition F ij = C, the differential aggregator is equivalent to a Laplacian operator L normalized using the degree</p><formula xml:id="formula_17">D B dx (CA)x = D -1 (A -D)x = -D -1 Lx A.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Global field normalization</head><p>The proposed aggregators are defined with a row-wise normalized field Fi,: = F i,: ||F i,: || L P meaning that all the vectors are of unit-norm and the aggregation/message passing is done only according to the direction of the vectors, not their amplitude. However, it is also possible to do a global normalization of the field F by taking a a matrix-norm instead of a vector-norm. Doing so will modulate the aggregation by the amplitude of the field at each node. One need to be careful since a global normalization might be very sensible to the number of nodes in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Center-balanced aggregators</head><p>A problem arises in the aggregators B dx and B av proposed in equations 5 and 6 when there is an imbalance between the positive and negative terms of F ? . In that case, one of the directions overtakes the other in terms of associated weights.</p><p>An alternative is also to normalize the forward and backward directions separately, to avoid having either the backward or forward direction dominating the message.</p><formula xml:id="formula_18">B av-center (F ) i,: = F + i,: + F - i,: ||F + i,j + F - i,j || L1 , F ? i,: = |F ? i,: | ||F ? i,: || L 1 +<label>(11)</label></formula><p>The same idea can be applied to the derivative aggregator equation 12 where the positive and negative parts of the field F ? are normalized separately to allow to project both the forward and backward messages into a vector field of unit-norm. F + is the out-going field at each node and is used for the forward direction, while F -is the in-going field used for the backward direction. By averaging the forward and backward derivatives, the proposed matrix B dx-center represents the centered derivative matrix.</p><formula xml:id="formula_19">B dx-center (F ) i,: = F i,: -diag ? ? j F :,j ? ? i,:</formula><p>,</p><formula xml:id="formula_20">F i,: = 1 2 ? ? ? ? ? F + i,: ||F + i,: || L 1 + forward field + F - i,: ||F - i,: || L 1 + backward field ? ? ? ? ?<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Hardening the aggregators</head><p>The aggregation matrices that we proposed, mainly B dx and B av depend on a smooth vector field F . At any given node, the aggregation will take a weighted sum of the neighbours in relation to the direction of F . Hence, if the field F v at a node v is diagonal in the sense that it gives a non-zero weight to many neighbours, then the aggregator will compute a weighted average of the neighbours.</p><p>Although there are clearly good reasons to have this weighted-average behaviour, it is not necessarily desired in every problem. For example, if we want to move a single node across the graph, this behaviour will smooth the node at every step. Instead, we propose below to soften and harden the aggregations by forcing the field into making a decision on the direction it takes.</p><p>Soft hardening the aggregation is possible by using a softmax with a temperature T on each row to obtain the field</p><formula xml:id="formula_21">F softhard . (F softhard ) i,: = sign(F i,: )softmax(T |F i,: |) (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>Hardening the aggregation is possible by using an infinite temperature, which changes the softmax functions into argmax. In this specific case, the node with the highest component of the field will be copied, while all other nodes will be ignored.</p><formula xml:id="formula_23">(F hard ) i,: = sign(F i,: )argmax(|F i,: |) (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>An alternative to the aggregators above is to take the softmin/argmin of the negative part and the softmax/argmax of the positive part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Forward and backward copy</head><p>The aggregation matrices B av and B dx have the nice property that if the field is flipped (change of sign), the aggregation gives the same result, except for the sign of B dx . However, there are cases where we want to propagate information in the forward direction of the field, without smoothing it with the backward direction. In this case, we can define the strictly forward and strictly backward fields below, and use them directly with the aggregation matrices.</p><formula xml:id="formula_25">F forward = F + , F backward = F -<label>(15)</label></formula><p>Further, we can use the hardened fields in order to define a forward copy and backward copy, which will simply copy the node in the direction of the highest field component.</p><formula xml:id="formula_26">F forward copy = F + hard , F backward copy = F - hard<label>(16)</label></formula><p>A.6 Phantom zero-padding Some recent work in computer vision have shown the importance of zero-padding to improve CNNs by allowing the network to understand it's position relative to the border <ref type="bibr" target="#b15">[16]</ref>. In contrast, using boundary conditions or reflection padding makes the network completely blind to positional information. In this section, we show that we can mimic the zero-padding in the direction of the field F for both aggregation matrices B av and B dx .</p><p>Starting with the B av matrix, in the case of a missing neighbour in the forward/backward direction, the matrix will compensate by adding more weights to the other direction, due to the denominator which performs a normalization. Instead, we would need the matrix to consider both directions separately so that a missing direction would result in zero padding. Hence, we define B av,0pad below, where either the F + or F -will be 0 on a boundary with strictly in-going/out-going field.</p><p>(B av,0pad ) i,: = 1 2</p><formula xml:id="formula_27">|F + i,: | ||F + i,: || L 1 + + |F - i,: | ||F - i,: || L 1 +<label>(17)</label></formula><p>Following the same argument, we define B dx,0pad below, where either the forward or backward term are ignored. The diagonal term is also removed at the boundary so that the result is a center derivative equal to the subtraction of forward term with the 0-term on the back (or vice-versa), instead of a forward derivative.</p><formula xml:id="formula_28">B dx-0pad (F ) i,: = ? ? ? ? ? ? ? F + i,: if j F - i,j = 0 F - i,: if j F + i,j = 0 1 2 F + i,: + F - i,: -diag j F + :,j + F - :,j i,:</formula><p>, otherwise</p><formula xml:id="formula_29">F + i,: = F + i,: ||F + i,: || L 1 + F - i,: = F - i,: ||F - i,: || L 1 +<label>(18)</label></formula><p>A.7 Extending the radius of the aggregation kernel</p><p>We aim at providing a general radius-R kernel B R that assigns different weights to different subsets of nodes n u at a distance R from the center node n v .</p><p>First, we decompose the matrix B(F ) into positive and negative parts B ? (F ) representing the forward and backward steps aggregation in the field F .</p><formula xml:id="formula_30">B(F ) = B + (F ) -B -(F )<label>(19)</label></formula><p>Thus, defining</p><formula xml:id="formula_31">B ? f b (F ) i,: = F ? i,:</formula><p>||Fi,:|| L p , we can find different aggregation matrices by using different combinations of walks of radius R. First demonstrated for a grid in theorem 2.7, we generalize it in equation 20 for any graph G. Definition 6 (General radius R n-directional kernel). Let S n be the group of permutations over n elements with a set of directional fields F i .</p><formula xml:id="formula_32">B R := V ={v1,v2,...,vn}?N n ||V || L 1 ?R, -R?vi?R</formula><p>Any choice of walk V with at most R steps using all combinations of v1, v2, ..., vn ??Sn optional permutations</p><formula xml:id="formula_33">a V N j=1 (B sgn(v ?(j) ) f b (F ?(j) )) |v ?(j) |</formula><p>Aggregator following the steps V , permuted by Sn <ref type="bibr" target="#b19">(20)</ref> In this equation, n is the number of directional fields and R is the desired radius. V represents all the choices of walk {v 1 , v 2 , ..., v n } in the direction of the fields {F 1 , F 2 , ..., F n }. For example, V = {3, 1, 0, -2} has a radius R = 6, with 3 steps forward of F 1 , 1 step forward of F 2 , and 2 steps backward of F 4 . The sign of each B ? f b is dependant to the sign of v ?(j) , and the power |v ?(j) | is the number of aggregation steps in the directional field F ?(j) . The full equation is thus the combination of all possible choices of paths across the set of fields F i , with all possible permutations. Note that we are restricting the sum to v i having only a possible sign; although matrices don't commute, we avoid choosing different signs since it will likely self-intersect a lower radius walk. The permutations ? are required since, for example, the path up ? left is different (in a general graph) than the path left ? up.</p><formula xml:id="formula_34">This matrix B R has a total of R r=0 (2n) r = (2n) R+1 -1 2n-1</formula><p>parameters, with a high redundancy since some permutations might be very similar, e.g. for a grid graph we have that up ? left is identical to left ? up. Hence, we can replace the permutation S n by a reverse ordering, meaning that N j B j = B N ...B 2 B 1 . Doing so does not perfectly generalize the radius-R kernel for all graphs, but it generalizes it on a grid and significantly reduces the number of parameters to</p><formula xml:id="formula_35">R r=0 min(n,r) l=1 2 r n l r-1 l-1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Appendix -Implementation details B.1 Benchmarks and datasets</head><p>We use a variety of benchmarks proposed by <ref type="bibr" target="#b8">[9]</ref> to test the empirical performance of our proposed methods. In particular, to have a wide variety of graphs and tasks we chose:</p><p>1. ZINC, a graph regression dataset from molecular chemistry. The task is to predict a score that is a subtraction of computed properties logP -SA, with logP being the computed octanol-water partition coefficient, and SA being the synthetic accessibility score <ref type="bibr" target="#b16">[17]</ref>.</p><p>2. CIFAR10, a graph classification dataset from computer vision <ref type="bibr" target="#b21">[22]</ref>. The task is to classify the images into 10 different classes, with a total of 5000 training image per class and 1000 test image per class. Each image has 32 ? 32 pixels, but the pixels have been clustered into a graph of ? 100 super-pixels. Each super-pixel becomes a node in an almost grid-shaped graph, with 8 edges per node. The clustering uses the code from <ref type="bibr" target="#b19">[20]</ref>, and results in a different number of super-pixels per graph.</p><p>3. PATTERN, a node classification synthetic benchmark generated with Stochastic Block Models, which are widely used to model communities in social networks. The task is to classify the nodes into 2 communities and it tests the fundamental ability of recognizing specific predetermined subgraphs.</p><p>Our goal is to provide a fair comparison to demonstrate the capacity of our proposed aggregators. Therefore, we compare the various methods on both types of architectures using the same hyperparameters tuned in previous works <ref type="bibr" target="#b6">[7]</ref> for similar networks. The models vary exclusively in the aggregation method and the width of the architectures to keep a set parameter budget.</p><p>In CIFAR10 it is impossible to numerically compute a deterministic vector field with eigenvectors due to the multiplicity of ? 1 being greater than 1. This is caused by the symmetry of the square image, and is extremely rare in real-world graphs. Therefore, we used as underlying vector field the gradient of the coordinates of the image. Note that these directions are provided in the nodes' features in the dataset and available to all models, that they are co-linear to the eigenvectors of the grid as per lemma C.1, and that they mimic the inductive bias in CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Implementation and computational complexity</head><p>Unlike several more expressive graph networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>, our method does not require a computational complexity superlinear with the size of the graph. The calculation of the first k eigenvectors during pretraining, done using Lanczos method <ref type="bibr" target="#b22">[23]</ref> and the sparse module of Scipy, has a time complexity of O(Ek) where E is the number of edges. During training the complexity is equivalent to a m-aggregator GNN O(Em) <ref type="bibr" target="#b6">[7]</ref>.</p><p>To all the architectures we added residual connections <ref type="bibr" target="#b13">[14]</ref>, batch normalization <ref type="bibr" target="#b14">[15]</ref> and graph size normalization <ref type="bibr" target="#b8">[9]</ref>.</p><p>For all the datasets with non-regular graphs, we combine the various aggregators with logarithmic degree-scalers as in <ref type="bibr" target="#b6">[7]</ref>.</p><p>An important thing to note is that, for dynamic graphs, the eigenvectors need to be re-computed dynamically with the changing edges. Fortunately, there are random walk based algorithms that can estimate ? 1 quickly, especially for small changes to the graph <ref type="bibr" target="#b7">[8]</ref>. In the current empirical results, we do not work with dynamic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Appendix -Mathematical proofs</head><p>C.1 Proof for Theorem 2.1 (Directional smoothing)</p><p>The operation y = B av x is the directional average of x, in the sense that y u is the mean of x v , weighted by the direction and amplitude of F .</p><p>Proof. This should be a simple proof, that if we want a weighted average of our neighbours, we simply need to multiply the weights by each neighbour, and divide by the sum of the weights. Of course, the weights should be positive. Proof. Since F rows have unit L 1 norm, F = F . The i-th coordinate of the vector F -diag j F :,j x is Indeed if we want to choose i, j to be at the farthest distance we need to minimise</p><formula xml:id="formula_36">(P ) i,j = (K? 0 ? T 0 + ?? 1 ? T 1 ) i,j = K n + ?? 1 (i)? 1 (j)</formula><p>which is minimum when ? 1 (i)? 1 (j) is minimum.</p><p>The quantity ? 1 (i)? 1 (j) is minimised when it has negative sign and highest absolute value, hence when i, j are associated with the negative and positive values with the highest absolute value: the lowest and the highest value of ? 1 .</p><p>C.4 Informal argument in support of Conjecture 2.4(Gradient steps reduce expected hitting time)</p><p>Suppose that x, y are uniformly distributed random nodes such that ? i (x) &lt; ? i (y). Let z be the node obtained from x by taking one step in the direction of ?? i , then the expected hitting time is decreased proportionally to ? -1 i and</p><formula xml:id="formula_37">E x,y [Q(z, y)] ? E x,y [Q(x, y)]</formula><p>In <ref type="bibr" target="#b3">[4]</ref>, it is shown the hitting time Q(x, y) is given by the equation</p><formula xml:id="formula_38">Q(x, y) = vol G(y, y) d y - G(x, y) d x</formula><p>With ? k and ? k being the k-th eigenvalues and eigenvectors of the symmetric normalized Laplacian L sym , vol the sum of the degrees of all nodes, d x the degree of node x and G Green's function for the graph</p><formula xml:id="formula_39">G(x, y) = d 1 2 x d -1 2 y k&gt;0 1 ? k ? k (x)? k (y)</formula><p>Since the sign of the eigenvector is not deterministic, the choice ? i (x) &lt; ? i (y) is used to simplify the argument without having to consider the change in sign.</p><p>Supposing ? 1 ? 2 , the first term of the sum of G has much more weight than the following terms. With z obtained from x by taking a step in the direction of the gradient of ? 1 we have</p><formula xml:id="formula_40">? 1 (z) -? 1 (x) &gt; 0</formula><p>We want to show that the following inequality holds</p><formula xml:id="formula_41">E x,y (Q(z, y)) &lt; E x,y (Q(x, y))</formula><p>this is equivalent to the following inequality</p><formula xml:id="formula_42">E x,y [G(z, y)] &gt; E x,y [G(x, y)] By the hypothesis ? 1 ? 2 , we can approximate G(x, y) ? d 1 2 x d -1 2 y 1 ?1 ? 1 (x)? 1 (y) so the last inequality is equivalent to E x,y d 1 2 z d -1 2 y 1 ? 1 ? 1 (z)? 1 (y) &gt; E x,y d 1 2 x d -1 2 y 1 ? 1 ? 1 (x)? 1 (y)</formula><p>Removing all equal terms from both sides, the inequality is equivalent to</p><formula xml:id="formula_43">E x,y d 1 2 z ? 1 (z) &gt; E x,y d 1 2</formula><p>x ? 1 (x)</p><p>But showing this last inequality is not easy. We know that ? 1 (z) &gt; ? 1 (x) and from the choice of z being a step in the direction of ?? 1 , we know it is less likely to be on the border of the graph so we believe E(d z ) ? E(d x ). Thus we also believe that the conjecture should hold in general.</p><p>We believe this should be true even without the assumption on ? 1 and ? 2 and for more eigenvectors than ? 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Proof for Lemma C.1 (Cosine eigenvectors)</head><p>Consider the lattice graph ? of size N 1 ? N 2 ? ... ? N n , that has vertices i=1,...,n {1, ..., N i } and the vertices (x i ) i=1,...,n and (y i ) i=1,...,n are connected by an edge iff |x i -y i | = 1 for one index i and 0 for all other indices. Note that there are no diagonal edges in the lattice. The eigenvector of the Laplacian of the grid L(?) are given by ? j .</p><p>Lemma C.1 (Cosine eigenvectors). The Laplacian of ? has an eigenvalue 2 -2 cos ? Ni with the associated eigenvector ? j that depends only the variable in the i-th dimension and is constant in all others, with ?</p><formula xml:id="formula_44">j = 1 N1 ? 1 N2 ? ... ? x 1,Ni ? ... ? 1 Nn , and x 1,Ni (j) = cos ?j n -?<label>2n</label></formula><p>Proof. First, recall the well known result that the path graph on N vertices P N has eigenvalues</p><formula xml:id="formula_45">? k = 2 -2 cos ?k n with associated eigenvector x k with i-th coordinate x k (i) = cos ?ki n + ?k<label>2n</label></formula><p>The Cartesian product of two graphs</p><formula xml:id="formula_46">G = (V G , E G ) and H = (V H , E H ) is defined as G ? H = (V G?H , E G?H ) with V G?H = V G ? V H and ((u 1 , u 2 ), ((v 1 , v 2 )) ? E G?H iff either u 1 = v 1 and (u 2 , v 2 ) ? E H or (u 1 , v 1 ) ? V G and u 2 = v 2 .</formula><p>It is shown in <ref type="bibr" target="#b9">[10]</ref> that if (? i ) i=1,...,m and (? j ) j=1,...,n are the eigenvalues of G and H respectively, then the eigenvalues of the Cartesian product graph G ? H are ? i + ? j for all possible eigenvalues ? i and ? j . Also, the eigenvectors associated to the eigenvalue ? i + ? j are u i ? v j with u i an eigenvector of the Laplacian of G associated to the eigenvalue ? i and v j an eigenvector of the Laplacian of H associated to the eigenvalue ? j .</p><p>Finally, noticing that a lattice of shape </p><formula xml:id="formula_47">N 1 ? N 2 ? ... ? N n is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Radius 1 convolution kernels in a grid</head><p>In this section we show any radius 1 convolution kernel can be obtained as a linear combination of the B dx (?? i ) and B av (?? i ) matrices for the right choice of Laplacian eigenvectors ? i . First we show this can be done for 1-d convolution kernels. Theorem C.2. On a path graph, any 1D convolution kernel of size 3 k is a linear combination of the aggregators B av , B dx and the identity I.</p><p>Proof. Recall from the previous proof that the first non zero eigenvalue of the path graph P N has associated eigenvector ? 1 (i) = cos( ?i N -? 2N ). Since this is a monotone decreasing function in i, the i-th row of ?? 1 will be (0, ..., 0, s i-1 , 0, -s i+1 , 0, ..., 0) with s i-1 and s i+1 &gt; 0. We are trying to solve (aB av + bB dx + cId) i,: = (0, ..., 0, x, y, z, 0, ..., 0) with x, y, z, in positions i -1, i and i + 1. This simplifies to solving</p><formula xml:id="formula_48">a 1 s L 1 |s| + b 1 s L 2</formula><p>s + c(0, 1, 0) = (x, y, z) with s = (s i-1 , 0, -s i+1 ), which always has a solution because s i-1 , s i+1 &gt; 0.</p><p>Theorem C.3 (Generalization radius-1 convolutional kernel in a grid). Let ? be the n-dimensional lattice as above and let ? j be the eigenvectors of the Laplacian of the lattice as in theorem C.1. Then any radius 1 kernel k on ? is a linear combination of the aggregators B av (? i ), B dx (? i ) and I.</p><p>Proof. This is a direct consequence of C.2 obtained by adding n 1-dimensional kernels, with each kernel being in a different axis of the grid as per Lemma C.1. See figure <ref type="figure" target="#fig_1">2</ref>.5 for a visual example in 2D.</p><p>C.7 Proof for Theorem 2.7 (Generalization radius-R convolutional kernel in a lattice)</p><p>For an n-dimensional lattice, any convolutional kernel of radius R can be realized by a linear combination of directional aggregation matrices and their compositions.</p><p>Proof. For clarity, we first do the 2 dimensional case for a radius 2, then extended to the general case. Let k be the radius 2 kernel on a grid represented by the matrix</p><formula xml:id="formula_49">a 5?5 = ? ? ? ? ? 0 0 a -2,0 0 0 0</formula><p>a -1,-1 a -1,0 a -1,1 0 a 0,-2 a 0,-1 a 0,0 a 0,1 a 0,2 0 a 1,-1 a 1,0 a 1,1 0 0 0 a 2,0 0 0</p><formula xml:id="formula_50">? ? ? ? ?</formula><p>since we supposed the N 1 ? N 2 grid was such that N 1 &gt; N 2 , by theorem C.1, we have that ? 1 is depending only in the first variable x 1 and is monotone in x 1 . Recall from C.1 that</p><formula xml:id="formula_51">? 1 (i) = cos ?i N 1 + ? 2N 1</formula><p>The vector N1 ? ? arccos(? 1 ) will be denoted by F 1 in the rest. Notice all entries of F 1 are 0 or ?1. Denote by F 2 the gradient vector N2 ? ? arccos(? k ) where ? k is the eigenvector given by theorem C.1 that is depending only in the second variable x 2 and is monotone in x 1 and recall</p><formula xml:id="formula_52">? k (i) = cos ?i N 2 + ? 2N<label>2</label></formula><p>For a matrix B, let B ? the positive/negative parts of B, ie matrices with positive entries such that = B + -B -. Let B r1 be a matrix representing the radius 1 kernel with weights a 3?3 = 0 a -1,0 0 a 0,-1 a 0,0 a 0,1 0 a 1,0 0</p><p>The matrix B r1 can be obtained by theorem C.3. Then the radius 2 kernel k is defined by all the possible combinations of 2 positive/negative steps, plus the initial radius-1 kernel.</p><formula xml:id="formula_53">B r2 = -2?i,j?2<label>|i|+|j|=2</label></formula><p>a i,j (F sgn(i) 1</p><p>) |i| (F sgn(j) 2 Aggregator following the steps defined in V with F j = Nj ? ? arccos ? j ,? j the eigenvector with lowest eigenvalue only dependent on the j-th variable and given in theorem C.1 and is the matrix multiplication. V represents all the choices of walk {v 1 , v 2 , ..., v n } in the direction of the fields {F 1 , F 2 , ..., F n }. For example, V = {3, 1, 0, -2} has a radius R = 6, with 3 steps forward of F 1 , 1 step forward of F 2 , and 2 steps backward of F 4 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Possible directional flows in different types of graphs. The node coloring is a potential map and the edges represent the gradient of the potential with the arrows in the direction of the flow. The first 3 columns present the arcosine of the normalized eigenvectors (acos ?) as node coloring, and their gradients represented as edge intensity. The last column presents examples of inductive bias introduced in the choice of direction. (a) The eigenvectors 1 and 2 are the horizontal and vertical flows of the grid. (b) The eigenvectors 1 and 2 are the flow in the longest and second longest directions. (c) The eigenvectors 1, 2 and 3 flow respectively in the South-North, suburbs to city center and West-East directions. We ignore ? 0 since it is constant and has no direction.</figDesc><graphic url="image-1.png" coords="3,72.00,72.00,467.98,166.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of how the directional aggregation works at a node n v , with the arrows representing the direction and intensity of the field F .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>CNN equivalent on image ? ??? , with ? &gt; ? ; ?%? ? 0 Graph aggregation 1 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Fine tuned results of the DGN model against other models from [9] and<ref type="bibr" target="#b6">[7]</ref>: GCN<ref type="bibr" target="#b17">[18]</ref>, GraphSage<ref type="bibr" target="#b11">[12]</ref>, GIN<ref type="bibr" target="#b32">[33]</ref>, GAT<ref type="bibr" target="#b30">[31]</ref>, MoNet<ref type="bibr" target="#b27">[28]</ref>, GatedGCN<ref type="bibr" target="#b1">[2]</ref> and PNA<ref type="bibr" target="#b6">[7]</ref>. All the models are given a parameter budget ? 100k. In ZINC we used aggregators {mean, dx 1 , max, min}, in PATTERN {mean, dx 1 , av 1 } and in CIFAR10 {mean, dx 1 , dx 2 , max}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Accuracy of the various models using the complex architecture and trained on 10% of the CIFAR10 training set (4.5k images) with various ranges of rotation angles. An angle of x corresponds to a rotation of the kernel of a random angle sampled uniformly in (-x?, x?) using definition 4 with F 1,2 being the gradient of the horizontal/vertical coordinates. The mean baseline model is not affected by the augmentation since it does not use the underlining vector field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>C. 2</head><label>2</label><figDesc>Proof for Theorem 2.2 (Directional derivative) Suppose F have rows of unit L 1 norm. The operation y = B dx ( F )x is the centered directional derivative of x in the direction of F , in the sense of equation 4, i.e. y = D F x = F -diag j F:,j x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>really the Cartesian product of path graphs of length N 1 up to N n , we conclude that there are eigenvalues 2 -2 cos ? Ni . Denoting by 1 Nj the vector in R Nj with only ones as coordinates, then the eigenvector associated to the eigenvalue 2 -2 cos ? Ni is1 N1 ? 1 N2 ? ... ? x 1,Ni ? ... ? 1 Nnwhere x 1,Ni is the eigenvector of the Laplacian of P Ni associated to its first non-zero eigenvalue. 2 -2 cos ? Ni .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>possible single-steps with sgn the sign function sgn(i) = + if i ? 0 andif i &lt; 0. The matrix B r2 then realises the kernel a 5?5 .We can further extend the above construction to N dimension grids and radius R kernels kV ={v1,v2,...,v N }?N n ||V || L 1 ?R -R?vi?RAny choice of walk V with at most R-steps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Direc?onal smoothing aggrega?on ? ?? ? ? Direc?onal deriva?ve aggrega?on ? ?? ? ? Graph features focused on the neighbourhood of ? ? ?:</head><label></label><figDesc>Features of the node receiving the message ? 1,2,3 : Features of the neighbouring nodes ? ?,? : Direc?onal vector field between the node ? and ?</figDesc><table><row><cell>? ?,?1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?,?2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?,?3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Weighted forward deriva?ve with ? 1</cell><cell>+</cell><cell>Weighted backward deriva?ve with ? 2 +</cell><cell>Weighted backward deriva?ve with ? 3</cell></row><row><cell cols="4">Sum of the absolute weights</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>edge features Edge features No edge features No edge features Edge features</figDesc><table><row><cell></cell><cell>MAE</cell><cell>MAE</cell><cell>% acc</cell><cell>% acc</cell><cell>% acc</cell></row><row><cell>GCN</cell><cell>0.469 ?0.002</cell><cell></cell><cell>65.880 ?0.074</cell><cell>54.46 ?0.10</cell><cell></cell></row><row><cell>GraphSage</cell><cell>0.410 ?0.005</cell><cell></cell><cell>50.516 ?0.001</cell><cell>66.08 ?0.24</cell><cell></cell></row><row><cell>GIN</cell><cell>0.408 ?0.008</cell><cell></cell><cell>85.590 ?0.011</cell><cell>53.28 ?3.70</cell><cell></cell></row><row><cell>GAT</cell><cell>0.463 ?0.002</cell><cell></cell><cell>75.824 ?1.823</cell><cell>65.48 ?0.33</cell><cell></cell></row><row><cell>MoNet</cell><cell>0.407 ?0.007</cell><cell></cell><cell>85.482 ?0.037</cell><cell>53.42 ?0.43</cell><cell></cell></row><row><cell>GatedGCN</cell><cell>0.422 ?0.006</cell><cell>0.363 ?0.009</cell><cell>84.480 ?0.122</cell><cell>69.19 ?0.28</cell><cell>69.37 ?0.48</cell></row><row><cell>PNA</cell><cell>0.320 ?0.032</cell><cell>0.188 ?0.004</cell><cell>86.567 ?0.075</cell><cell>70.46 ?0.44</cell><cell>70.47 ?0.72</cell></row><row><cell>DGN</cell><cell>0.219 ?0.010</cell><cell>0.168 ?0.003</cell><cell>86.680 ?0.034</cell><cell>72.70 ?0.54</cell><cell>72.84 ?0.42</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>The authors thank <rs type="person">Luca Cavalleri</rs> for providing access and support for a distributed computing framework used for running experiments.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b2">3</ref> <p>Proof for Theorem 2.3 (K-Gradient of the low frequency eigenvectors) Let ? i and ? i be the eigenvalues and eigenvectors of the normalized Laplacian of a connected graph L norm and let a, b be the nodes that have a farthest K-walk distance. Suppose that ? 2 ? 1 , then the optima of ? 1 approximates a, b.</p><p>Proof. First we need the following proposition:</p><p>Proposition 1 (K-walk distance matrix). The K-walk distance matrix P associated with a graph is the matrix such that (P ) i,j = d K (v i , v j ) can be written as</p><p>A is the random walk matrix.</p><p>Let's define W = D -1 A the random walk matrix of the graph.</p><p>First, we are going to show that W is jointly diagonalizable with L norm and we are going to relate its eigenvectors ? i and its eigenvalues ? i with the ones of W .</p><p>Indeed, L sym is a symmetric real matrix which is semi-positive definite diagonalizable by the spectral theorem. Since the matrix L norm is similar to</p><p>, a positive definite matrix, L norm is diagonalizable and semi-positive definite.</p><p>By</p><p>the random walk matrix is jointly diagonalizable with the random walk Laplacian. Also their eigenvalues and eigenvectors are related to each other by ? i = ? n+1-i and ? i = 1 -? n+1-i</p><p>Moreover, the constant eigenvector associated with eigenvalue 0 of the Random walk Laplacian, is the eigenvector associated with the highest eigenvalue of the Random walk matrix and by the formula obtained,</p><p>Now, we are going to approximate the K-walk distance matrix P using the 2 eigenvectors of the Random walk matrix associated with the highest eigenvalues.</p><p>By Proposition 1 we have that P = K p=1 W p , which can be written as</p><p>by eigen-decomposition.</p><p>Since ? n-i = 1 -? i and ? 2 ? 1 , we have that ? n-1 ? n-2 , hence we can approximate</p><p>where ? = K p=1 (1 -? 1 ) p is a positive constant. Now we are going to show that the farthest nodes with respect to the K-walk distance are the ones associated with the highest and lowest value of ? 1 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<title level="m">Residual gated graph convnets</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Yau</surname></persName>
		</author>
		<title level="m">Discrete green&apos;s functions</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Discrete green&apos;s functions</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Yau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="191" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Spectral Graph Theory. CBMS Regional Conference Series. Conference Board of the mathematical sciences</title>
		<meeting><address><addrLine>U.S.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>CBMS Conference on Recent Advances in Spectral Graph Theory, National Science Foundation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Vishwaraj</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Do</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eun</forename></persName>
		</author>
		<title level="m">Fiedler vector approximation via interacting random walks</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algebraic connectivity of graphs</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Fiedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Czechoslovak Mathematical Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="298" to="305" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Graph Representation Learning</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">How much position information do convolutional neural networks encode?</title>
		<author>
			<persName><forename type="first">Amirul</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Bruce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janek</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4204" to="4214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hy</forename><surname>Truong Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02144</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An iteration method for the solution of the eigenvalue problem of linear differential and integral operators</title>
		<author>
			<persName><forename type="first">Cornelius</forename><surname>Lanczos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950">1950</date>
			<publisher>United States Governm. Press Office Los</publisher>
			<pubPlace>Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">CayleyNets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Laplace-beltrami eigenfunctions towards an algorithm that &quot;understands&quot; geometry</title>
		<author>
			<persName><forename type="first">B</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Shape Modeling and Applications 2006 (SMI&apos;06)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="13" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10943" to="10953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09902</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">An overview of early vision in InceptionV1</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<idno>00024.002</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graph convolutional neural networks via motif-based attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiran</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senzhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanxing</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph wavelet neural network</title>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
