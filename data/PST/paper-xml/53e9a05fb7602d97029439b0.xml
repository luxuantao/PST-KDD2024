<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Eye Movements as Implicit Relevance Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Georg</forename><surname>Buscher</surname></persName>
							<email>georg.buscher@dfki.de</email>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
							<email>andreas.dengel@dfki.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. for Knowledge-Based Systems</orgName>
								<orgName type="institution">University of Kaiserslautern</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. for Knowledge-Based Systems University of Kaiserslautern And Knowledge Management Dept. DFKI GmbH</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Ludger van Elst Knowledge Management Dept. DFKI GmbH</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Eye Movements as Implicit Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E6C4D85E2B74722F47A445EC16A292F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Eye tracking</term>
					<term>relevance feedback</term>
					<term>reading and skimming detection</term>
					<term>relevance prediction Algorithms</term>
					<term>Experimentation</term>
					<term>Measurement H.5.2 [User Interfaces]: Input devices and strategies</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reading detection is an important step in the process of automatic relevance feedback generation based on eye movements for information retrieval tasks. We describe a reading detection algorithm and present a preliminary study to find expressive eye movement measures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Eye trackers as input devices come more and more into the focus of computer science research. An especially interesting area of application is their usage for diagnostic purposes. In this paper, we report on started work that generally focuses on using gaze data to automatically generate relevance feedback for documents in information retrieval tasks, e.g. web search.</p><p>In information retrieval it is well known that relevance feedback can significantly improve the search process.</p><p>The typical relevance feedback scenario is as follows:</p><p>The user tells the search engine which of the results for a given query were relevant or irrelevant (i.e., explicit relevance feedback). The user feedback can be then used to find an improved query and enhance the ranking of the search results. However, since most users are reluctant to provide relevance feedback explicitly, much research is done on determining such relevance feedback implicitly from user behavior (cf. <ref type="bibr" target="#b3">[4]</ref>).</p><p>In this paper, we focus on our first steps in the process of automatically predicting relevance from eye movements. We describe an algorithm that detects and differentiates reading and skimming behavior. Based on the output of that algorithm we define and investigate the quality of a new eye-movement measure for determining the relevance of documents. The identified correlations between eye movement measures and explicit relevance feedback will be used for creating prediction methods in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Closely Related Work</head><p>The idea to use gaze data for estimating relevance is not new and has been approached, e.g., in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>. However, there is a main drawback in their studies with regard to a practical application: None of them applied a preprocessing method like our reading and skimming detection algorithm. Instead, they operated on the raw gaze data to detect fixations, etc. Yet, if such a gaze analysis system should be put into practice then a preprocessing step is necessary: For example, when looking at a document one is not necessarily engaged in it; one might probably think about something else and stare at the document. But when one is reading or skimming a document, the probability is much higher that one is indeed engaged in the document. Therefore, having the goal of a practical application in mind, our relevance prediction method will include such a preprocessing filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reading Physiology</head><p>A lot of research has been done during the last one hundred years concerning eye movements while reading. The results being most important for reading and skimming detection are as follows (see <ref type="bibr" target="#b6">[7]</ref> for comprehensive overview): When reading silently the eye shows a very characteristic behavior composed of fixations and saccades. A fixation is a time interval of about 200-250 ms on average when the eye is steadily gazing at one point. A saccade is a rapid eye movement from one fixation to the next. The mean left-to-right saccade size during reading is 7-9 letter spaces. It depends on the font size and is relatively invariant concerning the distance between the eyes and the text. Approximately 10-15% of the eye movements during reading are regressions, i.e., movements to the left along the current line or to a previously read line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm for Reading Detection</head><p>This knowledge about eye movement behavior during reading can be exploited in order to detect whether a person is reading or skimming. The following algorithm has been tuned for a Tobii 1750 desk-mounted eye tracker which has a data generation frequency of 50 Hz and an accuracy of around 40 pixel at a resolution of 1280x1024. We use such a kind of eye tracking device since we have the goal of a practical application in mind and since we believe that such kinds of eye tracking devices might become widespread in the future. As it does not require any head-mounted part and works (currently) after a quick calibration, those kinds have the potential to be used in normal office environments.</p><p>The general idea of the algorithm is as follows: First, fixations are detected. Second, the transitions from one fixation to the next are classified resulting in so-called features. Third, scores associated with the features are accumulated. Finally, it is determined whether thresholds for "reading" and "skimming" behavior are exceeded. If this is the case, the respective most plausible behavior is detected.</p><p>The idea of the algorithm is related to that of <ref type="bibr" target="#b2">[3]</ref>. However, some major modifications have been introduced, primarily concerning the detection of fixations, the accumulation strategy, and the differentiation between reading and skimming behavior. In the following, we describe the steps of the algorithm in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fixation Detection</head><p>The fixation detection works in two steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">A new fixation is detected if 4 successive nearby</head><p>gaze locations from the eye tracker are accumulated (compare figure <ref type="figure" target="#fig_0">1</ref>, locations 1-4). Four gaze locations at 50 Hz correspond to a duration between 80 and 100 ms. This is the minimum fixation duration according to the literature (see above). Gaze points are considered nearby when they fit together in a rectangle of 30x30 pixel.</p><p>2. For any further gaze location generated by the eye tracker, it is checked whether it fits in a 50x50 pixel rectangle together with all gaze locations already belonging to the current fixation. If yes, then the new gaze location will be assigned to the current fixation (fig. <ref type="figure" target="#fig_3">4</ref>, locations 5-7, 10). If no, then it is either ignored as an outlier (e.g., in case that the one of the three next gaze location belongs to the current fixation, locations 8, 9), or it is used to-gether with three nearby gaze points as initiator of a new fixation (according to step 1, locations 11-14). The slightly larger rectangle in this step, allows tolerating noise from the eye tracker and very small eye movements like microsaccades and drifts.</p><p>If there are at least 4 successive gaze locations that cannot be merged with the current fixation, the fixation has ended. Then it is propagated to the next processing step of the reading detection algorithm. In this way, up to 3 gaze location "outliers" can be ignored, that might occur from time to time due to eye tracker inaccuracies or light reflections in the user's glasses (if any). Blinking of the eyes is treated as the end of a fixation. If a fixation has ended, all the contributing gaze locations are averaged to get one specific fixation coordinate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification of Fixation Transitions</head><p>Each transition from one fixation to the next is classified according to its length and direction. This results in features that occur more or less often during reading or skimming (e.g., read forward, skim forward, regression, reset jump). A list of all possible features is given in figure <ref type="figure" target="#fig_1">2</ref>. Because the length of a typical saccade during reading depends on the font size, this transition classification method is based on letter space distances and not on absolute pixel distances. Information about the font size of the currently fixated text is received by the screen OCR tool OCRopus <ref type="bibr" target="#b4">[5]</ref>: it gets a small screen shot around the current fixation as input and returns the font size of the nearest text line as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reading and Skimming Detection</head><p>The detection of reading or skimming behavior is done on the basis of feature sequences that are separated by reset jump features or unrelated moves (compare figure <ref type="figure" target="#fig_1">2</ref>). This is similar to the method described by <ref type="bibr" target="#b0">[1]</ref>.</p><p>To differentiate between reading and skimming behavior, two independent detectors analyze these sequences of features and accumulate the associated scores. Be-cause the distribution of the features during reading and skimming behavior is different, the reading detector r uses different scores s r than the skimming detector s: s s . The concrete scores are motivated by the literature (Rayner <ref type="bibr" target="#b6">[7]</ref>, Campbell and Maglio <ref type="bibr" target="#b2">[3]</ref>).</p><p>For each feature sequence represented by a multiset DF of contained features and for each detector d∈ {r, s}, it is tested whether there is enough evidence for reading or skimming behavior, respectively. That is done by comparing the accumulated scores to detector-specific thresholds t d , i.e., testing whether</p><formula xml:id="formula_0">∑ ∈ &gt; DF f d d t f s ) (</formula><p>for each d (we use t r = 30 and t s = 20). If only one of the detectors has accumulated enough evidence, then the appropriate behavior is detected. Otherwise, if both detectors have found enough evidence for a text row, the more plausible behavior is determined simply by comparing the accumulated scores of the detectors with each other.</p><p>In figure <ref type="figure" target="#fig_2">3</ref> an exemplary result of the reading and skimming detection algorithm is shown. The circles represent the fixations, while their diameters correspond to the fixation durations. The classification result of the fixation transitions (i.e., detected features) is shown by the abbreviations on the connecting lines (R: read forward; S: skim forward; L: short regression; Reset: reset jump). Dashed lines mean that the feature sequence is more characteristic for skimming behavior. Likewise, solid lines stand for reading behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards Eye Movement Measures to Predict Relevance</head><p>The next step towards a method to predict relevance from eye movements is to find specific eye movement measures that are correlated to the users' explicit relevance feedback. Therefore, we designed a study where 19 participants had to rate explicitly 16 one screen long text documents according to their relevance to a given task. The rating scale had 4 categories: relevant (selected 112 times) and irrelevant (90 times) in the extremes and two intermediary categories (together 64 times, ignored in the following due to space constraints). The participants' eye movements while viewing the documents were analyzed and filtered by the reading detection algorithm.</p><p>Based on those filtered eye movements (i.e., movements that really belong to reading or skimming), we calculated, among others, the read-to-skimmed ratio. It is computed as the ratio of the length of all read lines to the length of all read or skimmed lines. Thus, it contains information about whether and to which extend different reading velocities have been applied on a page. The upper diagram of figure <ref type="figure" target="#fig_3">4</ref> the distribution of the explicit relevance ratings (only the categories relevant and irrelevant) over 20%-intervals of the read-toskimmed ratio. E.g., all the pages that got a read-toskimmed ratio between 0 and 20, around 8% of those pages were rated relevant and 26% were rated irrelevant. It has to be noted that for the upper diagram the eye movement data was merged across participants. Yet, it is well-known that there are individual differences in eye movements (e.g., see <ref type="bibr" target="#b6">[7]</ref>). Therefore, for the lower diagram of figure <ref type="figure" target="#fig_3">4</ref>, we applied a simple individual normalization: First, for each participant the individual minimum and maximum of the read-toskimmed ratio was determined (some people read more quickly on average than others). Next, the absolute values of the read-to-skimmed ratio were normalized with respect to the individual [min, max]-intervals. That resulted in a percentage for each absolute value stating its relative position in the individual interval. For example, if a participant only produced values between 60% and 100% for the read-to-skimmed ratio (i.e., a slow reader), then the individual [min, max]-interval would be [60, 100]. The specific read-to-skimmed  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>The preliminary analysis of our experiment shows that the algorithm for reading and skimming detection is very useful. Our eye movement measure, the read-toskimmed ratio, seems to be very well discriminating with respect to explicit relevance feedback. Next steps will be to analyze further measures based on the reading detection algorithm and to design binary classification tests to predict relevance from eye movements.</p><p>Finally, we aim at applying such a classification test to automatically generate relevance feedback in practical search applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>figure 1 .</head><label>1</label><figDesc>figure 1. Gaze locations produced by the eye tracker (illustrated by the circles; numbers indicate sequence) are agglomerated resulting in fixation frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>figure 2 .</head><label>2</label><figDesc>figure 2. The transitions from one fixation to the next are classified resulting in features. Detector-specific scores are associated with each feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>figure 3 .</head><label>3</label><figDesc>figure 3. Visualization of the result of the reading and skimming detection algorithm.</figDesc><graphic coords="5,230.88,121.26,453.00,117.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>figure 4 .</head><label>4</label><figDesc>figure 4. The read-to-skimmed ratio as eye movement measure.</figDesc><graphic coords="6,198.48,279.60,240.24,117.60" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Daniel Keysers and Christian Kofler for providing a customized version of OCRopus. This work was supported by the German Federal Ministry of Education, Science, Research and Technology (bmb+f), (Grant 01 IW F01, Project Mymory: Situated Documents in Personal Information Spaces).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">WebGazeAnalyzer: a system for capturing and analyzing web reading behavior using eye gaze</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;05</title>
		<meeting>CHI &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Measuring the utility of gaze detection for task modeling: A preliminary study</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guimbretire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A robust algorithm for reading detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Maglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PUI &apos;01</title>
		<meeting>PUI &apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Implicit feedback for inferring user preference: a bibliography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="18" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">an open source document analysis and OCR system</title>
		<author>
			<persName><surname>Ocropus</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/ocropus/)" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining eye movements and collaborative filtering for proactive information retrieval</title>
		<author>
			<persName><forename type="first">K</forename><surname>Puolamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salojärvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Savia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR &apos;05</title>
		<meeting>SIGIR &apos;05</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="146" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eye movements in reading and information processing: 20 years of research</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="372" to="422" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Inferring relevance from eye movements: Feature extraction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salojärvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Puolamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kovanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Helsinki Univ. of Technology, Publications in Computer and Information Science</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. A82</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
