<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Yum-Me: A Personalized Nutrient-Based Meal Recommender System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11-19">2018 November 19.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Pollak</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">West Loop Road</orgName>
								<orgName type="institution" key="instit1">Cornell Tech</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
								<address>
									<postCode>10044</postCode>
									<region>NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UCLA HONGJIAN YANG</orgName>
								<address>
									<addrLine>2 West Loop Road</addrLine>
									<postCode>4732, 90095, 10044</postCode>
									<settlement>Boelter Hall, Los Angeles</settlement>
									<region>CA, NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Cornell Tech</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">West Loop Road</orgName>
								<orgName type="institution" key="instit1">Cornell Tech</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
								<address>
									<postCode>10044</postCode>
									<region>NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">NICOLA DELL</orgName>
								<address>
									<addrLine>2 West Loop Road</addrLine>
									<postCode>10044</postCode>
									<region>NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">The Jacobs Institute</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Cornell Tech</orgName>
								<orgName type="institution" key="instit2">Cornell University SERGE BELONGIE</orgName>
								<address>
									<addrLine>2 West Loop Road</addrLine>
									<postCode>10044</postCode>
									<region>NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">Cornell Tech</orgName>
								<orgName type="institution" key="instit2">Cornell University CURTIS COLE</orgName>
								<address>
									<addrLine>505 East 70th Street, Helmsley Tower, 4th Floor</addrLine>
									<postCode>10021</postCode>
									<region>NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Weill Cornell Medical College</orgName>
								<address>
									<addrLine>Cornell University DEBORAH ESTRIN 2 West Loop Road</addrLine>
									<postCode>10044</postCode>
									<region>NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution" key="instit1">Cornell Tech</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Yum-Me: A Personalized Nutrient-Based Meal Recommender System</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-11-19">2018 November 19.</date>
						</imprint>
					</monogr>
					<idno type="MD5">097DA3D29D6DE05147BCC2526AFAAEE9</idno>
					<idno type="DOI">10.1145/3072614</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts</term>
					<term>Information systems ‚Üí Information retrieval</term>
					<term>Users and interactive retrieval</term>
					<term>Personalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nutrient-based meal recommendations have the potential to help individuals prevent or manage conditions such as diabetes and obesity. However, learning people's food preferences and making recommendations that simultaneously appeal to their palate and satisfy nutritional expectations are challenging. Existing approaches either only learn high-level preferences or require a prolonged learning period. We propose Yum-me, a personalized nutrient-based meal recommender system designed to meet individuals' nutritional expectations, dietary restrictions, and fine-grained food preferences. Yum-me enables a simple and accurate food preference profiling procedure via a visual quiz-based user interface and projects the learned profile into the domain of nutritionally appropriate food options to find ones that will appeal to the user. We present the design and implementation of Yum-me and further describe and evaluate two innovative contributions. The</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Healthy eating plays a critical role in our daily well-being and is indispensable in preventing and managing conditions such as diabetes, high blood pressure, cancer, mental illnesses, asthma, and so on <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">41]</ref>. In particular, for children and young people, the adoption of healthy dietary habits has been shown to be beneficial to early cognitive development <ref type="bibr" target="#b43">[46]</ref>. Many applications designed to promote healthy behaviors have been proposed and studied <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Among those applications, the studies and products that target healthy meal recommendations have attracted much attention <ref type="bibr">[40,</ref><ref type="bibr" target="#b53">56]</ref>. Fundamentally, the goal of these systems is to suggest food alternatives that cater to individuals' health goals and help users develop healthy eating behavior by following the recommendations <ref type="bibr" target="#b64">[67]</ref>. Akin to most recommender systems, learning users' preferences is a necessary step in recommending healthy meals that users are more likely to find desirable <ref type="bibr" target="#b64">[67]</ref>. However, the current food preference elicitation approaches, including (1) on-boarding surveys and (2) food journaling, still suffer from major limitations, as discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢</head><p>Preferences elicited by surveys are coarse-grained. A typical on-boarding survey asks a number of multi-choice questions about general food preferences. For example, PlateJoy [40], a daily meal planner app, elicits preferences for healthy goals and dietary restrictions with the following questions:</p><p>1.</p><p>How do you prefer to eat? No restrictions, dairy free, gluten free, kid friendly, pescatarian, paleo, vegetarian‚Ä¶</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Are there any ingredients you prefer to avoid? avocado, eggplant, eggs, seafood, shellfish, lamb, peanuts, tofu‚Ä¶.</p><p>While the answers to these questions can and should be used to create a rough dietary plan and avoid clearly unacceptable choices, they do not generate meal recommendations that cater to each person's fine-grained food preferences, and this may contribute to their lower-than-desired recommendation-acceptance rates, as suggested by our user testing results.</p><p>‚Ä¢ Food journaling approach suffers from cold-start problem and is hard to maintain. For example, Nutrino <ref type="bibr">[38]</ref>, a personal meal recommender, asks users to log their daily food consumption and learn users' fine-grained food preferences. As is typical of systems relying on user-generated data, food journaling suffers from the cold-start problem, where recommendations cannot be made or are subject to low accuracy when the user has not yet generated a sufficient amount of data. For example, a previous study showed that an active food-journaling user makes about 3.5 entries per day <ref type="bibr" target="#b12">[13]</ref>. It would take a nontrivial amount of time for the system to acquire sufficient data to make recommendations, and the collected samples may be subject to sampling biases as well <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>. Moreover, the photo food journaling of all meals is a habit difficult to adopt and maintain and therefore is not a generally applicable solution to generate complete food inventories <ref type="bibr" target="#b12">[13]</ref>.</p><p>To tackle these limitations, we develop Yum-me, a meal recommender that learns finegrained food preferences without relying on the user's dietary history. We leverage people's apparent desire to engage with food photos 1 to create a more user-friendly medium for asking visually based diet-related questions. The recommender learns users' fine-grained food preferences through a simple quiz-based visual interface <ref type="bibr" target="#b56">[59]</ref> and then attempts to generate meal recommendations that cater to the user's health goals, food restrictions, as well as personal appetite for food. It can be used by people who have food restrictions, such as vegetarian, vegan, kosher, or halal. Particularly, we focus on the health goals in the form of nutritional expectations, for example adjusting calories, protein, and fat intake. The mapping from health goals to nutritional expectations can be accomplished by professional nutritionists or personal coaches and is out of the scope of this article. We leave it as future work. In designing the visual interface <ref type="bibr" target="#b56">[59]</ref>, we propose a novel online learning framework that is suitable for learning users' potential preferences for a large number of food items while requiring only a modest number of interactions. Our online learning approach balances exploitation-exploration and takes advantage of food similarities through preference-propagation among locally connected graphs. To the best of our knowledge, this is the first interface and algorithm that learns users' food preferences through real-time interactions without requiring specific diet history information.</p><p>For such an online learning algorithm to work, one of the most critical components is a robust food image analysis model. Towards that end, as an additional contribution of this work, we present a novel, unified food image analysis model, called FoodDist. Based on deep convolutional networks and multi-task learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>, FoodDist is the best-of-its-kind Euclidean distance embedding for food images, in which similar food items have smaller 1 Collecting, sharing, and appreciating high-quality, delicious-looking food images is growing in popularity in our everyday lives. For example, food photos are immensely popular on Instagram (#food has over 177M posts and #foodporn has over 91M posts at the time of writing).</p><p>distances while dissimilar food items have larger distances. FoodDist allows the recommender to learn users' fine-grained food preferences accurately via similarity assessments on food images. Besides preference learning, FoodDist can be applied to other food-image-related tasks, such as food image detection, classification, retrieval, and clustering. We benchmark FoodDist with the Food-101 dataset <ref type="bibr" target="#b5">[6]</ref>, the largest dataset for food images. The results suggest the superior performance of FoodDist over prior approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b56">59</ref>]. FoodDist will be made available on Github on publication.</p><p>We evaluate our online learning framework in a field study of 227 anonymous users and we show that it is able to predict the food items that a user likes or dislikes with high accuracy. Furthermore, we evaluate the desirability of Yum-me recommendations end to end through a 60-person user study, where each user rates the meal recommendations made by Yum-me relative to those made using a traditional survey-based approach. The study results show that, compared to the traditional survey based recommender, our system significantly improves the acceptance rate of the recommended healthy meals by 42.63%. We see Yumme as a complement to the existing food preference elicitation approaches that further filters the food items selected by a traditional onboarding survey based on users' fine-grained taste for food and allows a system to serve tailored recommendations on the first use of the system. We discuss some potential use cases in Section 7.</p><p>The rest of the article is organized as follows. After discussing related work in Section 2, we introduce the structure of Yum-me and our backend database in Section 3. In Section 4, we describe the algorithmic details of the proposed online learning algorithm, followed by the architecture of FoodDist model in Section 5. The evaluation results of each component, as well as the recommender are presented in Section 6. Finally, we discuss the limitations, potential impact, and real-world applications in Section 7 and conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Our work benefits from, and is relevant to, multiple research threads: (1) healthy meal recommender system, (2) cold-start problem and preference elicitation, (3) pairwise algorithms for recommendation, and (4) food image analysis, which will be surveyed in detail next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Healthy Meal Recommender System</head><p>Traditional food and recipe recommender systems learn users' dietary preferences from their online activities, including ratings <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>, past recipe choices <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b48">51]</ref>, and browsing history <ref type="bibr">[38,</ref><ref type="bibr" target="#b51">54,</ref><ref type="bibr" target="#b53">56]</ref>. For example, the authors of Reference <ref type="bibr" target="#b48">[51]</ref> build a social navigation system that recommends recipes based on the previous choices made by the user; the authors of Reference <ref type="bibr" target="#b53">[56]</ref> propose to learn a recipe similarity measure from crowd cardsorting and make recommendations based on the self-reported meals; and the authors of References <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref> generate healthy meal plans based on user's ratings towards a set of recipes and the nutritional requirements calculated for the person. In addition, previous recommenders also seek to incorporate users' food consumption histories recorded by the food logging and journaling systems (e.g. taking food images <ref type="bibr" target="#b12">[13]</ref> or writing down ingredients and meta-information <ref type="bibr" target="#b53">[56]</ref>).</p><p>The above systems, while able to learn users' detailed food preference, share a common limitation; that is, they need to wait until a user generates enough data before their recommendations can be effective for this user (i.e., the cold-start problem). Therefore, most commercial applications, for example, Zipongo [68] and Shopwell <ref type="bibr" target="#b44">[47]</ref> adopt onboarding surveys to more quickly elicit users' coarse-grained food preferences. For instance, Zipongo's questionnaires [68] ask users about their nutrient intake, lifestyle, habits, and food preferences and then make day-to-day and week-to-week healthy meals recommendations; ShopWell's survey <ref type="bibr" target="#b44">[47]</ref> is designed to avoid certain food allergens, for example, gluten, fish, corn, or poultry, and find meals that match to particular lifestyles, for example, healthy pregnancy or athletic training.</p><p>Yum-me fills a vacuum that the prior approaches were not able to achieve, namely a rapid elicitation of users' fine-grained food preferences for immediate healthy meal recommendations. Based on the online learning framework <ref type="bibr" target="#b56">[59]</ref>, Yum-me infers users' preferences for each single food item among a large food dataset and projects these preferences for general food items into the domain that meets each individual user's health goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cold-Start Problem and Preference Elicitation</head><p>To alleviate the cold-start problem mentioned above, several models of preference elicitation have been proposed in recent years. The most prevalent method of elicitation is to train decision trees to poll users in a structured fashion <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b63">66]</ref>. These questions are either generated in advance and remain static <ref type="bibr" target="#b39">[42]</ref> or change dynamically based on real-time user feedback <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b63">66]</ref>. Also, another previous work explores the possibility of eliciting item ratings directly from the user <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b60">63]</ref>. This process can either be carried at item level <ref type="bibr" target="#b60">[63]</ref> or within category (e.g., movies) <ref type="bibr" target="#b10">[11]</ref>.</p><p>The preference elicitation methods we mentioned above largely focus on the domain of movie recommendations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b60">63]</ref> and visual commerce <ref type="bibr" target="#b14">[15]</ref> (e.g., cars, cameras) where items can be categorized based on readily available metadata. When it comes to real dishes, however, categorical data (e.g., cuisines) and other associated information (e.g., cooking time) possess a much weaker connection to a user's food preferences. Therefore, in this work, we leverage the visual representation of each meal to better capture the process through which people make diet decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Pairwise Algorithms for Recommendation</head><p>Pairwise approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b41">44,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b54">57,</ref><ref type="bibr" target="#b55">58,</ref><ref type="bibr" target="#b57">60]</ref> are widely studied in recommender system literature. For example, Bayesian Personalized Ranking (BPR) <ref type="bibr" target="#b41">[44,</ref><ref type="bibr" target="#b42">45]</ref> and Weighted Approximate-Rank Pairwise (WARP) loss <ref type="bibr" target="#b54">[57]</ref>, which learn users' and items' representations from user-item pairs, are two representative and popular approaches in this category. Such algorithms have successfully powered many state-of-the-art systems <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b55">58]</ref>.</p><p>In terms of the cold-start scenario, the authors of Reference <ref type="bibr" target="#b37">[39]</ref> developed a pairwise method to leverage users' demographic information in recommending new items.</p><p>Compared to previous methods, our problem setting fundamentally differs in the sense that Yum-me elicits preferences in an active manner where the input is incremental and contingent on the previous decisions made by the algorithm, while prior work focuses on the static circumstances where the training data is available up-front, and there is no need for the system to actively interact with the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Food Image Analysis</head><p>The tasks of analyzing food images are very important in many ubiquitous dietary applications that actively or passively collect food images from mobile <ref type="bibr" target="#b12">[13]</ref> and wearable <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">52]</ref> devices. The estimation of food intake and its nutritional information is helpful to our health <ref type="bibr" target="#b36">[37]</ref> as it provides detailed records of our dietary history. Previous work mainly conducted the analysis by leveraging the crowd <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">53]</ref> and computer vision algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Noronha et al. <ref type="bibr" target="#b36">[37]</ref> crowdsourced nutritional analysis of food images by leveraging the wisdom of untrained crowds. The study demonstrated the possibility of estimating a meal's calories, fat, carbohydrates, and protein by aggregating the opinions from a large number of people; the authors of Reference <ref type="bibr" target="#b50">[53]</ref> elicit the crowd to rank the healthiness of several food items and validate the results against the ground truth provided by trained observers.</p><p>Although this approach has been justified to be accurate, it inherently requires human resources that restrict it from scaling to large number of users and providing real-time feedback.</p><p>To overcome the limitations of crowds and automate the analysis process, numerous articles discuss algorithms for food image analysis, including classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>, retrieval <ref type="bibr" target="#b30">[31]</ref>, and nutrient estimation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">49]</ref>. Most of the previous work <ref type="bibr" target="#b5">[6]</ref> leveraged handcrafted image features. However, traditional approaches were only demonstrated in special contexts, such as in a specific restaurant <ref type="bibr" target="#b3">[4]</ref> or for particular types of cuisine <ref type="bibr" target="#b29">[30]</ref>, and the performance of the models might degrade when they are applied to food images in the wild.</p><p>In this article, we designed FoodDist using deep convolutional neural network based multitask learning <ref type="bibr" target="#b7">[8]</ref>, which has been shown to be successful in improving model generalization power and performance in several applications <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b61">64]</ref>. The main challenge of multitask learning is to design appropriate network structures and sharing mechanisms across tasks. With our proposed network structure, we show that FoodDist achieves superior performance when applied to the largest available real-world food image dataset <ref type="bibr" target="#b5">[6]</ref> and when compared to prior approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">YUM-ME: PERSONALIZED NUTRIENT-BASED MEAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RECOMMENDATIONS</head><p>Our personalized nutrient-based meal recommendation system, Yum-me, operates over a given inventory of food items and suggests the items that will appeal to the users' palate and meet their nutritional expectations and dietery restrictions. A high-level overview of Yumme's recommendation process is shown in Figure <ref type="figure" target="#fig_0">1</ref> and briefly described as follows:</p><p>‚Ä¢</p><p>Step 1: Users answer a simple survey to specify their dietary restrictions and nutritional expectations. This information is used by Yum-me to filter food items and create an initial set of recommendation candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢</head><p>Step 2: Users then use an adaptive visual interface to express their fine-grained food preferences through simple comparisons of food items. The learned preferences are used to further re-rank the recommendations presented to them.</p><p>In the rest of this section, we describe our backend large-scale food database and aforementioned two recommendation steps: (1) a user survey that elicits user's dietary restrictions and nutritional expectations and (2) an adaptive visual interface that elicits users' fine-grained food preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Large Scale Food Database</head><p>To account for the dietary restrictions in many cultures and religions, or people's personal choices, we prepare a separate food database for each of the following dietary restrictions:</p><p>No restrictions, Vegetarian, Vegan, Kosher, Halal<ref type="foot" target="#foot_1">2</ref> -For each diet type, we pulled over 10,000 main dish recipes along with their images and metadata (ingredients, nutrients, tastes, etc.) from the Yummly API <ref type="bibr" target="#b59">[62]</ref>. The total number of recipes is around 50,000. To customize food recommendations for people with specific dietary restrictions, for example, vegetarian and vegan, we filter recipes by setting the allowedDiet parameter in the search API. For kosher or halal, we explicitly rule out certain ingredients by setting excludedIngredient parameter. The lists of excluded ingredients are shown below:</p><p>‚Ä¢ Kosher: pork, rabbit, horse meat, bear, shellfish, shark, eel, octopus, octopuses, moreton bay bugs, frog.</p><p>‚Ä¢ Halal: pork, blood sausage, blood, blood pudding, alcohol, grain alcohol, pure grain alcohol, ethyl alcohol.</p><p>One challenge in using a public food image API is that many recipes returned by the API contain non-food images and incomplete nutritional information. Therefore, we further filter the items with the following criteria: The recipe should have (1) nutritional information of calories, protein, and fat and (2) at least one food image. To automate this process, we build a binary classifier based on a deep convolutional neural network to filter out non-food images. As suggested in Reference <ref type="bibr" target="#b34">[35]</ref>, we treat the whole training set of Food-101 dataset <ref type="bibr" target="#b5">[6]</ref> as one generic food category and sampled the same number of images (75,750) from the ImageNet dataset <ref type="bibr" target="#b15">[16]</ref> as our non-food category. We took the pretrained VGG CNN model <ref type="bibr" target="#b45">[48]</ref> and replaced the final 1,000-dimensional softmax with a single logistic node. For the validation, we use the Food-101 testing dataset along with the same number of images sampled from ImageNet <ref type="bibr" target="#b24">(25,</ref><ref type="bibr">250)</ref>. We trained the binary classifier using the Caffe framework <ref type="bibr" target="#b26">[27]</ref>, and it reached 98.7% validation accuracy. We applied the criteria to all the datasets, and the final statistics are shown in Table <ref type="table" target="#tab_1">I</ref>.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the visualizations of the collected datasets. For each of the recipe images, we embed it into a 1,000-dimensional feature space using FoodDist (described later in Section 5) and then project all the images onto a two-dimensional (2D) plane using t-Distributed Stochastic Neighbor Embedding(t-SNE) <ref type="bibr" target="#b52">[55]</ref>. For visibility, we further divide the 2D plane into several blocks, from each of which we sample a representative food image residing in that block to present in the figure. Figure <ref type="figure" target="#fig_1">2</ref> demonstrates the large diversity and coverage of the collected datasets. Also, the embedding results clearly demonstrate the effectiveness of FoodDist in grouping similar food items together while pushing dissimilar items away. This is important to the performance of Yum-me, as discussed in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">User Survey</head><p>The user survey is designed to elicit user's high-level dietary restrictions and nutritional expectations. Users can specify their dietary restrictions among the five categories mentioned-above and indicate their nutritional expectations in terms of the desired amount of calories, protein, and fat. We choose these nutrients for their high relevance to many common health goals, such as weight control <ref type="bibr" target="#b17">[18]</ref>, sports performance <ref type="bibr" target="#b6">[7]</ref>, and so on. We provide three options for each of these nutrients, including reduce, maintain, and increase.</p><p>The user's diet type is used to select the appropriate food dataset, and the food items in the dataset are further ranked by their suitability to users' health goals based on the nutritional facts.</p><p>To measure the suitability of food items given users' nutritional expectations, we rank the recipes in terms of different nutrients in both ascending and descending order, such that each recipe is associated with six ranking values, that is, r calories , a , r calories , d , r protein , a , r protein , d , r fat , a , and r fat , d , where a and d stand for ascending and descending, respectively. The final suitability value for each recipe given the health goal is calculated as follows:</p><formula xml:id="formula_0">u = ‚àë n ‚àà ùïå Œ± n, a r n, a + ‚àë n ‚àà ùïå Œ± n, d r n, d ,<label>(1)</label></formula><p>where ùïå = calories, protein, fat . The indicator coefficient Œ± n,a = 1 ‚áî nutrient n is rated as reduce and Œ± n,d = 1 ‚áî nutrient n is rated as increase. Otherwise Œ± n,a = 0 and Œ± n,d = 0. If user's goal is to maintain all nutrients, then all recipes are given equal rankings. Eventually, given a user's responses to the survey, we rank the suitability of all the recipes in the corresponding database and select top-M items (around top 10%) as the candidate pool of healthy meals for this user. In our initial prototype, we set M = 500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Visual Interface</head><p>Based on the food suitability ranking, a candidate pool of healthy meals is created. However, not all the meals in this candidate pool will suit the user's palate. Therefore, we design an adaptive visual interface to further identify recipes that cater to the user's taste through eliciting their fine-grained food preferences. We propose to learn users' fine-grained food preferences by presenting users with food images and ask them to choose ones that look delicious.</p><p>Formally, the food preference learning task can be defined as follows: Given a large target set of food items ùïä, we represent user's preferences as a distribution over all the possible food items, that is, p = [p 1 , ‚Ä¶, p ùïä ], ‚àë i p i = 1 where each element p i denotes the user's favorable scale for item i. Since the number of items, ùïä , is usually quite large and intractable to elicit individually from the user, 3 the approach we take is to adaptively choose a specific and much smaller subset ùïç to present to the user and propagate the users' preferences for those items to the rest items based on their visual similarity. Specifically, as Figure <ref type="figure" target="#fig_0">1</ref> shows, the preference elicitation process can be divided into two phases:</p><p>Phase I: In each of the first two iterations, we present 10 food images and ask users to tap on all the items that look delicious to them.</p><p>Phase II: In each of the subsequent iterations, we present a pair of food images and ask users to either compare the food pair and tap on the one that looks delicious to them or tap on "Yuck" if neither of the items appeal to their taste.</p><p>To support the preference elicitation process, we design a novel exploration-exploitation online learning algorithm built on a state-of-the-art food image embedding model, which will be discussed in the Section 4 and Section 5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ONLINE LEARNING FRAMEWORK</head><p>We model the interaction between the user and our backend system at iteration t,</p><formula xml:id="formula_1">(t ‚àà ‚Ñõ + , t = 1, 2, ‚Ä¶, T)</formula><p>as Figure <ref type="figure" target="#fig_2">3</ref> shows. The symbols that will be used in our algorithms are defined as follows:</p><p>‚Ä¢ ùí¶ t : Set of food items that are presented to user at iteration t (ùí¶ 0 = √ò).</p><p>‚àÄk ‚àà ùí¶ t , k ‚àà ùïä,</p><p>‚Ä¢ ‚Ñí t-1 : Set of food items that are user prefer(select) among k k ‚àà ùí¶ t -1 .</p><p>‚Ñí t -1 ‚äÜ ùí¶ t -1 ;</p><formula xml:id="formula_2">‚Ä¢ p t = [p 1 t , ‚Ä¶, p ùïä t ]:</formula><p>User's preference distribution on all food items at iteration t, where ‚Äñp t ‚Äñ 1 = 1. p 0 is initialized as</p><formula xml:id="formula_3">p i 0 = 1 ùïä ; ‚Ä¢ ‚Ñ¨ t :</formula><p>Set of food images that have been already explored until iteration t (‚Ñ¨ 0 = √ò).</p><formula xml:id="formula_4">‚Ñ¨ i ‚äÜ ‚Ñ¨ j (i &lt; j); ‚Ä¢ ‚Ñ± = f (x 1 ), ‚Ä¶, f (x ùïä ) : Set of feature vectors of food images x i (i = 1, ‚Ä¶, ùïä )</formula><p>extracted by a feature extractor, denoted by f. We use FoodDist as the feature extractor. More details about FoodDist appear in Section 5.</p><p>3 The target set is often the whole food database that different applications use. For example, the size of Yummly database can be up to 1 million <ref type="bibr" target="#b59">[62]</ref>.</p><p>Based on the workflow depicted in Figure <ref type="figure" target="#fig_2">3</ref>, for each iteration t, the backend system updates vector p t-1 to p t and set ‚Ñ¨ t -1 to ‚Ñ¨ t based on users' selections ‚Ñí t-1 and previous image set ùí¶ t -1 . After that, it decides the set of images that will be immediately presented to the user (i.e., ùí¶ t ). Our food preference elicitation framework can be formalized in Algorithm. 1. The core procedures are update and select, which will be described in the following subsections for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALGORITHM 1</head><p>Food Preference Elicitation Framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">User State Update</head><p>Based on user's selections ‚Ñí t-1 and the image set ùí¶ t -1 1, the update module renews user's state from ‚Ñ¨ t -1 , p t -1 to ‚Ñ¨ t , p t . Our intuition and assumption behind following design is that people tend to have close preferences for similar food items.</p><p>Preference vector p t -Our strategy of updating preference vector p t is inspired by the Exponentiated Gradient Algorithm in bandit settings (EXP3) <ref type="bibr" target="#b2">[3]</ref>. Specifically, at iteration t, each p i t in vector p t is updated by</p><formula xml:id="formula_5">p i t p i t -1 √ó e Œ≤u i t -1 p i t -1 ,<label>(2)</label></formula><p>where Œ≤ is the exponentiated coefficient that controls update speed and</p><formula xml:id="formula_6">u t -1 = u 1 t -1 , ‚Ä¶, u ùïä t -1</formula><p>is the update vector used to adjust each preference value.</p><p>To calculate update vector u, we formalize the user's selection process as a data labeling problem <ref type="bibr" target="#b62">[65]</ref> where for item i ‚àà ‚Ñí t-1 , label y i t -1 = 1, and for item j ‚àà ùí¶ t -1 \‚Ñí t -1 , label</p><formula xml:id="formula_7">y j t -1 = -1. Thus the label vector y t -1 = y i t -1</formula><p>, ‚Ä¶, y ùïä t -1 provided by the user is</p><formula xml:id="formula_8">y i t -1 = 1 : i ‚àà ‚Ñí t -1 0 : i ‚àâ ùí¶ t -1 -1 : i ‚àà ùí¶ t -1 \‚Ñí t -1 .<label>(3)</label></formula><p>For update vector u, we expect that it is close to label vector y but with smooth propagation of label values to nearby neighbors (for convenience, we omit superscript that denotes current iteration). The update vector u can be regarded as a soften label vector compared with y. To make the solution more computationally tractable, for each item i with y i ‚â† 0, we construct a locally connected undirected graph G i as Figure <ref type="figure" target="#fig_3">4</ref> shows: ‚àÄ j ‚àà ùïä, add an edge (i, j) if ||f (x i ) -f (x j )|| ‚â§ Œ¥. The labels y i for vertices s j in graph G i are calculated as</p><formula xml:id="formula_9">y j i = 0( j = 1, ‚Ä¶, ùïä \i), y i i = y i .</formula><p>For each locally connected graph G i , we fix u i i value as u i i = y i i and propose the following regularized optimization method to compute other elements ( ‚àÄu j i , j ‚â† 1) of update vector u i , which is inspired by the traditional label propagation method <ref type="bibr" target="#b62">[65]</ref>. Consider the problem of minimizing following objective function Q(u i ):</p><formula xml:id="formula_10">min u i ‚àë j = 1, j ‚â† i ùïä wi j(y i i -u j i ) 2 + ‚àë j = 1, j ‚â† i ùïä (1 -wi j)(u j i -y j i ) 2 .<label>(4)</label></formula><p>In Equation ( <ref type="formula" target="#formula_10">4</ref>), w ij represents the similarity measure between food item s i and s j :</p><formula xml:id="formula_11">w i j = e - 1 2Œ± 2 f (x i ) -f (x j ) 2 : f (x i ) -f (x j ) ‚â§ Œ¥ 0 : f (x i ) -f (x j ) &gt; Œ¥ ,<label>(5)</label></formula><p>where,</p><formula xml:id="formula_12">Œ± 2 = 1 ùïä 2 ‚àë i, j ‚àà ùïä f (x i ) -f (x j ) 2</formula><p>The first term of the objective function Q(u i ) is the smoothness constraint, as the update value for similar food items should not change too much. The second term is the fitting constraint, which makes u i close to the initial labeling assigned by user (i.e., y i ). However, unlike in Reference <ref type="bibr" target="#b62">[65]</ref>, in our algorithm, the tradeoff between these two constraints is dynamically adjusted by the similarity between item i and j where similar pairs are weighed more with smoothness and dissimilar pairs are forced to be close to initial labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALGORITHM 2</head><p>User State Update Algorithm.</p><p>With Equation ( <ref type="formula" target="#formula_10">4</ref>) being defined, we can take the partial derivative of Q(u i ) with respect to different u j i as follows:</p><formula xml:id="formula_13">‚àÇQ(u i ) u j, j ‚â† i i = 2wi j(u j i -u i i ) + 2(1 -wi j)(u j i -y j i ) = 0.<label>(6)</label></formula><p>As u j i -y i i , then</p><formula xml:id="formula_14">u j i = wi ju i i = wi jy i i ( j = 1, 2, ‚Ä¶, ùïä ) . (<label>7</label></formula><formula xml:id="formula_15">)</formula><p>After all u i are calculated, the original update vector u is then the sum of u i , that is,</p><formula xml:id="formula_16">u = ‚àë i u i</formula><p>The pseudo code for the algorithm of updating preference vector is shown in Algorithm. 2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explored food image set ‚Ñ¨ t -To balance the exploitation and exploration in image</head><p>selection phase, we maintain a set ‚Ñ¨ t that keeps track of all similar food items that have already been visited by user and the updating rule for ‚Ñ¨ t is as follows</p><formula xml:id="formula_17">‚Ñ¨t ‚Ñ¨t -1 ‚à™ i ‚àà ùïä min j ‚àà ùí¶ t -1 f (x i ) -f (x j ) ‚â§ Œ¥ (8)</formula><p>With the algorithms designed for updating preference vector p t and explored image set ‚Ñ¨ t , the overall functionality of procedure update is shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Images Selection</head><p>After updating user state, the select module then picks food images that will be presented in the next round. To trade off between exploration and exploitation in our algorithm, we propose different images selection strategies based on current iteration t.</p><p>4.2.1. Food Exploration-For each of the first two iterations, we select ten different food images by using k-means++ <ref type="bibr" target="#b1">[2]</ref> algorithm, which is a seeding method used in k-means clustering and can guarantee that selected items are evenly distributed in the feature space.</p><p>For our use case, the k-means++ algorithm is summarized in Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Food Exploitation-Exploration-Starting</head><p>from the third iteration, users are asked to make pairwise comparisons between food images. To balance the Exploitation and Exploration, we always select one image from the area with higher preference value based on current p t and another one from unexplored area, that is, ùïä\‚Ñ¨ t . (Both selections are random in a given subset of food items.) With the above explanations, the image selection method we propose in this application is shown in Algorithm 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALGORITHM 3</head><p>k-means++ Algorithm for Exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALGORITHM 4</head><p>Images Selection Algorithm -select</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">FOODDIST: FOOD IMAGE EMBEDDING</head><p>Formally, the goal of FoodDist is to learn a feature extractor (embedding) f such that given an image x, f(x) projects it to an N dimensional feature vector for which the Euclidean distance to other such vectors will reflect the similarities between food images, as Figure <ref type="figure" target="#fig_4">5</ref> shows. Formally speaking, if image x 1 is more similar to image x 2 than image x 3 , then ‚Äñf(x 1 ) -f(x 2 )‚Äñ &lt; ‚Äñf(x 1 ) -f(x 3 )‚Äñ.</p><p>We build FoodDist based on recent advances in deep Convolutional Neural Networks (CNN), which provide a powerful framework for automatic feature learning. Traditional feature representations for images are mostly hand-crafted and were used with feature descriptors, such as Scale Invariant Feature Transform <ref type="bibr" target="#b33">[34]</ref>, which aims for invariance to changes in object scale and illumination, thereby improving the generalizability of the trained model. However, in the face of highly diverse image characteristics, the one-size-fitsall feature extractor performs poorly. In contrast, deep learning adapts the features to particular image characteristics and extracts features that are most discriminative in the given task <ref type="bibr" target="#b40">[43]</ref>.</p><p>As we present below, a feature extractor for food images can be learned through classification and metric learning, or through multitask learning, which concurrently performs these two tasks. We demonstrate that the proposed multitask learning approach enjoys the benefits of both classification and metric learning and achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Learning with Classification</head><p>One common way to learn a feature extractor for labeled data is to train a neural network that performs classification (i.e., mapping input to labels) and takes the output of a hidden layer as the feature representations; specifically, using a feedforward deep CNN with nlayers (as the upper half of the Figure <ref type="figure" target="#fig_5">6</ref> shows),</p><formula xml:id="formula_18">F(x) = g n g n -1 ‚Ä¶g i (‚Ä¶g 1 (x)‚Ä¶) , (9) f (x) = g n -1 ‚Ä¶g i (‚Ä¶g 1 (x)‚Ä¶) .<label>(10)</label></formula><p>Usually, the last few layers will be fully connected layers, and the last layer g n (.) is roughly equivalent to a linear classifier that is built on the features f(x) <ref type="bibr" target="#b25">[26]</ref>. Therefore, f(x) is discriminative in separating instances under different categorical labels, and the Euclidean distances between normalized feature vectors can reflect the similarities between images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Metric Learning</head><p>Differing from the classification approach, where the feature extractor is a by-product, metric learning proposes to learn the distance embedding directly from the paired inputs of similar and dissimilar examples. Prior work <ref type="bibr" target="#b56">[59]</ref> used a Siamese network to learn a feature extractor for food images. The structure of a Siamese network resembles that in Figure <ref type="figure" target="#fig_5">6</ref> but without the Class label, Fully connected, 101, and Softmax Loss layers. The inputs to the Siamese network are pairs of food images x 1 , x 2 . The images pass through CNNs with shared weights, and the output of each network is regarded as the feature representation, that is, f(x 1 ) and f(x 2 ), respectively. Our goal is for f(x 1 ) and f(x 2 ) to have a small distance value (close to 0) if x 1 and x 2 are similar food items; otherwise, they should have a larger distance value. The value of contrastive loss is then back-propagated to optimize the Siamese network:</p><formula xml:id="formula_19">‚Ñí(x 1 , x 2 , l) = 1 2 lD 2 + 1 2 (1 -l) max (0, m -D) 2 , (<label>11</label></formula><formula xml:id="formula_20">)</formula><p>where similarity label l ‚àà {0, 1} indicates whether the input pair of food items x 1 , x 2 are similar (l = 1 for similar, l = 0 for dissimilar), m &gt; 0 is the margin for dissimilar items, and D is the Euclidean distance between f(x 1 ) and f(x 2 ) in embedding space. Minimizing the contrastive loss will pull similar pairs together and push dissimilar pairs farther away (larger than a margin m), and it exactly matches the goal.</p><p>The major advantage of metric learning is that the network will be directly optimized for our final goal, that is, a robust distance measure between images. However, as shown in the model benchmarks, using the pairwise information alone does not improve the embedding performance as the process of sampling pairs loses the label information, which is arguably more discriminative than (dis)similar pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Multitask Learning: Concurrently Optimize Both Tasks</head><p>Both methods above have their pros and cons. Learning with classification leverages the label information, but the network is not directly optimized to our goal. As a result, although the feature vectors are learned to be separable in the linear space, the intra-and inter-categorical distances might still be unbalanced. On the other hand, metric learning is explicitly optimized for our final objective by pushing the distances between dissimilar food items apart beyond a margin m. Nevertheless, sampling the similar or dissimilar pairs loses valuable label information. For example, given a pair of items with different labels, we only consider the dissimilarity between the two categories they belong to, but overlook the fact that each item also differs from the remaining n -2 categories, where n is the total number of categories.</p><p>To leverage the benefits of both tasks, we propose a multitask learning design <ref type="bibr" target="#b25">[26]</ref> for FoodDist. The idea of multitask learning is to share part of the model across tasks to improve the generalization ability of the learned model <ref type="bibr" target="#b25">[26]</ref>. In our case, as Figure <ref type="figure" target="#fig_5">6</ref> shows, we share the parameters between the classification network and Siamese network and optimize them simultaneously. We use the base structure of the Siamese network and share the upper CNN with a classification network where the output of the CNN is fed into a cascade of a fully connected layer and a softmax loss layer. The final loss of the whole network is the weighted sum of the softmax loss ‚Ñí softmax and contrastive loss ‚Ñí contrastive :</p><formula xml:id="formula_21">‚Ñí = œâ‚Ñí softmax + (1 -œâ)‚Ñí contrastive (12)</formula><p>Our benchmark results (Section 6.2) suggest that the feature extractor built with multitask learning achieves the best of both worlds: It achieves the best performance for both classification and Euclidean distance-based retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>‚Ä¢ H1: Our online learning framework learns more accurate food preference profile than baseline approaches.</p><p>‚Ä¢ H2: FoodDist generates better similarity measure for food images than state-ofthe-art embedding models.</p><p>‚Ä¢ H3: Yum-me makes more accurate nutritionally appropriate meal recommendations than traditional surveys, as it integrates coarse-grained item filtering (provided by a survey) with fine-grained food preference learned through adaptive elicitation.</p><p>In this section, we first present user testing results for the online learning framework in Section 6.1 and then the offline benchmark FoodDist model with a large-scale real-world food image dataset in Section 6.2 and, finally, discuss the results of end-to-end user testing in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">User Testing for Online Learning Framework</head><p>To evaluate the accuracy of our online learning framework, we conducted a field study among 227 anonymous users recruited from social networks and university mailing lists. The experiment was approved by Institutional Review Board (ID: 1411005129) at Cornell University. All participants were required to use this system independently 3 times. Each time the study consisted of following two phases:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢</head><p>Training Phase. Users conducted the first T iterations of food image comparisons, and the system learnt and elicited preference vector p T based on the algorithms proposed in this article or baseline methods, which will be discussed later. We randomly picked T from set {5, 10, 15} at the beginning but made sure that each user experienced different values of T only once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢</head><p>Testing Phase. After T iterations of training, users entered the testing phase, which consisted of 10 rounds of pairwise comparisons. We picked testing images based on preference vector p T that were learned from online interactions: One of them was selected from food area that user liked (i.e., item with top 1% preference value) and the other one from the area that user disliked (i.e., item with bottom 1% preference value). Both of the images were picked randomly among unexplored food items. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">Prediction Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LE+RS:</head><p>This algorithm retains our method for user state update (LE) but Random Select images to present to user without any exploitation or exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OP+EE:</head><p>As each item is represented by a 1,000-dimensional feature vector, we can adopt the idea of regression to tackle this online learning problem (i.e., learning weight vector w such that w f(x i ) is higher for item i that user prefer). Hence, we compare our method with Online Perceptron algorithm that updates w whenever it makes error, that is, if y i w f(x i ) ‚â§ 0, then assign w ‚Üê w + y i w f(x i ), where y i is the label for item i (pairwise comparison is regarded as binary classification such that the food item that user select is labeled as +1 and otherwise -1). In this algorithm, we retain our strategy of images selection (i.e., EE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OP+RS:</head><p>The last algorithm is the Online Perceptron mentioned above but with Random images Selection strategy.</p><p>Of the 227 participants in our study, 58 of them finally used algorithm LE+EE and 57 used OP+RS. For the rest of the users (112), half of them <ref type="bibr" target="#b53">(56)</ref> tested OP+EE and the other half (56) tested LE+RS. Overall, the participants for different algorithms are totally random so the performances of different models are directly comparable.</p><p>After all users go through the training and testing phases, we calculate the prediction accuracy of each individual user and aggregate them based on the context that they encountered (i.e. the number of training iterations T and the algorithm settings mentioned above). The prediction accuracies and their cumulative distributions are shown in Figures <ref type="figure" target="#fig_6">7,</ref><ref type="figure" target="#fig_7">8</ref>, and 9 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Length effects of training iterations:</head><p>As shown in Figure <ref type="figure" target="#fig_6">7</ref> and Figure <ref type="figure" target="#fig_7">8</ref>, the prediction accuracies of our online learning algorithm are all significantly higher than the baselines. The algorithm performance is further improved with longer training period. As is clearly shown in Figure <ref type="figure" target="#fig_7">8</ref>, when the number of training iterations reaches 15, about half of the users will experience the prediction accuracy that exceeds 80%, which is fairly promising and decent considering the small number of interactions that the system elicited from scratch.</p><p>The results above justify that the online preference learning algorithm can adjust itself to explore users' preference area as more information is available from their choices. For the task of item-based food preference bootstrapping, our system can efficiently balance the exploration-exploitation while providing reasonably accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons across different algorithms:</head><p>As mentioned previously, we compared our algorithm with several obvious alternatives. As shown in Figure <ref type="figure" target="#fig_6">7</ref> and Figure <ref type="figure" target="#fig_8">9</ref>, none of these algorithms works very well and the accuracy of prediction is actually decreasing as the user provides more information. Additionally, as is shown in Figure <ref type="figure" target="#fig_8">9</ref>, our algorithm has particular advantages when users are making progress (i.e., the number of training iterations reaches 15). The reason why these techniques are not suited for our application is mainly due to the following limitations:</p><p>Random Selection: Within a limited number of interactions, random selection cannot maintain the knowledge that it has already learned about the user (exploitation) or explore unknown areas (exploration). In addition, it is more likely that the system will choose food items that are very similar to each other and thus hard for the user to make decisions. Therefore, after short periods of interactions, the system is messed up, and the performance degrades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Underfitting:</head><p>The algorithm that will possibly have the underfitting problem is the online perceptron (OP). For our application, each food item is represented by a 1,000-dimensional feature vector, and OP is trying to learn a separate hyperplane based on a limited number of training data. As each single feature is directly derived from a deep neural network, the linearity assumptions made by the perceptron might yield wrong predictions for the dishes that have not been explored before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">System</head><p>Efficiency-As another two aspects of online preference elicitation system, computing efficiency and user experience are also very important metrics for system evaluation. Therefore, we recorded the program execution time and user response time as a lens into the real-time performance of the online learning algorithm. As shown in Figure <ref type="figure" target="#fig_9">10</ref>(b), the program execution time is about 0.35s for the first two iterations and less than 0.025s for the iterations afterwards. 4 Also, according to Figure <ref type="figure" target="#fig_9">10</ref>(a), the majority of users can make their decisions in less than 15s for the task of comparison among 10 food images while the payload for the pairwise comparison is less than 2 to 3s. As a final cumulative metric for the system overhead, it is shown in Table <ref type="table" target="#tab_1">II</ref> that even for 15 iterations of training, users can typically complete the whole process within 53s, which further justify that our online learning framework is lightweight and user friendly in efficiently eliciting food preference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3.">User</head><p>Qualitative Feedback-After the study, some participants send us emails regarding their experiences towards the adaptive visual interface. Most of the comments reflect the participants' satisfactions and that our system is able to engage the user throughout the elicitation process, for example, "Now I'm really hungry and want a grilled cheese sandwhich!," "That was fun seeing tasty food at top of the morning," and "Pretty cool tool." However, they also highlight some limitations of our current prototype, for example, "I am addicted to spicy food and it totally missed it. There may just not be enough spicy alternatives in the different dishes to pick up on it" points out that the prototype is limited in the size of the food database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Offline Benchmarking for FoodDist</head><p>We develop FoodDist and baseline models (Section 5) using the Food-101 training dataset, which contains 75,750 food images from 101 food categories (750 instances for each category) <ref type="bibr" target="#b5">[6]</ref>. To the best of our knowledge, Food-101 is the largest and most challenging publicly available dataset for food images. We implement models using Caffe <ref type="bibr" target="#b26">[27]</ref> and experiment with two CNN architectures in our framework: AlexNet <ref type="bibr" target="#b32">[33]</ref>, which won first place at the ILSVRC2012 challenge, and VGG <ref type="bibr" target="#b45">[48]</ref>, which is the state-of-the-art CNN model. The inputs to the networks are image crops of sizes 224 √ó 224 (VGG) or 227 √ó 227 (AlexNet). They are randomly sampled from a pixelwise mean-subtracted image or its horizontal flip. In our benchmark, we train four different feature extractors: AlexNet +Learning with classification (AlexNet+CL), AlextNet+Multitask learning (AlexNet+MT), VGG+Learning with classification (VGG+CL), and VGG+Multitask learning (VGG+ML, FoodDist). For the multitask learning framework, we sample the similar and dissimilar image pairs with 1:10 ratio from the Food-101 dataset based on the categorical labels to be consistent with the previous work <ref type="bibr" target="#b56">[59]</ref>. The models are fine-tuned based on the networks pre-trained with the ImageNet data. We use Stochastic Gradient Decent with a mini-batch size of 64, and each network is trained for 10 √ó 10<ref type="foot" target="#foot_2">4</ref> iterations. The initial learning rate is set to 0.001, and we use a weight decay of 0.0005 and momentum of 0.9.</p><p>We compare the performance of four feature extractors, including FoodDist, with the stateof-the-art food image analysis models using the Food-101 testing dataset, which contains 25,250 food images from 101 food categories (250 instances for each category). The performance for the classification and retrieval tasks are evaluated as follows:</p><p>‚Ä¢ Classification: We test the performance of using learned image features for classification. For the classification deep neural network in each of the models above, we adopt the standard 10-crop testing. that is, the network makes a prediction by extracting 10 patches (the four corner patches and the center patch in the original images and their horizontal reflections) and averaging the predictions at the softmax layer. The metrics used in this article are Top-1 accuracy and Top-5 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢ Retrieval:</head><p>We use a retrieval task to evaluate the quality of the Euclidean distances between extracted features. Ideally, the distances should be smaller for similar image pairs and larger for dissimilar pairs. Therefore, as suggested by previous work [59, 61], we check the nearest k-neighbors of each test image, for k = 1, 2, ‚Ä¶, N, where N = 25, 250 is the size of the testing dataset, and calculate the Precision and Recall values for each k. We use mean Average Precision (mAP) as the evaluation metric to compare the performance. For every method, the Precision/Recall values are averaged over all the images in the testing set.</p><p>The classification and retrieval performance of all models are summarized in Table <ref type="table" target="#tab_2">III</ref> and Table <ref type="table" target="#tab_3">IV</ref>, respectively. FoodDist performs the best among four models and is significantly better than the state-of-the-art approaches in both tasks. For the classification task, the classifier built on FoodDist features achieves 83.09% Top-1 accuracy, which significantly outperforms the original RFDC <ref type="bibr" target="#b5">[6]</ref> model and the proprietary GoogLeNet model <ref type="bibr" target="#b34">[35]</ref>; for the retrieval task, FoodDist doubles the mAP value reported by previous work <ref type="bibr" target="#b56">[59]</ref> that only used the AlexNet and Siamese network architecture. The benchmark results demonstrate that FoodDist features possess high generalization ability and the Euclidean distances between feature vectors reflect the similarities between food images with great fidelity. In addition, as we can observe from both tables, the multitask learning-based approach always performs better than learning with classification for both tasks no matter which CNN is used. This further justifies the proposed multitask learning approach and its advantage of incorporating both label and pairwise distance information that makes the learned features more generalizable and meaningful in the Euclidean distance embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">End-to-End User Testing</head><p>We conducted end-to-end user testing to validate the efficacy of Yum-me recommendations. We recruited 60 participants through the university mailing list, Facebook, and Twitter. The goal of the user testing was to compare Yum-me recommendations with a widely used user onboarding approach, that is, a traditional food preference survey (a sample survey used by PlateJoy is shown in Figure <ref type="figure" target="#fig_12">13</ref>). As Yum-me is designed for scenarios where no rating or food consumption history is available (which is common when the user is new to a platform or is visiting a nutritionist's office), a collaborative filtering algorithm that has been adopted by many state-of-the-art recommenders is not directly comparable to our system.</p><p>In this study, we used a within-subjects study design in which each participant expressed their opinions regarding the meals recommended by both of the recommenders, and the effectiveness of the systems were compared on a per-user basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1.">Study Design-</head><p>We created a traditional recommendation system by randomly picking N of M meals in the candidate pool to recommend to the users. The values of N and M are controlled such that N = 10, M = 500 for both Yum-me and the traditional baseline.</p><p>The user study consists of three phases, as Figure <ref type="figure" target="#fig_10">11</ref> shows: (1) Each participant was asked to indicate his or her diet type and health goals through our basic user survey. (2) Each participant was then asked to use the visual interface. (3) Twenty meal recommendations were arranged in a random order and presented to the participant at the same time, where 10 of them are made by Yum-me, and the other 10 are generated by the baseline. The participant was asked to express their opinion by dragging each of the 20 meals into either the Yummy or the No way bucket. To overcome the fact that humans would tend to balance the buckets if their previous choices were shown, the food item disappeared after the user dragged it into a bucket. In this way, users were not reminded of how many meals they had put into each bucket.</p><p>The user study systems were implemented as web services and participants accessed the study from desktop or mobile browsers. We chose a web service for its wide accessibility to the population, but we could easily fit Yum-me into other ubiquitous devices, as mentioned earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2.">Participants-</head><p>The most common dietary choice among our 60 participants was No restrictions (48), followed by Vegetarian (9), Halal (2), and Kosher <ref type="bibr" target="#b0">(1)</ref>. No participants chose Vegan. Participant preferences in terms of nutrients are summarized in Table <ref type="table" target="#tab_4">V</ref>. For Calories and Fat, the top two goals were Reduce and Maintain. For Protein, participants tended to choose either Increase or Maintain. For health goals, the top four participant choices were Maintain calories-Maintain protein-Maintain fat <ref type="bibr" target="#b19">(20)</ref>, Reduce calories-Maintain protein-Reduce fat <ref type="bibr" target="#b9">(10)</ref>, Reduce calories-Maintain protein-Maintain fat <ref type="bibr" target="#b9">(10)</ref> and Reduce calories-Increase protein-Reduce fat <ref type="bibr" target="#b4">(5)</ref>. The statistics match well with the common health goals among the general population, that is, people who plan to control weight and improve sports performance tend to reduce the intake calories and fat and increase the amount of protein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.3.3.</head><p>Quantitative Analysis-We use a quantitive approach to demonstrate that (1) Yum-me recommendations yield higher meal acceptance rates than traditional approaches and (2) meals recommended by Yum-me satisfy users' nutritional needs.</p><p>To show higher meal acceptance rates, we calculated the participant acceptance rate of meal recommendations as #Meals in Yummy bucket #Recommended meals .</p><p>The cumulative distribution of the acceptance rate is shown in Figure <ref type="figure" target="#fig_11">12</ref>, and the average acceptance rate, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE) of each approach are presented in Table <ref type="table" target="#tab_4">VI</ref>. The results demonstrate that Yum-me significantly improves the quality of the presented food items. The per-user acceptance rate difference between two approaches was normally distributed, <ref type="foot" target="#foot_3">5</ref> and a paired Student's t-test indicated a significant difference between the two methods (p &lt; 0.0001). <ref type="foot" target="#foot_4">6</ref>To quantify the improvement provided by Yum-me, we calculated the difference between the acceptance rates of the two systems, that is, difference = Yum-me acceptance ratebaseline acceptance rate. The distribution and average values of the differences are presented in Figure <ref type="figure" target="#fig_13">14</ref> and Table <ref type="table" target="#tab_4">VI</ref>, respectively. It is noteworthy that Yum-me outperformed the baseline by 42.63% in terms of the number of preferred recommendations, which demonstrates its utility over the traditional meal recommendation approach. However, another observed phenomenon in Figure <ref type="figure" target="#fig_13">14</ref> is that there are 12 users (20%) with zero acceptance rate differences, which may due to the following two reasons: (1) Yum-me is not effective to this set of users, and it does not improve their preferences towards recommended food items.</p><p>(2) As we did not conduct participant control and filtering, some participants may not be well involved in the study and randomly select or drag items.</p><p>To examine meal nutrition, we compare the nutritional facts of paticipants' favorite meals with those of meals recommended (by Yum-me) and accepted (items dragged into the yummy bucket) by the user. As shown in Figure <ref type="figure" target="#fig_4">15</ref>, for users with the same nutritional needs and no dietary restrictions, we calculate the average amount of protein, calories, and fat (perserving) in ( <ref type="formula" target="#formula_0">1</ref>) their favorite 20 meals (as determined by our online learning algorithm) and ( <ref type="formula" target="#formula_5">2</ref>) their recommended and accepted meals, respectively. The mean values presented in Figure <ref type="figure" target="#fig_4">15</ref> are normalized by the average amount of corresponding nutrients in their favorite meals. The results demonstrate that by using a relatively simple nutritional ranking approach, Yum-me is able to satisfy most of the nutritional needs set by the users, including reduce, maintain and increase calories, increase protein, and reduce fat. However, our system fails to meet two nutritional requirments, that is, maintain protein and maintain fat. Our results also show where Yum-me recommendations result in unintended nutritional composition. For example, the goal of reducing fat results in the reduction of protein and calories, and the goal of increasing calories ends up increasing the protein in meals. This is partially due to the inherent inter-dependence between nutrients, and we leave further investigation of this issue to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.3.4.</head><p>Qualitative Analysis-To qualitatively understand the personalization mechanism of Yum-me, we randomly pick three participants with no dietary restrictions and with the health goal of reducing calories. For each user, we select top-20 general food items the user likes most (inferred by the online learning algorithm). These food items played important roles in selecting the healthy meals to recommend to the user. To visualize this relationship, among these top-20 items, we further select two food items that are most similar to the healthy items Yum-me recommended to the users and present three such examples in Figure <ref type="figure" target="#fig_5">16</ref>. Intuitively, our system is able to recommend healthy food items that are visually similar to the food items a user like, but the recommended items are of lower calories due to the use of healthier ingredients or different cooking styles. These examples showcase how Yum-me can project users' general food preferences to the domain of the healthy options and find the ones that can most appeal to users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.3.5.</head><p>Error Analysis-Through a closer examination of the cases where our system performed, or did not perform, well, we observed a negative correlation between the entropy of the learned preference distribution p 7 and the improvement of Yum-me over the baseline (r = -0.32, p = 0.026). This correlation suggests that when user's preference distributions are more concentrated, the recommended meals tend to perform better. This is not too surprising, because the entropy of the preference distribution roughly reflects the degree of confidence the system has in the users' preferences, where the confidence is higher if the entropy is lower and vice versa. In Figure <ref type="figure" target="#fig_15">17</ref>, we show the evolution of the entropy value as the users are making more comparisons. The results demonstrate that the system becomes more confident about user's preferences as users provide more feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION</head><p>In this section, we discuss the limitations of the current prototype and study and present realworld scenarios where Yum-me and its sub-modules can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Limitations of the Evaluations</head><p>In evaluating the online learning framework, because there is no previous algorithm that can end-to-end solve our preference elicitation problem, the baselines are constructed by combining methods that intuitively fit user state update and images selection modules, respectively. This introduces potential biases in baseline selections. Additionally, in the endto-end user testing, the participants' judgements of whether the food is Yummy or No way is potentially influenced by the image quality and the health concerns. These may be confounding factors in measuring users' preferences towards food items and can be eliminated by explicitly instructing the participants to not consider these factors. We leave further evaluations as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Limitations of Yum-me in Recommending Healthy Meals</head><p>The ultimate effectiveness of Yum-me in generating healthy meal suggestions is contingent on the appropriateness of the nutritional needs input by the user. To conduct such recommendations for people with different conditions, Yum-me could be used in the context of personal health coaches, nutritionists, or coaching applications that provide reliable nutritional suggestions based on the user's age, weight, height, exercise, and disease history. For instance, general nutritional recommendations can be calculated using online services built on the guidelines from National Institutes of Health, such as weight-success 8 and active. 9 Also, although we have demonstrated the feasibility of building a personalized meal recommender catering to people's fine-grained food preference and nutritional needs, the current prototype of Yum-me assumes a relatively simple strategy to rank the nutritional appropriateness, and is limited in terms of the available options for nutrition. Future work should investigate more sophisticated ranking approaches and incorporate options relevant to the specific application context. 7 Entropy of preference distribution: H(p) = -‚àë i p i logp i .</p><p>8 http://www.weighing-success.com/NutritionalNeeds.html. 9 http://www.active.com/fitness/calculators/nutrition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Yum-Me for Real-World Dietary Applications</head><p>We envision that Yum-me has the potential to power many real-world dietary applications. For example, (1) User onboarding. Traditionally, food companies, for example, Zipongo and Plated, address the cold start problem by asking each new user to answer a set of pre-defined questions, as shown in Section 6.3, and then recommend meals accordingly. Yum-me can enhance this process by eliciting user's fine-grained food preference and informing an accurate dietary profile. Service providers can customize Yum-me to serve their own businesses and products by using a specialized backend food item database and then use it as a step after the general questions. (2) Nutritional assistants. While visiting a doctor's office, patients are often asked to fill out standard questionnaires to indicate food preferences and restrictions. Patients' answers are then investigated by the professionals to come up with effective and personalized dietary suggestions. In such a scenario, the recommendations made by Yum-me could provide a complementary channel for communicating the patient's fine-grained food preferences to the doctor to further tailor suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">FoodDist for a Wide Range of Food Image Analysis Tasks</head><p>FoodDist provides a unified model to extract features from food images so they are discriminative in the classification and clustering tasks, and its pairwise Euclidean distances are meaningful in reflecting similarities. The model is rather efficient (&lt;0.5s/f on eight-core commodity processors) and can be ported to mobile devices with the publicly available caffe-android-lib framework. <ref type="foot" target="#foot_5">10</ref>In addition to enabling Yum-me, we released the FoodDist model to the community (https:// github.com/ylongqi/FoodDist), so it can be used to fuel other nutritional applications. For the sake of space, we only briefly discuss two sample use cases below:</p><p>‚Ä¢ Food/Meal recognition: Given a set of labels, for example, food categories, cuisines, and restaurants, the task of food and meal recognition could be approached by first extracting food image features from FoodDist and then training a linear classifier, for example, logistic regression or SVM, to classify the food images that are beyond the categories given in the Food-101 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢ Nutrition Facts estimation:</head><p>With the emergence of large-scale food item or recipe databases, such as Yummly, the problem of nutritional fact estimation might be converted to a simple nearest-neighbor retrieval task: Given a query image, we find its closest neighbor in the FoodDist based on Euclidean distance and use that neighbor's nutritional information to estimate the nutrition facts of the query image <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION AND FUTURE WORK</head><p>In this article, we propose Yum-me, a novel nutrient-based meal recommender that makes meal recommendations catering to users' fine-grained food preferences and nutritional needs. We further present an online learning algorithm that is capable of efficiently learning food preference and FoodDist, a best-of-its-kind unified food image analysis model. The user study and benchmarking results demonstrate the effectiveness of Yum-me and the superior performance of the FoodDist model.</p><p>Looking forward, we envision that the idea of using visual similarity for preference elicitation may have implications to the following research areas. (1) User-centric modeling: the fine-grained food preference learned by Yum-me can be seen as a general dietary profile of each user and be projected to other domains to enable more dietary applications, such as suggesting proper meal plans for diabetes patients. Moreover, a personal dietary API can be built on top of this profile to enable sharing and improvementment across multiple dietary applications. ( <ref type="formula" target="#formula_5">2</ref>) Food image analysis API for deeper content understanding: With the release of the FoodDist model and API, many dietary applications, in particular the ones that capture a large number of food images, might benefit from a deeper understanding of their image contents. For instance, food journaling applications could benefit from the automatic analysis of food images to summarize the dayto-day food intake or trigger timely reminders and suggestions when needed. (3) Finegrained preference elicitation leveraging visual interfaces. The idea of eliciting users' fine-grained preference via visual interfaces is also applicable to other domains. The key insight here is that visual contents capture many subtle variations among objects that text or categorical data cannot capture, and the learned representations can be used as an effective medium to enable fine-grained preferences learning. For instance, the IoT, wearable, and mobile systems for entertainments, consumer products, and general content deliveries might leverage such an adaptive visual interface to design an onboarding process that learn users' preferences in a much shorter time and potentially provide a more pleasant user experience than traditional approaches.   User-system interaction at iteration t. Locally connected graph with item i. Euclidean embedding of FoodDist. This figure shows the pairwise Euclidean distances between food images in the embedding. A distance of 0.0 means two food items are identical and a distance of 2.0 represents that the image contents completely differ. For this example, if the threshold is set to 1.0, then all the food images can be correctly classified.    Comparison of cumulative distribution of prediction accuracy across different algorithms. Timestamp records for user response time and system execution time. User study workflow for personalized nutrient-based meals recommendation system. We compare Yum-me (blue arrows) with the baseline method (violet arrow) that makes recommendations solely based on nutritional facts and dietary restrictions. Cumulative distribution of acceptance rate for both recommender systems. The survey used for user onboarding of PlateJoy. The questions are up to date at the time of the writing of this article, and we only include top four questions for illustration purpose. Distribution of the acceptance rate differences between two recommender systems.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1.Overview of Yum-me. This figure shows three sample scenarios in which Yum-me can be used: desktop browser, mobile, and smart watch. The fine-grained dietary profile is used to re-rank and personalize meal recommendations.</figDesc><graphic coords="30,144.84,62.00,382.32,117.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of two sample databases: (a) Database for users without dietary restrictions and (b) Database for vegetarian users.</figDesc><graphic coords="31,144.40,62.00,383.21,115.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.</figDesc><graphic coords="32,192.00,62.00,288.00,105.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4.</figDesc><graphic coords="33,192.00,62.00,288.00,122.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Multitask learning structure of FoodDist. Different types of layers are denoted by different colors. The format of each type of layer: Convolution layer: [receptive field size:step size ‚Ä¶, #channels]; Pooling layer: [pooling size:step size ‚Ä¶]; Fully connected layer: [‚Ä¶, output dimension].</figDesc><graphic coords="35,143.76,62.00,384.48,186.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Prediction accuracy for different algorithms in various training settings (asterisks represent different levels of statistical significance: ***: p &lt; 0.001, **: p &lt; 0.01, *: p &lt; 0.05).</figDesc><graphic coords="36,192.16,62.00,287.68,218.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Cumulative distribution of prediction accuracy for LE+EE algorithm (Numbers in the legend represent the number of training iterations (i.e., values of T)).</figDesc><graphic coords="37,192.18,62.00,287.65,196.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9.</figDesc><graphic coords="38,144.48,62.00,383.04,95.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10.</figDesc><graphic coords="39,144.04,62.00,383.92,164.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11.</figDesc><graphic coords="40,143.76,62.00,384.48,143.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12.</figDesc><graphic coords="41,192.25,62.00,287.51,140.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13.</figDesc><graphic coords="42,176.97,62.00,318.06,414.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14.</figDesc><graphic coords="43,192.00,62.00,288.00,139.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .Fig. 16 .</head><label>1516</label><figDesc>Fig. 15. Nutritional facts comparison between paticipants' favorite meals and recommended (Yumme) and accepted meals. The meal is accepted if it is dragged into the yummy bucket. The mean values are normalized by the average amount of corresponding nutrient in the favorite meals (orange bar). (Only seven of nine nutritional goals are used by at least one partipant.)</figDesc><graphic coords="44,143.76,62.00,384.48,306.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Entropy of preference distributions in different iterations of online learning. (Data are from 48 users with no dietary restrictions.)</figDesc><graphic coords="46,192.00,62.00,288.00,134.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,155.00,220.32,394.32,131.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,155.00,123.75,395.28,174.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="14,155.00,123.75,395.28,127.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="45,176.77,62.00,318.45,287.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I</head><label>I</label><figDesc>Sizes of Databases that Catered to Different Diet Types. Unit: Number of Unique Recipes</figDesc><table><row><cell>Database</cell><cell cols="2">Original size Final size</cell></row><row><cell>No restriction</cell><cell>9,405</cell><cell>7,938</cell></row><row><cell>Vegetarian</cell><cell>10,000</cell><cell>6,713</cell></row><row><cell>Vegan</cell><cell>9,638</cell><cell>6,013</cell></row><row><cell>Kosher</cell><cell>10,000</cell><cell>4,825</cell></row><row><cell>Halal</cell><cell>10,000</cell><cell>5,002</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III</head><label>III</label><figDesc>Model Performance of Classification Task.</figDesc><table><row><cell>Method</cell><cell cols="2">Top-1 ACC (%) Top-5 ACC(%)</cell></row><row><cell>RFDC * [6]</cell><cell>50.76%</cell><cell>-</cell></row><row><cell>GoogleLeNet * [35]</cell><cell>79%</cell><cell>-</cell></row><row><cell>AlexNet+CL</cell><cell>67.63%</cell><cell>89.02%</cell></row><row><cell>AlexNet+MT</cell><cell>70.50%</cell><cell>90.36%</cell></row><row><cell>VGG+CL</cell><cell>82.48%</cell><cell>95.70%</cell></row><row><cell>VGG+MT (FoodDist)</cell><cell>83.09%</cell><cell>95.82%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table IV</head><label>IV</label><figDesc>Model Performance of Retrieval Task. Represents State-of-the-Art Approach and Bold Text Indicates the Method with the Best Performance Note: The mAP value that we report for food-cnn is higher because we use pixel-wise mean subtraction while the original paper only used perchannel mean subtraction.</figDesc><table><row><cell>Method</cell><cell>mean Average Precision (mAP)</cell></row><row><cell>Food-CNN * [59]</cell><cell>0.3084</cell></row><row><cell>AlexNet+CL</cell><cell>0.3751</cell></row><row><cell>AlexNet+MT</cell><cell>0.4063</cell></row><row><cell>VGG+CL</cell><cell>0.6417</cell></row><row><cell>VGG+MT (FoodDist)</cell><cell>0.6670</cell></row></table><note><p>*</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table V</head><label>V</label><figDesc>Statistics of Health Goals Among 60 Participants. Unit: Number of Participants</figDesc><table><row><cell cols="4">Nutrient Reduce Maintain Increase</cell></row><row><cell>Calories</cell><cell>30</cell><cell>28</cell><cell>2</cell></row><row><cell>Protein</cell><cell>1</cell><cell>44</cell><cell>15</cell></row><row><cell>Fat</cell><cell>23</cell><cell>36</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ACM Trans Inf Syst. Author manuscript; available in PMC 2018 November 19.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our system is not restricted to these five dietary restrictions, and we will extend the system functionalities to other categories in the future.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Our web system implementation is based on Amazon EC2 t2-micro Linux 64-bit instance</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>A Shapiro Wilk W test was not significant (p = 0.12), which justifies that the difference is normally distributed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We also performed a non-parametric Wilcoxon signed-rank test and found a comparable result.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_5"><p>https://github.com/sh1r0/caffe-android-lib.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>* Represents State-of-the-Art Approach and Bold Text Indicates the Method with the Best Performance ACM Trans Inf Syst. Author manuscript; available in PMC 2018 November 19.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insightful comments and thank Yin Cui, Fan Zhang, Tsung-Yi Lin, and Dr. Thorsten Joachims for discussions of machine-learning algorithms. This work is funded through Awards from NSF (#1344587, #1343058) and NIH (#1U54EB020404); as well as gift funding from AOL, RWJF, UnitedHealth Group, Google, and Adobe.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Manuscript</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feasibility testing of an automated image-capture method to aid dietary recall</title>
		<author>
			<persName><forename type="first">Arab</forename><surname>Lenoreestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborahkim</forename><surname>Donnie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Burke</forename><surname>Jeffgoldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur J Clin Nutr</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1156" to="1162" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
	<note>PubMed: 21587282</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Davidvassilvitskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The nonstochastic multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">Auer</forename><surname>Petercesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Bianchi</forename><surname>Nicolofreund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoavschapire</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="77" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Beijbom</forename><surname>Oscarjoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neelmorris</forename><surname>Dansaponas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scottkhullar</forename><surname>Siddharth</surname></persName>
		</author>
		<title level="m">Proceedings of the 2015 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE; 2015. Menu-match: restaurant-specific food logging from images</title>
		<meeting>the 2015 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE; 2015. Menu-match: restaurant-specific food logging from images</meeting>
		<imprint>
			<biblScope unit="page" from="844" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nutrition and depression: implications for improving mental health among childbearing-aged women</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bodnar Lisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wisner</forename><surname>Katherine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol Psychiat</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">16040007</biblScope>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName><forename type="first">Bossard</forename><surname>Lukasguillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieuvan</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nutrition and sports performance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Brotherhood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sports Med</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="350" to="389" />
			<date type="published" when="1984">1984. 1984</date>
		</imprint>
	</monogr>
	<note>PubMed: 6390609</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Caruana Rich. Multitask learning. Mach Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Volume estimation using food specific shape templates in mobile imagebased dietary assessment</title>
		<author>
			<persName><forename type="first">Chae</forename><surname>Junghoonwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Insookim</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yemaciejewski</forename><surname>Rosszhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengqingdelp</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Boushey</forename><surname>Carol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ebert</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Imaging</title>
		<imprint>
			<biblScope unit="page">78730</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Chang</forename><surname>Kerry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Pingdanis</forename><surname>Catalina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Farrell</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<title level="m">Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing. ACM; 2014. Lunch Line: Using public displays and mobile devices to encourage healthy eating in an organization</title>
		<meeting>the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing. ACM; 2014. Lunch Line: Using public displays and mobile devices to encourage healthy eating in an organization</meeting>
		<imprint>
			<biblScope unit="page" from="823" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using groups of items for preference elicitation in recommender systems</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Harper</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terveen</forename><surname>Loren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Computer-Supported Cooperative Work (CSCW&apos;15)</title>
		<meeting>the ACM Conference on Computer-Supported Cooperative Work (CSCW&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Consolvo</forename><surname>Sunnymcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Toscos</forename><surname>Tammychen Mike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Froehlich</forename><surname>Jonharrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beverlyklasnja</forename><surname>Predraglamarca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthonylegrand</forename><surname>Louislibby Ryan</surname></persName>
		</author>
		<title level="m">Proceedings of the Special Interest Group on Computer-Human Interaction Conference on Human Factors in Computing Systems. ACM; 2008. Activity sensing in the wild: aA field trial of ubifit garden</title>
		<meeting>the Special Interest Group on Computer-Human Interaction Conference on Human Factors in Computing Systems. ACM; 2008. Activity sensing in the wild: aA field trial of ubifit garden</meeting>
		<imprint>
			<biblScope unit="page" from="1797" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Cordeiro</forename><surname>Feliciabales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabethcherry</forename><surname>Erinfogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename></persName>
		</author>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM; 2015. Rethinking the mobile food journal: Exploring opportunities for lightweight photo-based capture</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM; 2015. Rethinking the mobile food journal: Exploring opportunities for lightweight photo-based capture</meeting>
		<imprint>
			<biblScope unit="page" from="3207" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName><forename type="first">Dai</forename><surname>Jifenghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaimingsun</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to question: leveraging user preferences for shopping advice</title>
		<author>
			<persName><forename type="first">Das</forename><surname>Mahashwetade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisci</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianmarcogionis</forename><surname>Aristidesweber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference Knowledge Discovery in Databases (KDD&apos;13)</title>
		<meeting>the Conference Knowledge Discovery in Databases (KDD&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Deng</forename><surname>Jiadong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisocher</forename><surname>Richardli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jiali Kaifei-Fei</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>IEEE; 2009. Imagenet: A largescale hierarchical image database</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Elsweiler</forename><surname>Davidharvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename></persName>
		</author>
		<title level="m">Proceedings of the 9th ACM Conference on Recommender Systems. ACM; 2015. Towards automatic meal plan recommendations for balanced nutrition</title>
		<meeting>the 9th ACM Conference on Recommender Systems. ACM; 2015. Towards automatic meal plan recommendations for balanced nutrition</meeting>
		<imprint>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effect of diet and controlled exercise on weight loss in obese children</title>
		<author>
			<persName><forename type="first">Epstein</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Wing</forename><surname>Rena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Penner</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kress</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeanne</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Pediatr</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="358" to="361" />
			<date type="published" when="1985">1985. 1985</date>
		</imprint>
	</monogr>
	<note>PubMed: 4032130</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Forbes</forename><surname>Peterzhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename></persName>
		</author>
		<title level="m">Proceedings of the 5th ACM Conference on Recommender Systems. ACM; 2011. Content-boosted matrix factorization for recommender systems: experiments with recipe recommendation</title>
		<meeting>the 5th ACM Conference on Recommender Systems. ACM; 2011. Content-boosted matrix factorization for recommender systems: experiments with recipe recommendation</meeting>
		<imprint>
			<biblScope unit="page" from="261" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Freyne</forename><surname>Jillberkovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomo</forename></persName>
		</author>
		<title level="m">Proceedings of the 15th International Conference on Intelligent User Interfaces. ACM; 2010. Intelligent food planning: personalized recipe recommendation</title>
		<meeting>the 15th International Conference on Intelligent User Interfaces. ACM; 2010. Intelligent food planning: personalized recipe recommendation</meeting>
		<imprint>
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geleijnse GijsNachtigall PeggyKaam Pim vanWijgergangs Luci√´nne</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Intelligent User Interfaces. ACM; 2011. A personalized recipe advice system to promote healthful choices</title>
		<meeting>the 16th International Conference on Intelligent User Interfaces. ACM; 2011. A personalized recipe advice system to promote healthful choices</meeting>
		<imprint>
			<biblScope unit="page" from="437" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive bootstrapping of recommender systems using decision trees</title>
		<author>
			<persName><forename type="first">Golbandi</forename><surname>Nadavkoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehudalempel</forename><surname>Ronny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Web Search and Data Mining (WSDM&apos;11)</title>
		<meeting>the International Conference on Web Search and Data Mining (WSDM&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You are what you eat: Learning user tastes for rating prediction</title>
		<author>
			<persName><forename type="first">Harvey</forename><surname>Morganludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berndelsweiler</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on String Processing and Information Retrieval</title>
		<meeting>the International Symposium on String Processing and Information Retrieval</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="153" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">He</forename><surname>Yexu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changkhanna</forename><surname>Nehaboushey Carol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Delp</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Multimedia and Expo (ICME&apos;13</title>
		<meeting>the 2013 IEEE International Conference on Multimedia and Expo (ICME&apos;13</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>IEEE; 2013. Food image analysis: Segmentation, identification and weight estimation</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Hsieh</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Kangyang</forename><surname>Longqicui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinlin</forename><surname>Tsung-Yibelongie Sergeestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename></persName>
		</author>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<publisher>Collaborative Metric Learning</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Bengio</forename><surname>Yoshuagoodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iancourville</forename><surname>Aaron</surname></persName>
		</author>
		<ptr target="http://goodfeli.github.io/dlbook/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<title level="s">Jia YangqingShelhamer EvanDonahue JeffKarayev SergeyLong JonathanGirshick RossGuadarrama SergioDarrell Trevor</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Azusali</forename><surname>Kadomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Changchu</forename><surname>Cheng-Yuanchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kojisiio</forename><surname>Hao-Huatsukada</surname></persName>
		</author>
		<author>
			<persName><surname>Itiro</surname></persName>
		</author>
		<title level="m">Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication</title>
		<meeting>the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="71" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Persuasive technology to improve eating behavior using a sensor-embedded fork</title>
		<author>
			<persName><forename type="first">Kadomura</forename><surname>Azusali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Yuantsukada Kojichu Hao-Huasiio</forename><surname>Itiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<meeting>the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="319" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Food image recognition with deep convolutional features</title>
		<author>
			<persName><forename type="first">Kawano</forename><surname>Yoshiyukiyanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiji</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication</title>
		<meeting>the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="589" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FoodLog: capture, analysis and retrieval of personal food images via web</title>
		<author>
			<persName><forename type="first">Kitamura</forename><surname>Keigoyamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihikoaizawa</forename><surname>Kiyoharu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia 2009 Workshop on Multimedia for Cooking and Eating Activities</title>
		<meeting>the ACM Multimedia 2009 Workshop on Multimedia for Cooking and Eating Activities</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Who underreports dietary intake in a dietary recall? Evidence from the Second National Health and Nutrition Examination Survey</title>
		<author>
			<persName><forename type="first">Klesges</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eck</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Joanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Consult Clinical Psychol</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">438</biblScope>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
	<note>PubMed: 7608356</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Krizhevsky</forename><surname>Alexsutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilyahinton</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">Lowe</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Im2Calories: towards an automated mobile vision food diary</title>
	</analytic>
	<monogr>
		<title level="m">Meyers AustinJohnston NickRathod VivekKorattikara AnoopGorban AlexSilberman NathanGuadarrama SergioPapandreou GeorgeHuang JonathanMurphy Kevin P</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1233" to="1241" />
		</imprint>
	</monogr>
	<note>Proceedings of the International Conference on Computer Vision (ICCV&apos;15)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding food consumption lifecycles using wearable cameras</title>
		<author>
			<persName><forename type="first">Ng</forename><surname>Kher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishipp</forename><surname>Victoriamortier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richardbenford</forename><surname>Steveflintham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martinrodden</forename><surname>Tom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pers Ubiq Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1183" to="1195" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Noronha</forename><surname>Jonhysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ericzhang</forename><surname>Haoqigajos Krzysztof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<title level="m">Proceedings of the ACM Symposium of User Interface Software and Technology (UIST&apos;11)</title>
		<meeting>the ACM Symposium of User Interface Software and Technology (UIST&apos;11)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pairwise preference regression for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Park</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Taekchu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM Conference on Recommender Systems</title>
		<meeting>the 3rd ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Diabetes and healthy eating: A systematic review of the literature</title>
		<author>
			<persName><forename type="first">Povey</forename><surname>Rachel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clareclark-Carter</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes Educator</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="931" to="959" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
	<note>PubMed: 18057263</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Getting to know you: Learning new user preferences in recommender systems</title>
		<author>
			<persName><forename type="first">Rashid</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mamunuralbert</forename><surname>Istvancosley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danlam</forename><surname>Shyong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mcnee</forename><surname>Sean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Konstan</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Riedl</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Intelligent User Interfaces Conference</title>
		<meeting>the ACM Intelligent User Interfaces Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">Razavian</forename><surname>Aliazizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hosseinsullivan</forename><surname>Josephinecarlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving pairwise learning for item recommendation from implicit feedback</title>
		<author>
			<persName><forename type="first">Rendle</forename><surname>Steffenfreudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 7th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Rendle</forename><surname>Steffenfreudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophgantner</forename><surname>Zenoschmidt-Thieme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename></persName>
		</author>
		<title level="m">Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 25th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
	<note>Bayesian personalized ranking from implicit feedback</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Shepherd JonathanHarden AngelaRees RebeccaBrunton GinnyGarcia JoOliver SandyOakley Ann. Young people and healthy eating: a systematic review of research on barriers and facilitators</title>
	</analytic>
	<monogr>
		<title level="j">Health Educ Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="257" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note>PubMed: 16251223</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><surname>Shopwell</surname></persName>
		</author>
		<ptr target="http://www.shopwell.com/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Simonyan Karenzisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Estimating nutritional value from food images based on semantic segmentation</title>
		<author>
			<persName><forename type="first">Sudo</forename><surname>Kyokomurasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuhikoshimamura</forename><surname>Juntaniguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukinobu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication</title>
		<meeting>the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="571" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning multiplequestion decision trees for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Sun</forename><surname>Mingxuanli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuxinlee</forename><surname>Joonseokzhou Kelebanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guyzha</forename><surname>Hongyuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conferene on Web Search and Data Mining (WSDM&apos;13)</title>
		<meeting>the ACM International Conferene on Web Search and Data Mining (WSDM&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Designing and evaluating kalas: A social navigation system for food recipes</title>
		<author>
			<persName><forename type="first">Svensson</forename><surname>Martinh√∂√∂k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristinac√∂ster</forename><surname>Rickard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Comput-Hum Interact</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="374" to="400" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Feasibility of identifying eating moments from first-person images leveraging human computation</title>
		<author>
			<persName><forename type="first">Thomaz</forename><surname>Edisonparnami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanessa Irfanabowd</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International SenseCam and Pervasive Imaging Conference</title>
		<meeting>the 4th International SenseCam and Pervasive Imaging Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The use of crowdsourcing for dietary self-monitoring: crowdsourced ratings of food pictures are comparable to ratings by trained observers</title>
		<author>
			<persName><forename type="first">Turner-Mcgrievy</forename><surname>Gabrielle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Helander</forename><surname>Elina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipainen</forename><surname>Kirsikkaperez-Macias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">Mariakorhonen</forename><surname>Ilkka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Med Inf Assoc</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="e119" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recipe recommendation method by considering the users preference and ingredient quantity of target recipe</title>
		<author>
			<persName><forename type="first">Ueda</forename><surname>Mayumiasanuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syungomiyawaki</forename><surname>Yusukenakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsuke</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the International MultiConference of Engineers and Computer Scientists</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Maaten</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Laurenshinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deriving a recipe similarity measure for recommending healthful meals</title>
		<author>
			<persName><forename type="first">Pinxteren</forename><surname>Youri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gijskamsteeg</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM International Conference on Intelligent User Interfaces</title>
		<meeting>the 16th ACM International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName><forename type="first">Weston</forename><surname>Jasonbengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyusunier</forename><surname>Nicolas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Weston</forename><surname>Jasonyee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hectorweiss</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<title level="m">Proceedings of the 7th ACM Conference on Recommender Systems. ACM; 2013. Learning to rank recommendations with the k-order statistic loss</title>
		<meeting>the 7th ACM Conference on Recommender Systems. ACM; 2013. Learning to rank recommendations with the k-order statistic loss</meeting>
		<imprint>
			<biblScope unit="page" from="245" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Longqicui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinzhang</forename><surname>Fanpollak John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Belongie</forename><surname>Sergeestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename></persName>
		</author>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM; 2015. PlateClick: Bootstrapping Food Preferences Through an Adaptive Visual Interface</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management. ACM; 2015. PlateClick: Bootstrapping Food Preferences Through an Adaptive Visual Interface</meeting>
		<imprint>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Longqifang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenjin</forename><surname>Hailinhoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattestrin</forename><surname>Deborah</surname></persName>
		</author>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee; 2017. Personalizing Software and Web Services by Integrating Unstructured Application Usage Traces</title>
		<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee; 2017. Personalizing Software and Web Services by Integrating Unstructured Application Usage Traces</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Beyond classification: Latent user interests profiling from visual contents analysis</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Longqihsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Kangestrin</forename><surname>Deborah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining Workshop (ICDMW&apos;15)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1410" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><surname>Yummly</surname></persName>
		</author>
		<ptr target="http://developer.yummly.com" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Dualds: A dual discriminative rating elicitation framework for cold start recommendation. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Xicheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiu</forename><surname>Shuangzhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guibolu</forename><surname>Hanqing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="161" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Zhanpengluo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingloy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changetang</forename><surname>Xiaoou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
	<note>Computer Vision-ECCV</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Dengyongbousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivierlal</forename><surname>Thomas Navinweston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasonsch√∂lkopf</forename><surname>Bernhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Functional matrix factorizations for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Keyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang-Hongzha</forename><surname>Hongyuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Interest Group on Information Retrieval (SIGIR&apos;11)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Personalizing Food Recommendations with Data Science</title>
		<ptr target="https://zipongo.com/" />
		<imprint>
			<date type="published" when="2015">2015. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
