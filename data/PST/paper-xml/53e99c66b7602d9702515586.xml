<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reliability Mechanisms for SDD-1 : A System for Distributed Databases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Hammer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Shipman</surname></persName>
						</author>
						<author>
							<persName><surname>Catastrophes</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Computer Corporation of America</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reliability Mechanisms for SDD-1 : A System for Distributed Databases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3E96225C78BF95690005E10D1558D1E9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>distributed databases</term>
					<term>reliability</term>
					<term>recovery</term>
					<term>atomicity CR Categories: 4.33</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the reliability mechanisms of SDD-1, a prototype distributed database system being developed by the Computer Corporation of America. Reliability algorithms in SDD-1 center around the concept of the Reliable Network (RelNet). The RelNet is a communications medium incorporating facilities for site status monitoring, event timestamping, multiply buffered message delivery, and the atomic control of distributed transactions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION 1 .l The RelNet</head><p>This paper describes the Reliable Network, a subsystem of the SDD-1 distributed database management system, whose function it is to provide the level of reliability and robustness demanded of a system that is charged with the responsibility for an organization's data. One of the prime motivations for building a distributed system is to achieve reliability enhanced over that which would be provided by a single-site system; the redundancy of data and processors provided by a distributed system potentially enables it to continue in operation despite the failure of individual sites. (In a single-site system, of course, site failure causes the entire system to cease operation.) However, although a multisite system Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. presents opportunities for enhanced reliability, it also presents challenges in the same area, because the likelihood that some part of the total system will fail becomes much higher. The goal is to design and implement distributed systems that exhibit global robustness and toleration of local site failures, and that can continue to operate as a whole despite the asynchronous failures and optional recoveries of individual elements of the system.</p><p>The Reliable Network (known as the RelNet) has been designed to provide a set of capabilities to support such reliable distributed system operation. Its original conception was in the context of the SDD-1 system, and some of its features were motivated by the particular requirements of that system. However, we believe that the functional capabilities of the RelNet have wide applicability to distributed systems of many kinds; it may, in fact, represent a forerunner of a general reliable distributed operating system. This paper describes the functionality, architecture, and implementation techniques of the RelNet. The particular ways in which the RelNet is used to support reliable operation of SDD-1 are described in <ref type="bibr" target="#b2">[3]</ref>&lt;</p><p>The RelNet consists of a set of facilities intended to ensure reliable communication and coordination among related processes operating at sites connected by means of a communications network. In a distributed system, a function will in general be realized by means of a number of processes, executing in parallel at distinct sites of a network. As execution of these processes proceeds, they will find occasion to communicate and synchronize with each other. The designer of a distributed system will have to recognize the reality that individual sites and processes in this system may fail at any point in time; consequently, each site must be prepared to recognize and react to the failure(s) of its "cohorts" <ref type="bibr" target="#b6">[7]</ref>, the other sites with which it cooperates and interacts. One approach would be to embed this responsibility in the application logic and code of each cohort. However, following general principles of good software design, it would be preferable to provide the application programmer with a (fictional) view of the environment, which exhibits a degree of reliability that simplifies his system design and implementation. This view is the RelNet. The RelNet provides each process running in the system with a set of facilities for reliable communication and interaction with other processes; these facilities can be utilized by invoking a set of procedure calls. Thus the RelNet is used instead of whatever communication facilities are provided by the actual communication network connecting the sites in the distributed system. Many implementations of the RelNet facilities would be possible; we have chosen to realize it "on top" of the real network, by means of a software front-end on each machine in the distributed system. This front-end represents an interface to the communication network, intervening between application programs and the operating/communication system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">RelNet Facilities</head><p>The basic function of any network is to allow for intersite communication. The RelNet can be effectively thought of as a virtual network that provides the following additional capabilities.</p><p>(1) There exists within the network a single Global Clock that can be accessed from any site. The function of such a clock is to impose a uniform and Implements a "virtual network" with a global clock facility and "clean" site failures and recoveries Enables communication between sites, regardless of status of recipient site Supports atomicity of distributed transactions consistent ordering on events occurring at different sites in a distributed system. The current value of the clock can be inspected by a user process. (2) Every site in the network is at any time in one of two states, UP or DOWN.</p><p>The UP state is characterized by correct operation and by timely reponse to messages sent by other sites; a site in the DOWN state is not operating at all. Transitions between these states (called "crashes" and "recoveries") occur instantaneously with respect to the global clock. A user process is provided with the ability to ascertain the current status of any site in the network, and to request that it be informed when that site changes its state. <ref type="bibr" target="#b2">(3)</ref> The RelNet provides a reliable communication service that makes two guarantees. First, that messages sent from one site to another are received in the same order that they are sent. Second, that on user request, a message can be marked for guaranteed delivery. That is, a message can be sent to a site that is DOWN, with the RelNet guaranteeing that the message will be received by that site upon its recovery. Receipt takes place even if the sending site is DOWN at the time the destination site recovers. (4) A facility for distributed transaction control is provided. This provides for a process running at one site to coordinate the activites of a number of "cohort" processes that are running at different sites and seeking to realize a global activity. The principal feature of this facility is its global abort/commit capability, which enables the controlling process to cancel the transaction at any point instantaneously or to signal its successful completion and cause the results to take effect uniformly at all involved sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Layered Architecture</head><p>The RelNet is itself organized and implemented as a series of software layers, each of which provides a subset of the facilities as a whole; furthermore, the lower layer capabilities are utilized in implementing those of the higher levels. The lowest level of the RelNet is known as the Global Time Layer, and it provides the global clock and site status features described above. The next layer is the Guaranteed Delivery Layer, which enables a user to send messages to DOWN sites, with the assurance that they will be received when the site recovers. The topmost layer of the RelNet is the Transaction Control Layer, which provides the ability to manage and coordinate a distributed transaction and deal with it atomically (see Table <ref type="table" target="#tab_0">I</ref>). We believe that this layered architecture contributes to the comprehensibility and implementability of the system as a whole.</p><p>The RelNet is a system constructed out of discrete components-sites and communications lines-each of which is subject to failure. It is our goal to achieve the desired level of reliable functionality by techniques that will allow for failures of some number of these components. In other words, the RelNet is designed to be resilient to the failure of some of its parts, and to function correctly so long as enough of the components behave correctly. However, no multicomponent system can survive the failure of too large a number of its constituents, and the same is true of the RelNet. When "too many" failures occur, the result is termed a catastrophe. Under catastrophe situations, the RelNet is not guaranteed to operate correctly. In some cases, it will simply fail to provide one of its functions, while in other cases it may operate in unanticipated and unpredictable ways. Although some catastrophe situations can be automatically detected by the RelNet, others can only be observed from outside the system. In either cases, manual intervention by a system administrator or other responsible human authority is necessary in order to rectify the situation. This will often entail shutting down part of the system and reinitializing it. The various catastrophes that can befall the RelNet facilities are described together with the individual mechanisms that realize them.</p><p>It is a principle of the RelNet design that, although catastrophes cannot be entirely avoided, they can be made arbitrarily unlikely by the increased replication of reliability mechanisms. In other words, the reliability of the RelNet is parameterizable in terms of such factors as the number of sites performing various backup functions. The price to be paid for such increased reliability is, of course, commensurate increased overhead. The trade-off between the two is one that must be made by the system administrator, depending on such factors as the requirements of his application and the individual reliabilities of his system components. Having made his decision, the system administrator implements it by selecting values for a number of parameters that are identified in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Assumptions</head><p>The components out of which the RelNet is constructed are computer sites and communications lines connecting them. We assume that the communications lines are already organized into a basic computer communication network with a conventional set of capabilities. Consequently, failures of individual communications lines are not explicitly considered or addressed by the RelNet; they are the province of the underlying network. That is, the RelNet assumes that the underlying network will detect the failure of a communication line between two sites and employ others to send messages between them. Furthermore, the RelNet assumes that the network at all times remains fully connected; that is, that there is at all times some path of communications lines between any two sites in the system. Should this assumption be violated by the failure of key communications lines that result in certain sites being disconnected from others, the result is a RelNet catastrophe, known as a partition catastrophe. For a brief discussion of the issues involved in partition catastrophe, see [El. Other communications failures are not addressed in this paper. Further assumptions about the capabilities of the communications network are described in Section 2.</p><p>We also make assumptions about the way in which the computer sites of the system may fail. First, we assume that every site is equipped with a "stable storage" mechanism. This is a device that enables any site to ensure that critical information that it has received can be stored in a way that will enable it to survive a failure of the site. Techniques for implementing stable storage are discussed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">171</ref>. We do allow for failures to occur asynchronously, and at any point in the system's operation. However, we do by and large restrict our attention to "clean" failures, in which the site completely ceases operation. We further assume that a site that is recovering from failure is aware of that fact and can be made to institute suitable recovery procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Relation to Previous Work</head><p>A number of other researchers have considered the problem of reliability in a distributed system [l, 9, 11, 12, 14, 161; however, the RelNet differs significantly from their work. In the first place, the RelNet is, to the best of our knowledge, the only attempt thus far to develop a complete and integrated facility for reliable distributed database system operation, one which addresses the full range of issues that arise in that context. Individual problems have been studied in isolation, but the interactions among them had not been previously explored.</p><p>Furthermore, previously proposed solutions to specific reliability problems are not directly applicable to the SDD-1 environment; this is primarily a consequence of our insistence that no transaction be forced to delay its execution until another site has recovered from failure (except when the failed site possesses the only copy of some data that the transaction requires). In some cases we have been able to build on earlier work, while in others entirely new mechanisms were required. Finally, a number of the facilities and features of the RelNet have been motivated by the particular needs and requirements of SDD-1; this has led to a specific structure of our reliability techniques, which is based on a global clock ----lll~~~~??c~ul~.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.7">Structure of the Paper</head><p>This paper seeks to set forth the basic facilities of the RelNet and a particular set of implementation mechanisms that can be used to realize them. Our presentation follows the layered architecture of the RelNet itself. For each layer, we identify the functionality that it provides and then describe its implementation. In general, the implementation of each layer will be expressed in terms of the facilities provided by the lower layers of the system. There are two aspects to the implementation of each RelNet mechanism; the first concerns how sites are to behave under normal operation, and the second how they react to the failure and recovery of sites (including themselves). Consequently, an important part of the RelNet implementation is concerned with how a recovering site manages to bring itself back into normal operation. In the RelNet, site recovery is also a layered operation, corresponding to the layers of RelNet implementation. Thus a recovering site will first execute the lowest level of recovery mechanism, then the next, and so on, until it has completed the entire process; at that point, it is considered to be fully recovered.</p><p>Distributed system reliability is an extremely intricate problem. Here we focus on the fundamental concepts of the mechanisms employed in the RelNet and indicate by footnote where they need extension in order to handle special cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE MESSAGE TRANSMISSION LAYER</head><p>The lowest level of the RelNet consists of the basic message transmission facilities of the underlying computer network on top of which the RelNet is constructed. (In the SDD-1 implementation, the ARPANET is used.) For our purposes, we shall assume that this layer provides facilities for sending and receiving messages between sites. However, no guarantees are made by this layer that a message that is sent by one site will actually be received at the intended destination. In particular, a message is lost if the receiver fails before the message arrives. The only guarantee that the Message Transmission Layer does provide is the following. If receiver A receives two messages from sender B, then they are received in the order in which they were sent; furthermore, if A did not fail between the receipt of these two messages, then it also received any other messages sent to it by B between these two messages. The failure to meet this guarantee is a RelNet catastrophe, which can have a variety of impacts on the higher levels of the RelNet and on SDD-1 user processes.</p><p>In general, the only way for a sender to be sure that a message has been received by the destination is to require an acknowledging response from the destination site. Such acknowledgment protocols, however, are not provided by the Message Transmission Layer but are the responsibility of programs that use it. In particular, higher levels of the RelNet expect to receive various responses to or acknowledgments of the messages that they send. These responses are typically returned by the same level of the system that is responsible for issuing the original message. Frequently, the sender will employ a time-out mechanism to limit the amount of time it will wait for such a response; if no such response is received within this period, the sender will assume that the intended recipient is 7bte actio*~all%iskn subsequent sections of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE GLOBAL TIME LAYER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction</head><p>Since it is a distributed system, SDD-1 must possess some mechanism to allow it to coordinate and synchronize actions being performed at different sites in a network. It employs a global clock mechanism for this purpose. A clock is simply a uniformly increasing counter whose values can be associated with events; this technique is known as event timestamping. Timestamps must be consistent with the order of occurrence of events so that a later event has a greater timestamp. Among the events that need to be consistently ordered by timestamps are the sending and receiving of messages between sites and the failure and recovery of sites. The principal function of the Global Time Layer of the RelNet is to provide SDD-1 with such a consistent and accurate global clock mechanism. In order to do so, it must also encompass the functions of message transmittal and reception and the monitoring of site status.</p><p>Specifically, the Global Time Layer presents to higher levels of the RelNet and to SDD-1 a virtual network with the following characteristics: (1) There exists within the network a single global clock by means of which events at any site in the system can be timestamped and thereby ordered; (2) every site in the network is at any time in one of two states, UP or DOWN; and (3) transitions between these states, called crashes and recoveries, occur instantaneously with respect to the global clock. The interface to the Global Time Layer provides higher levels of the RelNet and user processes the following abilities: To send a message to another site, which will be timestamped with the value of the global clock at the time that it is sent; to receive a message; to determine the value of the global clock; to determine the current status of any site in the network; and to request that a "Watch" be placed on any site, so that the requesting process is notified when that site changes its status. The central concept in the Global Time Layer is that of a global clock; this is a mechanism used to achieve an ordering of events occurring in a distributed system. Lamport observes that events in a distributed system should be considered partially rather than totally ordered <ref type="bibr" target="#b7">[8]</ref>. That is, a relative order need be established between events in different processes only if there is some communication between the processes that could serve to pass information about event occurrence from one to the other and thus enable knowledge of one event to influence the other. A global clock can be used to order events by associating with each event the value of the clock at the time of its occurrence. The clock value associated with an event is known as its timestamp. The absolute value of a timestamp is of no interest; only the relative values of those of ordered events need concern us. (In other words, the principal requirement of a global clock is that it support and model our notion of causality.) The following rules determine when two events need have a defined order.</p><p>(1) Events within a single process are totally ordered by their execution sequence.</p><p>(2) If process B learns of event 1 in process A before performing event 2, then event 1 must precede event 2.</p><p>In this section, we shall see how the Global Time Layer is implemented by means of local facilities (especially local clocks and status tables). The design task with which we are faced is to coordinate local clocks and local status tables in such a way as to present an interface that will simplify the design of procedures operating outside the Global Time Layer.</p><p>The implementation of the Global Time Layer is itself a layered one; however, the lower layers do not necessarily provide coherent and useful packages of capabilities, but are designed so as to realize a structured implementation of the Global Time Layer. Each layer in the implementation performs its functions by calling upon the facilities provided by lower levels. In some cases, similar facilities are provided at several levels; for example, each layer has Send and Receive primitives. However, the clean separation of the layers should contribute to the system's understandability and implementability. Table <ref type="table" target="#tab_1">II</ref> summarizes the structure of the Global Time Layer. Table <ref type="table" target="#tab_1">III</ref> presents a more detailed summary of all the layers of the RelNet. For each layer, it illustrates the interface that the layer presents (in the form of procedure calls available to higher levels), the special messages that the layer sends and processes  in implementing its functionality, and the internal data structures that it employs. The reader may wish to consult Table III during the ensuing exposition of RelNet operation and implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Local Clock Layer</head><p>The first level of the Global Time Layer is the Local Clock Layer. This implements a local clock facility that is used by higher levels of the system to simulate a uniform and consistent network-wide global clock. A local clock is simply a monotonically increasing counter that is logically independent of any real-time measurement. The local clock can be read to provide timestamps for events occurring at the site. In particular, a timestamp will be assigned to every message sent from one site to another. These timestamps are constructed by appending the current local clock value (as high-order bits) to the unique site identifier (as low-order bits). After a timestamp has been requested, the clock value will be incremented so that the next timestamp assigned will differ from the last one.</p><p>In brief, the interface to the Local Clock Layer consists of the following functions:</p><p>Readclock( ), which returns the current value of the local clock. Bumpclock(n),which increments the value of the clock to be greater than the value n (this has no effect if the clock value is already greater than n). Send(m,d), which assigns a timestamp to the message m and dispatches it to its destination d. Receive( ), which receives a (timestamped) message.</p><p>In order to support the global ordering of events demanded by a global clock, the Local Clock Layer will examine the timestamp of each message that it receives. If its value is greater than that of the current local clock, then the Local Clock Layer will bump the local clock beyond the timestamp value. In this way the (local) time at which a recipient receives a message is greater than the (local) time at which the sender sent it. (This capability is necessary but not sufficient for implementing a full global clock; the remaining issues are dealt with in a subsequent section.)</p><p>During site recovery from failure, the Local Clock Layer simply sets the value of the local clock to be 0. Subsequent receipt of timestamped messages from other sites and attendant clock manipulations will restore the local clock to an appropriate value.</p><p>In addition to the logical clock facilities just described, the RelNet depends on the existence of a local real-time clock at each site. This clock is principally used to implement a time-out feature by means of which a site that does not respond to a message within a given period is assumed to be no longer operating. In general, there need be no relationship between a site's real time and logical clocks; the former is used to measure actual time, while the latter is for timestamping events. However, in some cases (described in a subsequent section), it is desirable that over a period of elapsed real time, an equal amount of logical time should also have elapsed. To this end, the two clocks are kept in rough synchrony by the following technique. The two clocks are commensurable, in that they both employ the same units. The real-time clock is coarse grained and the logical clock is fine grained. For example, each time a new timestamp is required, the value of the logical clock will be incremented by 1, while each half-second (say), the real-time clock will be incremented by 10,000 (say). In addition, each time the real-time clock is updated, the logical clock is bumped past the new value of the real-time clock. That is, if the new value of the real-time clock is greater than that of the logical clock, the latter is pushed beyond that value. The value by which the realtime clock is incremented is chosen to be large enough so that as a rule, the logical clock will not advance that much on its own within the interval between real-time clock advances. (Occasional lapses from this assumption are not critical.)</p><p>The net effect of this mechanism is to bound the discrepancy between the logical and the real-time clocks. In this way, elapsed real time can be roughly measured on the logical clock when necessary.' This rough synchrony between logical and real clocks has two additional desirable consequences. First, it keeps the logical clocks at different sites approximately synchronized (assuming that their real-time clocks are). This is desirable for reasons of efficiency in the SDD-1 concurrency control mechanisms <ref type="bibr" target="#b2">[3]</ref>. In addition, in the event of a catastrophe that requires logical clocks to be manually reset, the system administrator can employ the real-time clock in this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Local Status Layer</head><p>The Local Status Layer provides a set of primitives for manipulating and utilizing a site's Local Status Table, which is used to represent the site's view of the condition of all other sites in the network. These primitives are used to support the simulation of the global clock and in informing user processes of the current status of sites with which they seek to communicate.</p><p>For each site in the network (including itself) a site's Local Status Layer maintains an entry in the Local Status Table that specifies the condition of that site. (The entry for the site itself is handled in a special way, as discussed below.) Entries are made into the table upon instruction from higher layers of the Global Time Layer implementation.</p><p>The only status values that may be entered for a site are UP and DOWN.</p><p>The interface to the Local Status Layer provides the following capabilities:</p><p>(1) sending and receiving messages;</p><p>(2) setting the status value of any site (via Markup and Markdown primitives):</p><p>(3) requesting that a Watch be placed on a given site (or removed from it).</p><p>The Watch facility enables higher levels of the RelNet and user processes to state that they wish to be informed when a given site achieves a specified status. (Thus there are two types of Watch: one which waits for a site to become UP and one for it to go DOWN.) The Local Status Layer will determine when that situation obtains and interrupt the requesting process to inform it that the condition has been met. (If the site is already in the designated state when the Watch is issued, the call to set the Watch will immediately return.)</p><p>The Local Status Table has an entry for each site in the network, which states whether that site has last been observed to be UP or DOWN. Furthermore, flags may be set to indicate if a Watch has been set on the site and by which user processes.</p><p>Detection of site status change (either a crash from UP to DOWN or a recovery from DOWN to UP) is not performed by the Local Status Layer. The Local Status Layer is only responsible for managing the status table; all changes to a site's status are initiated from higher levels of the Global Time Layer. When such a change is detected, the Local Status Layer is instructed (by means of the Markdown and Markup primitives) to change the appropriate entry in the Local Status Table . It is at this point that any processes that have requested a Watch against the site will be interrupted.</p><p>When the Local Status Layer is given a message to send to a site, it first inspects the Local Status Table entry for that site. If the site is listed as DOWN, it discards the message and informs the issuing process that the message could not be delivered because the destination site is down. (The sending process could then decide to take other steps, or it could issue a recovery Watch on the site in question and resend the message when that event occurs.) If the site is listed as UP, the Local Status Layer sends the message to the destination, using the send primitives of the Local Clock Layer.</p><p>All messages destined for user processes or higher levels of the RelNet also pass through the Local Status Layer. Upon receiving a message, the Local Status Layer will first check the status of the sending site. (Let us call the sending site A and the receiving site B.) If the site A is marked as UP at B, then the message is simply passed through. If the site is marked DOWN, then the nature of the message is examined; one message type is handled differently from all others. An I'M-UP message is sent out by a site upon its recovery; such messages are sent and processed by a higher level of the RelNet. When B's Local Status Layer receives an I'M-UP message from A (a site that it has marked as DOWN), it passes through the message to be processed by a higher level of the system. If B's Local Status Layer receives any other kind of message from A, then B had incorrectly assumed that A was down (when in fact it had probably only been slow to respond to some earlier message). However, B may already have taken some action based on the assumption that A was down; consequently, it must force A to indeed be down. This is accomplished by sending A a YOU'RE-DOWN message. This message, when processed by A's Local Status Layer, will cause it to behave as though it had indeed failed, thereby validating the assumption made by B. (Subsequent recovery of A will then be instituted.)</p><p>In addition to issuing YOU'RE-DOWN messages, the Local Status Layer also has responsibility for processing incoming YOU'RE-DOWN messages. Upon receiving such a message, it should cause the local site to crash (and then recover). (This might be accomplished simply by transferring control to the RelNet recovery module.) This should be done even if the sender of the YOU'RE-DOWN message is a site that itself is marked as DOWN. Such an occurrence indicates that two sites had made similar and mistaken assumptions about each other. In this case, the receiving site both issues a YOU'RE-DOWN message to the sender and acts on the YOU'RE-DOWN message that it received. On recovery, the Local Status Layer simply sets the status of all sites in the network to be UP, except for its own status; that is set to be DOWN. Subsequent analyses of the responses to the I'M-UP messages (which are received and processed by a higher level of the system during recovery) will cause these values to be set more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Global Clock Layer</head><p>The purpose of the Global Clock Layer is to present to higher levels of the RelNet and to user processes the view that the network as a whole possesses a single, uniform clock, which is used by all sites to timestamp messages and thereby to order events. That is, above this level it will appear that timestamps are assigned by a single global clock residing within the RelNet. The essential property of a global clock is that it be consistent with intersite event ordering. This means that if an event occurs at site A at time tl, then an event that occurs at site B after B learns of the A event must occur at time tz, where tz &gt; tl. In reality of course, there is no "global clock"; rather, each site operates according to the timestamps issued by its own local clock. Therefore, the goal of the Global Clock Layer is to simulate a global clock by means of a collection of independently running local clocks.</p><p>The interface of the Global Clock Layer provides the ability to send and receive messages, to examine and increment the value of the global clock, and to assure (by explicitly crashing it) that a site assumed to be DOWN is indeed in that state. In addition, the primitives of the Local Status Layer for manipulating the Local Status Table are passed through to higher levels of the RelNet.</p><p>The local clock mechanism described above almost realizes a global clock. That is, by timestamping every message and incrementing the local clock upon receipt of a message to be greater than the timestamp on the message, the collection of local clocks almost provides an ordering of events occurring at different sites that models their actual times of occurrence and influence on one another. However, there is a way in which the local clock facility does not truly achieve a network clock; this is caused by implicit communication through the observation of a site's failure. In our discussion of local clocks, we assumed that all communication between sites occurred through the medium of timestamped messages, and that consequently incrementing a local clock beyond the timestamp of any incoming messages sufficed to order events in the system appropriately. However, the detection of one site's failure by another also represents a form of intersite communication, and it too must conform to the requirements of a global clock; that is, the (B-local) time at which B discovers that A has crashed must be greater than the (A-local) time at which A did crash. In other words, the detection of a site's crash is equivalent to the receipt of a (hypothetical) I-HAVE-CRASHED message, which is however untimestamped. When B detects A's failure, B should increment its local clock to be greater than the value of A's local clock at the time A failed. Unfortunately, it requires additional mechanism to enable B to determine the (A-local) time at which A failed, since A's clock is unavailable for inspection after A has crashed.</p><p>The Global Clock Layer uses the mechanisms of guardians and TIMESIGNAL messages to construct a true global clock out of a collection of local clocks. The basic concept of this mechanism is that for each site, a set of "guardians" is maintained, each of whose clocks is guaranteed to be at most a constant value less than the value of the guarded site's clock; this is achieved by means of special messages (TIMESIGNALs) sent among these sites. Then when the failure of site A is detected by site B, B will be informed by one of A's guardians of an upper bound on A's clock at the time it failed; B can then bump its own clock to be greater than this value. This will achieve the event ordering demanded by a global clock.</p><p>First we shall describe how guardians are implemented. Each site W has associated with it a fixed set of guardian sites, G1,G2, . . . , Gk. W is called the ward of its guardians. The system is designed to guarantee the following constraint.</p><p>Guardian Constraint. If a site W and one of its guardians G are both UP, then Clock(G) + Tdelta &gt; Clock(W), where Tdelta is a fixed system parameter and Clock(x) is the latest timestamp to have been issued by the local clock at site X.</p><p>This guarantee is implemented by having W maintain counters that tell it a lower bound on the clock values of each of its guardians. Before issuing a timestamp, W must check that this new timestamp cannot possibly violate the Guardian Constraint. If it might cause its violation, then W cannot issue the timestamp; instead, it will have to wait until new messages are received from the guardians that inform it that their clocks have advanced far enough so that the new timestamp will be consistent with their clocks under the terms of the Guardian Constraint. To avoid ever getting into such situations, in which it will have to wait before issuing a timestamp that it wishes to use, W will periodically issue TIMESIGNAL messages, which have the effect of keeping the clocks of W's guardians in relatively close proximity to W's own. Specifically, for each of its guardians G, each site W maintains LBClock(G,W), which simply contains the value of the timestamp on the latest message from G received by W. (W can be sure that if G is up, then G's local clock is greater than LBClock(G,W), and if it is down, then G's clock at the time of failure was greater than LBClock(G,W).) Our goal is to ensure that W's clock does not get more than Tdelta ahead of LBClock(G,W), for each G that is up. This is accomplished by having the Global Clock Layer monitor the values of LBClock(G,W). Whenever it observed that Clock(W) &gt; LBClock(G,W) + Tdelta -RTMD, where RTMD is the typical network round trip message delay, then W should send a TIMESIGNAL message to G. The TIMESIGNAL message is a timestamped but otherwise null message that requires a response. A received TI-MESIGNAL message is processed by the Global Clock Layer of the recipient site, which returns the expected response, itself a timestamped message. If no response to a TIMESIGNAL message is received within a specified time-out period, then the issuing site assumes that the guardian site has failed and invokes Crashsite against it? When the condition just cited holds, it indicates that G's clock may be approaching being more than Tdelta behind W's; sending the TIMESIGNAL message will cause G's local clock to be incremented past the value of W's clock at the time that the signal is sent and bring them closer into proximity. The particular value of RTMD is chosen so that by the time W's own clock has advanced by RTMD, the response to the TIMESIGNAL message can be expected to have been received, enabling W to increment LBClock(G,W).</p><p>(Note that there is an interaction between the logical and the real-time clocks here. The goal of the TIMESIGNAL messages is to keep the logical clocks of the guardian and the ward in approximately synchrony, yet the condition governing their issuance is expressed in terms of the average round trip message delay, a real-time quantity. In general, the elapsing of a real-time period (such as RTMD) would not necessarily coincide with an equivalent change in a site's logical clock. It is to address this problem that the coupling of the two clocks, described previously, is performed. Therefore, the specified computation can be performed using logical clock values exclusively.)</p><p>To summarize, before a site W can issue a timestamp (i.e., assign it to a message), it must make sure that the issuance of this timestamp does not violate the Guardian Constraint. Therefore, the Send primitive of the Global Clock Layer, before passing on an outbound message to the lower layers of the Global Time Layer implementation, must first determine the timestamp that will be assigned that message by the Local Clock Layer and check it against the LBClock values. If issuing the timestamp (i.e., sending the message) would violate the Guardian Constraint, the W must wait until it receives some additional messages from its guardians that enable it to increment the LBClock values and thereby allow the timestamp to be legally issued. (In other words, in such a case the Global Clock Layer will not complete the sending of the message until that later time.) The purpose of the TIMESIGNAL message is to avoid forcing a site to wait (arbitrarily long) periods before issuing a new timestamp. Some additional comments are in order concerning the guardian mechanism. The number of guardians is a system parameter and represents a trade-off between cost and reliability. In order to operate correctly, the Global Clock Layer requires that at least one of a site's guardians be UP while the site is down; this argues for a larger number of guardians. However, there is expense (principally in terms of message traffic) associated with an increased number of guardians. It should also be observed that one site can serve as the guardian of several other sites, and that in particular two sites can be each other's guardians.</p><p>The Crashsite procedure has been alluded to above as the mechanism employed when a site fails to respond to a message within an anticipated time-out period. The functions of Crashsite are to ensure that the site is really DOWN and not just slow to respond, to mark the site as DOWN in the Local Status Table, and to increment the local clock in such a way that the timestamps of any messages subsequently locally issued will be greater than the time of the site's failure. (This latter action is required for the simulation of a global clock.)</p><p>The implementation of Crashsite has two versions: The first is performed by a site that is a valid guardian of the timed-out site, and the second is used in all other cases. A valid guardian of a site is defined to be a guardian that believes itself to be UP, that is, a guardian of a site whose own value in its Local Status Table is UP. (Recall that during the first stages of its recovery from a failure, a site is operating but has its own entry in its Local Status Table set to DOWN. This is not switched until a later stage of recovery. Until that point, the site's own local clock is not yet accurate, and so it ought not perform the guardian version of Crashsite, since, as described below, that impacts the clock of the timed-out site.) In the first case, the procedure is as follows.</p><p>( It should be observed that these two steps must be performed atomically; that is, the local clock cannot be accessed or updated by any other process between these steps. This is necessary to ensure that site status and the clock value are mutually consistent.</p><p>If Crashsite is not being invoked at a guardian against one of its wards, then the following procedure is followed.</p><p>(1) A CRASHREPORT message is sent to some (arbitrarily chosen) guardian of the timed-out site.</p><p>(2) A response to this message is received. (If the guardian does not respond within the time-out period, then Crashsite is invoked against the guardian, and a new guardian for the original site is selected for step (1) .) (3) The timed-out site is marked as DOWN in the Local Status Table by means of the Markdown primitive.</p><p>The CRASHREPORT message, when received by a guardian, is processed by its Global Clock Layer. Specifically, the guardian behaves as if it itself had timedout the ward. The guardian invokes Crashsite (version 1) locally against the timed-out site and then returns a (null but timestamped) response to the CRASHREPORT.</p><p>The timestamp on the response will be the guardian's local clock value after it completes local execution of Crashsite (which in turn is guaranteed to be greater than the clock value of the timed-out site). Consequently, when the response is received by the site that issued the CRASHREPORT, its clock will be pushed past that of the timed-out site at the time of its crash, consistent with the order imposed by a global clock. Furthermore, the secondary execution of Crashsite at the guardian will have had the effect of crashing the timed-out site if it had not really been DOWN.</p><p>It should be noted that performing Crashsite does not entail notifying all sites in the network that the site in question has failed. Only the site discovering the fact (and one of the failed site's guardians) need know of the failure. Our approach is that only sites that attempt to interact with a site need know its status, and that each of these can learn of a failure independently. This approach obviates the need for synchronizing the communication of site failure and recovery information among all the other sites in the network; this latter issue becomes especially troublesome in the presence of additional failures and recoveries. Instead, each site makes its own determination of the status of other sites. The necessary consistency among these interpretations is provided by means of the global clock. If a site recovers before another one has learned of its failure, then the I'M-UP message (see below) and its processing will ensure that any assumptions made about the failed site's clock are universally upheld.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The Global Status Layer</head><p>The Global Status Layer is the topmost level of the Global Time Layer implementation As such, it is responsible for coordinating the various facilities provided by the layers beneath it and presenting a virtual network with the following characteristics: the existence of a single global clock by means of which all events in the system are timestamped and thereby ordered, every site is at any time in one of two states, either UP or DOWN; transitions between these states, called crashes and recoveries, occur instantaneously with respect to the global clock. The interface to the Global Status Layer provides the abilities to send and receive messages that are timestamped by the global clock, to inquire as to the state of any site, and to set a Watch on any site. These latter two sets of routines return (status,timestamp) pairs, which indicate that when the Network Clock was at the time indicated by the timestamp, then the status of the site was that specified. This capability is implemented so that the timestamp returned is a current timestamp, rather than one which is obsolete and consequently of little interest, and is demanded in this form by the SDD-1 concurrency control mechanisms.</p><p>As has been mentioned, the Global Time Layer provides the view that any site is either UP or DOWN at any time. In reality, of course, this is a fiction. Rather, the behavior of a site in a distributed system can be characterized by one of the following four states.</p><p>(1) DOWN: that is, not operating.</p><p>(2) SLOW: Running, but slow to respond to messages, that is, not acknowledging messages within a prespecified time-out interval. (3) FAULTY: Running, responding to messages within the time-out interval, but operating incorrectly, that is, producing erroneous messages. (4) UP: Running correctly, and responding to messages within the time-out interval.</p><p>It is impossible to distinguish accurately among all these four possibilities. A foreign site cannot determine whether another site is failing to respond because it is DOWN or merely because it is SLOW. Consequently, the Global Time Layer merges these two cases. Any site that fails to respond to a message that demands a response within a time-out interval is assumed to be DOWN; however, the system recognizes the possibility that it may merely be SLOW and takes appropriate action (in the form of YOU'RE-DOWN messages). On the other hand, it is not possible for a message handler, such as the RelNet, to distinguish between a site's being FAULTY and its being UP. Consequently, the Local Status Layer only records sites as being UP or DOWN, and the Global Status Layer maintains this view. (However, the RelNet does incorporate the capability for components outside the RelNet to explicitly crash a site that is believed to be FAULTY. This is accomplished by exporting the Crashsite procedure.)</p><p>We observe that the Local Status Table and the Global Clock together realize the property that if the site A is marked DOWN in B's Local Status Table, then A crashed before its local clock reached the value of B's clock when it finds the DOWN entry in the table. This fact is exploited by the routine that handles inquiries into the status of a site. The operation of this routine is as follows.</p><p>(1) Read the value of the Global Clock into t.</p><p>(2) Examine the entry in the Local Status Table for the site in question.</p><p>(3) If it is listed as DOWN, then return (DOWN,t); this indicates that the site crashed before time t (and that it will recover after time t). Note that t is a "current timestamp," since it has just been returned by a read of the global clock. (4) If the site is listed as UP, then it is still possible that the site is not really UP, that it failed at a time prior to that returned by the global clock, and that this fact has simply not yet been recorded in the Local Status Table . To clarify this, a PROBE is sent to the specified site.</p><p>(a) If a response to the PROBE is received within the specified time-out period, then the site is indeed known to be up at time t, and the pair (UP,t) can be returned. (b) If no response is received, then it must be presumed that the site has failed. Then the Crashsite procedure is called, as it is whenever a site fails to respond within a time-out period. Control then transfers to step (1) of this procedure. The purpose of so doing is to get a new clock value at which to state that the site is DOWN.</p><p>The first two steps of this procedure must be performed atomically; that is, no changes of the Local Status Table can be allowed between the time the Global Clock is read and the Local Status Table is inspected. (Again, this is to ensure that the site status and clock value are mutually consistent.)</p><p>A user process may request the Global Status Layer to institute a Watch (of either failure or recovery varieties) on a designated site. The Watch primitive at this level will call the Watch primitive provided by the Local Status Layer to set the table entry. If the user has requested a failure Watch (notification when a site crashes), then the Global Status Layer will then periodically issue PROBE messages to the site being watched. If the site responds, the Watch continues; if it fails to respond within a time-out period, then it is assumed to have failed, Crashsite is invoked against it, and the caller is informed of the failure (together with a relevant timestamp, as specified above).</p><p>When a site recovers, the Global Status Layer is responsible for bringing it back to full operational status. It accomplishes this by sending I'M-UP messages ' M. Hammer and D. Shipman to all other sites in the network and then waiting for responses. Each such response is timestamped with the local clock value at the responding site. (Each time one of these responses is received, the Local Status Layer will automatically push the local clock past the timestamp on the response. This will have the effect of pushing the clock of the recovering site past any time that another site may have thought that it was DOWN.) If some site does not respond to the I'M-UP message, then the recovering site invokes Crashsite against that site; this will also have the effect of pushing the local clock of the recovering site past the clock value of the (presumably) failed site. After each site has responded or been crashed, the recovering site is ready to resume operation; it does so by calling Markup to set its own value in the Local Status Table to be UP and then transferring control to an appropriate location.</p><p>By symmetry, the Global Status Layer processes I'M-UP messages received from other sites. It does so by calling Markup on the issuing site to cause it to be marked as UP in the Local Status Table ; it then issues a response to the recovering site. Note that the change to the Local Status Table will cause any processes that had issues a Watch against the recovering site to be interrupted and notified of its change in status.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Summary</head><p>The Global Status Layer is the topmost stratum in the implementation of the Global Time Layer. Its primitives, plus some of those of the lower levels, constitute the facilities that the Global Time Layer provides to other parts of the RelNet and to SDD-1 user processes. Specifically, these include a Send primitive, which timestamps each message with the current value of the Global Clock; an associated Receive primitive; a primitive that returns the current status of any site in the network together with a value of the Global Clock at which that status is valid; and the Watch facilities, which notify the user when a designated site changes its status. All of these features are provided in the context of a consistent Global Clock; which accurately models the relative ordering of events occurring at different sites in the network. Some of these facilities are employed in the remaining sections of this paper, while others are needed by the SDD-1 concurrency control mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Catastrophe</head><p>The principal catastrophe situation that can befall the Global Time Layer is brought about by the failure of all of a site's guardians while the site is down. If this results, then other sites in the network will be unable to ascertain the value of the failed site's local clock at the time that it failed; this in turn prevents the accurate simulation of a global clock mechanism. This catastrophe can be detected from within the RelNet, because a site performing Crashsite against the site in question will find itself unable to communicate with any of that site's guardians. However, it would be unsafe for the system to proceed in the face of this catastrophe. By neglecting to synchronize accurately with the failed site's clock, the system would fail to uphold the global clock; in other words, the clock would fail to model the sequence of events in the system correctly. The result might be an inconsistent database whose contents do not represent the result of a legitimate sequence of database operations. In such an eventuality, a human system administrator manually sets the clocks so as to avoid conflict with the failed site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GUARANTEED DELIVERY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Introduction</head><p>It was observed in the discussion of the Message Transmission Layer that its facilities make no guarantee that a message sent will eventually be delivered to the destination site. The reason for this is that the intended recipient may fail before the message can be delivered. The situation is unacceptable for the SDD-1 context, since in order to ensure database consistency, a transaction updating a distributed database must be sure that certain messages (such as UPDATES <ref type="bibr" target="#b2">[3]</ref>) will be eventually delivered to all destination sites. SDD-1 demands a facility by which messages may be designated for "guaranteed delivery." Such messages would be assured of reaching their destination site irrespective of the current or future status of either the sender or receiver. This facility is provided by the Guaranteed Delivery Layer of the RelNet.</p><p>Specifically, the Guaranteed Delivery Layer affords the following functionality. Primitives are provided for sending and receiving messages. A sender may designate a message for "guaranteed delivery." In this case, the RelNet guarantees that if the destination site is currently down, it will receive the message upon its recovery. More precisely, the RelNet guarantees that if a sender sends two messages to a receiver, where the first message is marked for guaranteed delivery, then the receiver will receive the first message before the second. Note that the RelNet cannot guarantee that any message (even one marked for guaranteed delivery) is certain to be received, since the destination may never recover from its failure.</p><p>The Guaranteed Delivery Layer will provide the sender of a guaranteed message with the subsequent acknowledgment (via an interrupt, for example) when the message has been processed by the RelNet for eventual delivery to the destination. (This will entail making an appropriate number of copies of the message and storing them within the RelNet; this mechanism is detailed below.) Once it has received this acknowledgment, the sender can be certain the message will reach the destination, should the destination eventually recover from its failure. Upon receipt of a guaranteed message by the destination, the receiving process must provide a further acknowledgment to the Guaranteed Delivery Layer. This acknowledgment of receipt is a guarantee by the receiving process that the message has been or will be acted upon. (Typically this requires that the message has already been processed and its effects secured on stable storage or that the receiving process has placed the message on stable storage for future processing.) Only after this acknowledgment of receipt has been issued to the RelNet will the Guaranteed Delivery Layer consider the message to have been delivered. That is, if the destination site fails before issuing the acknowledgment, the Guaranteed Delivery Layer will again provide it with the message upon its subsequent recovery.</p><p>It should be noted that the Guaranteed Delivery Layer accepts for sending both guaranteed and nonguaranteed messages. Although no special handling is performed for the nonguaranteed messages, their proper sequencing with respect to guaranteed messages is assured. Thus between any sender-receiver pair, messages (guaranteed and nonguaranteed) will be received in the order sent. Nonguaranteed messages, however, may be lost; such losses occur during periods when the receiving site is down.</p><p>A further capability provided by the Guaranteed Delivery Layer is the Check primitive; this enables a site to determine if it has received all messages sent to it by another site before a given time. Thus a call on this primitive has two arguments: a site and a timestamp. A positive response is returned to the caller if all messages from the given site sent prior to the given timestamp have been received; that is, the response is positive if the next message received from that site is certain to have a higher timestamp than that given as an argument. Otherwise, the response is negative. This capability is employed in SDD-1 by a recovering site to ensure that it has received all messages that were sent to it while it was down, and to ensure that all messages sent by a failed site before it crashed have been received <ref type="bibr" target="#b2">[3]</ref>. The details of the implementation of this facility are straightforward and will not be discussed in this paper.</p><p>The guaranteed delivery of messages is accomplished by the use of a mechanism we call a Reliable Buffer. There is one such buffer for each destination site. When a user flags a message to a down site for guaranteed delivery, the RelNet establishes the message in that site's buffer; having done so, it returns an acknowledgment to the sending process, since (assuming the RelNet does not fail) the message will now be available for retrieval when the destination site recovers. During the recovery process of a failed site, it will request the RelNet to provide it with all messages in its Reliable Buffer. The recovering recipient will then establish these messages on its own stable storage, and upon completion of its recovery process, the site will process these messages as though they appeared normally in its input queue.</p><p>The Reliable Buffer is a mechanism internal to the RelNet; it is implemented by means of the appropriate replication and coordination of each message at several sites in the network. This approach differs significantly from a technique that has been called "persistent communication"</p><p>(see, e.g., [l]). In a persistent communication strategy, a message to be delivered to a crashed site is buffered only at the sending site. In such a situation, if the sender is down when the recipient site recovers, the message cannot be forwarded to the recipient. The assumption of a persistent communication scheme is that, at some point in the future, both the sending and the receiving sites will simultaneously be up and the message can be delivered at that time. This assumption is unsatisfactory for the SDD-1 environment. SDD-1 demands that a recovering site be immediately able to retrieve all messages sent to it while it was down; persistent communication does not provide this capability, because the sender might be down when the recipient recovers.</p><p>This capability of retrieving all messages sent prior to a given timestamp is needed for the efficient implementation of the SDD-1 concurrency control protocols. Under the SDD-1 concurrency control strategy, it is necessary, in synchronizing with a crashed site, to obtain all UPDATE messages sent by it before it failed. If these messages could not be immediately obtained, then it would be necessary for the concurrency control mechanisms to wait until such time as the messages could be obtained (see <ref type="bibr" target="#b2">[3]</ref>). Under a "persistent delivery" strategy as outlined above, this would mean that the synchronizing site would have to wait for the recovery of the site with which it was synchronizing. In the design of the SDD-1 reliability mechanisms, we have avoided any approach that might require one site to wait for others to recover, because such recovery might be arbitrarily delayed.</p><p>The problem with which we are faced is to design a Reliable Buffer that will be available in spite of site crashes. The presentation of a design for accomplishing this is the main topic of this section. An outline of the remainder of this section follows:</p><p>(1) We introduce the basic implementation of the Reliable Buffer. For purposes of robustness, the Reliable Buffer is replicated at a number of different sites, each replication being called a spoaler. (2) We discuss alternative strategies for recovering messages from the spoolers and for switching them cleanly from a spooling to a nonspooling mode. We present the details of the particular strategy that is used, under the simplifying assumption that no spooler crashes during the message recovery process. (3) We next consider the possibility of spooler crashes at three critical points in the algorithm.</p><p>(a) First, we consider the case in which the spooler crashes while messages are being removed from the spooler by the recovering site. In this case, the recovering site simply switches to a different spooler. However, the new spooler may have a different message ordering than the original spooler. The notion of an acknowledgment-vector is introduced to deal with this problem. (b) We then consider the possibility that a spooler site crashes while messages are being inserted into it by sending sites. We argue that the spooler site can recover by inserting a gap marker into its message stream to indicate a point at which it may be missing messages. (c) Third, we consider the possibility that the spooler crashes during the transition from spooling to nonspooling mode, and introduce mechanisms to eliminate the undesirable effects this may have. ( <ref type="formula">4</ref>) Finally, we consider the possibility that a recovering site crashes while it is removing messages from a spooler. We show that no harmful consequences result from this, so long as the acknowledgment-vector is maintained on stable storage. Furthermore, the mechanism operates correctly when spoolers as well as the recovering site crash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reliable Buffer Implemented As Multiple Spoolers</head><p>As mentioned above, there is a Reliable Buffer associated with each site, whose function it is to hold messagese sent to it while it was down. A Reliable Buffer is implemented as a set of physical buffers located at several sites in the network. These buffers are known as spoolers. Associated with each site is a set of spooler sites at which the Reliable Buffer for that site is implemented. A single spooler would, in general, be inadequate since it would be susceptible to crashing itself.</p><p>To protect against this occurrence, a number of spoolers are used for each site. In * M. Hammer and D. Shipman the sequel, we will assume that while the destination site is DOWN, at least one spooler is always UP. A RelNet catastrophe occurs if this is not the case. The basic strategy for spooler implementation is as follows. If a sender wishes to reliably buffer a message, the RelNet will send a copy of that message to all spoolers associated with the destination site. When all the spoolers have acknowledged receipt, the message is considered to be reliably buffered. When the recipient recovers, it issues a request to any one of its spoolers to obtain its buffered messages.</p><p>One might be tempted to believe that each spooler holds an identical copy of the (logical) Reliable Buffer. However, this need not be true, since different spoolers may contain the same messages in different orders. Consider the case of two spoolers Sl and S2 associated with a destination site C, and two sending sites, A and B. The following sequence of events may occur.</p><p>(1) A sends message Ml to Sl and receives acknowledgment of its receipt.</p><p>(2) B sends message M2 to S2 and receives acknowledgment of its receipt.</p><p>(3) A sends message Ml to S2 and receives acknowledgment. of its receipt. (4) B sends message M2 to Sl and receives acknowledgment of its receipt.</p><p>In this case Sl has Ml preceding M2, while in S2, M2 precedes Ml. However, both message orderings are "correct."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Alternatives</head><p>While the general strategy of using spoolers is not conceptually difficult, the details of the implementation may become quite intricate. Furthermore, a number of different implementation strategies are possible. We can outline three general approaches, differing in the way in which new messages destined for a recovering site are handled while the spoolers are being emptied by that site. The three approaches are as follows.</p><p>Strategy 1. Prior to emptying the spoolers, the recovering site sends a message to all sending sites indicating that they are to cease sending messages to it, either indirectly (via spoolers) or directly. Any messages destined to the recovering site are to be held at the sender. After the spoolers have been emptied, a further message is sent from the recovering site to the sending sites directing them to henceforth send messages directly to the recovering site. In particular, any pending messages being held at the sender may now be sent and will then be acknowledged on receipt. Strategy 2. Rather than hold their messages while the spoolers are being emptied, the sending sites continue to send messages to the spoolers. Eventually the spoolers will be emptied, after which new messages will be sent directly to the recovered site. (Since we can safely assume a receiver can receive messages faster than the senders are sending them, the spoolers will eventually be emptied.) Strategy 3. While the spoolers are being emptied, new messages are sent directly to the recovering site where they are kept in a temporary local buffer. After emptying the spoolers, the recovering site empties this local buffer before receiving any further messages directly from senders.</p><p>The strategies have been presented in order of increasing efficiency and complexity. Strategy 1 is simple to implement but suffers from the disadvantage that new messages cannot be acknowledged until the spoolers have been emptied by the recovering site. Since spooler emptying may be a lengthy process, this strategy was not deemed acceptable. Strategy 2 overcomes this problem but suffers from the disadvantage that even though the crashed site has recovered, messages to be sent to it must make two "hops" through the network, one to the spoolers and another from the spoolers to the recovering site. In Strategy 3 only one "hop" is needed, but the details of implementation are quite complex. Particularly troublesome are the problems raised should the recovering site crash while it is in the midst of emptying its spoolers. When it subsequently recovers, there will be both old and new messages in the spoolers; the old ones must precede, and the new ones must follow, the messages that had been placed in the temporary local buffer at the recovering site during its previous recoveries. Further, these old temporarily buffered messages must be kept separate from any new temporarily buffered messages that may arrive during recovery. After complete details had been developed for this approach, it was felt its improved performance did not justify the complexity of the resulting software. Consequently, Strategy 2 was selected for use in SDD-1, representing a trade-off between efficiency and simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Basic Implementation Algorithm</head><p>In this section we present the basic algorithm employed by the Guaranteed Delivery Layer in implementing its send and receive primitives for messages designated for guaranteed delivery. This implementation employs several of the facilities provided by the Global Time Layer. The algorithm below is cast in terms of a set of senders, a receiver, and a set of spoolers. (The generalization to multiple receivers is immediate.) We assume that the set of spooler sites is known to the RelNet component at the sending site. Each of these sites is in one of two modes (spooling or nonspooling). The collection as a whole is said to be nonspooling if both the sender and the receiver are in nonspooling mode, and spooling if the sender and the spoolers are in spooling mode and the receiver is either down or spooling. All other combinations of local states correspond to transient or illegal global states. Global state change is effected by sending messages to cause appropriate local state changes. Basically, the sender is responsible for changing the global state to spooling when it discovers that the receiver is down. The spoolers, after they have been emptied, are responsible for changing the global state back to the nonspooling mode. In the basic algorithm outlined below, we do not provide for spooler crashes. These are considered in the following section. The algorithm is expressed by describing how each site behaves when its local mode is spooling, when its local mode is nonspooling, and upon its recovery from a crash. To send a message, send the message directly to the receiver and await acknowledgment. If the acknowledgment is not received within the time-out period, invoke Crashsite against the receiver, send START-SPOOLING messages to all the indicating, for every sender site, the timestamp of the last message from that site that the receiver has received and acknowledged. Before emptying a new spooler, the receiver sends an ACKNOWLEDGMENT-VECTOR message which contains the receiver's current acknowledgment-vector.</p><p>Upon receipt of the ACKNOWL-EDGMENT-VECTOR message, the spooler uses it to delete all messages in its queue that have already been received by the receiver.</p><p>The acknowledgment-vector is also used by the receiver to allow it to ignore messages that have been previously received but are nonetheless received again. This can occur, for example, when a sender crashes after having sent a message to some, but not all, of the spoolers for the recipient. Suppose that the recipient then recovers and obtains the message from one of the spoolers that did get it. When the sender subsequently recovers, the process that initiated the message may resend it to the recipient, since the Guaranteed Delivery Layer only acknowledges the message after it has been established at all spoolers. Thus the recipient will receive two copies of this message.</p><p>We now consider the problem of spoolers that crash while the receiver is DOWN. If the spooler remains DOWN until the receiver has recovered and emptied some other spooler, no problems can arise. However, if a spooler does crash and subsequently recovers while the receiver is still DOWN, that spooler's message queue will reflect a "gap" during which it received no messages. If the receiver, upon its recovery, chooses to empty this spooler, then the receiver wilI not receive those messages sent while the spooler in question was down.</p><p>One simple solution to this problem would be to disallow the receiver from emptying spoolers that had crashed and recovered in this manner. The difficulty with this approach is that it will unnecessarily result in a catastrophe in those cases where every spooler has crashed at least once during the period that the receiver was down; even though every message sent to the receiver may be available in some buffer, no spooler would have them all. This would mean, under this approach, that at least one spooler must remain up during the entire period that the receiver is DOWN. This is an unreasonable expectation considering the fact that sites may be DOWN for very long periods of time. Instead, it would be better to have spoolers (on recovery) mark the gaps in their queues during which they were DOWN, and to have the receiver fti those gaps from messages held in other spoolers. Under this strategy, a catastrophe is prevented so long as that collection of spoolers that are up at the time of the receiver's recovery hold all of the messages that had been sent to it while the receiver was down.</p><p>To this end, the following conventions are followed: Whenever a spooler recovers, it immediately places a gap marker in its message buffer. This indicates the point at which messages may have been lost. Second, each sender remembers the timestamp of the last message it has sent to the receiver.3 (This can be accomplished by maintaining at each sender an array, PMT, which contains the previous message timestamp for each receiver. PMT must be maintained on stable storage.) When a sender sends a message to a spooler, it appends to that message the timestamp of the previous message that it sent to the receiver. When unspooling, the spooler now behaves as follows. When the spooler receives a NEXT-MESSAGE-PLEASE message and the next item in the buffer is a gap marker, a GAP message is sent to the recovering receiver. The gap marker remains in the message buffer. Upon receipt of such a GAP message, the receiver chooses another spooler from which to obtain the remainder of its messages. (The first step in this process, as described above, is to send the new spooler an ACKNOWLEDGMENT-VECTOR message, which it uses to delete messages already obtained elsewhere by the receiver.) The receiver then retrieves messages from this second spooler until the spooler is emptied, until this spooler crashes, or until another gap marker is encountered. In the latter two cases, the receiver will move on to another spooler; in particular, it may return to one that it had left earlier upon receiving a GAP message. (The receiver may return to an earlier spooler only after it has received at least one additional message from some other spooler. As usual, the recipient site will reestablish the interaction with the earlier spooler by sending it an ACKNOWLEDGMENT-VECTOR message.) In addition to deleting messages that the acknowledgment-vector indicates have already been received by the recipient, the receipt of an ACKNOWLEDG-MENT-VECTOR message by a spooler wilI cause it to delete any gap markers in its buffer that are no longer operative. Intuitively, a gap marker is operative if there may be messages for the receiver that are missing from the buffer and whose place is occupied by the gap marker. The spooler can establish if a gap marker is operative by examining the acknowledgment-vector sent to it by the receiver and the messages in its own queue. A gap marker is no longer operative if, for each sender site, there is a message in the buffer following the gap marker whose predecessor (as indicated by the previous message timestamp that is attached to each message) has already been received by the recipient. After deleting the inoperative gap markers, the spooler can resume sending spooled messages to the receiver.</p><p>We must also consider the situation in which the spooler crashes immediately after having sent a STOP-SPOOLING message to the receiver. A sender in this case may not know that the receiver has entered nonspooling mode and will continue to send messages to the spoolers that are still UP (who also have not learned of the state change). To prevent this situation, the receiver, after receiving a STOP-SPOOLING message from one spooler, does not immediately enter nonspooling mode. Instead, it switches to another spooler in the usual way and attempts to retrieve messages from it. (In most cases, the second spooler will not have any additional messages, but in the situation described above, where the first spooler crashed before notifying senders of the state change, there may indeed be some new messages there.) The receiver then enters nonspooling mode only after having received a STOP-SPOOLING message from all of the UP spoolers.</p><p>Finally, we must consider how the sender will deal with failing and recovering spoolers. The sender will crash any spooler that does not acknowledge receipt of a message sent to it; this will ensure that the spooling process is accurately begun and that messages are securely spooled. When a failed spooler recovers, the sender brings it into the spooling process by issuing it a START-SPOOLING message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Crash of the Recovering Receiver</head><p>The recipient may crash while it is in the process of unspooling. Upon its recovery, it simply chooses a spooler and resumes the unspooling operation. It should be noted that the acknowledgment-vector must be the same as at the time of the recipient's crash in order for messages not to be received twice. This requires that the recipient keep its acknowledgment-vector (or more precisely, the information that it contains) on stable storage. If the recipient maintains its input queue on stable storage, then the information needed to reconstruct the acknowledgment-vector is available from this input queue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Complete Algorithm for Reliable Buffer Implementation</head><p>The complete algorithm is summarized below: Upon entering spooling mode, a particular UP spooler is chosen. An ACKNOWL-EDGMENT-VECTOR message containing the current acknowledgment-vector is sent to the spooler. (A response is expected; if none is received within a time-out period, Crashsite is invoked against the spooler and another is selected.) Successive NEXT-MESSAGE-PLEASE messages are then sent to retrieve messages from the spooler. If the spooler does not respond to a NEXT-MESSAGE-PLEASE message within a time-out period, Crashsite is invoked against it and another spooler is selected for unspooling. If the spooler replies with a regular message, update the acknowledgment-vector, establish the message on the input queue on stable storage, and acknowledge it. If the spooler replies with a GAP message or a STOP-SPOOLING message, initiate unspooling from another spooler. The unspooling procedure terminates when a STOP-SPOOLING message has been received from all UP spoolers, at which point nonspooling mode is entered. (c) Upon recovery of receiver:</p><p>Reconstruct the acknowledgment-vector if necessary; enter spooling mode. Upon receipt of message from a sender, add message to the queue (on stable storage) and acknowledge. Upon receipt of ACKNOWLEDGMENT-VECTOR message from the receiver, delete all previously received messages from the queue, as well as gap markers that are no longer operative, and acknowledge. If the queue becomes empty, enter nonspooling mode. Upon receipt of NEXT-MESSAGE-PLEASE message from the receiver, if the next item in the queue is a gap marker, send a GAP message. If the next item in the queue is a message, strip the previous message timestamp from it and forward it to the receiver, and remove the message from the queue when the receiver has acknowledged it. If the buffer becomes empty, enter nonspooling mode. (c) Upon recovery of spooler:</p><p>Place gap marker in the message buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Spooler Catastrophe</head><p>The algorithm as just described requires that at least one spooler be UP whenever the receiver is down. When this condition is not met, a spooling catastrophe may occur. This catastrophe is detected by the sender when no UP spoolers are available for spooling. The catastrophe is detected by the receiver, when all spoolers that are UP return GAP messages in response to a NEXT-MESSAGE-PLEASE message. By providing for additional or more robust spoolers, the likelihood of spooler catastrophe can be decreased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">THE TRANSACTION CONTROL LAYER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Atomicity of Transactions</head><p>In this section, we describe how the RelNet supports the atomic execution of distributed database transactions, which access and modify data items that may be stored (and replicated) at several sites in the network. Reliability mechanisms are needed to ensure the correct execution of these transactions despite asynchronous site failures (in particular, those that occur during the execution of a transaction).</p><p>Following <ref type="bibr" target="#b4">[5]</ref>, we define a transaction as an atomic database operation at the user level. That is, the user is given the ability of grouping together a number of primitive database operations and designating the group to be a transaction; the system must then behave as if each such transaction is processed as an atomic, indivisible, unit. A transaction is specified to the system in terms of a sequence of actions, each action being an atomic operation at the system level. Even though execution of actions from different transactions may be interleaved by the system, it must preserve the outward appearance of having executed one transaction to completion before beginning another. The atomicity constraint guarantees, for example, that it is not possible for any transaction to read data that have been partially, but not completely, updated by another transaction. Nor is it possible for two or more transactions to interleave their read and write operations so as to result in "reader-writer" anomalies. Such an anomaly could result from a scenario in which one transaction, Tl, reads a variable, x; another transaction, T2, then reads the variable x; next transaction T2 updates x; and finally Tl updates x. (Consider what happens when both Tl and T2 both increment x by 1.) The difficulty arises because Tl's update is based on a data value that has become invalidated by T2's update.</p><p>It is the responsibility of the concurrency control mechanism of SDD-1 to guarantee the atomicity of transactions. Atomicity is achieved by forcing read operations to wait until appropriate update operations have completed before they are allowed to execute. (Discussion of the SDD-1 concurrency control techniques is given in <ref type="bibr" target="#b2">[3]</ref>, and a general survey of distributed concurrency control techniques can be found in <ref type="bibr" target="#b1">[2]</ref>.)</p><p>One may reasonably ask why reliability mechanisms are necessary for transaction atomicity if the concurrency control strategy is correct. There are two reasons. First, the concurrency control algorithms themselves must be made safe against site failures. Second, the execution of a transaction will typically entail the sending of update messages from one site to a number of other sites; and even with properly functioning concurrency control, the sending site may fail before having issued all of the update messages associated with some transaction. A situation in which some, but not all, of the update messages associated with a transaction have been received and processed results in database inconsistency and negates the principle of atomicity. Although the unsent messages, which lie "buried" at the failed site, could presumably be issued upon the node's recovery, any read operations waiting on the completion of that transaction would then be forced to wait until such time as the recovery actually occurred.4 Such delays would be intolerable. It is a general principle of SDD-1 that a trantiction should never be forced to wait for a site to recover in order to run to completion.5</p><p>The first issue identified above, that of failure-proofing the concurrency control mechanism, is dealt with elsewhere [3] and will not be covered here. The solution to that problem is based on the judicious use of RelNet capabilities.</p><p>The second issue, the problem of unsent updates at a failed site, is the central focus of this section. This problem can also be conceived of as that of "atomic message broadcast," enabling a site to send a collection of messages to different sites as a single package (i.e., eliminating the possibility of the sending site failing partway through the process). The problem is resolved by an atomic commit procedure. This procedure ensures that whenever any update messages become buried, the transaction with which they are associated will be aborted; i.e., none of its updates will be performed. Only when all of the update messages for a transaction have been issued will the transaction be committed; at that point, all of the updates will be guaranteed to transpire. Thus it will not be necessary for any read operation to wait for a site to recover before proceeding. In those cases where a read might have been forced to wait for a buried update, the updating transaction will be aborted, hence obviating the need for the read to continue waiting.</p><p>It is important to distinguish the intent of our atomic commit operation from similar mechanisms proposed in other contexts. For example, the two-phase commit operation of <ref type="bibr" target="#b6">[7]</ref>, upon which our atomic commit is based, is designed for systems in which an update message may be rejected (for example, as a result of concurrency control considerations). In such a scheme, any update message rejection mandates the abortion of the entire transaction, The two-phase commit protocol insures that no update takes place until all update messages have been accepted. However, this mechanism, as well as other proposed variants of it (e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>), may withhold the commit/abort decision until after a failed site has recovered (and given the opportunity to reject the update message). Thus a site may hold uncommitted update messages for long periods of time. As discussed above, this would not be acceptable in the SDD-1 environment, where other transactions may be waiting for commitment of the update messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transaction Control Functionality</head><p>The SDD-1 mechanisms for transaction control are oriented toward a particular model of transaction structure and operation. In this model, a transaction is invoked by user command at a given site. (Each transaction is assigned a unique identifier.) The invocation creates a process at that site, which is the controlling process for the transaction. This process may then cause the creation of other (cohort) processes (at this or at other sites); the transaction will be realized by coordinated activity of the set of processes. (A process is uniquely associated with a transaction, and is identified by a (transaction identifier, process identifier) pair.) The processes associated with a transaction may communicate with one another and perform local computations. At the end of the transaction, the controlling process sends out update messages, indicating to each site the changes that are to be made to certain database elements stored at the site. The update identifies the data elements and their new values. After this action is completed, the controller successfully terminates (commits) the transaction, causing the updates to take effect. At any point during its execution, the controller may abort the transaction; this may be caused by the failure of a site running one of the cohorts. Until the transaction is committed, it has no effects that can be seen by other transactions.</p><p>In order to support such atomic transactions, the RelNet provides the following capabilities.</p><p>(1) The capability to obtain a new transaction identifier. The requesting process will be the controlling process for the transaction.</p><p>(2) The capability to create a cohort process at the local or a foreign site. The new process will be associated with the same transaction as the requesting process and will be destroyed when the transaction is committed or aborted. (3) The capability to request that a transaction be committed or aborted. Both types of request signal the completion of the transaction and may be executed only by the transaction's controlling process. When the transaction is aborted, any updates that may have already been performed by the transaction will be undone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Implementation Environment for Transaction Control</head><p>In this section we examine the components from which the Transaction Control capability described above will be implemented. As the outermost software layer in the RelNet, the Transaction Control Layer may utilize the functionality provided by the Reliable Delivery, Status Monitoring, and Global Time components. In addition, it relies on a number of facilities assumed to be provided by the local operating system or database management system at each site. These latter capabilities include the following.</p><p>(1) The capability to create a new process and associate that process with a given transaction number.</p><p>(2) The capability to delete such processes.</p><p>(3) The capability to perform local database.updates in a tentative mode. Updates made in this mode are not installed until they have been committed. (4) The capability to ,abort all tentative updates associated with a transaction, restoring the original value to updated data items. (5) The capability to commit all tentative updates associated with a transaction, causing the new data item values to be visible.</p><p>It wiIl be noted that we are essentially assuming the existence of a local transaction control capability out of which we are building a distributed transaction control capability. The implementation of global process primitives in terms of message communication and the local primitives is straightforward and will not be discussed here. Our primary concern will be with the implementation of the global commit/abort facility in terms of the local commit/abort primitives. The difficult technical problem here is that of uniformly committing a transaction in the presence of local site failures and recoveries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Two-Phase Commit</head><p>A two-phase commit procedure, similar to the one described in <ref type="bibr" target="#b6">[7]</ref>, forms the core of our atomic commit mechanism. This procedure is described below. Let C be the controlling process of the transaction and U1, UZ, . . . , U, be those cohort processes for this transaction that perform local updating on its behalf.</p><p>Phase la. C issues UPDATE messages to U1 through U,. These cause the local databases to be updated in tentative mode in accordance with the instructions of the UPDATE Phase lb. C waits for Guaranteed Delivery Layer acknowledgment that these UPDATE messages have been delivered or reliably buffered. Phase 2a. C issues COMMIT messages to Ui through U,. These cause the transaction to be locally committed. Phase 2b. C waits for Guaranteed Delivery Layer acknowledgment of these COMMIT messages.</p><p>It should be noted that, in the RelNet environment, it is not necessary for the UPDATE message to actually reach the destination process. So long as the message is stable within the Guaranteed Delivery Layer, it is sure to eventually crash, then no takeover will occur. This constitutes a catastrophe and is discussed below.) (2) If a backup process, B,, fails while it is in control of the transaction, then again at most one backup process will take over. This will be the backup Bk that was watching B,, at the time of its failure. Having watched IB, fail, Bk will examine each of B,'s predecessors, find them all down, and eventually discover that C itself is down. At that point, Bk assumes control.</p><p>Thus, at most, one process, either C or one of its backup processes, will be in control at any given time. Having assumed control, a backup will proceed to issue COMMIT or ABORT messages, depending on its state; this issue is addressed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Atomic Commit with Backups</head><p>The following four-phase procedure is used to implement atomic commit in the RelNet. Below, C is the process initiating the commit, and U1, UB, . . . , U, are its cohort processes that perform local updating on behalf of the transaction. We first describe the procedure as executed by C. Phase la. C establishes 112 commit backup processes B1, Ba, . . . , B,. When it creates process I$, C informs it of the identity of all lower numbered backup processes and of cohort processes of the transaction.</p><p>Phase lb. C waits for an initial message from each of the backup processes to confirm its existence. Crashsite is invoked against any backup site that fails to respond within a time-out interval. Phase 2a. C issues UPDATE messages to U1 through U,. These will cause the local databases to be updated in tentative mode. Phase 2b. C waits for Guaranteed Delivery acknowledgment of these DATE messages. Phase 3a. C issues COMMIT messages to all B1 through B,.</p><p>Phase 3b. C waits for each backup to acknowledge receipt of the COMMIT message. Crashsite is invoked against any backup that does not acknowledge within the time-out period. Phase 4a. C issues COMMIT messages to U1 through U,. These cause the transaction to be locally committed. Phase 4b. C waits for Guaranteed Delivery acknowledgment of these COM-MIT messages and then destroys the backup processes. This represents the successful completion of the transaction.</p><p>Each backup is always in one of two states: the abort state or the commit state. Its state is determined by the following transition rules: When created, a backup process enters the abort state; receipt of a COMMIT message causes it to enter the commit state; receipt of an ABORT message (discussed below) causes it to return to the abort state. The backup process state corresponds exactly to the desired global effect to be achieved. Thus if a backup process assumes control four-phase procedure. If this condition does not hold, a commit catastrophe occurs. This catastrophe is not automatically detected by the RelNet. The effect of the catastrophe, however, is simply that some or all of the updates will not be installed. This may force other transactions to wait indefinitely for the completion of the suspended transaction, but it will not produce an inconsistent database. It is left as the responsibility of a system administrator to detect the commit catastrophe and manually issue COMMIT or ABORT messages as appropriate. The likelihood of a commit catastrophe can be lessened by using additional or more stable backup commit sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we have set forth the basic functionality and architecture of the RelNet, and described implementation mechanisms for its most important facilities. A number of the individual algorithms (such as those for guaranteed delivery and transaction control) should be transportable to other contexts. The Global Time Layer, on the other hand, describes a facility, based on a global clock, which may be specific to the needs of SDD-1. However, we believe that this notion of a uniform clock as a means for coordinating a distributed activity possesses numerous attractive features and may serve as the basis for further developments in distributed system design.</p><p>An initial implementation of the RelNet, developed at Computer Corporation of America, became operational in the fall of 1979. Needless to say, we could not specify the complete details of such an implementation effort in this paper. Numerous challenging problems are being confronted and solved in that context. We have sought to emphasize the basic principles of the RelNet on which the implementation is based.</p><p>Distributed system reliability remains very much an active problem. We have only proposed a first cut at a solution to it. Many of our algorithms need improvement or enhancement; other techniques may be necessary for different environments that dictate different trade-offs between reliability and efficiency. However, we believe that our results will form a foundation on which others will be able to build.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This research was supported by the Advanced Research Projects Agency of the Department of Defense under Contract N60039-77-C-0074, ARPA Order No. 3175-6. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. Authors' present addresses: M. Hammer, Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square, Cambridge, MA 02139; D. Shipman, Research Laboratory for Electronics, Massachusetts Institute of Technology, Cambridge, MA 02139. 0 1980 ACM 0362~5915/80/1200-0431$00.75</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>lM.</head><label></label><figDesc>Hammer and D.Shipman    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1) The local clock is incremented by Tdelta. Since the local site (the one performing Crashsite) is a guardian of the timed-out site, the local clock cannot be more than Tdelta behind the ward's clock at the time that the latter crashed. Consequently, this incrementation has the effect of pushing the local clock value past the last timestamp issued by the timed-out site.(2) A YOU'RE-DOWN message is sent to the timed-out site, and it is marked as DOWN in the Local Status Table (by means of the Markdown primitive). If the timed-out site is actually UP, this action will have the effect of crashing the timed-out site while its local clock is still less than the value just assigned to the guardian's local clock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Sender in nonspooling mode:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(1) SENDER (a) Sender in nonspooling mode: To send a message, send message directly to receiver and await acknowledgment. If the acknowledgment times-out, invoke Crashsite against the receiver, send START-SPOOLING messages to all the spoolers for that receiver, enter spooling mode, and send the message as specified below. (b) Sender in spooling mode: Upon entering spooling mode, establish recovery Watches against those spoolers that are DOWN. To send a message, append to message the current value of PMT [receiver] and then set PMT [receiver] to the current global clock time. Send message to all spoolers that are currently UP, and await acknowledgments. If instead of acknowledgment a STOP-SPOOLING message is received from some spooler, cancel the recovery Watches against DOWN spoolers, enter nonspooling mode, and send the message as specified there. If an acknowledgment times-out, invoke Crashsite against that spooler and establish a recovery Watch against it. If a crashed spooler recovers, send START-SPOOLING message to the spooler. (c) Upon recovery of sender: Enter nonspooling mode. (2) RECEIVER (a) Receiver in nonspooling mode: Await messages sent directly from senders; upon receipt, update acknowledgmentvector and acknowledge the message. (b) Receiver in spooling mode:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(3) SPOOLER (a) Spooler in nonspoolmg mode: Upon receipt of message from a sender, if it is a START-SPOOLING message, acknowledge and enter spooling mode; otherwise, respond with a STOP-SPOOL-ING message. Upon receipt of NEXT-MESSAGE-PLEASE message or ACKNOWLEDG-MENT-VECTOR message from the receiver; send STOP-SPOOLING message to the receiver. (b) Spooler in spooling mode:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I .</head><label>I</label><figDesc>Relnet Architecture</figDesc><table><row><cell></cell><cell>l</cell><cell>433</cell></row><row><cell>Layer</cell><cell>Functionality</cell></row><row><cell>Global Time</cell><cell></cell></row><row><cell>Guaranteed Delivery</cell><cell></cell></row><row><cell>Transaction Control</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II .</head><label>II</label><figDesc>Structure of Global Time Laver</figDesc><table><row><cell>Layer</cell><cell>Functionality</cell></row><row><cell>Local Clock</cell><cell>Maintains logical local clock</cell></row><row><cell>Local Status</cell><cell>Manages site status information</cell></row><row><cell>Global Clock</cell><cell>Simulates global clock by maintaining consis-</cell></row><row><cell></cell><cell>tency among local clocks</cell></row><row><cell>Global Status</cell><cell>Provides "clean" site failures and recoveries</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ACM Transactions on Database Systems, Vol. 5, No. 4, December 1980, Pages 431-466.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>ACM Transactions on Database Systems, Vol. 5, No. 4, December 1980.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>' It should be noted that the basic mechanism presented above is vulnerable if some site has a "runaway" logical or real-time clock. Additional mechanism can be used to resolve this problem. ACM Transactions on Database Systems, Vol. 5, No. 4, December 1980.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>' An additional mechanism is required to address a situation in which a ward mistakenly believes that a guardian site has failed. This mechanism centers around special handling of YOU'RE-DOWN messages.ACM Transactions on Database Systems, Vol. 5, No. 4, December 1980.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>This message need not yet have been acknowledged. The intention is for the sender to be able to inform a recovering spooler of the last message that it may have missed. ACM Transactions on Database Systems, Vol. 5, No.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>4, December 1980.   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>ACM Transactions on Database Systems, Vol. 5, No. 4, December 1980</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_7"><p>Assuming that the site has not failed permanently, in which case the reading transaction would have to wait forever!</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_8"><p>Except, of course, in the case where the transaction seeks to read data that are stored at the failed site and nowhere else in the network. ACM Transactions on Database Systems, Vol. 5, No. 4, December 1980.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_9"><p>(June 1978), 167-196. Received August 1979; accepted May 1980 ACM Transactions on Database Systems, Vol. 5, No. 4, December 1980.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to express their gratitude to many employees of Computer Corporation of America for the contributions that they have made to the research described here. In particular, Jim Rothnie provided the leadership for the SDD-1 project as a whole and served as an effective critic of our work; Sunil Sarin and Terry Landers proved invaluable in locating often subtle flaws in earlier versions of some of the algorithms; the technique for commit processing discussed in Section 5 is based on a suggestion of Nat Goodman. We would also like to recognize the assistance of Phil Bernstein, Steve Fox, and Chris Reeve.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To send a message i, send the message to all spoolers,. and await acknowledgments. If instead of an ac nowledgment, a STOP-SPOOLING message is received from some spooler, enter nonspooling mode and send the message as specified there. (c) Upon recovery of sender:</p><p>Enter nonspooling mode. Upon entering spooling mode, a particular UP spooler is chosen. An ACKNOWL-EDGMENT-VECTOR message containing the current acknowledgment-vector is sent to the spooler. (A response is expected, if none is received, within a time-out period, Crashsite is invoked against the spooler and another is selected.) Successive NEXT-MESSAGE-PLEASE messages are then sent to retrieve messages from the spooler. If the spooler does not respond to a NEXT-MESSAGE-PLEASE message within a time-out period, Crashsite is invoked against it and another spooler is selected for unspooling. If the spooler replies with a regular message, update the acknowledgment-vector, establish the message on the input queue on stable storage, and acknowledge it. If the spooler replies with GAP message or a STOP-SPOOLING message, initiate unspooling from another spooler. The unspooling procedure terminates when a STOP-SPOOLING message has been received from all UP spoolers, at which point nonspooling mode is entered. (c) Upon recovery of receiver:</p><p>Reconstruct the acknowledgment-vector if necessary; entering spooling mode. Upon receipt of message from a sender: add message to buffer and acknowledge. Upon receipt of NEXT-MESSAGE-PLEASE message from the receiver: send next message in buffer to the receiver, and after it has been acknowledged, delete the message from the queue. When the last message has been deleted from the queue, enter nonspooling mode. (c) Upon recovery of spooler:</p><p>We have assumed spoolers do not crash in this version of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Spooler Crashes</head><p>We will examine the problem of spooler crashing in two contexts. The first case is that of spoolers crashing while they are being emptied by a recovering receiver.</p><p>The second is that of spoolers crashing while the receiver is st,ill DOWN. (If a spooler crashes at any other time, it can be handled in the same way as in this second case.) After considering these issues we present the complete algorithm including provision for these spooler crashes. If a spooler crashes while it is being emptied, the recovering receiver should switch to a new spooler. However, there is a complication here: Many of the messages in the new spooler may have already been received from the former spooler. The new spooler ought not to have to send these messages. To this end, an acknowledgment-vector is maintained by the receiver. This is an array ACM Transactions on Database Systems, Vol. 5, No. 4, December 1980. l M. Hammer and D. Shipman reach its destination. This acknowledgment is sufficient to ensure that the UPDATE message will reach its destination and be executed there (in tentative mode).</p><p>This procedure is, therefore, insensitive to crashes of U1 through U, before or during its execution. It is, however, sensitive to failures of the commit process C. In particular, if C crashes during Phase 1, some or all of the updating processes will have received UPDATE messages in tentative mode for which no COMMIT message will be forthcoming. In such a situation we would like to abort the transaction and discard any UPDATE messages that have been received. More importantly, should C crash during Phase 2, then all processes (including those that have received UPDATE messages but no corresponding COMMIT messages) should commit the transaction, to ensure uniform installation of the updates.</p><p>Our solution to this involves the use of a number of commit backup processes; these are processes that can assume responsibility for completing C's activity in the event of its failure. These backup processes are created by C before it issues any UPDATE messages; each backup, when it is created, is informed of the identity of C's cohort process. Then, if C should fail before completion of the transaction, one of the backups will take control. If C failed before issuing all the UPDATE messages, then the backup will abort the transaction; otherwise it will commit it. In either case the desired effect is realized by sending COMMIT or ABORT messages to the cohorts, as appropriate. Naturally, proper coordination among the commit backup processes is crucial; a key issue is the selection of a single commit backup process to take over in the event of C's failure. Our technique for backup selection is described in the next section. Following this discussion, we describe the atomic commit procedure incorporating backups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Backup Selection Algorithm</head><p>In the event of the failure of the committing process, C, we wish one, and only one, commit backup process to assume control of transaction completion. The algorithm for accomplishing this assumes an ordering of the commit backup processes, designated in order as B1, BQ, . . . , B,. Process Bkml is referred to as the predecessor of process Bk (for k &gt; O), and process C is taken as the predecessor of process B1. Initially, each of the backup processes is watching for the failure of its predecessor; that is, Bi+l is assumed to have issued a Watch on Bi. The following conventions are then followed.</p><p>(1) If a watched backup process is found to be down, then its watching process begins to watch the predecessor of the failed process instead. (2) If process C is found to be down, the backup process that is watching it assumes control of the transaction. (3) If a backup process recovers, it ceases to be a part of the backup mechanism (i.e., it behaves as if it had stayed down).</p><p>These rules have the following consequences.</p><p>(1) If C fails, at most one backup process will assume control, This will be the lowest numbered backup that has not failed during the procedure. (Note: If all backups have failed at least once since their creation at the time of C's when it is in the abort state, it should send ABORT messages to all of II1 through U,; if it is the commit state, it should send COMMIT messages instead. The operation of a backup process can then be described as follows.</p><p>(1) When created, a backup sends a message to C, confirming its creation. It also issues a failure Watch against its predecessor. The foregoing works correctly so long as the backup that assumes control does not crash in the midst of sending the COMMIT or ABORT messages to U1, . . . , U,. If it were to so crash, there is a danger that the next backup to assume control would be in a different state than the failing backup. In such a case some update process might receive both a COMMIT and an ABORT message. To avoid this problem, each backup process, when created, will be informed of the identity of its higher numbered backups, which represent the sites that might assume control from it. Then before performing any backup activity, a backup process will ensure that these higher numbered backups are in the same state that it is in.</p><p>Specifically, upon assuming control, a backup process B performs the following two-phase procedure (replacing (3a) above).</p><p>Phase Bla. B issues COMMIT or ABORT messages (depending on its current state) to all higher numbered backup processes. Phase Blb. B waits for each of these backups to acknowledge receipt of the message. Crashsite is called against any backup that does not acknowledge within the time-out period. (This is to ensure that any backup that is UP has received the message.) Phase B2a. B issues COMMIT or ABORT messages (depending on its current state) to all of U1 through U,.</p><p>Phase B2b. B waits for Guaranteed Delivery acknowledgment of these messages, and then destroys all the backup processes.</p><p>By following this procedure, B ensures that before it ever sends out any messages to the updating processes, all remaining backup processes are in the same state that it is in. Therefore, it will not be possible for an updating site to receive both a COMMIT and an ABORT message issued by different backups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Catastrophe in Commit</head><p>The algorithm presented in the preceding section will succeed so long as at least one member of the set of the sites {C, B1, . . . , B,J remains UP throughout the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A principle for resilient sharing of distributed resources</title>
		<author>
			<persName><forename type="first">I</forename><surname>Alsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Znt. Confi Software Engineering</title>
		<meeting>2nd Znt. Confi Software Engineering</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approaches to concurrency control in distributed database systems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AFZPS 1979 NCC</title>
		<meeting>AFZPS 1979 NCC<address><addrLine>Arlington, Va</addrLine></address></meeting>
		<imprint>
			<publisher>AFIPS Press</publisher>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="813" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Concurrency control in a system for distributed databases (SDD-1)</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Rothnie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="51" />
			<date type="published" when="1980-03">March 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The correctness of concurrency control in a system for distributed databases (SDD-1)</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Shipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="1980-03">March 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The notions of consistency and predicate locks in a database system</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Eswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lorie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Traiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="624" to="633" />
			<date type="published" when="1976-11">Nov. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Query processing in SDD-1: A system for distributed databases</title>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Reeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Rothnie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Notes on data base operating systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Operating Systems: An Advanced Course</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1978">1978</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="393" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Time, clocks, and the ordering of events in distributed system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lamport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="558" to="565" />
			<date type="published" when="1978-07">July 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crash recovery in a distributed data storage system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sturgis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint/>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Physical integrity in a large segmented database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lorie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="104" />
			<date type="published" when="1977-03">March 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust concurrency control for a distributed information system</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Montgomery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">M.I.T. Lab. for Computer Science</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<date type="published" when="1978-12">Dec. 1978</date>
			<pubPlace>Cambridge, Masss</pubPlace>
		</imprint>
	</monogr>
	<note>LCS Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Naming and synchronization in a decentralized computer system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LCS Tech. Rep. 205, M.I.T. Lab. for Computer Science</title>
		<meeting><address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1978-09">Sept. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Introduction to a system for distributed databases (SDD-1)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Rothnie</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="1980-03">March 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Failure recovery in a distributed database system</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Millstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1978 IEEE COMPCON Spring Conf</title>
		<meeting>1978 IEEE COMPCON Spring Conf</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Handling network partitions in distributed databases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="8" />
			<date type="published" when="1979-04">April 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reliability issues in distributed information processing systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Svobodova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th IEEE Fault Tolerant Computing Symp</title>
		<meeting>9th IEEE Fault Tolerant Computing Symp</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recovery techniques for database systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S M</forename><surname>Verhofstad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
