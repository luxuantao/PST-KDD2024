<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Reuse, Performance, and Hardware Cost of DNN Dataflows: A Data-Centric Approach Using MAESTRO</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-11">11 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
							<email>hyoukjun@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<address>
									<addrLine>MICRO-52, October 12-16</addrLine>
									<postCode>2019</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prasanth</forename><surname>Chatarasi</surname></persName>
							<email>cprasanth@gatech.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<address>
									<addrLine>MICRO-52, October 12-16</addrLine>
									<postCode>2019</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
							<email>mpellauer@nvidia.com</email>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA Westford</orgName>
								<address>
									<region>Massachusetts</region>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<address>
									<addrLine>MICRO-52, October 12-16</addrLine>
									<postCode>2019</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
							<email>aparashar@nvidia.com</email>
							<affiliation key="aff3">
								<orgName type="institution">NVIDIA Westford</orgName>
								<address>
									<region>Massachusetts</region>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<address>
									<addrLine>MICRO-52, October 12-16</addrLine>
									<postCode>2019</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
							<email>vsarkar@gatech.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
							<email>tushar@ece.gatech.edu</email>
							<affiliation key="aff5">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Reuse, Performance, and Hardware Cost of DNN Dataflows: A Data-Centric Approach Using MAESTRO</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-11">11 May 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3352460.3358252</idno>
					<idno type="arXiv">arXiv:1805.02566v6[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural networks</term>
					<term>Dataflow</term>
					<term>Cost modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The data partitioning and scheduling strategies used by DNN accelerators to leverage reuse and perform staging are known as dataflow, which directly impacts the performance and energy efficiency of DNN accelerators. An accelerator microarchitecture dictates the dataflow(s) that can be employed to execute layers in a DNN. Selecting a dataflow for a layer can have a large impact on utilization and energy efficiency, but there is a lack of understanding on the choices and consequences of dataflows, and of tools and methodologies to help architects explore the co-optimization design space.</p><p>In this work, we first introduce a set of data-centric directives to concisely specify the DNN dataflow space in a compiler-friendly form. We then show how these directives can be analyzed to infer various forms of reuse and to exploit them using hardware capabilities. We codify this analysis into an analytical cost model, MAESTRO (Modeling Accelerator Efficiency via Spatio-Temporal Reuse and Occupancy), that estimates various cost-benefit tradeoffs of a dataflow including execution time and energy efficiency for a DNN model and hardware configuration. We demonstrate the use of MAESTRO to drive a hardware design space exploration experiment, which searches across 480M designs to identify 2.5M valid designs at an average rate of 0.17M designs per second, including Pareto-optimal throughput-and energy-optimized design points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computer systems organization → Neural networks; • Hardware → Modeling and parameter extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks (DNNs) are being deployed at an increasing scale-across the cloud and IoT platforms-to solve complex regression and classification problems in image recognition <ref type="bibr" target="#b39">[41]</ref>, speech recognition <ref type="bibr" target="#b3">[5]</ref>, language translation <ref type="bibr" target="#b44">[46]</ref>, and many more fields, with accuracy close to and even surpassing that of humans <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b42">44]</ref>. Tight latency, throughput, and energy constraints when running DNNs have led to a meteoric increase in hardware accelerators.</p><p>DNN accelerators achieve high performance by exploiting parallelism over hundreds of processing elements (PEs) and high energy efficiency by maximizing data reuse within PEs and on-chip scratchpads <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b36">38]</ref>. For a specific DNN workload and a hardware accelerator, the achieved utilization and data-reuse directly depends on (1) how we schedule the DNN computations (e.g., choice of loop transformations) and (2) how we map computations across PEs. These two components are collectively referred to as dataflow in the accelerator literature <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b29">31]</ref>. It has been shown that the energy cost of moving data exceeds the cost of computation <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b15">17]</ref>, and so understanding and optimizing dataflow is a critical component of DNN accelerator design, as it directly determines how data is transferred between multipliers (L0), staged in local buffers (L1), and in the global buffer hierarchy (L2 and beyond).</p><p>The performance and energy efficiency of DNN accelerators depend on (1) target DNN model and its layers types/dimensions, (2) dataflow, and (3) available hardware resources and their connectivity. These three dimensions are tightly coupled, and optimizing DNN accelerators across these dimensions is a challenging task. For example, a dataflow that exploits input channel parallelism <ref type="bibr" target="#b0">[1]</ref> in convolutional neural networks (CNNs) may not achieve high utilization on layers with a small number of channels. Alternatively, dataflows that require more transfer bandwidth than the network-onchip (NoC) provides may result in under-utilization of the hardware. In such cases, increasing the L1 scratchpad size may allow the same dataflow to require less data bandwidth, but this larger L1 may increase area and energy consumption. Thus, co-optimizing the hardware microarchitecture and the dataflows it supports is one of the primary optimization targets for any accelerator design. This remains an open challenge, as observed by the number of novel dataflows and microarchitectures that continue to be proposed recently <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b25">27]</ref>.</p><p>Regrettably, these proposals do not cover the complete space of dataflows at an exhaustive-enough level to serve as a reference for architects designing custom accelerators within a variety of constraints. In contrast, recent proposals on compilation <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b31">33]</ref> and analysis tools <ref type="bibr" target="#b28">[30]</ref> for DNNs analyze a broad space of software mappings of a DNN workload onto a given architecture, but the relationship between software mappings and hardware dataflows is not elucidated, and these black-box tools do not provide architects with intellectual intuitions on the consequences of dataflow selection and their impact on reuse. In fact, the very term "dataflow" is used in an inconsistent manner across both architecture and analysis proposals. Architects are thus left with an incomplete and unstructured set of intuitions on dataflows and the complex interplay between dataflow and microarchitecture choices.</p><p>In this paper, we seek to remedy this situation by providing a thorough set of insights on the choices and consequences of dataflow selection and their interplay with microarchitectural alternatives, and a structured mechanism to reason about them quantitatively. To that end, we make the following specific contributions.</p><p>First, we introduce a data-centric notation to represent various accelerator dataflows with data mappings and reuses being first-class entities, unlike the compute-centric notation used by prior proposals which infer the data reuses from a loop-nest representation <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b28">30]</ref>. These data-centric directives can express a wide range of data-reuses (across space, time, and space-time) over arbitrary hierarchies of PEs for both dense and sparse DNN layers such as convolutions, LSTMs, and fully-connected layers. We believe that our data-centric notation can complement the commonly used loopnest notation, i.e., our notation can be viewed as an intermediate representation (IR) which can be extracted from a high-level loopnest notation or specified directly.</p><p>Second, we show how these data-centric directives can be used to reason about reuse in a structured manner. We demonstrate the relationship between each directive, the specific form of algorithmic reuse exposed by the directive, and the potential ways to exploit that reuse using a hardware capability to improve efficiency. This analysis covers the complete space of ways in which any dataflow can exploit reuse.</p><p>Third, we introduce an analytical cost model named MAESTRO (Modeling Accelerator Efficiency via Spatio-Temporal Reuse and Occupancy) that programmatically implements the above analysis. MAESTRO takes as input 1) a DNN model with a set of layers, 2) a dataflow description for each layer specified using our proposed directives, and 3) the hardware configuration. Based on these inputs, MAESTRO outputs estimates of end-to-end execution time, energy (including all compute, buffer, and interconnect activities), NoC costs, and so on. A key challenge in our proposed approach is to provide a cost estimation that is both efficient and sufficiently precise to effectively support design space exploration. We demonstrate MAESTRO's abstract hardware model and analytic model to be   within 90-95% accuracy of actual open-source RTL <ref type="bibr" target="#b22">[24]</ref> while being 1029-4116× faster (10ms to run MAESTRO versus 7.2-28.8 hours for an equivalent RTL simulation on a workstation with Xeon E5-2699 processor and 64GB memory). Finally, we demonstrate how the MAESTRO cost model can be used by accelerator designers to determine Pareto-optimal parameters for an accelerator with a given area, energy, or throughput budget. For a NVDLA <ref type="bibr" target="#b0">[1]</ref>-like dataflow (KC-Partitioned in Table <ref type="table">3</ref>) in VGG16 <ref type="bibr" target="#b40">[42]</ref> CONV layer 11, we see up to a 2.16× difference in power consumption between energy-versus throughput-optimized design points. The energy-optimized design employs 10.6× more SRAM and 80% the PEs of the throughput-optimized design. This leads to an energy-delay product improvement of 65%, with 62% throughput. The range of these numbers is a concrete example of the significance of this problem for accelerator architects.</p><formula xml:id="formula_0">O[k][y-r][x-s] += W[k][c][r][s] * I[c][y][x];</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>To understand the cost-benefit tradeoffs of various approaches to compute convolutions, we discuss core concepts related to data reuse and dataflows in the context of DNN accelerators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tensors in DNNs</head><p>We present an example of a multi-channel 2D convolution in Figure <ref type="figure" target="#fig_1">1</ref> that involves seven data dimensions across three data structures: input/output activation and weight tensors. Although our approach can be applied to various DNN layers-CONV2D, fully-connected (FC), LSTM, separable convolution, and so on-we focus on CONV2D and its variants in this paper because convolutional neural networks (CNNs) are popular, and CONV2D accounts for more than 90% of overall computation in CNNs <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b12">14]</ref>.</p><p>Tensors in DNNs are addressed using seven dimensions in a complex manner. For example, the row/column indices of output can be deduced using input row/column and filter row/column indices (i.e., an input-centric view of the convolution loop nest). Also, the input channel index c appears in both filter and input activation, and the output channel k appears in both filter and output activation. We call these dimensions coupled to these indices, as the position in the data space changes when the index is modified. Because of these  <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b36">38]</ref>. The illustrated base architecture can be hierarchically organized.</p><p>specific data access patterns, we can transform the loop nest to keep one of the data structures stationary over a range of space or time (i.e., unchanged in a local buffer), which can significantly reduce global/local buffer access counts in DNN accelerators, as well as energy consumption by keeping local wires unchanging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DNN Accelerators</head><p>DNN accelerators are specialized architectures to run DNN applications with high throughput and energy efficiency. As described in Figure <ref type="figure" target="#fig_2">2</ref>, most DNN accelerators employ hundreds of processing elements (PEs) to exploit inherent parallelism in DNN applications. PEs typically include scratchpad memories (L1) and ALUs that perform multiply-accumulate operations (MACs). To reduce energyand time-consuming DRAM accesses, most DNN accelerators also include a shared scratchpad buffer (L2) large enough to stage data to feed all the PEs. Shared L2 buffer and PEs are interconnected with a network-on-chip (NoC). Our approach supports a wide range of interconnect designs in the NoC module. For example, a systolic array could be represented as a 2D array that provides unidirectional links toward East and South. Depending on the hardware parameters selected, our approach can support architecture designs that can efficiently execute a wide range of DNN operations, including convolutions, because it enables exploiting not only parallelism but also data reuse via buffers and forwarding/multicasting NoCs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Reuse Taxonomy</head><p>We observe that data reuse originates from two behaviors of DNN accelerators over time and space -multicasting (input tensors) and reduction (output tensors). Multicasting. Spatial multicasting reads a data point from a buffer only once, spatially replicates the data point via wires, and delivers the data point to multiple spatial destinations (i.e., PEs), which reduces expensive remote buffer accesses and saves energy. Likewise, temporal multicasting also reads a data point from a large remote buffer only once, temporally replicates the data point via a smaller local buffer, and delivers the data point to multiple temporal destinations (i.e., different time instances) at the same PE, which also reduces expensive remote buffer accesses and saves energy. Reduction. Spatial reduction accumulates partial outputs from multiple spatial sources and spatially accumulates them via multiple compute units (e.g., an adder tree or reduce-and-forward). Similarly, temporal reduction accumulates partial outputs from multiple temporal sources (i.e., partial sums computed at different time) and temporally accumulates them via an accumulation register or buffer (e.g., accumulation buffer in TPU <ref type="bibr" target="#b17">[19]</ref>).</p><formula xml:id="formula_1">W0 W1 W2 W3 I0 I1 I2 I3 I5 I6 I7 I8 I10 I11 I12 I13 I4 I9 I14 O0 O1 O2 O3 O5 O6 O7 O8 O0 P0_1 P0_0 P0_3 P0_2 = + + + W0 x I0 W1 x I1 W2 x I5 W3 x I6 Weight Input Output Spatial Dimension (PE) PE0 PE1 PE2 PE3 W0 I0 P0_0 W0 I1 P1_0 W1 I1 P0_1 W1 I2 P1_1 P0_0 P1_0 P0_1 P1_1 W0 I2 P2_0 W0 I3 P3_0 W1 I3 P2_1 W1 I4 P3_1 P2_0 P3_0 … 0 1 2 3 P3_1 P2_1<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dataflow Definition and Example</head><p>In order to leverage these opportunities, the accelerator must schedule operations such that the PEs proceed through the data tensors in a coordinated fashion, which can be viewed as transformations (e.g., ordering and tiling) applied to the convolution in Figure <ref type="figure" target="#fig_1">1</ref>, along with a partitioning of data to PEs. Such schedules are termed as dataflows in prior work <ref type="bibr" target="#b9">[11]</ref>, which categorizes dataflows into classes based on the tensor which is scheduled to change least frequently, e.g., weight-stationary, output-stationary, and input-stationary. Figure <ref type="figure" target="#fig_3">3</ref> shows an example weight-stationary dataflow run on four PEs. We can observe that W 1 is multicast across time (temporal multicasting), I 1 is multicast across PEs (spatial multicasting), and P 3_1 is reduced across space and time. That is, the example accelerator temporally reuses W 1 and spatially reuses I 1 and P 3_1 . Note that the name "weight-stationary" conveys intuition and a high-level characterization of scheduling strategy, but detailed insight and analysis requires more precise description.</p><p>Chen et al. <ref type="bibr" target="#b10">[12]</ref> refine the definition of dataflow by additionally specifying that two schedules which differ only in the concrete bounds should be considered instances or mappings of the same dataflow. This is an important distinction, as it allows families of accelerators to be categorized together even if they have different buffer sizes-i.e., a mobile chip and a datacenter chip may use the same traversal orderings despite large differences in tile size. For brevity, for the remainder of this work, we make no distinction between schedules with fully-specified or partially unspecified concrete bounds but refer to them all as dataflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Existing Expressions of Dataflow</head><p>To convey the scheduling decisions of a particular architecture, dataflows have been expressed as loop nests, a syntax that resembles a simple imperative programming language with explicit parallelism,  as presented in Eyeriss v2 <ref type="bibr" target="#b10">[12]</ref>. We term the loop nest notation a compute-centric representation since the data movement is implicit from the loop order and the explicit parallelism specified by the programmer. The loop order dictates the schedule (or, ordering) of computations, the explicit annotation of loops with parallel-for captures parallelism, and the combination of loop ordering, tiling, and parallelism enables data reuse. Therefore, architects started to explore optimized loop nests encompassing all of the three aspects; loop order, parallelism, and tiling. For example, Eyeriss v2 <ref type="bibr" target="#b10">[12]</ref> describes its dataflow in a 22-dimensional loop nest.</p><p>Compute-centric representation including the polyhedral model has been a huge help to compilers in estimating reuses in guiding optimal loop transformations for both parallelism and locality <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b43">45]</ref>. Those works provide sufficiently accurate cost estimations to drive a series loop transformation in a compiler. However, they do not precisely model data reuse, so therefore computing throughput and energy-efficiency with high accuracy is challenging for those works. Bao et al. <ref type="bibr" target="#b5">[7]</ref> developed an analytical model to accurately estimate cache behavior (thereby computing reuses) for a class of affine programs that can be precisely analyzed by a polyhedral model at compile time. However, they use heavyweight linear-algebra frameworks within the polyhedral model to compute reuse, thereby making it impractical to use these techniques on real large applications. Also, it is very challenging for the polyhedral-based frameworks to compute reuse arising from array subscripts involving non-affine expressions or complex subscripts, such as modulus operations which are common in strided convolutions.</p><p>In addition, although there exists a body of past compiler work that performs reuse analysis on sequential programs <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b43">45]</ref>, they lack the ability to analyze loop nests with explicit parallelism, while DNN dataflows often contain multiple levels of parallelism. Also, those past works did not consider spatial reuse (which does not refer to the spatial locality in cache-based architectures but data reuse via wires or across PEs) that leverages multicasting and reduction support of accelerators, which plays a key role in estimating the overall throughput and energy efficiency of spatial DNN accelerators.</p><p>Such limitations and challenges motivate us to explore an alternative intermediate representation (IR) of dataflows, a data-centric representation where data movement and organization are first-class entities. Since data movement is explicit in the data-centric representation, our analytical model becomes simpler and relatively faster as there is no need to leverage heavyweight linear-algebra frameworks to precisely estimate data movement/reuse behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESCRIBING DATAFLOWS</head><p>Our data-centric representation consists of four key directives -1) spatial map, 2) temporal map, 3) data movement order, and 4) clusters. We briefly explain all the key directives using 1D convolution (shown in Figure <ref type="figure" target="#fig_4">4</ref> (a)) as a pedagogical example, and then discuss various hardware implementation choices for supporting a wide range of data-reuse across space, time, and space-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data-Centric Representation</head><p>We define the dataflow of an accelerator design to consist of two major aspects -(1) the schedule of DNN computations (e.g., choice of loop transformations) across time for exploiting a wide range of reuse, and (2) the mapping of the DNN computations across PEs for parallelism. The representation is based on four key components, and we briefly discuss the first three components below. The fourth component, Cluster, will be introduced in Section 3.2. shows mapping described using our data-centric directives. The second row shows iteration spaces whose points correspond to each partial sum. In row three to five, we show data mapping of each data structure. Finally, we describe temporal and spatial reuse opportunities from each mapping.</p><p>describes the shift in the starting indices of α across consecutive time steps in a PE. (3) Data Movement Order: The sequence of spatial and temporal maps in the dataflow specification dictate the order of data movement, i.e., the change of the data mappings to PEs across time.</p><p>We demonstrate reuse opportunities presented by various dataflows using the 1D convolution example in Figure <ref type="figure" target="#fig_4">4(a)</ref>. We start by creating a unique dataflow for this program by the loop nest representation in Figure <ref type="figure" target="#fig_4">4</ref>(b), assuming the accelerator has 2-level hierarchy (L0 register at PE + L1 local scratchpad buffer). The two loops enclosed in the red box are indicative of the mapping over the PEs, and their corresponding data-centric representation is in Figure <ref type="figure" target="#fig_4">4(c) and (d)</ref>.</p><p>As can be seen from Figure <ref type="figure" target="#fig_4">4</ref>(e), the data elements corresponding to outputs (dimension X') is spatially distributed across three PEs, i.e., each PE receives different chunks of two output elements. This particular data distribution can be captured with our spatial map directive with size and offset parameters being 2, resulting in SpatialMap(2,2) X' where X' is the first dimension of output data structure. Also, the data elements corresponding to weights (dimension S) is replicated across multiple PEs, i.e., each PE receives a same chunk of three weight elements in the first iteration, and receives different chunk of weight elements in the next iterations. This particular replicated and temporal distribution can be captured with our temporal map directive with size and offset parameter being 3, resulting in TemporalMap(3,3) S, where S is the first dimension of the weight data structure. Putting it together, spatial map on X' followed by a temporal map on S captures data mapping and movement behavior across PEs and time corresponding to the two loops in the loop-nest version, and these two directives are enclosed in the red box in Figure <ref type="figure" target="#fig_4">4(c)</ref>. Each data-centric representation is a complete description of a unique dataflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataflow Playground</head><p>We build six example dataflows upon the simple 1D convolution discussed in Figure <ref type="figure" target="#fig_4">4</ref> (d) to demonstrate how small changes to a dataflow expose various forms of reuse-both spatial and temporal. Figure <ref type="figure">5</ref> illustrates those six example dataflows, which consists of a base dataflow Figure <ref type="figure">5</ref>(A) and its variants. We modify the directive order, spatially/temporally mapped dimensions, mapping size, and PE clustering and discuss their impact on data reuse. Directive Order. A change in directive order can result in an entirely different temporal reuse (or, stationary behavior). For example, the sequence of directives in mapping in Figure <ref type="figure">5(A)</ref> indicates that all data indices of S should be explored before working on the next chunk of X' indices. This order results in temporally reusing values of data corresponding to X' indices (i.e., partial sums) for all indices of S. Therefore, this dataflow is informally referred to as output-stationary and partitioned across multiple outputs in parallel. Figure <ref type="figure">5</ref>(B) shows the impact of interchanging the order of directives. This results in a weight-stationary dataflow, because PEs can temporally reuse weight values corresponding to S indices, for all indices of X' before going to next chunk of S indices. Similarly, Figure <ref type="figure">5</ref>(C) and (D) shows the spatial distribution on S instead of X', and also the impact of data movement order on temporal reuse leading to different dataflow variations. This indicates why the informal dataflow name should not be taken as a complete and precise specification of its behavior. Spatially and Temporally Mapped Dimensions. In Figure <ref type="figure">5</ref>(A) the directive SpatialMap(1,1) X' (where X' refers to the first dimension of the output data structure), spatially distributes indices of the X' dimension with a chunk size of one (the size parameter) across PEs with an offset of one (the offset parameter). This means that each PE works on a different column of the output data space. If the number of PEs is not sufficient to cover all indices of the dimension mapped, then the mapping is folded over time across the same set of PEs. Also, if offset value is smaller than size value, then there will be an overlap of indices across consecutive PEs, and this is useful in describing mappings on input activation dimensions X and Y because their iteration space is skewed.</p><p>Similarly, TemporalMap(1,1) S (where S refers to the first dimension of filter weight data structure), distributes indices of the S dimension with a chunk size of one across time steps with an offset of one. This means that each PE works on the same column of the weight data space. Since all PEs get the same data indices corresponding to a temporally mapped dimension, this creates an opportunity for spatial reuse, i.e., multicasting the same data values across PEs in a time step. Mapping Size. In all of the mappings from Figure <ref type="figure">5A</ref>-D, the mapping sizes (first argument) of weights and outputs are one -resulting in full temporal reuse of weights but no temporal reuse of outputs (e.g., mapping B and D) or vice versa (e.g., mapping A and C). There is no temporal reuse of inputs in any mapping. Increasing the map size of the spatial or temporal maps can help in presenting opportunities for partial temporal reuse, which can capture convolutional reuse of inputs in CNN layers. For example, the spatial map corresponding to the S dimension in Figure <ref type="figure">5</ref>(E) helps in exploiting the partial temporal reuse of input data across time steps. PE Clustering for Multi-dimensional Spatial Distributions. As can be seen in Figure <ref type="figure">5(A-E</ref>), data mappings related to a map in the outer position get updated after a full exploration of a map in the inner position. This inherent assumption can limit certain dataflow behaviors where one might be interested in simultaneously exploiting spatial distribution of more than one data dimensions.</p><p>To address this, we introduce another directive called Cluster as a mean to support the simultaneous spatial distribution of multiple data dimensions. The cluster directive logically groups multiple PEs or nested sub-clusters (when a dataflow has multiple cluster directives) of size parameter. For example, CLUSTER (3) in Figure <ref type="figure">5(F)</ref> arranges available PEs into groups of three, resulting in two clusters of three PEs.</p><p>All the mapping directives specified above a CLUSTER directive perform the mapping across logical clusters created by the CLUSTER directive. All the mapping directives specified below a CLUSTER directive perform the mapping across PEs or lower level logical clusters inside a logical cluster created by the CLUSTER directive. That is, all the mapping directives above a CLUSTER directive see logical clusters while those below the CLUSTER directive see inside of each logical cluster. With this mechanism, one can specify complex dataflows with multiple parallelization dimensions represented by multiple SPATIALMAP directives (one in each cluster level). An example of this can be seen in Figure <ref type="figure">5</ref>(F), where the X' dimension is spatially distributed across clusters, and the S dimension is spatially distributed within the cluster. The cluster directives enable us to represent existing real-world accelerator dataflows, such as Eyeriss <ref type="bibr" target="#b9">[11]</ref> since it involves the spatial distribution of R and Y dimensions simultaneously, and also NVDLA <ref type="bibr" target="#b0">[1]</ref> which involves the spatial distribution of K and C dimensions. Another advantage of the cluster directive is that its notion of grouping multiple PEs can represent coarse-grained PEs in accelerators, such as SIMD units <ref type="bibr" target="#b41">[43]</ref> and matrix tensor accelerators like GPU Tensor Cores.</p><p>In summary, we discussed five transformations that capture all possible aspects of dataflows: scheduling, tiling, and mapping. As shown in Figure <ref type="figure">5</ref> the data-centric directives can concisely represent all of those aspects. We envision that the data-centric representation could be either auto-generated from a loop nest version of the dataflow (with affine constraints), or manually written.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hardware Implications of Reuse</head><p>As we discussed above, various data reuse opportunities appear based on the dataflow. Table <ref type="table">1</ref> summarizes how such opportunities appear in the relationship of spatially mapped dimension within a cluster (Map column) and inner-most temporally mapped dimension (InnerMap column). For example, if output channels (K) are spatially mapped, a decoupled data structure, input feature map, does not change over space. That is, all the PEs receive the same input feature map, which implies a full spatial reuse opportunity (broadcast). In the same example, when the inner-most temporally mapped dimension is the input channels (C), the input channel changes every iteration, which provides temporal reduction opportunities of outputs.</p><p>Although a dataflow provides temporal or spatial data reuse opportunities, appropriate hardware support is required to actually exploit these phenomena. Table <ref type="table" target="#tab_2">2</ref> summarizes four reuse categories and corresponding hardware implementation to support them. As the table shows, reuse can be either spatial or temporal. Based on the data structure, the communication type can be either multicast (input tensors) or reduction (output tensors). Multicast is a communication type that delivers the same data to multiple targets over space (different PEs at the same time) or time (the same PE in different time). Therefore, multicast is one to many communication type, which requires either a fan-out network-on-chip structure such as bus or tree, or a "stationary" buffer to hold the data and deliver it to the future. In contrast, the reduction is many to one communication type, which applies to partial sums to generate final outputs. The reduction also can be either spatial or temporal. Example hardware to support Table <ref type="table">1</ref>: Reuse opportunities based on spatially-mapped dimensions in combination with innermost temporally-mapped dimensions. Filters (F), Inputs (I), and Outputs (O) are considered separately. For brevity, X/Y should be interpreted as X'/Y' as appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapped</head><p>Dim. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K</head><formula xml:id="formula_2">C R/S X/Y Coupling F I O Reuse Opportunity F I O ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Multicast Reduction Multicast Multicast Spatial Innermost Mapped Dim. Coupling F I O C R/S X/Y K R/S X/Y K C X/Y K C R/S ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Reuse Opportunity F I O Reduction Multicast Multicast Multicast Temporal Multicast Multicast Multicast Multicast Reduction Multicast Reduction Multicast</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Buf MAC</head><p>Multiple read-modify-write to a buffer spatial reduction is a reduction tree or reduce-and-forward chain such as systolic arrays. Temporal reduction can be supported by a read-modify-write buffer. In summary, different dataflows (expressed via our directives) expose different forms of reuse: spatial and temporal, both for multicasts and reductions, which in turn can have multiple hardware implementations. Reasoning about dataflows in this structured manner exposes new insights and potential microarchitectural solutions. The discussion so far focused on a simple 1D convolution, which itself exposed many possible dataflows and reuse opportunities. We extend this to a full convolution loop and analyze reuse opportunities within a specific dataflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Extended Example: Row-stationary Dataflow</head><p>Figure <ref type="figure">6</ref> presents detailed mapping and reuse patterns across two unit time steps of an example row-stationary dataflow <ref type="bibr" target="#b9">[11]</ref> over a six-PE accelerator. The accelerator has two PE clusters with three PEs in each cluster. We use the same example layer previously used in Figure <ref type="figure" target="#fig_1">1</ref>. Figure <ref type="figure">6</ref>(a) and (b) are compute-and data-centric representations of the row-stationary dataflow. Figure <ref type="figure">6(c)</ref> shows how the mapping moves across space (PE clusters) and time Figure <ref type="figure">6(d)</ref> shows the actual coordinates of each tensor across two time steps and two clusters (i.e., time and space). Each colored box in Figure <ref type="figure">6</ref>(d) represents replicated data points, which imply reuse opportunities. Based on the replicated data points, we can infer data reuse over the PE array, as shown in data reuse row in Figure <ref type="figure">6(d)</ref>. The mapping in Figure <ref type="figure">6</ref>(d) shows that the same set of input activation values are replicated across two clusters in a skewed manner within the same time step, which implies spatial reuse opportunities in the diagonal direction of the example PE array. Similarly, Figure <ref type="figure">6(d)</ref> shows that the same set of weight values are replicated over two time steps within the same PE, which implies temporal reuse opportunities and weight-stationary style dataflow in unit time step granularity. Note that the dataflow is still row-stationary in a coarse-grained time step although it is weight stationary in unit time steps we define in Figure <ref type="figure">6</ref> (a) and (b). Finally, Figure <ref type="figure">6</ref> (d) shows the same set of output activation over PEs in each PE cluster, which means that all the PEs in each cluster cooperate to generate a set of output activation data. That is, each PE in a PE cluster generates different partial sums for the same output activation, and they need to be accumulated across PEs in each PE cluster to generate final output activation values.</p><p>Based on the example analysis in Figure <ref type="figure">6</ref>, we observe that the data reuse pattern exactly matches the original work <ref type="bibr" target="#b9">[11]</ref>: reuse in the horizontal direction for filter weights and vertical for outputs (partial sum accumulation), and reuse in the diagonal direction for input activations.</p><p>In summary, reuse opportunities are based on the replicated data across time or space (PEs), which implies temporal and spatial reuse opportunities, respectively. The examples in this section demonstrate the need for a fast, accurate quantitative methodology to compute reuse for complex dataflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">QUANTITATIVE DATAFLOW ANALYSIS</head><p>In this section, we present our approach to quantitatively estimating runtime and energy efficiency of dataflows on a target DNN model and hardware configuration. Based on the approach, we implement an analysis framework, MAESTRO, which consists of five engines: tensor, cluster, reuse, performance analysis, and cost analysis. Figure <ref type="figure">7</ref> provides a high-level overview of the five engines. In the interest of space, we only discuss high-level algorithms without edge case handling, multiple layers, and multiple cluster levels. For details, we present them in our open-source repository [2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminary Engines</head><p>Tensor Analysis. As described in Figure <ref type="figure">7</ref>, the tensor analysis engine identifies dimension coupling for each tensor based on specified layer operations. For example, in depth-wise convolutions, output    </p><formula xml:id="formula_3">[n][k][c][y-r][x-s][r][s] = W[k][c][r][s] * I[n][c][y][x]; O[n][k][c][y-r][x-s] += P[n][k][c][y-r][x-s][r][s];</formula><formula xml:id="formula_4">PE N C Y X 0 0-2 0 0-2 0 0-2 1 0-2 0 0-2 2 0-2 0 0-2 1 0-2 0 0-2 2 0-2 0 0-2 3 0-2 N C Y X 0 0-2 0 1-3 0 0-2 1 1-3 0 0-2 2 1-3 0 0-2 1 1-3 0 0-2 2 1-3 0 0-2<label>3</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight[K][C][R][S]</head><formula xml:id="formula_5">K C R S 0-1 0-2 0 0-2 0-1 0-2 1 0-2 0-1 0-2 2 0-2 0-1 0-2 0 0-2 0-1 0-2 1 0-2 0-1 0-2 2 0-2 K C R S 0-1 0-2 0 0-2 0-1 0-2 1 0-2 0-1 0-2 2 0-2 0-1 0-2 0 0-2 0-1 0-2 1 0-2 0-1 0-2 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output[N][K][Y'][X']</head><p>N K Y' X' An extended example of a row-stationary style dataflow mapped on a six-PE accelerator. We select our own tile sizes for any not specified in the original work <ref type="bibr" target="#b9">[11]</ref>. We do not apply additional mapping optimizations to minimize PE under-utilization. Colors represent data replication either across time or space (PEs). Directives with asterisks indicate fully unrolled directives that cover entire data dimension with one mapping.  activation is not coupled with the output-channel dimension but coupled with the input channel dimension. Note that depth-wise convolution can be understood either in this manner or by eliminating input channel dimension (C). We select this convention because it aligns with MAESTRO's input-centric cost model. MAESTRO allows users to specify tensors with arbitrary dimension coupling, and such coupling relationship is input to the rest of engines, which provides generality to MAESTRO. Cluster Analysis. A PE cluster refers to a group of PEs that processes one or more data dimensions in parallel, specified by the CLUSTER directive. Figure <ref type="figure">7</ref> (b) describes the analysis in Cluster Analysis (CLA) engine. The CLA engine analyzes a given dataflow description written in dataflow directives to identify the number of sub-clusters, extract cluster dataflow directives and data dimensions, and augment the given dataflow descriptions for missing directives, stride handling, and so on, for each cluster level. Reuse Analysis. Figure <ref type="figure">7</ref> (b) includes a high-level description of analysis in data reuse analysis (RA) engine. RA engine identifies the amount of temporal and spatial reuse across adjacent time steps, which is the data iteration corresponding to the inner-most nontemporally/spatially unrolled mapping directive.</p><formula xml:id="formula_6">0 0-1 0 1 0 0-1 0 1 0 0-1 0 1 0 0-1 1 2 0 0-1 1 2 0 0-1 1 2 N K Y' X' 0 0-1 0 0 0 0-1 0 0 0 0-1 0 0 0 0-1 1 0 0 0-1 1 0 0 0-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Analysis</head><p>Figure <ref type="figure">7</ref> (a) presents a high-level overview of the performance and cost analysis engine, and Figure <ref type="figure" target="#fig_11">8</ref> shows high-level algorithm of the performance analysis (PA) engine. Utilizing the reuse information computed in the RA engine, PA engine computes the runtime for all the possible cases based on the data dimension and dataflow. The computed runtime is multiplied with the number of each case's occurrences and accumulated to compute the total runtime. The runtime of a DNN accelerator consists of communication delay (L2 to L1, L1 to L2, local forwarding) and computation delay in each PE, which are directly related to the accelerator's hardware parameters. PA engine considers double buffering when it computes the outstanding delay (the worst case delay of communication/computation delay) that directly contributes to the runtime.</p><p>To estimate communication delays, MAESTRO relies on its analytical network-on-chip (NoC) model based on a pipe model similar to other analytic models <ref type="bibr" target="#b28">[30]</ref>. The pipe model utilizes two parameters, the pipe width (bandwidth) and length (average delay), to estimate the communication delay via NoC. The model incorporates a pipelining effect as many packet-switching NoCs have similar behavior. Various combinations of the bandwidth and average delay enables to model NoC structures with reasonable accuracy. For example, Eyeriss <ref type="bibr" target="#b9">[11]</ref> has a two-level hierarchical bus with dedicated channels for input, weight, and output tensors. Therefore, a bandwidth of 3X properly models the top level NoC. The average latency depends on implementation details; users should choose an appropriate value considering implementation details (e.g., the use of ingress/egress buffers, which adds one cycle delay each). For more complicated NoC architectures, users should select bisection bandwidth and average latency considering uniform communication to all the PEs from a global buffer. For example, a N × N 2D mesh network with the injection point at one of the corners, the bisection bandwidth is N, and the average latency is N. Assuming that the user has access to the NoC implementation information, the NoC model is precise when the NoC is a bus or a crossbar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cost Analysis</head><p>Figure <ref type="figure" target="#fig_11">8</ref> describes how the cost analysis (CA) engine computes the number of buffer accesses and estimates the buffer size requirements for each tensor, considering data reuse computed in the RA engine and data iteration cases. Utilizing the access counts and the number of MAC operation information, MAESTRO computes the energy cost. MAESTRO includes an energy model based on those activity counts and Cacti <ref type="bibr" target="#b27">[29]</ref> simulation, which can be replaced by any other energy model based on such activity counts (e.g., Accelergy <ref type="bibr" target="#b45">[47]</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Complex Dataflow Analysis</head><p>Multi-cluster Analysis. Multi-cluster cases can be split into singlecluster cases with the data dimension size set as the mapping size of the corresponding mapping directive in the upper cluster. The outstanding delay of a cluster level becomes the computation delay of the next cluster level above. To handle various edge cases that affects all the lower cluster levels, MAESTRO recursively performs performance and cost analysis, as illustrated in Figure <ref type="figure">7</ref>. In the recursive analysis, the base case is the inner-most cluster whose subclusters are actual PEs. Although MAESTRO performs recursion, the complexity is not high because the number of PE cluster levels are typically two or three. Note that each of the edge cases at each cluster level also needs to be recursively processed. However, in most cases, we observe the number of edge cases across cluster levels is less than 20, which is still in a tractable scale.</p><p>Other DNNs. Although we used dense convolution as examples for simplicity, MAESTRO can model a variety of layers (LSTM hidden layer, pooling, fully-connected, transposed convolution, and so on) based on the generality of the data-centric approach. Our data-centric approach supports all the operations represented as the loop nest with two input tensors and one output tensor wherein all the tensor indices are coupled in only one or two data dimensions in affine functions. MAESTRO also can model uniformly distributed sparsity for any supported dataflow. Support for more complex statistical sparsity distributions is future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Validation</head><p>We validated MAESTRO's performance model against RTL simulations of two accelerators -MAERI <ref type="bibr" target="#b22">[24]</ref> and Eyeriss <ref type="bibr" target="#b11">[13]</ref> when running VGG16 and AlexNet respectively 1 . Figure <ref type="figure" target="#fig_12">9</ref> shows that the runtime estimated by MAESTRO are within 3.9% absolute error of the cycle-accurate RTL simulation and reported processing delay <ref type="bibr" target="#b11">[13]</ref> in average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDIES</head><p>Table <ref type="table">4</ref> summarizes the features of frequently used DNN operators from state-of-the-art DNN models <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35]</ref>. Early and late layers refer to layers with high-resolution activation with shallow channels and vice versa, respectively. We label them as early and late layers because such layers appear early and late in classification networks <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b40">42]</ref>. We compare the number of input channels and the input activation height to identify them 2 . 1 MAERI RTL is open-source. For Eyeriss, we use the reported runtime for AlexNet because detailed mapping parameters are described for only AlexNet in the paper.</p><p>2 If C &gt; Y, late layer. Else, early layer Table <ref type="table">3</ref>: Five example dataflows used for the evaluation. For conciseness, we omit redundant directives that are automatically inferred by MAESTRO. YX-P, YR-P, and CK-P dataflows are motivated by Shidiannao <ref type="bibr" target="#b13">[15]</ref>, Eyeriss <ref type="bibr" target="#b9">[11]</ref>, and NVDLA <ref type="bibr" target="#b0">[1]</ref>, respectively. The name of each dataflow is based on spatial dimensions from the upper-most cluster level.</p><p>C-Partitioned (C-P)</p><p>X-Partitioned (X-P)</p><p>YX-Partitioned (YX-P) KC-Partitioned (KC-P) With MAESTRO, we perform deeper case studies about the costs and benefits of various dataflows when they are applied to different DNN operations listed in Table <ref type="table">4</ref>. We evaluate five distinct dataflow styles listed in Table <ref type="table">3</ref> in Section 5.1 and the preference of each dataflow to different DNN operators. For energy estimation, we multiply activity counts with base energy values from Cacti <ref type="bibr" target="#b27">[29]</ref> simulation (28nm, 2KB L1 scratchpad, and 1MB shared L2 buffer). We also present distinct design space of an early layer (wide and shallow) and a late layer (narrow and deep) to show the dramatically different hardware preference of different DNN operator styles and dataflow in Section 5.2.</p><formula xml:id="formula_7">TemporalMap (1,1) K TemporalMap (Sz(R),1) Y TemporalMap (Sz(S),1) X TemporalMap (Sz(R),Sz(R)) R TemporalMap (Sz(S),Sz(S)) S SpatialMap (1,1) C TemporalMap (1,1) K TemporalMap (1,1) C TemporalMap (Sz(R),Sz(R)) R TemporalMap (Sz(S),Sz(S)) S TemporalMap (Sz(R),1) Y SpatialMap (Sz(S),1) X TemporalMap (1,1) K SpatialMap (Sz(R),1) Y TemporalMap (8+Sz(S)-1,8) X TemporalMap (1,1) C TemporalMap (Sz(R),Sz(R)) R TemporalMap (Sz(S),Sz(S)) S Cluster(8) SpatialMap (Sz(S),1) X TemporalMap (2,2) C TemporalMap (2,2) K SpatialMap (Sz(R),1) Y TemporalMap (Sz(S),1) X TemporalMap (Sz(R),Sz(R)) R TemporalMap (Sz(S),Sz(S)) S Cluster(Sz(R)) SpatialMap<label>(1,1) Y SpatialMap (1,1) R SpatialMap (1,1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case study I: Dataflow Trade-offs</head><p>Figure <ref type="figure" target="#fig_14">10</ref> shows the DNN-operator granularity estimation of runtime and energy of each dataflow across five state-of-the-art DNN models listed in Section 5. Note that this should be considered a comparison of dataflows-not of actual designs, which can contain several low-level implementation differences, e.g., custom implementations of logic/memory blocks, process technology, and so on. We observe that KC-P dataflow style dataflow provides overall low runtime and energy. However, the energy efficiency in VGG16 (Figure <ref type="figure" target="#fig_14">10 (b)</ref>) is worse than YR-P (Eyeriss <ref type="bibr" target="#b9">[11]</ref> style) dataflow, and the runtime is worse than YX-P (Shidiannao <ref type="bibr" target="#b13">[15]</ref> style) dataflow in UNet ( Figure <ref type="figure" target="#fig_14">10 (e)</ref>). This is based on the different preference We evaluate all the dataflows using five different DNN model; Resnet50 <ref type="bibr" target="#b16">[18]</ref>, VGG16 <ref type="bibr" target="#b40">[42]</ref>, ResNeXt50 <ref type="bibr" target="#b33">[35]</ref>, MobileNetV2 <ref type="bibr" target="#b26">[28]</ref>,</p><p>and UNet <ref type="bibr" target="#b32">[34]</ref>. The final column (f) presents the average results across models for each DNN operator type listed in Table <ref type="table">4</ref> and the adaptive dataflow case.</p><p>Table <ref type="table">4</ref>: Operators in state-of-the-art DNNs and their features and implication. Bottleneck <ref type="bibr" target="#b16">[18]</ref> and depth-wise separable convolution <ref type="bibr" target="#b4">[6]</ref> are listetd in a fine-grained operators (point-wise convolution, depthwise convolution, and residual links). Examples are based on notable networks (VGGnet <ref type="bibr" target="#b40">[42]</ref> and DCGAN <ref type="bibr" target="#b2">[4]</ref>) and state-of-the-art networks (MobileNetV2 <ref type="bibr" target="#b26">[28]</ref>, ResNet50 <ref type="bibr" target="#b16">[18]</ref>, ResNeXt50 <ref type="bibr" target="#b33">[35]</ref>.  <ref type="table">3</ref> with 256 PEs for four common DNN operators from Table 4. We select representative operators from state-of-the-art DNN models (Early layer: CONV1 in Resnet50 <ref type="bibr" target="#b16">[18]</ref>, late layer: CONV13 in VGG16 <ref type="bibr" target="#b40">[42]</ref>, depth-wise convolution (DWCONV): DWCONV of CONV2 in ResNeXt50 <ref type="bibr" target="#b33">[35]</ref>, point-wise convolution: first conv of bot-tleneck1 in MobilenetV2 <ref type="bibr" target="#b26">[28]</ref> C, X, YX, YR, and KC refers to C-P, X-P, YX-P, YR-P, and KC-P dataflows. A refers to algorithmic maximum reuse.).  <ref type="table">3</ref>. The access counts generated by MAESTRO are multiplied by appropriate energy values from Cacti <ref type="bibr" target="#b27">[29]</ref>. The values are normalized to the MAC energy of C-P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN Operators</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONV</head><p>and 15.17× higher activation and filter reuse factors, respectively, in early layers. However, in late layers, the reuse factors of YR-P and KC-P dataflow are almost similar (difference &lt; 11%), so the KC-P dataflow provides similar energy efficiency as YR-P in these cases. This can also be observed in the late layer (blue) bars in Figure <ref type="figure" target="#fig_14">10</ref> bottom-row plots.</p><p>Although the KC-P and YX-P dataflows provide low runtime (Figure <ref type="figure" target="#fig_14">10</ref>), it comes with high NoC cost, as the high bandwidth requirements shown in Figure <ref type="figure" target="#fig_1">11</ref> (c) highlight. Based on the operator type, some dataflows require dramatically higher NoC bandwidth than others. For example, YX-P requires high bandwidth for pointwise convolution as it has no convolutional reuse (i.e., overlapped activation data points among sliding windows) because of its 1x1 kernel while YX-P is optimized to exploit convolutional reuse via spatial reuse.</p><p>The diverse preference to dataflows of different DNN operators motivates us to employ optimal dataflow for each DNN operator type. We refer such an approach as adaptive dataflow and present the benefits in Figure <ref type="figure" target="#fig_14">10</ref> (f), the average case analysis across entire models in DNN operator granularity. By employing the adaptive approach, we could observe a potential 37% runtime and 10% energy reduction. Such an optimization opportunity can be exploited by flexible accelerators like Flexflow <ref type="bibr" target="#b23">[25]</ref> and MAERI <ref type="bibr" target="#b22">[24]</ref> or via heterogeneous accelerators that employ multiple sub-accelerators with various dataflow styles in a single DNN accelerator chip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case study II: Hardware Design-Parameters and Implementation Analysis</head><p>Using MAESTRO, we implement a hardware design space exploration (DSE) tool that searches four hardware parameters (the number of PEs, L1 buffer size, L2 buffer size, and NoC bandwidth) optimized for either energy efficiency, throughput, or energy-delayproduct (EDP) within given hardware area and power constraints.</p><p>The DSE tool receives the same set of inputs as MAESTRO with hardware area/power constraints and the area/power of building blocks synthesized with the target technology. For the cost of building blocks, we implement float/fixed point multiplier and adder, bus, bus arbiter, and global/local scratchpad in RTL and synthesis them using 28nm technology. For bus and arbiter cost, we fit the costs into a linear and quadratic model using regression because bus cost increases linearly and arbiter cost increases quadratically (e.g., matrix arbiter).</p><p>The DSE tool sweeps a target design space specified in the range of each parameter and search granularity. However, it skips design spaces at each iteration of hardware parameters by checking the minimum area and power of all the possible design points from inner loops of hardware parameters. This optimization allows it to skip invalid design points in a various granularity that reduces a large number of futile searches, which led to a large effective DSE rate ranging from 3.3K to 0.46M designs per second, as presented in Figure <ref type="figure" target="#fig_8">13 (c</ref>). Figure <ref type="figure" target="#fig_8">13 (c</ref>) shows statistics of four DSE runs explored the design space. We ran DSEs on a machine with i7-8700k CPU and 32GB memory operating Linux Mint 19 OS. We run four sets of the DSE on the machine at the same time, and all of them terminated within 24 minutes, with effective DSE rate of 0.17M designs per second, on average. Design Space Analysis: Using the DSE tool, we explore the design space of KC-P and YR-P dataflow accelerators. We set the area and power constraint as 16mm 2 and 450mW, which is the reported chip area and power of Eyeriss <ref type="bibr" target="#b11">[13]</ref>. We plot the entire design space we explored in Figure <ref type="figure" target="#fig_8">13</ref>.</p><p>Whether an accelerator can achieve peak throughput depends on not only the number of PEs but also NoC bandwidth. In particular, although an accelerator has sufficient number of PEs to exploit the maximum degree of parallelism a dataflow allows, if the NoC does not provide sufficient bandwidth, the accelerator suffers a communication bottleneck in the NoC. Such design points can be observed in the area-throughput plot in Figure <ref type="figure" target="#fig_8">13</ref> (a). YR-P dataflow requires low NoC bandwidth as shown in Figure <ref type="figure" target="#fig_1">11</ref> (c) so it does not show the same behavior as KC-P dataflow. However, with more stringent area and power constraints, YR-P dataflow will show the same behavior.</p><p>During DSE runs, MAESTRO reports buffer requirements for each dataflow and the DSE tool places the exact amount buffers MAESTRO reported. Contrary to intuition, larger buffer sizes do not always provide high throughput, as shown in buffer-throughput plots in Figure <ref type="figure" target="#fig_8">13</ref> (plots in the second column). The optimal points regarding the throughput per buffer size are in the top-left region of the buffer-throughput plots. The existence of such points indicates that the tiling strategy of the dataflow (mapping sizes in our directive representation) significantly affects the efficiency of buffer use.</p><p>We also observe the impact of hardware support for each data reuse, discussed in Table <ref type="table" target="#tab_2">2</ref>. Table <ref type="table" target="#tab_4">5</ref> shows such design points found in the design space of KC-P dataflow on VGG16-conv2 layer presented in the first row of Figure <ref type="figure" target="#fig_8">13</ref> (a). The first design point is the throughput-optimized design represented as a star in the first row of Figure <ref type="figure" target="#fig_8">13</ref>. When bandwidth gets smaller, the throughput significantly drops, but energy remains similar. However, the lack of spatial multicast or reduction support resulted in approximately 47% energy increase, as the third and fourth design points shows.</p><p>We observe that the throughput-optimized designs have a moderate number of PEs and buffer sizes, implying that hardware resources need to be distributed not only to PEs but also to NoC and buffers for high PE utilization. Likewise, we observe that the buffer amount does not directly increase throughput and energy efficiency. These results imply that all the components are intertwined, and they need to be well-balanced to obtain a highly-efficient accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORKS</head><p>Hardware DSE and dataflow optimization: Dataflow optimization is one of the key optimization targets in many recent DNN accelerators such as Eyeriss <ref type="bibr" target="#b9">[11]</ref>, Flexflow <ref type="bibr" target="#b23">[25]</ref>, SCNN <ref type="bibr" target="#b29">[31]</ref>, and NVDLA <ref type="bibr" target="#b0">[1]</ref>. C-brain <ref type="bibr" target="#b41">[43]</ref>, Flexflow <ref type="bibr" target="#b23">[25]</ref>, and analyzed the costbenefit tradeoff of three dataflows and explored the opportunity of adaptive dataflow based on the tradeoff. Ma et al. <ref type="bibr" target="#b24">[26]</ref> constructed model for convolutions on FPGAs focusing on three loop transformations; interchange, unroll, and tiling. Although their analytic model provides an intuition for trade-offs of dataflows, the model focus on one dataflow style they propose, does not consider regional spatial reuse, spatio-temporal reuse opportunities in DNN accelerators, and also doesn't consider communication delay in the NoC, which can dominate for dataflows with large tile sizes. Also, the target dataflow is optimized for HLS flow, and it show their significantly different hardware preference. We apply the area and power of Eyeriss <ref type="bibr" target="#b11">[13]</ref> as area/power constraints to the DSE.(16mm 2 , 450mW <ref type="bibr" target="#b11">[13]</ref>). The color of each data point indicates the number of PEs. Design points with fewer PEs can be paired with larger buffer sizes, up to the area budget. We mark the throughput-and energy-optimized designs using a star and cross. requires expressing using complex annotated loop nest with HLS synthesis directives. Caffeine <ref type="bibr" target="#b46">[48]</ref> proposed a full automatic FPGA flow that includes pragma-based annotation in programs, dataflow optimization framework, and DSE for FPGAs based on the analytic model defined over loop tiling and unrolling. However, the dataflow search space is limited due to fixed loop orders; three presets termed straightforward, input-major, and weight-mapping.</p><p>Past works related to data-centric approaches: There have been some works related to exploring data-centric approaches <ref type="bibr" target="#b19">[21]</ref><ref type="bibr" target="#b20">[22]</ref><ref type="bibr" target="#b21">[23]</ref>, where the approaches reason about flow of data through memory hierarchy, instead of control-structure centric analysis, for localityenhancement transformations such as multi-level data blocking <ref type="bibr" target="#b20">[22]</ref> and data shackling <ref type="bibr" target="#b19">[21]</ref>. But, the above data-centric approaches have been explored in the context of driving optimizations for multi-level caches, but not estimating energy or throughput of input kernels precisely. We discuss related work on loop-nest notation and reuse analysis in compilers in Section 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND FUTURE WORK</head><p>This work is motivated by the observation that co-optimizing DNN accelerator microarchitecture and its internal dataflow(s) is crucial for accelerator designers to achieve both higher performance and energy efficiency. In this work, we introduced data-centric directives to specify DNN dataflows in a compact form and understand data reuse opportunities. We also presented an analytical model called MAESTRO to estimate execution time, energy efficiency, and the hardware cost of dataflows. We evaluated our analytical model relative to the MAERI and Eyeriss accelerators and found our model to be highly consistent with cycle-accurate simulations and reported runtime, which shows the soundness of the analytic model. We provided cases studies about the costs and benefits of dataflow choices over in five state-of-the-art DNN models with a focus on common DNN operators in them, showing diverse preference to dataflow and hardware, which motivates adaptive dataflow accelerator and heterogeneous accelerators. Finally, we also demonstrated the use of MAESTRO for design-space exploration of two dataflows in early and late layers, showing dramatically different hardware preference of each layer. Our DSE tool based on MAESTRO enabled fast DSE based on optimization to skip invalid designs, which led to a high average DSE rate of 0.17M designs per second.</p><p>In the future, we plan to leverage MAESTRO to implement a dataflow auto-tuner to find an optimal dataflow on the specified DNN model and hardware configuration. With the optimal dataflow, we plan to extend our infrastructure to automatically generate RTL, facilitating end-to-end DNN acceleration flow.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Convolutional layer example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Abstract DNN accelerator architecture model which is pervasive in many state-of-the-art accelerators<ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b36">38]</ref>. The illustrated base architecture can be hierarchically organized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An operational example of a weight-stationary style accelerator with four PEs. For simplicity, input/output channels and batch are omitted. A 2x2 kernel (R=2, S=2) is used in this example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>XFigure 4 :</head><label>4</label><figDesc>Figure 4: An example 1D convolution and an example output-stationary dataflow on the convolution. We represent the dataflow (b) in loop nest and (c) data-centric directives. In (c), gray boxes represent omittable descriptions, which can be inferred (upper gray box) or do not affect the data reuse over PEs (lower gray box). (d) shows an abbreviated form of the dataflow description in data-centric directives. (e) and (f) show resulting mapping on PEs and iteration space, whose dots correspond to computation (or, partial sums).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 1 )Figure 5 :</head><label>15</label><figDesc>Figure 5: The impact of directive order, spatial/temporal maps, tile sizes, and clustering over 1D convolution presented in Figure 4. The first row</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>; c = 3*c1+c0; r = y0; s = x0; y=stride*y1 + y0; x=stride*x1 + x0; P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 - 3 Time</head><label>13</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 6:  An extended example of a row-stationary style dataflow mapped on a six-PE accelerator. We select our own tile sizes for any not specified in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A high-level overview of algorithms in performance and cost analysis engines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Runtime model validation against MAERI<ref type="bibr" target="#b22">[24]</ref> RTL simulation with 64 PEs and Eyeriss<ref type="bibr" target="#b11">[13]</ref> runtime reported in the paper with 168 PEs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>K-</head><label></label><figDesc>TemporalMap (64,64) C TemporalMap (Sz(R),Sz(R)) R TemporalMap (Sz(S),Sz(S)) S TemporalMap (Sz(R),1) Y TemporalMap (Sz(S),Large spatial reduction opportunities (Large output activation reuse) -Small input activation/filter reuse -Input channel (C) parallelism -No local reuse -Large temporal reuse of filter -Spatial reuse opportunities (via halo in input activation) -Input column (X) parallelism -Weight-stationary -Large temporal reuse of filter -Better spatial reuse opportunities over X-P (via 2D halo in input activation) -2D activation (X and Y) parallelism -Output-stationary -Motivated by Shi-diannao [14]-Large temporal reuse of input activation and filter -Spatial reduction opportunities (spatial reuse of output activations) -Activation row (Y) and filter column (S) parallelism -Row-stationary -Motivated by Eyeriss<ref type="bibr" target="#b8">[10]</ref> -Spatial reuse of input activation -Large spatial reduction factor (64-way) over input channel (C) -Input/output channel (C and K) parallelism -Weight-stationary -Motivated by NVDLA<ref type="bibr" target="#b0">[1]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Plots in top and bottom rows present runtime and energy estimation of five dataflows listed in the table, respectively. We apply 256 PEs and 32GBps NoC bandwidth. We evaluate all the dataflows using five different DNN model; Resnet50<ref type="bibr" target="#b16">[18]</ref>, VGG16<ref type="bibr" target="#b40">[42]</ref>, ResNeXt50<ref type="bibr" target="#b33">[35]</ref>, MobileNetV2<ref type="bibr" target="#b26">[28]</ref>, and UNet<ref type="bibr" target="#b32">[34]</ref>. The final column (f) presents the average results across models for each DNN operator type listed in Table4and the adaptive dataflow case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Characteristics- 5 -Figure 11 :</head><label>511</label><figDesc>Figure 11: Reuse and NoC bandwidth requirements of dataflows in Table3with 256 PEs for four common DNN operators from Table 4. We select representative operators from state-of-the-art DNN models (Early layer: CONV1 in Resnet50<ref type="bibr" target="#b16">[18]</ref>, late layer: CONV13 in VGG16<ref type="bibr" target="#b40">[42]</ref>, depth-wise convolution (DWCONV): DWCONV of CONV2 in ResNeXt50<ref type="bibr" target="#b33">[35]</ref>, point-wise convolution: first conv of bot-tleneck1 in MobilenetV2<ref type="bibr" target="#b26">[28]</ref> C, X, YX, YR, and KC refers to C-P, X-P, YX-P, YR-P, and KC-P dataflows. A refers to algorithmic maximum reuse.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The breakdown of energy consumption (MAC and L1/L2 scratchpad access energy) of the dataflows from Table3. The access counts generated by MAESTRO are multiplied by appropriate energy values from Cacti<ref type="bibr" target="#b27">[29]</ref>. The values are normalized to the MAC energy of C-P.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>9 Figure 13 :</head><label>913</label><figDesc>Figure 13: The design space of an accelerator with (a) KC-P and (b) YR-P dataflow. We highlight the design space of an early and a late layer to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>-Map Target: On-Chip Global Buffer -- TemporalMap (6, 6) X' TemporalMap (6, 6) S Cluster (NumPEs=3) // We have one global buffer --Map Target: PE L1 buffer -- SpatialMap (2, 2) X' TemporalMap (3, 3) S Cluster (1) // Each PE includes one L1 buffer --Map Target: PE L0 buffer (Reg) -- TemporalMap (1, 1) X' TemporalMap (1, 1) S X': 2 S: 3 X': 12 S: 6</head><label></label><figDesc></figDesc><table><row><cell>for (x'=0; x' &lt; 12; x'++) for (s=0; s &lt; 6; s++)</cell><cell>-</cell><cell>PE 0</cell><cell>PE 1</cell><cell>PE 2</cell></row><row><cell>O[x'] += W[s] * I[x'+s]</cell><cell></cell><cell>x' = {0,1}</cell><cell>x' = {2,3}</cell><cell>x' = {4,5}</cell></row><row><cell></cell><cell></cell><cell>s = {0,1,2}</cell><cell>s = {0,1,2}</cell><cell>s = {0,1,2}</cell></row><row><cell>--PE clusters --</cell><cell></cell><cell>x' = {0,1}</cell><cell>x' = {2,3}</cell><cell>x' = {4,5}</cell></row><row><cell>for (x'2=0; x'2 &lt; 2; x'2++)</cell><cell></cell><cell>s = {3,4,5}</cell><cell>s = {3,4,5}</cell><cell>s = {3,4,5}</cell></row><row><cell>par_for (s2=0; s2 &lt; 1; s2++)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>--Map Target: PE L1 buffer --</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>par_for (x'1=0; x'1 &lt; 3; x'1++)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>for (s1=0; s1 &lt; 2; s1++)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>--Map Target: PE L0 buffer (Reg) --</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>for (x'0=0; x'0 &lt;2; x'0++)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>for (s0=0; s0 &lt; 3; s0++)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s = 2 * 3 * s2 + 3 * s1 + s0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x' = 3 * 2 * x'2 + 2 * x'1 + x'0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>O[x'] += W[s] * I[x'+s]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Hardware Implementation Choices for supporting spatial and temporal reuse. Note -by temporal multicast, we refer to stationary buffers from which the same data is read over time.</figDesc><table><row><cell>Reuse Type</cell><cell>Communication Type</cell><cell cols="3">HW Implementation Choices</cell></row><row><cell>Spatial</cell><cell>Multicast Reduction</cell><cell>GBM Fanout (e.g., Bus, Tree) PE PE PE PE PE PE PE + + + GBM</cell><cell></cell><cell>PE Store-and-Fwd PE PE (e.g., Systolic Array) Reduce-and-Fwd PE PE PE</cell></row><row><cell></cell><cell></cell><cell cols="2">Fanin (e.g., Reduction Tree)</cell><cell>(e.g., Systolic Array)</cell></row><row><cell></cell><cell>Multicast</cell><cell>Buf</cell><cell>PE</cell><cell>MAC</cell></row><row><cell>Temporal</cell><cell></cell><cell cols="3">Multiple reads from a buffer</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>An overview of MAESTRO's analysis framework. For simplicity, we omit components other than analysis engines. A tensor information table (tensor_tbl), a cluster information table (cluster_info_tbl), a mapping information table with mapping and reuse size for all the possible data iteration position cases for each cluster level (mapping_info_tbl), an abstract hardware model of the target DNN accelerator(hw_model). Output: Statistics of the target DNN, accelerator, and dataflow (stats) Object: To compute the mapping size and the amount of data reuse for all the possible data iteration position cases.</figDesc><table><row><cell>Multi-Level Cluster Analysis (Recursive) Performance Analysis Engine Spatial Ingress / Egress Traffic Analysis Communication Delay Analysis Cost Analysis Engine Buffer Access / MAC Count Analysis Buffer Size Analysis Data Iteration Case Analysis Computation Delay Analysis Data Iteration Case Number of Case Occurrences Ingress/Egress Traffic Latency Hiding Analysis (a) MAESTRO Analysis Framework Cluster Analysis Engine Cluster Structure Analysis Cluster Data Mapping Size Analysis Cluster Dataflow Augmentation Reuse Analysis Engine Unit Temporal / Spatial Data Reuse Analysis Sub-cluster Array Temporal / Spatial Data Reuse Analysis Cluster Dataflow Cluster Data Dim. Cluster Structure Tensor list Coupled Dim. Unit Mapping Sz Unit Data Reuse Array Mapping Sz Array Data Reuse /* Extracts the cross product of all the possible data iteration cases Tensor Analysis Engine Coupled Dim. Analysis Number of PEs NoC BW/Latency Buffer Sz DNN Model Description HW Resource Description Data-centric Dataflow Description Data dimensions Layer Operations Strides Initialize(stats); (Init, Steady, and Edge) of each data dimension */ iteration_cases = ExtractDataIterationCases(tensor_tbl , cluster_info_tbl); for each iter_case in iteration_cases num_case_occurrences = GetNumCaseOccurrences(iter_case, tensor_tbl, cluster_info_tbl); /* Considering iteration case, compute the number of partial sums for each PE */ num_psums = GetNumPSums(iter_case, cluster_info_tbl , mapping_info_tbl); /* Considering reuse and iteration case, compute the amount of new input tensor data to be fetched from a buffer in upper cluster levels */ cluster_ingress_traffic = GetNumPSums(iter_case, tensor_tbl , cluster_info_tbl , mapping_info_tbl); /* Considering reuse and iteration case, compute the amount of output tensor data to be committed to a buffer in upper cluster levels */ cluster_egress_traffic = GetNumOutputs(iter_case, tensor_tbl , cluster_info_tbl , mapping_info_tbl); //// Core cost analysis //// for each tensor in tensor_tbl stats.upsteream_buffer_read[tensor] += cluster_ingress_traffic[tensor]; stats.downsteream_buffer_write[tensor] += cluster_ingress_traffic[tensor]; stats.upstream_buffer_write[tensor] += cluster_egress_traffic[tensor]; stats.downstream_buffer_read[tensor] += num_psums; stats.upstream_buffer_size_req[tensor] = 2*Max(stats.upstream_buffer_size_req[tensor], cluster_ingress_traffic[tensor], cluster_egress_traffic[tensor]); stats.downstream_buffer_size_req[tensor] = 2*Max(stats.downstream_buffer_size_req[tensor], num_psums, cluster_egress_traffic[tensor]); end //// Core performance analysis //// ingress_delay = GetDelay(cluster_ingress_traffic, hw_model); egress_delay = GetDelay(cluster_output_traffic, hw_model); compute_delay = GetComputeDelay(num_psums, hw_model); compute_delay += GetPSumFwdDelay(iter_case, tensor_tbl, cluster_info_tbl, mapping_info_tbl); /* Considers double-buffering; treats the initialization case as an exeception */ if IsFullInit(iter_case) then outstanding_delay = ingress_delay + compute_delay + egress_delay; else outstanding_delay = Max(ingress_delay, egress_delay, compute_delay); end stats.run_time += outstanding_delay * num_case_occurrences; stats.num_macs += num_psums * num_active_clusters * num_case_occurrences; end Return(stats); Figure 7: Procedure PerformanceAndCostAnalysisEngine: endprocedure</cell><cell>&lt;Performance Report&gt; Runtime Bottleneck Info. NoC BW Requirements … &lt;Cost Report&gt; Activity Counts (Energy) Buffer Sz Requirements Overall Data Reuse …</cell><cell>Engine Tensor Analysis Cluster Analysis Reuse Analysis Performance Analysis Cost Analysis</cell><cell>(b) Analysis in Each Engine Analysis Description -Extract tensor names, tensor types (input/output), coupled dimensions, and size of each coupled dimenisons -Extract dataflow directives for each cluster level -Extract data dimension sizes for each cluster level -Infer missing directives, apply stride, and so on -Analyze the number of sub-clusters at each cluster level -Analyze unit mapping size and the amount of reuse across unit steps for every possible combination of data iteration position cases. (computes temporal reuse). -Consolidate unit step reuse analysis results to compute reuse over entire sub-cluster array for each cluster level. (computes spatial reuse). -Identifies all the possible data iteration cases and the number of occurrences of the cases -Computes ingress/egress traffic considering data reuse -Computes communication (ingress/egress) and computation delay -Considering latency hiding, determine outstanding delay -For each data iteration case, computes the number of buffer accesses and MAC operations -Identify buffer size requirements based on worst-case analysis</cell></row></table><note>Inputs:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The impact of multicasting capability, bandwidth, and buffer size. Design points are from the design space of Figure13(a) VGG16-CONV2.</figDesc><table><row><cell>Design Point</cell><cell>Num PEs</cell><cell>NoC BW (Data pt/Cycle)</cell><cell cols="2">Spatial Reuse Support Multicast Reduction</cell><cell>Temporal Reuse Support Buffer Size (KB)</cell><cell>Throughput (MAC/cycle)</cell><cell>Energy (X MACs)</cell><cell></cell></row><row><cell>Reference</cell><cell>56</cell><cell>40</cell><cell>Yes</cell><cell>Yes</cell><cell>6.13</cell><cell>48.6</cell><cell>5.26 x 10</cell><cell>9</cell></row><row><cell>Small bandwidth</cell><cell>56</cell><cell>24</cell><cell>Yes</cell><cell>Yes</cell><cell>6.13</cell><cell>34.54</cell><cell cols="2">5.26 x 10 9</cell></row><row><cell>No multcast</cell><cell>56</cell><cell>40</cell><cell>No</cell><cell>Yes</cell><cell>2.26</cell><cell>33.39</cell><cell>7.56 x 10</cell><cell>9</cell></row><row><cell>No Sp. reduction</cell><cell>56</cell><cell>40</cell><cell>Yes</cell><cell>No</cell><cell>4.68</cell><cell>32.05</cell><cell>7.77 x 10</cell><cell>9</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank Joel Emer for insightful advice and constructive comments to improve this work; Vivienne Sze and Yu-Hsin Chen for their insights and taxonomy that motivated this work. This work was supported by NSF Awards 1755876 and 1909900.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://nvdla.org" />
	</analytic>
	<monogr>
		<title level="j">NVDLA Deep Learning Accelerator</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Snapea: Predictive early activation for reducing computation in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Aklaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kambiz</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02595</idno>
		<title level="m">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><forename type="middle">Weyand</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreetto</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analytical Modeling of Cache Behavior for Affine Programs</title>
		<author>
			<persName><forename type="first">Wenlei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Noel</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3158120</idno>
		<ptr target="https://doi.org/10.1145/3158120" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Program. Lang</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2017-12">2017. Dec. 2017</date>
		</imprint>
	</monogr>
	<note>POPL, Article</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Practical Automatic Polyhedral Parallelizer and Locality Optimizer</title>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Hartono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<idno type="DOI">10.1145/1375581.1375595</idno>
		<ptr target="https://doi.org/10.1145/1375581.1375595" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI &apos;08)</title>
				<meeting>the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Architectural support for programming languages and operating systems (ASPLOS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi18/presentation/chen" />
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). USENIX Association</title>
				<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<idno>arXiv:cs.DC/1807.07928</idno>
		<title level="m">Eyeriss v2: A Flexible and High-Performance Accelerator for Emerging Deep Neural Networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Minimizing computation in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingjun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks (ICANN)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ShiDianNao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">Clement</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tetris: Scalable and efficient neural network acceleration with 3d memory</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="751" to="764" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Norman P Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data-centric Multi-level Blocking</title>
		<author>
			<persName><forename type="first">Induprakas</forename><surname>Kodukula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nawaaz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Pingali</surname></persName>
		</author>
		<idno type="DOI">10.1145/258915.258946</idno>
		<ptr target="https://doi.org/10.1145/258915.258946" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 1997 Conference on Programming Language Design and Implementation (PLDI &apos;97)</title>
				<meeting>the ACM SIGPLAN 1997 Conference on Programming Language Design and Implementation (PLDI &apos;97)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="346" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data-Centric Transformations for Locality Enhancement</title>
		<author>
			<persName><forename type="first">Induprakas</forename><surname>Kodukula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Pingali</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1011172104768</idno>
		<ptr target="https://doi.org/10.1023/A:1011172104768" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="319" to="364" />
			<date type="published" when="2001-06-01">2001. 01 Jun 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An Experimental Evaluation of Tiling and Shackling for Memory Hierarchy Management</title>
		<author>
			<persName><forename type="first">Induprakas</forename><surname>Kodukula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dror</forename><surname>Maydan</surname></persName>
		</author>
		<idno type="DOI">10.1145/305138.305243</idno>
		<ptr target="https://doi.org/10.1145/305138.305243" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Supercomputing (ICS &apos;99)</title>
				<meeting>the 13th International Conference on Supercomputing (ICS &apos;99)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="482" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="461" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Wenyan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizing loop operation and dataflow in fpga acceleration of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarma</forename><surname>Vrudhula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Sun</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays (FPGA)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diffy: a Déja vu-Free Differential Deep Neural Network Accelerator</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CACTI 6.0: A tool to model large caches</title>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP laboratories</title>
		<imprint>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Timeloop: A Systematic Approach to DNN Accelerator Evaluation</title>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<meeting>the 2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combined Iterative and Model-driven Optimization in an Automatic Parallelization Framework</title>
		<author>
			<persName><forename type="first">Louis-Noël</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cédric</forename><surname>Bastoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC.2010.14</idno>
		<ptr target="https://doi.org/10.1109/SC.2010.14" />
	</analytic>
	<monogr>
		<title level="m">Conference on High Performance Computing Networking, Storage and Analysis</title>
				<meeting><address><addrLine>SC; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-11-13">2010. 2010. November 13-19, 2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Polyhedral Optimization of TensorFlow Computation Graphs</title>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Pradelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baskaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Lethin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Extreme-scale Programming Tools</title>
				<imprint>
			<publisher>ESPT</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollã Ąr Zhuowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Saining Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic selection of high-order transformations in the IBM XL FORTRAN compilers</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.413.0233</idno>
		<ptr target="https://doi.org/10.1147/rd.413.0233" />
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="233" to="264" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">An analytical model for loop tiling and its solution</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimrod</forename><surname>Megiddo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2000.842294</idno>
		<idno>ISPASS. 146-153</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2000.842294" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From highlevel deep neural models to FPGAs</title>
		<author>
			<persName><forename type="first">Hardik</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongse</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenkai</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asit</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Oil and Water Can Mix: An Integration of Polyhedral and AST-based Transformations</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shirako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Noël</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC.2014.29</idno>
		<ptr target="https://doi.org/10.1109/SC.2014.29" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &apos;14)</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &apos;14)<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="287" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Analytical Bounds for Optimal Tile Size Selection</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shirako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naznin</forename><surname>Fauzia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Noël</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-28652-0_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-28652-0_6" />
	</analytic>
	<monogr>
		<title level="m">Compiler Construction -21st International Conference, CC 2012, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2012</title>
				<meeting><address><addrLine>Tallinn, Estonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03-24">2012. March 24 -April 1, 2012</date>
			<biblScope unit="page" from="101" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks For Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">C-brain: a deep learning accelerator that tames the diversity of CNNs through adaptive data-level parallelization</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bosheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation Conference (DAC)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A Data Locality Optimizing Algorithm</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.1145/113445.113449</idno>
		<ptr target="https://doi.org/10.1145/113445.113449" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 1991 Conference on Programming Language Design and Implementation (PLDI &apos;91)</title>
				<meeting>the ACM SIGPLAN 1991 Conference on Programming Language Design and Implementation (PLDI &apos;91)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="30" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs</title>
		<author>
			<persName><forename type="first">Yannan</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference On Computer Aided Design (ICCAD)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Caffeine: Towards uniformed representation and acceleration for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenman</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peichen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
