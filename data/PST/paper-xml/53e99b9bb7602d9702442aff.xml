<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extended local binary patterns for texture classification ☆</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Science and Engineering</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<addrLine>47 Yanwachi</addrLine>
									<postCode>410073</postCode>
									<settlement>Changsha, Hunan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingjun</forename><surname>Zhao</surname></persName>
							<email>lingjun.zhao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Science and Engineering</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<addrLine>47 Yanwachi</addrLine>
									<postCode>410073</postCode>
									<settlement>Changsha, Hunan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunli</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Science and Engineering</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<addrLine>47 Yanwachi</addrLine>
									<postCode>410073</postCode>
									<settlement>Changsha, Hunan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gangyao</forename><surname>Kuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Science and Engineering</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<addrLine>47 Yanwachi</addrLine>
									<postCode>410073</postCode>
									<settlement>Changsha, Hunan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
							<email>pfieguth@uwaterloo.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Systems Design Engineering</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extended local binary patterns for texture classification ☆</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A40A04BB30279B1679293730BDEFDFAE</idno>
					<idno type="DOI">10.1016/j.imavis.2012.01.001</idno>
					<note type="submission">Received 19 April 2011 Received in revised form 20 October 2011 Accepted 4 January 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Texture classification Local binary pattern (LBP) Bag-of-words (BoW) Rotation invariance</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel approach for texture classification, generalizing the well-known local binary pattern (LBP) approach. In the proposed approach, two different and complementary types of features (pixel intensities and differences) are extracted from local patches. The intensity-based features consider the intensity of the central pixel (CI) and those of its neighbors (NI); while for the difference-based feature, two components are computed: the radial-difference (RD) and the angular-difference (AD). Inspired by the LBP approach, two intensity-based descriptors CI-LBP and NI-LBP, and two difference-based descriptors RD-LBP and AD-LBP are developed. All four descriptors are in the same form as conventional LBP codes, so they can be readily combined to form joint histograms to represent textured images. The proposed approach is computationally very simple: it is totally training-free, there is no need to learn a texton dictionary, and no tuning of parameters. We have conducted extensive experiments on three challenging texture databases (Outex, CUReT and KTHTIPS2b). Outex results show significant improvements over the classical LBP approach, which clearly demonstrates the great power of the joint distributions of these proposed descriptors for gray-scale and rotation invariant texture classification. The proposed method produces the best classification results on KTHTIPS2b, and results comparable to the state-of-the-art on CUReT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Texture classification is a fundamental issue in computer vision and image processing, playing a significant role in a wide range of applications that include medical image analysis, remote sensing, object recognition, document analysis, environment modeling, content-based image retrieval etc. <ref type="bibr" target="#b0">[1]</ref>. For four decades, texture analysis has been an area of intense research, however analyzing real world textures has proven to be surprisingly difficult, in many cases caused by natural texture inhomogeneity of varying illumination, scale changes, and variability in surface shape.</p><p>Recently, the orderless Bag-of-Words (BoW) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref> approach, representing texture images statistically as histograms over a discrete texton dictionary, has proven extremely popular and successful in texture classification tasks. Robust and discriminative local texture descriptors and global statistical histogram characterization have supplied complementary components toward the BoW feature extraction of texture images. The former attempts to extract a collection of powerful and distinctive appearance descriptors from local patches; while the latter first utilizes the fact that texture images contain self-repeating patterns by vector-quantifying (typically by k-means) the local feature vectors to form a texton dictionary, and then represent texture images statistically as compact histograms over the learned texton dictionary.</p><p>In this simple and efficient BoW framework, it is generally agreed that the local descriptors play a much more important role, and have therefore received considerable attention <ref type="bibr">[2-5, 8, 9, 6]</ref>. The approaches can be grouped into sparse and dense types, with the sparse approach using appearance descriptors at a sparse set of detected interest points. Noticeable sparse descriptors include SPIN, SIFT and RIFT <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. In contrast, dense approaches use appearance descriptors pixel by pixel <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b8">9]</ref>. The sparse approach largely relies on the sparse output of local interest region detectors, which might miss important texture primitives and fail to provide enough regions for a robust statistical characterization of the texture.</p><p>Among the most popular dense descriptors are the various filter banks, such as Gabor filters <ref type="bibr" target="#b10">[11]</ref>, the filter bank of Schmid <ref type="bibr" target="#b4">[5]</ref>, the filter bank of Leung and Malik <ref type="bibr" target="#b4">[5]</ref>, the MR8 <ref type="bibr" target="#b1">[2]</ref>, the filter bank of Crosier <ref type="bibr" target="#b8">[9]</ref> and many others <ref type="bibr" target="#b11">[12]</ref>. The design of a filter bank is nontrivial and likely to be application dependent. Although enormous efforts have been carried out along this direction, the supremacy of filter bank-based descriptors for texture analysis has been challenged by several authors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7]</ref> who have demonstrated that using the intensities or differences in a local small patch directly can produce superior or comparable classification performance to filter banks with large spatial support. In <ref type="bibr" target="#b6">[7]</ref>, the authors propose sparse modeling of local texture patches, however the sparse texton learning and sparse coding process is computationally expensive. Two particularly important works along these lines are the VZ-Joint classifier <ref type="bibr" target="#b2">[3]</ref> and the LBP method <ref type="bibr" target="#b3">[4]</ref>. The simple, elegant and 1. The LBP operator produces long histograms which are sensitive to image rotation. 2. The LBP has small spatial support; in its basic form, the LBP operator cannot properly detect large-scale textural structures. <ref type="bibr" target="#b2">3</ref>. LBP loses local textural information, since only the signs of differences of neighboring pixels are utilized. 4. LBP is very sensitive to noise. The slightest fluctuation above or below the value of the central pixel is treated as equivalent to a major contrast between the central pixel and its surroundings.</p><p>On the basis of the above issues, researchers have proposed a variety of LBP variants. In terms of locality, the authors in <ref type="bibr" target="#b19">[20]</ref> propose to extract global features from Gabor filter responses as a complementary descriptor. In order to recover the loss of information created by computing the LBP value, the local image contrast has been introduced by Ojala et al. <ref type="bibr" target="#b3">[4]</ref> as a complementary measure, and better performance has been reported therein. Moreover, Guo et al. <ref type="bibr" target="#b20">[21]</ref> propose to include the information contained in the magnitudes of local differences as complementary to the signs used by LBP, and claim better performance.</p><p>Regarding LBP robustness, especially to noise, the influential work by Ojala et al. <ref type="bibr" target="#b3">[4]</ref> extends basic LBP to a multiresolution context, and rotation invariant patterns are introduced and successfully used in reducing the dimension of the LBP histogram and enhancing robustness and speed. Ahonen et al. introduce soft histograms <ref type="bibr" target="#b27">[28]</ref>, and Tan and Triggs <ref type="bibr" target="#b28">[29]</ref> introduced local ternary patterns (LTP), using tertiary numbers instead of binary. Noting that uniform LBPs are not necessary to occupy the major pattern proportions, Liao et al. <ref type="bibr" target="#b19">[20]</ref> proposed to use dominant LBP (DLBP) which considers the most frequently occurred patterns in a texture image.</p><p>Very recently, Heikkilä et al. <ref type="bibr" target="#b21">[22]</ref> exploit circular symmetric LBP (CS-LBP) for local interest region description, and Chen et al. present a WLD descriptor by including orientation information as a robust descriptor <ref type="bibr" target="#b22">[23]</ref>.</p><p>The LBP approach is based on the assumption that the local differences of the central pixel and its neighbors are independent of the central pixel itself. However, in practice an exact independence is not warranted: the superiority of both VZ-Joint and VZ-MRF classifiers over LBP clearly demonstrates the benefits of explicitly including the information contained in the central pixel <ref type="bibr" target="#b2">[3]</ref>.</p><p>The fundamental question being raised here is whether explicitly modeling the joint distribution of the central pixel and its neighbors is an advantage or not, and how to effectively include the missing between-scale information so that better texture classification can be achieved? Motivated by the work of Varma and Zisserman <ref type="bibr" target="#b2">[3]</ref> and the LBP approach studied by Ojala et al. <ref type="bibr" target="#b3">[4]</ref>, in this paper we propose a simple, yet very powerful and novel local texture descriptor to generalize the conventional LBP approach. In the proposed approach, two different but complementary types of features in a local patch, the pixel intensities and the pixel differences, are utilized by using a common concept, the LBP coding strategy. The pixel intensities are divided into two components: the intensity of the central pixel and the intensities of its neighboring pixels. For pixel differences, we study radial and angular differences.</p><p>All four descriptors (two intensity based, two difference based) are in the same form as the conventional LBP codes, thus they can be readily combined to form a joint histogram. The fusing of these descriptors will be shown to lead to significantly improved classification results on the experimental protocols designed for verifying the performance of the LBP approach in <ref type="bibr" target="#b3">[4]</ref>. The key to our proposed approach is that it employs the advantages of VZ-Joint/VZ-MRF in its strong performance from having a joint distribution, and those of LBP in computational efficiency.</p><p>The paper is organized as follows: we start with a brief review of the classical LBP approach in Section 2, followed by details of the derivation of the proposed descriptors and the classification scheme. In Section 3, we verify the proposed approach with extensive experiments on popular texture datasets and comparisons with various state-of-the-art texture classification techniques. Section 4 provides concluding remarks and possible extensions of the proposed method. A short, preliminary version of this work appeared in <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed descriptors</head><p>This section begins by reviewing conventional LBP, followed by the new descriptors designed to address the limitations of LBP. Finally, the multiresolution analysis and classification scheme of this work is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">A brief review of LBP</head><p>The LBP method, first proposed by Ojala et al. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4]</ref>, encodes the pixel-wise information in textured images. Images are probed locally by sampling grayscale values at a central point x 0, 0 and p points x r, 0 , …, x r, p -1 spaced equidistantly around a circle of radius r (the choice of which acts as a surrogate for controlling the scale of description), as shown in Fig. <ref type="figure">1</ref>. In LBP, a "local pattern" operator describes the relationships between a pixel and its neighborhood pixels; all neighbors that have values higher than or equal to the value of the central pixel are given a value of 1, and all those lower a value of 0. The binary values associated with the neighbors are then read sequentially, clockwise, to form a binary number which may be used to characterize the local texture. Formally,</p><formula xml:id="formula_0">LBP p;r ¼ X p-1 n¼0 s x r;n -x 0;0 2 n ; s x ð Þ ¼ 1; x ≥ 0 0; x b 0 :<label>ð1Þ</label></formula><p>1 A bibliography of LBP-related research can be found at http://www.cse.oulu.fi/ MVG/LBP_Bibliography/.</p><p>Fig. <ref type="figure">1</ref>. A central pixel x 0, 0 and its p circularly and evenly spaced neighbors {x r, i } i = 0 p -1 on radius r.</p><p>Relative to the origin at (0, 0), the coordinates of the neighbors are given by r sin(2πn/p), r cos(2πn/p). The gray values of neighbors which do not fall exactly in the center of pixels are estimated by interpolation.</p><p>Given an N × M image I, let LBP p, r (i, j) be the identified LBP pattern of each pixel (i,j), then the whole textured image is represented by a histogram vector h of length K:</p><formula xml:id="formula_1">h k ð Þ ¼ X N i¼1 X M j¼1 δ LBP p;r i; j ð Þ-k<label>ð2Þ</label></formula><p>where 0 ≤ k ≤ K -1, K = 2 p is the number of LBP codes, and δ(•) is the Dirac delta function. This formulation has several attractive properties that favor its usage: gray-scale invariance, computational speed, few parameters, satisfactory discriminant power, and rotation invariance achieved by simple cyclic shifts.</p><p>On the other hand, the basic LBP operator produces rather long histograms (2 p distinct values), and it can become intractable to estimate h due to the overwhelming dimensionality of h with large p. Moreover, it is easy to realize that due to the way LBP numbers are created, they are very sensitive to noise: the slightest fluctuation above or below the value of the central pixel is treated the same way as a major contrast between the central pixel and its surroundings. One way to avoid noisy patterns is to simply ignore them: a noisy pattern due to its randomness will create neighbors that fluctuate above and below the value of the central pixel, with 0 s and the 1 s frequently succeeding each other. Therefore, one improvement suggested by Ojala et al. <ref type="bibr" target="#b3">[4]</ref> is to consider only the so-called "uniform" patterns by proposing the LBP p, r riu2 operator, merging nonuniform patterns directly into one pattern. The success of the LBP p, r riu2 operator also comes from that fact that the "uniform" patterns appear to be fundamental properties of local image textures <ref type="bibr" target="#b3">[4]</ref>, and represent some prominent and salient local texture structures. The LBP p, r riu2 operator is formally defined as</p><formula xml:id="formula_2">LBP riu2 p;r ¼ ∑ p-1 n¼0 s x r;n -x 0;0 ; if U LBP p;r ≤2 p þ 1; otherwise (<label>ð3Þ</label></formula><formula xml:id="formula_3">where U LBP p;r ¼ X p-1 n¼0 s x r;n -x 0;0 -s x r; mod n þ 1; p -x 0;0<label>ð4Þ</label></formula><p>where superscript riu2 denotes the rotation invariant "uniform" patterns that have U values at most 2. Therefore, mapping from LBP p, r to LBP p, r riu2 results in only p+ 1 distinct groups of patterns, leading to a much shorter histogram representation for the whole image.</p><p>It is obvious that LBP oversimplifies local structure and loses textural information. Therefore, Ojala et al. <ref type="bibr" target="#b3">[4]</ref> made a further important correction by including the local contrast of each pattern and proposing a complementary local descriptor called VAR p, r . Using the 2D joint histogram of LBP p, r riu2 and VAR p, r , denoted as LBP p, r riu2 /VAR p, r is demonstrated in <ref type="bibr" target="#b3">[4]</ref>.  In conventional LBP the central pixel is discarded (despite the implicit use of the intensity of the central pixel as the threshold to achieve local gray-scale invariance), and only the joint distribution of the neighborhood around each pixel is considered. However, in their recent extensive texture study, Zhang et al. <ref type="bibr" target="#b7">[8]</ref> suggested that it is vital to use a combination of several detectors and descriptors. Motivated by the work of Lazebnik et al. <ref type="bibr" target="#b9">[10]</ref> and Zhang et al. <ref type="bibr" target="#b7">[8]</ref>, in this paper we seek to propose a method which possesses the strengths of combining complementary local features, with those of LBP in computational efficiency and smaller support regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Intensity-based descriptors</head><p>The brightness level at a point in an image is highly dependent on the brightness levels of its neighboring points unless the image is simply random noise <ref type="bibr" target="#b23">[24]</ref>. In MRF modeling <ref type="bibr" target="#b23">[24]</ref>, the probability of the central pixel depends only on its neighborhood as</p><formula xml:id="formula_4">p Ι x c ð Þ ð jΙ x ð Þ; ∀x≠x c Þ ¼ p Ι x c ð Þ ð jΙ x ð Þ; ∀x∈N x c ð ÞÞ<label>ð5Þ</label></formula><p>where x c is a site in the 2D integer lattice on which the image I has been defined and N x c ð Þ is the neighborhood of that site. The center pixel also has discriminant information, however its distribution is conditioned on its neighbors alone.</p><p>Inspired by such MRF models, and related to the ideas explored by Varma and Zisserman <ref type="bibr" target="#b2">[3]</ref>, we propose to use only local neighborhood distributions in our NI-LBP descriptor. We explicitly model the joint distribution of the central pixels and its neighbors, in order to test how significant this conditional probability distribution is for classification.</p><p>Next, inspired by the coding strategy of LBP, we define the following NI-LBP descriptor (see also Fig. <ref type="figure" target="#fig_0">2</ref>):</p><formula xml:id="formula_5">NI-LBP p;r ¼ X p-1 n¼0 s x r;n -μ 2 n ; s x ¼ ð Þ 1; x ≥ 0 0; x b 0<label>ð6Þ</label></formula><formula xml:id="formula_6">where μ ¼ 1 2 ∑ p-1 n¼0 x r;n . Similar to LBP p, r<label>riu2</label></formula><p>, the rotation invariant version of NI -LBP, denoted by NI -LBP p, r riu2 , can also be defined to achieve rotation invariant classification.</p><p>Regarding the selection of the threshold μ, although it was motivated by intuition and experimental studies, it is also selected in order to preserve LBP characteristics and to increase robustness. Hafiane et al. <ref type="bibr" target="#b16">[17]</ref> proposed Median Binary Pattern (MBP) which seeks to derive the localized binary pattern by thresholding the pixels against their median value over a 3 ×3 neighborhood. In MBP, the central pixel is also included in this filtering process, resulting 2 9    neighborhood intersection. Taking the patterns in Fig. <ref type="figure" target="#fig_3">4</ref>(b) and (c) for example, which would be considered as the same pattern type according to LBP and VAR, they are actually two patterns with different textural properties. Moreover, MBP also fails to distinguish patterns (b) and (c). Clearly, our proposed NI-LBP approach can distinguish all the three different patterns, as shown in (a2) (b2) and (c2). Therefore, the proposed NI-LBP approach is more discriminative and effective.</p><formula xml:id="formula_7">MBP 0 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 (a3) (b3) (c3) 0 0 0 NI-LBP 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 (a2) (b2)<label>(c2)</label></formula><p>In order to make further comparisons, we conducted texture classification on test suite Outex_TC_00000, which was used in <ref type="bibr" target="#b16">[17]</ref>. The results are listed in Table <ref type="table" target="#tab_0">1</ref>. For test suite Outex_TC_00000, there are 24 texture classes, with each class having 20 monochrome texture images (128 × 128) with incandescent constant illumination and a spatial resolution 100 dpi. The images in each class are divided into two nonoverlap groups with 10 images as training and the other 10 as testing. Outex_TC_00000 provides 100 couples of test and train files for this category, we have performed experiments on all the 100 couples and we report classification accuracy as the average over the 100 couples. Note that in <ref type="bibr" target="#b16">[17]</ref>, the authors selected randomly one couple in their evaluation, see Table <ref type="table" target="#tab_0">1</ref> in <ref type="bibr" target="#b16">[17]</ref>. From Table <ref type="table" target="#tab_0">1</ref>, we can clearly observe that our NI-LBP performs the best, especially for two texture classes canvas033 and carpet009.</p><p>Furthermore, LBP thresholding at the value of the central pixel x 0, 0 tends to be sensitive to noise, particularly in near-uniform image regions, and smooths weak illumination gradients. While MBP thresholding against the median value is claimed to be robust to "salt and pepper" noise <ref type="bibr" target="#b16">[17]</ref>. However, MBP is not robust to Gaussian noise. In contrast, the proposed NI-LBP descriptor has the following advantages:</p><p>1. Thresholding at μ is equivalent to making the local neighborhood vector zero-mean, therefore resistant to local lighting effects, and specifically invariant to gray scale changes. 2. Compared with LBP, weak edges are preserved by NI-LBP, as illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>. We can clearly observe that LBP does not match the visual patterns, producing output unrelated to the peak in (a) or the edge in (b). In contrast, the proposed NI-LBP outputs more consistent patterns, owing to the better thresholding of μ. 3. Better noise robustness, as shown in Fig. <ref type="figure">5</ref>.</p><p>Recall that the local contrast measure proposed by Ojala et al. <ref type="bibr" target="#b3">[4]</ref> is defined as follows:</p><formula xml:id="formula_8">VAR p;r ¼ 1 2 X p-1 n¼0 x r;n -μ 2 ; where μ ¼ 1 2 X p-1 n¼0 x r;n :<label>ð7Þ</label></formula><p>We can see that NI -LBP p, r and VAR p, r capture similar types of texture information, with slight differences:</p><p>1. VAR p, r achieves rotation invariance by summing up the whole variation in the circular neighborhood, whereas NI -LBP p, r is rotation sensitive, by default; 2. NI -LBP p, r is independent of gray scale, whereas VAR p, r is not; 3. Finally, VAR p, r is continuous-valued and needs to be quantized. The latter quantization step has associated limitations of additional training to determine threshold values, and the difficulty in setting the number of bins. Too few bins will fail to provide enough discriminative information while too many bins would make the feature size too large. Although there are some rules to guide selection <ref type="bibr" target="#b3">[4]</ref>, it is hard to obtain an optimal number of bins in terms of accuracy and feature size. On the basis of the above discussion, we expect that the proposed NI -LBP p, r will be a better choice over VAR p, r .</p><p>To make it consistent with the binary coding strategy, the 1D distribution of the central pixels' intensity is represented by two bins, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CI-LBP</head><formula xml:id="formula_9">¼ s x 0;0 -μ r ; s x ð Þ ¼ 1; x ≥ 0 0; x b 0<label>ð8Þ</label></formula><p>where μ I is the mean of the whole image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Difference-based descriptors</head><p>As a parallel development to the intensity descriptors just developed, we also propose pixel differences in radial and angular directions on a circular grid, different from the traditional pixel differences which are computed in horizontal and vertical directions. More specifically, we propose two different descriptors, Radial Difference Local Binary Pattern and Angular Difference Local Binary Pattern (denoted as RD-LBP and AD-LBP respectively, as illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>). We define the RD-LBP descriptor as follows:</p><formula xml:id="formula_10">RD-LBP p;r;δ ¼ X p-1 n¼0 s Δ Rad δ;n 2 n ; s x ð Þ ¼ 1; x ≥ 0 0; x b 0<label>ð9Þ</label></formula><p>where Δ δ, n Rad = x r, nx r -δ, n is the radial difference computed with given integer radial displacement δ, x r, n and x r -δ, n correspond to the gray values of pairs of pixels of δ equally spaced pixels of the same radial direction.</p><p>Similarly, the AD-LBP descriptor is defined as</p><formula xml:id="formula_11">AD-LBP p;r;δ;ε X p-1 n¼0 s Δ Ang δ;n 2 n ; s x ð Þ ¼ 1; x ≥ ε 0; x b ε<label>ð10Þ</label></formula><p>where Δ δ, n Ang = x r, nx r, mod(n + δ, p) is the angular difference computed with given angular displacement δ(2π/p), where δ is an integer such that 1 ≤ δ ≤ p/2, x r, n and x r, mod(n + δ, p) correspond to the gray values of pairs of pixels of δ equally spaced pixels on a circular radius r, and function mod(x, y) is the modulus x of y. ε is a threshold value,  and is 1% of the pixel value range in our experiments. We experimentally set ε = 0.01. We can see that when δ = p/2, our descriptor AD -LBP p, r, p/2, ε is equivalent to the CS-LBP descriptor proposed by Heikkilä <ref type="bibr" target="#b21">[22]</ref> for local interesting region description.</p><p>As discussed in Section 2.1, limiting attention to the uniform binary patterns has attractive and elegant advantages over using all the binary patterns, specifically that the uniform patterns represent meaningful and fundamental characteristics of the texture, they appear to be the major parts of all binary patterns and are relatively reliable, and they lead to texture image representation of low dimensionality. We wish to see to what extent the proposed descriptors maintain these properties; Fig. <ref type="figure">6</ref> shows the proportions of the uniform patterns for three different descriptors (Basic LBP, RD-LBP and AD-LBP), extracted from texture images of test suite Outex_TC_00010. It can clearly be seen that the proportions of the uniform patterns of AD-LBP are too small to provide a reliable and meaningful description of texture. Consequently, we have decided against including AD-LBP descriptor in further experiments in this paper, and focus instead on RD-LBP.</p><p>Based on the above analysis, in order to produce acceptable dimensionality of histogram features, we merely use the uniform patterns motivated by the work of Ojala et al. <ref type="bibr" target="#b3">[4]</ref>. The uniform patterns produce low dimensionality features so that they can be conveniently used together for pattern classification. There are two ways to combine the NI-LBP and RD-LBP codes: calculating the histograms separately and concatenating, or jointly, calculating a joint two dimensional histogram of the NI-LBP and RD-LBP codes, represented as NI-LBP/RD-LBP. Following the work of Varma and Zisserman <ref type="bibr" target="#b2">[3]</ref> and Guo et al. <ref type="bibr" target="#b20">[21]</ref>, who showed the joint approach to produce better results, we will prefer joint histogramming (shown in Fig. <ref type="figure" target="#fig_0">2</ref>). Following <ref type="bibr" target="#b3">[4]</ref>, we use only joint distributions of operators that have the same (p, r) values, although nothing would prevent us from using joint distributions of operators computed from different neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Multiresolution analysis and classification</head><p>The proposed descriptors described above are extracted from a single resolution with a circularly symmetric neighbor set of p pixels placed on a circle of radius r (as in Fig. <ref type="figure" target="#fig_0">2</ref>). Now clearly by altering (p,r), we can realize operators for any quantization of the angular space and for any spatial resolution. Motivated by the idea of <ref type="bibr" target="#b3">[4]</ref>, we conduct the multiresolution analysis by combining the information provided by multiple descriptors of varying (p, r), as illustrated in Fig. <ref type="figure" target="#fig_5">7</ref>. The histogram feature vector of multiresolution analysis is obtained by concatenating the histograms from a single resolution analysis realized with different (p, r).</p><p>To perform the actual texture classification, there are two crucial components: (i) texture feature extraction, and (ii) the classifier and the associated similarity measure used within the classifier. In this work the focus is on evaluating the discrimination properties of the proposed descriptors, so for classification we wish to make as few assumptions as possible and have chosen a non-parametric technique, since non-parametric classifiers can handle a large number of classes, avoid parameter overfitting, and require no learning/training. Of non-parametric classifiers, the k nearest neighbor (kNN) is one of the most popular and simplest methods, which we adopt with k = 1. The samples are then classified according to their normalized histogram feature vectors h i and h i , using χ 2 distance metric</p><formula xml:id="formula_12">χ 2 h i ; h j ¼ 1 2 ∑ k h i k ð Þ-h j k ð Þ h i 2 hi k ð Þ þ h j k ð Þ<label>ð11Þ</label></formula><p>the same distance metric used in [2, 3, 27, 30].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental evaluation</head><p>In this section, we demonstrate the performance of the proposed method with comprehensive experiments on six texture datasets, summarized in Table <ref type="table" target="#tab_1">2</ref>, which are derived from four popular publicly   available texture databases namely, the Brodatz <ref type="bibr" target="#b30">[31]</ref>, the Outex <ref type="bibr" target="#b31">[32]</ref>,</p><p>the CUReT <ref type="bibr" target="#b2">[3]</ref>, and the KTHTIPS2b <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref> databases. The presentation of the experimental results is divided into two groups with corresponding objectives.</p><p>Experiment #1, presented in Section 3.2, aims at investigating the proposed approach for gray scale and rotation invariant texture classification, comparing our proposed descriptors with the classical LBP and VAR descriptors proposed by Ojala et al. <ref type="bibr" target="#b3">[4]</ref> and with other LBP based approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20]</ref>. This setup utilizes all the same texture test suites and experimental setup as those used by Ojala et al. <ref type="bibr" target="#b3">[4]</ref> (except that additional training angles where tested for Out-ex_TC_00010 and Outex_TC_00012).</p><p>Experiment #2, presented in Section 3.3, examines the classification performance of the proposed approach for two more realistic and challenging texture classification tasks:</p><p>1. Material classification dealing with exemplar identification, where instances are imaged from single images obtained under unknown viewpoint and illumination, using the popular CUReT database <ref type="bibr" target="#b2">[3]</ref>. 2. Material categorization where each material consists of instances imaged from multiple different physical samples under different viewpoints, illuminations and imaging distances, using the material database KTHTIPS2b <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In both cases, comparisons are made with state-of-the-art methods that have reported results on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Methods tested</head><p>LBP and VAR <ref type="bibr" target="#b3">[4]</ref>: three descriptors: joint LBP p, r riu2 /V AR p, r , LBP p, r riu2 and VAR p, r , are used in comparison. We follow the experimental setup in <ref type="bibr" target="#b3">[4]</ref> for these three descriptors, see <ref type="bibr" target="#b3">[4]</ref> for details. VAR needs pretraining.</p><p>DLBP and NGF <ref type="bibr" target="#b19">[20]</ref>: DLBP is an LBP variant extracting the dominant LBP patterns in texture image for classification. It is suggested in <ref type="bibr" target="#b19">[20]</ref> that DLBP in combination with another complementary Gabor based descriptor NGF, which captures global texture information, can yield improved and robust classification results. This method also needs pretraining.</p><p>CLBP <ref type="bibr" target="#b20">[21]</ref>: A local texture patch is represented by its center pixel, the sign and magnitude of the differences of the neighborhoods against the center pixel. CLBP is training free.</p><p>The following three state-of-the-art approaches all need a time consuming universal texton dictionary learning stage: VZ-MR8 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>: Eight filter responses derived from the responses of 38 filters with large spatial support. A complicated anisotropic Gaussian filtering method was used to calculate the MR8 responses, a texton dictionary is learned from the MR8 feature space, and a histogram model is learned for an image by labeling each of the image pixels with the texton that lies closest to it in filter response space. VZ-Joint <ref type="bibr" target="#b2">[3]</ref>: The VZ-Joint is identical to the VZ-MR8 except the local descriptor used, instead of using a dense filter bank descriptor, the raw pixel intensities of an N × N square neighborhood around that point are taken as features. VZ-MRF <ref type="bibr" target="#b2">[3]</ref>: A texture image is represented using a twodimensional histogram: one for the quantized bins of the patch center pixel, the other for the learned textons from the patch with the center pixel excluded. The number of bins for the center pixel in <ref type="bibr" target="#b2">[3]</ref> is as large as 200, and the size of the texton dictionary is 61 × 40 = 2440, resulting in an extremely high dimensionality of 2440 × 200 = 488,000.</p><p>Implementation details: To make the comparisons as meaningful as possible, we keep our experimental settings as in <ref type="bibr" target="#b3">[4]</ref>. The descriptor Table <ref type="table">5</ref> Classification accuracies (%) for the three Outex test suites, where training was done at angle 0 and testing at the remaining 9 angles. The mean accuracy is the average over the three test suites. The results for LBP, VAR, and LBP/VAR are quoted directly from the original paper by Ojala et al. <ref type="bibr" target="#b3">[4]</ref>.</p><p>The bold numbers indicate the highest classification score achieved on each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>The number of misclassified samples for each texture class and rotation angle for NI -LBP 16, 2 riu2 /LBP _ R 16, 2 riu2 /CI on test suites Outex_TC_00010 (italic), Outex_TC_00012 "tl84" (plain)</p><p>and "horizon" (bold). Only texture classes with misclassified samples are shown, and all other texture classes are all correctly classified. This table can be compared with Table <ref type="table">5</ref> from Ojala et al. <ref type="bibr" target="#b3">[4]</ref>, where however only results for Outex_TC_00010 are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Texture 0°5°10°15°30°45°60°75°90°Total</head><p>All abbreviations are summarized in Table <ref type="table" target="#tab_2">3</ref>. In all experiments, each texture sample is normalized to be zero mean and unit standard deviation.</p><formula xml:id="formula_13">canvas001 • • • • • • • • • • • • • 1 • • • • • 1 • • 1 • • • • • 3 • 3 canvas033 • • • • 2 • 1 3 2 1 3 3 • 1 2 1 3 2 • 5 4 3 6 5 3 8 4 9 31 22 62 canvas038 • • 1 • • 1 • • 2 • • 1 • • 2 • • 2 • • 5 • 2 4 1 4 8 1 6 26 33 tile005 • 4 1 • 5 • • 3 1 • 5 • • 2 • • 2 • • 1 • • • • • 1 • • 23 2 25 tile006 • • 3 • 1 3 3 2 2 • 4 2 2 8 2 4 5 3 4 4 2 4 3 5 4 6 19 22 38 79 carpet002 • • • • • • • • • • • • • 1 • • 1 • • • • • 1 • • 1 • • 4 • 4 total 0 4 5 0 8 4<label>4</label></formula><p>Results for the CUReT database are reported over 100 random partitions of training and testing sets. 1NN is used for classification.</p><p>3.2. Experiment #1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Image data and experimental setup</head><p>Contrib_TC_00001: This test suite consists of 16 texture classes from the Brodatz database <ref type="bibr" target="#b30">[31]</ref> (a few shown in Fig. <ref type="figure">8</ref>). This test suite was designed for rotation invariant texture classification note <ref type="bibr" target="#b1">[2]</ref> http://www.ee.oulu.fi/mvg/page/image_data. There are eight samples of size 180 × 180 in each class, out of which the first sample is utilized for training and the other seven as testing. Given ten rotation angles, the classifier is trained with samples artificially rotated to just one angle and tested against samples rotated to the other nine angles. In each experiment, the classifier was trained with 16 images and tested with 1008 (16 × 7 × 9) samples, 63 in each of the 16 texture classes.</p><p>Following <ref type="bibr" target="#b3">[4]</ref>, each training sample is split into 121 disjoint 16× 16 subsamples, whose histograms are then merged into one model histogram. We point out that the seven testing images in each texture class are physically different from the one designated training image.</p><p>Outex_TC_00010: 24 Outex texture classes (shown in Fig. <ref type="figure" target="#fig_6">9</ref>) with each class having 20 samples. It was created by Ojala et al. <ref type="bibr" target="#b3">[4]</ref>, again for rotation invariant texture classification. All textures in this test suite have the same illuminant "inca". The training and testing scheme is the same as that for Contrib_TC_00001 but with nine different rotation angles. All of the 480 (24 × 20) samples rotated by one angle are adopted as the training data, and testing data consists of all 480 samples rotated by the other 8 angles. Hence, there are 480 models for training, and 3840 (480 × 8) for validation.</p><p>Outex_TC_00012: Created by Ojala et al. <ref type="bibr" target="#b3">[4]</ref> for rotation and illumination invariant texture classification. The texture classes are the same as Outex_TC_00010. The classifier was trained with the same training samples as Outex_TC_00010, but tested with all samples captured at all 9 rotation angles under different illuminants "t184" or "horizon". Due Fig. <ref type="figure">10</ref>. Some example texture samples from tile005 (top row) and tile006 (bottom row). We can see that they look fairly similar. to the varying illuminants, some texture samples have a large tactile dimension which induces significant local gray-scale distortions, therefore Outex_TC_00012 is more challenging than Outex_TC_00010.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Experimental results on Contrib_TC_00001</head><p>Ojala et al. <ref type="bibr" target="#b3">[4]</ref> reported a near-perfect classification accuracy of 99.7% for the joint descriptor LBP VAR when using two spatial resolutions (8,1) + (24,3) or three spatial resolutions (8,1) + (16,2) + 24,3. Table <ref type="table" target="#tab_3">4</ref> presents the results for our proposed descriptors, comparing with the state-of-the-art methods <ref type="bibr" target="#b3">[4]</ref>.</p><p>The This is also the case with RD-LBP. Interestingly, the performance of NI-LBP increases with the neighborhood size, while for RD-LBP, the best performance is achieved by RD -LBP 16, 2 riu2 . On average, between the individual descriptors, LBP performs the best and VAR the worst. The center pixel also provides useful discriminative information, since it is apparent in Table <ref type="table" target="#tab_3">4</ref> that combining the center pixel CI-LBP with NI-LBP or RD-LBP can generally improve classification performance. Neglecting the center pixel clearly results the loss in information, similar to how <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b20">[21]</ref> demonstrated the benefits of explicitly including the information of the center pixel in the classifier.</p><p>Fig. <ref type="figure">11</ref>. Comparing the best classification scores of our approach with various state-of-the-art methods on all the three test suites. All the results are as originally reported, except for those of VZ-MR8 and VZ-Joint, which are obtained by us using the exact same experimental setup as Varma and Zisserman did <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. For VZ-MR8 and VZ-Joint, 40 textons per class is used for building the universal texton dictionary. The fusion of NI-LBP and RD-LBP produced perfect classification results at <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b1">2)</ref>. The much improved classification accuracy by combining NI-LBP and RD-LBP, which were on their own no better than LBP, implies that they capture truly complementary texture information.</p><p>It is evident, based on the results, that the performance of the proposed NI-LBP/RD-LBP and NI-LBP/RD-LBP/CI-LBP descriptors are superior to that of LBP/VAR. Note that here we only consider a single resolution for our descriptors. It may be argued that this test suite is too easy for texture classification; more challenges of test suites follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Experimental results on Outex_TC_00010 and Outex_TC_00012</head><p>Table <ref type="table">5</ref> presents results for our proposed descriptors and those proposed in <ref type="bibr" target="#b3">[4]</ref> on test suites Outex_TC_00010 and Outex_TC_00012. The conclusions from these results are similar to those from Table <ref type="table" target="#tab_3">4</ref>, with the following additional observations. First, our proposed descriptor NI -LBP 16, 2 riu2 /RD -LBP 16, 2 riu2 /CI -LBP produces consistently the best classification scores across all three test suites, a considerable improvement over the best reported results by Ojala et al. <ref type="bibr" target="#b3">[4]</ref>, especially for Outex_TC_00012 "tl84" and "horizon". Second, among individual descriptors, although NI-LBP and RD-LBP did not outperform LBP or VAR, their combination significantly outperformed LBP/VAR. We maintain that the NI-LBP/RD-LBP strength stems from their complementarity, in that NI-LBP measures the variation of the neighboring pixels on the same circumference, while RD-LBP captures the edge information between circumferences, analogous to the combination of RIFT and SIFT used by Zhang et al. <ref type="bibr" target="#b7">[8]</ref>.  Incidentally, for all the three test suites nearly all of the misclassified samples in tile006 and tile005 were assigned to each other. Fig. <ref type="figure">10</ref> shows some example textures from class tile005 and tile006, where we can observe the high degree of perceptual similarity.</p><p>Motivated by its excellent classification performance and in order to fully examine the classification performance of descriptor NI-LBP/ RD-LBP/CI-LBP, Table <ref type="table" target="#tab_5">7</ref> shows the results of extensive experiments which we conducted on the three Outex test suites by varying the training angle. We can see that the performance is very robust, especially true with NI -LBP 16, 2 riu2 /RD -LBP 16, 2 riu2 /CI -LBP. We can also observe the better results obtained by multiresolution analysis over single resolution. We acknowledge that the multiresolution analysis  will increase the dimensionality of the histogram feature, however the largest dimensionality of 2200 for three resolutions is not a big problem.</p><p>To conclude Experiment #1, Fig. <ref type="figure">11</ref> compares the best scores achieved by our proposed method and those reported by six other state-of-the-art methods. It is quite clear that our approach consistently outperforms all state-of-the-art methods in gray scale and rotation invariant texture classification.</p><p>It is important to emphasize that although our proposed descriptors are motivated by LBP, in practice we are extracting very different local texture information, whereas DLBP, LBPV and CLBP are all LBP-based approaches. LBP, CLBP, and our proposed approach share the advantage of being training-free and computationally simple, since they are based upon a pre-defined dictionary rather than one derived with reference to the dataset to be analyzed. In contrast, VZ-MR8 and VZ-Joint require a time-consuming universal texton dictionary learning stage by clustering local feature vectors extracted from training samples. From Fig. <ref type="figure">11</ref> we see that our approach has about a 5%-7% improvement over VZ-MR8 and VZ-Joint, most likely in part due to the limited training samples for learning the universal texton dictionary, leading to a drop in accuracy for VZ-MR8 and VZ-Joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experiment #2</head><p>Motivated by the excellent performance for the proposed approach demonstrated in the previous section, here we test the performance of the proposed approach for material classification and categorization, using the CUReT and KTHTIPS2b databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Image data and experimental setup</head><p>CUReT <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">35]</ref>: The original CUReT database <ref type="bibr" target="#b34">[35]</ref> consists of 61 texture classes, shown in Fig. <ref type="figure" target="#fig_8">12</ref>, with each class containing 205 images of a physical texture sample photographed under a (calibrated) range of viewing and lighting angles, but without significant variation in scale or in-plane rotation. CUReT is a challenging test of texture descriptors because of the large intra-class variation including the effects of specularities, interreflections, shadowing, and other surface normal variations due to lighting geometry. Consistent with other CUReT studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, we consider only the 92 images per class which afford the extraction of a 200 × 200 pixel foreground region of texture, the same subset of images as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>KTHTIPS2b <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b26">27]</ref>: It is generally agreed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27]</ref> that the major drawback of the CUReT database is that materials are imaged at a constant scale. The acquisition procedure for KTHTIPS2b has been described in more detail in <ref type="bibr" target="#b35">[36]</ref>, with 3 viewing angles, 4 illuminants, and 9 different scales, producing 432 images per class. Fig. <ref type="figure" target="#fig_10">13</ref> illustrates an example of the 11 materials. Notice in particular the striking differences between samples of the same class. There is almost no intra-class variation due to in-plane rotation for this database.</p><p>For the experiments on KTHTIPS2b, we follow the training and testing scheme used in <ref type="bibr" target="#b26">[27]</ref>. We perform experiments training on one, two, or three samples; testing is always conducted only on unseen samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Experimental results</head><p>Table <ref type="table" target="#tab_7">8</ref> presents the results on CUReT. Our method consistently outperformed LBP/VAR, and the multiresolution analysis of the proposed approach helps to improve classification performance, producing slightly higher classification scores than VZ-MR8 and VZ-Joint. Moreover, it is clear that the degree of improvement of our descriptor NI-LBP/RD-LBP/CI-LBP over that of LBP/VAR is increased given fewer training samples, in accordance with the findings based on the three Outex test suites.</p><p>In order to make the comparison fair, Table <ref type="table" target="#tab_8">9</ref> compares the best classification scores achieved by various state-of-the-art methods on CUReT. We can see that our proposed approach is outperformed by VZ-MR8, VZ-Joint, and VZ-MRF with large neighborhood size and more textons. This is because VZ-MR8 and VZ-Joint are statistical approaches, and the very large number of training samples in this dataset allows those methods to find representative textons; with reduced training data the performance of the VZ methods would decrease. Nevertheless, our method, despite a small spatial support, can compete with VZ-MR8 and VZ-Joint having a much larger spatial support. The lesser performance, in general, of the LBP methods is that there are scale and affine variations in the CUReT database, while LBP-based approaches are proposed for rotation and gray level invariance and have limited capability to address scale and affine invariance.</p><p>We have conducted experiments with larger neighborhood sizes on CUReT, and our descriptor NI/RD/CI at multiresolutions (8,1)+ (16,2) + <ref type="bibr" target="#b23">(24,</ref><ref type="bibr" target="#b4">5)</ref> gives classification scores of 97.29%, 94.48%, 88.96%, 80.70% for 46, 23, 12, and 6 training samples per texture class, respectively, which are slightly better than the results achieved by the CLBP approach (97.39%, 94.19%, 88.72%, and 79.88%, respectively).  <ref type="bibr" target="#b23">(24,</ref><ref type="bibr" target="#b4">5)</ref>. All the results from other methods are quoted directly from the original papers except for those of LBP/VAR, which are obtained by us.</p><p>As shown in Table <ref type="table" target="#tab_0">10</ref> and Fig. <ref type="figure" target="#fig_3">14</ref>, we also compare our method with state-of-the-art methods on the material categorization task of the KTHTIPS2b textures, with all results from other methods quoted directly from <ref type="bibr" target="#b26">[27]</ref>. For this database, our proposed NI-LBP/RD-LBP/CI-LBP descriptor outperforms all compared state-of-the-art methods by a significant margin. We should bear in mind that the classification results of all of the methods are obtained with a 1NN classifier, since we mainly focus our attention on the effectiveness of the descriptors rather than on the capabilities of the classifier. Using a more advanced classifier (SVM or k &gt; 1) might improve performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and future work</head><p>This paper has proposed a novel local texture descriptor, generalizing the well-known LBP approach. Four LBP-like descriptors, two local intensity-based CI-LBP and NI-LBP, and two local difference-based descriptors RD-LBP and AD-LBP, were presented to extract complementary texture information of local spatial patterns. We showed that combining complementary descriptors played an important role in texture discrimination. In addition, we found that information contained in radial differences is more discriminative than those contained in angular difference.</p><p>The advantages of the proposed approach include its computational simplicity, no training (in the feature extraction stage), and a dataindependent universal texton dictionary. Extensive experimental results show that the joint distribution of CI-LBP, NI-LBP and RD-LBP significantly outperform the conventional LBP approach and its various invariants on the Outex test suites. Furthermore, results on the material database KTHTIPS2b demonstrate the best performance of the proposed approach in comparison with several state-of-the-art methods with a nearest neighbor classifier.</p><p>In the future, we plan to explore how to reduce the feature dimension of the multiresolution CI-LBP/NI-LBP/RD-LBP. We also believe that an in-depth investigation of the AD-LBP descriptor would be valuable for local region description, looking at the parallels between AD-LBP and the CS-LBP of <ref type="bibr" target="#b21">[22]</ref> developed for image matching.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The proposed NI-LBP preserves weak edge patterns. Two 5 × 5 example image patches are shown in (a,b). (a1) NI-LBP patterns of (a) at two resolutions with thresholds of 108 and 52.4 respectively. (b1) NI-LBP patterns of (b) at two resolutions with thresholds of 90 and 85.4 respectively. (a2,b2) are the patterns given by LBP.</figDesc><graphic coords="3,130.88,520.31,325.16,184.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>binary patterns. NI-LBP, LBP and MBP differ in the selection of thresholding value. The capability of encoding image configuration and pixelwise relationships might be different as they use different thresholds. For illustration purpose, Fig. 4 gives three different example local texture patterns. The patterns shown in Fig. 4(a) and (b) would be classified into the same class. But the textural surfaces they represent are quite different from each other, which means they probably belong to different classes. While the other three descriptors NI-LBP, MBP and VAR can all tell the difference between (a) and (b). This is why Ojala et al. use the combination of LBP and VAR. However, the joint histogram of LBP and VAR cannot fully solve the problem. The classification might be misled without considering the relationships among</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Three different original texture patterns (a, b, c) and their corresponding LBPs (a1, b1, c1), NI-LBPs (a2, b2, c2), MBPs (a3, b3, c3) and VAR values. All three LBP patterns (a1, b1, c1) are the same. Patterns in (a) and (b) would be considered as the same pattern type by LBP, though corresponding textural surfaces might be quite different from each other. By incorporating LBP with local variance information, patterns in (a) and (b) could be distinguished, while patterns in (b) and (c) would still be considered as the same pattern type because of the same variance. But they are different in configuration, which is not due to the rotation but underlying textural properties. In terms of MBP, MBP can distinguish (a) and (b). However, MBP cannot distinguish (b) and (c). In contrast, all three NI-LBPs are different. Therefore, all three patterns can be distinguished by our proposed NI-LBP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 . 1 riu2.Fig. 6 .</head><label>516</label><figDesc>Fig. 5. Comparison of the robustness to additive Gaussian noise of different signal-to-noise ratios (SNR) for the proposed NI-LBP and the conventional LBP on the Outex textures: (a) NI-LBP 8, 1 vs. LBP 8, 1 ; (b) NI-LBP 8, 1 riu2 vs. LBP 8, 1 riu2. We have used all the original texture images present in the Outex_TC_00010 training set (20 samples of illuminant "inca" and angle 0 in each of the 24 texture classes, totaling 480 images). Training is done with all the 480 noise free images and testing is done with the same images, but added with additive Gaussian noise with different SNR. The nearest neighbor classifier is used for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Proposed multiresolution scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. 128 × 128 samples of the 24 textures from Outex used in Experiment #1.</figDesc><graphic coords="8,78.56,56.21,447.91,345.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>individual descriptor NI-LBP and RD-LBP perform similarly, with NI-LBP doing slightly better. NI -LBP 16, 2 riu2 and NI -LBP 24, 3 riu2 significantly outperformed their simpler counterpart NI -LBP 8, 1 riu2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. One sample of each of the 61 texture classes from CUReT.</figDesc><graphic coords="11,39.91,374.29,506.78,353.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Finally, we can see that NI -LBP 16, 2 riu2 /RD -LBP 16, 2 riu2 and NI -LBP 16, 2 riu2 / RD -LBP 16, 2 riu2 /CI -LBP produce very robust classification performance in all three cases. This is in contrast to the descriptor LBP/VAR, the performance of which decreases considerably in gray scale and rotation invariant texture classification. The excellent classification results demonstrate that NI-LBP/RD-LBP/CI0-LBP is more stable for texture classification irrespective of the different imaging geometries of the illuminants affecting the appearance of local distortions caused by the tactile dimension of textures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>13 .</head><label>13</label><figDesc>The variations within each category of the new KTHTIPS2b database. Each row shows one example image from each of four samples of a category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,70.40,504.17,445.75,221.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,33.73,82.77,519.08,130.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,144.85,53.18,314.36,208.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Comparison of the detail mean classification accuracy for NI-LBP, LBP and MBP on test suite Outex_TC_00000. Results are obtained as the average of the 100 test groups. The patch size used is 3 × 3. The 1NN classifier is used. The distance measure is χ 2 . Each image is normalized to have zero mean and unit standard deviation. "NI-LBP (512)" encodes the binary value of the center pixel, similar to "MBP (512)". "MBP (256)" excludes the binary value of the center pixel. Since LBP uses the value of the center pixel as the threshold, therefore it is unnecessary to include the center pixel in this case. The numbers in the brackets denote the number of bins of the histogram.</figDesc><table><row><cell>Class</cell><cell cols="3">NI-LBP (512) NI-LBP (256) LBP</cell><cell cols="2">MBP (512) MBP (256)</cell></row><row><cell cols="2">canvas001 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas002 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas003 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas005 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas006 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas009 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas011 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas021 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas022 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell>canvas023</cell><cell>99.6%</cell><cell>99.4%</cell><cell>99.6%</cell><cell>99.8%</cell><cell>99.8%</cell></row><row><cell cols="2">canvas025 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas026 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas031 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell cols="2">canvas032 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell>canvas033</cell><cell>95.5%</cell><cell>96.0%</cell><cell>92.0%</cell><cell>94.4%</cell><cell>92.5%</cell></row><row><cell cols="2">canvas035 100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell>canvas038</cell><cell>95.5%</cell><cell>98.2%</cell><cell>100.0%</cell><cell>99.7%</cell><cell>99.6%</cell></row><row><cell>canvas039</cell><cell>99.8%</cell><cell>99.5%</cell><cell>100.0%</cell><cell>99.6%</cell><cell>99.8%</cell></row><row><cell>tile005</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell>tile006</cell><cell>100.0%</cell><cell>99.6%</cell><cell>100.0%</cell><cell>99.8%</cell><cell>99.7%</cell></row><row><cell>carpet002</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell>carpet004</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell>carpet005</cell><cell>100.0</cell><cell>99.6%</cell><cell>100.0%</cell><cell>99.4%</cell><cell>96.5%</cell></row><row><cell>carpet009</cell><cell>99.9%</cell><cell>99.9%</cell><cell>99.3%</cell><cell>95.2%</cell><cell>94.4%</cell></row><row><cell>Mean</cell><cell>99.76%</cell><cell>99.68%</cell><cell>99.62%</cell><cell>99.50%</cell><cell>99.26%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Summary of texture datasets used in our experiments.</figDesc><table><row><cell cols="2">Experiment # 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Texture</cell><cell>Texture</cell><cell>Samples per</cell><cell>Sample size</cell><cell>Test suite</cell><cell>Training or</cell><cell>Number of</cell><cell>Illuminant</cell><cell>Samples in</cell></row><row><cell>dataset</cell><cell>classes</cell><cell>class</cell><cell></cell><cell></cell><cell>testing</cell><cell>angles</cell><cell>used</cell><cell>total</cell></row><row><cell>Brodatz</cell><cell>16</cell><cell>8</cell><cell>180 × 180</cell><cell>Contrib TC 00001 (problem</cell><cell>Training</cell><cell>1</cell><cell>"inca"</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>000-009)</cell><cell>Testing</cell><cell>9</cell><cell>"inca"</cell><cell>1008</cell></row><row><cell>Outex</cell><cell>24</cell><cell>20</cell><cell>127 × 128</cell><cell>Outex TC 00010</cell><cell>Training</cell><cell>1</cell><cell>"inca"</cell><cell>480</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Testing</cell><cell>9</cell><cell>"inca"</cell><cell>3840</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Outex TC 00012 (problem 000)</cell><cell>Training</cell><cell>1</cell><cell>"inca"</cell><cell>480</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Testing</cell><cell>10</cell><cell>"tl84"</cell><cell>4320</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Outex TC 00012 (problem 001)</cell><cell>Training</cell><cell>1</cell><cell>"inca"</cell><cell>480</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Testing</cell><cell>10</cell><cell>"horizon"</cell><cell>4320</cell></row><row><cell cols="2">Experiment # 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Texture</cell><cell>Dataset</cell><cell cols="2">Image rotation Controlled</cell><cell>Scale variation</cell><cell cols="2">Texture classes Sample size</cell><cell>Samples per</cell><cell>Samples in</cell></row><row><cell>dataset</cell><cell>notation</cell><cell></cell><cell>illumination</cell><cell></cell><cell></cell><cell></cell><cell>class</cell><cell>total</cell></row><row><cell>CUReT</cell><cell>D c</cell><cell>√</cell><cell>√</cell><cell></cell><cell>61</cell><cell>200 × 200</cell><cell>92</cell><cell>5612</cell></row><row><cell>KTH-TIPS2b</cell><cell>D KT2b</cell><cell></cell><cell>√</cell><cell>√</cell><cell>11</cell><cell>200 × 200</cell><cell>432</cell><cell>4752</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Abbreviations for the notations of methods.</figDesc><table><row><cell>Name of Proposed method</cell><cell>Abbreviation</cell></row><row><cell>LBP p, r riu2</cell><cell>LBP</cell></row><row><cell>VAR p, r</cell><cell>VAR</cell></row><row><cell>LBP p, r riu2 /VAR p, r</cell><cell>LBP/VAR</cell></row><row><cell>CI-LBP</cell><cell>CI</cell></row><row><cell>NI -LBP p, r riu2</cell><cell>NI</cell></row><row><cell>RD -LBP p, r riu2</cell><cell>RD</cell></row><row><cell>RD -LBP p, r riu2 /CI -LBP</cell><cell>RD/CI</cell></row><row><cell>NI -LBP p, r riu2 /CI -LBP</cell><cell>NI/CI</cell></row><row><cell>NI -LBP p, r riu2 /RD -LBP p, r riu2</cell><cell>NI/RD</cell></row><row><cell>NI -LBP p, r riu2 /RD -LBP p, r riu2 /CI -LBP</cell><cell>NI/RD/CI</cell></row></table><note><p>Fig. 8. 128 × 128 samples of the textures from Brodatz used in Experiment #1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Classification</figDesc><table><row><cell>2  *  Method</cell><cell>2  *  (p, r)</cell><cell>2  *  Bins</cell><cell>Rotation angle for training</cell></row></table><note><p><p><p>accuracies (%) on Contrib_TC_00001, where training is done at just one rotation angle and the average accuracy over 10 angles. The results for LBP, VAR, and LBP/VAR are quoted directly from the original paper by Ojala et al.</p><ref type="bibr" target="#b3">[4]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>Classification accuracies (%) of descriptor NI/RD/CI for Outex_TC_00010 and Outex_TC_00012: training is done at just one rotation angle, and the average accuracy over 9 angles. The bold numbers indicate the highest classification score achieved on each dataset.</figDesc><table><row><cell>Test suite</cell><cell>(p, r)</cell><cell cols="3">Rotation Angle for Train ("inca")</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="10">0°5°10°15°30°45°60°75°90°Average</cell></row><row><cell>7  *  [c] Outex_</cell><cell>(8, 1)</cell><cell>90.9</cell><cell>91.6</cell><cell>92.1</cell><cell>93.0</cell><cell>91.3</cell><cell>90.8</cell><cell>88.9</cell><cell>89.0</cell><cell>84.3</cell><cell>90.2</cell></row><row><cell>TC_00012</cell><cell>(16, 2)</cell><cell>98.0</cell><cell>98.3</cell><cell>99.1</cell><cell>98.6</cell><cell>98.4</cell><cell>98.6</cell><cell>98.6</cell><cell>97.7</cell><cell>96.8</cell><cell>98.3</cell></row><row><cell>("tl84")</cell><cell>(24, 3)</cell><cell>97.3</cell><cell>98.3</cell><cell>98.5</cell><cell>98.7</cell><cell>97.2</cell><cell>96.4</cell><cell>93.4</cell><cell>94.2</cell><cell>94.1</cell><cell>96.5</cell></row><row><cell></cell><cell>(8, 1) + (16, 2)</cell><cell>97.4</cell><cell>98.0</cell><cell>98.4</cell><cell>98.5</cell><cell>98.3</cell><cell>98.3</cell><cell>97.8</cell><cell>97.1</cell><cell>95.6</cell><cell>97.7</cell></row><row><cell></cell><cell>(8, 1) + (24, 3)</cell><cell>97.7</cell><cell>93.3</cell><cell>98.7</cell><cell>98.7</cell><cell>98.5</cell><cell>97.9</cell><cell>96.4</cell><cell>96.6</cell><cell>96.4</cell><cell>97.7</cell></row><row><cell></cell><cell>(16, 2) + (24, 3)</cell><cell>98.3</cell><cell>99.0</cell><cell>99.3</cell><cell>99.2</cell><cell>98.9</cell><cell>98.9</cell><cell>98.3</cell><cell>98.1</cell><cell>98.1</cell><cell>98.7</cell></row><row><cell></cell><cell>(8, 1) + (16, 2) + (24, 3)</cell><cell>98.5</cell><cell>98.9</cell><cell>99.1</cell><cell>99.1</cell><cell>99.0</cell><cell>98.9</cell><cell>98.4</cell><cell>98.2</cell><cell>98.1</cell><cell>98.7</cell></row><row><cell>7  *  [c] Outex_</cell><cell>(8, 1)</cell><cell>92.7</cell><cell>92.8</cell><cell>93.3</cell><cell>93.6</cell><cell>92.7</cell><cell>91.6</cell><cell>90.3</cell><cell>91.1</cell><cell>86.6</cell><cell>91.6</cell></row><row><cell>TC_00012</cell><cell>(16, 2)</cell><cell>98.0</cell><cell>98.0</cell><cell>98.3</cell><cell>98.4</cell><cell>97.7</cell><cell>97.9</cell><cell>98.2</cell><cell>98.3</cell><cell>98.1</cell><cell>98.1</cell></row><row><cell>("horizon")</cell><cell>(24, 3)</cell><cell>96.2</cell><cell>97.0</cell><cell>97.0</cell><cell>97.3</cell><cell>95.5</cell><cell>95.1</cell><cell>92.7</cell><cell>93.7</cell><cell>94.1</cell><cell>95.4</cell></row><row><cell></cell><cell>(8, 1) + (16, 2)</cell><cell>98.2</cell><cell>97.8</cell><cell>98.3</cell><cell>97.9</cell><cell>97.1</cell><cell>97.8</cell><cell>98.2</cell><cell>97.8</cell><cell>97.0</cell><cell>97.8</cell></row><row><cell></cell><cell>(8, 1) + (24, 3)</cell><cell>97.8</cell><cell>97.5</cell><cell>97.7</cell><cell>97.7</cell><cell>96.2</cell><cell>96.1</cell><cell>95.1</cell><cell>95.2</cell><cell>95.1</cell><cell>96.3</cell></row><row><cell></cell><cell>(16, 2) + (24, 3)</cell><cell>97.8</cell><cell>98.3</cell><cell>98.2</cell><cell>98.3</cell><cell>97.3</cell><cell>97.5</cell><cell>96.9</cell><cell>97.0</cell><cell>97.7</cell><cell>97.7</cell></row><row><cell></cell><cell>(8, 1) + (16, 2) + (24, 3)</cell><cell>97.8</cell><cell>98.4</cell><cell>98.4</cell><cell>98.2</cell><cell>97.4</cell><cell>97.7</cell><cell>97.5</cell><cell>97.1</cell><cell>97.6</cell><cell>97.8</cell></row><row><cell>7  *  [c] Outex_</cell><cell>(8, 1)</cell><cell>96.5</cell><cell>96.3</cell><cell>97.4</cell><cell>97.6</cell><cell>96.2</cell><cell>95.3</cell><cell>92.7</cell><cell>94.9</cell><cell>91.8</cell><cell>95.4</cell></row><row><cell>TC_00010</cell><cell>(16, 2)</cell><cell>99.3</cell><cell>99.4</cell><cell>99.5</cell><cell>99.7</cell><cell>99.6</cell><cell>99.6</cell><cell>99.5</cell><cell>99.0</cell><cell>99.0</cell><cell>99.4</cell></row><row><cell>("inca")</cell><cell>(24, 3)</cell><cell>99.2</cell><cell>99.5</cell><cell>99.4</cell><cell>99.5</cell><cell>99.5</cell><cell>99.5</cell><cell>99.2</cell><cell>99.3</cell><cell>99.1</cell><cell>99.4</cell></row><row><cell></cell><cell>(8, 1) + (16, 2)</cell><cell>99.4</cell><cell>99.4</cell><cell>99.6</cell><cell>99.6</cell><cell>99.5</cell><cell>99.4</cell><cell>99.4</cell><cell>99.0</cell><cell>98.6</cell><cell>99.3</cell></row><row><cell></cell><cell>(8, 1) + (24, 3)</cell><cell>99.3</cell><cell>99.5</cell><cell>99.5</cell><cell>99.5</cell><cell>99.6</cell><cell>99.6</cell><cell>99.7</cell><cell>99.4</cell><cell>99.2</cell><cell>99.5</cell></row><row><cell></cell><cell>(16, 2) + (24, 3)</cell><cell>99.6</cell><cell>99.7</cell><cell>99.8</cell><cell>99.7</cell><cell>99.7</cell><cell>99.9</cell><cell>99.8</cell><cell>99.7</cell><cell>99.5</cell><cell>99.7</cell></row><row><cell></cell><cell>(8, 1) + (16, 2) + (24, 3)</cell><cell>99.7</cell><cell>99.7</cell><cell>99.7</cell><cell>99.6</cell><cell>99.6</cell><cell>99.8</cell><cell>99.9</cell><cell>99.7</cell><cell>99.4</cell><cell>99.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>shows the number of misclassified samples for each texture and rotation angle for the best descriptor NI -LBP 16, 2 riu2 /LBP _ R 16, 2 riu2 /</figDesc><table><row><cell>CI -LBP for all three cases, allowing a detailed analysis of discrimina-</cell></row><row><cell>tion of individual textures and the effect of rotation. Overall, NI -</cell></row><row><cell>LBP 16, 2 riu2 /RD -LBP 16, 2 riu2 /CI -LBP classified 18 out of the 24 classes com-</cell></row><row><cell>pletely correctly, having most difficulties with tile006, followed by</cell></row><row><cell>canvas033.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>Comparing classification accuracy (%) on CUReT: N tr is the number of training samples per class used. All results are obtained by us except for VZ-Joint, which are quoted from the recent comparative study of Zhang et al.<ref type="bibr" target="#b7">[8]</ref>. For VZ-MR8, we learn 10 textons per class.</figDesc><table><row><cell>(p, r)</cell><cell>(8, 1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(16, 2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(24, 3)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>N tr</cell><cell>46</cell><cell>23</cell><cell>12</cell><cell>6</cell><cell>2</cell><cell>46</cell><cell>23</cell><cell>12</cell><cell>6</cell><cell>2</cell><cell>46</cell><cell>23</cell><cell>12</cell><cell>6</cell><cell>2</cell></row><row><cell cols="2">LBP/VAR 93.76</cell><cell>88.71</cell><cell>81.80</cell><cell>71.08</cell><cell>50.43</cell><cell>4.00</cell><cell>89.76</cell><cell>81.53</cell><cell>71.09</cell><cell>52.77</cell><cell>91.90</cell><cell>85.34</cell><cell>77.12</cell><cell>66.04</cell><cell>48.64</cell></row><row><cell cols="2">NI/RD/CI 95.15</cell><cell>92.00</cell><cell>86.19</cell><cell>77.97</cell><cell>57.96</cell><cell>9563</cell><cell>92.7</cell><cell>87.12</cell><cell>79.57</cell><cell>60.89</cell><cell>92.59</cell><cell>87.85</cell><cell>80.92</cell><cell>70.33</cell><cell>52.21</cell></row><row><cell>(p, r)</cell><cell>(8, 1) + (16, 2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(8, 1) + (24, 3)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(8, 1) + (16, 2) + (24, 3)</cell><cell></cell><cell></cell></row><row><cell>N tr</cell><cell>46</cell><cell>23</cell><cell>12</cell><cell>6</cell><cell>2</cell><cell>46</cell><cell>23</cell><cell>12</cell><cell>6</cell><cell>2</cell><cell>46</cell><cell>23</cell><cell>12</cell><cell>6</cell><cell>2</cell></row><row><cell>NI/RD</cell><cell>94.78</cell><cell>91.17</cell><cell>85.73</cell><cell>76.67</cell><cell>57.28</cell><cell>95.79</cell><cell>91.17</cell><cell>86.26</cell><cell>77.25</cell><cell>55.68</cell><cell>95.75</cell><cell>91.88</cell><cell>85.41</cell><cell>75.71</cell><cell>57.80</cell></row><row><cell cols="2">NI/RD/CI 96.88</cell><cell>93.55</cell><cell>89.29</cell><cell>80.18</cell><cell>61.28</cell><cell>96.66</cell><cell>93.57</cell><cell>88.41</cell><cell>79.90</cell><cell>60.52</cell><cell>96.78</cell><cell>93.45</cell><cell>88.94</cell><cell>79.69</cell><cell>62.14</cell></row><row><cell>Method</cell><cell>Neighborhood size</cell><cell>46</cell><cell>23</cell><cell>12</cell><cell>6</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VZ-MR8</cell><cell>19 × 19</cell><cell>96.37</cell><cell>92.34</cell><cell>86.96</cell><cell>77.17</cell><cell>54.88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VZ-Joint</cell><cell>7 × 7</cell><cell>96.19</cell><cell>92.00</cell><cell>86.56</cell><cell>76.87</cell><cell>54.69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9</head><label>9</label><figDesc>Comparison of highest classification performance on CUReT with state-of-the-art results using NNC classifier. Number of training and testing samples per class is equal, i.e. 46. Our score 97.29% is obtained with NI/RD/CI at multiresolutions (8, 1) + (16, 2) +</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>L. Liu et al. / Image and Vision Computing 30 (2012) 86-99</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ours LBP/ VAR VZ-MR8 <ref type="bibr" target="#b1">[2]</ref> VZ-Joint <ref type="bibr" target="#b2">[3]</ref> VZ-MRF <ref type="bibr" target="#b2">[3]</ref> CLBP <ref type="bibr" target="#b20">[21]</ref> Neighborhood size </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Texture analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tuceryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook Pattern Recognition and Computer Vision</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Pau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S P</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="235" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A statistical approach to texture classification from single images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="61" to="81" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A statistical approach to material classification using image patches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2032" to="2047" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monogenic-LBP: a new approach for rotation invariant texture classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2677" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Texture classification via patch-based sparse texton learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2737" to="2740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local features and kernels for classification of texture and object categories: a comprehensive study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="238" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using basic image features for texture classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Crosier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="460" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A sparse texture representation using local affine regions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1265" to="1278" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Texture features for browsing and retrieval of image data</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Filtering for texture classification: a comparative study</title>
		<author>
			<persName><forename type="first">T</forename><surname>Randen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Husøy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="310" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on feature distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face authentication using adapted local binary pattern histograms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="321" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Median binary pattern for textures classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hafiane1</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zavidovique</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of ICIAR</title>
		<imprint>
			<biblScope unit="page" from="387" to="398" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A texture-based method for detecting moving objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heikkilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">View-based recognition of real-world textures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nurmela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="323" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dominant local binary patterns for texture classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W K</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1107" to="1118" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A completed modeling of local binary pattern operator for texture classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1657" to="1663" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Description of interest regions with local binary patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heikkilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikänen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="436" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">WLD: A robust local image descriptor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1705" to="1720" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Markov random field texture models</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="39" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Texture discrimination with multidimensional distributions of signed gray-level differences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Valkealahti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="727" to="739" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized local binary patterns for texture classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC2011)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Class-specific material categorization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mallikarjuna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<title level="m">Soft histograms for local binary patterns, Finnish Signal Processing Symposium</title>
		<meeting><address><addrLine>Oulu, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enhanced local texture feature sets for face recognition under difficult lighting conditions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1635" to="1650" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Classifying materials in the real world</title>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="163" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Texture: A Photographic Album for Artists and Designers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brodatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>Dover</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Outex-new frame work for empirical evaluation of texture analysis algorithm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Viertola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kyllönen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huovinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="701" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rotation invariant texture classification using LBP variance (LBPV) with global matching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="706" to="719" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Local binary patterns variants as texture descriptors for medical image analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lumini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brahnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="125" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reflectance and texture of real-world surfaces</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Mallikarjuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Targhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-O</forename><surname>Eklundh</surname></persName>
		</author>
		<ptr target="http://www.nada.kth.se/cvap/databases/kth-tips/2006" />
		<title level="m">The KTH-TIPS and KTH-TIPS2 Databases</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
