<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Robust Deep Model for Improved Classification of AD/MCI Patients</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Old Dominion University</orgName>
								<address>
									<postCode>23529</postCode>
									<settlement>Norfolk</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Loc</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Old Dominion University</orgName>
								<address>
									<postCode>23529</postCode>
									<settlement>Norfolk</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kim-Han</forename><surname>Thung</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Old Dominion University</orgName>
								<address>
									<postCode>23529</postCode>
									<settlement>Norfolk</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<region>NC</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Brain and Cognitive Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Old Dominion University</orgName>
								<address>
									<postCode>23529</postCode>
									<settlement>Norfolk</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Robust Deep Model for Improved Classification of AD/MCI Patients</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">22BB94FC159453DDEA6586038B256325</idno>
					<idno type="DOI">10.1109/JBHI.2015.2429556</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Alzheimer&apos;s Disease</term>
					<term>Early Diagnosis</term>
					<term>MRI</term>
					<term>PET</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate classification of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), plays a critical role in possibly preventing progression of memory impairment and improving quality of life for AD patients. Among many research tasks, it is of particular interest to identify noninvasive imaging biomarkers for AD diagnosis. In this paper, we present a robust deep learning system to identify different progression stages of AD patients based on MRI and PET scans. We utilized the dropout technique to improve classical deep learning by preventing its weight co-adaptation, which is a typical cause of over-fitting in deep learning. In addition, we incorporated stability selection, an adaptive learning factor, and a multi-task learning strategy into the deep learning framework. We applied the proposed method to the ADNI data set and conducted experiments for AD and MCI conversion diagnosis. Experimental results showed that the dropout technique is very effective in AD diagnosis, improving the classification accuracies by 5.9% on average as compared to the classical deep learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A LZHEIMER'S disease is the sixth-leading cause of death in the United States <ref type="bibr" target="#b0">[1]</ref>. AD patients usually undergo progressive stages of cognitive and memory function impairment, including prodromal, MCI and AD. For each of these stages, significant amount of research has been conducted aiming to understanding the underlying pathological mechanisms. In addition, imaging biomarkers have been identified using different imaging modalities such as magnetic resonance imaging (MRI) <ref type="bibr" target="#b1">[2]</ref>, positron emission tomography (PET) <ref type="bibr" target="#b2">[3]</ref>, and functional MRI (fMRI) <ref type="bibr" target="#b3">[4]</ref>. Imaging biomarkers are a set of indicators computed from image modalities and can be used for early detection of AD disease. It has been shown that fusing these different modalities may lead to more effective imaging biomarkers <ref type="bibr" target="#b5">[6]</ref>.</p><p>The first successful deep learning framework, auto-encoder, was developed in 2006 <ref type="bibr" target="#b6">[7]</ref>. It was subsequently used in other application fields and achieved state-of-the-art performance in speech recognition, image classification and computer vision <ref type="bibr" target="#b7">[8]</ref>. Deep learning itself also evolves after 2006. For instance, the multimodal deep learning framework boosted speech classification by learning a shared representation between video and audio modalities <ref type="bibr" target="#b8">[9]</ref>. A dropout technique further improved zip code recognition, document classification, and image recognition <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this paper, we developed a robust deep learning framework for AD diagnosis by fusing complementary information from MRI and PET scans. These 3D scans were preprocessed and their features were further extracted. Specifically, we first Manuscript received December 31, 2014; revised March 30, 2015. This work was partially supported by NIH grants (EB006733, EB008374, EB009634, MH100217, AG041721, AG042599). Corresponding authors: Jiang Li (JLi@odu.edu) and Dinggang Shen (dgshen@med.unc.edu).</p><p>applied principal component analysis (PCA) to obtain PCs as new features. We then utilized the stability selection technique <ref type="bibr" target="#b12">[13]</ref> together with the least absolute shrinkage and selection operator (Lasso) method <ref type="bibr" target="#b13">[14]</ref> to select the most effective features. The selected features were subsequently processed by the deep learning structure. Model weights in the deep structure were first initialized by unsupervised training and then fine-tuned by AD patient labels. During the fine-tuning phase, the dropout technique was employed to improve the model's generalization capability. Finally, the learned feature representation was used for AD/MCI classification by a support vector machine (SVM).</p><p>In addition to discrete patient labels (AD, MCI or Healthy), there are two additional clinical scores, namely Minimum Mental State Examination (MMSE) and Alzheimer's Disease Assessment Scale-Cognitive subscale (ADAS-Cog) associated with each patient. MMSE is a 30-point questionnaire widely used to measure cognitive impairment <ref type="bibr" target="#b14">[15]</ref>. It is used to estimate the severity and progression of cognitive impairment, instead of providing any AD information. ADAS-Cog is the most popular cognitive testing instrument to measure the severity of the most important symptoms of AD, including the disturbances of memory, language, praxis, attention and other cognitive abilities, which have been referred as the core symptoms of AD <ref type="bibr" target="#b15">[16]</ref>. The information from these scores is related and identifying the commonality among them may help AD diagnosis. We configured the deep learning structure as a multi-task learning (MTL) framework, and treated the learning of class label, MMSE and ADAS-Cog as related tasks to improve the prediction of main task (class label).</p><p>We evaluated the proposed method on the ADNI 1 data set and compared it with a baseline method and a similar deep learning system, where the auto-encoder was used as a feature extractor for AD diagnosis <ref type="bibr" target="#b5">[6]</ref>. The baseline method contains feature selection and SVM steps but does not use deep learning. We also evaluated the impact on performance of each of the components in the proposed system. A brief version of this paper was published in <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MATERIALS AND METHODS</head><p>The proposed system consists of multiple components, including PCA, stability selection, unsupervised feature learning, multi-task deep learning and SVM training, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We detail each of these components in the following subsections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data preprocessing</head><p>We utilized the public ADNI data set to validate our proposed deep learning framework. The data set consists of MRI, PET, and CSF data from 51 AD patients, 99 MCI patients (43 MCI patients who converted to AD (MCI.C), and 56 MCI patients who did not progress to AD in 18 months (MCI.NC)) as well as 52 healthy normal controls. In addition to the crisp diagnostic result (AD or MCI), this data set contains two additional clinical scores, MMSE and ADAS-Cog, for each patient. A typical procedure of image processing was applied to the 3D MRI and PET images <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> including anterior commissure-posterior commissure correction, skull-stripping, cerebellum removal, and spatially normalization. Finally, we extracted 93 region-of-interest (ROI) based volumetric features from MRI and PET images, respectively, which together with three CSF biomarkers, i.e., Aβ 42 , t-tau, and p-tau, sum up to 189 features for each subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Principal component analysis</head><p>Principal component analysis (PCA) is a linear orthogonal transformation that converts a set of features into linearly uncorrelated variables in which each of the new variable is a linear combination of all original features <ref type="bibr" target="#b4">[5]</ref>. The first principal component (PC) is defined as the one that can explain the largest variance in the original data set. The second PC has the second largest variance under the constraint that it is orthogonal to the first component. If correlations exist among features, the number of PCs that can be found is usually less than the number of features in the original data. PCA is optimal for preserving energy and it is often used for dimensionality reduction by just keeping the first few PCs.</p><p>Let F denote a feature data set with a size of n×p, where n is the number of data samples and p is the number of features in the data, and each column in F is centered. PCA can be achieved by performing the singular value decomposition (SVD) on F as</p><formula xml:id="formula_0">F = UΣV T , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where U is an n × n matrix with orthogonal unit columns (left singular vectors of F), Σ is an n × p diagonal matrix consisting of singular values of F from the largest to least, and V is an p × p matrix whose columns are orthogonal unit vectors (right singular vectors of F).</p><p>To achieve dimensionality reduction, the first l columns in V corresponding to the first l largest singular values of F can be used as a transformation matrix to be applied on F,</p><formula xml:id="formula_2">x = FV l ,<label>(2)</label></formula><p>where V l consists of the first l columns of V.</p><p>Geometrically, PCA analysis rotates data to align its maximum variance direction of the data with the coordinate system as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. PCA is an effective tool for dimensionality reduction but the preserved PCs may not be useful for classification. The two dimensional artificial data set in Fig. <ref type="figure" target="#fig_1">2</ref> consists of 'blue' and 'red' classes. After PCA, the whole data set was rotated and its main axis was aligned with the coordinate system. However, even though PC 1 has the largest variance, it does not contain any discriminating information for the two classes. For the purpose of classification, PC 2 is preferred and a feature selection step is necessary. This example shows that feature selection may be applied after PCA to retain discriminating information for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stability selection</head><p>In this paper, we first applied PCA to the 189 features and used the resulting PCs as new features. We then applied Lasso <ref type="bibr" target="#b13">[14]</ref> to identify the most effective features for AD diagnosis. Lasso tries to minimize the following cost function for feature selection:</p><formula xml:id="formula_3">min s ||t -xs|| 2 2 + λ||s|| 1 ,<label>(3)</label></formula><p>where t ∈ {+1, -1} n is a class label vector of size n × 1 associated with the feature matrix x of size n × l, where l is the number of features (PCs) found in PCA, s = [s 1 , s 2 ...s l ] T is the weight vector associated with the l features (columns in x), λ is a regularization parameter, and || • || 2 and || • || 1 denote L 2 and L 1 norms, respectively. Because of the L 1 norm constraint on the weight magnitude, the solution minimizing the above cost function is usually sparse, meaning that if a feature is not correlated with the target class label, the feature will have a zero value for its weight. Features having nonzero weights will be selected and otherwise will be excluded.</p><p>It is well known that the solution of L 1 norm based optimizations are sensitive to the choice of λ, and it is difficult to determine how many features should be kept in the model. A recent breakthrough sheds a light on selecting the right amount of regularization for stability selection <ref type="bibr" target="#b12">[13]</ref>. The idea is to repeat the feature selection procedure multiple times based on bootstrapped data sets and compute the probability of the features to be selected. The final selected features are those having probabilities above a predefined threshold t h . It has been shown experimentally and theoretically that the feature selection results vary little for sensible choices in a range of the cut-off value for t h <ref type="bibr" target="#b12">[13]</ref>. We incorporated the stability selection concept into the AD patient diagnosis in this paper. In particular, we repeated the Lasso procedure 50 times and each time with a different value for the parameter λ (We used the SLEP toolbox for Lasso 2 ). A probability, p i , for the ith feature was computed by counting the frequency of the feature being selected in the 50 experiments. The ith feature was selected if p i is larger than a pre-defined threshold t h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-task deep learning with dropout</head><p>In contrast to traditionary three-layer neural network (shallow structure), deep learning is based on a deep architecture consisting of many layers of hidden neurons for modeling. A shallow architecture would involve many duplications of effort to express things and such a fat architecture has been shown to suffer from the problem of over-fitting, which leads to a poor generalization capability. Instead, deep architecture could more gracefully reuse previous computations and discover complicated relations of input <ref type="bibr" target="#b19">[20]</ref>.</p><p>To train a deep architecture, the standard Backpropagation (BP) algorithm did not work well with randomly initialized weights because the error feedback becomes progressively noisier as it goes back to lower levels (closer to inputs), making the low-level weight updates less effective. Even though experiments have shown that if top layers have enough units, the deep structure can still bring down training errors small enough, it cannot generalize well to new data <ref type="bibr" target="#b20">[21]</ref>. This is because the top layers can be effectively trained by gradient based algorithms but low-levels cannot. The randomly initialized low-level layers behave like random feature detectors so good representations for original data were not achieved leading to degraded generalization capability <ref type="bibr" target="#b20">[21]</ref>. In 2006, 2 Available at http://www.public.asu.edu/ jye02/Software/SLEP/index.htm a breakthrough in deep learning has made deep architecture training possible by utilizing the restricted Boltzmann machine (RBM) to initialize multiple hidden layers one layer at a time in an unsupervised manner <ref type="bibr" target="#b6">[7]</ref>. With the unsupervised learning, deep learning tries to understand data first, i.e., to obtain a task specific representation from data so that a better classification can be achieved. It has experimentally proven that the unsupervised learning step plays a critical role in the success of deep learning <ref type="bibr" target="#b7">[8]</ref>. The proposed deep model shown in Fig. <ref type="figure" target="#fig_2">3</ref> consists of several components that will be described bellow.</p><p>1) Pre-training with RBM Each layer in the proposed deep model is an RBM and the deep model used in this paper consists of a stack of RBMs. RBM is an energy-based model in which a scalar energy is associated with each configuration of the variables in the model, and a probability distribution function (PDF) through the energy function is defined. The purpose of learning is to modify the energy function so that a desirable PDF can be achieved, i.e., to have low energy. A basic RBM model having a visible (input) layer and a hidden (output) layer is shown in Fig. <ref type="figure">4</ref>. The visible layer of the bottom RBM contains real-valued units (receiving data) and all other RBM layers have binary units. Let v ∈ R M represent input data (visible units) and h ∈ 0, 1 N denote binary hidden units for the bottom RBM, we used Gaussian-Bernoulli RBMs to train it <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. All other RBMs were trained by utilizing Bernoulli-Bernoulli distribution. Variables v and h have a joint probability distribution defined as Fig. <ref type="figure">4</ref>. A basic RBM model. where E(v, h) is an energy function and Z is a normalization constant. For real-valued visible layer RBMs, E(v, h) is defined as</p><formula xml:id="formula_4">p(v, h) = 1 Z exp -E(v,h) ,<label>(4)</label></formula><formula xml:id="formula_5">E(v, h) = 1 2σ 2 i v 2 i - 1 σ 2 ( i c i v i + j b j h j + i,j v i w ij h j ),<label>(5)</label></formula><p>where c i and b j are biases of the ith and jth units in the visible and hidden layers, respectively. w ij is the weight connecting v i and h j , and σ 2 is the variance of v. The conditional probability distributions are</p><formula xml:id="formula_6">P (h j = 1|v) = sigmoid( 1 σ 2 ( i w ij v i + b j )),<label>(6)</label></formula><formula xml:id="formula_7">P (v i |h) = N ( j w ij h j + c i , σ 2 ).<label>(7)</label></formula><p>If both visible and hidden layers are binary, the energy function and conditional probability distributions are defined as</p><formula xml:id="formula_8">E(v, h) = -( i c i v i + j b j h j + ij v i w ij h j ),<label>(8)</label></formula><formula xml:id="formula_9">P (h j = 1|v) = sigmoid( i w ij v i + b j ),<label>(9)</label></formula><formula xml:id="formula_10">P (v i = 1|h) = sigmoid( j w ij h j + c i ).<label>(10)</label></formula><p>Model parameters w, b and c are updated using contrastive divergence <ref type="bibr" target="#b22">[23]</ref>. For RBM having a real-valued visible layer, the formulas for updating those parameters during each iteration are</p><formula xml:id="formula_11">∆W t+1 ij = η∆W t ij -(&lt; 1 2 i v i h j &gt; d -&lt; 1 2 i v i h j &gt; m ), (<label>11</label></formula><formula xml:id="formula_12">)</formula><formula xml:id="formula_13">∆b t+1 i = η∆b t i -(&lt; 1 2 i v i &gt; d -&lt; 1 2 i v i &gt; m ),<label>(12)</label></formula><formula xml:id="formula_14">∆c t+1 j = η∆c t j -(&lt; h j &gt; d -&lt; h j &gt; m ). (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where &lt; • &gt; d and &lt; • &gt; m denote the expectation computed over data and model distributions accordingly, t is iteration index, η is momentum and is learning rate. For binary RBM, equations ( <ref type="formula" target="#formula_11">11</ref>) and ( <ref type="formula" target="#formula_13">12</ref>) become</p><formula xml:id="formula_16">∆W t+1 ij = η∆W t ij -(&lt; v i h j &gt; d -&lt; v i h j &gt; m ), (<label>14</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">∆b t+1 i = η∆b t i -(&lt; v i &gt; d -&lt; v i &gt; m ). (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>Note that the pre-training of RBM is unsupervised, i.e., class label (classification task) or desired output (regression) is not needed in the training. After the pre-training, we attached the class label on top of the stacked RBMs and utilized an adaptive backpropagation algorithm to fine-tune the weights in the model. All binary layers were also converted to realvalued units by using their continuous activities. Thus the deep learning model turned to be a traditional multilayer perceptron (MLP) but its weights were initialized by RBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Multi-task learning</head><p>In multi-task learning, related tasks are learned simultaneously by extracting and utilizing appropriate shared information across tasks to improve performance. It has received attention in broad areas recently such as machine learning, data mining, computer vision, and bioinformatics <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. This approach is particularly effective when only limited training data for each task is available. It is worth noting that neural networks can simultaneously model multiple outputs, making deep learning a natural multi-task learning framework if multiple tasks share inputs <ref type="bibr" target="#b6">[7]</ref>. The proposed multi-task deep learning framework is shown in Fig. <ref type="figure" target="#fig_2">3</ref>, where we treated the predictions of class label, MMSE and ADAS-Cog as three different tasks and modeled them simultaneously. MMSE, and ADAS-Cog were normalized to the range of [0,1] and we used the deep structure as a regression model. The class label was coded by the 1-of-k scheme. To classify an input vector, we checked the corresponding k outputs and assign it to the class having the largest output. One drawback of deep model is overfitting due to large capacity. This is more prominent if training data is limited. To overcome this limitation, we utilized the dropout technique to improve training.</p><p>3) Dropout with adaptive adaptation Deep learning achieved excellent results in applications where training data size is large. For small sized data sets such as the one in this paper, it is still possible for a deep structure to over-fit the data given the fact that it usually has tens of thousands or even millions of parameters. To improve the generalization capability of the model, the dropout technique tries to prevent weight co-adaptation by randomly dropping out some units in the model during training <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. We incorporated the dropout technique in the multi-task learning context to improve AD diagnosis as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. In the training process, each hidden unit in the model was dropped with a probability of 0.5 when a batch of training cases were present. Previous experiments <ref type="bibr" target="#b9">[10]</ref> showed that it is also beneficial if we apply the "dropout" process to the input layer but with a lower probability (i.e., 0.2 in this paper). In the testing procedure, all hidden units and inputs were used to compute model outputs for a testing case with appropriate compensations, i.e., weights between inputs and the first hidden layer were scaled by 0.8 and all other weights were halved.</p><p>During the multi-task fine-tuning step, the stochastic gradient descent method with a fixed learning factor is usually utilized as <ref type="bibr" target="#b6">[7]</ref>,</p><formula xml:id="formula_20">w ij = w ij + ∆w ij = w ij -α ∂L ∂w ij ,<label>(16)</label></formula><p>where ∂L ∂wij is the gradient of the cost function L α is a learning factor. Sometimes, the weights update may contain a momentum term <ref type="bibr" target="#b9">[10]</ref>. We proposed an adaptive learning factor to speed up the adaptation. The motivation of the adaptive learning is that the learning factor should be large at locations where gradient is small and vice versa. Assume the decrease of L due to the change in w ij is approximated by</p><formula xml:id="formula_21">∆L ij = L ij new -L ij old ≈ ∂L ∂w ij × ∆w ij = -α[ ∂L ∂w ij ] 2 , (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>then ∆L due to all w ij can be computed as</p><formula xml:id="formula_23">∆L = -α i j [ ∂L ∂w ij ] 2 . (<label>18</label></formula><formula xml:id="formula_24">)</formula><p>Suppose we want to decrease L by β%, then L new = (1β)L old , and an adaptive learning factor α can be determined as</p><formula xml:id="formula_25">α = βL old i j [ ∂L ∂wij ] 2 . (<label>19</label></formula><formula xml:id="formula_26">)</formula><p>We set β as 10% in our experiments in this paper. Once the new feature representation is learned, an SVM classifier <ref type="bibr" target="#b11">[12]</ref> was trained using the learned feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. SVM Classifier</head><p>Given a set of data pairs {r i , t i } n i=1 , where r i ∈ R M is the learned feature representation from subjects, t i ∈ {+1, -1} is a class label (e.g., AD vs. non-AD) associated with r i . An SVM defines a hyperplane</p><formula xml:id="formula_27">f (r) = k T φ(r) + e = 0 (<label>20</label></formula><formula xml:id="formula_28">)</formula><p>separating the data points into 2 classes. In equation <ref type="bibr" target="#b19">(20)</ref>, k and e are the hyperplane parameters, and φ(r) is a function mapping the vector r to a higher dimensional space. The hyperplane (20) is determined using the concept of Structural Risk Minimization <ref type="bibr" target="#b11">[12]</ref> by solving the following optimization problem,</p><formula xml:id="formula_29">min k,e,ξ 1 2 k T k + C n i=1 ξ i ,<label>(21)</label></formula><p>subject to</p><formula xml:id="formula_30">t i (k T φ(r i ) + e) ≥ 1 -ξ i , ξ i ≥ 0, (<label>22</label></formula><formula xml:id="formula_31">)</formula><p>where C is a regularization parameter and ξ i is a slack variable. After the hyperplane is determined, an AD case is declared if f (r i ) &gt; 0, or otherwise a non-AD case is declared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS AND DISCUSSIONS</head><p>A. Experimental setup 1) Ten-fold cross-validation We consider four classification tasks including AD patients vs Healthy Control subjects (AD vs HC), MCI patients vs HC (MCI vs HC), AD patients vs MCI patients (AD vs MCI) and MCI-converted vs MCI-non converted (MCI.C vs MCI.NC). For each task, we utilized a ten-fold cross-validation (CV) scheme to evaluate the proposed method. In the ten-fold CV, we randomly divided the data set into 10 parts and for one run, we separated one part for testing and applied the proposed framework to the remaining data to train a classification model. This procedure was repeated 10 times so that each part was tested once. Finally, testing accuracies were computed. To obtain a more reliable estimate of the performance, we repeated the ten-fold CV ten times for each task with different random data partitions and computed average accuracy. To compare different classification models, we kept the same data partitions in the ten-fold CV and utilized the paired-t test to evaluate if there is a significant performance difference.</p><p>2) Hyperparameter determination We did preliminary experiments to determine the structure of the deep learning model. It was found that using three hidden layers with hidden units of 100-50-20 worked the best among the candidate structures considered and was thus utilized in our experiments. For the SVM classifier, we tried different kernels and a linear kernel was chosen. We also did a grid search for the "soft margin" parameter in the linear kernel SVM model but it did not improve the classification accuracies. Therefore, in all experiments, we utilized a three hidden-layer model with a structure of 100-50-20 for feature learning and a linear SVM with default soft margin as the classifier.</p><p>3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Impact assessment for individual component</head><p>There are four components in the proposed framework including PCA, stability selection, dropout and multi-task learning. Inspired by "sensitivity analysis" and "impact assessment" that analyze inputs of or components in a model and identify their impacts on the model objectives by varying the inputs <ref type="bibr" target="#b27">[28]</ref>. We incorporated a similar concept to evaluate the impact of each component on model performance by varying the component (presence vs absence). 'Absence' means that the component was not included in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Methods for comparison</head><p>We compared the proposed method with a baseline method and a similar deep learning system proposed in <ref type="bibr" target="#b5">[6]</ref>. The baseline method consists of all components in the proposed system except the deep learning step. The work by Suk in <ref type="bibr" target="#b5">[6]</ref> is a auto-encoder based deep learning method in which feature representations for MRI, PET and CSF from the same data set were learned separately and combined by a linear SVM classifier. They also combined the learned representations with original features for AD diagnosis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>Table <ref type="table" target="#tab_0">I</ref> shows the overall performances of the proposed method and the impact of each component in the framework. The proposed method performed the best in diagnosing AD and MCI patients, and discriminating MCI patients from AD patients with accuracies of 91.4%, 77.4% and 70.1%, respectively. It is significantly better than the baseline method that obtained accuracies of 86.4%, 72.1% and 61.5% for the diagnoses. In the MCI conversion diagnosis (MCI.C vs MCI.NC), the PCA component slightly degraded the proposed method (from 58.1% to 57.4%) but it is still significantly better than the baseline method (57.4% vs 50.6%).</p><p>Among those components, it is obvious that "dropout" has the most significant impact on the performances. Without "dropout", deep learning did not significantly improve the baseline method (68.2% vs 67.7% in terms of average acc.). The least important component is "PCA", i.e., the average acc. slightly dropped from 74.1% to 73.4% without the PCA component. Without "stability selection" and "multi-task learning", the average accuracy dropped from 74.1% to 72.5% and 72.4%, respectively.</p><p>We conducted a paired-t test between results by the proposed method and those from classical deep learning ("-Dropout"). Table II lists the improvements and p-values. The average improvement is 5.9% and the improvements for all the four classification tasks are significant.</p><p>The work by Suk <ref type="bibr" target="#b5">[6]</ref> on the same data set is also shown in Table <ref type="table" target="#tab_1">II</ref>, where "SAEF" corresponds to the method using features learned by a deep auto-encoder and "LLF+SAEF" represents the method that combines original features with the SAEF features for AD diagnosis. The AD vs MCI classification experiment was not conducted in <ref type="bibr" target="#b5">[6]</ref>. The proposed method (75.4%) outperformed the SAEF method (with an average accuracy of 70.6%). By combining SAEF with LLF (LLF+SAEF), the average accuracy was increased to 74.2% (Last column in Table <ref type="table" target="#tab_1">II</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussions</head><p>There are usually two ways to increase the generalization capability of a model, adding regularization (L 1 or L 2 norm) on weights or using a committee machine. However, solving the regularization problem is usually challenging especially in the deep learning context. In addition, the committee machine technique requires averaging many separately trained models to compute a prediction for a testing case, which is time consuming for deep learning. The dropout procedure does the both (constraint and committee machine) simultaneously in a very efficient way. 1) Each sub-model in training is a sampled model from all possible ones and all sub-models share weights. The weight sharing property is equivalent to the L 1 or L 2 norm constraint on weights, and 2) the testing procedure is an approximation of averaging all trained sub-models for a testing case but it does not separately store them because they share weights. This is an extremely efficient and smart implementation of a committee machine <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>The impact evaluation method was inspired by the "sensitivity analysis" and "impact assessment" <ref type="bibr" target="#b27">[28]</ref>. We were aiming to identify the impact on performance of each component in the model by excluding the component from the pipeline. Note that we did not try to decouple the component from the system. This evaluation method may not be a strict sensitivity analysis or impact assessment by means of their definitions, but we can verify each component if it can improve the AD diagnosis when it is included in the proposed system. Our experiments showed that the dropout component has the largest impact on the performance, multi-task learning ranked the second, stability selection the third, and PCA has the least impact on the performance.</p><p>In terms of stability selection and computational efficiency, there were usually around 40 features left after the stability selection and it took about 1 hour for a personal computer to conduct a ten-fold CV evaluation for one task. The number of features that were chosen was determined by stability selection, in which the Lasso algorithm ran 50 times with different values of regularization parameter (λ). In each run, Lasso chose different features and a probability of being chosen for each feature was computed in the 50 runs. Finally, a feature was chosen if its probability is larger than 0.5.</p><p>It is worth to note that the results by the proposed method in Table <ref type="table" target="#tab_0">I</ref> and Table II only used the new representations learned by the deep model. We tried to combine the new representations with the original features but the combination did not improve the performance. In <ref type="bibr" target="#b5">[6]</ref>, new representations learned from auto-encoder did not perform well unless they were combined with the original features. Our experiment also showed that the deep model without dropout just performed comparably as the baseline method. It seems that traditional deep learning cannot extract information effectively from small data sets unless it is regularized by techniques such as dropout.</p><p>In <ref type="bibr" target="#b28">[29]</ref>, a multi-kernel SVM (MK-SVM) method was applied to the same data set to combine the original LLF features for AD diagnosis, and achieved 93.2% and 76.4% for AD vs HC and MCI vs HC classifications, respectively. The MCI conversion diagnosis and AD vs MCI classification were not conducted. In <ref type="bibr" target="#b5">[6]</ref>, utilizing the MK-SVM method to combine SAEF features from MRI, PET and CSF boosted the performances to 95.9%, 85.0% and 75.8% for the three tasks (AD vs MCI classification was not performed), respectively. Since the dropout technique improved upon the basic deep learning, we are currently investigating if the MK-SVM method can further boost the performance of the proposed system.</p><p>We did not attempt to perform a comprehensive comparison study of the proposed method with others that have been applied to this data set in the literature. Instead, we have evaluated some recently proposed advanced machine learning techniques for AD diagnosis including Lasso, stability selection, multi-task learning, deep learning and dropout. The dropout technique seems to be an effective method of regularization for learning with small data sets. Without dropout, deep learning has no advantage over the baseline method on ANDI data set (68.2% vs 67.7%). Note that dropout is computationally very efficient as compared to either L 1 norm based regularization or committee machine and it can be extended to many models other than the deep model as discussed in this paper.</p><p>IV. CONCLUSION Our proposed method achieved 91.4%, 77.4%, 70.1% and 57.4% accuracies for AD vs HC, MCI vs HC, AD vs MCI, and MCI.c vs MCI.NC classifications, respectively. The framework consists of multiple components including PCA, stability selection, dropout and multi-task deep learning. We showed that dropout is the most effective one. This is not surprising because the size of ADNI data is relatively small compared to that of the deep structure utilized in this paper. Classical deep learning does not perform well on this small data set, but with the dropout technique, the average accuracy was improved by 5.9% on average. We plan to incorporate MK-SVM <ref type="bibr" target="#b5">[6]</ref> into our method for further improving AD diagnosis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Diagram of the proposed multi-task deep learning framework.</figDesc><graphic coords="2,49.75,238.84,249.47,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Principal component analysis example. PC 1 contains the most energy of the data but does not have any discrimination information for the 'red' and 'blue' classes.</figDesc><graphic coords="3,151.37,56.72,309.25,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Multi-task deep learning with dropout. "x" denotes a dropped unit.</figDesc><graphic coords="4,143.43,56.73,325.12,133.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>COMPARISON (IN%) OF THE COMPETING METHODS. THE PROPOSED METHOD CONSISTS OF FOUR COMPONENTS. "-PCA" STANDS FOR "THE PROPOSED METHOD WITHOUT THE PCA COMPONENT" AND "SS" STANDS FOR STABILITY SELECTION, "BASELINE" DENOTES THE FRAMEWORK WITHOUT THE DEEP LEARNING COMPONENT.</figDesc><table><row><cell>Tasks</cell><cell>Proposed</cell><cell>-PCA</cell><cell>-Dropout</cell><cell>-SS</cell><cell>-MultTask</cell><cell>Baseline</cell></row><row><cell>AD vs HC</cell><cell>91.4(1.8)</cell><cell>89.6(1.3)</cell><cell cols="2">84.2(3.0) 89.4(1.6)</cell><cell>90.3(1.7)</cell><cell>86.4(2.0)</cell></row><row><cell>MCI vs HC</cell><cell>77.4(1.7)</cell><cell>76.4(1.5)</cell><cell cols="2">73.1(3.1) 74.3(1.6)</cell><cell>75.6(1.7)</cell><cell>72.1(3.0)</cell></row><row><cell>AD vs MCI</cell><cell>70.1(2.3)</cell><cell>69.5(2.7)</cell><cell cols="2">65.1(3.7) 68.7(2.1)</cell><cell>67.1(2.9)</cell><cell>61.5(2.9)</cell></row><row><cell>MCI.C vs MCI.NC</cell><cell>57.4(3.6)</cell><cell>58.1(1.8)</cell><cell cols="2">50.2(3.3) 57.7(1.8)</cell><cell>56.7(3.0)</cell><cell>50.6(4.7)</cell></row><row><cell>Average</cell><cell>74.1</cell><cell>73.4</cell><cell>68.2</cell><cell>72.5</cell><cell>72.4</cell><cell>67.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PAIRED</head><label>II</label><figDesc>-t TEST BETWEEN RESULTS OF THE PROPOSED METHOD VS DEEP LEARNING WITHOUT DROPOUT. THE METHODS OF "SAEF" AND "LLF+SAEF" WERE PROPOSED BY SUK<ref type="bibr" target="#b5">[6]</ref>. "SAEF" STANDS FOR STACKED AUTO-ENCODER FEATURES AND "LLF" DENOTES LOW LEVEL FEATURES.</figDesc><table><row><cell>Tasks</cell><cell>Proposed</cell><cell cols="2">-Dropout Improvement</cell><cell>p-value</cell><cell>SAEF</cell><cell>LLF+SAEF</cell></row><row><cell>AD vs HC</cell><cell>91.4(1.8)</cell><cell>84.2(3.0)</cell><cell>7.2</cell><cell>&lt; 10 -3</cell><cell>83.2(2.7)</cell><cell>85.3(3.2)</cell></row><row><cell>MCI vs HC</cell><cell>77.4(1.7)</cell><cell>73.1(3.1)</cell><cell>4.3</cell><cell>0.0034</cell><cell>70.1(2.8)</cell><cell>76.9(2.3)</cell></row><row><cell>AD vs MCI</cell><cell>70.1(2.3)</cell><cell>65.1(3.7)</cell><cell>5.0</cell><cell>0.0017</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>MCI.C vs MCI.NC</cell><cell>57.4(3.6)</cell><cell>50.2(3.3)</cell><cell>7.2</cell><cell>&lt; 10 -3</cell><cell>58.4(4.1)</cell><cell>60.3(2.3)</cell></row><row><cell>Overal Average</cell><cell>74.1</cell><cell>68.2</cell><cell>5.9</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Average w/o AD vs MCI</cell><cell>75.4</cell><cell>69.2</cell><cell>6.2</cell><cell>N/A</cell><cell>70.6</cell><cell>74.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Available at http://www.loni.ucla.edu/ADNI.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s Association: 2012 Alzheimer&apos;s disease facts and figures</title>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s &amp; Dementia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="168" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Prediction of MCI to AD conversion, via MRI, CSF biomarkers, and pattern classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Trojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiology of Aging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>2322.e19-2322.e27</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The use of PET in Alzheimer disease</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nordberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Rinne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Langstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neurology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="87" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Default-mode network activity distinguishes Alzheimer&apos;s disease from healthy aging: Evidence from functional MRI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Greicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Menon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="4637" to="4642" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis, Series: Springer Series in Statistics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning-based feature representation for AD/MCI classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<biblScope unit="page" from="583" to="590" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Grivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Multimodal deep learning, ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="1929">1929-1958. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">273</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stability selection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Buhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. B</title>
		<imprint>
			<biblScope unit="page" from="417" to="473" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the Lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An Examination of Psychometric Properties of the Mini-Mental State Examination and the Standardized Mini-Mental State Examination: Implications for Clinical Practice</title>
		<author>
			<persName><surname>Pangman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vc; Sloan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>; Guse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Nursing Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">209213</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ADAS-cog (Alzheimer&apos;s Disease Assessment Scale-cognitive subscale)-validation of the Slovak version</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kolibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Korinkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vajdickova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hunakova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bratisl Lek Listy</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="598" to="602" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust Deep Learning for Improved Classification of AD/MCI Patients</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Kh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning in Medical Imaging</title>
		<imprint>
			<biblScope unit="page" from="240" to="247" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A 3D atlas of the human brain</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">717</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predictive markers for AD in a multi-modality framework: An analysis of MCI progression in the ADNI population</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="574" to="589" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1127</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved learning of Gaussian-Bernoulli restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine LearningICANN</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Categorization by learning and combining object parts</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heisele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An accelerated gradient method for trace norm minimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">457464</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-task learning for classification with dirichlet process priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="35" to="63" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multitask learning: A knowledge-based source of inductive bias</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Saltelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ratto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Campolongo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cariboni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saisana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tarantola</surname></persName>
		</author>
		<title level="m">Global Sensitivity Analysis. The Primer</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multimodal Classification of Alzheimers Disease and Mild Cognitive Impairment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="856" to="867" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
