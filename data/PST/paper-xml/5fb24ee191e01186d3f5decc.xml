<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EDITOR: an Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weijia</forename><surname>Xu</surname></persName>
							<email>weijia@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
							<email>marine@cs.umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EDITOR: an Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce an Edit-Based TransfOrmer with Repositioning (EDITOR), which makes sequence generation flexible by seamlessly allowing users to specify preferences in output lexical choice. Building on recent models for non-autoregressive sequence generation <ref type="bibr" target="#b22">(Gu et al., 2019)</ref>, EDI-TOR generates new sequences by iteratively editing hypotheses. It relies on a novel reposition operation designed to disentangle lexical choice from word positioning decisions, while enabling efficient oracles for imitation learning and parallel edits at decoding time. Empirically, EDITOR uses soft lexical constraints more effectively than the Levenshtein Transformer <ref type="bibr" target="#b22">(Gu et al., 2019)</ref> while speeding up decoding dramatically compared to constrained beam search <ref type="bibr" target="#b36">(Post and Vilar, 2018)</ref>. EDITOR also achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer on standard Romanian-English, English-German, and English-Japanese machine translation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (MT) architectures <ref type="bibr" target="#b3">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b49">Vaswani et al., 2017)</ref> make it difficult for users to specify preferences that could be incorporated more easily in statistical MT models <ref type="bibr">(Koehn et al., 2007)</ref> and have been shown to be useful for interactive machine translation <ref type="bibr" target="#b19">(Foster et al., 2002;</ref><ref type="bibr" target="#b5">Barrachina et al., 2009)</ref> and domain adaptation <ref type="bibr" target="#b23">(Hokamp and Liu, 2017)</ref>. Lexical constraints or preferences have previously been incorporated by re-training NMT models with constraints as inputs <ref type="bibr" target="#b43">(Song et al., 2019;</ref><ref type="bibr" target="#b16">Dinu et al., 2019)</ref> or with constrained beam search that drastically slows down decoding <ref type="bibr" target="#b23">(Hokamp and Liu, 2017;</ref><ref type="bibr" target="#b36">Post and Vilar, 2018)</ref>.</p><p>In this work, we introduce a translation model that can seamlessly incorporate users' lexical unconstrained MT output constraints: plague ankle</p><p>The 29-year-old has been plagued with a troublesome ankle for two years.</p><p>JucƒÉtorul de 29 de ani sa luptat doi ani cu problemele la gleznƒÉ.</p><p>The 29-year-old has struggled for two years with problems in the bullying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>hard-constrained MT output</head><p>The 29-year-old has been plague for two years with problems in the ankle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>soft-constrained MT output</head><p>The 29-year-old has struggled for two years with problems in the ankle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>source reference</head><p>Figure <ref type="figure">1</ref>: Romanian to English MT example. Unconstrained MT incorrectly translates "glezn»É" to "bullying". Given constraint words "plague" and "ankle", soft-constrained MT correctly uses "ankle" and avoids disfluencies introduced by using "plague" as a hard constraint in its exact form. choice preferences without increasing the time and computational cost at decoding time, while being trained on regular MT samples. We apply this model to MT tasks with soft lexical constraints. As illustrated in Figure <ref type="figure">1</ref>, when decoding with soft lexical constraints, user preferences for lexical choice in the output language are provided as an additional input sequence of target words in any order. The goal is to let users encode terminology, domain or stylistic preferences in target word usage, without strictly enforcing hard constraints that might hamper NMT's ability to generate fluent outputs.</p><p>Our model is an Edit-Based TransfOrmer with Repositioning (EDITOR), which builds on recent progress on non-autoregressive sequence generation <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b20">Ghazvininejad et al., 2019)</ref>. 1  Specifically, the Levenshtein Transformer <ref type="bibr" target="#b22">(Gu et al., 2019)</ref> showed that iteratively refining output sequences via insertions and deletions yields a fast and flexible generation process for MT and automatic post-editing tasks. EDITOR replaces the deletion operation with a novel reposition opera-1 Our implementation will be released on github. arXiv:2011.06868v1 [cs.CL] 13 Nov 2020 tion to disentangle lexical choice from reordering decisions. As a result, EDITOR exploits lexical constraints more effectively and efficiently than the Levenshtein Transformer, as a single reposition operation can subsume a sequence of deletions and insertions. To train EDITOR via imitation learning, the reposition operation is defined to preserve the ability to use the Levenshtein edit distance <ref type="bibr" target="#b27">(Levenshtein, 1966)</ref> as an efficient oracle. We also introduce a dual-path roll-in policy which lets the reposition and deletion models learn to refine their respective outputs more effectively.</p><p>Experiments on Romanian-English, English-German, and English-Japanese MT show that EDITOR achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer <ref type="bibr" target="#b22">(Gu et al., 2019)</ref> on the standard MT tasks and exploit soft lexical constraints better: it achieves significantly better translation quality and matches more constraints with faster decoding speed than the Levenshtein Transformer. It also drastically speeds up decoding compared to lexically constrained decoding algorithms <ref type="bibr" target="#b36">(Post and Vilar, 2018)</ref>. Furthermore, Results highlight the benefits of soft constraints over hard ones -EDITOR with soft constraints achieves translation quality on par or better than both EDITOR and Levenshtein Transformer with hard constraints <ref type="bibr" target="#b47">(Susanto et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Non-Autoregressive MT While autoregressive models that decode from left-to-right are the de facto standard for many sequence generation tasks <ref type="bibr" target="#b12">(Cho et al., 2014;</ref><ref type="bibr" target="#b13">Chorowski et al., 2015;</ref><ref type="bibr" target="#b50">Vinyals and Le, 2015)</ref>, non-autoregressive models offer a promising alternative to speed up decoding by generating a sequence of tokens in parallel <ref type="bibr" target="#b21">(Gu et al., 2018;</ref><ref type="bibr" target="#b33">van den Oord et al., 2018;</ref><ref type="bibr" target="#b28">Ma et al., 2019)</ref>. However, their output quality suffers due to the large decoding space and strong independence assumptions between target tokens <ref type="bibr" target="#b28">(Ma et al., 2019;</ref><ref type="bibr" target="#b52">Wang et al., 2019)</ref>. These issues have been addressed via partially parallel decoding <ref type="bibr" target="#b51">(Wang et al., 2018;</ref><ref type="bibr" target="#b46">Stern et al., 2018)</ref> or multi-pass decoding <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b20">Ghazvininejad et al., 2019;</ref><ref type="bibr" target="#b22">Gu et al., 2019)</ref>. This work adopts multipass decoding, where the model generates the target sequences by iteratively editing the outputs from previous iterations. Edit operations such as substitution <ref type="bibr" target="#b20">(Ghazvininejad et al., 2019)</ref> and insertion-deletion <ref type="bibr" target="#b22">(Gu et al., 2019)</ref> have reduced the quality gap between non-autoregressive and autoregressive models. However, we argue that these operations limit the flexibility and efficiency of the resulting models for MT by entangling lexical choice and reordering decisions.</p><p>Reordering vs. Lexical Choice EDITOR's insertion and reposition operations connect closely with the long-standing view of MT as a combination of a translation or lexical choice modelwhich selects appropriate translations for source units given their context -and reordering model -which encourages the generation of a target sequence order appropriate for the target language. This view is reflected in architectures ranging from the word-based IBM models <ref type="bibr">(Brown et al., 1990)</ref>, sentence-level models that generate a bag of target words that is reordered to construct a target sentence <ref type="bibr" target="#b4">(Bangalore et al., 2007)</ref>, or the Operation Sequence Model <ref type="bibr" target="#b17">(Durrani et al., 2015;</ref><ref type="bibr" target="#b44">Stahlberg et al., 2018)</ref>, which views translation as a sequence of translation and reordering operations over bilingual minimal units. By contrast, autoregressive NMT models <ref type="bibr" target="#b3">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b49">Vaswani et al., 2017)</ref> do not explicitly separate lexical choice and reordering, and previous non-autoregressive models break up reordering into sequences of other operations. This work introduces the reposition operation which makes it possible to move words around during the refinement process, as reordering models do. However, we will see that reposition differs from typical reordering to enable efficient oracles for training via imitation learning, and parallelization of edit operations at decoding time (Section 3).</p><p>MT with Soft Lexical Constraints NMT models lack flexible mechanisms to incorporate users preferences in their outputs. Lexical constraints have been incorporated in prior work via 1) constrained training where NMT models are trained on parallel samples augmented with constraint target phrases in both the source and target sequences <ref type="bibr" target="#b43">(Song et al., 2019;</ref><ref type="bibr" target="#b16">Dinu et al., 2019)</ref>, or 2) constrained decoding where beam search is modified to include constraint words or phrases in the output <ref type="bibr" target="#b23">(Hokamp and Liu, 2017;</ref><ref type="bibr" target="#b36">Post and Vilar, 2018)</ref>. These mechanisms can incorporate domain-specific knowledge and lexicons which is particularly helpful in low-resource cases <ref type="bibr" target="#b2">(Arthur et al., 2016;</ref><ref type="bibr" target="#b48">Tang et al., 2016)</ref>. Despite their success at domain adaptation for MT <ref type="bibr" target="#b23">(Hokamp and Liu, 2017)</ref> and caption generation <ref type="bibr" target="#b1">(Anderson et al., 2017)</ref>, they suffer from several issues: constrained training requires building dedicated models for constrained language generation, while constrained decoding adds significant computational overhead and treats all constraints as hard constraints which may hurt fluency. In other tasks, various constraint types have been introduced by designing complex architectures tailored to specific content or style constraints <ref type="bibr" target="#b0">(Abu Sheikha and Inkpen, 2011;</ref><ref type="bibr" target="#b29">Mei et al., 2016)</ref>, or via segmentlevel "side-constraints" <ref type="bibr" target="#b41">(Sennrich et al., 2016a;</ref><ref type="bibr" target="#b18">Ficler and Goldberg, 2017;</ref><ref type="bibr" target="#b40">Scarton and Specia, 2018)</ref>, which condition generation on users' stylistic preferences, but do not offer fine-grained control over their realization in the output sequence. We refer the reader to Yvon and Abdul Rauf (2020) for a comprehensive review of the strengths and weaknesses of current techniques to incorporate terminology constraints in NMT.</p><p>Our work is closely related to Susanto et al. (2020)'s idea of applying the Levenshtein Transformer to MT with hard terminology constraints. We will see that their technique can directly be used by EDITOR as well (Section 3.3), but this does not offer empirical benefits over the default EDITOR model (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The EDITOR Model</head><p>We cast both constrained and unconstrained language generation as an iterative sequence refinement problem modeled by a Markov Decision Process (Y, A, E, R, y 0 ), where a state y in the state space Y corresponds to a sequence of tokens y = (y 1 , y 2 , ..., y L ) from the vocabulary V up to length L, and y 0 ‚àà Y is the initial sequence For standard sequence generation tasks, y 0 is the empty sequence ( s , /s ). For lexically constrained generation tasks, y 0 consists of the words to be used as constraints ( s , c 1 , ..., c m , /s ).</p><p>At the k-th decoding iteration, the model takes as input y k‚àí1 , the output from the previous iteration, chooses an action a k ‚àà A to refine the sequence into y k = E(y k‚àí1 , a k ), and receives a reward r k = R(y k ). The policy œÄ maps the input sequence y k‚àí1 to a probability distribution P (A) over the action space A. Our model is based on the Transformer encoder-decoder <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref> and we extract the decoder representa- Figure <ref type="figure">2</ref>: Applying the reposition operation r to input y: r i &gt; 0 is the 1-based index of token y i in the input sequence; y i is deleted if r i = 0.</p><p>tions (h 1 , ..., h n ) to make the policy predictions. Each refinement action is based on two basic operations: reposition and insertion.</p><p>Reposition For each position i in the input sequence y 1...n , the reposition policy œÄ rps (r | i, y) predicts an index r ‚àà [0, n]: if r &gt; 0, we place the r-th input token y r at the ith output position, otherwise we delete the token at that position (Figure <ref type="figure">2</ref>). We constrain œÄ rps (1 | 1, y) = 1, œÄ rps (n | n, y) = n to maintain sequence boundaries. Note that reposition differs from typical reordering since 1) it makes it possible to delete tokens, and 2) it places tokens at each position independently, which enables parallelization at decoding time.</p><p>In principle, the same input token can thus be placed at multiple output positions. However, this happens rarely in practice as the policy predictor is trained to follow oracle demonstrations which cannot contain such repetitions by design. <ref type="foot" target="#foot_0">2</ref>The reposition classifier gives a categorical distribution over the index of the input token to be placed at each output position:</p><formula xml:id="formula_0">œÄ rps (r | i, y) = softmax(h i ‚Ä¢ [b, e 1 , ..., e n ]) (1)</formula><p>where e j is the embedding of the j-th token in the input sequence, and b ‚àà R d model is used to predict whether to delete the token. The dot product in the softmax function captures the similarity between the hidden state h i and each input embedding e j or the deletion vector b.</p><p>Insertion Following <ref type="bibr" target="#b22">Gu et al. (2019)</ref>, the insertion operation consists of two phases: (1) placeholder insertion: given an input sequence y 1...n , the placeholder predictor œÄ plh (p | i, y) predicts the number of placeholders p ‚àà [0, K max ] to be inserted between two neighboring tokens (y i , y i+1 ); 3 (2) token prediction: given the output of the placeholder predictor, the token predictor œÄ tok (t | i, y) replaces each placeholder with an actual token.</p><p>The Placeholder Insertion Classifier gives a categorical distribution over the number of placeholders to be inserted between every two consecutive positions:</p><formula xml:id="formula_1">œÄ plh (p | i, y) = softmax([h i ; h i+1 ] ‚Ä¢ W plh ) (2)</formula><p>where Kmax+1) .</p><formula xml:id="formula_2">W plh ‚àà R (2d model )√ó(</formula><p>The Token Prediction Classifier predicts the identity of each token to fill in each placeholder:</p><formula xml:id="formula_3">œÄ tok (t | i, y) = softmax(h i ‚Ä¢ W tok ) (3)</formula><p>where</p><formula xml:id="formula_4">W tok ‚àà R d model √ó|V| .</formula><p>Action Given an input sequence y 1...n , an action consists of repositioning tokens, inserting and replacing placeholders. Formally, we define an action as a sequence of reposition (r), placeholder insertion (p), and token prediction (t) operations: a = (r, p, t). r, p, and t are applied in this order to adjust non-empty initial sequences via reposition before inserting new tokens. Each of r, p, and t consists of a set of basic operations that can be applied in parallel:</p><formula xml:id="formula_5">r = {r 1 , ..., r n } p = {p 1 , ..., p m‚àí1 } t = {t 1 , ..., t l }</formula><p>where m = n i I(r i &gt; 0) and l = m‚àí1 i p i . We define the policy as</p><formula xml:id="formula_6">œÄ(a|y) = r i ‚ààr œÄ rps (r i | i, y) ‚Ä¢ p i ‚ààp œÄ plh (p i | i, y )‚Ä¢ t i ‚ààt œÄ tok (t i | i, y )</formula><p>with intermediate outputs y = E(y, r) and y = E(y , p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual-Path Imitation Learning</head><p>We train EDITOR using imitation learning <ref type="bibr" target="#b15">(Daum√© III et al., 2009;</ref><ref type="bibr" target="#b39">Ross et al., 2011;</ref><ref type="bibr" target="#b38">Ross and Bagnell, 2014)</ref> to efficiently explore the space of valid action sequences that can reach a reference translation. The key idea is to construct a roll-in policy œÄ in to generate sequences to be refined and a roll-out policy œÄ out to estimate cost-to-go for all possible actions given each input sequence. The model is trained to choose actions that minimizes the cost-to-go estimates. We use a search-based oracle policy œÄ * as the roll-out policy and train the model to imitate the optimal actions chosen by the oracle.</p><p>Formally, d œÄ in rps and d œÄ in ins denote the distributions of sequences induced by running the roll-in policies œÄ in rps and œÄ in ins respectively. We update the model policy œÄ = œÄ rps ‚Ä¢ œÄ plh ‚Ä¢ œÄ tok to minimize the expected cost C(œÄ ; y, œÄ * ) by comparing the model policy against the cost-to-go estimates under the oracle policy œÄ * given input sequences y:</p><formula xml:id="formula_7">E y rps ‚àºd œÄ in rps C(œÄ rps ; y rps , œÄ * ) + E y ins ‚àºd œÄ in ins [C(œÄ plh , œÄ tok ; y ins , œÄ * )]<label>(4)</label></formula><p>The cost function compares the model vs. oracle actions. As prior work suggests that cost functions close to the cross-entropy loss are better suited to deep neural models than the squared error <ref type="bibr" target="#b25">(Leblond et al., 2018;</ref><ref type="bibr" target="#b11">Cheng et al., 2018)</ref>, we define the cost function as the KL divergence between the action distributions given by the model policy and by the oracle <ref type="bibr" target="#b53">(Welleck et al., 2019)</ref>:</p><formula xml:id="formula_8">C(œÄ ; y, œÄ * ) =D KL [ œÄ * (a | y, y * )|| œÄ(a | y)] =E a‚àºœÄ * (a | y,y * ) [‚àí log œÄ(a | y)] + const.</formula><p>(5)</p><p>where the oracle has additional access to the reference sequence y * . By minimizing the cost function, the model learns to imitate the oracle policy without access to the reference sequence.</p><p>Next, we describe how the reposition operation is incorporated in the roll-in policy (Section 3.2.1) and the oracle roll-out policy (Section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Dual-Path Roll-in Policy</head><p>As shown in Figure <ref type="figure" target="#fig_0">3</ref>, the roll-in policies œÄ in ins and œÄ in rps for the reposition and insertion policy predictors are stochastic mixtures of the noised reference sequences and the output sequences sampled from their corresponding dual policy predictors. Figure <ref type="figure" target="#fig_1">4</ref> shows an example for creating the roll-in sequences: we first create the initial sequence y 0 by applying random word dropping <ref type="bibr" target="#b22">(Gu et al., 2019)</ref> and random word shuffle <ref type="bibr">(Lample et al., 2018)</ref> with probability of 0.5 and maximum shuffle distance of 3 to the reference sequence y * , and produce the roll-in sequences for each policy predictor as follows:</p><formula xml:id="formula_9">ùë¶ ! ‚àº ùëõùëúùëñùë†ùëí(ùë¶ * ) ùúã #$% ùúã #$% ùúã $&amp;' * ùúã ()* ùë¶ +,% ùë¶ #$% ùúã #$% * ùúã $&amp;' ùúã ()* ùúã ()* * ùõΩ 1 ‚àí ùõΩ ùõº 1 ‚àí ùõº Dual-Path Roll-In ùúã $&amp;' * ùúã *</formula><p>Oracle Roll-Out  </p><formula xml:id="formula_10">ùë¶ #$% ùë¶ &amp;'% noise ùúã &amp;'% ùúã '() * + ùúã *+, ùõº 1 ‚àí ùõΩ ùõΩ 1 ‚àí ùõº</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Reposition: the roll-in policy œÄ in rps is a stochastic mixture of the initial sequence y 0 and the output sequence by applying one iteration of the oracle placeholder insertion policy p * ‚àº œÄ * and the model's token prediction policy t ‚àº œÄ tok to y 0 :</p><formula xml:id="formula_11">d œÄ in rps = y 0 , if u &lt; Œ≤ E(E(y 0 , p * ), t), otherwise<label>(6)</label></formula><p>where the mixture factor Œ≤ ‚àà [0, 1] and random variable u ‚àº Uniform(0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Insertion: the roll-in policy œÄ in ins is a stochastic mixture of the initial sequence y 0 and the output sequence by applying one iteration of the model's reposition policy r ‚àº œÄ rps to y 0 :</p><formula xml:id="formula_12">d œÄ in ins = y 0 , if u &lt; Œ± E(y 0 , r), otherwise<label>(7)</label></formula><p>where the mixture factor Œ± ‚àà [0, 1] and random variable u ‚àº Uniform(0, 1).</p><p>While <ref type="bibr" target="#b22">Gu et al. (2019)</ref> define roll-in using only the model's insertion policy, we call our approach dual-path because roll-in creates two distinct intermediate sequences using the model's reposition or insertion policy. This makes it possible for the reposition and insertion policy predictors to learn to refine one another's outputs during rollout, mimicking the iterative refinement process used at inference time.<ref type="foot" target="#foot_2">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Oracle Roll-Out Policy</head><p>Policy Given an input sequence y and a reference sequence y * , the oracle algorithm finds the optimal action to transform y into y * with the minimum number of basic edit operations: Oracle(y, y * ) = arg min a NumOps(y, y * | a)</p><p>(8) The associated oracle policy is defined as:</p><formula xml:id="formula_13">œÄ * (a | y, y * ) = 1, if a = Oracle(y, y * ) 0, otherwise<label>(9)</label></formula><p>Algorithm The reposition and insertion operations used in EDITOR are designed so that the Levenshtein edit distance algorithm <ref type="bibr" target="#b27">(Levenshtein, 1966)</ref> can be used as the oracle. The reposition operation (Section 3.1) can be split into two distinct types of operations: (1) deletion and (2) replacing a word with any other word appearing in the input sequence, which is a constrained version of the Levenshtein substitution operation. As a result, we can use dynamic programming to find the optimal action sequence in O(|y||y * |) time. By contrast, the Levenshtein Transformer restricts the oracle and model to insertion and deletion operations only. While in principle substitutions can be performed indirectly by deletion and re-insertion, our results show the benefits of using the reposition variant of the substitution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>During inference, we start from the initial sequence y 0 . For standard sequence generation tasks, y 0 is an empty sequence, whereas for lexically constrained generation y 0 is a sequence of lexical constraints. Inference then proceeds in the exact same way for constrained and unconstrained tasks. The initial sequence is refined iteratively by applying a sequence of actions (a 1 , a 2 , ...) = (r 1 , p 1 , t 1 ; r 2 , p 2 , t 2 ; ...). We greedily select the best action at each iteration given the model policy in Eqs.</p><p>(1) to (3). We stop refining if 1) the output sequences from two consecutive iterations are the same <ref type="bibr" target="#b22">(Gu et al., 2019)</ref>, or 2) the maximum number of decoding steps is reached <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b20">Ghazvininejad et al., 2019)</ref>.<ref type="foot" target="#foot_3">5</ref> </p><p>Incorporating Soft Constraints Although ED-ITOR is trained without lexical constraints, it can be used seamlessly for MT with constraints without any change to the decoding process except using the constraint sequence as the initial sequence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating Hard Constraints</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the EDITOR model on standard (Section 4.2) and lexically constrained machine translation (Sections 4.3-4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Dataset Following <ref type="bibr" target="#b22">Gu et al. (2019)</ref>, we experiment on three language pairs spanning different language families and data conditions (Table <ref type="table" target="#tab_1">1</ref>): Romanian-English (Ro-En) from WMT16 (Bojar et al., 2016), English-German (En-De) from WMT14 <ref type="bibr" target="#b6">(Bojar et al., 2014)</ref>, and English-Japanese (En-Ja) from WAT2017 Small-NMT Task <ref type="bibr" target="#b30">(Nakazawa et al., 2017)</ref>. We also evaluate EDITOR on the two En- with initial learning rate of 0.0005 and a batch size of 64,800 tokens for maximum 300,000 steps. <ref type="foot" target="#foot_5">7</ref>We select the best checkpoint based on validation BLEU <ref type="bibr" target="#b35">(Papineni et al., 2002)</ref>. All models are trained on 8 NVIDIA V100 Tensor Core GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Distillation</head><p>We apply sequencelevel knowledge distillation from autoregressive teacher models as widely used in nonautoregressive generation <ref type="bibr" target="#b21">(Gu et al., 2018;</ref><ref type="bibr" target="#b26">Lee et al., 2018;</ref><ref type="bibr" target="#b22">Gu et al., 2019)</ref>. Specifically, when training the non-autoregressive models, we replace the reference sequences y * in the training data with translation outputs from the AR teacher model (Sockeye, with beam = 4). <ref type="foot" target="#foot_6">8</ref> We also report the results when applying knowledge distillation to autoregressive models.</p><p>Evaluation We evaluate translation quality via case-sensitive tokenized BLEU (as in Gu et al. ( <ref type="formula">2019</ref>))<ref type="foot" target="#foot_7">9</ref> and RIBES <ref type="bibr" target="#b24">(Isozaki et al., 2010)</ref>, which is more sensitive to word order differences. Before computing the scores, we tokenize the German and English outputs using Moses and Japanese outputs using KyTea.<ref type="foot" target="#foot_8">10</ref> For lexically constrained decoding, we report the constraint preservation rate (CPR) in the translation outputs. We quantify decoding speed using latency per sentence. It is computed as the average time (in ms) required to translate the test set using batch size of one (excluding the model loading time) divided by the number of sentences in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MT Tasks</head><p>Since our experiments involve two different toolkits, we first compare the same Transformer AR models built with Sockeye and with fairseq: the AR models achieve comparable decoding speed and translation quality regardless of toolkit -the Sockeye model obtains higher BLEU than the fairseq model on Ro-En and En-De but lower on En-Ja (Table <ref type="table" target="#tab_2">2</ref>). Further comparisons will therefore center on the Sockeye AR model to better compare EDITOR with the lexically constrained decoding algorithm <ref type="bibr" target="#b36">(Post and Vilar, 2018)</ref>.</p><p>Table <ref type="table" target="#tab_2">2</ref> also shows that knowledge distillation has a small and inconsistent impact on AR models (Sockeye): it yields higher BLEU on Ro-En, close BLEU on En-De, and lower BLEU on En-Ja.<ref type="foot" target="#foot_9">11</ref> Thus, we use the AR models trained without distillation in further experiments.</p><p>Next, we compare the NAR models against the AR (Sockeye) baseline. As expected, both EDI-TOR and LevT achieve close translation quality to their AR teachers with 2-4 times speedup. BLEU differences are small (‚àÜ &lt; 1.1) as in prior work <ref type="bibr" target="#b22">(Gu et al., 2019)</ref>. The RIBES trends are more surprising: both NAR models significantly outperform the AR models (Sockeye) on RIBES, except for En-Ja, where EDITOR and the AR models significantly outperforms LevT. This illustrates the strength of EDITOR in word reordering.</p><p>Finally, results confirm the benefits of EDI-TOR's reposition operation over LevT: decoding with EDITOR is 6-7% faster than LevT on Ro-En and En-De, and 33% faster on En-Ja -a more distant language pair which requires more reordering but no inflection changes on reordered words -with no statistically significant difference in BLEU nor RIBES, except for En-Ja, where ED-ITOR significantly outperforms LevT on RIBES. Overall, EDITOR is shown to be a good alternative to LevT on standard machine translation tasks and can also be used to replace the AR models in settings where decoding speed matters more than small differences in translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MT with Soft Lexical Constraints</head><p>We now turn to the main evaluation of EDITOR on machine translation with soft lexical constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Conditions</head><p>We conduct a controlled comparison of the following approaches:</p><p>‚Ä¢ NAR models: EDITOR and LevT view the lexical constraints as soft constraints, provided via the initial target sequence. We also explore the decoding technique introduced in Susanto et al. ( <ref type="formula">2020</ref>) to support hard constraints.</p><p>‚Ä¢ AR models: they use the provided target words as hard constraints enforced at decoding time by an efficient form of constrained beam search: dynamic beam allocation (DBA) <ref type="bibr" target="#b36">(Post and Vilar, 2018)</ref>.<ref type="foot" target="#foot_10">12</ref> Crucially, all models, including EDITOR, are the exact same models evaluated on the standard MT tasks above, and do not need to be trained specifically to incorporate constraints.</p><p>We define lexical constraints as <ref type="bibr" target="#b36">Post and Vilar (2018)</ref>: for each source sentence, we randomly select one to four words from the reference as lexical constraints. We then randomly shuffle the constraints and apply BPE to the constraint sequence. Different from the terminology test sets in <ref type="bibr" target="#b16">Dinu et al. (2019)</ref> which contain only several hundred sentences with mostly nominal constraints, our constructed test sets are larger and include lexical constraints of all types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>Table <ref type="table" target="#tab_3">3</ref> shows that EDITOR exploits the soft constraints to strike a better balance between translation quality and decoding speed than other models. Compared to LevT, EDITOR preserves 7-17% more constraints and achieves significantly higher translation quality (+1.1-2.5 on   with beam = 4, EDITOR yields significantly higher BLEU (+1.0-2.2) and RIBES (+4.1-6.9) with 3-4 times decoding speedup. After increasing the beam to 10, EDITOR obtains lower BLEU but comparable RIBES with 6-7 times decoding speedup. 13 Note that AR models treat provided words as hard constraints and therefore achieve over 99% CPR by design, while NAR models treat them as soft constraints.</p><p>Results confirm that enforcing hard constraints increases CPR but degrades translation quality compared to the same model using soft constraints: for LevT, it degrades BLEU by 2.2-3.9 and RIBES by 5.0-6.6. For EDITOR, it degrades BLEU by 1.6-4.3 and RIBES by 3.1-4.4 (Table 3). By contrast, EDITOR with soft constraints strikes a better balance between translation quality and constraint preservation.</p><p>The strengths of EDITOR hold when varying the number of constraints (Figure <ref type="figure" target="#fig_2">5</ref>). For all tasks and models, adding constraints helps BLEU up to a certain point, ranging from 4 to 10 words. When 13  <ref type="bibr" target="#b36">Post and Vilar (2018)</ref> show that the optimal beam size for DBA is 20. Our experiment on En-De shows that increasing the beam size from 10 to 20 improves BLEU by 0.7 at the cost of doubling the decoding time.</p><p>excluding the slower AR model (beam = 10), EDITOR consistently reaches the highest BLEU score with 2-10 constraints: EDITOR outperforms LevT and the AR model with beam = 4. Consistent with <ref type="bibr" target="#b36">Post and Vilar (2018)</ref>, as the number of constraints increases, the AR model needs larger beams to reach good performance. When the number of constraints increases to 10, EDI-TOR yields higher BLEU than the AR model on En-Ja and Ro-En, even after incurring the cost of increasing the AR beam to 10.</p><p>Are EDITOR improvements limited to preserving constraints better? We verify that this is not the case by computing the target word F1 binned by frequency <ref type="bibr" target="#b31">(Neubig et al., 2019)</ref>. Figure <ref type="figure" target="#fig_3">6</ref> shows that EDITOR improves over LevT across all test frequency classes and closes the gap between NAR and AR models: the largest improvements are obtained for low and medium frequency words -on En-De and En-Ja, the largest improvements are on words with frequency between 5 and 1000, while on Ro-En, EDITOR improves more on words with frequency between 5 and 100. EDITOR also improves F1 on rare words (frequency in [0, 5)), but not as much as for more frequent words. now conduct further analysis to better understand the factors that contribute to EDITOR's advantages over LevT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Reposition</head><p>We compare the average number of basic edit operations (Section 3.1) of different types used by EDITOR and LevT on each test sentence (averaged over the 5 runs): reposition (excluding deletion for controlled comparison with LevT), deletion, and insertion performed by LevT and EDITOR at decoding time. Table <ref type="table">4</ref> shows that LevT deletes tokens 2-3 times more often than EDITOR, which explains its lower CPR  faith Stephen think I think faith that Stephen Thom@@ p@@ son can think .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EDITOR:</head><p>[Terminate] think Stephen faith I think Stephen Thom@@ p@@ son has faith in us .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Terminate]</head><p>Action ùëé ! :</p><formula xml:id="formula_14">ùë¶ " = ‚Ñ∞ ùë¶ # , ùëë ! : ùë¶ ! = ‚Ñ∞ ùë¶‚Ä≤‚Ä≤, ùë° ! : Action ùëé ! :</formula><p>no further actions:</p><formula xml:id="formula_15">[plh] [plh] faith [plh] Stephen [plh] [plh] [plh] [plh] think [plh] ùë¶ "" = ‚Ñ∞ ùë¶‚Ä≤, ùëù ! : faith Stephen think ùë¶ # : faith Stephen think ùë¶ # : ùë¶ " = ‚Ñ∞ ùë¶ # , ùëü ! : ùë¶ "" = ‚Ñ∞ ùë¶‚Ä≤, ùëù ! : [plh] think Stephen [plh] [plh] [plh] [plh] faith [plh] [plh] [plh] ùë¶ ! = ‚Ñ∞ ùë¶‚Ä≤‚Ä≤, ùë° ! :</formula><p>no further actions:  cessfully reduces redundancy in edit operations and makes decoding more efficient by replacing sequences of insertions and deletions with a single repositioning step. Furthermore, Figure <ref type="figure" target="#fig_4">7</ref> illustrates how reposition increases flexibility in exploiting lexical constraints, even when they are provided in the wrong order. While LevT generates an incorrect output by using constraints in the provided order, EDI-TOR's reposition operation helps generate a more fluent and adequate translation.  <ref type="bibr" target="#b16">(Dinu et al., 2019)</ref> provided with correct terminology entries (exact matches on both source and target sides). EDITOR with soft constraints achieves higher BLEU than LevT with soft constraints, and on par or higher BLEU than LevT with hard constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Dual</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">MT with Terminology Constraints</head><p>We evaluate EDITOR on the terminology test sets released by <ref type="bibr" target="#b16">Dinu et al. (2019)</ref> to test its ability to incorporate terminology constraints and to further compare it with prior work <ref type="bibr" target="#b16">(Dinu et al., 2019;</ref><ref type="bibr" target="#b36">Post and Vilar, 2018;</ref><ref type="bibr" target="#b47">Susanto et al., 2020)</ref>. Compared to <ref type="bibr" target="#b36">Post and Vilar (2018)</ref> and <ref type="bibr" target="#b16">Dinu et al. (2019)</ref>, EDITOR with soft constraints achieves higher absolute BLEU, and higher BLEU improvements over its counterpart without constraints (Table <ref type="table" target="#tab_6">6</ref>). Consistent with previous findings by <ref type="bibr" target="#b47">Susanto et al. (2020)</ref>, incorporating soft constraints in LevT improves BLEU by +0.3 on Wiktionary and by +0.4 on IATE. Enforcing hard constraints as in <ref type="bibr" target="#b47">Susanto et al. (2020)</ref> increases the term usage by +8-10% and improves BLEU by +0.3-0.6 over LevT using soft constraints. 14  For EDITOR, adding soft constraints improves BLEU by +0.5 on Wiktionary and +0.9 on IATE, with very high term usages (96.8% and 97.1% respectively). EDITOR thus correctly uses the provided terms almost all the time when they are provided as soft constraints, so there is little benefit to enforcing hard constraints instead: they help close the small gap to reach 100% term usage and do not 14 We use our implementations of <ref type="bibr" target="#b47">Susanto et al. (2020)</ref>'s technique for a more controlled comparison. The LevT baseline in <ref type="bibr" target="#b47">Susanto et al. (2020)</ref> achieves higher BLEU than ours on the small Wiktionary and IATE test sets, while it underperforms our LevT on the full WMT14 test set (26.5 vs. 26.9). improve BLEU. Overall, EDITOR achieves on par or higher BLEU than LevT with hard constraints.</p><p>Results also suggest that EDITOR can handle phrasal constraints even though it relies on tokenlevel edit operations, since it achieves above 99% term usage on the terminology test sets where 26-27% of the constraints are multi-token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced EDITOR, a non-autoregressive transformer model that iteratively edits hypotheses using a novel reposition operation. Reposition combined with a new dual-path imitation learning strategy helps EDITOR generate output sequences that flexibly incorporate user's lexical choice preferences. Extensive experiments showed that ED-ITOR exploits soft lexical constraints more effectively than the Levenshtein Transformer <ref type="bibr" target="#b22">(Gu et al., 2019)</ref> while speeding up decoding dramatically compared to constrained beam search <ref type="bibr" target="#b36">(Post and Vilar, 2018)</ref>. Results also confirm the benefits of using soft constraints over hard ones in terms of translation quality. EDITOR also achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer on three standard MT tasks. These promising results open several avenues for future work, including using EDITOR for other generation tasks than MT and investigating its ability to incorporate more diverse constraint types into the decoding process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Our dual-path imitation learning process uses both the reposition and insertion policies during roll-in so that they can be trained to refine each other's outputs: Given an initial sequence y 0 , created by noising the reference y * , the roll-in policy stochastically generates intermediate sequences y ins and y rps via reposition and insertion respectively. The policy predictors are trained to minimize the costs of reaching y * from y ins and y rps estimated by the oracle policy œÄ * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The roll-in sequence for the insertion predictor is a stochastic mixture of the noised reference y 0 and the output by applying the model's reposition policy œÄ rps to y 0 . The roll-in sequence for the reposition predictor is a stochastic mixture of the noised reference y 0 and the output by applying the oracle placeholder insertion policy œÄ * plh and the model's token prediction policy œÄ tok to y 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: EDITOR improves BLEU over LevT for 2-10 constraints (counted pre-BPE) and beats the best AR model on 2/3 tasks with 10 constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Target word F1 score binned by word test set frequency: EDITOR improves over LevT the most for words of low or medium frequency. AR achieves higher F1 than EDITOR for words of low or medium frequency at the cost of much longer decoding time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Ro-En translation with soft lexical constraints: while LevT uses the constraints in the provided order, EDITOR's reposition operation helps generate a more fluent and adequate translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>MT Tasks: data statistics (# sentence pairs) and provenance per language pair.</figDesc><table><row><cell>We adopt the</cell></row><row><cell>decoding technique introduced by Susanto et al.</cell></row><row><cell>(2020) to enforce hard constraints at decoding</cell></row><row><cell>time by prohibiting deletion operations on con-</cell></row><row><cell>straint tokens or insertions within a multi-token</cell></row><row><cell>constraints.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>The test sets are subsets of the WMT17 En-De</cell></row><row><cell>test set (Bojar et al., 2017) with terminology con-</cell></row><row><cell>straints extracted from Wiktionary and IATE. 6 For</cell></row><row><cell>each test set, they only select the sentence pairs in</cell></row><row><cell>which the exact target terms are used in the ref-</cell></row><row><cell>erence. The resulting Wiktionary and IATE test</cell></row><row><cell>sets contain 727 and 414 sentences respectively.</cell></row><row><cell>We follow the same preprocessing steps in Gu</cell></row><row><cell>et al. (2019): we apply normalization, tokeniza-</cell></row><row><cell>tion, true-casing, and BPE (Sennrich et al., 2016b)</cell></row><row><cell>with 37k and 40k operations for En-De and Ro-En.</cell></row><row><cell>For En-Ja, we use the provided subword vocabu-</cell></row><row><cell>laries (16,384 BPE per language from Sentence-</cell></row><row><cell>Piece (Kudo and Richardson, 2018)).</cell></row><row><cell>Experimental Conditions We train and evalu-</cell></row><row><cell>ate the following models in controlled conditions</cell></row><row><cell>to thoroughly evaluate EDITOR:</cell></row><row><cell>‚Ä¢ Auto-Regressive Transformers (AR) built</cell></row><row><cell>using Sockeye (Hieber et al., 2017) and</cell></row><row><cell>fairseq (Ott et al., 2019). We report AR base-</cell></row><row><cell>lines with both toolkits to enable fair com-</cell></row><row><cell>parisons when using our fairseq-based im-</cell></row><row><cell>plementation of EDITOR and Sockeye-based</cell></row></table><note>De test sets with terminology constraints released by<ref type="bibr" target="#b16">Dinu et al. (2019)</ref>. Machine Translation Results. For each metric, we underline the top scores among all models and boldface the top scores among NAR models based on the paired bootstrap test with p &lt; 0.05<ref type="bibr" target="#b14">(Clark et al., 2011)</ref>. EDITOR decodes 6-7% faster than LevT on Ro-En and En-De, and 33% faster on En-Ja, while achieving comparable or higher BLEU and RIBES.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>on RIBES) and faster decoding speed. Compared to the AR model Distill Beam BLEU ‚Üë RIBES ‚Üë CPR ‚Üë Latency (ms) ‚Üì Machine Translation with soft lexical constraints (averages over 5 runs). For each metric, we underline the top scores among all models and boldface the top scores among NAR models based on the independent Student's t-test with p &lt; 0.05. EDITOR exploits constraints better than LevT. It also achieves comparable RIBES to the best AR model with 6-7 times decoding speedup.</figDesc><table><row><cell></cell><cell>AR + DBA (sockeye)</cell><cell>4</cell><cell>31.0</cell><cell>79.5</cell><cell>99.7</cell><cell>436.26</cell></row><row><cell>Ro-En</cell><cell>AR + DBA (sockeye) NAR: LevT</cell><cell>10 -</cell><cell>34.6 31.6</cell><cell>84.5 83.4</cell><cell>99.5 80.3</cell><cell>696.68 121.80</cell></row><row><cell></cell><cell>+ hard constraints</cell><cell>-</cell><cell>27.7</cell><cell>78.4</cell><cell>99.9</cell><cell>140.79</cell></row><row><cell></cell><cell>NAR: EDITOR</cell><cell>-</cell><cell>33.1</cell><cell>85.0</cell><cell>86.8</cell><cell>108.98</cell></row><row><cell></cell><cell>+ hard constraints</cell><cell>-</cell><cell>28.8</cell><cell>81.2</cell><cell>95.0</cell><cell>136.78</cell></row><row><cell></cell><cell>AR + DBA (sockeye)</cell><cell>4</cell><cell>26.1</cell><cell>74.7</cell><cell>99.7</cell><cell>434.41</cell></row><row><cell>En-De</cell><cell>AR + DBA (sockeye) NAR: LevT</cell><cell>10 -</cell><cell>30.5 27.1</cell><cell>81.9 80.0</cell><cell>99.5 75.6</cell><cell>896.60 127.00</cell></row><row><cell></cell><cell>+ hard constraints</cell><cell>-</cell><cell>24.9</cell><cell>74.1</cell><cell>100.0</cell><cell>134.10</cell></row><row><cell></cell><cell>NAR: EDITOR</cell><cell>-</cell><cell>28.2</cell><cell>81.6</cell><cell>88.4</cell><cell>121.65</cell></row><row><cell></cell><cell>+ hard constraints</cell><cell>-</cell><cell>25.8</cell><cell>77.2</cell><cell>96.8</cell><cell>134.10</cell></row><row><cell></cell><cell>AR + DBA (sockeye)</cell><cell>4</cell><cell>44.3</cell><cell>81.6</cell><cell>100.0</cell><cell>418.71</cell></row><row><cell>En-Ja</cell><cell>AR + DBA (sockeye) NAR: LevT</cell><cell>10 -</cell><cell>48.0 42.8</cell><cell>85.9 84.0</cell><cell>100.0 74.3</cell><cell>736.92 161.17</cell></row><row><cell></cell><cell>+ hard constraints</cell><cell>-</cell><cell>39.7</cell><cell>77.4</cell><cell>99.9</cell><cell>159.27</cell></row><row><cell></cell><cell>NAR: EDITOR</cell><cell>-</cell><cell>45.3</cell><cell>85.7</cell><cell>91.3</cell><cell>109.50</cell></row><row><cell></cell><cell>+ hard constraints</cell><cell>-</cell><cell>43.7</cell><cell>82.6</cell><cell>96.4</cell><cell>132.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>-Path Roll-In Ablation experiments (Table5) show that EDITOR benefits Term usage percentage (Term%) and BLEU scores of En-De models on terminology test sets</figDesc><table><row><cell></cell><cell cols="3">BLEU‚Üë RIBES‚Üë CPR‚Üë</cell><cell>Lat.‚Üì</cell></row><row><cell>Ro-En</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EDITOR</cell><cell>33.1</cell><cell>85.0</cell><cell cols="2">86.8 108.98</cell></row><row><cell>-dual-path</cell><cell>32.2</cell><cell>84.4</cell><cell cols="2">74.8 119.61</cell></row><row><cell></cell><cell>31.6</cell><cell>83.4</cell><cell cols="2">80.3 121.80</cell></row><row><cell>En-De</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EDITOR</cell><cell>28.2</cell><cell>81.6</cell><cell cols="2">88.4 121.65</cell></row><row><cell>-dual-path</cell><cell>27.2</cell><cell>80.4</cell><cell cols="2">78.7 130.85</cell></row><row><cell>LevT</cell><cell>27.1</cell><cell>80.0</cell><cell cols="2">75.6 127.00</cell></row><row><cell>En-Ja</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EDITOR</cell><cell>45.3</cell><cell>85.7</cell><cell cols="2">91.3 109.50</cell></row><row><cell>-dual-path</cell><cell>44.0</cell><cell>83.9</cell><cell cols="2">80.0 154.10</cell></row><row><cell>LevT</cell><cell>42.8</cell><cell>84.0</cell><cell cols="2">74.3 161.17</cell></row><row><cell cols="5">Table 5: Ablating the dual-path roll-in policy hurts</cell></row><row><cell cols="5">EDITOR on soft-constrained MT, but still outper-</cell></row><row><cell cols="5">forms LevT, confirming that reposition and dual-</cell></row><row><cell cols="4">path imitation learning both benefit EDITOR.</cell></row><row><cell cols="5">greatly from dual-path roll-in. Replacing dual-</cell></row><row><cell cols="5">path roll-in with the simpler roll-in policy used</cell></row><row><cell cols="5">in Gu et al. (2019), model's translation quality</cell></row><row><cell cols="5">drops significantly (by 0.9-1.3 on BLEU and 0.6-</cell></row><row><cell cols="5">1.9 on RIBES) with fewer constraints preserved</cell></row><row><cell cols="5">and slower decoding. It still achieves better trans-</cell></row><row><cell cols="5">lation quality than LevT thanks to the reposi-</cell></row><row><cell cols="5">tion operation: specifically, it yields significantly</cell></row><row><cell cols="5">higher BLEU and RIBES on Ro-En, comparable</cell></row><row><cell cols="5">BLEU and significantly higher RIBES on En-De,</cell></row><row><cell cols="5">and comparable RIBES and significantly higher</cell></row><row><cell cols="2">BLEU on En-Ja than LevT.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Empirically, fewer than 1% of tokens are repositioned to more than one output position.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">In our implementation, we set Kmax = 255.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Different from the inference process, we generate the roll-in sequences by applying the model's reposition or insertion policy for only one iteration.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"> Following Stern et al. (2019), we also experiment with adding penalty for inserting "empty" placeholders during inference by subtracting a penalty score Œ≥ = [0, 3] from the logits of zero in Eq. (2) to avoid overly short outputs. However, preliminary experiments show that zero penalty score achieves the best performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">Available at https://www.wiktionary.org/ and https://iate.europa.eu.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">Our preliminary experiments and prior work show that NAR models require larger training batches than AR models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6">This teacher model was selected for a fairer comparison on MT with soft lexical constraints.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7">https://github.com/pytorch/fairseq/ blob/master/fairseq/clib/libbleu/ libbleu.cpp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8">http://www.phontron.com/kytea/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"> Kasai et al. (2020)  found that AR models can benefit from knowledge distillation but with a Transformer large model as a teacher, while we use the Transformer base model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10">Although the beam pruning option in<ref type="bibr" target="#b36">Post and Vilar (2018)</ref> is not used here (since it is not supported in Sockeye anymore), other Sockeye updates improve efficiency. Constrained decoding with DBA is 1.8-2.7 times slower than unconstrained decoding here, while DBA is 3 times slower when beam = 10 in<ref type="bibr" target="#b36">Post and Vilar (2018)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Kiant√© Brantley, Hal Daum√© III, Ani  Nenkova, Cindy Robinson, Fran√ßois Yvon, and   the CLIP lab at UMD for their helpful and constructive comments. This research is supported in part by an Amazon Web Services Machine Learning Research Award and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract #FA8650-17-C-9117. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generation of formal and informal sentences</title>
		<author>
			<persName><forename type="first">Abu</forename><surname>Fadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Sheikha</surname></persName>
		</author>
		<author>
			<persName><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
				<meeting>the 13th European Workshop on Natural Language Generation<address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="187" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Guided open vocabulary image captioning with constrained beam search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1098</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="936" to="945" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incorporating discrete translation lexicons into neural machine translation</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1557" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3th International Conference on Learning Representations</title>
				<meeting>the 3th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical machine translation through global lexical selection and sentence reconstruction</title>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Kanthak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
				<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Re</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical approaches to computer-assisted translation</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Barrachina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elsa</forename><surname>Cubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahram</forename><surname>Khadivi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Lagarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jes√∫s</forename><surname>Tom√°s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan-Miguel</forename><surname>Vilar</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.2008.07-055-R2-06-29</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName><forename type="first">Ond≈ôej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ale≈°</forename><surname>Tamchyna</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-3302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
				<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Findings of the 2017 conference on machine translation (WMT17)</title>
		<author>
			<persName><forename type="first">Ond≈ôej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4717</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
				<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="169" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation</title>
		<author>
			<persName><forename type="first">Ond≈ôej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><forename type="middle">Jimeno</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur√©lie</forename><surname>N√©v√©ol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
				<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietra</forename><surname>Della</surname></persName>
		</author>
		<imprint>
			<publisher>Fredrick</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A statistical approach to machine translation</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><surname>Roossin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Ching-An</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Wagener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Boots</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10413</idno>
		<title level="m">Fast policy learning through imitation and reinforcement</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri√´nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-4012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
				<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daum√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MLJ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training neural machine translation to apply terminology constraints</title>
		<author>
			<persName><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1294</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3063" to="3068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The operation sequence Model-Combining ngram-based and phrase-based statistical machine translation</title>
		<author>
			<persName><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch√ºtze</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00218</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="186" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Controlling linguistic style aspects in neural language generation</title>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4912</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Stylistic Variation</title>
				<meeting>the Workshop on Stylistic Variation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">User-friendly text prediction for translators</title>
		<author>
			<persName><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Langlais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
				<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1633</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sockeye: A toolkit for neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<idno>CoRR, abs/1712.05690</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Vilar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc. Felix Hieber</publisher>
			<date type="published" when="2017">2019. 2017</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11181" to="11191" />
		</imprint>
	</monogr>
	<note>Levenshtein transformer</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lexically constrained decoding for sequence generation using grid beam search</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1535" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic evaluation of translation quality for distant language pairs</title>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010</title>
				<meeting>the 2010</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Katsuhito Sudoh, and Hajime Tsukada</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SEARNN: Training RNNs with global-local losses</title>
		<author>
			<persName><forename type="first">R√©mi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1149</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
				<imprint>
			<date type="published" when="1966">1966</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FlowSeq: Non-autoregressive conditional sequence generation with generative flow</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1437</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4282" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using LSTMs with coarseto-fine alignment</title>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1086</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Overview of the 4th workshop on Asian translation</title>
		<author>
			<persName><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Higashiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideya</forename><surname>Mino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Asian Translation (WAT2017)</title>
				<meeting>the 4th Workshop on Asian Translation (WAT2017)<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="54" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">compare-mt: A tool for holistic comparison of language generation systems</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danish</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving lexical choice in neural machine translation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parallel WaveNet: Fast high-fidelity speech synthesis</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning<address><addrLine>Alex Graves, Helen King, Tom Walters, Dan Belov; Stockholmsm√§ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3918" to="3926" />
		</imprint>
	</monogr>
	<note>and Demis Hassabis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
				<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast lexically constrained decoding with dynamic beam allocation for neural machine translation</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Computational</title>
				<meeting>the 15th Conference of the European Chapter of the Association for Computational Computational</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Reinforcement and imitation learning via interactive no-regret learning</title>
		<author>
			<persName><forename type="first">St√©phane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<idno>CoRR, abs/1406.5979</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning simplifications for specific target audiences</title>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2113</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="712" to="718" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Controlling politeness in neural machine translation via side constraints</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Code-switching for enhancing NMT with pre-specified translation</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="449" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An operation sequence model for explainable neural machine translation</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5420</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Insertion transformer: Flexible sequence generation via insertion operations</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5976" to="5985" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Blockwise parallel decoding for deep autoregressive models</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10086" to="10095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lexically constrained neural machine translation with Levenshtein transformer</title>
		<author>
			<persName><forename type="first">Raymond Hendy</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liling</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3536" to="3543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Neural machine translation with external phrase memory</title>
		<author>
			<persName><forename type="first">Yaohua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip Lh</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01792</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semi-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">Chunqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nonautoregressive machine translation with auxiliary regularization</title>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5377" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Non-monotonic sequential text generation</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiant√©</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><forename type="middle">Daum√©</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6716" to="6726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Utilisation de ressources lexicales et terminologiques en traduction neuronale</title>
		<author>
			<persName><forename type="first">Fran√ßois</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadaf</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rauf</forename></persName>
		</author>
		<idno>2020-001</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>LIMSI-CNRS</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
