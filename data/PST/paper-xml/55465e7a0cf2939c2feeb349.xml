<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-Stage Learning to Predict Human Eye Fixations via SDAEs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<email>junweihan2010@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Georgia</orgName>
								<address>
									<postCode>30602</postCode>
									<settlement>Athens</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shifeng</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Georgia</orgName>
								<address>
									<postCode>30602</postCode>
									<settlement>Athens</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Georgia</orgName>
								<address>
									<postCode>30602</postCode>
									<settlement>Athens</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Tianming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<email>xuelong_li@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for OPTical IMagery Analysis and Learning (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hoffmann</forename><forename type="middle">J</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Georgia</orgName>
								<address>
									<postCode>30602</postCode>
									<settlement>Athens</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two-Stage Learning to Predict Human Eye Fixations via SDAEs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">12DAF0E4A40C5CE05CF638EB7D4C817D</idno>
					<idno type="DOI">10.1109/TCYB.2015.2404432</idno>
					<note type="submission">received June 12, 2014; revised November 12, 2014 and January 10, 2015; accepted February 3, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep networks</term>
					<term>eye fixation prediction</term>
					<term>saliency detection</term>
					<term>stacked denoising autoencoders (SDAEs)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Saliency detection models aiming to quantitatively predict human eye-attended locations in the visual field have been receiving increasing research interest in recent years. Unlike traditional methods that rely on hand-designed features and contrast inference mechanisms, this paper proposes a novel framework to learn saliency detection models from raw image data using deep networks. The proposed framework mainly consists of two learning stages. At the first learning stage, we develop a stacked denoising autoencoder (SDAE) model to learn robust, representative features from raw image data under an unsupervised manner. The second learning stage aims to jointly learn optimal mechanisms to capture the intrinsic mutual patterns as the feature contrast and to integrate them for final saliency prediction. Given the input of pairs of a center patch and its surrounding patches represented by the features learned at the first stage, a SDAE network is trained under the supervision of eye fixation labels, which achieves both contrast inference and contrast integration simultaneously. Experiments on three publically available eye tracking benchmarks and the comparisons with 16 state-of-the-art approaches demonstrate the effectiveness of the proposed framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A N IMPORTANT ability of the human visual system is to select a small subset of the massive amount of visual input to fully process while the redundant information is filtered out. The selection mechanism in the brain is often named visual attention, which involves two forms of attention: 1) bottom-up saliency-driven attention and 2) top-down task-driven attention. The former is rapid and decided by exogenous stimulus from the environment. The latter is slower and guided by endogenous stimulus.</p><p>The first attempt to study visual attention can be traced back to 1890 by William <ref type="bibr" target="#b0">[1]</ref>. In the computer science field, the study on computational modeling of bottom-up visual attention has gained an increasing interest in recent years. This paper normally automatically establishes saliency models to quantitatively predict attended locations in the visual field based on computer vision techniques. We observe that the research on saliency models has been categorized into two directions: 1) eye fixation prediction and 2) salient object detection. The former focuses on identifying a number of human fixation locations when they are viewing natural scenes, while the latter aims to accurately extract objects that grab human attention. The saliency models have benefited a wide range of multimedia applications such as image segmentation <ref type="bibr" target="#b1">[2]</ref>, image collage <ref type="bibr" target="#b2">[3]</ref>, image categorization <ref type="bibr" target="#b3">[4]</ref>, weakly supervised learning <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, object retrieval/detection <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, and video summarization <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>This paper focuses on the eye fixation prediction task. Motivated by the evidence that locations in the visual field that are distinctive from their surroundings are more likely to attract human attention, most existing fixation prediction models mainly deal with the problem of measuring the distinctiveness at a location <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Previous approaches typically address the problem of saliency modeling in three components: 1) early feature extraction; 2) feature contrast inference; and 3) contrast integration. For the early feature extraction component, a variety of features have been used to represent image content. For example, Itti et al. <ref type="bibr" target="#b16">[17]</ref> presented a set of biologically plausible early features including color, intensity, and orientation. In <ref type="bibr" target="#b17">[18]</ref>, more early features were utilized including the local energy of the steerable pyramid filters, sub-band pyramids-based features, and 3-D color histogram. All the above-mentioned features are hand-crafted features. To obtain powerful hand-crafted features, domain-specific knowledge is usually required. However, there is still a lack of thorough understanding of the biological mechanisms and knowledge of human visual attention. Additionally, those classical hand-crafted features may not be universally suitable to every type of image. Recently, although a few learning-based feature extraction approaches using sparse coding <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and lowrank matrix recovery <ref type="bibr" target="#b20">[21]</ref> are proposed, they adopt shallowstructured architectures which have limited representational power and may be insufficient to capture high-level information and latent patterns of complex image. Hence, in this paper, we propose to learn good features directly from the raw pixels of the image using a deep network in an unsupervised manner. Inferring contrast over early features is another key component in saliency models. The most influential mechanism called "center-surround difference" was invented by Itti et al. <ref type="bibr" target="#b16">[17]</ref>. It utilized a "difference of Gaussians" operator to compute the contrast between a center location and its surrounding areas across multiple scales. A group of approaches <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref> measured the contrast in frequency domain, which manipulate the image's frequency spectrum to highlight sparse salient locations. Recently, Han et al. <ref type="bibr" target="#b25">[26]</ref> applied sparse coding techniques to encode a center location by the dictionary trained on its surrounding locations. The coding sparseness and residual are combined to reflect the contrast. In general, most existing approaches adopt the human-designed mechanisms to calculate the contrast. These human-designed mechanisms may be insufficient to handle large-scale data with complex distributions. In this paper, we propose a new idea that develops a supervised deep networks to learn an optimal contrast measurement directly from pairs of training data. The supervised deep learning gives our contrast measurement better discriminative ability.</p><p>The last component for saliency modeling is to integrate various contrast features or contrast features with other factors into a saliency map. In <ref type="bibr" target="#b16">[17]</ref>, linearly combined three different contrast features with a set of predefined weights. Zhao and Koch <ref type="bibr" target="#b26">[27]</ref> adopted a least-square technique to learn the optimal weights upon a set of eye tracking data. Judd et al. <ref type="bibr" target="#b17">[18]</ref> and Liu et al. <ref type="bibr" target="#b27">[28]</ref> applied supervised learning algorithms to combine contrasts, respectively. A number of approaches <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> integrate the contrast feature with specific features for directing attention like face, human, car, and text to generate the saliency map. In this paper, we learn the integration component in saliency model by adding a logistic regression layer on top of the deep network. Thus, it leads to a unified deep network that learns the inference and integration components jointly under the supervision of the labels of the input data.</p><p>Although extensive efforts have been devoted to eye fixation prediction, the performance of existing approaches can still be substantially improved. Since previous methods heavily depend on hand-designed features and contrast calculation mechanisms, we argue that those traditional hand-designed features and mechanisms may not be universally suitable to every type of image. It results in the fact that previous approaches have the weak ability to handle large-scale data with complex distribution. In response, this paper proposes to learn better feature representation, contrast inference, and contrast integration directly from the natural image stimulus as well as the corresponding eye fixation data by using deep learning networks. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the proposed method implements these critical components in a two-stage learning framework. At the first learning stage, we develop a stacked denoising autoencoder (SDAE) model for the objective of learning robust, representative features from raw data. Here, we train the deep network by using the layer-wise unsupervised learning scheme because we require the learned representation to be generative, which can produce good reconstructions of the input data. The second learning stage aims to jointly learn an optimal mechanism to capture the intrinsic mutual patterns as the feature contrast and integrate them for final saliency prediction. Given the input of center-surrounding (CS) pairs (a center patch and its surrounding patches), a SDAE network is trained under the supervision of eye fixation labels to accomplish both contrast inference and contrast integration simultaneously. The use of deep and wide network architecture enables us to characterize center-surround difference from different aspects with good robustness. It also gives our model the better capability to handle large-scale data with complex distributions. Moreover, the supervised learning manner ensures the learned contrast inference mechanism is discriminative and the contrast integration mechanism is robust.</p><p>To be best of our knowledge, the proposed framework is among the earliest efforts of applying deep learning approaches to eye fixation prediction. We have noticed that Shen et al. <ref type="bibr" target="#b30">[31]</ref> also used deep networks for saliency detection. However, the underlying premise of this paper and <ref type="bibr" target="#b30">[31]</ref> are quite different. Reference <ref type="bibr" target="#b30">[31]</ref> is built on the view that visual attention is directed by semantic and subject-dependent factors, such as faces, cars, and other objects with semantic meaning. Therefore, it applies a single two-layer sparse coding network for learning semantic high-level representation as the absolute feature for image patches. In comparison, this paper is built on the underlying and widely accepted biological evidence that guiding bottom-up attention is feature contrast rather than absolute feature strength <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b31">[32]</ref> and we adopt two separated SDAEs to model the feature contrast.</p><p>In summary, the novelties and contributions of this paper are threefold.</p><p>1) The dominant solutions for eye fixation prediction rely on hand-designed features and contrast inference mechanisms, which are developed based on limited understanding of the biological knowledge about human visual attention. Therefore, existing models may not achieve universal good performance for each type of data. In contrast, this paper explores a novel insight that learns features and contrast inference mechanisms from image data itself by using deep learning approaches. The better results compared with a number of state-of-the-art algorithms demonstrate the feasibility and effectiveness of this paper. This paper opens a novel window to address saliency detection problem and to understand the human visual attention system. 2) We develop a layer-wise unsupervised learning scheme to train a SDAE in the first learning stage for obtaining robust representative features, which can capture the generative patterns of image patches. 3) Different from the traditional methods which process the contrast inference component and contrast integration component separately, the proposed approach embeds these two components in a unified deep SDAE network. Under the supervision of the eye fixation data, complex mutual relations among the center patch and its surrounding patches can be learned and final saliency prediction is achieved based on the optimized contrast inference and integration mechanism. The rest of this paper is organized as follows. Section II introduces related work. Section III describes the details of the proposed approach. Section IV presents the experiments. Finally, the conclusion is drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Generally, previous works can be categorized into three classes: 1) local contrast-based methods; 2) global contrastbased methods; and 3) hybrid method in which local contrast and global contrast are integrated or contrast and other factors for directing attention are integrated. This paper only provides a brief review of related works that are most relevant to this paper. More comprehensive review can be found in <ref type="bibr" target="#b32">[33]</ref>.</p><p>Local contrast-based methods measure the saliency of an image location by computing the contrast against its local and small neighborhood. As a milestone work, Itti et al. <ref type="bibr" target="#b16">[17]</ref> invented the center-surround difference operator to derive the contrasts in color, intensity, and orientation. These three contrasts are then linearly combined into a saliency map. Zhao and Koch <ref type="bibr" target="#b26">[27]</ref> improved Itti's work by using a leastsquare technique to learn the optimal weights for different contrasts. Gao et al. <ref type="bibr" target="#b33">[34]</ref> extended Itti's work by modeling a discriminant center-surround hypothesis which assumes salient locations enable the discrimination between center and surround with the smallest expected probability of error. Han et al. <ref type="bibr" target="#b25">[26]</ref> proposed a sparse coding-based approach to calculate the local contrast, where the center location is encoded by the dictionary trained on its surrounding locations. The coding sparseness and residual are combined to measure the center-surround difference. Seo and Milanfar <ref type="bibr" target="#b34">[35]</ref> explored the local regression kernels to measure the similarity between a location and its neighboring surrounds. Garcia-Diaz et al. <ref type="bibr" target="#b12">[13]</ref> proposed an "adaptive whitening saliency" (AWS) model based on the decorrelation and the distinctiveness of local responses.</p><p>Alternatively, another school of approaches use the global contrast that calculates rarity of locations over the entire image for saliency prediction. Zhang et al. <ref type="bibr" target="#b35">[36]</ref> proposed to model the rarity by using a generalized Gaussian model. Han et al. <ref type="bibr" target="#b19">[20]</ref> firstly extracted features using sparse coding techniques and then applied Gaussian mixture models to calculate the rarity over features. Hou and Zhang <ref type="bibr" target="#b18">[19]</ref> and Bruce and Tsotsos <ref type="bibr" target="#b36">[37]</ref> measured the global contrast based on the information theoretic model which regards the computation of saliency as the process of maximizing information sampled from one's environment. A number of methods efficiently compute the global contrast in frequency domain. Specifically, Hou and Zhang <ref type="bibr" target="#b21">[22]</ref> measured the global contrast as the difference between the raw and the smoothed log Fourier amplitude spectrum of an image. Guo et al. <ref type="bibr" target="#b22">[23]</ref> incorporated the phase spectrum of the Fourier transform rather than the amplitude transform, which achieves better performance. Recently, Hou et al. <ref type="bibr" target="#b23">[24]</ref> leveraged the sign of each discrete cosine transform component and Schauerte and Stiefelhagen <ref type="bibr" target="#b24">[25]</ref> proposed to use the eigenaxis and eigenangles based on the quaternion Fourier transform to discover the salient locations, respectively. Li et al. <ref type="bibr" target="#b37">[38]</ref> performed the convolution of the image amplitude spectrum with a low-pass Gaussian kernel of an appropriate scale to detect saliency. Lately, an interesting idea proposed in <ref type="bibr" target="#b20">[21]</ref> addressed the saliency detection problem via matrix decomposition where the input image matrix consists of the regular part represented by a low-rank matrix and the salient part represented by a sparse matrix.</p><p>The third category of methods generally combines the global contrast and local contrast or combines contrast and other cues of directing attention. Borji and Itti <ref type="bibr" target="#b13">[14]</ref> combined local contrast and global contrast measured in the red green blue and Lab color space to yield the saliency map. Liu et al. <ref type="bibr" target="#b27">[28]</ref> developed a conditional random field model to integrate three types of features such as multiscale contrast, center-surround histogram, and color spatial distribution which represent salient objects locally, regionally, and globally, respectively. Judd et al. <ref type="bibr" target="#b17">[18]</ref> combined a number of contrast features and a few factors including face, text, car, and horizontal line to predict eye fixations. Likewise, IEEE TRANSACTIONS ON CYBERNETICS Cerf et al. <ref type="bibr" target="#b29">[30]</ref> fused the low-level contrast features and face detector to generate the saliency map. Geferman et al. <ref type="bibr" target="#b14">[15]</ref> developed the saliency detection model by considering four aspects of features including local contrast, global contrast, visual organization, and high-level factors such as human faces. Recently, Borji <ref type="bibr" target="#b28">[29]</ref> built super vector machine and AdaBoost classifiers for saliency detection based on the combination of low-level features including contrast and saliency maps obtained by state-of-the-art models, and high-level cues including horizontal line, person, car, and face.</p><p>A number of deep learning algorithms <ref type="bibr" target="#b38">[39]</ref> have been proposed in recent years toward learning more informative features from a large number of unlabeled data in an unsupervised manner. For tasks such as classification and recognition <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, the entire deep network can be trained end-to-end, which integrates the feature extraction and classifier establishment into a unified hierarchical model. The method of <ref type="bibr" target="#b41">[42]</ref> also developed a hierarchical model to infer features and recognize actions in which local and global regression model are considered as two different layers. The remarkable results have been achieved. Besides unsupervised learning and supervised learning, <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b45">[46]</ref> focused on applying deep learning methods to tackle the challenges in transfer learning. Typically, transfer learning is achieved by learning a transformation of the raw input data among the source domain into a new space of the target domain. Deep learning models with hierarchical architectures have been demonstrated to represent higher-level abstractions or concepts which capture the most important factors of the unknown distribution and share statistical strength across different but related types of data with an overlapping but different distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>In this section, we elaborate on the proposed two-stage learning to predict human eye fixation. We firstly introduce the basic idea of SDAE algorithm since it is adopted by our two learning stages. Next, detailed description of each learning stage is presented. The explanation of the testing stage is described in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SDAE</head><p>Autoencoder is one type of neural network using a set of encoder weights to convert input vector into code vector, and then utilizing another set of decoder weights to map the code vector into an approximate reconstruction of the input vector <ref type="bibr" target="#b46">[47]</ref>. In this paper, we are interested in adopting autoencoder to learn a mapping from input to new representations. A good representation is expected to meet three important criteria: 1) a significant amount of the information about the original input should be retained during feature representation <ref type="bibr" target="#b46">[47]</ref>; 2) the learned feature should be sparse enough for powerful representation, which can represent a high-dimensional original signal by using a few representative atoms on a low-dimensional manifold <ref type="bibr" target="#b47">[48]</ref>; 3) the learned representation should be robust enough for undoing the effect of stochastic corruptions of the inputs and be useful for recovering the corresponding clean input <ref type="bibr" target="#b48">[49]</ref>. In order to meet these requirements, stochastic mapping and sparsity constraint are added into the classical autoencoder for generating denoising autoencoder which can capture the informative hidden patterns and obtain powerful representation.</p><p>Specifically, the framework of autoencoder is made up of two parts: 1) encoding process and 2) decoding process. Before encoding, the original input data x i is corrupted into xi by means of a stochastic mapping xi = qD(x i |x i ), in which some of the input units are set to be zero randomly <ref type="bibr" target="#b48">[49]</ref>. Then, the encoder procedure is defined as a nonlinear affine mapping function f (x i , θ f ), which transforms the corrupted input xi into a hidden representation y i by (1) xi + b (1)  (1</p><formula xml:id="formula_0">y i = f xi , θ f = sigm W</formula><formula xml:id="formula_1">)</formula><p>where θ f is the parameter set including a project weight matrix W 1 and an offset vector b 1 . As one of the most typical way, the sigmoid function sigm(υ) = 1/(1 + exp(-υ)) is used here for the deterministic mapping. A decoder is then illustrated as a projection which maps the hidden representation y i back to a reconstruction vector z i in a similar transformation (2) y i + b (2) .</p><formula xml:id="formula_2">z i = g y i , θ g = sigm W</formula><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>To meet the first criterion of feature representation mentioned above, features in the data can be learned by minimizing the reconstruction error of the loss function</p><formula xml:id="formula_4">L = 1 2 m i=1 ||x i -z i || 2 2 (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where m denotes the number of all the training and reconstructed data, respectively, and the learnt features are encapsulated in W (1) . Inspired by the evidence that neural activity in the brain seems to be sparse, a sparsity constraint is applied to the target activation on each hidden units in the representation layer to enhance the probability of linear separability, which is called a sparse autoencoder <ref type="bibr" target="#b49">[50]</ref>. To achieve this, meet the second criterion of feature representation, and prevent overfitting, the objective function for autoencoder learning is modified to minimize the reconstruction error with a sparsity constraint and weight decay term</p><formula xml:id="formula_6">L s = 1 2 m i=1 ||x i -z i || 2 2 + β N j=1 KL ρ|| ρj + ω T i=1 N j=1 W (1) ij 2 (4) KL ρ|| ρj = ρlog ρ ρj + (1 -ρ)log 1 -ρ 1 -ρj (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where β is the weight of the sparsity penalty aiming to control the weight of the sparsity penalty term, N is the nodes number of hidden layer which controls the representation dimension of intermediate layer, W (1)  ij denotes the value in row i and column j in matrix W (1) , ω is the coefficient of weight penalty which controls the relative importance of the three terms, T is the dimension of input or the number nodes of visible layer, ρ is the target average activation of the hidden units, and ρj = m i=1 [ y i ] j /m is the average activation of the jth hidden unit over the m training data. The Kullback-Leibler divergence KL(•) provides the sparsity constraint. As mentioned in <ref type="bibr" target="#b50">[51]</ref>, a nonredundant over-complete feature set is learned when ρ is small. Note that autoencoder with stochastic mapping is still to minimize the same reconstruction loss between the original input x i and its reconstruction from y i . Thus, the learning process involves a denoising procedure that can extract more robust and structural representation <ref type="bibr" target="#b48">[49]</ref>. It enables to meet the third criterion of feature representation. Specially, to train the autoencoder, our goal is to minimize the cost function in (4) as a function of i to a small random value near zero, and then apply the gradient descent optimization algorithm to update the corresponding parameters in iterations. The update rules are given by</p><formula xml:id="formula_8">W (l) = {W (l) ij }</formula><formula xml:id="formula_9">W (l) ij = -ε ∂L s ∂W (l) ij (6) b (l) i = -ε ∂L s ∂b (l) i (7)</formula><p>where the ε is the learning rate which controls the size of weight and bias changes in parameter iteration. The partial derivatives in ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>) can be calculated by the back-propagation algorithm <ref type="bibr" target="#b51">[52]</ref>.</p><p>Recent studies in machine learning have demonstrated that a deep or hierarchical architecture is useful to find highly nonlinear and complex patterns in data <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Motivated by these studies, we consider SDAE, in which denoising autoencoders are treated as building blocks stacked in the deep architecture (Fig. <ref type="figure" target="#fig_1">2</ref>), for performing learning tasks in this paper. Given a set of training data, the greedy layer-wise unsupervised learning allows the use of denoising autoencoders as independent blocks for training the whole deep network. The key concept in the greedy layer-wise learning is to train one layer at a time. The bottom layer denoising autoencoder is firstly trained by ( <ref type="formula">6</ref>) and <ref type="bibr" target="#b6">(7)</ref> with the original input data to obtain its hidden representations, and then obtained hidden representations are used as the input data for training the higher-level denoising autoencoder, and so on. This greedy layer-wise learning is called "pretraining" [Fig. <ref type="figure" target="#fig_1">2(a)</ref>] because it is a task-free process and focuses on hierarchical generative representation learning in an unsupervised manner.</p><p>After the layer-wise pretraining, a logistic regression layer can be added on top of autoencoders, yielding a deep neural network amenable to the task-specific supervised learning. Suppose, we have a training set {x 1 , x 2 , . . . , x m } with its label set { 1 , 2 , . . . , m }. For each input data x i∈ <ref type="bibr">[1,m]</ref> , its representation in the second hidden layer [Fig. <ref type="figure" target="#fig_1">2(b)</ref>] is noted as H V,d (x i ), where V and d indicate the parameters to be learned in the bottom two-layer neural network, which contain the weight matrix V 1 and bias d 1 between input layer and the first hidden layer, and V (2) , d (2) between the two hidden layers. In the logistic regression layer, the hypothesis function is</p><formula xml:id="formula_10">h H V,d (x i ) = 1 1 + exp -T H V,d (x i ) (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where is the model parameter to be learned in logistic regression, which is trained to minimize the cost function</p><formula xml:id="formula_12">J = - 1 m m i=1 i logh H V,d (x i ) + (1 -i )log 1 -h H V,d (x i ) + ω R-1 k=1 S k i=1 S k+1 j=1 Q (k) ij 2 (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where R is the total number of layers, S k and S k+1 are the number of nodes in layer k and k + 1 respectively, and Q (k) ij is the weight matrix between the layer k and k + 1. The training process in this phase is similar with it in the autoencoder, where V and d are initialized by the pretraining process and is initialized by small random values. Then, all these model parameters are optimized under the supervised information in the top logistic regression layer by using the gradient descent algorithm with back-propagation to minimize the cost function in <ref type="bibr" target="#b8">(9)</ref>. This supervised refinement step is called "fine-tuning" [Fig. <ref type="figure" target="#fig_1">2(b)</ref>]. From an optimization point of view, it is known that the parameters obtained from the pretraining step help the fine-tuning optimization to reduce the risk of falling into a poor local optimum <ref type="bibr" target="#b52">[53]</ref>. This makes the SDAE different from conventional models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Stage 1: Learning Feature Representation</head><p>In this section, we propose to adopt a two-layer SDAE to learn feature representation in our first learning stage. In this paper, all image pixels are represented in Commission Internationale d'Eclairage Lab color space, which is known for its perceptual uniformity <ref type="bibr" target="#b53">[54]</ref>. To train this SDAE, we randomly select 300 square image patches with the size of 8 × 8 pixels from each training image. For each image patch, we concatenate all the pixel values in each color channel (i.e., the L channel, a channel, and b channel) to form the input vector for SDAE (as shown in Fig. <ref type="figure">3</ref>). By applying the layer-wise unsupervised learning scheme to minimize the cost function in (4) and ( <ref type="formula" target="#formula_6">5</ref>) for each layer separately, the project weight matrix W (l) and offset vector b (l) in each single-layer autoencoder are obtained, which involve the representative structures Fig. <ref type="figure">3</ref>. Left: illustration of unsupervised feature learning. Right: dictionary with 400 elements learned with patches in the lab color space using SDAE. We visualize the first layer bases above. In the first layer, low-level features such as oriented and localized Gabor-like edge filters are learned. In the second layer or higher layers, we may obtain filters responded selectively to contours, corners, angles, and surface boundaries in the images. With the increase of depth, object parts, faces, or semantic information may be obtained.</p><p>as well as the generative patterns of the training image patches. After obtaining all the parameters in SDAE, we can represent the nonoverlapping image patches in each input image by a forward-propagation in the trained deep network and use the outputs in the second representation layer as the final feature representations. The architecture of feature learning in stage 1 and the low-level representation we learned are shown in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning Stage 2: Learning Mechanism for Contrast Inference and Integration</head><p>Many research results <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b54">[55]</ref> have shown that the most significant factor to direct free-viewing visual attention is contrast, which is a type of mutual information between CS region pair. In order to explore how the contrast is exactly formulated in the human attention mechanism, a large number of researchers have proposed a variety of hypotheses and designed different mechanisms for contrast inference. However, these works may be difficult to achieve satisfactory performance due to the limited understanding of human attention mechanism. To tackle this problem, we propose to adopt SDAE to abstract informative patterns hierarchically and learn complex mapping relations between the designed CS pair input data and its eye fixation labels to yield the saliency map. In this paper, contrast inference and contrast integration are addressed jointly in our second learning stage. Different from the existing works that process the contrast inference and integration in a separate manner, we argue that these two components should be established in a unified learning framework for better saliency prediction.</p><p>To accomplish this, we first collect CS pairs from each training image in the datasets. As suggested in <ref type="bibr" target="#b30">[31]</ref>, we convolve a Gaussian mask with accumulated fixation data from all the subjects on that image to form the eye fixation maps. Then, we crop each square image patch with the size of 8 × 8 pixels centered at position of local maximum with its surrounding patches as one CS pair for generating positive examples. Similarly, the negative examples are randomly Illustration of learning mechanism for contrast inference and integration. selected as CS pairs with 8 × 8 image patches centered at positions in nonfixation areas and their surrounding patches (Fig. <ref type="figure" target="#fig_3">4</ref>). Before training, all the image patches in each CS pair are represented by the features learned in the first learning stage. In order to establish the relationship between center patch and its surrounding patches, all the image patch features in one CS pair should be concatenated into a single feature vector for representing the CS pair. Because the number of surrounding patches is far more than the center patch, we firstly average the feature vectors of surrounding patches into one feature vector to address the imbalanced data dimensionality, and then concatenate it with the feature vector of the center patch. Therefore, the dimensionality of input vectors of SDAE in this learning stage is twice the dimensionality of each patch representation (Fig. <ref type="figure" target="#fig_3">4</ref>).</p><p>In this learning stage, both the layer-wise pretraining and the supervised fine-tuning are used to train the SDAE model. Specifically, the former helps to reduce the risk of falling into a poor local optimum of the whole network, while the later plays a more important role for learning complex mapping relations between the input CS pair stimulus and corresponding eye fixation label of the center patches. In this deep architecture, the mutual information among CS pairs can be inferred hierarchically among the hidden layers in SDAE. By adding a logistic regression layer on top of the denoising autoencoders, the extracted mutual patterns are integrated to generate the final saliency label. Since all the parameters in hidden layers and logistic regression layer are trained jointly based on the eye fixation labels of input CS pairs, the optimization of the contrast inference and integration are processed simultaneously in a unified network, which is quite different from the existing methods.</p><p>Scale of the surrounding area is a critical factor when generating CS pairs. Every salient object in an image can visually pop out maximally from the contextual background at a specific scale. This scale can be local, or global, or any others. Therefore, in order to achieve a reasonably good performance, we adopt a multiscale scheme. In our current implementation, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we utilize three scales, which form three sizes of surrounding area by using 8, 24, and 48 surrounding patches (each patch contains 8 × 8 pixels), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Testing Stage</head><p>For each input test image, we first divide it into image patches of 8 × 8 pixels (the red patch shown in Fig. <ref type="figure" target="#fig_0">1</ref>) with no overlap in each direction. Then, each of the image patch is treated as the center patch and the corresponding neighborhood is generated under three different scales as mentioned before (shown in Fig. <ref type="figure" target="#fig_0">1</ref>). In each scale, the image patches are represented by the features learned in the first learning stage, and then they are concatenated to generate the feature vectors for CS pairs. Putting these feature vectors into the SDAE models learned in the second learning stage, we can obtain the saliency map in each scale. Finally, the final saliency map is calculated by averaging each pixel from the saliency maps in three scales with equal weights, which is simple but effective. This method is adopted for all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>This section reports experimental results to validate the proposed algorithms. We first introduce the experimental setup which includes the introduction of eye tracking datasets, evaluation metrics, and implementation details of our algorithm. Then, comprehensive experiments are conducted to demonstrate the effectiveness of the feature representation learned in the first learning stage. Finally, the fixation prediction performance of the proposed two-stage learning framework is evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup 1) Evaluation Datasets:</head><p>We evaluated our framework on three publically available benchmark eye tracking datasets, which are Massachusetts Institute of Technology (MIT) dataset <ref type="bibr" target="#b17">[18]</ref>, Toronto dataset <ref type="bibr" target="#b36">[37]</ref>, and Cerf dataset <ref type="bibr" target="#b29">[30]</ref>. Specifically, the MIT dataset <ref type="bibr" target="#b17">[18]</ref> contains 1003 images collected from Flicker and LabelMe datasets and is the largest eye tracking dataset recorded by 15 users available in the field. It includes 779 landscape images and 228 portrait pictures with the resolution ranging from 405 × 1024 to 1024×1024 pixels. Toronto dataset <ref type="bibr" target="#b36">[37]</ref> contains 120 color images with resolution of 511 × 681 pixels which are collected from indoor and outdoor circumstance. The eye tracking data of these images are recorded by 20 different participants. Cerf dataset <ref type="bibr" target="#b29">[30]</ref> is made up of 181 images with resolution of 1024 × 768 pixels, where the objects of interest are faces and some other small objects like cell phone, toys, etc.</p><p>2) Evaluation Metrics: In order to evaluate the performance of a saliency model for eye fixation prediction, receiver operating characteristic (ROC) curve and the area under the ROC curve is adopted traditionally. However, as pointed out by Zhang et al. <ref type="bibr" target="#b35">[36]</ref>, strong center-bias may exist in human fixations and it can influence the performance of a saliency model. To tackle the effect of center-bias for fair comparison, Hou et al. <ref type="bibr" target="#b23">[24]</ref> and Borji et al. <ref type="bibr" target="#b32">[33]</ref> suggest to adopt the Shuffled area-under-the-curve (AUC) as a good measure of the power of the saliency map to accurately predict where fixations occurred on an image. For each image, the positive sample set PS is composed of the fixation points of all subjects on that image, whereas the negative sample set NS is composed of the union of all fixation points across all images from the same data set-except for the PSs. The shuffled AUC is generated by classifying the pixels in the saliency map into salience or nonsalience by varying the quantization threshold within the range [0, 255]. Under each threshold, the false positive rate (FPR) and true positive rate (TPR) are calculated by</p><formula xml:id="formula_14">TPR = |SF ∩ PS| |PS| FPR = |SF ∩ NS| |NS|<label>(10)</label></formula><p>where SF denotes for the set of segmented foreground pixels. Sweeping over thresholds, the obtained FPRs and TPRs yield an ROC curve, of which the area beneath is called shuffled AUC. In shuffled AUC, the score of chance level is 0.5, while the score of perfect prediction is 1. Noticing that shuffled AUC scores are sensitive to the level of blurring applied on the saliency maps, small Gaussian kernels with varying Gaussian blur standard deviation σ are usually convolved to smooth the obtained saliency maps as in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b53">[54]</ref> to show its influence on the shuffled-AUC score and help to find the optimal one for maximizing the performance of each saliency model. All the shuffled-AUC scores reported in our experiments are evaluated by using the code released in http://www.klab.caltech.edu/∼xhou/ with the default parameter setting 3) Implementation Details: Like in <ref type="bibr" target="#b25">[26]</ref>, the input images are down-sampled to 0.125 of original size for MIT and Cerf datasets, and 0.25 of original size for Toronto dataset. We evaluated the model in three different datasets, respectively. For each dataset, by following tenfold cross-validation approach used in <ref type="bibr" target="#b28">[29]</ref>, we averagely and randomly divided it into ten partitions. Nine partitions were used for training and the remaining one partition was used for testing. This was repeated such that each partition in the dataset is used once as the testing data. For each training, learning stages 1 (unsupervised learning) and 2 used the same set of training images. In learning stage 1, we extracted 200 patches from each image in MIT dataset, but 300 patches from each image in Cerf dataset and Toronto dataset because the Cerf dataset and Toronto dataset contain a small number of images. In learning stage 2, we extracted 40 positive and 80 negative CS pairs from each training image randomly. As suggested in <ref type="bibr" target="#b28">[29]</ref>, the input should be normalized to have zero mean and unit standard deviation on each feature dimension before each learning stage, and the same normalization parameters were used in the testing stage. We conducted tenfold cross-validation experiments five times on each dataset to verify the robustness of our method and then reported the average results in Tables <ref type="table" target="#tab_1">II</ref> and<ref type="table" target="#tab_2">III</ref>.</p><p>In deep networks, there are a number of hyperparameters affecting the performance of the model. Specifically, the hyperparameters required for training the SDAE models are the target mean activation ρ, the weight of the sparsity penalty β, the coefficient of weight penalty ω, the learning rate ε for the backpropagation optimization, as well as the number of units N in each hidden layer. According to <ref type="bibr" target="#b50">[51]</ref>, we set the five parameters empirically as shown in Table <ref type="table" target="#tab_0">I</ref>. For the other hyperparameters, we use a coordinate ascent-like method <ref type="bibr" target="#b55">[56]</ref> to optimize them for each layer.  <ref type="figure">5</ref>. Some experimental results of the LG method, the LG-deep method, and the proposed two-stage learning approach. GT denotes the ground-truth saliency map built by convolving the eye fixation locations with a Gaussian for smoothing, which is implemented by following <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation of the Feature Representation</head><p>One contribution of this paper is that we propose a novel feature representation method via unsupervised deep networks (learning stage 1) for fixation prediction models. In this section, we conducted an experiment to evaluate the effectiveness of the proposed feature representation. In our experiment, we incorporated the proposed feature representation into a state-of-the-art saliency detection model called LG <ref type="bibr" target="#b13">[14]</ref> to form a new saliency detection model. We then compared the new model with LG <ref type="bibr" target="#b13">[14]</ref> on three benchmark databases to show the effectiveness of the proposed feature representation.</p><p>The LG method <ref type="bibr" target="#b13">[14]</ref> can be briefly described as follows. It first divided the input image into nonoverlapping patches and then each patch was represented as a vector of coefficients obtained by sparse coding. Next, two contrast measures (local and global) were calculated over the sparse coding-based features. The final saliency map was built by normalizing and fusing local and global contrasts. The LG method essentially used sparse coding, an unsupervised representation learning approach with shallow architecture, to obtain powerful feature for image patch representation.</p><p>In our experiment, we utilized our feature representation obtained by two-layer SDAE to substitute the feature representation obtained by sparse coding in the LG method, which thus forms a new saliency computation model called LGdeep. Afterwards, on three benchmarks, we compared three models of LG-deep, LG, and the proposed two-stage learning model to evaluate the effectiveness of our feature representation. Fig. <ref type="figure">5</ref> shows a few experimental results obtained by various models, where "OURS" indicates the proposed model using two-stage learning and "GT" indicates the ground truth eye fixation map. Fig. <ref type="figure" target="#fig_4">6</ref> displays the shuffled AUC scores  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of the Human Eye Fixation Prediction</head><p>To demonstrate the effectiveness of the proposed algorithm in yielding the saliency map for predicting human eye fixation, we compared it with 16 state-of-the-art approaches, which includes AIM <ref type="bibr" target="#b36">[37]</ref>, AWS <ref type="bibr" target="#b12">[13]</ref>, SUN <ref type="bibr" target="#b35">[36]</ref>, SR <ref type="bibr" target="#b21">[22]</ref>, GB <ref type="bibr" target="#b54">[55]</ref>, ICL <ref type="bibr" target="#b18">[19]</ref>, LG <ref type="bibr" target="#b13">[14]</ref>, HFT <ref type="bibr" target="#b37">[38]</ref>, BMS <ref type="bibr" target="#b53">[54]</ref>, JUDD <ref type="bibr" target="#b17">[18]</ref>, CA <ref type="bibr" target="#b14">[15]</ref>, SDSR <ref type="bibr" target="#b34">[35]</ref>, PMT <ref type="bibr" target="#b56">[57]</ref>, SP-Itti <ref type="bibr" target="#b57">[58]</ref>, QDCT <ref type="bibr" target="#b24">[25]</ref>, and IS <ref type="bibr" target="#b23">[24]</ref>. These algorithms are selected for comparison mainly because: 1) they were published during the past few years; 2) they were published in major computer vision conferences or journals; 3) their source codes, executable codes, or results on benchmark datasets are publically available. Fig. <ref type="figure">7</ref> displays a number of saliency maps yielded by using the proposed method and 16 state-of-the-art Fig. <ref type="figure">7</ref>. Comparison results of 16 state-of-the-art approaches, ours, and the GT saliency map built by convolving the eye fixation locations with a Gaussian for smoothing <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b53">[54]</ref>.</p><p>algorithms. The subjective evaluations by comparing with the GT suggest that the proposed method can yield saliency maps correctly and robustly in all the datasets. As it can be observed, the AWS, CA, JUDD, LG, and SUN algorithms tend to highlight the object boundaries, which are not the real eye fixation regions in the images. The AIM, SR, IS, QDCT, and  SDSR algorithms look like to be influenced by the clustered background significantly. On the contrary, we found that the proposed method tends to be less distracted by the object boundaries as well as the highly cluttered backgrounds than most of the existing methods, and it is capable of highlighting real eye fixations.</p><p>For quantitative comparison, we evaluate the shuffled AUC scores for our approach and the other 16 state-of-the-art algorithms. We smooth the saliency maps of each method by varying the Gaussian blur standard deviation σ and show its influence on the shuffled-AUC score of each method over different datasets in Fig. <ref type="figure" target="#fig_5">8</ref>. Please note that because we performed the cross validation five times for our method, to draw the shuffled-AUC scores in Fig. <ref type="figure" target="#fig_5">8</ref>, we used the results of one cross validation which are closest to the average scores. The maximum shuffled AUC scores of each model together with the corresponding Gaussian blur σ over three datasets as well as average scores of models are reported in Table <ref type="table" target="#tab_2">III</ref>.</p><p>From Fig. <ref type="figure" target="#fig_5">8</ref> and Table <ref type="table" target="#tab_2">III</ref>, we can see that due to the variation of the selection of visual stimuli, the composition of participants and the experimental environment, evaluations on different datasets gave different ranks of various methods. The proposed method obtains the best performance on all three datasets. Specifically, the proposed method achieves significant improvement on the MIT dataset while relatively slight improvement on the Toronto and Cerf datasets. It is worth mentioning that the proposed framework is built based on deep learning networks. As we know, the performance of deep learning algorithms relies on the amount of training data to certain extent. To achieve the superior performance, deep learning models are generally required to be trained by using abundant training data. However, the capability of deep learning models may be limited when insufficient training data is available. On the MIT dataset, relatively enough training data are available (i.e., 900 training images). The proposed method thus achieves the outstanding performance. In contrast, on the Toronto and Cerf datasets, only very little training data is available (i.e., 108 and 162 training images, respectively), which results in insignificant improvement of this paper compared with existing approaches. We predict that the proposed method may perform better on these two datasets if sufficient training data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In the past 15 years, the study of saliency detection is dominated by the idea of using hand-crafted features and contrast inference mechanisms. To design powerful hand-crafted features and contrast inference mechanisms, domain-specific knowledge is usually required. Unfortunately, there is still a lack of thorough understanding of the biological knowledge of human visual attention. Therefore, previous models have not achieved satisfactory performance. In this paper, we have explored a novel view to address the saliency detection problem, which adopted deep learning networks to learn optimal features and contrast inference mechanism from image data itself. We compared the proposed work with 16 state-of-the-art models on three widely used benchmark datasets. Our experimental results have shown that the proposed outperformed all other methods.</p><p>Our future work will focus on the following directions. First, this paper mainly adopted the bottom-up contrast information. However, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> have shown other top-down factors such as face, text, person also can direct visual attention. Therefore, one extension is to incorporate top-down factors into the proposed model. Second, deep learning algorithms generally require a large number of training data to achieve good performance. Current eye tracking benchmark datasets are still relatively small, which may limit the capability of deep learning algorithms. Thus, we believe it is important to gather larger datasets in future work to give full play to the advantage of deep learning. Third, we will extend the proposed method to video saliency detection by concerning temporal information. The final extension is to apply this paper to some real-world multimedia applications such as image retrieval <ref type="bibr" target="#b58">[59]</ref> and image classification <ref type="bibr" target="#b59">[60]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of the proposed framework for eye fixation prediction.</figDesc><graphic coords="2,67.50,52.84,476.88,198.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of the SDAE and its two training schemes.</figDesc><graphic coords="5,54.99,52.84,239.28,141.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and b (l) = {b (l) i }, where l = {1, 2} indicates the representation layer and the reconstruction layer. We first initialize parameters W (l) ij and b (l)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4.Illustration of learning mechanism for contrast inference and integration.</figDesc><graphic coords="6,316.50,53.52,241.92,148.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Evaluation of the proposed feature representation over three datasets. x-axis represents the Gaussian blur standard deviation σ (in image width) by which maps are smoothed and y-axis represents the shuffled AUC score on one dataset.</figDesc><graphic coords="8,323.00,53.68,228.96,167.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Quantitative model comparisons. Fixation prediction accuracy of our saliency model along with 16 state-of-the-art models over three benchmark datasets. x-axis indicates the Gaussian blur standard deviation σ (in image width) by which maps are smoothed and y-axis indicates the shuffled-AUC score.</figDesc><graphic coords="10,58.50,53.52,495.12,112.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I HYPERPARAMETERS</head><label>I</label><figDesc>OF SDAE MODEL IN TWO LEARNING STAGES Fig.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II MAXIMUM</head><label>II</label><figDesc>PERFORMANCE OF MODELS SHOWN IN FIG. 6. NUMBERS IN THE SECOND ROW OF EACH DATASET ARE THE OPTIMAL σ WHERE MODELS TAKE THE MAXIMUM PERFORMANCE against various Gaussian blur standard deviation σ , while Table II shows the maximum shuffled AUC scores of each model together with the corresponding Gaussian blur σ . As can be seen, over all three benchmark datasets, our proposed model achieves the best performance and LG-deep achieves is the second best. The comparison between LG and LG-deep demonstrates that the proposed deep SDAE network can learn more powerful feature representation than the shallow sparse coding model. In addition, the comparison between LG-deep and the proposed method demonstrates that the learning-based contrast inference and integration mechanisms are also critical factors for further improving the performance of the model.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III MAXIMUM</head><label>III</label><figDesc>PERFORMANCE OF MODELS SHOWN IN FIG. 8. NUMBERS IN THE SECOND ROW OF EACH DATASET ARE THE OPTIMAL σ WHERE MODELS TAKE THE MAXIMUM PERFORMANCE. ACCURACIES OF THE BEST MODELS OVER EACH DATASET ARE UNDERLINED AND SHOWN IN BOLD FACE FONT</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Science Foundation of China under Grant 61473231 and Grant 61333017, and in part by the Doctoral Fund of Ministry of Education of China under Grant 20136102110037. This paper was recommended by Associate Editor F.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Principles of Psychology</title>
		<author>
			<persName><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1890">1890</date>
			<publisher>Harvard Univ. Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised extraction of visual attention objects in color images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Picture collage</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1225" to="1239" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image visual attention computation and application via the learning of object attributes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised learning for target detection in remote sensing images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="701" to="705" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3325" to="3337" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly-supervised cross-domain dictionary learning for visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Specific object retrieval based on salient regions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recognit</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1932" to="1948" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Invariant salient regions based image retrieval under viewpoint and illumination variations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video abstraction based on fMRI-driven visual attention model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">281</biblScope>
			<biblScope unit="page" from="781" to="796" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning computational models of video memorability from fMRI brain imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2014.2358647</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Saliency from hierarchical adaptation through decorrelation and variance normalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Fdez-Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dosil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="64" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting local and global patch rarities for saliency detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Background prior based salient object detection via deep reconstruction residual</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2014.2381471</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Conf. Comput. Kyoto, Japan</title>
		<meeting>12th Int. Conf. Comput. Kyoto, Japan</meeting>
		<imprint>
			<date type="published" when="2009-10">Oct. 2009</date>
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic visual attention: Searching for coding length increments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Adv</title>
		<meeting>Conf. Adv</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An object-oriented visual saliency detection framework based on sparse coding representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2009">2009-2021, Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A unified approach to salient object detection via low rank matrix recovery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Patt. Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Patt. Recognit</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="853" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Patt. Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Patt. Recognit</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Patt. Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Patt. Recognit</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image signature: Highlighting sparse salient regions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quaternion-based spectral saliency detection for eye fixation prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schauerte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="116" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bottom-up saliency based on weighted sparse coding residual</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12">Dec. 2011</date>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a saliency map using fixated locations in natural scenes</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boosting bottom-up and top-down visual features for saliency estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="438" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predicting human gaze using low-level saliency combined with face detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cerf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Einhäuser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Adv. Neural Inform</title>
		<meeting>Conf. Adv. Neural Inform</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning high-level concepts by training a deep network on eye fixations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Deep Learn. Unsupervised Feature Learn</title>
		<meeting>Deep Learn. Unsupervised Feature Learn<address><addrLine>Workshop, Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Computational modelling of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="194" to="203" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the plausibility of the discriminant center-surround hypothesis for visual saliency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SUN: A Bayesian framework for saliency using natural statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Saliency, attention, and visual search: An information theoretic approach</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual saliency based on scale-space analysis in the frequency domain</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="996" to="1010" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning of representations: Looking forward</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Stat</title>
		<meeting>Stat</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Blind image quality assessment via deep learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2014.2336852</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-feature fusion via hierarchical regression for multimedia analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="572" to="581" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. JMLR Workshops Conf</title>
		<meeting>JMLR Workshops Conf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learners benefit more from out-of-distribution examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Worshop</title>
		<meeting>Worshop</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="164" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Patt. Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Patt. Recognit</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vision</title>
		<meeting>Eur. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length, and Helmholtz free energy</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Adv</title>
		<meeting>Conf. Adv</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sparse feature learning for deep belief networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Marc'aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Adv</title>
		<meeting>Conf. Adv</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning Representations by Back-Propagating Errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Saliency detection: A Boolean map approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Adv</title>
		<meeting>Conf. Adv</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D patient data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Probabilistic multi-task learning for visual saliency estimation in video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="165" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visual saliency with statistical priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="253" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Content-based retrieval of human actions from realistic video databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">236</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning object-to-class kernels for scene classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3241" to="3253" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
