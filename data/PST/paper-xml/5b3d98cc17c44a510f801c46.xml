<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Diverse Relevance Patterns in Ad-hoc Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
							<email>fanyixing@software.ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<email>czhai@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">‡</forename><forename type="middle">2018</forename><surname>Modeling</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">†University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">‡ CAS Key Lab of Network Data Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Diverse Relevance Patterns in Ad-hoc Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3209978.3209980</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>relevance patterns</term>
					<term>ad-hoc retrieval</term>
					<term>neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Assessing relevance between a query and a document is challenging in ad-hoc retrieval due to its diverse patterns, i.e., a document could be relevant to a query as a whole or partially as long as it provides sufficient information for users' need. Such diverse relevance patterns require an ideal retrieval model to be able to assess relevance in the right granularity adaptively. Unfortunately, most existing retrieval models compute relevance at a single granularity, either document-wide or passage-level, or use fixed combination strategy, restricting their ability in capturing diverse relevance patterns. In this work, we propose a data-driven method to allow relevance signals at different granularities to compete with each other for final relevance assessment. Specifically, we propose a HIerarchical Neural maTching model (HiNT) which consists of two stacked components, namely local matching layer and global decision layer. The local matching layer focuses on producing a set of local relevance signals by modeling the semantic matching between a query and each passage of a document. The global decision layer accumulates local signals into different granularities and allows them to compete with each other to decide the final relevance score. Experimental results demonstrate that our HiNT model outperforms existing state-of-the-art retrieval models significantly on benchmark ad-hoc retrieval datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A central question in ad-hoc retrieval is how to learn a generalizable function that can well assess relevance between a query and a document. One of the major difficulties for relevance assessment lies in that there might be diverse relevance patterns between a query and a document. As revealed by the evaluation policy in TREC ad-hoc task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>, "a document is judged relevant if any piece of it is relevant (regardless of how small the piece is in relation to the rest of the document) <ref type="foot" target="#foot_0">1</ref> ". In other words, a document could be relevant to a query as a whole or partially as long as it provides sufficient information for users' needs. Such diverse relevance patterns might be highly related to the heterogeneity of long documents in ad-hoc retrieval. As discussed by Robertson and Walker <ref type="bibr" target="#b29">[30]</ref>, there are two underlying hypotheses concerning document structures in relevance judgement, i.e. verbosity hypothesis and scope hypothesis <ref type="bibr" target="#b29">[30]</ref>. With the Verbosity Hypothesis a long document might be relevant to a query as a whole, while with the Scope Hypothesis the relevant parts could be in any position of a long document, and thus it could be partially relevant to a query.</p><p>The diverse relevance patterns call for a retrieval model to be able to assess relevance at the right granularity adaptively in ad-hoc retrieval. Unfortunately, most existing retrieval models operate at a single granularity, either document-wide or passage-level. Specifically, document-wide approaches compare a document as a whole to a query. For example, most probabilistic retrieval methods (e.g., BM25 or language models) and learning-to-rank models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref> rely on document-wide feature statistics for relevance computation. Obviously, such document-wide approaches are difficult to model finer-granularity relevance signals, leading to potential biases on the competition between long and short documents <ref type="bibr" target="#b24">[25]</ref>. On the other hand, Passage-level approaches segment a document into passages and aggregate passage-level signals for relevance computation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref>. However, the performance of existing passagebased approaches is mixed when applied to a variety of test beds <ref type="bibr" target="#b34">[35]</ref> by only using simple manually designed operations over the passage-level signals. There have been a few efforts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref> trying to combine both document-wide and passage-level methods. For example, Bendersky et al. <ref type="bibr" target="#b1">[2]</ref> integrated the query-similarity on a document and its passages using document-homogeneity. Wang et al. <ref type="bibr" target="#b34">[35]</ref> combined the document retrieval results with passage retrieval results using a heuristic function <ref type="bibr" target="#b18">[19]</ref>. However, by using a fixed combination strategy, these models cannot fully capture the diverse relevance patterns for different query-document pairs. Recently, deep neural models have been applied to ad-hoc retrieval. These data-driven methods have shown their expressive power in end-to-end learning relevance matching patterns between queries and documents <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>. However, most existing neural matching models, either representation-focused <ref type="bibr" target="#b13">[14]</ref> or interactionfocused <ref type="bibr" target="#b9">[10]</ref>, belong to the document-wide approaches. For example, the representation-focused models aim to learn a document representation to compare with the query representation, while the interaction-focused models learn from a matching matrix/histogram between a document and a query. To the best of our knowledge, so far there have been no neural matching model proposed to learn relevance signals from both document-wide and passage-level explicitly for modeling diverse relevance patterns in ad-hoc retrieval.</p><p>In this paper, we propose a data-driven method to automatically learn relevance signals at different granularities (i.e. passage-level and document-wide), and allow them to compete with each other for final relevance assessment. Specifically, we propose a HIerarchical Neural maTching model (HiNT) which consists of two stacked components, namely local matching layer and global decision layer. The local matching layer focuses on producing a set of local relevance signals between a query and each passage of a document. Many well-known information retrieval (IR) heuristics that characterize the relevance matching between a query and a passage can be encoded in this layer for high quality signal generation. Specifically, we employ a spatial GRU model <ref type="bibr" target="#b33">[34]</ref> for the relevance matching between a query and a passage, which can well capture semantic relations, proximities, and term importance. The global decision layer aims to accumulate passage-level signals into different granularities and allow them to compete with each other to form the final relevance score. Flexible strategies are applied in this layer to model diverse relevance patterns. Specifically, we utilize a hybrid network architecture to accumulate local signals, and select signals from both passage-level and document-wide to generate the final relevance score.</p><p>We evaluate the effectiveness of the proposed model based on two representative ad-hoc retrieval benchmark datasets from the LETOR collection <ref type="bibr" target="#b27">[28]</ref>. For comparison, we take into account several well-known traditional retrieval models, learning to rank models, and deep neural matching models. These models belong to document-wide, passage-level and hybrid approaches. The empirical results show that our model can outperform all the baselines in terms of all the evaluation metrics. We also provide detailed analysis on HiNT model, and conducte case studies to verify the diverse relevance patterns captured by our model over different query-document pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A large number of retrieval methods have been proposed in the past few decades <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. Without loss of generality, we divide existing methods into three folds, namely document-wide approaches, passage-level approaches and hybrid approaches, based on what kind of relevance signals they rely on for relevance assessment. We will briefly review these studies in the follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document-wide Approaches</head><p>Document-wide approaches, by its name, collect and make relevance assessment based on document-wide signals. There have been a large number of retrieval models under this branch. Firstly, traditional retrieval models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref>, collect lexical matching signals (e.g., term frequency) from the whole document, and make relevance assessment under some probabilistic framework. For example, the well-known BM25 model <ref type="bibr" target="#b29">[30]</ref> collects term frequencies and document length and employs a scoring function derived under the 2poisson model to compute relevance based on these document-wide signals. Secondly, most machine learning based retrieval methods, including learning to rank models and deep learning models, are also belong to this branch. For learning to rank methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref>, they typically involve two stages, a feature construction stage and a model learning stage. The feature construction stage could be viewed as to define (or collect) relevance signals for a document given a query. Typically, there are three type of features, including query dependent features, document independent features, and query-document dependent features. Most of these features are defined at document level, such as tf-idf scores, BM25 scores, and PageRank. Based on these features, linear <ref type="bibr" target="#b14">[15]</ref> or non-linear <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38]</ref> models are learned to produce the relevance score by optimizing some ranking based loss functions. For deep learning methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>, they can be categorized into two types according to their architectures <ref type="bibr" target="#b9">[10]</ref>, namely representation-focused models and interaction-focused models. The representation-focused models, such as ARCI <ref type="bibr" target="#b12">[13]</ref> and DSSM <ref type="bibr" target="#b13">[14]</ref>, aim to learn a low-dimensional abstractive representation for the whole document, and compare it with the query representation. The interaction-focused models, such as DRMM <ref type="bibr" target="#b9">[10]</ref> and Duet <ref type="bibr" target="#b23">[24]</ref>, learn from a matching matrix/histogram between a document and a query to produce a set of document-wide matching signals for final relevance prediction. Although in interaction-focused models, document-wide signals are usually generated from local signals, there is no competition between document-wide signals and local signals for capturing diverse relevance patterns.</p><p>Since document-wide approaches take document as a whole, these methods are often difficult to model fine-granularity relevance signals. Meanwhile, by using document-wide statistics as relevance signals, it often leads to certain bias on the competition between long and short documents <ref type="bibr" target="#b24">[25]</ref>, since long documents are likely to contain stronger signals on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage-level Approaches</head><p>As opposed to document-wide methods, passage-level methods collect signals from passages to make relevance assessment on the document. Note here we focus on document retrieval using passage-level signals, and will not include the work taking passages as retrieval units <ref type="bibr" target="#b15">[16]</ref>.</p><p>In passage-level methods, documents are usually pre-segmented into small passages. Callan <ref type="bibr" target="#b3">[4]</ref> studied how passages can be defined, and how passage signals can be incorporated into document retrieval. In <ref type="bibr" target="#b19">[20]</ref>, Liu et al. computed a language model for each passage as relevance signals. The final assessment is made by choosing the highest score from all the passages. Their results showed passage-based retrieval can provide more reliable performance than As we can see, passages are convenient text units for local signal collection to support flexible relevance assessment over documents. However, previous passage-level methods often employed simplified aggregation strategies, thus cannot well capture the diverse relevance patterns for different query-document pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hybrid Approaches</head><p>There also have been a few efforts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref> trying to combine both document-wide and passage-based methods. For example, Callan <ref type="bibr" target="#b3">[4]</ref> conducted experiments on four TREC 1 and 2 collections and concluded that it was always better to combine documentwide scores and passage-level scores. A later study by Xi et al. <ref type="bibr" target="#b36">[37]</ref> re-examined fixed-size window passages on TREC 4 and 5. Contrary to Callan <ref type="bibr" target="#b3">[4]</ref>, they did not obtain an improvement by linearly combine passage-level score and document-level score. Wang et al. <ref type="bibr" target="#b34">[35]</ref> proposed a discriminative probabilistic model in capturing passage-level signals, and combined the document-level scores and passage-level scores through a heuristic function (i.e., CombMNZ function <ref type="bibr" target="#b18">[19]</ref>). However, by using a unified combination strategy, these models cannot fully capture the diverse relevance patterns in different query-document pairs, leading to the mixed performance on different datasets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HIERARCHICAL NEURAL MATCHING MODEL</head><p>In this work, we introduce a HIerarchical Neural maTching (HiNT) model for ad-hoc retrieval to explicitly model the diverse relevance patterns. In an abstract level, the model consists of two stacked components, namely local matching layer and global decision layer. The local matching layer employs deep matching networks to automatically learn the passage-level relevance signals ; The global decision layer accumulates passage-level signals into different granularities and allows them to compete with each other to generate the final relevance score. The architecture is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. We will describe these components in detail in the follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local Matching Layer</head><p>The local matching layer focuses on producing a set of passage-level relevance signals by modeling the relevance matching between a query and each passage of a document. Formally, each document D is first represented as a set of passages D = [P 1 , P 2 , ..., P K ], where K denotes the number of passages in a document. Then, a set of passage-level relevance signals E = [e 1 , e 2 , ..., e K ] are produced by applying some relevance matching model f over a query Q and each passage P i .</p><formula xml:id="formula_0">e i = f (P i , Q), i = 1, . . . , K .</formula><p>There are two major questions concerning this layer, i.e., how to define the passage P i and how to define the relevance matching model f . For passages, there have been three types of definitions: discourse, semantic, and window <ref type="bibr" target="#b3">[4]</ref>. Discourse passages are defined based upon textual discourse units (e.g., sentences, paragraphs, and sections). Semantic passages are based upon the subject or content of the text (e.g., TextTiling). Window passages are obtained based upon a number of words. Among these methods, window passage is the most widely adopted due to its simplicity but surprisingly effectiveness as demonstrated by many previous passage-level retrieval models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>For the relevance matching model, in general, any model that can address the relevance matching between a query and a passage can be leveraged here. For example, one may employ statistical language model <ref type="bibr" target="#b38">[39]</ref>, or use manually defined features <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>, or even employ some deep models for text matching <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>. However, the quality of the passage-level signals produced by the relevance matching model is a critical foundation for the final relevance assessment. Therefore, we argue that a relevance matching model should be able to encode many well-known IR heuristics to be qualified in this layer. According to previous studies, such heuristics at least include the modeling of exact matching and semantic matching <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, proximity <ref type="bibr" target="#b32">[33]</ref>, term importance <ref type="bibr" target="#b9">[10]</ref> and so on. Based on these above ideas, in this work, we propose to use fixed-size window to define the passage, and employ an existing spatial GRU model <ref type="bibr" target="#b33">[34]</ref> for the relevance matching between a query and each passage. This spatial GRU model is good at modeling the matching between two pieces of texts based on primitive word features, and has shown better performances as compared with other deep matching models <ref type="bibr" target="#b33">[34]</ref>. We now describe the specific implementation in the follows.</p><p>3.1.1 The Input Layer. Following the idea in <ref type="bibr" target="#b10">[11]</ref>, term vectors are employed as basic representations so that rich semantic relations between query and document terms can be captured. Formally, both query and document are represented as a sequence of term vectors denoted by</p><formula xml:id="formula_1">Q = [w (Q ) 1 , ..., w (Q ) M ] and D = [w (D) 1 , ..., w (D)</formula><p>N ], where w (Q ) i , i = 1, ..., M and w (D) j , j = 1, ..., N denotes a query term vector and a document term vector, respectively. To obtain the passages, we follow previous approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35]</ref> to use fixed-size sliding window to segment the document into passages. In this way, the passage is defined as P = [w (P ) 1 , ..., w (P ) L ], where L denotes the window size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Deep Relevance Matching</head><p>Network. The architecture of the relevance matching network is shown in Figure <ref type="figure" target="#fig_1">2</ref>. Firstly, the termlevel interaction matrix is constructed based on the term vectors from the query-passage pair. Here we constructed two matching matrices, a semantic matching matrix M cos and an exact matching matrix (i.e., xor-matrix) M xor , defined as follows:</p><formula xml:id="formula_2">M cos i j = w (Q ) i w (P ) j |w (Q ) i | • |w (P ) j | , M xor i j = 1, i f w (Q ) i = w (P ) j 0, otherwise .</formula><p>The key idea of two input matrices is to distinguish the exact matching signals from the semantic matching signals explicitly since the former provides critical information for ad-hoc retrieval as suggested by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. Note that in M cos exact matching and semantic matching signals are mixed together. To further incorporate term importance, we extend each element of M i j to a three-dimensional vector S i j = [x i , y j , M i j ] by concatenating two corresponding compressed term embeddings as in <ref type="bibr" target="#b26">[27]</ref>, where x i = w Q i * W s and y j = w (P ) j * W s , here, W s is the transformation parameter to be learned.</p><p>Based on these two query-passage interaction tensors, a spatial GRU (Gated Recurrent Units) is applied to generate the relevance matching evidences. The spatial GRU, also referred to as 2-dimensional Gated-RNN, is a special case of multidimensional RNN <ref type="bibr" target="#b8">[9]</ref>. It is a recursive model which scans the input tensor from top left to bottom right:</p><formula xml:id="formula_3">− → H cos i j = д( − → H cos i−1, j , − → H cos i, j−1 , − → H cos i−1, j−1 , S cos i j ), − → H xor i j = д( − → H xor i−1, j , − → H xor i, j−1 , − → H xor i−1, j−1 , S xor i j ),</formula><p>where д denotes the spatial GRU unit as described in <ref type="bibr" target="#b33">[34]</ref>, − → H cos i j and − → H xor i j denotes the hidden state of the spatial GRU over S cos and S xor , respectively. We can take the last hidden representation H M, L as the matching output. The local relevance evidence − → e is then generated by concatenating the two matching outputs:</p><formula xml:id="formula_4">− → e = [ − → H cos M, L , − → H xor M, L ]</formula><p>. Furthermore, in order to enrich the relevance signals, we also applied the spatial GRU in the reverse direction, i.e., from bottom right to top left. The final passage-level signal is defined as the concatenation of the two-direction matching signals:</p><formula xml:id="formula_5">e = [ − → e , ← − e ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Global Decision Layer</head><p>Based on passage-level signals generated in the previous step, the global decision layer attempts to accumulate these signals into different granularities and allow them to compete with each other for final relevance assessment. As we have discussed before, the relevance patterns of a query-document pair can be rather flexible and diverse, allowing a document to be relevant to a query partially or as a whole. Accordingly, the global decision layer is expected to be able to accommodate various decision strategies, rather than using some restrictive combination rules <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref> .</p><p>In this work, we propose to employ a hybrid neural network architecture which has sufficient expressive power to support flexible relevance patterns. Before we describe the hybrid model, we first introduce two basic models under some simplified relevance assumptions.</p><p>1. Independent Decision (ID) Model assumes the independence among passage-level signals, and selects top-k signals directly for final relevance assessment. This model is under the assumption that a document is relevant if any piece of it can provide sufficient relevance information. Specifically, as shown in Figure <ref type="figure" target="#fig_3">3</ref>  Here, we also applied the LSTM in the reverse direction to accumulate the relevance signals, as user's reading can be in any direction of the document <ref type="bibr" target="#b28">[29]</ref>.</p><p>Note here if we directly use the last/first hidden state in LSTM as signals for relevance assessment, it would reduce to a document-wide method. By using k-max pooling over all the positions, we actually assume the relevance could be based on a text span flexible in scale, ranging from multiple passages to the whole document.</p><p>Based on the two basic models, now we introduce the Hybrid Decision (HD) Model as a specific implementation of the global decision layer. The HD model is a mixture of the previous two models, and picks top-k signals from passage-level or accumulated signals for final relevance assessment. Obviously, this is the most flexible relevance model, which allows a document to be assessed as a whole or partially adaptively. Specifically, as depicted in figure 3(c), we allow the relevance signals from different passages to compete with accumulated signals. Note here in order to make a fair competition, for the passage-level signals, we conduct an additional non-linear transformation to ensure a similar scale to the accumulated relevance signals.</p><formula xml:id="formula_6">v t = tanh(W v e t + b v ),</formula><p>where v t denotes the t-th transformed passage signals, W v and b v are parameters to be learned. We then apply a dimension-wise k-max pooling layer to select top-k signals, and feed the selected signals into a multi-layer perceptron for final assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training</head><p>Since the ad-hoc retrieval task is fundamentally a ranking problem, we utilize the pairwise ranking loss such as hinge loss to train our model. Specifically, given a triple (q, d + , d − ), where d + is ranked higher than d − with respect to a query q, the loss function is defined as:</p><formula xml:id="formula_7">L(q, d + , d − ; θ ) = max(0, 1 − s(q, d + ) + s(q, d − )),</formula><p>where s(q, d) denotes the relevance score for (q, d), and θ includes the parameters in both local matching layer and global decision layer. The optimization is relatively straightforward with standard backpropagation. We apply stochastic gradient decent method Adam <ref type="bibr" target="#b16">[17]</ref> with mini-batches(100 in size), which can be easily parallelized on a single machine with multi-cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>In this section, we conduct experiments to demonstrate the effectiveness of our proposed model on benchmark collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We first introduce our experimental settings, including datasets, baseline methods/implementations, and evaluation methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Data Sets.</head><p>To evaluate the performance of our model, we conducted experiments on two LETOR benchmark datasets <ref type="bibr" target="#b27">[28]</ref>: Million Query Track 2007 (MQ2007) and Million Query Track 2008 (MQ2008). We choose these two datasets according to three criteria: 1) there is a large number of queries, 2) the original document content is available, and 3) the dataset is public. The first two criterias are important for learning deep neural models for ad-hoc retrieval, and the last one is critical for reproducibility. Both datasets use the GOV2 collection which includes 25 million documents in 426 gigabytes. The details of the two datasets are given in Table <ref type="table" target="#tab_0">1</ref>. As we can see, there are 1692 queries on MQ2007 and 784 queries on MQ2008. The number of queries with at least one relevant document is 1455 and 564, respectively. The average number of relevant document per query is about 10.3 and 3.7 on MQ2007 and MQ2008, respectively.</p><p>For pre-processing, all the words in documents and queries are white-space tokenized, lower-cased, and stemmed using the Krovetz stemmer <ref type="bibr" target="#b17">[18]</ref>. Stopword removal is performed on query and document words using the INQUERY stop list <ref type="bibr" target="#b4">[5]</ref>. Words occurred less than 5 times in the collection are removed from all the document. We further segmented documents into passages for all the models using passage-level information. We utilized fixed-size sliding window without overlap to generate passages. We have also studied the performance of different window size in Section 4.5.</p><p>4.1.2 Baselines Methods. We adopt three types of baselines for comparison, including traditional retrieve models, learning to rank models and deep matching models.</p><p>For traditional retrieval models, we consider both documentwide methods, passage-level methods, and hybrid methods:</p><p>BM25: The BM25 model <ref type="bibr" target="#b29">[30]</ref> is a classical and highly effective document-wide retrieval model. MSP: The Max-Scoring Passage model <ref type="bibr" target="#b19">[20]</ref> utilizes language model for each passage and rank the document according to the score of their best passage. PLM: The passage language model <ref type="bibr" target="#b1">[2]</ref> integrates passagelevel and document-wide language model scores according to the document homogeneity for ad-hoc retrieval. PPM: The probabilistic passage model <ref type="bibr" target="#b34">[35]</ref> is a discriminative probabilistic model in capturing passage-level signals, and combines document retrieval scores with passage retrieval scores through a linear interpolate function. Learning to rank models include AdaRank: AdaRank <ref type="bibr" target="#b37">[38]</ref> is a representative pairwise model which aims to directly optimize the performance measure based on boosting approach. Here we utilize NDCG as the performance measure function. LambdaMart: LambdaMart <ref type="bibr" target="#b2">[3]</ref> is a representative listwise model that uses gradient boosting to produce an ensemble of retrieval models. It is the state-of-the-art learning to rank algorithm. Here, AdaRank and LambdaMart were implemented using Rank-Lib 2 , which is a widely adopted learning to rank tool. All the learning to rank models leveraged the 46 human designed features from LETOR. Furthermore, since our model utilized passage-level information, we introduced 9 passage-based features for fair comparison. Specifically, we calculated tf-idf, BM25 and language model scores for each query-passage pair, and picked the maximum, minimum and average scores across passages as the new features for a document. We applied the full set of features (original+passage features) on both two learning to rank models for additional comparison, denoted by AdaRank(+P) and LambdaMart(+P), respectively.</p><p>Deep matching models include Here, for DSSM, we directly utilize the trained model<ref type="foot" target="#foot_2">3</ref> released by their authors since training these complex models on small benchmark datasets could lead to severe over-fitting problem. For DeepRank <ref type="foot" target="#foot_3">4</ref> , we use the code released by their authors. For Duet model, we train it by ourselves since there is no trained model released. To avoid overfitting, we reduce the parameters of the convolutional network and fully-connected network to adapt the model to the limited size of LETOR datasets. Specifically, we set the filter size as 10 in both local model and global model, and the hidden size as 20 in the fully-connected layer. Other parameters are the same as the original paper.</p><p>We refer to our proposed model as HiNT<ref type="foot" target="#foot_4">5</ref> . For network configurations (e.g., numbers of layers and hidden nodes), we tuned the hyper-parameters via the validation set. Specifically, in the local matching layer, the dimension of the spatial GRU is set to 2 which is tuned in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. In the global decision layer, the dimension of LSTM is set to 6 which is tuned in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, the k-max pooling size is set to 10 which is tuned in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>, and the multi-layer perceptron is a 2-layers feed forward network without hidden layers. All the trainable parameters are initialized randomly by uniform distribution within [−0.1, 0.1]. Overall, the number of trainable parameters is about 930 in our HiNG model. Note that the MQ2008 dataset has much smaller query and document size, we find it is not sufficient to train deep models purely based on this dataset. Therefore, for all the deep models, we chose to use the trained model on MQ2007 as the initialization and fine tuned the model on MQ2008.</p><p>For all deep models based on term vector inputs, we used 50dimension term vectors. The term vectors were trained on wikipedia corpus<ref type="foot" target="#foot_5">6</ref> using the CBOW model <ref type="bibr" target="#b21">[22]</ref> with the default parameters <ref type="foot" target="#foot_6">7</ref> . Specifically, we used 10 as the context window size, 10 negative samples and a subsampling of frequent words with sampling threshold of 10 −4 . Out-of-vocabulary words are randomly initialized by sampling values uniformly from (−0.02, 0.02).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Evaluation Methodology.</head><p>We follow the data partition on this dataset in Letor4.0 <ref type="bibr" target="#b27">[28]</ref>, and 5 fold cross-validation is conducted to minimize over-fitting as in <ref type="bibr" target="#b9">[10]</ref>. Specifically, the parameters for each model are tuned on 4-of-5 folds. The last fold in each case is used for evaluation. This process is repeated 5 times, once for each fold. The results reported are the average over the 5 folds.</p><p>As for evaluation measure, precision (P), mean average precision (MAP) and normalized discounted cumulative gain (NDCG) at position 1, 5, and 10 were used in our experiments. We performed significant tests using the paired t-test. Differences are considered statistically significant when the p−value is lower than 0.05. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis on the HiNT Model</head><p>In this section we conducted experiments to compare different implementations of the two components in the HiNT model. Through these experiments, we try to gain a better understanding of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Analysis on Local Matching Layer.</head><p>As mentioned in Section 3.1, the local matching layer should be able to encode many well-known IR heuristics in order to well capture the relevance matching between a query and a passage. The heuristics at least include the modeling of exact matching and semantic matching signals, the differentiation between them, the modeling of proximity, the term importance, and so on. Here we conduct experiments to test a variety of implementations of the local matching layer which encode different IR heuristics by fixing the rest parts of the model.</p><p>The implementations include: (1) We apply a multi-layer perceptron over the exact matching matrix M xor to produce the passagelevel signals. In this way, only exact matching signals are encoded into the passage-level signal; (2) We apply a multi-layer perceptron over the semantic matching matrix M cos to produce the passagelevel signals. In this way, both exact and semantic matching signals are encoded but mixed together; (3) We follow the idea in <ref type="bibr" target="#b9">[10]</ref> to turn the semantic matching matrix M cos into matching histograms M hist , and use a multi-layer perceptron to produce the passage-level signals. In this way, both exact matching and semantic matching signals are encoded and these two types of signals are differentiated by using the histogram; (4) We apply a spatial GRU over the exact matching matrix M xor to produce the passage-level signals. In this way, only exact matching signals and proximity are encoded into the signals; <ref type="bibr" target="#b4">(5)</ref> We apply a spatial GRU over the semantic matching matrix M cos to produce the passage-level signals. In this way, exact matching and semantic matching signals are mixed and encoded together with proximity information. <ref type="bibr" target="#b5">(6)</ref> We use a spatial GRU over both exact matching and semantic matching signals. Here, the exact matching and semantic matching signals can be clearly differentiated. Finally, we use our proposed implementation, i.e., a spatial GRU over the S xor and S cos tensors, which can encode exact matching signals, semantic matching signals, proximity and term importance. The different implementations of the local matching layer as well as their performance results are shown in Table <ref type="table" target="#tab_2">2</ref>.</p><p>From the results we observe that, when modeling exact matching signals alone  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison of Retrieval Models</head><p>In this section, we compare our proposed HiNT against existing retrieval models over the two benchmark datasets. Note here we refer HiNT to the model using hybrid decision model based on both exact matching and semantic matching tensors. The main results of our experiments are summarized in Table <ref type="table" target="#tab_4">3</ref>. Firstly, for the traditional models, we can see that BM25 is a strong baseline which performs better than MSP. The relative poor performance of MSP indicates that it is deficient in capturing the diverse relevance patterns by only using the passage-level signals.</p><p>By integrating document-wide with passage-level signals, the performance of PLM and PPM is mixed compared with BM25, demonstrating the deficiency of the simple combination strategy, which is consistent with previous findings <ref type="bibr" target="#b34">[35]</ref>. Secondly, all the learning to rank models perform significantly better than the traditional retrieval models. This is not surprising since learning to rank models can make use of rich features, where BM25 scores and LM scores are typical features among them. Among the two learning to rank models, LambdaMart performs better. Moreover, we can see that adding passage-level features might improve the performance of learning to rank models, but not consistent on different datasets. For example, the performance of AdaRank and LambdaMart in terms of P@1 on MQ2008 drops when adding passage features. Thirdly, as for the deep matching models, we can see that DSSM obtain relatively poor performances on both datasets, even cannot compete with the traditional retrieval models. This is consistent with previous findings <ref type="bibr" target="#b9">[10]</ref>, showing that one may not work well on ad-hoc retrieval by only leveraging the cosine similarity between high-level abstract representations of short query and long document. As for DRMM and Duet, they have achieved relative better performance compared with DSSM. This may due to the fact that they are specifically designed for the relevance matching in adhoc retrieval, and they have incorporated important IR heuristics into their model. However, they can only reach comparable performance as learning to rank models by only using document-wide Session 3D: Learning to Rank II SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Finally, we observe that HiNT can outperform all the existing models in terms of all the evaluation measures on both datasets by allowing the competition between the passage-level signals and document-wide signals explicitly. It is worth noting that in the learning to rank methods, there are many human designed features including those document-wide and passage-level matching signals, as well as those about the quality of the document (e.g., PageRank). While in our HiNT, all the assessment are purely learned from the primitive word features of queries and documents. Therefore, the superior performance of HiNT suggests the importance and effectiveness of modeling the diverse relevance patterns for ad-hoc retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Passage Size</head><p>Since we leverage the fixed-size sliding windows to segment a document into passages, we would like to study the effect of the passage size on the ranking performance. Here we report the performance results on MQ2007 with the passage size set as 10, 50, 100, 200, 300 and 500 words. As shown in Figure <ref type="figure" target="#fig_5">6</ref>, the performance first increases and then drops with the increase of the passage size. The possible reason might be that too small passage size may hurt the quality of passage signals (i.e., relevance matching between the query and the passage) due to the information sparsity, while too large passage size would produce limited number of coarse passagelevel signals which restrict the ability of the global decision layer. Our results shows that the best performance can be achieved when the passage size is set to 100 words on MQ2007. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>To better understand what can be learned by HiNT, here we conduct some case studies. For better visualization and analysis, we simplified our model by replacing k-max pooling with max pooling so that only the most significant signal is used in decision. Based on the learned model, we pick up a query and a relevant document, and plot all the signals E used in the hybrid decision model along with the corresponding document content. Here each small bar in E denotes a passage or accumulated signal at that position, with the color corresponding to the signal strength. We highlight the final selected signal with a red box and the corresponding passages with green background color. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, we can find two significantly different decision strategies between a query and a document. In the first case, the document is relevant to a query because of a strong passage signal. By checking the query and the document, we find that the query is "average annual salary in 1975" which conveys very specific information need, and the passage at the 5-th position (i.e., the strongest signal) contains a table whose content can well address this information need. In the second case, the document is relevant to a query because of a strong accumulated signal. Again by checking the query and the document, we find that the query is about the "cause of migraine headache" which is informational, and the document is mostly relevant to this query with many passages addressing this problem (i.e., from the beginning to the 15-th passage).</p><p>The two cases show that there are indeed quite diverse relevance patterns in real-world retrieval scenario, and our HiNT model can capture these diverse relevance patterns successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In paper, we have introduced a hierarchical neural matching model to capture the diverse relevance patterns in ad-hoc retrieval. The model consists of two components, namely local matching layer and global decision layer. We employed deep neural network in both layers to support high-quality relevance signal generation and flexible relevance assessment strategies, respectively. Experimental results on two benchmark datasets demonstrate that our model can outperform all the baseline models in terms to all the evaluation metrics, especially state-of-the-art learning to rank methods that use manually designed features.</p><p>For future work, it would be interesting to try other implementation of the two components in HiNT, e.g., to employ some attentionbased neural network for the global decision layer. We would also like to expand our model to accommodate features beyond relevance matching, e.g. PageRank, to help improve the retrieval performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Architecture of the Hierarchical Neural Matching Model.</figDesc><graphic url="image-1.png" coords="3,166.25,83.69,279.50,153.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Architecture of Relevance Matching Network.</figDesc><graphic url="image-2.png" coords="4,82.34,83.69,183.17,139.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a), a dimension-wise k-max pooling layer is first applied over the passage-level signals to select top-k signals, and the selected signals are then concatenated and projected into a multi-layer perceptron to get the final decision score. 2. Accumulative decision (AD) Model accumulates the passage-level signals in a sequential manner, and selects top-k accumulated signals for relevance assessment. Here,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Different decision models based on the collected passage signals.</figDesc><graphic url="image-3.png" coords="5,113.76,83.69,384.48,137.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of different decision strategies over the query-document pair.</figDesc><graphic url="image-4.png" coords="9,102.80,83.68,406.40,201.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance comparison of HiNT over different passage sizes on MQ2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets used in this study.</figDesc><table><row><cell></cell><cell cols="4">#queries #docs #q_rel #rel_per_q</cell></row><row><cell>MQ2007</cell><cell>1692</cell><cell cols="2">65,323 1455</cell><cell>10.3</cell></row><row><cell>MQ2008</cell><cell>784</cell><cell>14,384</cell><cell>564</cell><cell>3.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Duet<ref type="bibr" target="#b23">[24]</ref> is a joint model which learns local lexical matching and global semantic matching together. DeepRank: DeepRank<ref type="bibr" target="#b26">[27]</ref> is a state-of-the-art deep matching model which models relevance by simulating the human judgement process.</figDesc><table><row><cell>DSSM: DSSM [14] is a neural matching model proposed for</cell></row><row><cell>Web search. It consists of a word hashing layer, two non-</cell></row><row><cell>linear hidden layers, and an output layer.</cell></row><row><cell>DRMM: DRMM [10] is a neural relevance model designed</cell></row><row><cell>for ad-hoc retrieval. It consists of a matching histogram</cell></row><row><cell>mapping, a feed forward matching network and a term gating</cell></row><row><cell>network.</cell></row></table><note>2 https://sourceforge.net/p/lemur/wiki/RankLib/ Duet:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Analysis on local matching layer on MQ2007. Significant improvement or degradation with respect to our implementation (S xor +S cos +spatial GRU) is indicated (+/-) (p-value ≤ 0.05)</figDesc><table><row><cell>IR Heuristics</cell><cell>Performance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, M xor + MLP can already obtain reasonably good retrieval performance. It indicates that exact matching signals are These results demonstrate that semantic matching signals are also useful for retrieval, but should better be distinguished from exact matching signals if the deep model itself (e.g., MLP) cannot differentiate them. If the deep model can somehow implicitly distinguish these two types of signals, e.g., spatial GRU using input gates, we can observe better performance on the semantic matching matrix (M cos + spatialGRU ) than that on the exact matching matrix (M xor + spatialGRU ). However, we can further observe performance increase if we explicitly distinguish exact matching and semantic matching signals (M xor + M cos + spatialGRU ). Analysis on Global Decision Layer. We further study the effect of different implementations of the global decision layer. Here we compare the proposed hybrid decision model (i.e., HiNT HD ) with the two basic decision models introduced in Section 3.2 (i.e.,</figDesc><table><row><cell>P@10 38.9 40.5 41.8 HiNT ID Figure 4: Performance comparison of HiNT over different NDCG@10 MAP 36 38 40 42 44 46 48 50 44.6 46.4 47.2 48.3 49.0 50.2 performance (%) HiNT AD HiNT HD decision models on MQ2007. critical in ad-hoc retrieval [10]. Meanwhile, the performance drops significantly when semantic matching signals are mixed with exact matching signals (M Besides, the local matching layers using spatial GRU can in general obtain bet-ter results, indicating that proximity is very helpful for retrieval. Finally, by further considering term importance, our proposed im-plementation (S xor + S cos + spatialGRU ) can outperform all the variants significantly. All the results demonstrate the importance of encoding a variety of IR heuristics in the local matching layer for a successful relevance judgement model. 4.2.2 Session 3D: Learning to Rank II SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA</cell></row></table><note>cos + MLP), but increases if these two types of signals are clearly differentiated (M hist + MLP).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different retrieval models over the MQ2007 and MQ2008 datasets. Significant improvement or degradation with respect to HiNT is indicated (+/-) (p-value ≤ 0.05).HiNT ID and HiNT AD ) by fixing the rest parts of the model. The comparison results are shown in Figure4. As we can see, the simplest relevance model HiNT ID performs worst. It seems that selecting passage-level signals independently might be too simple to capture diverse relevance patterns. Meanwhile, HiNT AD performs better than HiNT ID , indicating that it is more beneficial to make relevance assessment based on accumulated signals from a variety of text spans. Finally, HiNT HD achieves the best performance in terms of all the evaluation measures. This further indicates that there might be very diverse relevance patterns across different query-document pairs. By allowing competition between passage-level and accumulated signals, the expressive power of the HD model is the largest, leading to the best performance among the three variants.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MQ2007</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Model Name BM25 MSP PLM PPM AdaRank LambdaMart AdaRank(+P) LambdaMart(+P) 0.484 − 0.427 − 0.391 − P@1 P@5 P@10 NDCG@1 NDCG@5 NDCG@10 MAP 0.427 − 0.388 − 0.366 − 0.358 − 0.384 − 0.414 − 0.450 − 0.361 − 0.358 − 0.350 − 0.302 − 0.341 − 0.378 − 0.422 − 0.416 − 0.389 − 0.371 − 0.348 − 0.377 − 0.413 − 0.449 − 0.431 − 0.393 − 0.370 − 0.361 − 0.392 − 0.424 − 0.453 − 0.449 − 0.403 − 0.372 − 0.394 − 0.410 − 0.436 − 0.460 − 0.481 − 0.418 − 0.384 − 0.412 − 0.421 − 0.446 − 0.468 − 0.457 − 0.408 − 0.380 − 0.393 − 0.408 − 0.438 − 0.467 − 0.413 − 0.427 − 0.454 − 0.473 − DSSM 0.345 − 0.359 − 0.352 − 0.290 − 0.335 − 0.371 − 0.409 − DRMM 0.450 − 0.417 − 0.388 − 0.380 − 0.408 − 0.440 − 0.467 − Duet 0.473 − 0.428 − 0.398 − 0.409 − 0.431 − 0.453 − 0.474 − DeepRank 0.508 0.452 − 0.412 − 0.441 0.457 − 0.482 − 0.497</cell></row><row><cell>HiNT</cell><cell>0.515</cell><cell>0.461</cell><cell>0.418</cell><cell>0.447</cell><cell>0.463</cell><cell>0.490</cell><cell>0.502</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MQ2008</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Model Name BM25 MSP PLM PPM AdaRank LambdaMart AdaRank(+P) LambdaMart(+P) 0.441 − 0.348 P@1 P@5 0.408 − 0.337 − 0.245 P@10 NDCG@1 NDCG@5 NDCG@10 MAP 0.344 − 0.461 − 0.220 − 0.465 − 0.332 − 0.314 − 0.236 − 0.283 − 0.415 − 0.193 − 0.426 − 0.396 − 0.326 − 0.240 − 0.327 − 0.438 − 0.208 − 0.452 − 0.412 − 0.338 − 0.241 − 0.350 − 0.464 − 0.220 − 0.468 − 0.434 − 0.342 0.243 0.368 − 0.468 0.221 0.476 0.449 − 0.346 0.249 0.376 − 0.471 0.230 0.478 0.428 − 0.345 0.247 0.368 − 0.475 0.225 0.478 0.249 0.372 − 0.479 0.232 0.480 DSSM 0.341 − 0.284 − 0.221 − 0.286 − 0.378 − 0.178 − 0.391 − DRMM 0.450 − 0.337 − 0.242 − 0.381 − 0.466 − 0.219 − 0.473 − Duet 0.452 − 0.341 − 0.240 − 0.385 − 0.471 − 0.216 0.476 − DeepRank 0.482 − 0.359 − 0.252 0.406 − 0.496 0.240 0.498 −</cell></row><row><cell>HiNT</cell><cell>0.491</cell><cell>0.367</cell><cell>0.255</cell><cell>0.415</cell><cell>0.501</cell><cell>0.244</cell><cell>0.505</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://trec.nist.gov/data/reljudge_eng.html Session 3D: Learning to Rank II SIGIR'18, July 8-12,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2018" xml:id="foot_1">, Ann Arbor, MI, USA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://www.microsoft.com/en-us/research/project/dssm/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/pl8787/textnet-release</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/faneshion/HiNT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">http://en.wikipedia.org/wiki/Wikipedia_database</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://code.google.com/archive/p/word2vec/ Session 3D: Learning to Rank II SIGIR'18, July</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">-12, 2018, Ann Arbor, MI, USA</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>This work was funded by the 973 Program of China under Grant No. 2014CB340401, the National Natural Science Foundation of China (NSFC) under Grants No. 61425016, 61472401, 61722211, and 20180290, the Youth Innovation Promotion Association CAS under Grants No. 20144310 and 2016102, and the National Key R&amp;D Program of China under Grants No. 2016QY02D0405.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelis Joost</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Utilizing passage-based language models for document retrieval</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Kurland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="162" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From ranknet to lambdarank to lambdamart: An overview</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="581" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Passage-level evidence in document retrieval</title>
		<author>
			<persName><surname>James P Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TREC and TIPSTER experiments with INQUERY</title>
		<author>
			<persName><forename type="first">Bruce</forename><surname>James P Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><surname>Broglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="327" to="343" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The TREC 2005 Terabyte Track</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Falk</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName><surname>Soboroff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>In TREC</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic term matching in axiomatic approaches to information retrieval</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient boosting algorithm for combining preferences</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="933" to="969" />
			<date type="published" when="2003-11">2003. Nov (2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic matching by non-linear word transportation for information retrieval</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training linear SVMs in linear time</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Passage retrieval revisited</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kaszkiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Viewing morphology as an inference process</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
				<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analyses of multiple evidence combination</title>
		<author>
			<persName><forename type="first">Joon</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Forum</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Passage retrieval based on language models</title>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Positional language models for information retrieval</title>
		<author>
			<persName><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhai</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01509</idno>
		<title level="m">Neural Models for Information Retrieval</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to Match using Local and Distributed Representations of Text for Web Search</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Bhaskar Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1291" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two-stage document length normalization for information retrieval</title>
		<author>
			<persName><forename type="first">Seung-Hoon</forename><surname>Na</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04648</idno>
		<title level="m">A study of matchpyramid models on ad-hoc retrieval</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LETOR: A benchmark collection for research on learning to rank for information retrieval</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="346" to="374" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Eye movements in reading and information processing: 20 years of research</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">372</biblScope>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Approaches to passage retrieval in full text information systems</title>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Test collection based evaluation of information retrieval systems</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An exploration of proximity measures in information retrieval</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04378</idno>
		<title level="m">Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminative probabilistic models for passage based retrieval</title>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A retrospective study of a hybrid document-context based retrieval model. Information processing &amp; management</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">Wp</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><surname>Kwok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1308" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Incorporating window-based passage-level evidence in document retrieval</title>
		<author>
			<persName><forename type="first">Wensi</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Xu-Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">Sg</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of information science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="73" to="80" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adarank: a boosting algorithm for information retrieval</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.384019</idno>
		<ptr target="http://dx.doi.org/10.1145/383952.384019" />
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
