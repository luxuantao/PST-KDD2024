<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparison of Coverage-Based and Distribution-Based Techniques for Filtering and Prioritizing Test Cases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Leon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering &amp; Computer Science Department Case Western Reserve University</orgName>
								<address>
									<addrLine>10900 Euclid Avenue Cleveland</addrLine>
									<postCode>44106, +1 216 368 4231, +1 216, 368 6884</postCode>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andy</forename><surname>Podgurski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering &amp; Computer Science Department Case Western Reserve University</orgName>
								<address>
									<addrLine>10900 Euclid Avenue Cleveland</addrLine>
									<postCode>44106, +1 216 368 4231, +1 216, 368 6884</postCode>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comparison of Coverage-Based and Distribution-Based Techniques for Filtering and Prioritizing Test Cases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">197F0AB96460347B3F6CD1E84B11F07A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an empirical comparison of four different techniques for filtering large test suites: test suite minimization, prioritization by additional coverage, cluster filtering with one-per-cluster sampling, and failure pursuit sampling. The first two techniques are based on selecting subsets that maximize code coverage as quickly as possible, while the latter two are based on analyzing the distribution of the tests' execution profiles. These techniques were compared with data sets obtained from three large subject programs: the GCC, Jikes, and javac compilers. The results indicate that distributionbased techniques can be as efficient or more efficient for revealing defects than coverage-based techniques, but that the two kinds of techniques are also complementary in the sense that they find different defects. Accordingly, some simple combinations of these techniques were evaluated for use in test case prioritization. The results indicate that these techniques can create more efficient prioritizations than those generated using prioritization by additional coverage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is often desirable to filter a pool of test cases for a program in order to identify a subset that will actually be executed and audited (checked for conformance to requirements) at a particular time. For example:</p><p>• The suite of regression tests for a long-lived system may become so large that it is feasible to execute only a fraction of them when the program is modified [ <ref type="bibr">[9]</ref>]. • A deployed application may be instrumented to capture its own executions, so that developers can replay and audit them [ <ref type="bibr" target="#b25">[26]</ref>] or so that captured inputs can be used to "refresh" a regression test suite. The number of executions that are captured may exceed the number it is feasible to audit. • An automatic test data generator may be capable of producing many more tests than a tester is able to run and audit. Naturally, it is desirable to select those test cases that are most likely to reveal defects in the program under test. To be worthwhile, the sum of the cost of the filtering process and the costs of executing and auditing the selected tests should be less than the cost of executing and auditing all of the tests in the original pool. Note that it is often necessary to audit test executions manually, in which case the cost of auditing typically dominates other costs.</p><p>Recent papers have investigated several techniques for test case filtering and for the closely related problem of test-case prioritization that are based on analyzing profiles of test executions [ <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>]. These profiles characterize aspects of test executions that are thought to be relevant to whether the tests reveal defects. For example, a profile may indicate which statements, conditional branches, or data flow relationships in a program were covered by a test.</p><p>Some approaches to filtering or prioritizing regression tests employ profiles collected when testing previous versions of a program. These approaches emphasize economical reuse of test cases and hence they are not applicable to new tests cases. For example, one of the most basic forms of filtering that is used in regression testing involves eliminating old tests that do not execute any code that has been changed since the last time the tests were run [ <ref type="bibr" target="#b21">[22]</ref>]. Other approaches employ profiles that are produced by executing the current version of the program under test on the entire pool of test cases [ <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>]. These approaches focus on reducing the cost of auditing test executions and are applicable to new test cases as well as to old ones.</p><p>This paper reports the results of an empirical comparison of two basic approaches to filtering and prioritizing test cases. One approach is based on "greedily" maximizing the code coverage achieved by a selected set of tests -that is, first selecting the test case that covers the largest number of statements, then selecting the test case that covers the largest number of statements not covered by the first one, and so on. This approach is often called test suite minimization in the literature, because the greedy selection algorithm can be viewed as attempting to find a minimal test set that covers all of the code covered by the original pool of test cases. The other approach to test case filtering and prioritization that we consider is based on how the execution profiles induced by test cases are distributed in the profile space. It is illustrated by two closely related techniques: cluster filtering and failure-pursuit. Cluster filtering calls for using automatic cluster analysis to partition the pool of test cases according to how similar the corresponding profiles are, as measured by a dissimilarity metric <ref type="bibr">[[3]</ref>]. Tests are then selected from each cluster or from particular clusters.</p><p>For example, one-per-cluster sampling calls for randomly selecting one test from each cluster. Failure-pursuit sampling is an extension of oneper-cluster sampling that calls for selecting the k nearest neighbors of any failures found by auditing a one-percluster sample of tests <ref type="bibr">[[4]</ref>]. If any additional failures are found, each of their k nearest neighbors is selected, and so on, until no additional failures are found.</p><p>Maximizing code coverage is a fundamental idea in software testing. It is motivated by the idea that one cannot have confidence in parts of a program that have not been executed during testing. However, it has been argued that code coverage by itself is not an adequate criterion for selecting test cases, e.g., because testers often achieve high coverage by selecting simple test cases that do not reflect how the program is likely to be used in practice <ref type="bibr">[[20]</ref>]. Techniques like cluster filtering and failure-pursuit sampling are based on the idea that the distribution of test profiles reflects additional information about test cases that is useful in filtering or prioritizing them. For example, the distribution of synthetic tests may indicate which test cases are unique and which are similar (and hence possibly redundant), and the distribution of operational tests indicates which kinds of executions are common in the field and which kinds are rare. Of course, the distribution of test profiles depends on the nature of the profiles and on how dissimilarity between profiles is measured. For example, the distribution of code-coverage profiles may be quite different from the distribution of variable-value profiles. In this paper, we focus on the use of code coverage profiles of different granularities.</p><p>Distribution-based filtering and prioritization techniques can exploit the information in coverage profiles that is ignored by coverage maximization (namely, the magnitude of execution counts), together with the additive nature of common dissimilarity metrics, to discriminate more finely between tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>A number of empirical studies comparing different regression test selection or prioritization techniques in terms of their defect-detection effectiveness have been reported recently. (Note that the study reported in this paper does not address regression testing techniques in particular and does not make use of information about program changes.) Wong et al <ref type="bibr">[[27]</ref>] [ <ref type="bibr" target="#b27">[28]</ref>] studied the effectiveness of several test-suite reduction techniques.  <ref type="bibr" target="#b18">[19]</ref>]. Cluster filtering and several variants of it are presented and evaluated empirically in [ <ref type="bibr" target="#b3">[3]</ref>]. Failure pursuit sampling is presented and compared empirically to cluster filtering in <ref type="bibr">[[4]</ref>]. This previous work on distribution-based filtering techniques did not address the effects of profile granularity or whether distribution-based techniques reveal different defects than techniques based on greedy coverage maximization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Test case selection and prioritization</head><p>The goal of test case filtering is to select a relatively small subset of a test suite that finds a large portion of the defects that would be found if the whole test suite were to be used. In this paper we examine the case where there are enough resources to execute and profile all the tests, but not enough resources to manually audit all of them. This is different from the regression-testing scenario, where profiles are available for the old version of the program, where tests are often designed to be self-validating (so manual auditing is not required<ref type="foot" target="#foot_1">1</ref> ), and where the goal is to reduce the total cost of running the tests on the new version.</p><p>When it is uncertain how many tests can be run and audited, it is advantageous to order the test cases so that defects are most likely to be found early on, which permits developers to start fixing problems quickly. This idea is called test case prioritization [ <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>]. Note that test case filtering and prioritization are closely related. In fact, given a prioritization of test cases one can filter them simply choosing the first N tests in the ordering. Therefore, any test case prioritization algorithm can be used as a test case selection algorithm. The reverse is not true in general.</p><p>When evaluating test case prioritization techniques one has to take into account the number of defects found as a function of time. The test case prioritization literature uses a weighted average of the percentage of faults detected, or APFD [ <ref type="bibr" target="#b5">[5]</ref>]. APFD is basically the area under the curve in a plot of defects found versus time. <ref type="bibr">Figures 9,</ref><ref type="bibr">10 and 11</ref> are examples of such plots. The value of the APFD metric ranges from 0 to 100, with higher values indicating more defects were found earlier on. We will use this measure when evaluating prioritization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Coverage-based techniques</head><p>An important idea in traditional software testing is that the test suite used should execute, or cover the whole program under test [ <ref type="bibr" target="#b1">[1]</ref>]. That is, for each program element of a certain kind (e.g., function, statement, branch, or def-use chain) there should be at least one test that exercises that element. It is generally agreed that a test set must achieve high code coverage in order to be considered "thorough". If code coverage is the principal determinant of the quality of a test set for a particular class of programs, then in selecting test cases to be audited from a pool of test cases for such a program,<ref type="foot" target="#foot_2">2</ref> it may be desirable to select the smallest subset of the pool that covers as many program elements as the entire pool does. This is called test suite minimization in the regression testing literature [ <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>]. Even when code coverage is not the principal determinant of test set quality, it may be desirable to select such a subset as the first step in a more complex selection procedure. Selecting the smallest subset of a test pool that maximizes code coverage is an instance of the set-cover problem, which is NP-complete but which admits a greedy approximation algorithm [ <ref type="bibr" target="#b11">[11]</ref>]. On each of its iterations, the greedy algorithm selects the test that covers the largest number of elements not covered by the previously selected tests. In the sequel, we will refer to this technique as basic coverage maximization to emphasize that code coverage is the basis for selecting test cases.</p><p>Elbaum et al extend this selection technique in order to prioritize the test cases in a test suite [ <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>], that is, to place the test cases in non-decreasing order with respect to their perceived likelihood of revealing defects. Their approach, which they call additional coverage prioritization, involves running the greedy coverage maximization algorithm repeatedly on the set of test cases that have not yet been prioritized. The priority of a test case corresponds to the order in which it is selected during this process. The earlier a test case is selected, the higher its priority is. In the sequel we will refer to this prioritization technique as repeated coverage maximization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Distribution-based techniques</head><p>Whereas basic and repeated coverage maximization filter and prioritize test cases, respectively, based on their incremental contribution to maximizing code coverage, distribution-based techniques select and prioritize test cases based on how their profiles are distributed in the multidimensional profile space defined by a particular dissimilarity metric <ref type="bibr">[[11]</ref>]. The dissimilarity metric is a function that for each pair of profiles outputs a real number representing their degree of dissimilarity. An example of a profile space is the n-dimensional space defined by applying the Euclidean distance metric to profiles that record basic-block execution counts for a program with n basic blocks. Note that the Euclidean metric accurately reflects the magnitude of execution counts, whereas greedy coverage maximization applied to the same profiles would consider only whether basic blocks were executed at least once. The tester is free to choose a dissimilarity metric emphasizing whatever aspects of the available profiles that he or she believes are most relevant to revealing defects. Typical dissimilarity metrics take the form of a (possibly weighted) sum in which there is a term for each profile feature (e.g., each execution count).</p><p>To demonstrate the idea of a profile-space, consider Figure <ref type="figure" target="#fig_0">1</ref>, which presents a multidimensional scaling plot of the dissimilarity metrics for the one of the GCC data sets that will be used in our experiments. MDS is a technique that produces a plot where the distance between points approximate the distance given by the dissimilarity metric [ <ref type="bibr" target="#b2">[2]</ref>]. Analyzing the figure, we notice for example a region set of points in the bottom-right that is separate from all the rest. These represent the tests for which the compiler was executed with no optimizations. Likewise we can see outliers and clusters of similar executions.</p><p>Distribution-based techniques for filtering and prioritizing test cases identify features of the profile distribution that are potentially relevant to revealing failures and then use these features to guide the selection or prioritization process. For example:</p><p>• Clusters of similar profiles may indicate redundant tests, in which case it may suffice to select one or a few representatives from each cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Isolated profiles may indicate tests inducing very</head><p>unusual conditions that may be especially likely to cause failures. • With operational executions, low-density regions of the profile space may indicate uncommon user behaviors that warrant investigation. • Leon et al found that linear regions in a correspondence analysis display of compiler tests corresponded to files compiled at the same optimization level [ <ref type="bibr" target="#b18">[19]</ref>]. In this paper, we consider two types of distance-based filtering and prioritization techniques: cluster filtering and failure pursuit. Cluster filtering [ <ref type="bibr" target="#b3">[3]</ref>] is based on automatic cluster analysis [ <ref type="bibr" target="#b16">[17]</ref>]. Cluster analysis is a multivariate analysis method for finding groups or clusters in a population of objects. Cluster analysis algorithms use a dissimilarity metric such as Euclidean distance or Manhattan distance to partition the population into clusters. Objects placed together in a cluster are more similar to one another than to objects in other clusters.</p><p>Cluster filtering uses cluster analysis to partition a set of tests into clusters based on the dissimilarity of their profiles. One or more test are selected for audit from each cluster or from particular clusters. A cluster filtering procedure is defined by a choice of clustering algorithm, dissimilarity metric, cluster count, and sampling method. An example of a sampling method is one-per-cluster (OPC) sampling. As its name suggests, it calls for selecting exactly one test from each cluster. One-percluster sampling has the following desirable properties:</p><p>• Redundancy in the selected subset is minimized, since only one test is selected from each cluster of similar tests. • Unusual executions are selected, since these are usually placed in a cluster by themselves. • The selected tests exercise each distinct program behavior represented by a cluster of tests. Failure-pursuit sampling is an adaptive extension of cluster filtering that seeks to exploit the observation that failed tests are often clustered together in small clusters [ <ref type="bibr" target="#b4">[4]</ref>]. Failure pursuit calls for selecting the k nearest neighbors of any failures found by auditing the initial subset of tests. If any additional failures are found, each of their k nearest neighbors is selected, and so on, until no additional failures are found.</p><p>(In the experiments reported in this paper, k = 5 is used.) Although failure pursuit was effective for revealing failures in the experiments reported in [ <ref type="bibr" target="#b4">[4]</ref>], it was not clear whether its adaptive phase revealed different defects from those found in the cluster filtering phase. We address this issue Section 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head><p>We performed some experiments using real world programs to evaluate the defect detection efficiency and other properties of basic coverage maximization, repeated coverage maximization, one-per-cluster sampling and failure pursuit.</p><p>Specifically, we investigated the following questions:</p><p>• How do the techniques compare to each other?</p><p>• What is the best technique, in terms of defectdetection efficiency, for selecting subsets of various sizes? • Do distribution-based techniques find the same defects as coverage-based techniques? • How does profile granularity affect the efficiency of the different techniques? After presenting our experimental results concerning filtering techniques, we consider some new prioritization techniques formed by combining filtering techniques, and we compare them to repeated coverage maximization and random ordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Subject programs and test suites</head><p>For this study, we experimented with three large programs and with different profile granularities. Part of this data set was used before in [ <ref type="bibr" target="#b20">[21]</ref>]. In this paper we also use profiles with finer granularities for the same GCC was tested by using part of the test suite distributed by the GCC team. We used the test suite shipped with GCC 3.0.2, which contains tests for defects present in GCC 2.95.2. This is a regression test suite maintained by the developers of the program. When a bug is fixed on the compiler, a test case is sometimes added to the test suite to make sure similar problems are caught in the future. We chose to use the execution tests in the test suite, that is, those tests for which the input is accepted by the compiler, and the resulting program is then tested for miscompilations. There were 3,333 such tests, of which 136 failed.</p><p>Jikes and javac were tested with the Jacks test suite [ <ref type="bibr" target="#b12">[12]</ref>] (as of 2/15/02), which tests adherence to the Java Language Specification <ref type="bibr">[[13]</ref>]. This test suite is maintained separately from both compiler projects, and includes tests for each section of the language specification. There were 3149 tests for Jikes, of which 225 failed, and 3,140 tests for javac, with 223 failures. The difference in size comes from compiler-specific tests.</p><p>The failures on that occurred when these test suites were executed were classified manually into groups of failures believed to have the same cause. (This was done as part of previous work in order to validate an automated technique for classifying failures; see [ <ref type="bibr" target="#b20">[21]</ref>] for details). We used the resulting classification to distinguish failures caused by different defects. The observed failures were associated with 27, 107, and 67 defects for GCC, Jikes and javac, respectively.</p><p>Both GCC and Jikes were profiled by using the basicblock profiler gcov, which is distributed with GCC. Gcov relies on instrumentation placed by GCC on every basic block to gather profiling information, rather than sampling the stack at fixed time intervals like other common profilers, making it more accurate for testing purposes. Gcov was modified to allow us to retrieve the raw output of the profiler. We were able to obtain profiles for each program including execution counts for three different granularities: functions, basic blocks, and control flow edges between basic blocks. During postprocessing, redundant information was removed. For example, if two basic blocks always had the same count, one of those counts was removed. After this step, GCC had 2,214 function execution counts, 28,144 basic-block execution counts and 36,407 basic-block-edge execution counts, while Jikes had 3,644 function execution counts, 11,502 basic-block execution counts, and 12,996 basic-blockedge execution counts.</p><p>Javac is written in Java, so it was instead profiled with a simple profiler written using the Java Virtual Machine Profiling Interface [ <ref type="bibr" target="#b13">[14]</ref>]. We were only able to obtain function call profiles for javac, so we were unable to use this program when studying the effect of granularity on test case filtering and prioritization techniques. Nonetheless, this data helps us evaluate whether our conclusions generalize to different programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental setup</head><p>For basic and repeated coverage maximization, we implemented the greedy algorithm described in Section 4. One often-overlooked property of this algorithm is that it can sometimes encounter ties when doing its greedy selection. One common solution to this is to pick the first test in the input, which might not be optimal. To study the effect of this property, we ran 1,000 replications on each program/granularity combination, by first shuffling the order of the test and running the algorithm. For each we recorded how many tests, failures <ref type="foot" target="#foot_3">3</ref> and defects were selected, as well as the order and APFD results for the prioritization technique.</p><p>For the experiments on one-per-cluster sampling and failure pursuit, it is necessary to choose a dissimilarity metric and a clustering algorithm. In [ <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>] Dickinson et al report experiments in which different dissimilarity metrics were used for cluster filtering and failure pursuit. They found that these techniques work best overall with the proportional binary metric, which is a modified Euclidean distance formula that takes into account whether an element was covered or not, and also the count of how many times the element was executed, while adjusting for differences in scale between counts. If we define C i,j as the count of the number of times that element j was executed on test run i, define P i,j as:</p><formula xml:id="formula_0">j k k j k k j k k j i j i C C C C P , , , , , min max min - - =</formula><p>That is, P i,j are the normalized counts. Also, define B i,j to be 0 if P i,j = 0 or 1 otherwise. Then the proportionalbinary distance between runs n and m is: </p><formula xml:id="formula_1">( ) ∑ - + - = k k m k n k m k n m n B B P P D , , 2 , , ,</formula><p>This metric emphasizes the effect of coverage, while also taking into account the actual value of the counts.</p><p>As a clustering algorithm we use hierarchical agglomerative clustering with average linkage [ <ref type="bibr" target="#b16">[17]</ref>]. This algorithm starts by placing each test execution in a separate cluster, and then iteratively finds the two most similar clusters and joins them into a larger one. Dissimilarities are updated between iterations to reflect the average distance between the components of the new cluster and all the others. The algorithm stops when the requested number of clusters is reached.</p><p>The experiments for one-per-cluster sampling and failure pursuit involved doing 1,000 replications of both of these techniques, and recording number of tests, failures, and defects selected, and also how many of these were also selected by a sample test suite selected by basic coverage maximization. This experiment was done for every combination of program and profile granularity. The number of clusters used by the algorithms was varied to correspond to different percentages of the size of the test suite. The sizes used were 1%, 2.5%, 5%, 10%, 15%, 25% and 30% of the test suite, and the size of the coverage test suite selected for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Effects of granularity</head><p>In this section we study the effects of profile granularity on each of the coverage-based and distribution-based filtering techniques. Only the GCC and Jikes data sets are considered, since javac was profiled at only one granularity level.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows the average results for basic coverage maximization for both programs. It is interesting to note that full coverage can be achieved with a relatively small subset. Even with basic-block edges, the size of the selected subset is at most 23.28% of the original size. Increasing the detail of the profile increases both the  number of tests selected and the number of defects found, as can be expected, but the amount by which the subset size increases is surprising. For GCC the selected subset for control-flow edges is 6.3 times the size for functions. Likewise, for Jikes, covering all edges requires 3.9 times more tests than are required for covering all functions.</p><p>On the other hand, even at the finest granularity level, basic coverage maximization finds at best 64.26% of the defects.</p><p>When considering one-per-cluster sampling and failure pursuit, there is little or no difference in the results for different granularities. For example, Figures <ref type="figure" target="#fig_1">2, 3, 4</ref> and<ref type="figure">5</ref> show number of defects found vs. number of tests selected for one-per-cluster sampling and failure pursuit for GCC and Jikes. On all the data sets, the results for the different granularities are very close to each other for any given subset size. For Jikes, the results are nearly identical, while for GCC, using function call counts yields better results than using other granularities for small subset sizes but worse for larger subsets. In any case, for Jikes the numbers of defects found for similar subset sizes are always within 2 of one another, and they are within 4 of each other for GCC. In fact, for GCC these numbers are within 2 each other except for the 5% sample-size cases. Since granularity has such a small effect on the results, we will only use function-level granularity when presenting the results in the next section.</p><p>For the repeated coverage maximization technique, the effects of changing granularity are different with each program. Figures <ref type="figure">6</ref> and<ref type="figure">7</ref> show the results of the repeated coverage maximization technique for subset sizes of up to 30% of the total. For both programs, the first coverage pass works well, in the sense that basic coverage finds a large proportion of defects. For GCC, the subsequent coverage passes are very inefficient, to the point that the total efficiency of the technique quickly drops below what would be expected for a random subset. For Jikes, the repeated coverage passes work better, so that the function-call data set finds slightly more defects than the other two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Comparison between filtering techniques</head><p>We now present the results of comparing basic coverage maximization, repeated coverage maximization, one-per-cluster sampling and failure pursuit. Figures <ref type="figure">8,</ref><ref type="figure">9</ref> and 10 show plots of subset size vs. defects found for the different techniques and subject programs. These figures include the results of random sampling, which serves as a control technique. In general we can see that these techniques make it possible to select test cases in a way that reveals more defects than simple random sampling would.</p><p>We will now present an overview of the results for each program and then present some general conclusions that can be drawn. Interestingly, the trends for GCC and Jikes are different depending on the subset size.</p><p>For GCC, using subsets of size less than 20% of the entire test pool, the most efficient technique is one-percluster sampling, followed closely by basic coverage maximization, and then failure pursuit. For larger sample sizes, failure pursuit and one-per-cluster sampling find a similar number of defects. It is interesting to note that repeated coverage maximization does not perform well at all on this data set, providing efficiency similar or below that of simple random sampling when the sample size is greater than that needed for basic coverage.</p><p>For Jikes, failure pursuit does about as well as one-percluster sampling for smaller sample sizes (less than 20%), and then becomes better for larger samples sizes. Both of Circles mark the results of basic coverage maximization.</p><p>these techniques are much better than basic or repeated coverage maximization, even though repeated coverage maximization does not perform as badly as it does with GCC.</p><p>For javac, basic coverage maximization does better than one-per-cluster sampling and failure pursuit, for the small sample size needed for full coverage. The efficiency of repeated coverage maximization quickly drops below that of one-per-cluster sampling and failure pursuit. Finally, one-per-cluster sampling is about as efficient as failure pursuit.</p><p>We can now draw some conclusions about these techniques:</p><p>For small sample sizes, basic coverage maximization can find defects efficiently. One-per-cluster sampling achieves comparably good, or better results, but does not necessarily achieve full coverage. Either technique can be used when selecting a small sample.</p><p>When running more tests than are selected by basic coverage maximization, it is more efficient to use oneper-cluster sampling or failure pursuit, rather than repeated coverage maximization.</p><p>For smaller sample sizes (less than 20%), One-percluster sampling seems to be as efficient or more efficient than failure pursuit, while the converse is true for larger sample sizes. This means that the sample size should be taken into consideration when selecting a distributionbased technique.</p><p>When studying failure pursuit, it is important to question whether it would be more efficient to simply increase the sample size for one-per-cluster sampling. It turns out that for larger sample sizes, failure pursuit finds new defects about as efficiently or more efficiently than simply increasing the sample size used for one-per-cluster sampling. This is somewhat surprising because failure pursuit looks for executions with profiles similar to those of known failures. One might expect that failures with similar profiles would have the same cause. In practice, failure pursuit found both new and old defects. In fact, failure pursuit found a similar or larger number of new defects than one-per-cluster sampling, but it also selected tests that revealed some defects repeatedly. For an example of this effect see Figure <ref type="figure" target="#fig_0">11</ref>, which shows the number of failure-causing test cases selected for GCC. In this figure we can see that failure pursuit selects many more failure-causing test cases than one-per-cluster sampling, even though Figure <ref type="figure">6</ref> shows that they find a similar number of defects. Notice that having multiple example of failures caused by a given defect can aid in debugging, and failure pursuit provides these without sacrificing efficiency in finding new defects. Another question of interest is whether coverage-based techniques and distribution-based techniques select similar tests. For each program and granularity we compared one of the subsets selected by basic coverage maximization with the subsets selected by one-per-cluster sampling. We found that for similar sample sizes, oneper-cluster sampling finds between 39% and 71% of the defects found by basic coverage maximization, plus a number of other defects. This indicates that the two techniques do find different defects. We experimented with combining the two techniques by simply running basic coverage maximization and then one-per-cluster sampling followed by failure pursuit. The result of doing this at the function level was negligible since coverage finds so few failures. Otherwise, for finer granularities and large one-per-cluster samples, the results are slightly better than a comparable failure pursuit for GCC, and they are similar for Jikes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Combining coverage-based and distributionbased techniques</head><p>As mentioned before, the fault-detection efficiency of the repeated coverage maximization technique drops after the first coverage pass, even falling below that of random sampling on one of the data sets. Because of this we explore the possibility of using the knowledge learned in Section 6.4 in order to create a prioritization algorithm that performs better on these data sets. The key observations are:</p><p>• The first coverage maximization pass is efficient at revealing defects. • One-per-cluster sampling and failure pursuit are efficient when selecting test sets larger than those selected by basic coverage maximization. • If additional tests are required, random sampling should work well enough. We examine the idea of prioritizing test cases by running the following 4 combinations of these algorithms:</p><p>• Basic Coverage Maximization + Random • Basic Coverage Maximization + One-per-cluster + Failure Pursuit + Random. • One-per-cluster + Failure Pursuit + Random • One-per-cluster + Failure Pursuit + Basic Coverage Maximization + Random For example, for the second technique listed above, we first execute the tests selected by the basic coverage maximization algorithm, in the order this algorithm selects them. Then we execute the tests selected by oneper-cluster sampling (with a 30% sample size) and then the ones selected by failure pursuit. For one-per-cluster sampling, clusters are processed in the order they were create by the clustering algorithm, which implies that tight clusters will be processed first. The remaining tests are then ordered randomly. We only ran one replication of the coverage, one-per-cluster sampling and pursuit parts.</p><p>Notice that this is a very simple way of combining these techniques. In the future we will explore better ways of creating prioritization techniques. show the results of comparing the best composite techniques for each program with the repeated coverage maximization technique. For Jikes and javac, the most efficient prioritization algorithm is to use basic coverage maximization + one-per-cluster sampling + pursuit + random, while done at a function call level. For GCC this combination is the second most efficient one, while the most efficient one (basic coverage maximization + random with basic blocks) has an APFD only larger by .2182 . Given that function call profiling can be done more efficiently, this combination should be attractive in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Comparison of prioritization techniques</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Future work</head><p>The results from this paper suggest future research in two different but related directions:</p><p>• We intend to evaluate distribution-based techniques and the new prioritization techniques in the setting of regression testing, where the only profiles available come from a previous version of the software. We expect the results to extend there, since the two settings are so similar. As part of this work, the distributionbased techniques will have to be extended to make use of the information on which elements of the program changed between the old and new versions, as has been done in the past for coverage-based techniques. • We intend to improve the distribution-based prioritization algorithms, since our results show that the order in which the test set was executed was not optimal. Different heuristics can be used to order the tests. One alternative is to take advantage of the hierarchical nature of our clustering algorithm. Another would be favor selection from small clusters, since it's been found that they are more likely to contain failures than are larger clusters [ <ref type="bibr" target="#b3">[3]</ref>]. Likewise we can integrate the coverage-and distributionbased techniques more tightly, so as to avoid redundant testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>Our experimental results show that both coveragebased and distribution-based filtering techniques can exhibit good defect-detection efficiency. Also, the results   suggest that when selecting test sets larger than those selected by basic coverage maximization, distributionbased techniques are more efficient than the repeated coverage maximization method. Most significantly, the results suggest that coverage-based and distribution-based techniques are complementary, in that they find different defects. Accordingly, some simple combinations of these techniques were evaluated and found to exhibit higher defect-detection efficiency than repeated coverage maximization. In order to determine the generality of these results, it will be necessary to replicate them with a variety of other subject programs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multidimensional Scaling display of the GCC dataset. Points represent test executions.</figDesc><graphic coords="4,63.36,75.65,240.00,162.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: One-per-cluster sampling results for GCC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 3 :Figure 4 :</head><label>534</label><figDesc>Figure 5: Failure pursuit results for Jikes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Repeated Coverage results for GCC.Circles mark the results of basic coverage maximization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :Figure 9 :Figure 10 :Figure 11 :</head><label>891011</label><figDesc>Figure 8: Comparison between techniques for GCC. Only the best granularity for each technique is shown.Circles mark the results of basic coverage maximization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 :</head><label>12</label><figDesc>Figure 13: Jikes prioritization results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Javac prioritization results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Graves et al examined the costs and benefits of several regression test selection techniques, including test suite minimization (greedy coverage maximization), a dataflow technique, a safe technique, and random selection [[8]]. In separate studies, Elbaum, et al [[5], [6]] and Rothermel et al [[24], [25]] compared several test case prioritization techniques, including ones based on code coverage, estimated fault proneness, and other factors. Kim and</figDesc><table><row><cell cols="4">Porter evaluated several regression test selection</cell></row><row><cell cols="4">techniques and a technique and a prioritization technique</cell></row><row><cell cols="4">of their own invention that exploits historical execution</cell></row><row><cell cols="4">data [[18]]. None of the selection or prioritization</cell></row><row><cell cols="4">techniques considered in the aforementioned studies are</cell></row><row><cell>distribution-based.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Distribution-based</cell><cell>filtering</cell><cell>and</cell><cell>prioritization</cell></row><row><cell cols="4">techniques are examples of observation-based testing,</cell></row><row><cell>which is described in [</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results for Basic Coverage Maximization</figDesc><table><row><cell></cell><cell>Selected</cell><cell>%</cell><cell>Defects</cell><cell>%</cell></row><row><cell>GCC Function</cell><cell>75.32</cell><cell>2.26</cell><cell>3.59</cell><cell>13.82</cell></row><row><cell>B.B.</cell><cell>388.03</cell><cell>11.64</cell><cell>15.33</cell><cell>58.96</cell></row><row><cell>Edges</cell><cell>477.18</cell><cell>14.31</cell><cell>16.70</cell><cell>64.26</cell></row><row><cell cols="2">Jikes Function 189.331</cell><cell>6.01</cell><cell>19.81</cell><cell>18.51</cell></row><row><cell>B.B.</cell><cell>599.76</cell><cell>19.04</cell><cell>40.18</cell><cell>37.55</cell></row><row><cell>Edges</cell><cell>733.10</cell><cell>23.28</cell><cell>46.35</cell><cell>43.30</cell></row><row><cell>javac Function</cell><cell>51.02</cell><cell>1.63</cell><cell>9.71</cell><cell>14.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Shows the APFD results for the different techniques being evaluated. These results confirm the previous observation that random ordering outperforms</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>APFD results for the various prioritization techniques. The best result for each row is in boldface. Jikes and javac. It is interesting to note that for GCC, on basic-block and arc granularity, the most efficient technique is to simply do one coverage maximization pass and then order the remaining test cases randomly.Figures 12, 13 and 14  </figDesc><table><row><cell>Program</cell><cell>Granularity</cell><cell>Random</cell><cell>R. Cov.</cell><cell>Basic</cell><cell></cell><cell>B. Cov. + OPC</cell><cell>OPC + Pursuit</cell><cell>OPC + Pursuit</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Coverage</cell><cell>+</cell><cell>+ Pursuit +</cell><cell>+ Random</cell><cell>+ B. Cov. +</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Random</cell><cell></cell><cell>Random</cell><cell></cell><cell>Random</cell></row><row><cell>GCC</cell><cell>Functions</cell><cell>80.17</cell><cell>57.0178</cell><cell>80.6450</cell><cell></cell><cell>83.7328</cell><cell>82.7494</cell><cell>82.6631</cell></row><row><cell></cell><cell>B.B.</cell><cell>80.17</cell><cell>79.1564</cell><cell>83.9510</cell><cell></cell><cell>83.3403</cell><cell>80.6029</cell><cell>80.8489</cell></row><row><cell></cell><cell>Edges</cell><cell>80.17</cell><cell>80.0984</cell><cell>83.5772</cell><cell></cell><cell>82.9071</cell><cell>81.8249</cell><cell>81.6054</cell></row><row><cell>Jikes</cell><cell>Functions</cell><cell>58.42</cell><cell>69.4726</cell><cell>61.7823</cell><cell></cell><cell>74.0731</cell><cell>73.5341</cell><cell>73.9989</cell></row><row><cell></cell><cell>B.B.</cell><cell>58.42</cell><cell>66.0189</cell><cell>61.8548</cell><cell></cell><cell>69.2586</cell><cell>72.8636</cell><cell>72.6568</cell></row><row><cell></cell><cell>Edges</cell><cell>58.42</cell><cell>66.383</cell><cell>62.5763</cell><cell></cell><cell>69.5447</cell><cell>73.025</cell><cell>73.4205</cell></row><row><cell>Javac</cell><cell>Functions</cell><cell>64.04</cell><cell>72.2793</cell><cell>68.7761</cell><cell></cell><cell>76.9962</cell><cell>75.4971</cell><cell>75.7261</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the 14th International Symposium on Software Reliability Engineering (ISSRE'03) 1071-9458/03 $ 17.00 © 2003 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Manual effort is needed initially to determine expected output, but no manual effort is required when self-validating tests are reused.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Some software defects have the property that almost any test case that exercises the defective code will induce an observable failure.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>We use the term failure to refer to a failed test case, as opposed to defect, which refers to the actual defect detected by that failure.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Validation, verification, and testing of computer software</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Adrion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Branstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Cherniavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="192" />
			<date type="published" when="1982-06">June 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Modern Multidimensional Scaling: Theory and Applications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Groenen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding failures by cluster analysis of execution profiles</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Podgurski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23 rd International Conference on Software Engineering</title>
		<meeting>the 23 rd International Conference on Software Engineering<address><addrLine>Toronto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pursuing failure: the distribution of program failures in a profile space</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Podgurski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10 th European Software Engineering Conference and 9th ACM SIGSOFT Symposium on the Foundations of Software Engineering</title>
		<meeting>the 10 th European Software Engineering Conference and 9th ACM SIGSOFT Symposium on the Foundations of Software Engineering<address><addrLine>Vienna</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001-09">September 2001</date>
			<biblScope unit="page" from="246" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prioritizing test cases for regression testing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Malishevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 International Symposium on Software Testing and Analysis</title>
		<meeting>the 2000 International Symposium on Software Testing and Analysis<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
			<biblScope unit="page" from="102" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Test case prioritization: a family of empirical studies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Malishevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="182" />
			<date type="published" when="2002-02">February 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The GCC Home Page</title>
		<author>
			<persName><surname>Gcc</surname></persName>
		</author>
		<ptr target="http://www.gnu.org/software/gcc/gcc.html" />
	</analytic>
	<monogr>
		<title level="m">Free Software Foundation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical study of regression test selection techniques</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Software Engineering and Methodology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="208" />
			<date type="published" when="2001-04">April, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A methodology for controlling the size of a test suite</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Software Engineering and Methodology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="285" />
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An empirical investigation of program spectra</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Approximation algorithms for NP-hard problems</title>
		<editor>Hochbaum, D. S.</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>PWS Publishing</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">International Business Machines Corporation, Jacks Project</title>
		<author>
			<persName><surname>Jacks</surname></persName>
		</author>
		<ptr target="www.ibm.com/developerworks/oss/cvs/jacks/" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="http://java.sun.com/j2se/1.3/docs/guide/jvmpi/jvmpi.html" />
		<title level="m">JavaTM Virtual Machine Profiler Interface (JVMPI)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sun Microsystems Inc., Java™ 2 Platform, Standard Edition, java.sun</title>
		<author>
			<persName><surname>Javac</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1995" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ibm</forename><surname>Jikes</surname></persName>
		</author>
		<author>
			<persName><surname>Developerworks</surname></persName>
		</author>
		<ptr target="www-124.ibm.com/developerworks/opensource/jikes/" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A history-based test prioritization technique for regression testing in resource constrained environments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24 th International Conference on Software Engineering</title>
		<meeting>the 24 th International Conference on Software Engineering<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multivariate visualization in observation-based testing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Podgurski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22 nd International Conference on Software Engineering</title>
		<meeting>the 22 nd International Conference on Software Engineering<address><addrLine>Limerick, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000-06">June 2000</date>
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Marick</surname></persName>
		</author>
		<title level="m">The Craft of Software Testing: Subsystem Testing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated support for classifying software failure reports</title>
		<author>
			<persName><forename type="middle">A</forename><surname>Podgurski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Masri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23 rd International Conference on Software Engineering</title>
		<meeting>the 23 rd International Conference on Software Engineering<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient Algorithm for Regression Test Selection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
		<author>
			<persName><surname>Safe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 Conference on Software Maintenance</title>
		<meeting>the 1993 Conference on Software Maintenance</meeting>
		<imprint>
			<date type="published" when="1993-09">September 1993</date>
			<biblScope unit="page" from="358" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Empirical studies of test suite reduction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Von Ronne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software Testing, Verification, and Reliability</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002-12">December, 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Test-case prioritization: an empirical study</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Untch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 International Conference on Software Maintenance</title>
		<meeting>the 1999 International Conference on Software Maintenance</meeting>
		<imprint>
			<date type="published" when="1999-08">August, 1999</date>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prioritizing test cases for regression testing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Untch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="929" to="948" />
			<date type="published" when="2001-10">October 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">jRapture: a capture/replay tool for observation-based testing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Podgurski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 International Symposium on Software Testing and Analysis</title>
		<meeting>the 2000 International Symposium on Software Testing and Analysis<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effect of test set size minimization and fault detection effectiveness</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Mathur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="347" to="369" />
			<date type="published" when="1998-04">April 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Test set size minimization and fault detection effectiveness: a case study in a space application</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pasquini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21 st Annual International Computer Software and Applications Conference</title>
		<meeting>the 21 st Annual International Computer Software and Applications Conference<address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-08">August 1997</date>
			<biblScope unit="page" from="522" to="528" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
