<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modal retrieval of trademark images using global similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">S</forename><surname>Ravela</surname></persName>
							<email>ravela@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Indexing and Retrieval Group Center for Intelligent Information Retrieval</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
							<email>manmatha¢@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Indexing and Retrieval Group Center for Intelligent Information Retrieval</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-modal retrieval of trademark images using global similarity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">95DEFF044E1DF9C07ED4D986001B66D7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper a system for multi-modal retrieval of trademark images is presented. Images are characterized and retrieved using associated text and visual appearance. A user initiates retrieval for similar trademarks by typing a text query. Subsequent searches can be performed by visual appearance or using both appearance and text information. Textual information associated with trademarks is searched using the INQUERY search engine. Images are searched visually using a method for global image similarity by appearance developed in this paper. Images are filtered with Gaussian derivatives and geometric features are computed from the filtered images. The geometric features used here are curvature and phase. Two images may be said to be similar if they have similar distributions of such features. Global similarity may, therefore, be deduced by comparing histograms of these features. This allows for rapid retrieval. The system's performance on a database of 2000 trademark images is shown. A trademark database obtained from the US Patent and Trademark Office containing 63000 design only trademark images and text is used to demonstrate scalability of the image search method and multi-modal retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Retrieval of similar trademarks is an interesting application for multimedia information retrieval. Consider the following example. The US Patent and Trademark Office has a repository that has to be searched for conflicting (similar) trademarks before one can be awarded to a company or individual. There are several issues that make this task attractive for multi-modal information retrieval techniques. First, current searches are labour intensive. The number of trademarks stored is enormous and examiners have to leaf through large number of trademarks before making a decision. Second, there is a distinct notion of visual similarity used to compare trademarks. This is usually a decisive factor in an award decision. Third, there is readily available text information describing and categorizing a trademark. A system that automates these functions and helps the examiner decide faster would be immensely valuable. Clearly trademarks need to be searched both by text and image content. Text retrieval is a better understood problem, and there are several search engines that are applicable.</p><p>However, the indexing and retrieval of images using their content is a difficult problem. A person using an image retrieval system usually seeks to find semantically relevant information. For example, a person may be looking for a picture of a leopard from a certain viewpoint. Or alternatively, the user may require a picture of Abraham Lincoln from a particular viewpoint. Since the automatic segmentation of an image into objects is a difficult and unsolved problem in computer vision, inferring semantic information from image content is difficult to do. However, many image attributes like color, texture, shape and "appearance" are often directly correlated with the semantics of the problem. For example, logos or product packages (e.g., a box of Tide) have the same color wherever they are found. The coat of a leopard has a unique texture while Abraham Lincoln's appearance is uniquely defined. These image attributes can often be used to index and retrieve images.</p><p>In this paper, a system for multi-modal retrieval combining textual information and visual appearance is presented. The system combines text search using INQUERY <ref type="bibr" target="#b1">[2]</ref> and image search. The image search was originally developed for general (heterogeneous) grey-level image collections <ref type="bibr" target="#b17">[18]</ref>. Here, it is applied to trademark images. Trademark images are large binary images rather than grey-level images. Trademark images may consist of geometric designs, more realistic pictures (for example, animals and people) as well as abstract images making them a challenging domain. Trademark images are also an example of a domain where there is an actual user need to find "similar" trademarks to avoid conflicts. Trademarks for this paper were obtained from the US Patent and Trademark office. The 63000 design trademarks used here contain images of trademarks and associated text describing the trademark.</p><p>Multi-modal retrieval begins with a user requesting trademarks that match a text query. The INQUERY search engine is used to find trademarks whose associated text match the query. The images associated with these trademarks are then displayed. Once an initial query is processed subsequent searches can be carried out by selecting the returned images and submitting them for retrieval by visual appearance or a combination of visual appearance and associated text.</p><p>INQUERY is a well known search engine for retrieving text which is based on a probabilistic retrieval model called an inference net The reader is referred to <ref type="bibr" target="#b1">[2]</ref> for details about the INQUERY engine. The current paper focuses on visual appearance representation, its quantitative evaluation with respect to trademarks, scalability to a large collection, and feasibility to multi-modal retrieval.</p><p>The visual appearance of an image is characterized here using the shape of the intensity surface. The images are filtered with Gaussian derivatives and geometric features are computed from the filtered images. The geometric features used here are the image shape index (which is a ratio of curvatures of the three dimensional intensity surface) and the local orientation of the gradient. Two images are said to be similar if they have similar distributions of such features. The images are, therefore, ranked by comparing histograms of these features. Recall/Precision results with this method is tabulated with a database of about 2000 trademark images. Then multi-modal retrieval is demonstrated on a collection of 63000 trademark images.</p><p>The rest of the paper is organized as follows. Section 2 provides some background on the image retrieval area as well as on the appearance matching framework used in this paper. Section 3 surveys related work in the literature. In section 4, the notion of appearance is developed further and characterized using Gaussian derivative filters and the derived global representation is discussed. Section 5 shows how the representation may be scaled for multi-modal retrieval from a database of about 63,000 trademark images. A discussion and conclusion follows in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation and Background</head><p>The different image attributes like color, texture, shape and appearance have all been used in a variety of systems for retrieving images similar to a query image (see 3 for a review). Systems like QBIC <ref type="bibr" target="#b5">[6]</ref> and Virage <ref type="bibr" target="#b4">[5]</ref> allow users to combine color, texture and shape to retrieve a database of general images. One weakness of such a system is that attributes like color do not have direct semantic correlates when applied to a database of general images. For example, say a picture of a red and green parrot is used to retrieve images based on their similarity in color with it. The retrievals may include other parrots and birds as well as red flowers with green stems and other images. While this is a reasonable result when viewed as a matching problem, clearly it is not a reasonable result for a retrieval system. The problem arises because color does not have a good correlation with semantics when used with general images. However, if the domain or set of images is restricted to say flowers, then color has a direct semantic correlate and is useful for retrieval (see <ref type="bibr" target="#b2">[3]</ref> for an example). Some attempts have been made to retrieve objects using their shape <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>. For example, the QBIC system <ref type="bibr" target="#b5">[6]</ref>, developed by IBM, matches binary shapes. It requires that the database be segmented into objects. Since automatic segmentation is an unsolved problem, this requires the user to manually outline the objects in the database. Clearly this is not desirable or practical.</p><p>Except for certain special domains, all methods based on shape are likely to have the same problem. An object's appearance depends not only on its three dimensional shape, but also on the object's albedo, the viewpoint from which it is imaged and a number of other factors. It is non-trivial to separate the different factors constituting an object's appearance and it is usually not possible to separate an object's three dimensional shape from the other factors. For example, the face of a person has a unique appearance that cannot just be characterized by the geometric shape of the 'component parts'. In this paper a characterization of the shape of the intensity surface of imaged objects is used for retrieval. The experiments conducted show that retrieved objects have similar visual appearance, and henceforth an association is made between 'appearance' and the shape of the intensity surface.</p><p>Similarity can be computed using either local or global methods. In local similarity, a part of the query is used to match a part of a database image or images. One approach to computing local similarity <ref type="bibr" target="#b17">[18]</ref> is to have the user outline the salient portions of the query (eg. the wheels of a car or the face of a person) and match the outlined portion of the query with parts of images in the database. Although, the technique works well in extracting relevant portions of objects embedded against backgrounds it is slow. The slow speed stems from the fact that the system must not only answer the question "is this image similar" but also the question "which part of the image is relevant". This paper focuses on a representation for computing global similarity. That is, the task is to find images that, as a whole, appear visually similar. The utility of global similarity retrieval is evident, for example, in finding similar scenes or similar faces in a face database. Global similarity also works well when the object in question constitutes a significant portion of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Appearance based retrieval</head><p>The image intensity surface is robustly characterized using features obtained from responses to multiscale Gaussian derivative filters. Koenderink <ref type="bibr" target="#b13">[14]</ref> and others <ref type="bibr" target="#b6">[7]</ref> have argued that the local structure of an image can be represented by the outputs of a set of Gaussian derivative filters applied to an image. That is, images are filtered with Gaussian derivatives at several scales and the resulting response vector locally describes the structure of the intensity surface. By computing features derived from the local response vector and accumulating them over the image, robust representations appropriate to querying images as a whole (global similarity) can be generated. One such representation uses histograms of features derived from the multi-scale Gaussian derivatives. Histograms form a global representation because they capture the distribution of local features (A histogram is one of the simplest ways of estimating a non parametric distribution). This global representation can be efficiently used for global similarity retrieval by appearance and retrieval is very fast.</p><p>The choice of features often determines how well the image retrieval system performs. Here, the task is to robustly characterize the 3-dimensional intensity surface. A 3-dimensional surface is uniquely de-termined if the local curvatures everywhere are known. Thus, it is appropriate that one of the features be local curvature. The principal curvatures of the intensity surface are invariant to image plane rotations, monotonic intensity variations and further, their ratios are in principle insensitive to scale variations of the entire image. However, spatial orientation information is lost when constructing histograms of curvature (or ratios thereof) alone. Therefore we augment the local curvature with local phase, and the representation uses histograms of local curvature and phase.</p><p>Local principal curvatures and phase are computed at several scales from responses to multi-scale Gaussian derivative filters. Then histograms of the curvature ratios <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref> and phase are generated. Thus, the image is represented by a single vector (multi-scale histograms). During run-time the user presents an example image as a query and the query histograms are compared with the ones stored, and the images are then ranked and displayed in order to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The choice of domain</head><p>There are two issues in building a content based image retrieval system. The first issue is technological, that is, the development of new techniques for searching images based on their content. The second issue is user or task related, in the sense of whether the system satisfies a user need. While a number of content based retrieval systems have been built ( <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>), it is unclear what the purpose of such systems is and whether people would actually search in the fashion described.</p><p>In this paper we describe how the techniques described here may be scaled to retrieve images from a database of about 63000 trademark images provided by the US Patent and Trademark Office. This database consists of all (at the time the database was provided) the registered trademarks in the United States which consist only of designs (i.e. there are no words in them). Trademark images are a good domain with which to test image retrieval. First, there is an existing user need: trademark examiners do have to check for trademark conflicts based on visual appearance. That is, at some stage they are required to look at the images and check whether the trademark is similar to an existing one. Second, trademark images may consist of simple geometric designs, pictures of animals or even complicated designs. Thus, they provide a test-bed for image retrieval algorithms. Third, there is text associated with every trademark and the associated text maybe used in a number of ways. One of the problems with many image retrieval systems is that it is unclear where the example or query image will come from. In this paper, the associated text is used to provide an example or query image. In addition associated text can also be combined with image searches. Using trademark images does have some limitations. First, we are restricted to binary images (albeit large ones). As shown later in the paper, this does not create any problems for the algorithms described here. Second, in some cases the use of abstract images makes the task more difficult. Others have attempted to get around it by restricting the trademark images to geometric designs <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Several authors have tried to characterize the appearance of an object via a description of the intensity surface. In the context of object recognition <ref type="bibr" target="#b20">[21]</ref> represent the appearance of an object using a parametric eigen space description. This space is constructed by treating the image as a fixed length vector, and then computing the principal components across the entire database. The images therefore have to be size and intensity normalized, segmented and trained. Similarly, using principal component representations described in <ref type="bibr" target="#b10">[11]</ref> face recognition is performed in <ref type="bibr" target="#b25">[26]</ref>. In <ref type="bibr" target="#b23">[24]</ref> the traditional eigen representation is augmented by using most discriminant features and is applied to image retrieval. The authors apply eigen representation to retrieval of several classes of objects. The issue, however , is that these classes are manually determined and training must be performed on each. The approach presented in this paper is different from all the above because eigen decompositions are not used at all to characterize appearance.</p><p>Further, the method presented uses no learning and, does not require constant sized images. It should be noted that although learning significantly helps in such applications as face recognition, however, it may not be feasible in many instances where sufficient examples are not available. This system is designed to be applied to a wide class of images and there is no restriction per se.</p><p>In earlier work we showed that local features computed using Gaussian derivative filters can be used for local similarity, i.e. to retrieve parts of images <ref type="bibr" target="#b17">[18]</ref>. Here we argue that global similarity can be determined by computing local features and comparing distributions of these features. This technique gives good results, and is reasonably tolerant to view variations. Schiele and Crowley <ref type="bibr" target="#b22">[23]</ref> used such a technique for recognizing objects using grey-level images. Their technique used the outputs of Gaussian derivatives as local features. A multi-dimensional histogram of these local features is then computed. Two images are considered to be of the same object if they had similar histograms. The difference between this approach and the one presented by Schiele and Crowley is that here we use 1D histograms (as opposed to multi-dimensional) and further use the principal curvatures as the primary feature.</p><p>The use of Gaussian derivative filters to represent appearance is motivated by their use in describing the spatial structure <ref type="bibr" target="#b13">[14]</ref> and its uniqueness in representing the scale space of a function <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25]</ref> The invariance properties of the principal curvatures are well documented in <ref type="bibr" target="#b6">[7]</ref>. Nastar <ref type="bibr" target="#b19">[20]</ref>, has independently used the image shape index to compute similarity between images. However, in his work curvatures were computed only at a single scale. This is insufficient.</p><p>In the context of global similarity retrieval it should be noted that representations using moment invariants have been well studied <ref type="bibr" target="#b18">[19]</ref>. In these methods global representation of appearance may involve computing a few numbers over the entire image. Two images are then considered similar if these numbers are close to each other (say using an L2 norm). We argue that such representations are not able to really capture the "appearance" of an image, particularly in the context of trademark retrieval where mo-ment invariants are widely used. In other work <ref type="bibr" target="#b17">[18]</ref> we compared moment invariants with the technique presented here and found that moment invariants work best for a single binary shape without holes in it, and, in general, fare worse than the method presented here. Jain and Vailaya <ref type="bibr" target="#b9">[10]</ref> used edge angles and invariant moments to prune trademark collections and then use template matching to find similarity within the pruned set. Their database was limited to 1100 images.</p><p>Texture based image retrieval is also related to the appearance based work presented in this paper. Using Wold modeling, in <ref type="bibr" target="#b15">[16]</ref> the authors try to classify the entire Brodatz texture and in <ref type="bibr" target="#b7">[8]</ref> attempt to classify scenes, such as city and country. Of particular interest is work by <ref type="bibr" target="#b16">[17]</ref> who use Gabor filters to retrieve texture similar images.</p><p>The earliest general image retrieval systems were designed by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>. In <ref type="bibr" target="#b5">[6]</ref> the shape queries require prior manual segmentation of the database which is undesirable and not practical for most applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Global representation of appearance</head><p>Three steps are involved in order to computing global similarity. First, local derivatives are computed at several scales. Second, derivative responses are combined to generate local features, namely, the principal curvatures and phase and, their histograms are generated. Third, the 1D curvature and phase histograms generated at several scales are matched. These steps are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Computing local derivatives:</head><p>Computing derivatives using finite differences does not guarantee stability of derivatives. In order to compute derivatives stably, the image must be regularized, or smoothed or band-limited. A Gaussian filtered image ¤ ¦¥ ¨ § ¤ © obtained by convolving the image I with a normalized Gaussian "! is a band-limited function. Its high frequency components are eliminated and derivatives will be stable. In fact, it has been argued by Koenderink and van Doorn <ref type="bibr" target="#b13">[14]</ref> and others <ref type="bibr" target="#b6">[7]</ref> that the local structure of an image I at a given scale can be represented by filtering it with Gaussian derivative filters (in the sense of a Taylor expansion), and they term it the N-jet. However, the shape of the smoothed intensity surface depends on the scale at which it is observed. For example, at a small scale the texture of an ape's coat will be visible. At a large enough scale, the ape's coat will appear homogeneous. A description at just one scale is likely to give rise to many accidental mis-matches. Thus it is desirable to provide a description of the image over a number of scales, that is, a scale space description of the image. It has been shown by several authors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7]</ref>, that under certain general constraints, the Gaussian filter forms a unique choice for generating scale-space. Thus local spatial derivatives are computed at several scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Histograms:</head><p>The normal and tangential curvatures of a 3-D surface (X,Y,Intensity) are defined as <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_0"># $ % &amp; "! ' § ( ) ) 0 ¤ 1 2 ¤ 43 53 76 8¤ 1 3 ¤ 2 92 A@ CB ¤ 2 ¤ 43 D¤ 2 3 E ¤ 1 2 6 C¤ 1 3 GF IH P Q SR R T U$ 7 "! V $ % &amp; "! ' § ( ) ) 0 E ¤ 1 2 @ ¤ 1 3 F ¤ 2 3 '6 W ¤ 2 92 A@ ¤ 43 53 X! Y¤ 2 ¤ D3 E ¤ 1 2 6 C¤ 13 F H P Q SR R T $ % I!</formula><p>Where ¤ 2 U$ 7 "! and ¤ 43 ' $ % &amp; "! are the local derivatives of Image I around point $ using Gaussian derivative at scale . Similarly ¤ 2 92 ab cd! , ¤ 2 3 ' ab cd! , and ¤ 43 53 ' ab cd! are the corresponding second derivatives. The normal curvature # and tangential curvature V are then combined <ref type="bibr" target="#b12">[13]</ref> to generate a shape index as follows:</p><formula xml:id="formula_1">e U$ 7 "! ' § Wf hg 5f pi rq # 6 V # @ V ts u U$ 7 "!</formula><p>The index value</p><formula xml:id="formula_2">e is v 1 when # § V</formula><p>and is undefined when either # and V are both zero, and is, therefore, not computed. This is interesting because very flat portions of an image (or ones with constant ramp) are eliminated. For example in Figure <ref type="figure" target="#fig_1">1</ref>, the background in most of these images does not contribute to the curvature histogram. The curvature index or shape index is rescaled and shifted to the range w yx Y c X as is done in <ref type="bibr" target="#b3">[4]</ref>. A histogram is then computed of the valid index values over an entire image.</p><p>The second feature used is phase. The phase is simply defined as</p><formula xml:id="formula_3">$ 7 "! ' § Wf hg 5f pi B ¤ 3 ' $ % &amp; "! ¤ 2 U$ 7 "! &amp;! .</formula><p>Note that is defined only at those locations where e is and ignored elsewhere. As with the curvature index is rescaled and shifted to lie between the interval w Sx Y X X .</p><p>At different scales different local structures are observed and, therefore, multi-scale histograms are a more robust representation. Consequently, a feature vector is defined for an image ¤ as the vector</p><formula xml:id="formula_4">§ t U &amp;! c c 9 t U h! 9 d 7 U &amp;! c X d % U h! &amp;e</formula><p>where d and are the curvature and phase histograms respectively. We found that using 5 scales gives good results and the scales are 'ff5g in steps of half an octave.</p><p>C. Matching feature histograms: Two feature vectors are compared using normalized cross-covariance defined as h yi § kj dl nm ` tj ol pm i q q q q q q kj dl nm q q q q q q q q q q q q tj ol pm i q q q q q q where rj sl nm § t @ ru wv f i p ! .</p><p>Retrieval is carried out as follows. A query image is selected and the query histogram vector "x is correlated with the database histogram vectors using the above formula. Then the images are ranked by their correlation score and displayed to the user. In this implementation, and for evaluation purposes, the ranks are computed in advance, since every query image is also a database image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments</head><p>The curvature-phase method is evaluated on a small database of 2048 images obtained from the US Patent and Trademark Office (PTO). The images obtained from the PTO are large, binary and are converted to gray-level and reduced for the experiments. This smaller set is used because relevance judgments can be obtained relatively easily.</p><p>In the following experiments an image is selected and submitted as a query. The objective of this query is stated and the relevant images are decided in advance. Then the retrieval instances are gauged against the stated objective. In general, objectives of the form 'extract images similar in appearance to the query' will be posed to the retrieval algorithm. A measure of the performance of the retrieval engine can be obtained by examining the recall/precision table for several queries. Briefly, recall is the proportion of the relevant material actually retrieved and precision is the proportion of retrieved material that is relevant <ref type="bibr" target="#b26">[27]</ref>. It is a standard widely used in the information retrieval community and is one that is adopted here. Queries were submitted for the purpose of computing recall/precision. The judgment of relevance is qualitative. For each query in both databases the relevant images were decided in advance. These were restricted to 48. The top 48 ranks were then examined to check the proportion of retrieved images that were relevant. All images not retrieved within 48 were assigned a rank equal to the size of the database. That is, they are not considered retrieved. These ranks were used to interpolate and extrapolate precision at all recall points. In the case of assorted images relevance is easier to determine and more similar for different people. However in the trademark case it can be quite difficult and therefore the recall-precision can be subject to some error. The recall/precision results are summarized in Table <ref type="table" target="#tab_0">1</ref> and both databases are individually discussed below.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> shows the performance of the algorithm on the trademark images. Each strip depicts the top 8 retrievals, given the leftmost as the query. Most of the shapes have roughly the same structure as the query. Note that, outline and solid figures are treated similarly (see rows one and two in Figure <ref type="figure" target="#fig_1">1</ref>). Six queries were submitted for the purpose of computing recall-precision in Table <ref type="table" target="#tab_0">1</ref>. Tests were also carried out with an assorted collection of 1561 grey-level images. These results are discussed elsewhere <ref type="bibr" target="#b0">[1]</ref>, and the recall/precision table is shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>While the queries presented here are not "optimal" with respect to the design constraints of global similarity retrieval, they are however, realistic queries that can be posed to the system. Mismatches can and do occur. The first is the case where the global appearance is very different. Second, mismatches can occur at the algorithmic level. Histograms coarsely represent spatial information and therefore will admit images with non-trivial deformations. The recall/precision presented here compares well with text retrieval. The time per retrieval is of the order of milli-seconds. In the next section we discuss the application of the presented technique to a database of 63000 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Trademark Retrieval</head><p>The system indexes 63,718 trademarks from the US Patent and Trademark office in the design only category. These trademarks are binary images. In addition, associated text consists of a design code that designates the type of trademark, the goods and services associated with the trademark, a serial number and a short descriptive text.</p><p>The system for browsing and retrieving trademarks is illustrated in Figure <ref type="figure" target="#fig_2">2</ref>. The netscape/Java user interface has two search-able parts. On the left a panel is included to initiate search using text. Any or all of the fields can be used to enter a query. In this example, the text "Merriam Webster' is entered. All images associated with it are retrieved using the INQUERY <ref type="bibr" target="#b1">[2]</ref> text search engine. The user can then use any of the example pictures to search for images that are similar visually or restrict it to images with <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8)</ref>. A histogram descriptor of the image is obtained by concatenating all the individual histograms across scales and regions. These two steps are conducted off-line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Execution:</head><p>The image search server begins by loading all the histograms into memory. Then it waits on a port for a query. A CGI client transmits the query to the server. Its histograms are matched with the ones in the database. The match scores are ranked and the top # requested retrievals are returned.  The original image is returned as the first result (as it should be). The images in positions 2,3 and 5 in the second window all contain circles inside squares and this configuration is similar to that of the query. Most of the other images are of objects contained inside a roughly square box and this is reasonable considering that similarity is defined on the basis of the entire image rather than a part of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Examples</head><p>The second example is shown in Figure <ref type="figure" target="#fig_3">3</ref>. Here the user has typed in the word Apple. The system returns trademarks associated with the word Apple. The user queries using Apple computer's logo (the image in the second row, first column of the first window). Images retrieved in response to this query are shown in the right window. The first eight retrievals are all copies of Apple Computer's trademark (Apple used the same trademark for a number of other goods and so there are multiple copies of the trademark in the database). Trademarks number 9 and 10 look remarkably similar to Apple's trademark.</p><p>They are considered valid trademarks because they are used for goods and services in areas other than computers. Trademark 13 is another version of Apple Computer's logo but with lines in the middle.</p><p>Although somewhat visually different it is still retrieved in the high ranks. Image 14 is an interesting example of a mistake made by the system. Although the image is not of an apple, the image has similar distributions of curvature and phase as is clear by looking at it. The second image is an actual conflict. The image is a logo which belongs to the Atlanta Macintosh User's Group. The text describes the image as a peach but visually one can see how the two images may be confused with each other (which is the basis on which trademark conflicts are adjudicated). This example shows that it does not suffice to go by the text descriptions alone and image search is useful for trademarks. Notice that the fourth image which some people describe as an apple and others as a tomato is also described in the text as an apple.</p><p>The system has been tried on a variety of different examples of both two dimensional and three dimensional pictures of trademarks and had worked quite well. Clearly, there are issues of how quantitative results can be obtained for such large image databases (it is not feasible for a person to look at every image in the database to determine whether it is similar). In future work, we hope to evolve a mechanism for quantitative testing on such large databases. It will also be important to use more of the textual information to determine trademark conflicts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Limitations</head><p>This paper demonstrates multi-modal retrieval of trademarks. Both text and images separately and together are used to retrieve trademarks. Text search is done using INQUERY while image search is done on the basis of similarity in visual appearance. Visual appearance is characterized using filter responses to Gaussian derivatives over scale space. In addition, we claim that global representations are better constructed by representing the distribution of robustly computed local features. The paper shows that it is not sufficient to use text or image search alone to retrieve trademarks.</p><p>Currently we are investigating three issues. First is to scale the database up to about 600000 images.</p><p>The second is to incorporate user feedback or preferences of retrieved images. The third is to combine text retrieval and image retrieval in a principled manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>£</head><label></label><figDesc>This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC-9209623, in part by the United States Patent and Trademarks Office and the Defense Advanced Research Projects Agency/ITO under ARPA order number D468, issued by ESC/AXS contract number F19628-95-C-0235, in part by the National Science Foundation under grant IRI-9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Trademark retrieval using Curvature and Phase</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Retrieval in response to a "Merriam Webster" query In Figure 2, the user typed in Merriam Webster in the text window. The system searches for trademarks which have either Merriam or Webster in th associated text and displays them. Here, the first two trademarks (first two images in the left window) belong to Merriam Webster. In this example, the user has chosen to 'click' the second image and search for images of similar trademarks. This search is based entirely on the image and the results are displayed in the right window in rank order. Retrieval takes a few seconds and is done by comparing histograms of all 63,718 trademarks on the fly.</figDesc><graphic coords="12,67.92,223.56,484.82,342.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Retrieval in response to the query "Apple"</figDesc><graphic coords="13,71.52,71.86,477.62,339.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Retrieval in response to the query "Apple" limited to text searches The third example demonstrates combining text and visual appearance for searching. We use the same apple image obtained in the previous image as the image query. However, in the text box we now type "computer" and turn the text combination mode on. We now search for trademarks which are visually similar to the apple query image but also have the words computer associated with them. The results are shown in Figure 4 on the right-hand side. Notice that the first image is the same as the query image.</figDesc><graphic coords="14,91.08,71.70,438.22,306.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Precision at standard recall points for six Queries</figDesc><table><row><cell>Recall 0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90 100</cell></row><row><cell cols="10">Precision(trademark) % 100 93.2 93.2 85.2 76.3 74.5 59.5 45.5 27.2 9.0 9.0</cell></row><row><cell cols="10">Precision(assorted) % 100 92.6 90.0 88.3 87.0 86.8 83.8 65.9 21.3 12.0 1.4</cell></row><row><cell>average(trademark) 61.1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>average(assorted) 66.3%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The file numbers for affidavits filed Live/ dead Whether the trademark is active or not relevant text, thereby combining the image and text searches. In the specific example shown, The second image is selected and retrieved results are displayed on the right panel. The user can then continue to search using any of the displayed pictures as the query.</p><p>Text was provided for each image in the collection of design only trademark category from the Patent and Trademark Office. This information contained specific fields such as the design code, the goods and services provided, the serial number, the manufacturer, among others. Table <ref type="table">2</ref> lists all the fields associated with a trademark. These were indexed and used for retrieval using the INQUERY search engine <ref type="bibr" target="#b1">[2]</ref>. The queries that can be submitted are conjunctive (and) of all the words that are entered in all the fields allowed within the interface with equal weighting. These fields are shown at the top of the left panel in Figure <ref type="figure">2</ref>.</p><p>In this section we describe the curvature/phase histograms to retrieve visually similar trademarks and demonstrate searches using text and visual information. The following steps are performed to retrieve images.</p><p>Preprocessing: Each binary image in the database is first size normalized, by clipping. Then they are converted to gray-scale and reduced in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation of Histograms:</head><p>Each processed image is divided into four equal rectangular regions. This is different than constructing a histogram based on pixels of the entire image. This is because in scaling the images to a large collection, we found that the added degree of spatial resolution significantly improves the retrieval performance. The curvature and phase histograms are computed for each tile at three scales</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">On Computing Global Similarity in Images</title>
		<imprint>
			<date type="published" when="1998-10">Oct 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The inquery retrieval system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3y {z International Conference on Database and Expert System Applications</title>
		<meeting>the 3y {z International Conference on Database and Expert System Applications</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Indexing flowers by color names using domain knowledge-driven segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proc. of the 4th IEEE Workshop on Applications of Computer Vision (WACV&apos;98)</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10">Oct 1998</date>
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cosmos -a representation scheme for free form surfaces</title>
		<author>
			<persName><forename type="first">Chitra</forename><surname>Dorai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Intl. Conf. on Computer Vision</title>
		<meeting>5th Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1024" to="1029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The virage image search engine: An open framework for image management</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE conf. on Storage and Retrieval for Still Image and Video Databases IV</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="133" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Query by image and video content: The qbic system</title>
		<author>
			<persName><forename type="first">Myron</forename><surname>Flickner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Magazine</title>
		<imprint>
			<biblScope unit="page" from="23" to="30" />
			<date type="published" when="1995-09">Sept. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Syntactical Structure of Scalar Images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Florack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Utrecht, Holland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Utrecht</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Texture orientation for sorting photos &apos;at a glance</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Gorkani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Conf. on Pattern Recognition</title>
		<meeting>12th Int. Conf. on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994-10">October 1994</date>
			<biblScope unit="volume">464</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Artisan -a shape retrieval system based on boundary family indexing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Eakins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Boardman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE conf. on Storage and Retrieval for Image and Video Databases IV</title>
		<meeting>SPIE conf. on Storage and Retrieval for Image and Video Databases IV<address><addrLine>San Jose</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-02">Feb 1996</date>
			<biblScope unit="volume">2670</biblScope>
			<biblScope unit="page" from="17" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shape-based retrieval: A case study with trademark image databases</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vailaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1369" to="1390" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Application of the kruhnen-loeve procedure for the characterization of human faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sirovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. and Mach. Intel</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="108" />
			<date type="published" when="1990-01">January 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The structure of images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="363" to="396" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Surface shape and curvature scales</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representation of local geometry in the visual system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="367" to="375" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scale-Space Theory in Computer Vision</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Periodicity, directionality, and randomness: Wold features for image modeling and retrieval</title>
		<author>
			<persName><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="722" to="733" />
			<date type="published" when="1996-07">July 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Texture-based pattern retrieval from image databases</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="51" />
			<date type="published" when="1996-01">January 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On computing local and global similarity in images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE conf. on Human and Electronic Imaging III</title>
		<meeting>SPIE conf. on Human and Electronic Imaging III</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shape Measures for Content Based Image Retrieval: A Comparison</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Methre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The image shape spectrum for image retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nastar</surname></persName>
		</author>
		<idno>3206</idno>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parametric appearance representation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Early Visual Learning</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1996-02">February 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photobook: Tools for content-based manipulation of databases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Storage and Retrieval for Image and Video Databases II</title>
		<meeting>Storage and Retrieval for Image and Video Databases II</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="34" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object recognition using multidimensional receptive field histograms</title>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th European Conf. Computer Vision</title>
		<meeting>4th European Conf. Computer Vision<address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-04">April 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using discriminant eigen features for retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Swets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. and Mach. Intel</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="831" to="836" />
			<date type="published" when="1996-08">August 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Geometry Driven Diffusion in Computer Vision</title>
		<author>
			<persName><forename type="first">Bart</forename><forename type="middle">M</forename><surname>Ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Har</forename><surname>Romeny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Cognitive NeuroScience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval. Butterworths</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scale-space filtering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Joint Conf. Art. Intell</title>
		<meeting>Intl. Joint Conf. Art. Intell</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="1019" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
