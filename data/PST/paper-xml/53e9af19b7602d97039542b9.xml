<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic generation of agents for collecting hidden Web pages for data extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Juliano</forename><surname>Palmieri Lage</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Ciência da Computac ßão</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais</orgName>
								<address>
									<postCode>31270-901</postCode>
									<settlement>Belo Horizonte</settlement>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Altigran</forename><forename type="middle">S</forename><surname>Da Silva</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Ciência da Computac ßão</orgName>
								<orgName type="institution">Universidade Federal do Amazonas</orgName>
								<address>
									<postCode>69077-000</postCode>
									<settlement>Manaus</settlement>
									<region>AM</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paulo</forename><forename type="middle">B</forename><surname>Golgher</surname></persName>
							<email>golgher@akwan.com.br</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Akwan Information Technologies</orgName>
								<address>
									<addrLine>Av. Antônio Abraão Caram, 430-4o. Andar</addrLine>
									<postCode>31275-000</postCode>
									<settlement>Belo Horizonte</settlement>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alberto</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
							<email>laender@dcc.ufmg.br</email>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Ciência da Computac ßão</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais</orgName>
								<address>
									<postCode>31270-901</postCode>
									<settlement>Belo Horizonte</settlement>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic generation of agents for collecting hidden Web pages for data extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">037B0E10AB9CD75B31B360F0B97E5440</idno>
					<idno type="DOI">10.1016/j.datak.2003.10.003</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collecting agents</term>
					<term>Hidden Web</term>
					<term>Navigation patterns</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the Web grows, more and more data has become available under dynamic forms of publication, such as legacy databases accessed by an HTML form (the so called hidden Web). In situations such as this, integration of this data relies more and more on the fast generation of agents that can automatically fetch pages for further processing. As a result, there is an increasing need for tools that can help users generate such agents. In this paper, we describe a method for automatically generating agents to collect hidden Web pages. This method uses a pre-existing data repository for identifying the contents of these pages and takes the advantage of some patterns that can be found among Web sites to identify the navigation paths to follow. To demonstrate the accuracy of our method, we discuss the results of a number of experiments carried out with sites from different domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the popularization of the World-Wide Web, a huge amount of data from a number of different domains has become available. However, managing and querying such data is not trivial since we cannot make use of traditional database techniques. One way to deal with this data is through the so called Web wrappers <ref type="bibr" target="#b0">[1]</ref>, programs that extract unstructured data from Web pages and store it in suitable formats such as XML and relational tables.</p><p>To accomplish their task, Web wrappers take as input a set of pages from a Web source. This set of pages is generally collected by Web agents such as spiders or crawlers. Traditionally, these agents cover only the so called Publicly Indexable Web (PIW) <ref type="bibr" target="#b1">[2]</ref>, which corresponds to the set of Web pages reachable only by following hyperlinks. However, recent studies point out that the great majority of the pages containing useful data on the Web is outside the PIW <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. This large portion of the Web is generally called the hidden <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> or deep <ref type="bibr" target="#b2">[3]</ref> Web. Pages in the hidden Web are dynamically generated by programs in response to forms submitted to searchable databases. Thus, to allow wrappers to deal with the data on such pages we need a special type of agent to collect them, a type we call the hidden Web agent.</p><p>To illustrate the functionality expected from a hidden Web agent, suppose we want to fetch a set of data-rich pages 1 from the Bookpool Web site. 2 We start from the siteÕs main page, depicted in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Although we have a simple search box there (see arrow 1), an advanced search form, where users can better specify their needs, is preferable. Selecting the ''Search'' hyperlink (arrow 2) takes us to the advanced search page depicted in Fig. <ref type="figure" target="#fig_0">1(b</ref>). After filling in some fields and submitting the form, an answer page is presented, as shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>. Note that not all information available is presented in the first answer page. Thus we should select the ''Next'' hyperlink (arrow 3) to get other pages, like the one in Fig. <ref type="figure" target="#fig_1">2(b</ref>). An agent to fetch these pages should act as described.</p><p>In this paper, we present a method to generate hidden Web agents for sites with common navigational characteristics. The method uses a set of heuristics and a sample data repository for 1 A data-rich page is a page that contains identifiable constants (data item values) related to a specific application domain <ref type="bibr" target="#b3">[4]</ref>. 2 http://www.bookpool.com. automatically finding relevant forms, filling them in, and collecting pages containing useful data. We also describe the results of a number of experiments carried out with sites from different domains to evaluate the accuracy of our method. These results show that our method is successful on 80% of the sites we have used in our experiments. The rest of the paper is organized as follows. Section 2 overviews related work. In Section 3 we introduce the navigation pattern concept. Section 4 outlines our method for generating hidden Web agents. Experimental results are described in Section 5. Finally, in Section 6 we conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In the last few years, we have witnessed an increasing interest in exploring the information available in the hidden Web. The term hidden Web was used by Lawrence and Giles <ref type="bibr" target="#b1">[2]</ref> to refer to the huge set of Web pages dynamically generated as the output of queries over databases (or other kinds of remote processing), which are usually produced as the result of filling and submitting HTML forms. Bergman <ref type="bibr" target="#b2">[3]</ref> experimentally demonstrated that a great portion of the hidden Web consists of pages generated from topic-specific databases, which contain large amounts of highquality data. These pages are often termed data-rich <ref type="bibr" target="#b3">[4]</ref> or data-intensive <ref type="bibr" target="#b4">[5]</ref>.</p><p>The literature has also addressed the problem of data extraction from data-rich Web pages <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. This problem is relevant because, once extracted, the data can be handled in a way similar to instances of a traditional database. In general, the goal is to provide automatic or semi-automatic tools for wrapper generation. The approaches proposed in the literature use techniques borrowed from areas such as natural language processing, languages and grammars, machine learning, information retrieval, databases, and ontologies. We refer the interested reader to <ref type="bibr" target="#b13">[14]</ref> for a brief survey and a qualitative comparison of several Web data extraction tools present in the literature.</p><p>Motivated by the abundance of data-rich pages in the hidden Web, many researchers have proposed solutions to the problem of automatically retrieving these pages. The pioneer work in automatically accessing pages in the hidden Web is ShopBot <ref type="bibr" target="#b14">[15]</ref>, a comparison-shopping agent that uses AI-based techniques to fetch and extract dynamically generated information to help users do their shopping. ShopBot operates by creating a description of a vendorÕs site, automatically navigating through its pages to find forms, and using sets of domain specific heuristic rules (e.g., regular expressions encoding attribute synonyms) to fill these forms and analyze whether a particular one is interesting or not for the the shopping task. This approach, however, is only applicable to some shopping domains and is not able to work as a general hidden Web agent.</p><p>More recently, works have been published that address the specific problem of automatic filling forms and obtaining the resulting pages <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. In all of them, some form of ontology is deployed to supply values for automatically filling the form fields.</p><p>The HiWE system <ref type="bibr" target="#b17">[18]</ref> aims at providing for a Web crawler the ability to automatically fill forms. In order to accomplish this, whenever the crawler finds a Web page containing a form, it performs three steps as follows: (1) constructs a logical representation of the form, trying to associate a label to each field so as to denote its meaning; (2) takes values from a pre-existing value set (initially provided by the user) and submits the form assigning values considered as promising to the fields; (3) analyzes the response and, if it is considered valid, processes the resulting pages.</p><p>Although this system provides a very effective approach to crawling the hidden Web, it is highly dependent on the target site, since the labeling of fields and the assignment of values depend on its structure and contents.</p><p>Liddle et al. <ref type="bibr" target="#b15">[16]</ref> go further in two ways. First, instead of relying entirely on an initial userprovided set of values, they propose to use default values available in the input pageÕs encoding to fill in the form and perform an initial submission. If this initial submission returns a non-empty result, it is analyzed for extracting values that can be used to build value sets for further submissions of the same form. Second, in each subsequent form submission the resulting set of pages is carefully analyzed to estimate when a significant percentage of the data behind the form has been reached. For this, it was also necessary to develop a strategy to find duplicate data in different answer pages.</p><p>A similar approach is adopted by Modica et al. <ref type="bibr" target="#b16">[17]</ref> for automatically building ontologies from a set of Web pages from different sites of a same domain (e.g., car rental forms from several companies). Once generated, the ontology can be used in a number of applications, such as automatic interaction with Web forms by agents. The proposed process for building an ontology begins with a user selecting an initial Web page and interacting with a tool to build the elements of an initial ontology from pieces of information found in the selected page (i.e., labels, forms, check boxes, tag names, etc.). This is called the training phase. In the next phase, the adaptation phase, the initial ontology is refined on by matching pages from other Web sites on the same domain against it. This process can be used to validate the initial ontology and to enrich it with useful new information. Notice that this approach requires a higher level of user interaction than HiWE does.</p><p>We notice that, in the previous works, the target Web sites are assumed to have a single and very simple navigation structure, that is, one in which the starting page contains the form to be filled and this form leads directly to the pages containing the data of interest.</p><p>Davulcu et al. <ref type="bibr" target="#b18">[19]</ref> propose to represent the navigational structure of Web sites, including HTML form filling, through a formalism called transaction F-logic. This allows the representation of complex navigation structures (e.g., menus, forms, page loops, link hubs, etc.) in a very flexible way, resulting what they call navigation maps. In order to help users build these maps, a tool called Navigation Map Builder is embedded in a Web browser and works by intercepting their navigation actions and mapping them into F-logic objects. Thus, this work relies heavily on user interaction to describe the target site structure.</p><p>ASByE <ref type="bibr" target="#b19">[20]</ref> is a tool used to generate agents for collecting sets of specific Web pages of interest. By using the toolÕs interactive graphical user interface, a user can specify paths within a Web site that an agent has to traverse to find pages containing useful data (i.e., data-rich pages). In cases in which forms are present along the paths, the user can indicate how they should be filled. Therefore, the tool is able to deal with different navigation structures, but it relies on the user to provide information on such structures.</p><p>In this paper, we argue that the knowledge of common navigational features of Web sites can be used to provide an automatic solution to the problem of generating agents to collect data-rich pages. The proposed solution solves most of the problems faced by the above mentioned tools, such as the dependence on the domain or on the structure of the site, the need of a high degree of user interaction, and the adoption of very simple navigational structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Navigation patterns</head><p>To develop a general solution to automatically generate agents to fetch hidden Web pages is a very difficult, if not impossible task because a dynamic page can be generated in many different ways. Thus, we restrict the scope of our method to sites respecting two common navigation patterns that reflect common ways found by users to navigate on the Web. We formalize the concept of navigation pattern as follows.</p><p>Definition 1. A navigation pattern is denoted by a quintuple NP ¼ ðP ; R; r; p 0 ; T Þ, where P is a set of pages, R is a set of transition mechanisms between pages, r is a partial function from P Â R to P representing a set of actions that drives a user from one page to another, p 0 2 P is the starting page, and T is a subset of P that represents the relevant pages for the user.</p><p>We can represent a navigation pattern as a directed graph, where each element in P is a vertex and each edge between vertexes p i and p j indicates a path from p i to p j through a mechanism expressed by the edge type. The symbols we use to diagrammatically represent such a graph are depicted in Fig. <ref type="figure" target="#fig_2">3</ref>. Notice that we are not restricted to only these vertexes and edges. This diagrammatic representation resembles the navigation maps proposed by Davulcu et al. <ref type="bibr" target="#b18">[19]</ref>.</p><p>Each specific sequence of paths followed by a user through a Web site is called a navigation. A navigation corresponds to an instance of a navigation pattern. In Fig. <ref type="figure">4</ref>(a) we show an example of a navigation pattern, pointing out each element of Definition 1. A corresponding navigation is shown in Fig. <ref type="figure">4(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Form Submission</head><p>HTML Link Applet Two navigation patterns are very popular among Web site designers. The first one is that depicted in Fig. <ref type="figure">4</ref>, in which the user starts from the siteÕs main page, fills in a search form, and then submits it, getting a set of answer pages, similarly to our example in Section 1. The second one adds an intermediate page containing a set of links designed to refine on the search, and from this page the user can get to the answer pages, as Fig. <ref type="figure">5</ref> illustrates. An example of such pattern can be found in the DBLP Web site<ref type="foot" target="#foot_1">3</ref> when we search for a specific conference or journal.</p><p>We restrict our method for generating hidden Web agents to only these two navigation patterns. Despite this limitation, our approach can cover a great number of Web sites for distinct application domains, as we show in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Agent generation</head><p>To fulfill its task, a hidden Web agent must be able to simulate a userÕs navigation through a Web site, i.e., it must follow links, fill in forms, and follow threads of answer pages. The generation of such an agent can be accomplished in a variety of ways. The traditional approach is by writing specific code in some specific language (e.g., Java), but this is known to be very time consuming and error prone, making maintenance very difficult. A different approach is adopted by the ASByE tool <ref type="bibr" target="#b19">[20]</ref>, in which the user provides navigation examples for accomplishing the agent plan. This approach, however, demands constant user interference in the agent generation process, leading to a tedious and costly process when repeated for many sites.</p><p>We recall that we are interested in automatically generating agents that, for a given application domain, are able to fill in forms and to fetch all answer pages. Our method to generate such agents involves three steps: (1) finding the forms, (2) learning to fill them in, and (3) identifying and fetching the target pages.</p><p>In the first step, we start crawling from the siteÕs main page looking for forms in a blind search. A set of heuristics is used to eliminate the undesired forms. In the second step, we extract the labels from the remaining forms and, using a sample data repository, we try to learn how to fill them in. Finally, in the last step, we submit all filled forms in order to identify data-rich pages. The process ends when these pages are found. Each dashed box in Fig. <ref type="figure" target="#fig_4">6</ref> illustrates a step of our method.</p><p>Next, we describe the sample data repository and discuss the basic steps of our agent generation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The sample data repository</head><p>A key component of our method is the sample data repository. During the process of generating an agent, it is used to identify evidences in the traversed pages that such pages belong to a specific application domain. Basically, this repository is a set of attribute-value pairs (avps) of the form hs; vi that describe objects from the application domain, where s is a type and v is a string that corresponds to a value of type s in this domain.</p><p>There are various methods to generate such a repository. Values can be taken from a database or from the output of some program, or we may even build the repository manually from scratch. Key to this step is the representativeness of the objects, i.e., whether objects in the repository are likely to be found in any site of that domain.</p><p>We generate our repositories by extracting data from Web sources of specific domains we are interested in (e.g., books, CDs, etc.). In order to certify the representativeness of the objects, we extract data related to ''popular'' objects in each domain. For instance, when dealing with the book domain we extract data from pages returned as the result of queries using keywords such as ''Java'' or ''Internet''. The more representative the objects are, less objects are needed in the repository. In our experiments, we used the DEByE tool <ref type="bibr" target="#b9">[10]</ref> to carry out the data extractions. In Fig. <ref type="figure" target="#fig_5">7</ref>, we show the example of an object from the book domain taken from one of these extractions. Notice that the type s is represented by the attribute type of the ATOM element and the value v by the VALUE element. Thus, for this object we have the avps AETitle, ''Building Internet Firewalls, 2nd Edition''ae, AEAuthor, ''Elizabeth Zwicky, et al.''ae, and AEPrice, 26.50ae.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Finding forms</head><p>To generate an agent for a Web site related to a specific domain S, we start from a Web directory that contains many URLs of sites from S. These URLs are regarded as entry points for starting the search on the sites (for instance, in our experiments we used the Google directory <ref type="foot" target="#foot_3">4</ref> ).</p><p>For each entry point, we start a blind breadth-first search looking for HTML forms, limiting the search depth and the number of fetched pages. Our main aim in this step is to locate advanced search forms or, when they are not present, any forms capable of returning data-rich pages. In practice, many forms are found, composing a set of candidate forms. However, few of them are actually relevant forms, making it necessary the application of heuristics to eliminate those that are clearly undesirable. We describe these heuristics below. Heuristic 1. Given a threshold T, forms with n &lt; T elements<ref type="foot" target="#foot_4">5</ref> are eliminated.</p><p>Heuristic 2. For a given form F, if F has any password HTML type element, F is eliminated. As in the HiWE system <ref type="bibr" target="#b17">[18]</ref>, Heuristic 1 aims at eliminating simple forms like the ones used for generic search. Heuristic 2 is used to eliminate register, purchase, or login forms. Although extremely simple, these two heuristics are very effective, as demonstrated by our experiments.</p><p>In some sites, however, an advanced search form is not present and after applying these two heuristics all candidate forms are eliminated. In such cases, it is very common to find search forms containing a simple text field and a combo-box or group of radio buttons indicating which data type that text field is related to. We call this type of form a restrict search form (see Fig. <ref type="figure" target="#fig_6">8</ref>). Thus, when all candidate forms are eliminated we look for restrict search ones. Notice that, when neither an advanced search form nor a restrict search one is present in the site, it is not possible to automatically determine which form to use to fetch the desired pages, since there is no information attached to simple search forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learning to fill in forms</head><p>After applying the two above heuristics, the remaining candidate forms are then inspected. Thus, using the sample data repository, we try to ''learn'' how to fill in the forms, as we now explain.</p><p>The form filling task consists of finding a mapping between form fields and repository attributes. This task is far from being a straightforward one, due to the variety of form types found on the Web. Our solution to solve this problem comes from two practical observations: (1) Web designers avoid complex navigation patterns in order to make the usersÕ task easier; (2) forms usually look very much alike (in fact, the great majority of forms is composed of fields identified with a text label).</p><p>Thus, the only clues we have to map each repository attribute to a form field are the form labels. HTML, however, does not provide any explicit mechanism to identify which label is related to each field, being restricted to the page layout markup. Thus, we apply another heuristic to try to acquire field labels by determining which labels are visually adjacent to a form field. In general, labels are placed on the left-hand side of or above to a form field and no other text is found inside the form (like in Fig. <ref type="figure" target="#fig_7">9</ref>). Our heuristic relies on this observation.</p><p>Unlike in the HiWE system <ref type="bibr" target="#b17">[18]</ref>, we do not use expensive complex parsing techniques to extract form labels. Rather, we use a simple event-driven parser and two buffers, one for the left-hand side text and another for the above text, as we explain in Heuristic 3. Heuristic 3. Let F be a form and B l and B a be two buffers (one for the left-hand side text and the other one for the above text). Each parsed text inside F is inserted in B l , whenever a break tag is found (e.g., &lt;BR&gt; or &lt;TR&gt;), the content of B l is moved to B a . When an input field of F is found, l f NomEmptyðB l ; B a Þ is assigned as the label of field f, and B l and B a become empty.</p><p>In Fig. <ref type="figure" target="#fig_8">10</ref> we show how Heuristic 3 operates to extract labels from a piece of the page depicted in Fig. <ref type="figure" target="#fig_7">9(b)</ref>, in which labels are above input fields. Due to space constraints, we kept only the necessary information to understand the example. Notice the contents of each buffer during the form parsing. When the form parsing begins, both buffers are empty (line 1). In line 2, a &lt;TR&gt; is found and the buffers remain empty. Then, in line 3, the string ÔTitle of BookÕ is inserted in B l . In line 4, a &lt;TR&gt; tag is found and the content of B l is moved to B a . When a new input field is found, in line 5, the first label l f 1 is identified, the buffers are cleared, and the process continues.</p><p>After identifying the form labels (for instance, ''Title of Book'' and ''AuthorÕs Name'' in Fig. <ref type="figure" target="#fig_8">10</ref>), we use the sample data repository to verify whether or not that form is really a relevant one. Thus, we try to find mappings between form labels and repository attributes looking for matches among them. We introduce two definitions to better explain our matching algorithm.  Definition 2. A character c is a word symbol iff it belongs to the set fa; . . . ; z; A; . . . ; Z; 0; . . . ; 9g. Otherwise, it is said to be a non-word symbol. Definition 3. A string w is a word if (i) w is composed of three or more word symbols, and (ii) no substring of w is a word. Let s be any string. The word set of s, referred to as WS(s), is the set of all words that are substrings of s.</p><p>Before trying to find matches, we normalize both the repository attributes and the form labels, using a stemming algorithm, removing stop words and keeping only words as stated in Definition 3. Notice that HTML tags are already removed.</p><p>Let WSðsÞ be the word set of a normalized attribute and WSðlÞ the word set of a normalized label. We consider that a label matches an attribute when the number of common elements in WSðsÞ and WSðlÞ overcomes a given threshold. In our experiments, a matching of 50% was considered a good measure. This is the same measure adopted in the HiWE system <ref type="bibr" target="#b17">[18]</ref>.</p><p>If no matches are found, the form is disregarded. Notice that for restrict search forms the matching is straightforward, since we can easily obtain the matching attributes, which are explicitly available in the &lt;SELECT&gt; HTML tags. If matches are found, the agent knows how to fill in the form and all necessary information to submit it is already available, so the process proceeds. Now we explain what to do after the form submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Identifying and fetching target pages</head><p>After the set of search forms is identified, we must select the navigation pattern that best models the site being crawled. Basically, this consists in discovering whether the site presents an intermediate page between the form submission and the data-rich pages of interest. Furthermore, the transition mechanisms (see Definition 1) between those pages must be identified.</p><p>To properly select the navigation pattern of the site being crawled, we devised a data-rich page test. This test aims at checking whether a given HTML page is data rich or not, i.e., if the page is part of the set T of relevant pages, as defined in Definition 1. Basically, the test tries to locate in the HTML pages a certain number of objects similar to those present in our sample data repository, using Heuristic 4 presented below. Heuristic 4. Let H be an HTML page and R a sample data repository, composed of a set of objects of the form hs; vi. H is considered data rich if we can identify at least n objects O, such that O 2 R.</p><p>To identify these objects and implement the data-rich page test we used the techniques proposed by Golgher et al. <ref type="bibr" target="#b20">[21]</ref>, which are able to automatically recognize complete objects in HTML pages given a sample data repository. We notice that there is a great probability of finding objects that are similar to the ones present in the repository. This is because the site and the repository share the same application domain and the repository values have been used to fill in the forms that led to this set of HTML pages. In our experiments, we considered an HTML page as being data rich if we were able to recognize at least two complete objects in it, i.e, we applied Heuristic 4 using n ¼ 2.</p><p>Using the data-rich page test, our method automatically locates what we call a data-rich page generator. A data-rich page generator is a Common Gateway Interface (CGI) or script that dynamically produces data-rich pages based on some parameters given as input.</p><p>In the navigation pattern depicted in Fig. <ref type="figure">4</ref>, the data rich page generator is the CGI or script that is activated by the form submission (e.g., the one coded in the formÕs action property). In the navigation pattern illustrated by Fig. <ref type="figure">5</ref>, we have to further crawl the site to locate the data-rich page generator, by following the set of links present in the page obtained as the result of the form submission.</p><p>After the identification of the correct data-rich page generator, we must infer the transition mechanism it uses for generating the complete set of data-rich pages. Many data-rich page generators produce pages with a hyperlink that links them to the next data-rich page. We call the complete set of these answer pages a thread. Our method tries to infer page threads using Heuristic 5 as presented below. Notice that when the link is an image, we use the text of the &lt;ALT&gt; HTML tag, when available. Heuristic 5. From all available text links, we look for those with at most two words, one of them being ''next'' or ''more'' or links with no words containing only one or more Ô&gt;Õ symbols. Then, when the link is found, we follow it and check whether the new page is a data-rich one. If we did the right choice, a pattern expression to this link is kept in the agent. When the agent is running, pages are collected until no more links that match the expression are found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generating the agent</head><p>The final result of the process carried out by our method is a program written in Java that is able to traverse Web forms for collecting pages from the hidden Web. The whole process can be summarized by the diagram in Fig. <ref type="figure" target="#fig_0">11</ref>, in which the steps inside the dashed box are those we have already discussed. The remaining step behind the agent generation process is our focus in this section.</p><p>The first three steps aims at discovering the navigation pattern of the site. Thus several navigations are tried when looking for data-rich pages for a given domain. Once these pages are found, all information needed to generate the agent has already been collected. The final step consists in using this information to fill in an agent template and generate a fully operational agent, as we explain next.</p><p>An agent template is an almost complete Java program able to act like a user navigating on the target Web site. In fact, just a few code lines are required in such a template to turn it into a complete agent. These code lines are the ones that describe specific characteristics of the target site, e.g., the page URL, form field names, default values, etc.</p><p>Each navigation pattern leads to a distinct kind of agent template which has been coded in such a way that the generated agent can follow the pattern it represents and collect the target pages. Since the code necessary to submit a form using an HTTP GET is quite different from the one using an HTTP POST, we decided to encode them as distinct templates. Thus, we have two templates for each navigation pattern covered by our method and the final agent withholds only features that can be used in scenarios it fits in. The set of agent templates is stored in an agent template repository and the information acquired on earlier steps is used to decide which agent template represents the siteÕs navigation pattern and to fill it accordingly to generate the final agent.</p><p>Fig. <ref type="figure" target="#fig_9">12</ref> shows an example of part of an agent generated by our method. The information acquired has been used to fill the template in such a way that the generated agent is able to execute any navigation on the site that resembles the one carried out during the navigation pattern discovery.</p><p>The example illustrates an agent generated for the Amazon bookstore. We kept only the main parts of it, since it has 352 lines of Java code. To better explain how the agent works, we divided it into four distinct sections. The first section (1) contains only program declarations. The second section (2) comprises the information that is filled to customize the agent. The third section (3) consists in the code that submits the form and collects the first target page. Finally, the fourth section (4) is responsible for collecting the thread of answer pages. Notice that only section (2) is actually filled by our method whereas the other ones already include the code required to perform their tasks.</p><p>In order to run the agent, the user must supply, as parameters, a set of values to fill in the form fields he or she desires. Thus, for each field we assign a name and a nickname, the first one to build the request and the second one to help the user run the agent, using meaningful names for each field. As nicknames, we use the type name of the attribute matched in the data repository. In Fig. <ref type="figure" target="#fig_9">12</ref>, due to lack of space, we show only two fields, one representing a text field (lines <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> and the other representing a radio buttons group (lines <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. For instance, from lines 8 to 11 we have a field ÔtitleÕ with name Ôquery-1Õ. For a radio buttons group, only a (small) set of predefined values are allowed (lines <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. If the user does not run the agent with a valid value for this field, the default value is used (line 14). In line 23, we inform that the agent ''knows'' how to follow the thread of answer pages, and next that the link label used is ÔMore ResultsÕ (line 24). As explained in Section 4.4, in this case the &lt;ALT&gt; tag was used, since in the Amazon site the link between target pages is an image.  In Fig. <ref type="figure" target="#fig_10">13</ref>, we expanded section (3) of the agent to better explain the form submission process. Based on the information in section (2) of the agent and on the parameters filled by the user, we build the HTTP request from lines 2 to 8. Then the connection to the server is opened. We submit the form, by POST, from lines 16 to 20, and the answer page is collected in line 23.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>In our experiments, we evaluated all steps of our method for automatically generating hidden Web agents, verifying the accuracy of each heuristic. We used 30 Web sites from three different domains: books, CDs, and software (see Table <ref type="table" target="#tab_1">1</ref>). For each domain, we selected the top ten sites listed at GoogleÕs Web directory, a directory ranked by popularity. We used those URLs as the sitesÕ entry points.</p><p>Almost all sites used have a simple search box on every page, as we can see from the large number of available forms in Table <ref type="table">2</ref>. We recall, however, that these simple search boxes are not useful when collecting pages for data extraction, since users are not able to adequately specify their needs using them. Thus, we considered as relevant forms only those comprehending an advanced or restricted search.</p><p>To evaluate our results, we use two standard measures common in the information retrieval field: precision and recall <ref type="bibr" target="#b21">[22]</ref>. In our context, precision is defined as the number of relevant forms that remain after applying some heuristic divided by the total number of remaining forms, and recall is defined as the number of relevant forms that remain after applying some heuristic divided by the total number of relevant forms.</p><p>Our first aim in the experiments was to evaluate how effective Heuristics 1 and 2 are for eliminating undesirable forms. In order to do this, we manually checked every form collected during the blind search step to identify the relevant ones. Another important point regarding these heuristics, besides eliminating undesired forms, is to make sure that relevant forms are not (mistakenly) eliminated. Table <ref type="table">2</ref> summarizes our results. We point out that almost all undesired forms were discarded, what left us with less than 4% of the available forms. This result is very relevant, since Heuristic 3 is more complex and should be applied to a small number of forms. Notice that no relevant forms were eliminated by Heuristics 1 and 2, as we can see by the 100% of recall.</p><p>Next, we evaluated how effective our method is to learn how to fill in the forms. As discussed in Section 4.3, Heuristic 3 is used to extract labels of the remaining forms and when no matches among these labels and object types in the repository are found, the form is eliminated. The main aim here is to verify that our method learns how to fill all relevant forms and that all undesirable ones are eliminated. From Table <ref type="table" target="#tab_2">3</ref> we notice that we correctly learned to fill in all relevant forms and no relevant form was eliminated, therefore achieving 100% of precision and 100% of recall.</p><p>Identifying and fetching data-rich pages is the final step of our method. Heuristic 4 described in Section 4.4 is used after a form submission to identify target pages and, when they are found, Heuristic 5 is used to discover how to follow the thread of answer pages. We correctly identified all target pages using Heuristic 4. However, as we can see from Table <ref type="table">4</ref>, we did not learn how to follow the thread of answer pages for one site of each domain, achieving an average recall of almost 87%.</p><p>In Table <ref type="table">5</ref> we summarize the overall results achieved by our method. As we can see, for the book and CD domains we were able to generate a complete agent for all sites but one of each domain. In both cases, the failure occurred due to the mechanism used to follow the threads of answer pages, since we only address the ordinary HTML form submission mechanism. For instance, in the case of the CD site, the generation of the thread is implemented using JavaScript.</p><p>The same occurred for one site from the software domain. We also notice that three sites from the software domain did not have an advanced or restrict search form to be filled in, which accounts for the poorer results. Thus, as an overall result, we automatically generated complete agents for 80% of the sites in the three domains, without any user interference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we presented a method for automatically generating agents for collecting pages from the hidden Web. Unlike other approaches that are domain dependent <ref type="bibr" target="#b14">[15]</ref> or assume sites with a single and very simple navigation structure <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>, our method is able to handle sites with more complex structures, generating agents with no user intervention.</p><p>Although restricted to sites that follow two navigation patterns, our method covers a large number of sites from different domains, as we show in our experiments. It uses a set of heuristics and a sample data repository for automatically finding relevant search forms, filling them in, and then identifying and collecting relevant pages. Our set of heuristics proved to be very effective and 80% of the agents were fully generated. It is important to notice that even when an heuristic fails, part of the agent is still automatically generated, which allows the user to complement its coding according to the application requirements. Our method extends the basic strategies adopted by the ASByE tool <ref type="bibr" target="#b19">[20]</ref> providing the facilities required for collecting hidden Web pages for data extraction. In this regard, the data-rich page test provided by Heuristic 4 plays an important role. Some of the ideas described in this paper have also been used as part of the Web-DL framework <ref type="bibr" target="#b22">[23]</ref> for building digital libraries from data extracted from the Web.</p><p>Several other mechanisms not covered by our method are used to hit the hidden Web (e.g., applets, JavaScript, etc.). As future work, we intend to extend our method to cover navigation patterns based on these mechanisms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Looking for advanced search forms. (a) Home page, (b) advanced search page.</figDesc><graphic coords="2,75.78,95.57,397.08,172.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Fetching answer pages. (a) First page, (b) second page.</figDesc><graphic coords="3,45.35,95.57,450.72,112.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Diagrammatic notation for representing a navigation pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. A navigation pattern and a navigation example. (a) Navigation pattern, (b) navigation.</figDesc><graphic coords="6,75.78,95.57,396.96,196.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Approach steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Data repository example.</figDesc><graphic coords="8,132.47,95.57,283.56,118.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. A restrict search form.</figDesc><graphic coords="9,212.60,95.57,113.40,97.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Form labels look-and-feel. (a) Labels to the left, (b) labels above.</figDesc><graphic coords="10,75.78,95.57,397.08,127.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Extracting labels.</figDesc><graphic coords="10,104.13,281.77,340.20,128.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Agent example.</figDesc><graphic coords="14,75.78,95.57,396.96,542.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Submitting the form.</figDesc><graphic coords="15,99.21,95.57,340.20,206.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Sites used in our experiments</figDesc><table><row><cell cols="2">Book CD Table 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Forms identification</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Domain</cell><cell>Available forms</cell><cell>Relevant forms</cell><cell>Forms remaining after</cell><cell>Recall (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Heuristics 1 and 2</cell><cell></cell></row><row><cell>Book</cell><cell>1112</cell><cell>10</cell><cell>29(2.6%)</cell><cell>100</cell></row><row><cell>CD</cell><cell>592</cell><cell>10</cell><cell>19(3.2%)</cell><cell>100</cell></row><row><cell>Software</cell><cell>1337</cell><cell>7</cell><cell>51(3.8%)</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Learning to fill in forms</figDesc><table><row><cell>Domain</cell><cell>Relevant forms</cell><cell>Remaining forms</cell><cell>Learned to fill</cell><cell></cell><cell>Precision (%)/Recall (%)</cell></row><row><cell>Book</cell><cell>10</cell><cell>29</cell><cell>10</cell><cell></cell><cell>100/100</cell></row><row><cell>CD</cell><cell>10</cell><cell>19</cell><cell>10</cell><cell></cell><cell>100/100</cell></row><row><cell>Software</cell><cell>7</cell><cell>51</cell><cell>7</cell><cell></cell><cell>100/100</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Following threads</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Domain</cell><cell>Sites with threads</cell><cell cols="2">Learned to follow</cell><cell cols="2">Recall (%)</cell></row><row><cell>Book</cell><cell>10</cell><cell>9</cell><cell></cell><cell>90.0</cell></row><row><cell>CD</cell><cell>7</cell><cell>6</cell><cell></cell><cell>85.7</cell></row><row><cell>Software</cell><cell>6</cell><cell>5</cell><cell></cell><cell>83.3</cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Overall results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Domain</cell><cell>Total of sites</cell><cell>Not able to fill</cell><cell cols="2">Not able to follow</cell><cell>Complete agents</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>generated</cell></row><row><cell>Book</cell><cell>10</cell><cell>0</cell><cell>1</cell><cell></cell><cell>9(90%)</cell></row><row><cell>CD</cell><cell>10</cell><cell>0</cell><cell>1</cell><cell></cell><cell>9(90%)</cell></row><row><cell>Software</cell><cell>10</cell><cell>3</cell><cell>1</cell><cell></cell><cell>6(60%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J. Palmieri Lage et al. / Data &amp; Knowledge Engineering 49 (2004) 177-196</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://dblp.uni-trier.de.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>J. Palmieri Lage et al. / Data &amp; Knowledge Engineering 49 (2004) 177-196</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://directory.google.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We call elements any input (text, radio or check box) or select (list or menu) HTML form field. Hidden input fields are disregarded.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by Project SIAM (MCT/CNPq/PRONEX grant 76.97.1016.00), CNPq grant 46.7775/00-1, and research funding from the Brazilian National Program in Informatics (Decree-law 3800/01). The authors would like to thank the anonymous reviewers whose comments helped to improve this paper.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Software www.amazon.com www.cdnow.com www.egghead.com www.iuniverse.com www.fye.com www.download.com www.fatbrain.com www.cduniverse.com www.cdw.com www.a1books.com www.virginmega.com www.shopforsoftware.com www.bn.com www.towerrecords.com www.softwareoutlet.com www.waterstones.com.uk www.hmv.com www.nextdaypc.com www.bookpool.com www.800.com www.amazon.com www.alphacraze.com www.samgoody.com www.zdnet.com www.half.com www.amazon.com www.softwareandstuff.com www.collinsbooks.au www.cdconnection.com www.virtualsoftware.com</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Database techniques for the World-Wide Web: a survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Florescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelzon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="74" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Searching the World-Wide Web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">280</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="98" to="100" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The deep Web: Surfacing hidden value, White Paper</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Bergman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Bright Planet</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conceptual-model-based data extraction from multiple-record Web pages</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Embley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Liddle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Quass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data and Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="251" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Grammars have exceptions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Crescenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mecca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="539" to="565" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A tool for semi-automatically extracting structured and semistructured data from text documents</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adelberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="283" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards automatic data extraction from large Web sites</title>
		<author>
			<persName><forename type="first">V</forename><surname>Crescenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Merialdo</surname></persName>
		</author>
		<author>
			<persName><surname>Roadrunner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Very Large Data Bases</title>
		<meeting>the 27th International Conference on Very Large Data Bases<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating finite-state transducers for semi-structured data extraction from the Web</title>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Dung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="521" to="538" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wrapper induction: Efficiency and expressiveness</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kushmerick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Journal</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="15" to="68" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DEByE--Data Extraction By Example</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Da Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data and Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="154" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">XWRAP: An XML-enabled wrapper construction system for Web information sources</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Data Engineering</title>
		<meeting>the 16th International Conference on Data Engineering<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="611" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical wrapper induction for semistructured information sources</title>
		<author>
			<persName><forename type="first">I</forename><surname>Muslea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Knoblock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="93" to="114" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building intelligent Web applications using lightweight wrappers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahuguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azavant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data and Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="316" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A brief survey of Web data extraction tools</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Teixeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="84" to="93" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A scalable comparison-shopping agent for the World-Wide Web</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Doorembos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Autonomous Agents</title>
		<meeting>the First International Conference on Autonomous Agents<address><addrLine>Marina del Rey, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extracting data behind Web forms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liddle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Embley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Conceptual Modeling Approaches for e-Business</title>
		<meeting>the Workshop on Conceptual Modeling Approaches for e-Business<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="38" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The use of machine-generated ontologies in dynamic information seeking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Modica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Jamil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Cooperative Information Systems</title>
		<meeting>the 9th International Conference on Cooperative Information Systems<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="433" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crawling the hidden Web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Very Large Data Bases</title>
		<meeting>the 27th International Conference on Very Large Data Bases<address><addrLine>Roma, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A layered architecture for querying dynamic Web content</title>
		<author>
			<persName><forename type="first">H</forename><surname>Davulcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="491" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An example-based environment for wrapper generation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Golgher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on The World Wide Web and Conceptual Modeling</title>
		<meeting>the 2nd International Workshop on The World Wide Web and Conceptual Modeling<address><addrLine>Salt Lake City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bootstrapping for example-based data extraction</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Golgher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 10th ACM International Conference on Information and Knowledge Management<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="371" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Information Retrieval</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Harlow, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Web-DL environment for building digital libraries from the Web</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Calado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gonc ßalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Roberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Joint Conference on Digital Libraries</title>
		<meeting>the 2003 Joint Conference on Digital Libraries<address><addrLine>Huston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="346" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lage received a B.Sc. degree in Computer Science from the Federal University of Minas Gerais, Brazil and currently is an M.Sc. student in the same institution. His research interests include compression algorithms and query processing on compressed data, databases, semistructured data, and information retrieval systems</title>
		<author>
			<persName><forename type="first">Juliano</forename><surname>Palmieri</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">He has been working on a number of research projects found by Brazilian national agencies such as the National Research Council (CNPq) and served as external reviewer and program committee member for conferences on databases and Web technology worldwide. In 2003, he is the organizing committee chair of 18th Brazilian Symposium on Databases/17th Brazilian Symposium on Software Engineering. His main research interests include extraction and management of semi-structured data, Web information retrieval and database modeling and design</title>
	</analytic>
	<monogr>
		<title level="m">Data Processing in 1990 from the Federal University of Amazonas (UFAM)</title>
		<editor>
			<persName><forename type="middle">D</forename><surname>Ph</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991">1991. 1995. 2002</date>
		</imprint>
	</monogr>
	<note>Brazil. Currently, he is an associate professor at the Computer Science Department of UFAM and participates as an associate researcher of the UFMG Database Group. He is a member of the Brazilian Computer Society</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Golgher is the Chief Technology Officer at Akwan Information Technologies, Brazil. His research interests include semistructured data, Web agents, and information retrieval</title>
		<author>
			<persName><forename type="first">B</forename><surname>Paulo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>He received an M.Sc. in computer science from the Federal University of Minas Gerais</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">he was a Visiting Scientist at the Hewlett-Packard Palo Alto Laboratories. He has served as a program committee member for several national and international conferences on databases and Web-related topics. He also served as a program committee co-chair for the 19</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Alberto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">October 2000, and as the program committee chair for the 9th International Symposium on String Processing and Information Retrieval</title>
		<title level="s">Sc. degree in Electrical Engineering and an M.Sc. degree in Computer Science from the Federal University of Minas Gerais, Brazil, and a Ph</title>
		<meeting><address><addrLine>Utah; Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997. 2002</date>
		</imprint>
	</monogr>
	<note>His research interests include conceptual database modeling, database design methods. database user interfaces, semistructured data, Web data management, and digital libraries</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
