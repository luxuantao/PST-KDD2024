<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Contrastive Learning with Adaptive Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-27">27 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
							<email>yanqiao.zhu@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Alibaba Group 5 RealAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
							<email>shu.wu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Contrastive Learning with Adaptive Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-27">27 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.14945v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies â†’ Unsupervised learning</term>
					<term>Neural networks</term>
					<term>Learning latent representations</term>
					<term>â€¢ Information systems â†’ Data mining Contrastive learning, graph representation learning, unsupervised learning, self-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, contrastive learning (CL) has emerged as a successful method for unsupervised graph representation learning. Most graph CL methods first perform stochastic augmentation on the input graph to obtain two graph views and maximize the agreement of representations in the two views. Despite the prosperous development of graph CL methods, the design of graph augmentation schemes-a crucial component in CL-remains rarely explored. We argue that the data augmentation schemes should preserve intrinsic structural and attribute information of graphs, which will force the model to learn representations that are insensitive to perturbation on unimportant nodes and edges. However, most existing methods adopt uniform data augmentation schemes, like uniformly dropping edges and uniformly shuffling features, leading to suboptimal performance. In this paper, we propose a novel graph contrastive representation learning method with adaptive augmentation that incorporates various priors for topological and semantic aspects of the graph. Specifically, on the topology level, we design augmentation schemes based on node centrality measures to highlight important connective structures. On the node attribute level, we corrupt node features by adding more noise to unimportant node features, to enforce the model to recognize underlying semantic information. We perform extensive experiments of node classification on a variety of real-world datasets. Experimental results demonstrate that our proposed method consistently outperforms existing state-of-the-art methods and even surpasses some supervised counterparts, which validates the effectiveness of the proposed contrastive framework with adaptive augmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Over the past few years, graph representation learning has emerged as a powerful strategy for analyzing graph-structured data. Graph representation learning using Graph Neural Networks (GNN) has received considerable attention, which aims to transform nodes to low-dimensional dense embeddings that preserve graph attributive and structural features. However, existing GNN models are mostly established in a supervised manner <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref>, which require abundant labeled nodes for training. Recently, Contrastive Learning (CL), as revitalization of the classical information maximization (InfoMax) principle <ref type="bibr" target="#b23">[24]</ref>, achieves great success in many fields, e.g., visual representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39]</ref> and natural language processing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref>. These CL methods seek to maximize the Mutual Information (MI) between the input (i.e. images) and its representations (i.e. image embeddings) by contrasting positive pairs with negative-sampled counterparts.</p><p>Inspired by previous CL methods, Deep Graph InfoMax (DGI) <ref type="bibr" target="#b43">[44]</ref> marries the power of GNN into InfoMax-based methods. DGI firstly augments the original graph by simply shuffling node features. Then, an objective based on MI maximization is proposed to maximize the MI between node embeddings and a global summary embedding. Following DGI, GMI <ref type="bibr" target="#b29">[30]</ref> proposes two node-level contrastive objectives to directly measure MI between input and representations of nodes and edges respectively, without explicit data augmentation. Moreover, to supplement the input graph with more global information, MVGRL <ref type="bibr" target="#b14">[15]</ref> proposes to augment the input graph using graph diffusion. Then, it constructs graph views by uniformly sampling subgraphs and learns to contrast node representations to global embeddings across the two views.</p><p>Despite the prosperous development of CL methods on graphs, data augmentation schemes, proved to be a critical component for visual representation learning <ref type="bibr" target="#b45">[46]</ref>, remain rarely explored in existing literature. Unlike abundant data transformation techniques available for images and texts, graph augmentation schemes are non-trivial to define in CL methods, since graphs are far more complex due to the non-Euclidean property. We argue that the augmentation schemes used in the aforementioned methods suffer from two drawbacks. At first, simple data augmentation in either the structural domain or the attribute domain, such as feature shifting in DGI <ref type="bibr" target="#b43">[44]</ref>, is not sufficient for generating diverse neighborhoods (i.e. contexts) for nodes, especially when node feature is sparse, leading to difficulty in optimizing the contrastive objective. Secondly, previous work ignores the discrepancy in the impact of nodes and edges when performing data augmentation. For example, if we construct graph views by uniformly dropping edges, removing some influential edges will deteriorate the embedding quality. As the representations learned by the contrastive objective tend to be invariant to corruption induced by the data augmentation scheme We first generate two graph views via stochastic augmentation that is adaptive to the graph structure and attributes. Then, the two graphs are fed into a shared Graph Neural Network (GNN) to learn representations. We train the model with a contrastive objective, which pulls representations of one node together while pushing node representations away from other node representations in the two views. N.B., we define the negative samples as all other nodes in the two views. Therefore, negative samples are from two sources, intra-view (in purple) and inter-view nodes (in red). <ref type="bibr" target="#b47">[48]</ref>, the data augmentation strategies should be adaptive to the input graph to reflect its intrinsic patterns. Again, taking the edge removing scheme as an example, we can give larger probabilities to unimportant edges and lower probabilities to important ones when randomly removing the edges. Then, this scheme is able to guide the model to ignore the introduced noise on unimportant edges and thus learn important patterns underneath the input graph.</p><p>To this end, we propose a novel contrastive framework for unsupervised graph representation learning (as shown in Figure <ref type="figure" target="#fig_0">1</ref>), which we refer to as Graph Contrastive learning with Adaptive augmentation, GCA for brevity. In GCA, we first generate two correlated graph views by performing stochastic corruption on the input. Then, we train the model using a contrastive loss to maximize the agreement between node embeddings in these two views. Specifically, we propose a joint data augmentation scheme at both topology and node attribute levels, namely removing edges and masking features, to provide diverse contexts for nodes in different views, so as to boost optimization of the contrastive objective. Moreover, we identify important edges and feature dimensions via centrality measures. Then, on the topology level, we adaptively drop edges by giving large removal probabilities to unimportant edges to highlight important connective structures. On the node attribute level, we corrupt attributes by adding more noise to unimportant feature dimensions, to enforce the model to recognize underlying semantic information.</p><p>The core contribution of this paper is two-fold, which is summarized as follows.</p><p>â€¢ Firstly, we propose a general contrastive framework for unsupervised graph representation learning with strong, adaptive data augmentation. The proposed GCA framework jointly performs data augmentation on both topology and attribute levels that are adaptive to the graph structure and attributes, which encourages the model to learn important features from both aspects.</p><p>â€¢ Secondly, we conduct comprehensive empirical studies using five public benchmark datasets on node classification under the commonly-used linear evaluation protocol. GCA consistently outperforms existing methods and our unsupervised method even surpasses its supervised counterparts on several transductive tasks.</p><p>The remaining of the paper includes the following sections. We briefly review related work in Section 2. In Section 3, we present the proposed GCA model in detail. The results of the experiments are analyzed in Section 4. Finally, we conclude the paper in Section 5. For readers of interest, additional configurations of experiments and details of proofs are provided in Appendix A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly review prior work on contrastive representation learning. Then, we review graph representation learning methods. At last, we provide a summary of comparisons between the proposed method and existing related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contrastive Representation Learning</head><p>Being popular in self-supervised representation learning, contrastive methods aim to learn discriminative representations by contrasting positive and negative samples. For visual data, negative samples can be generated using a multiple-stage augmentation pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7]</ref>, consisting of color jitter, random flip, cropping, resizing, rotation <ref type="bibr" target="#b8">[9]</ref>, color distortion <ref type="bibr" target="#b22">[23]</ref>, etc. Existing work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref> employs a memory bank for storing negative samples. Other work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b48">49]</ref> explores in-batch negative samples. For an image patch as the anchor, these methods usually find a global summary vector <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref> or patches in neighboring views <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref> as the positive sample, and contrast them with negative-sampled counterparts, such as patches of other images within the same batch <ref type="bibr" target="#b17">[18]</ref>.</p><p>Theoretical analysis sheds light on the reasons behind their success <ref type="bibr" target="#b32">[33]</ref>. Objectives used in these methods can be seen as maximizing the lower bounds of MI between input features and their representations <ref type="bibr" target="#b23">[24]</ref>. However, recent work <ref type="bibr" target="#b40">[41]</ref> reveals that downstream performance in evaluating the quality of representations may strongly depend on the bias that is encoded not only in the convolutional architectures but also in the specific estimator of the InfoMax objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Representation Learning</head><p>Many traditional methods on unsupervised graph representation learning employ the contrastive paradigm as well <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>. Prior work on unsupervised graph representation learning focuses on local contrastive patterns, which forces neighboring nodes to have similar embeddings. Positive samples under this circumstance are nodes appearing in the same random walk <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>. For example, the pioneering work DeepWalk <ref type="bibr" target="#b31">[32]</ref> models probabilities of node co-occurrence pairs using noise-contrastive estimation (NCE) <ref type="bibr" target="#b11">[12]</ref>. These random-walk-based methods are proved to be equivalent to factorizing some forms of graph proximity (e.g., transformation of the adjacent matrix) <ref type="bibr" target="#b34">[35]</ref>, which overly emphasize on the structural information encoded in these graph proximities and also face severe scaling problem with large-scale datasets. Also, these methods are known to be error-prone with inappropriate hyperparameter tuning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Recent work on graph neural networks (GNN) employs more powerful graph convolutional encoders over conventional methods. Among them, considerable literature has grown up around the theme of supervised GNN <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, which requires labeled datasets that may not be accessible in real-world applications. Along the other line of development, unsupervised GNNs receive little attention. Representative methods include GraphSAGE <ref type="bibr" target="#b13">[14]</ref>, which incorporates DeepWalk-like objectives. Recent work DGI <ref type="bibr" target="#b43">[44]</ref> marries the power of GNN and contrastive learning, which focuses on maximizing MI between global graph embeddings and local node embeddings. Specifically, to implement the InfoMax objective, DGI requires an injective readout function to produce the global graph embedding, where the injective property is too restrictive to fulfill. However, it is hard to fulfill the injective requirement of the graph readout function such that the graph embedding may be deteriorated. In contrast to DGI, previous work <ref type="bibr" target="#b50">[51]</ref> proposes to not rely on an explicit graph embedding, but rather focus on maximizing the agreement of node embeddings across two corrupted views of the graph.</p><p>Following DGI, GMI <ref type="bibr" target="#b29">[30]</ref> employs two discriminators to directly measure MI between input and representations of both nodes and edges without data augmentation; MVGRL <ref type="bibr" target="#b14">[15]</ref> proposes to learn both node-level and graph-level representations by performing node diffusion and contrasting node representations to augmented graph summary representations. Moreover, GCC <ref type="bibr" target="#b33">[34]</ref> proposes a pretraining framework based on contrastive learning. It proposes to construct multiple graph views by sampling subgraphs based on random walks and then learn model weights with several feature engineering schemes. However, these methods do not explicitly consider adaptive graph augmentation at both structural and attribute levels, leading to suboptimal performance. Unlike these work, the adaptive data augmentation at both topology and attribute levels used in our GCA is able to preserve important patterns underneath the graph through stochastic perturbation.</p><p>Summary of comparisons with related graph contrastive learning methods. In summary, we provide a brief comparison between the  <ref type="bibr" target="#b43">[44]</ref>, GMI <ref type="bibr" target="#b29">[30]</ref>, and MVGRL <ref type="bibr" target="#b14">[15]</ref> in Table <ref type="table" target="#tab_0">1</ref>, where the two columns "Topology" and "Attribute" denote data augmentation strategies at both levels. It is seen that the proposed GCA method simplifies previous nodeglobal contrastive scheme by defining contrastive objective at the node level. Most importantly, GCA is the only one that proposes adaptive data augmentation on both topology and attribute levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED METHOD</head><p>In this section, we present GCA in detail, starting with the overall contrastive learning framework, followed by the proposed adaptive graph augmentation schemes. Finally, we provide theoretical justification behind our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Let G = (V, E) denote a graph, where</p><formula xml:id="formula_0">V = {ğ‘£ 1 , ğ‘£ 2 , â€¢ â€¢ â€¢ , ğ‘£ ğ‘ }, E âŠ†</formula><p>V Ã— V represent the node set and the edge set respectively. We denote the feature matrix and the adjacency matrix as ğ‘¿ âˆˆ R ğ‘ Ã—ğ¹ and ğ‘¨ âˆˆ {0, 1} ğ‘ Ã—ğ‘ , where ğ’™ ğ‘– âˆˆ R ğ¹ is the feature of ğ‘£ ğ‘– , and ğ‘¨ ğ‘– ğ‘— = 1 iff (ğ‘£ ğ‘– , ğ‘£ ğ‘— ) âˆˆ E. There is no given class information of nodes in G during training in the unsupervised setting. Our objective is to learn a GNN encoder ğ‘“ (ğ‘¿, ğ‘¨) âˆˆ R ğ‘ Ã—ğ¹ â€² receiving the graph features and structure as input, that produces node embeddings in low dimensionality, i.e., ğ¹ â€² â‰ª ğ¹ . We denote ğ‘¯ = ğ‘“ (ğ‘¿, ğ‘¨) as the learned representations of nodes, where ğ’‰ ğ‘– is the embedding of node ğ‘£ ğ‘– . These representations can be used in downstream tasks, such as node classification and community detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Contrastive Learning Framework</head><p>The graph contrastive learning framework follows the common graph CL paradigm where the model seeks to maximize the agreement within representations between different views. To be specific, we first generate two graph views by performing stochastic graph augmentation on the input. Then, we employ a contrastive objective that enforces the encoded embeddings of each node in the two different views agree with each other and can be distinguished from embeddings of other nodes. In our GCA model, at each iteration, we sample two stochastic augmentation function ğ‘¡ âˆ¼ T and ğ‘¡ â€² âˆ¼ T , where T is the set of all possible augmentation functions. Then, we generate two graph views, denoted as ğº 1 = ğ‘¡ ( ğº ) and ğº 2 = ğ‘¡ â€² ( ğº ), and denote node embeddings in the two generated views as ğ‘¼ = ğ‘“ ( ğ‘¿ 1 , ğ‘¨ 1 ) and ğ‘½ = ğ‘“ ( ğ‘¿ 2 , ğ‘¨ 2 ), where ğ‘¿ * and ğ‘¨ * are the feature matrices and adjacent matrices of the views.</p><p>After that, we employ a contrastive objective, i.e. a discriminator, that distinguishes the embeddings of the same node in these two different views from other node embeddings. For any node ğ‘£ ğ‘– , its embedding generated in one view, ğ’– ğ‘– , is treated as the anchor, the embedding of it generated in the other view, ğ’— ğ‘– , forms the positive sample, and the other embeddings in the two views are naturally regarded as negative samples. Mirroring the InfoNCE objective <ref type="bibr" target="#b41">[42]</ref> in our multi-view graph contrastive learning setting, we define the pairwise objective for each positive pair (ğ’– ğ‘– , ğ’— ğ‘– ) as</p><formula xml:id="formula_1">â„“ (ğ’– ğ‘– , ğ’— ğ‘– ) = log ğ‘’ ğœƒ (ğ’– ğ‘– ,ğ’— ğ‘– )/ğœ ğ‘’ ğœƒ (ğ’– ğ‘– ,ğ’— ğ‘– )/ğœ positive pair + âˆ‘ï¸ ğ‘˜â‰ ğ‘– ğ‘’ ğœƒ (ğ’– ğ‘– ,ğ’— ğ‘˜ )/ğœ</formula><p>inter-view negative pairs</p><formula xml:id="formula_2">+ âˆ‘ï¸ ğ‘˜â‰ ğ‘– ğ‘’ ğœƒ (ğ’– ğ‘– ,ğ’– ğ‘˜ )/ğœ intra-view negative pairs ,<label>(1)</label></formula><p>where ğœ is a temperature parameter. We define the critic ğœƒ (ğ’–, ğ’—) = ğ‘  (ğ‘”(ğ’–), ğ‘”(ğ’—)), where ğ‘  (â€¢, â€¢) is the cosine similarity and ğ‘”(â€¢) is a nonlinear projection to enhance the expression power of the critic <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41]</ref>. The projection function ğ‘” in our method is implemented with a two-layer perceptron model. Given a positive pair, we naturally define negative samples as all other nodes in the two views. Therefore, negative samples come from two sources, that are inter-view and intra-view nodes, corresponding to the second and the third term in the denominator in Eq. ( <ref type="formula" target="#formula_2">1</ref>), respectively. Since two views are symmetric, the loss for another view is defined similarly for â„“ (ğ’— ğ‘– , ğ’– ğ‘– ). The overall objective to be maximized is then defined as the average over all positive pairs, formally given by</p><formula xml:id="formula_3">J = 1 2ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 [â„“ (ğ’– ğ‘– , ğ’— ğ‘– ) + â„“ (ğ’— ğ‘– , ğ’– ğ‘– )] .<label>(2)</label></formula><p>To sum up, at each training epoch, GCA first draws two data augmentation functions ğ‘¡ and ğ‘¡ â€² , and then generates two graph views G 1 = ğ‘¡ (G) and G 2 = ğ‘¡ â€² (G) of graph G. Then, we obtain node representations ğ‘¼ and ğ‘½ of G 1 and G 2 using a GNN encoder ğ‘“ . Finally, the parameters are updated by maximizing the objective in Eq. ( <ref type="formula" target="#formula_3">2</ref>). The training algorithm is summarized in Algorithm 1. Update parameters by applying stochastic gradient ascent to maximize J</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive Graph Augmentation</head><p>In essence, contrastive learning methods that maximize agreement between views seek to learn representation that is invariant to perturbation introduced by the augmentation schemes <ref type="bibr" target="#b47">[48]</ref>. In the model, we propose to design augmentation schemes that tend to keep important structures and attribute unchanged while perturbing possibly unimportant links and features. Specifically, we corrupt the input graph by randomly removing edges and masking node features in the graph, and the removing or masking probabilities are skewed for unimportant edges or features, that is, higher for unimportant edges or features, and lower for important ones. From an amortized perspective, we emphasize important structures and attributes over randomly corrupted views, which will guide the model to preserve fundamental topological and semantic graph patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Topology-level augmentation.</head><p>For topology level augmentation, we consider a direct way for corrupting input graphs where we randomly remove edges in the graph <ref type="bibr" target="#b50">[51]</ref>. Formally, we sample a modified subset E from the original E with probability</p><formula xml:id="formula_4">ğ‘ƒ {(ğ‘¢, ğ‘£) âˆˆ E } = 1 âˆ’ ğ‘ ğ‘’ ğ‘¢ğ‘£ ,<label>(3)</label></formula><p>where (ğ‘¢, ğ‘£) âˆˆ E and ğ‘ ğ‘’ ğ‘¢ğ‘£ is the probability of removing (ğ‘¢, ğ‘£). E is then used as the edge set in the generated view. ğ‘ ğ‘’ ğ‘¢ğ‘£ should reflect the importance of the edge (ğ‘¢, ğ‘£) such that the augmentation function are more likely to corrupt unimportant edges while keep important connective structures intact in augmented views.</p><p>In network science, node centrality is a widely-used measure that quantifies the influence of nodes in the graph <ref type="bibr" target="#b26">[27]</ref>. We define edge centrality ğ‘¤ ğ‘’ ğ‘¢ğ‘£ for edge (ğ‘¢, ğ‘£) to measure its influence based on centrality of two connected nodes. Given a node centrality measure ğœ‘ ğ‘ (â€¢) : V â†’ R + , we define edge centrality as the average of two adjacent nodes' centrality, i.e. ğ‘¤ ğ‘’ ğ‘¢ğ‘£ = (ğœ‘ ğ‘ (ğ‘¢)+ğœ‘ ğ‘ (ğ‘£)) /2, and on directed graph, we simply use the centrality of the tail node, i.e. ğ‘¤ ğ‘’ ğ‘¢ğ‘£ = ğœ‘ ğ‘ (ğ‘£), since the importance of edges is generally characterized by nodes they are pointing to <ref type="bibr" target="#b26">[27]</ref>.</p><p>Next, we calculate the probability of each edge based on its centrality value. Since node centrality values like degrees may vary across orders of magnitude <ref type="bibr" target="#b26">[27]</ref>, we first set ğ‘  ğ‘’ ğ‘¢ğ‘£ = log ğ‘¤ ğ‘’ ğ‘¢ğ‘£ to alleviate the impact of nodes with heavily dense connections. The probabilities can then be obtained after a normalization step that transform the values into valid probabilities, which is defined as</p><formula xml:id="formula_5">ğ‘ ğ‘’ ğ‘¢ğ‘£ = min ğ‘  ğ‘’ max âˆ’ ğ‘  ğ‘’ ğ‘¢ğ‘£ ğ‘  ğ‘’ max âˆ’ ğœ‡ ğ‘’ ğ‘  â€¢ ğ‘ ğ‘’ , ğ‘ ğœ ,<label>(4)</label></formula><p>where ğ‘ ğ‘’ is a hyperparameter that controls the overall probability of removing edges, ğ‘  ğ‘’ max is the maximum value of ğ‘  ğ‘’ ğ‘¢ğ‘£ and ğœ‡ ğ‘’ ğ‘  is the average. ğ‘ ğœ &lt; 1 is a cut-off probability. It is used to truncate the probabilities since overly-high removal probabilities may damage the graph structure.</p><p>For the choice of the node centrality function, we use the following three centrality measures, including degree centrality, eigenvector centrality, and PageRank centrality due to their simplicity and effectiveness. Degree centrality. Node degree itself can be a centrality measure <ref type="bibr" target="#b26">[27]</ref>. On directed networks, we use in-degrees since the influence of a node in directed graphs are mostly bestowed by nodes pointing at it <ref type="bibr" target="#b26">[27]</ref>. Despite that the node degree is one of the simplest centrality measures, it is quite effective and illuminating. For example, in citation networks where nodes represent papers and edges represent citation relationships, nodes with the highest degrees in citation networks are likely to correspond to influential papers.</p><p>Eigenvector centrality. The eigenvector centrality <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref> of a node is calculated as its eigenvector corresponding to the largest eigenvalue of the adjacency matrix. Unlike degree centrality, which assumes that all neighbors contribute equally to the importance of the node, eigenvector centrality also takes the importance of neighboring nodes into consideration. By definition, the eigenvector centrality of each node is proportional to the sum of centralities of its neighbors, nodes that are either connected to many neighbors or connected to influential nodes will have high eigenvector centrality values. On directed graphs, we use the right eigenvector to compute the centrality, which corresponds to incoming edges. Note that since only the leading eigenvector is needed, the computational burden for calculating the eigenvector centrality is negligible.</p><p>PageRank centrality. The PageRank centrality <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> is defined as the PageRank weights computed by the PageRank algorithm. The algorithm propagate influence along directed edges, and nodes that gather most influence are regarded as important nodes. Formally, the centrality values are defined by</p><formula xml:id="formula_6">ğˆ = ğ›¼ğ‘¨ğ‘« âˆ’1 ğˆ + 1,<label>(5)</label></formula><p>where ğœ âˆˆ R ğ‘ is the vector of PageRank centrality scores for each node and ğ›¼ is the damping factor that prevents sinks in the graph from absorbing all ranks from other nodes connected to the sinks. We set ğ›¼ = 0.85 as suggested in Page et al. <ref type="bibr" target="#b27">[28]</ref>. For undirected graphs, we execute PageRank on transformed directed graphs, where each undirected edge is converted to two directed edges.</p><p>To gain an intuition of these proposed adaptive structural augmentation schemes, we calculate edge centrality scores of the famous Karate club dataset <ref type="bibr" target="#b49">[50]</ref>, containing two groups of students leading by two coaches respectively. The edge centrality values calculated by different schemes are visualized in Figure <ref type="figure" target="#fig_3">2</ref>. As can be seen in the figure, though the three schemes exhibit subtle differences, all of the augmentation schemes tend to emphasize edges that connect the two coaches (in red) inside the two groups and put less attention to links between peripheral nodes across groups. This  verifies that the proposed node-centrality-based adaptive topology augmentation scheme can recognize fundamental structures of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Node-attribute-level augmentation.</head><p>On the node attribute level, similar to the salt-and-pepper noise in computer vision, we add noise to node attributes via randomly masking a fraction of dimensions with zeros in node features. Formally, we first sample a random vector ğ’ âˆˆ {0, 1} ğ¹ where each dimension of it independently is drawn from a Bernoulli distribution, i.e., ğ‘š ğ‘– âˆ¼ B (1 âˆ’ ğ‘ ğ‘“ ğ‘– ), âˆ€ğ‘–. Note that each dimension has a different probability for the Bernoulli distribution. Then, the generated node features ğ‘¿ is computed by</p><formula xml:id="formula_7">ğ‘¿ = [ğ’™ 1 â€¢ ğ’ ; ğ’™ 2 â€¢ ğ’ ; â€¢ â€¢ â€¢ ; ğ’™ ğ‘ â€¢ ğ’ ] âŠ¤ .<label>(6)</label></formula><p>Here [â€¢; â€¢] is the concatenation operator. Similar to topology level augmentation, the probability ğ‘ ğ‘“ ğ‘– should reflect the importance of the ğ‘–-th dimension of node features. We assume that feature dimensions frequently appearing in influential nodes should are important, and define the weights of feature dimensions as follows. For sparse one-hot nodes features, i.e. ğ‘¥ ğ‘¢ğ‘– âˆˆ {0, 1} for any node ğ‘¢ and feature dimension ğ‘–, we calculate the weight of dimension ğ‘– as</p><formula xml:id="formula_8">ğ‘¤ ğ‘“ ğ‘– = âˆ‘ï¸ ğ‘¢ âˆˆV ğ‘¥ ğ‘¢ğ‘– â€¢ ğœ‘ ğ‘ (ğ‘¢),<label>(7)</label></formula><p>where ğœ‘ ğ‘ (â€¢) is a node centrality measure that is used to quantify node importance. The first term ğ‘¥ ğ‘¢ğ‘– âˆˆ {0, 1} indicates the occurrence of dimension ğ‘– in node ğ‘¢, and the second term ğœ‘ ğ‘– (ğ‘¢) measures the node importance of each occurrence. To provide some intuition behind the above definition, consider a citation network where feature dimensions correspond to keywords. Keywords that frequently appear in high-impact paper should be considered informative and important.</p><p>For dense node features where ğ‘¥ ğ‘¢ğ‘– âˆˆ R for each node ğ‘£ and feature dimension ğ‘–, we can not directly count the occurrence of each dimension as on one-hot features, since the features are now continuous. We turn to measure the magnitude of dimension ğ‘– in node ğ‘¢ by its absolute value |ğ‘¥ ğ‘¢ğ‘– |. Formally, we calculate the weights by</p><formula xml:id="formula_9">ğ‘¤ ğ‘“ ğ‘– = âˆ‘ï¸ ğ‘¢ âˆˆV |ğ‘¥ ğ‘¢ğ‘– | â€¢ ğœ‘ ğ‘ (ğ‘¢).<label>(8)</label></formula><p>Following similar steps in topology augmentation, we perform a normalization operation on the weights to obtain the final probability. Formally,</p><formula xml:id="formula_10">ğ‘ ğ‘“ ğ‘– = min ğ‘  ğ‘“ max âˆ’ ğ‘  ğ‘“ ğ‘– ğ‘  ğ‘“ max âˆ’ ğœ‡ ğ‘“ ğ‘  â€¢ ğ‘ ğ‘“ , ğ‘ ğœ ,<label>(9)</label></formula><p>where ğ‘  Finally, we generate corrupted graph view G from the input graph G by jointly perform topology and node attribute level augmentation to obtain the corrupted edge set E and node features ğ‘¿ of the corrupted graph view. In GCA, The probabilities ğ‘ ğ‘’ and ğ‘ ğ‘“ is different for generating the two views to provide diverse node context for contrastive learning. The probabilities for the first and the second view are denoted ğ‘ ğ‘’,1 , ğ‘ ğ‘“ ,1 and ğ‘ ğ‘’,2 , ğ‘ ğ‘“ ,2 respectively. Note that all centrality and weight measures used in the GCA are only dependent on intrinsic topology and node attributes. Therefore, they only need to be computed once and do not bring much computational burden.</p><p>In this paper, we propose and evaluate three model variants, denoted as GCA-DE, GCA-EV, and GCA-PR. The three variants use degree, eigenvector, and PageRank centrality measures respectively for both topology and node attribute level augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Theoretical Justification</head><p>In this section, we provide theoretical justification behind our model from two perspectives, i.e. the mutual information maximization and the triplet loss. Detailed proofs can be found in Appendix B.</p><p>Connections to the mutual information. Firstly, we reveal the connections between our loss and mutual information maximization between node features and the embeddings in the two views, which has been widely applied in the representation learning literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>. MI quantifies the amount of information obtained about one random variable by observing the other random variable. Theorem 1. Let X ğ‘– = {ğ’™ ğ‘˜ } ğ‘˜ âˆˆN (ğ‘–) be the neighborhood of node ğ‘£ ğ‘– that collectively maps to its output embedding, where N (ğ‘–) denotes the set of neighbors of node ğ‘£ ğ‘– specified by GNN architectures, and X be the corresponding random variable with a uniform distribution ğ‘ (X ğ‘– ) =<ref type="foot" target="#foot_0">1</ref> /ğ‘ . Given two random variables U, V âˆˆ R ğ¹ â€² being the embedding in the two views, with their joint distribution denoted as ğ‘ (U, V), our objective J is a lower bound of MI between encoder input X and node representations in two graph views U, V. Formally, J â‰¤ ğ¼ (X; U, V).</p><p>(</p><formula xml:id="formula_11">)<label>10</label></formula><p>Proof sketch. We first observe that our objective J is a lower bound of the InfoNCE objective <ref type="bibr" target="#b41">[42]</ref>, which is defined as <ref type="bibr" target="#b32">[33]</ref>. According to van den Oord et al. <ref type="bibr" target="#b41">[42]</ref>, the InfoNCE estimator is a lower bound of the true MI. Therefore, the theorem directly follows from the application of data processing inequality <ref type="bibr" target="#b4">[5]</ref>, which states that ğ¼ (U; V) â‰¤ ğ¼ (X; U, V). â–¡</p><formula xml:id="formula_12">ğ¼ NCE (U; V) â‰œ E ğ‘– ğ‘ (ğ’– ğ‘– ,ğ’— ğ‘– ) 1 ğ‘ ğ‘ ğ‘–=1 log ğ‘’ ğœƒ (ğ’– ğ‘– ,ğ’— ğ‘– ) 1 ğ‘ ğ‘ ğ‘— =1 ğ‘’ ğœƒ (ğ’– ğ‘– ,ğ’— ğ‘— )</formula><p>Remark. Theorem 1 reveals that maximizing J is equivalent to explicitly maximizing a lower bound of the mutual information ğ¼ (X; U, V) between input node features and learned node representations. Recent work further provides empirical evidence that optimizing a stricter bound of MI may not lead to better downstream performance on visual representation learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, which further highlights the importance of the design of data augmentation strategies.</p><p>When optimizing ğ¼ (ğ‘¼ ; ğ‘½ ), a lower bound of ğ¼ (ğ‘¿ ; ğ‘¼ , ğ‘½ ), we encourage the model to encode shared information between the two views. Since our proposed adaptive augmentation can preserve fundamental topological and semantic patterns from the amortized perspective, corrupted views will follow a skewed distribution where important link structures and features are emphasized. By contrasting views from the skewed distribution, the model is enforced to encode the emphasized information into the representation, which improves embedding quality.</p><p>Connections to the triplet loss. Alternatively, we may also view the optimization problem in Eq. ( <ref type="formula" target="#formula_3">2</ref>) as a classical triplet loss, commonly used in deep metric learning. Theorem 2. When the projection function ğ‘” is the identity function and we measure embedding similarity by simply taking the inner product, i.e., ğ‘  (ğ’–, ğ’—) = ğ’– âŠ¤ ğ’—, and further assuming that positive pairs are far more aligned than negative pairs, minimizing the pairwise objective â„“ (ğ’– ğ‘– , ğ’— ğ‘– ) coincides with maximizing the triplet loss, as given in the sequel</p><formula xml:id="formula_13">âˆ’ â„“ (ğ’– ğ‘– , ğ’— ğ‘– ) âˆ 4ğ‘ğœ + âˆ‘ï¸ ğ‘—â‰ ğ‘– âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘– âˆ¥ 2 âˆ’ âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘— âˆ¥ 2 + âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘– âˆ¥ 2 âˆ’ âˆ¥ğ’– ğ‘– âˆ’ ğ’– ğ‘— âˆ¥ 2 . (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>Remark. Theorem 2 draws connections between the objective and the classical triplet loss. In other words, we may regard the problem in Eq. ( <ref type="formula" target="#formula_3">2</ref>) as learning graph convolutional encoders to encourage positive samples being further away from negative samples in the embedding space. Moreover, by viewing the objective from the metric learning perspective, we highlight the importance of appropriate data augmentation schemes, which is often neglected in previous InfoMax-based methods. Specifically, as the objective pulls together representation of each node in the two corrupted views, the model is enforced to encode information in the input graph that is insensitive to perturbation. Since the proposed adaptive augmentation schemes tend to keep important link structures and node attributes intact in the perturbation, the model is guided to encode essential structural and semantic information into the representation, which improves the quality of embeddings. Last, the contrastive objective used in GCA is cheap to optimize, since we do not have to generate negative samples explicitly and all computation can be performed in parallel. In contrast, the triplet loss is known to be computationally expensive <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct experiments to evaluate our model through answering the following research questions.</p><p>â€¢ RQ1. Does our proposed GCA outperform existing baseline methods in node classification tasks?</p><p>â€¢ RQ2. Do all proposed adaptive graph augmentation schemes benefit the learning of model? How does each graph augmentation scheme affect model performance?</p><p>â€¢ RQ3. Is the proposed model sensitive to hyperparameters? How do key hyperparameters impact the model performance?</p><p>We begin with a brief introduction of the experimental setup, and then we proceed to details of experimental results and their analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Coauthor-CS <ref type="foot" target="#foot_3">4</ref> , and Coauthor-Physics<ref type="foot" target="#foot_4">5</ref> , to study the performance of transductive node classification. The datasets are collected from real-world networks of different kinds; their detailed statistics is summarized in Table <ref type="table" target="#tab_1">2</ref>.</p><p>â€¢ Wiki-CS <ref type="bibr" target="#b24">[25]</ref> is a reference network constructed from Wikipedia.</p><p>The nodes correspond to articles about computer science and edges are hyperlinks between the articles. Nodes are labeled with ten <ref type="bibr" target="#b9">(10)</ref> classes each representing a branch of the field. Node features are calculated as the average of pretrained GloVe <ref type="bibr" target="#b30">[31]</ref> word embeddings of the words in each article.</p><p>â€¢ Amazon-Computers and Amazon-Photo <ref type="bibr" target="#b36">[37]</ref> are two networks of co-buy relationships. They are constructed from Amazon co-purchase graphs, where nodes are goods and two goods are connected when they are frequently bought together. Each node has a sparse bag-of-words feature encoding product reviews and is labeled with its category.</p><p>â€¢ Coauthor-CS and Coauthor-Physics <ref type="bibr" target="#b36">[37]</ref> are two co-author networks, which contain co-authorship graphs based on the Microsoft Academic Graph from the KDD Cup 2016 challenge. In these graphs, nodes represent authors and edges indicate co-authorship relationships; that is, two nodes are connected if they have co-authored a paper. Each node has a sparse bag-of-words feature based on paper keywords of the author. The label of a node is the author's most active research field. Among these datasets, Wiki-CS has dense numerical features. While the other four datasets only contain sparse one-hot features. For the Wiki-CS dataset, we evaluate the models on the public splits shipped with the dataset <ref type="bibr" target="#b24">[25]</ref>. Regarding the other four co-coauthor and co-purchase datasets, since they have no public splits available, we instead use the random splits where 10%, 10%, and the rest 80% of nodes are randomly selected to be the train, validate, and test set, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation protocol.</head><p>For every experiment, we follow the linear evaluation scheme as introduced in VeliÄkoviÄ‡ et al. <ref type="bibr" target="#b43">[44]</ref>, where each model is firstly trained in an unsupervised manner; then, the resulting embeddings are used to train and test a simple â„“ 2 -regularized logistic regression classifier. We train the model for twenty <ref type="bibr" target="#b19">(20)</ref> runs for different data splits and report the averaged performance on each dataset for a fair evaluation. Moreover, we measure performance in terms of classification accuracy in these experiments.</p><p>4.1.3 Baselines. We consider representative baseline methods belonging to the following two categories, (1) traditional methods including DeepWalk <ref type="bibr" target="#b31">[32]</ref> and node2vec <ref type="bibr" target="#b10">[11]</ref> and (2) deep learning methods including Graph Autoencoders (GAE, VGAE) <ref type="bibr" target="#b20">[21]</ref>, Deep Graph Infomax (DGI) <ref type="bibr" target="#b43">[44]</ref>, Graphical Mutual Information Maximization (GMI) <ref type="bibr" target="#b29">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref type="bibr" target="#b14">[15]</ref>. Furthermore, we report the performance obtained using a logistic regression classifier on raw node features and DeepWalk with embeddings concatenated with input node features. To directly compare our proposed method with supervised counterparts, we also report the performance of two representative models Graph Convolutional Networks (GCN) <ref type="bibr" target="#b21">[22]</ref> and Graph Attention Networks (GAT) <ref type="bibr" target="#b42">[43]</ref>, where they are trained in an endto-end fashion. For all baselines, we report their performance based on their official code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation details.</head><p>We employ a two-layer GCN <ref type="bibr" target="#b21">[22]</ref> as the encoder for all deep learning baselines due to its simplicity. The encoder architecture is formally given by</p><formula xml:id="formula_16">GC ğ‘– (ğ‘¿, ğ‘¨) = ğœ Dâˆ’ 1 2 Ã‚ Dâˆ’ 1 2 ğ‘¿ğ‘¾ ğ‘– ,<label>(12)</label></formula><formula xml:id="formula_17">ğ‘“ (ğ‘¿, ğ‘¨) = GC 2 (GC 1 (ğ‘¿, ğ‘¨), ğ‘¨).<label>(13)</label></formula><p>where Ã‚ = ğ‘¨ + ğ‘° is the adjacency matrix with self-loops, D = ğ‘– Ã‚ğ‘– is the degree matrix, ğœ (â€¢) is a nonlinear activation function, e.g., ReLU(â€¢) = max(0, â€¢), and ğ‘¾ ğ‘– is a trainable weight matrix. For experimental specifications, including details of the configurations of the optimizer and hyperparameter settings, we refer readers of interest to Appendix A for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance on Node Classification (RQ1)</head><p>The empirical performance is summarized in Table <ref type="table">3</ref>. Overall, from the table, we can see that our proposed model shows strong performance across all five datasets. GCA consistently performs better than unsupervised baselines by considerable margins on both transductive tasks. The strong performance verifies the superiority of the proposed contrastive learning framework. On the two Coauthor datasets, we note that existing baselines have already obtained high enough performance; our method GCA still pushes that boundary forward. Moreover, we particularly note that GCA is competitive with models trained with label supervision on all five datasets.</p><p>We make other observations as follows. Firstly, the performance of traditional contrastive learning methods like DeepWalk is inferior to the simple logistic regression classifier that only uses raw features on some datasets (Coauthor-CS and Coauthor-Physics), which suggests that these methods may be ineffective in utilizing node features. Unlike traditional work, we see that GCN-based methods, e.g., GAE, are capable of incorporating node features when learning embeddings. However, we note that on certain datasets (Wiki-CS), their performance is still worse than DeepWalk + feature, which we believe can be attributed to their naÃ¯ve method of selecting negative samples that simply chooses contrastive pairs based on edges. This fact further demonstrates the important role of selecting negative samples based on augmented graph views in contrastive representation learning. Moreover, compared to existing baselines DGI, GMI, and MVGRL, our proposed method performs strong, Table <ref type="table">3</ref>: Summary of performance on node classification in terms of accuracy in percentage (on transductive tasks) or microaveraged F1 score (on inductive tasks) with standard deviation. Available data for each method during the training phase is shown in the second column, where ğ‘¿, ğ‘¨, ğ’€ correspond to node features, the adjacency matrix, and labels respectively. The highest performance of unsupervised models is highlighted in boldface; the highest performance of supervised models is underlined. OOM indicates Out-Of-Memory on a 32GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training Data Wiki-CS Amazon-Computers Amazon Secondly, we observe that all three variants with different node centrality measures of GCA outperform existing contrastive baselines on all datasets. We also notice that GCA-DE and GCA-PR with the degree and PageRank centrality respectively are two strong variants that achieve the best or competitive performance on all datasets. Please kindly note that the result indicates that our model is not limited to specific choices of centrality measures and verifies the effectiveness and generality of our proposed framework.</p><p>In summary, the superior performance of GCA compared to existing state-of-the-art methods verifies the effectiveness of our proposed GCA framework that performs data augmentation adaptive to the graph structure and attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies (RQ2)</head><p>In this section, we substitute the proposed topology and attribute level augmentation with their uniform counterparts to study the impact of each component of GCA. GCA-T-A denotes the model with uniform topology and node attribute level augmentation schemes, where the probabilities of dropping edge and masking features are set to the same for all nodes. The variants GCA-T and GCA-A is defined similarly except that we substitute the topology and the node attribute augmentation scheme with uniform sampling in the two models respectively. Degree centrality is used in all the variants for fair comparison.</p><p>The results are presented in Table <ref type="table" target="#tab_3">4</ref>, where we can see that both topology-level and node-attribute-level adaptive augmentation scheme improve model performance consistently on all datasets. In addition, the combination of adaptive augmentation schemes on the two levels further benefits the performance. On the Amazon-Computers dataset, our proposed GCA gains 1.5% absolute improvement compared to the base model with no adaptive augmentation enabled. The results verify the effectiveness of our proposed adaptive augmentation schemes on both topology and node attribute levels.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sensitivity Analysis (RQ3)</head><p>In this section, we perform sensitivity analysis on critical hyperparameters in GCA, namely four probabilities ğ‘ ğ‘’,1 , ğ‘ ğ‘“ ,1 , ğ‘ ğ‘’,2 , and ğ‘ ğ‘“ ,2 that determine the generation of graph views to show the model stability under the perturbation of these hyperparameters.</p><p>We conduct transductive node classification by varying these parameters from 0.1 to 0.9. For sake of visualization brevity, we set ğ‘ ğ‘’ = ğ‘ ğ‘’,1 = ğ‘ ğ‘’,2 and ğ‘ ğ‘“ = ğ‘ ğ‘“ ,1 = ğ‘ ğ‘“ ,2 . In other words, ğ‘ ğ‘’ and ğ‘ ğ‘“ control the magnitude of the proposed topology and node attribute level augmentation. Note that we only change these four parameters in the sensitivity analysis, and other parameters remain the same as previously described.</p><p>The results on the Amazon-Photo dataset are shown in Figure <ref type="figure" target="#fig_5">3</ref>. From the figure, it can be observed that the performance of node classification in terms of accuracy is relatively stable when the parameters are not too large, as shown in the plateau in the figure. We thus conclude that, overall, our model is insensitive to these probabilities, demonstrating the robustness to hyperparameter tuning. If the probability is set too large (e.g., &gt; 0.5), the original graph will be heavily undermined. For example, when ğ‘ ğ‘’ = 0.9, almost every existing edge has been removed, leading to isolated nodes in the generated graph views. Then, under such circumstances, the graph convolutional network is hard to learn useful information from node neighborhoods. Therefore, the learned node embeddings in the two graph views are not distinctive enough, which will result in the difficulty of optimizing the contrastive objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have developed a novel graph contrastive representation learning framework with adaptive augmentation. Our model learns representation by maximizing the agreement of node embeddings between views that are generated by adaptive graph augmentation. The proposed adaptive augmentation scheme first identify important edges and feature dimensions via centrality measures. Then, on the topology level, we randomly remove edges by assigning large probabilities on unimportant edges to enforce the model to recognize important connective structures. On the node attribute level, we corrupt attributes by adding more noise to unimportant feature dimensions to emphasize the underlying semantic information. We have conducted comprehensive experiments using various real-world datasets. Experimental results demonstrate that our proposed method consistently outperforms existing state-of-the-art methods and even surpasses several supervised counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS A.1 Computing Infrastructures</head><p>All models are implemented using PyTorch Geometric 1.6.1 <ref type="bibr" target="#b7">[8]</ref>, PyTorch 1.6.0 <ref type="bibr" target="#b28">[29]</ref>, and NetworkX 2.5. All datasets used throughout experiments are available in PyTorch Geometric libraries. We conduct experiments on a computer server with four NVIDIA Tesla V100S GPUs (32GB memory each) and twelve Intel Xeon Silver 4214 CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyperparameter Specifications</head><p>All models are initialized with Glorot initialization <ref type="bibr" target="#b9">[10]</ref>, and trained using Adam SGD optimizer <ref type="bibr" target="#b19">[20]</ref> on all datasets. The â„“ 2 weight decay factor is set to 10 âˆ’5 and the dropout rate <ref type="bibr" target="#b37">[38]</ref> is set to zero on all datasets. The probability parameters controlling the sampling process, ğ‘ ğ‘Ÿ,1 , ğ‘ ğ‘š,1 for the first view and ğ‘ ğ‘Ÿ,2 , ğ‘ ğ‘š,2 for the second view, are all selected between 0.0 and 0.4, since the original graph will be overly corrupted when the probability is set too large. Note that to generate different contexts for nodes in the two views, ğ‘ ğ‘Ÿ,1 and ğ‘ ğ‘Ÿ,2 should be distinct, and the same holds for ğ‘ ğ‘š,1 and ğ‘ ğ‘š,2 . All dataset-specific hyperparameters are summarized in Table <ref type="table" target="#tab_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DETAILED PROOFS B.1 Proof of Theorem 1</head><p>Theorem 1. Let X ğ‘– = {ğ’™ ğ‘˜ } ğ‘˜ âˆˆN (ğ‘–) be the neighborhood of node ğ‘£ ğ‘– that collectively maps to its output embedding, where N (ğ‘–) denotes the set of neighbors of node ğ‘£ ğ‘– specified by GNN architectures, and X be the corresponding random variable with a uniform distribution ğ‘ (X ğ‘– ) = 1 ğ‘ . Given two random variables U, V âˆˆ R ğ¹ â€² being the embedding in the two views, with their joint distribution denoted as ğ‘ (U, V), our objective J is a lower bound of MI between encoder input X and node representations in two graph views U, V. Formally, J â‰¤ ğ¼ (X; U, V).</p><p>(</p><formula xml:id="formula_18">)<label>14</label></formula><p>Proof. We first show the connection between our objective J and the InfoNCE objective <ref type="bibr" target="#b41">[42]</ref> , which can be defined as <ref type="bibr" target="#b32">[33]</ref> ğ¼ NCE (U; V) â‰œ E   </p><p>Thus, we arrive at 2J â‰¤ ğ¼ (U; V) + ğ¼ (V; U) = 2ğ¼ (U; V),</p><p>which leads to the inequality</p><formula xml:id="formula_21">J â‰¤ ğ¼ (U; V).<label>(20)</label></formula><p>According to the data processing inequality <ref type="bibr" target="#b4">[5]</ref>, which states that, for all random variables X, Y, Z satisfying the Markov relation X â†’ Y â†’ Z, the inequality ğ¼ (X; Z) â‰¤ ğ¼ (X; Y) holds. Then, we observe that X, U, V satisfy the relation U â† X â†’ V. Since, U and V are conditionally independent after observing X, the relation is Markov equivalent to U â†’ X â†’ V, which leads to ğ¼ (U; V) â‰¤ ğ¼ (U; X). We further notice that the relation X â†’ (U, V) â†’ U holds, and hence it follows that ğ¼ (X; U) â‰¤ ğ¼ (X; U, V). Combining the two inequalities yields the required inequality ğ¼ (U; V) â‰¤ ğ¼ (X; U, V).</p><p>(</p><formula xml:id="formula_22">)<label>21</label></formula><p>Following Eq. ( <ref type="formula" target="#formula_21">20</ref>) and Eq. ( <ref type="formula" target="#formula_22">21</ref>), we finally arrive at inequality J â‰¤ ğ¼ (X; U, V), <ref type="bibr" target="#b21">(22)</ref> which concludes the proof. â–¡ </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed deep Graph Contrastive representation learning with Adaptive augmentation (GCA) model. We first generate two graph views via stochastic augmentation that is adaptive to the graph structure and attributes. Then, the two graphs are fed into a shared Graph Neural Network (GNN) to learn representations. We train the model with a contrastive objective, which pulls representations of one node together while pushing node representations away from other node representations in the two views. N.B., we define the negative samples as all other nodes in the two views. Therefore, negative samples are from two sources, intra-view (in purple) and inter-view nodes (in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 : 4</head><label>14</label><figDesc>GCA training algorithm 1 for ğ‘’ğ‘ğ‘œğ‘â„ â† 1, 2, â€¢ â€¢ â€¢ do 2 Sample two stochastic augmentation functions ğ‘¡ âˆ¼ T and ğ‘¡ â€² âˆ¼ T 3 Generate two graph views G 1 = ğ‘¡ (G) and G 2 = ğ‘¡ â€² (G) by performing corruption on G Obtain node embeddings ğ‘¼ of G 1 using the encoder ğ‘“ 5 Obtain node embeddings ğ‘½ of G 2 using the encoder ğ‘“ 6 Compute the contrastive objective J with Eq. (2) 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of edge centrality computed by three schemes in the Karate club dataset, where centrality values are shown in terms of the thickness of edges. Node colors indicate two classes inside the network; two coaches are in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>average. ğ‘ ğ‘“ is a hyperparameter that controls the overall magnitude of masking features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The performance of GCA with varying different hyperparameters on the Amazon-Photo dataset in terms of node classification accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>ğ‘–</head><label></label><figDesc>ğ‘ (ğ’– ğ‘– ,ğ’— ğ‘– ) ğœƒ (ğ’– ğ‘– ,ğ’— ğ‘— ) , where the critic function is defined as ğœƒ (ğ’™, ğ’š) = ğ‘  (ğ‘”(ğ’™), ğ‘”(ğ’š)). We further define ğœŒ ğ‘Ÿ (ğ’– ğ‘– ) = ğ‘ ğ‘—â‰ ğ‘– exp(ğœƒ (ğ’– ğ‘– , ğ’– ğ‘— )/ğœ) and ğœŒ ğ‘ (ğ’– ğ‘– ) = ğ‘ ğ‘—=1 exp(ğœƒ (ğ’– ğ‘– , ğ’— ğ‘— )/ğœ) for convenience of notation. ğœŒ ğ‘Ÿ (ğ’— ğ‘– ) and ğœŒ ğ‘ (ğ’— ğ‘– )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 17 )</head><label>17</label><figDesc>According to Poole et al.<ref type="bibr" target="#b32">[33]</ref>, the InfoNCE estimator is a lower bound of the true MI, i.e.ğ¼ NCE (U, V) â‰¤ ğ¼ (U; V).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>B. 2 2 Theorem 2 .âˆ¥ğ’– 2 âˆ</head><label>2222</label><figDesc>Proof of Theorem When the projection function ğ‘” is the identity function and we measure embedding similarity by simply taking inner product, and further assuming that positive pairs are far more aligned than negative pairs, minimizing the pairwise objective â„“ (ğ’– ğ‘– , ğ’— ğ‘– ) coincides with maximizing the triplet loss, as given in the sequelâˆ’â„“ (ğ’– ğ‘– , ğ’— ğ‘– ) âˆ 4ğ‘ğœ + âˆ‘ï¸ ğ‘—â‰ ğ‘– âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘– âˆ¥ 2 âˆ’ âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘— âˆ¥ 2 + âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘– âˆ¥ 2 âˆ’ âˆ¥ğ’– ğ‘– âˆ’ ğ’– ğ‘— âˆ¥ 2 . (23)Proof. Based on the assumptions, we can rearrange the pairwise objective asâˆ’â„“ (ğ’– ğ‘– , ğ’— ğ‘– ) = âˆ’ log ğ‘’ (ğ’– âŠ¤ ğ‘– ğ’— ğ‘– /ğœ) ğ‘ ğ‘˜=1 ğ‘’ (ğ’– âŠ¤ ğ‘– ğ’— ğ‘˜ /ğœ) + ğ‘ ğ‘˜â‰ ğ‘– ğ‘’ (ğ’– âŠ¤ ğ‘– ğ’– ğ‘˜ /ğœ) of first order, âˆ’ â„“ (ğ’– ğ‘– , ğ’— ğ‘– ) â‰ˆ ğ‘ âˆ‘ï¸ ğ‘˜â‰ ğ‘– exp ğ’– âŠ¤ ğ‘– ğ’— ğ‘˜ âˆ’ ğ’– âŠ¤ ğ‘– ğ’— ğ‘– ğœ + ğ‘ âˆ‘ï¸ ğ‘˜â‰ ğ‘– exp ğ’– âŠ¤ ğ‘– ğ’– ğ‘˜ âˆ’ ğ’– âŠ¤ ğ‘– ğ’— ğ‘– ğœ ğ‘– ğ’— ğ‘˜ âˆ’ ğ’– âŠ¤ ğ‘– ğ’— ğ‘– ) + ğ‘ âˆ‘ï¸ ğ‘˜â‰ ğ‘– (ğ’– âŠ¤ ğ‘– ğ’– ğ‘˜ âˆ’ ğ’– âŠ¤ ğ‘– ğ’— ğ‘– ) ğ‘– âˆ’ ğ’— ğ‘˜ âˆ¥ 2 âˆ’ âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘– âˆ¥ 2 + âˆ¥ğ’– ğ‘– âˆ’ ğ’– ğ‘˜ âˆ¥ 2 âˆ’ âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘– âˆ¥ 4ğ‘ğœ + ğ‘ âˆ‘ï¸ ğ‘˜â‰ ğ‘– âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘– âˆ¥ 2 âˆ’ âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘˜ âˆ¥ 2 + âˆ¥ğ’– ğ‘– âˆ’ ğ’— ğ‘– âˆ¥ 2 âˆ’ âˆ¥ğ’– ğ‘– âˆ’ ğ’– ğ‘˜ âˆ¥ 2 ,(25)which concludes the proof. â–¡</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with related work.</figDesc><table><row><cell>Method</cell><cell>Contrastive objective</cell><cell cols="2">Topology Attribute</cell></row><row><cell cols="3">DGI GMI MVGRL Node-global Uniform Node-global Uniform Node-node -</cell><cell>---</cell></row><row><cell>GCA</cell><cell cols="3">Node-node Adaptive Adaptive</cell></row><row><cell cols="4">proposed GCA and other state-of-the-art graph contrastive rep-resentation learning methods, including DGI</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets used in experiments.</figDesc><table><row><cell>4.1.1 Datasets. For comprehensive comparison, we use six widely-used datasets, Wiki-CS 1 , Amazon-Computers 2 , Amazon-Photo 3 ,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>in constructing negative samples, leading to better performance. Note that, although MVGRL employs diffusion to incorporate global information into augmented views, it still fails to consider the impacts of different edges adaptively on input graphs. The superior performance of GCA verifies that our proposed adaptive data augmentation scheme is able to help improve the embedding quality by preserving important patterns during perturbation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">-Photo Coauthor-CS Coauthor-Physics</cell></row><row><cell>Raw features node2vec DeepWalk DeepWalk + features</cell><cell>ğ‘¿ ğ‘¨ ğ‘¨ ğ‘¿, ğ‘¨</cell><cell>71.98 Â± 0.00 71.79 Â± 0.05 74.35 Â± 0.06 77.21 Â± 0.03</cell><cell>73.81 Â± 0.00 84.39 Â± 0.08 85.68 Â± 0.06 86.28 Â± 0.07</cell><cell>78.53 Â± 0.00 89.67 Â± 0.12 89.44 Â± 0.11 90.05 Â± 0.08</cell><cell>90.37 Â± 0.00 85.08 Â± 0.03 84.61 Â± 0.22 87.70 Â± 0.04</cell><cell>93.58 Â± 0.00 91.19 Â± 0.04 91.77 Â± 0.15 94.90 Â± 0.09</cell></row><row><cell>GAE VGAE DGI GMI MVGRL GCA-DE GCA-PR GCA-EV</cell><cell>ğ‘¿, ğ‘¨ ğ‘¿, ğ‘¨ ğ‘¿, ğ‘¨ ğ‘¿, ğ‘¨ ğ‘¿, ğ‘¨ ğ‘¿, ğ‘¨ ğ‘¿, ğ‘¨ ğ‘¿, ğ‘¨</cell><cell>70.15 Â± 0.01 75.63 Â± 0.19 75.35 Â± 0.14 74.85 Â± 0.08 77.52 Â± 0.08 78.30 Â± 0.00 78.35 Â± 0.05 78.23 Â± 0.04</cell><cell>85.27 Â± 0.19 86.37 Â± 0.21 83.95 Â± 0.47 82.21 Â± 0.31 87.52 Â± 0.11 88.94 Â± 0.15 88.77 Â± 0.19 88.49 Â± 0.20</cell><cell>91.62 Â± 0.13 92.20 Â± 0.11 91.61 Â± 0.22 90.68 Â± 0.17 91.74 Â± 0.07 92.49 Â± 0.09 92.53 Â± 0.16 92.24 Â± 0.21</cell><cell>90.01 Â± 0.71 92.11 Â± 0.09 92.15 Â± 0.63 OOM 92.11 Â± 0.12 93.10 Â± 0.01 93.06 Â± 0.03 92.95 Â± 0.13</cell><cell>94.92 Â± 0.07 94.52 Â± 0.00 94.51 Â± 0.52 OOM 95.33 Â± 0.03 95.68 Â± 0.05 95.72 Â± 0.03 95.73 Â± 0.03</cell></row><row><cell>GCN GAT</cell><cell>ğ‘¿, ğ‘¨, ğ’€ ğ‘¿, ğ‘¨, ğ’€</cell><cell>77.19 Â± 0.12 77.65 Â± 0.11</cell><cell>86.51 Â± 0.54 86.93 Â± 0.29</cell><cell>92.42 Â± 0.22 92.56 Â± 0.35</cell><cell>93.03 Â± 0.31 92.31 Â± 0.24</cell><cell>95.65 Â± 0.16 95.47 Â± 0.15</cell></row><row><cell>adaptive data augmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance of model variants on node classification in terms of accuracy in percentage with standard deviation. Degree centrality is used in all variants. The highest performance is highlighted in boldface.</figDesc><table><row><cell>Variant</cell><cell>Topology Attribute</cell><cell>WikiCS</cell><cell cols="3">Amazon-Computers Amazon-Photo Coauthor-CS Coauthor-Physics</cell></row><row><cell cols="3">GCA-T-A Uniform Uniform 78.19 Â± 0.01 GCA-T Uniform Adaptive 78.23 Â± 0.02 GCA-A Adaptive Uniform 78.25 Â± 0.02 GCA Adaptive Adaptive 78.30 Â± 0.01</cell><cell>87.46 Â± 0.22 87.91 Â± 0.12 88.71 Â± 0.18 88.94 Â± 0.15</cell><cell>92.15 Â± 0.24 92.20 Â± 0.26 92.23 Â± 0.20 92.49 Â± 0.09</cell><cell>92.93 Â± 0.01 93.07 Â± 0.01 93.02 Â± 0.01 93.10 Â± 0.01</cell><cell>95.26 Â± 0.02 95.59 Â± 0.04 95.54 Â± 0.02 95.68 Â± 0.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Hypeparameter specifications.Dataset ğ‘ ğ‘’,1 ğ‘ ğ‘’,2 ğ‘ ğ‘“ ,1 ğ‘ ğ‘“ ,2 ğ‘– ğ‘ (ğ’– ğ‘– ,ğ’— ğ‘– ) ğœƒ (ğ’– ğ‘– , ğ’— ğ‘– )/ğœ) âˆšï¸ (ğœŒ ğ‘ (ğ’– ğ‘– ) + ğœŒ ğ‘Ÿ (ğ’– ğ‘– )) (ğœŒ ğ‘ (ğ’— ğ‘– ) + ğœŒ ğ‘Ÿ (ğ’— ğ‘– )) . (15)Using the notation of ğœŒ ğ‘ , the InfoNCE estimator ğ¼ NCE can be written asğ¼ NCE (U, V) = E ğ‘– ğ‘ (ğ’– ğ‘– ,ğ’— ğ‘– ) ğœŒ ğ‘Ÿ (ğ’— ğ‘– )ğœŒ ğ‘ (ğ’— ğ‘– ) â‰¤ ğ¼ NCE (U, V) + ğ¼ NCE (V, U).</figDesc><table><row><cell>Learning rate</cell><cell>Training epochs</cell><cell>Hidden dimension</cell><cell>Activation function</cell></row></table><note>ğ‘– , ğ’— ğ‘– )/ğœ)ğœŒ ğ‘ (ğ’– ğ‘– ) .(16)Therefore,2J = ğ¼ NCE (U, V) âˆ’ E ğ‘– ğ‘ (ğ’– ğ‘– ,ğ’— ğ‘– ) 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 log 1 + ğœŒ ğ‘Ÿ (ğ’– ğ‘– ) ğœŒ ğ‘ (ğ’– ğ‘– ) + ğ¼ NCE (V, U) âˆ’ E ğ‘– ğ‘ (ğ’– ğ‘– ,ğ’— ğ‘– ) 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 log 1 +</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/pmernyei/wiki-cs-dataset/raw/master/dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_ electronics_computers.npz</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_ electronics_photo.npz</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs. npz</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_ phy.npz</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Representations by Maximizing Mutual Information Across Views</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Philip Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15509" to="15519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Power and Centrality: A Family of Measures</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Bonacich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. J. Sociology</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1170" to="1182" />
			<date type="published" when="1987-03">1987. March 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="10709" to="10719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
				<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
				<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pretrain and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. ACL</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics. ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Framework For Contrastive Self-Supervised Learning And Designing A New Approach</title>
		<author>
			<persName><forename type="first">William</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00104v1[cs.CV]</idno>
		<imprint>
			<date type="published" when="2020-09">2020. Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning by Predicting Image Rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
				<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the Difficulty of Training Deep Feedforward Neural Networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR.org</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR.org</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>HyvÃ¤rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Representation Learning on Graphs: Methods and Applications. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive Multi-View Representation Learning on Graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3451" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>HÃ©naff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272v2[cs.CV]</idno>
		<title level="m">Data-Efficient Image Recognition with Contrastive Predictive Coding</title>
				<imprint>
			<date type="published" when="2019-05">2019. May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Deep Representations by Mutual Information Estimation and Maximization</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification</title>
		<author>
			<persName><forename type="first">Fenyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. IJCAI.org</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence. IJCAI.org</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4532" to="4539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
				<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Deep Learning Workshop@NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Colorization as a Proxy Task for Visual Understanding</title>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-Organization in a Perceptual Network</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">PÃ©ter</forename><surname>Mernyei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catalina</forename><surname>Cangea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Word Embeddings Efficiently with Noise-Contrastive Estimation</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Networks: An Introduction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The PageRank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference</title>
				<meeting>the Web Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. ACL</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing. ACL</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On Variational Bounds of Mutual Information</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">AÃ¤ron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>GÃ¼nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868v2[cs.LG]</idno>
		<title level="m">Pitfalls of Graph Neural Network Evaluation</title>
				<imprint>
			<date type="published" when="2018-11">2018. Nov. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks From Overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849v4[cs.CV]</idno>
		<title level="m">Contrastive Multiview Coding</title>
				<imprint>
			<date type="published" when="2019-06">2019. June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243v1[cs.CV]</idno>
		<title level="m">What Makes for Good Views for Contrastive Learning</title>
				<imprint>
			<date type="published" when="2020-05">2020. May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On Mutual Information Maximization for Representation Learning</title>
		<author>
			<persName><forename type="first">Josip</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">K</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">AÃ¤ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748v2[cs.LG]</idno>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
				<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De</surname></persName>
		</author>
		<author>
			<persName><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Christopher Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Mosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13149v2[cs.LG]</idno>
		<title level="m">On Mutual Information in Contrastive Learning for Visual Representations</title>
				<imprint>
			<date type="published" when="2020-05">2020. May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2018 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05659v1[cs.CV]</idno>
		<title level="m">What Should Not Be Contrastive in Contrastive Learning</title>
				<imprint>
			<date type="published" when="2020-08">2020. Aug. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised Embedding Learning via Invariant and Spreading Instance Feature</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2019 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An Information Flow Model for Conflict and Fission in Small Groups</title>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">W</forename><surname>Zachary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Anthropological Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="452" to="473" />
			<date type="published" when="1977">1977. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep Graph Contrastive Representation Learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
