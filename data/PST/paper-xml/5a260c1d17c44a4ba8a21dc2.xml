<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3HAN: A Deep Neural Network for Fake News Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-21">21 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sneha</forename><surname>Singhania</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">International Institute of Information Technology -Bangalore</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nigel</forename><surname>Fernandez</surname></persName>
							<email>nigelsteven.fernandez@iiitb.org</email>
							<affiliation key="aff0">
								<orgName type="institution">International Institute of Information Technology -Bangalore</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shrisha</forename><surname>Rao</surname></persName>
							<email>shrao@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">International Institute of Information Technology -Bangalore</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3HAN: A Deep Neural Network for Fake News Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-21">21 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">F4CECA7AE112AD9BB7C6134A6F13D8C2</idno>
					<idno type="arXiv">arXiv:2306.12014v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fake news</term>
					<term>deep learning</term>
					<term>text representation</term>
					<term>attention mechanism</term>
					<term>text classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rapid spread of fake news is a serious problem calling for AI solutions. We employ a deep learning based automated detector through a three level hierarchical attention network (3HAN) for fast, accurate detection of fake news. 3HAN has three levels, one each for words, sentences, and the headline, and constructs a news vector: an effective representation of an input news article, by processing an article in an hierarchical bottom-up manner. The headline is known to be a distinguishing feature of fake news, and furthermore, relatively few words and sentences in an article are more important than the rest. 3HAN gives a differential importance to parts of an article, on account of its three layers of attention. By experiments on a large real-world data set, we observe the effectiveness of 3HAN with an accuracy of 96.77%. Unlike some other deep learning models, 3HAN provides an understandable output through the attention weights given to different parts of an article, which can be visualized through a heatmap to enable further manual fact checking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The spread of fake news is a matter of concern due to its possible role in manipulating public opinion. We define fake news in line with The New York Times as a "made up story with the intention to deceive, often with monetary gain as a motive" <ref type="bibr" target="#b0">[1]</ref>. The fake news problem is complex given its varied interpretations across demographics.</p><p>We present a three level hierarchical attention network (3HAN) which creates an effective representation of a news article called news vector. A news vector can be used to classify an article by assigning a probability of being fake. Unlike other neural models which are opaque in their internal reasoning and give results that are difficult to analyze, 3HAN provides an importance score for each word and sentence of an input article based on its relevance in arriving at the output probability of that article being fake. These importance scores can be visualized ⋆ These authors contributed equally to this work. through a heatmap, providing key words and sentences to be investigated by human fact-checkers.</p><p>Current work in detecting misinformation is divided between automated fact checking <ref type="bibr" target="#b1">[2]</ref>, reaction based analysis <ref type="bibr" target="#b2">[3]</ref> and style based analysis <ref type="bibr" target="#b3">[4]</ref>. We explore the nascent domain of using neural models to detect fake news. Current stateof-the-art general purpose text classifiers like Bag-of-words <ref type="bibr" target="#b4">[5]</ref>, Bag-of-ngrams with SVM <ref type="bibr" target="#b5">[6]</ref>, CNNs, LSTMs and GRUs <ref type="bibr" target="#b6">[7]</ref> can be used to classify articles by simply concatenating the headline with the body. This concatenation though, fails to exploit the article structure.</p><p>In 3HAN, we interpret the structure of an article as a three level hierarchy modelling article semantics on the principle of compositionality <ref type="bibr" target="#b7">[8]</ref>. Words form sentences, sentences form the body and the headline with the body forms the article. We hypothesize forming an effective representation of an article using the hierarchy and the interactions between its parts. These interactions take the form of context of a word in its neighbouring words, coherence of a sentence with its neighbouring sentences and stance of a headline with respect to the body. Words, sentences and headline are differentially informative dependent on their interactions in the formation of a news vector. We incorporate three layers of attention mechanisms <ref type="bibr" target="#b8">[9]</ref> to exploit this differential relevance.</p><p>The design of 3HAN is inspired by the hierarchical attention network (HAN) <ref type="bibr" target="#b9">[10]</ref>. HAN is used to form a general document representation. We design 3HAN unique to the detection of fake news. When manually fact-checking an article the first thing that catches the eye is the headline. We observe a headline to be (i) a distinctive feature of an article <ref type="bibr" target="#b10">[11]</ref>, (ii) a concise summary of the article body and (iii) inherently containing useful information in the form of its stance with respect to the body. We refer to these observations as our headline premise. The third level in 3HAN is especially designed to use our headline premise.</p><p>From our headline premise, we hypothesize that a neural model should accurately classify articles based on headlines alone. Using this hypothesis, we use headlines to perform a supervised pre-training of the initial layers of 3HAN for a better initialization of 3HAN. The visualization of attention layers in 3HAN indicates important parts of an article instrumental in detecting an article as fake news. These important parts can be further investigated by human fact-checkers.</p><p>We compare the performance of 3HAN with multiple state-of-the-art traditional and neural baselines. Experiments on a large real world news data set demonstrate the superior performance of 3HAN over all baselines with 3HAN performing with an accuracy of 96.24%. Our pre-trained 3HAN model is our best performing model with an accuracy of 96.77%. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Design</head><p>The architecture of 3HAN is shown in Fig. <ref type="figure">1</ref>. We define a news vector as a projection of a news article into a vector representation suitable for effective 1 Our code is available at: https://github.com/ni9elf/3HAN.</p><formula xml:id="formula_0">݄ ଶ ଷ ݄ ଶ ଷ ݄ ଷ ݄ ଷ ݄ ାଵ ଷ ݄ ାଵ ଷ ݄ ଵ ଷ ݄ ଵ ଷ ͙ ͙ ͙ ݄ ଵ ௦ ݄ ଵ ௦ ݄ ଶ ௦ ݄ ଶ ௦ ݄ ௦ ݄ ௦ ͙ ͙ ͙ ݄ ଶଵ ௪ ݄ ଶଵ ௪ ݄ ଶଶ ௪ ݄ ଶଶ ௪ ݄ ଶ் మ ௪ ݄ ଶ் మ ௪ ͙ ͙ ͙ sigmoid ‫ݓ‬ ଶଵ ‫ݓ‬ ଶଶ ‫ݓ‬ ଶ் మ ‫ݏ‬ ‫ݏ‬ ଵ ‫ݕ‬ ଵ ‫ݕ‬ ‫ݕ‬ ଶ ‫ݒ‬ ‫ݒ‬ α ଶଵ α ଶଶ α ଶ்¡ α α ଵ α ଶ β ଵ β ାଵ β ଶ β ‫ݑ‬ ௪ ‫ݑ‬ ௌ ‫ݑ‬ ଷ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Headline-Body Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Headline-Body Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>News Vector</head><p>Fig. <ref type="figure">1</ref>. Model Architecture of 3HAN classification of articles. A news vector is constructed using 3HAN. To capture the body hierarchy and interactions between parts when forming the news vector, 3HAN uses the following parts from HAN <ref type="bibr" target="#b9">[10]</ref>: word sequence encoder, word level attention (Layer 1), sentence encoder, sentence level attention (Layer 2). In addition to the preceding parts, we exploit our headline premise by adding: headline-body encoder and headline-body level attention (Layer 3).</p><p>Sequence Encoder using GRU. A Gated Recurrent Unit (GRU) <ref type="bibr" target="#b11">[12]</ref> adaptively captures dependencies between sequential input sequences over time. Gating signals control how the previous hidden state h t-1 and current input x t generate an intermediate hidden state h t to update the current hidden state h t . GRU consists of a reset gate r t and an update gate z t . r t determines how to combine x t with h t-1 while z t determines how much of h t-1 and h t to use. ⊙ denotes the Hadamard product. The GRU model is presented at time t as:</p><formula xml:id="formula_1">h t = tanh (W h x t + U h (r t ⊙ h t-1 ) + b h )<label>(1)</label></formula><formula xml:id="formula_2">h t = (1 -z t ) ⊙ h t-1 + z t ⊙ h t<label>(2)</label></formula><p>with the gates presented as:</p><formula xml:id="formula_3">z t = σ (W z x t + U z h t-1 + b z ) , r t = σ (W r x t + U r h t-1 + b r )<label>(3)</label></formula><p>Word Encoder. We denote word j of sentence i by w ij with sentence i containing T i words. Each word w ij is converted to a word embedding x ij using GloVe <ref type="bibr" target="#b12">[13]</ref> embedding W e (x ij = W e (w ij )). We use a bidirectional GRU <ref type="bibr" target="#b8">[9]</ref> to form an annotation of each word which summarizes the context of the word with preceding and following words in the sentence. A bidirectional GRU consists of a forward ---→ GRU and backward ← ---GRU. The overhead arrow in our notation does not denote a vector, it instead denotes the direction of the GRU run.</p><p>---→ GRU reads the word embedding sequence ordered (x i1 , x i2 , . . . , x iTi ) to form forward annotations using hidden states</p><formula xml:id="formula_4">-→ h w i1 , -→ h w i2 , . . . , -→ h w iTi . Similarly ← --- GRU reads the word embedding sequence ordered (x iTi , x iTi-1 , . . . , x i1 ) to form backward anno- tations ← - h w iTi , ← - h w iTi-1 , . . . , ← - h w i1 . h w ij is formed as -→ h w ij , ← - h w ij (concatenation). -→ h w ij = ---→ GRU (x ik ) , k ∈ [1, j]<label>(4)</label></formula><p>←h</p><formula xml:id="formula_5">w ij = ← --- GRU (x ik ) , k ∈ [T i , j]<label>(5)</label></formula><formula xml:id="formula_6">h w ij = -→ h w ij , ← - h w ij<label>(6)</label></formula><p>Word Attention. A sentence representation is formed using an attention layer to extract relevant words of a sentence. The word annotation h w ij is fed through a one-layer MLP to get a hidden representation u ij <ref type="bibr" target="#b9">[10]</ref>. The similarity of each word u ij with a word level relevance vector u w decides the attention weights α ij normalized using a softmax function <ref type="bibr" target="#b9">[10]</ref>. The sentence encoding s i is a weighted attentive sum of the word annotations. The relevance vector can be interpreted as representing the contextually most relevant word over all words in the sentence. u w is fixed over all inputs as a global parameter of our model and jointly learned in the training process.</p><formula xml:id="formula_7">u ij = tanh W w h w ij + b w<label>(7)</label></formula><formula xml:id="formula_8">α ij = exp u T ij u w j exp u T ij u w , s i = j α ij h w ij (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>Sentence Encoder. Similar to the word encoder, a bidirectional GRU is applied to (s 1 , s 2 , . . . , s L ) to compute the forward annotations -→ h s i and backward annotations ←h s i for each sentence. These annotations capture the coherence of a sentence with respect to its neighbouring sentences in both directions of the body.</p><formula xml:id="formula_10">h s i is formed as -→ h s i , ← - h s i</formula><p>.</p><p>Sentence Attention. Similar to word attention, we identify relevant sentences in the formation of the body vector v b by using an attention layer. A sentence level relevance vector u s decides attention weights α i for sentence annotation h s i . u s can be interpreted as representing the coherently most relevant sentence over all sentences in the body. v b is composed using i α i h s i .</p><p>Headline Encoder. To exploit our headline premise we design a third layer of encoding and attention with the headline being inputted word by word. We denote the k words of the headline by w 01 to w 0k . The word embedding y i for word w 0i is obtained using GloVe embeddings (W e ) by y i = W e (w 0i ). We denote v b as y k+1 . A bidirectional GRU is run on (y 1 , y 2 , . . . , y k+1 ) to compute the forward and backward annotations of each word. These annotations capture the stance of the headline words with respect to the body word. The digit 3 in our notation denotes the third level.</p><formula xml:id="formula_11">h 3 i is formed as -→ h 3 i , ← - h 3 i .</formula><p>-→ h</p><formula xml:id="formula_12">3 i = ---→ GRU (y j ) , j ∈ [1, i] , ← - h 3 i = ← --- GRU (y j ) , j ∈ [k + 1, i]<label>(9)</label></formula><p>Headline Attention. A relevance vector u 3 is used to compute the attention weights β i for annotation h 3 i . The news vector v n is formed as the weighted sum of the annotations h 3 i with β i as the weights.</p><formula xml:id="formula_13">u i = tanh W 3 h 3 i + b 3 (<label>10</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">β i = exp u T i u 3 i exp u T i u 3 , v n = i β i h 3 i (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>News Vector for Classification. We use the news vector v n as a feature vector for classification. We use the sigmoid layer z = sigmoid (W c v n + b c ) as our classifier with binary cross-entropy loss L =d p d log q d to train 3HAN. In the loss function q d is the predicted probability and p d is the ground truth label (either fake or genuine) of article d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Pre-training using Headlines</head><p>We propose a supervised pretraining of Layer 1 consisting of the word encoder and an attention layer of 3HAN for a better initialization of the model. The pre-training is performed using the headlines only. The output label for a headline input is the corresponding article label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">News Data Set</head><p>Due to the high turnaround time of manual fact-checking, the number of available manually fact-checked articles is too few to train deep neural models. We shift our fact-checked requirement from an article level to a website level. Keeping with our definition of fake news, we assume that every article from a website shares the same label (fake or genuine) as its containing website. PolitiFact <ref type="bibr" target="#b13">[14]</ref> a respected fact-checking website released a list of sites manually investigated and labelled. We use those sites from this list labelled fake. Forbes <ref type="bibr" target="#b14">[15]</ref> compiled a list of popular genuine sites across US demographics. Statistics of our data set is provided in Table <ref type="table" target="#tab_0">1</ref>. To maintain a similar distribution as fake articles, we use genuine articles from January 1, 2016 to June 1, 2017, with 65% coming from the 2016 US elections and politics, 15% from world news, 15% from regional news and 5% from entertainment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>To validate the effectiveness of our model, we compare 3HAN with current stateof-the-art traditional and deep learning models. The input is the article text formed by concatenating the headline with the body.</p><p>Word Count Based Models. These methods use a hand crafted feature vector derived from variations of frequency of words of an article. A binomial logistic regression is used as the classifier.</p><p>1. Majority uses the heuristic of taking the majority label in the training set as the assigning label to every point in the test set. 2. Bag-of-words and its TF-IDF constructs a vocabulary of the most frequent 50,000 words <ref type="bibr" target="#b4">[5]</ref>. The count of these words is used as features. The TF-IDF count is used as features in the other model variant. 3. Bag-of-ngrams and its TF-IDF uses the count of the 50,000 most frequent ngrams (n &lt;= 5). The features are formed as in the previous model. 4. SVM+Bigrams uses the count of the 50,000 most frequent bigrams as features with an SVM classifier <ref type="bibr" target="#b5">[6]</ref>.</p><p>Neural Models. The classifier used is a dense sigmoid layer.</p><p>1. GloVe-Ave flattens the article text to a word level granularity as a sequence of words. The GloVe embeddings of all words are averaged to form the feature vector. 2. GRU treats the article text as a sequence of words. A GRU with an annotation dimension of 300 is run on the sequence of GloVe word embeddings. The hidden annotation after the last time step is used as the feature vector. 3. GRU-Ave runs a GRU on the sequence of word embeddings and returns all hidden annotations at each time step. The average of these hidden annotations is used as the feature vector. 4. HAN and Variants include HAN-Ave, Han-Max and HAN <ref type="bibr" target="#b9">[10]</ref>. HAN uses a two level hierarchical attention network. HAN-Ave and Han-Max replaces the attention mechanism with average and max pooling for composition respectively. Since the code is not officially released we use our own implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Settings</head><p>We split sentences of bodies and tokenized sentences and headlines into words using Stanford CoreNLP <ref type="bibr" target="#b15">[16]</ref>. We lower cased and cleaned tokens by retaining alphabets, numerals and significant punctuation marks. When building the vocabulary we retained words with frequency more than 5. We treat words appearing exactly 5 times as a special single unknown token (UNK). We used 100 dimensional GloVe embeddings to initialize our word embedding matrix and allowed it to be fine tuned. For missing words in GloVe, we initialized their word embedding from a uniform distribution on (-0.25, 0.25) <ref type="bibr" target="#b16">[17]</ref>.</p><p>We padded (or truncated) each sentence and headline to an average word count of 32 and each article to an average sentence count of 21. Hyper parameters are tuned on the validation set. We used 100 dimensional GloVe embeddings and 50 dimensional GRU annotations giving a combined annotation of 100 dimensions. The relevance vector at word, sentence and headline-body level are of 100 dimensions trained as a parameter of our model. We used SGD with a learning rate of 0.01, momentum of 0.9 and mini batch size of 32 to train all neural models. Accuracy was our evaluation metric since our data set is balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results and Analysis</head><p>We used a train, validation and test split of 20% | 10% | 70% for neural models and a train and test split of 30% | 70% for word count based models. In 3HAN-Ave vectors are composed using average, in 3HAN-Max vectors are composed using max pooling, 3HAN is our proposed model with an attention mechanism for composition and 3HAN+PT denotes our pre-trained 3HAN model. Results are reported in Table <ref type="table" target="#tab_1">2</ref> and demonstrate the effectiveness of 3HAN and 3HAN+PT due to their best performance over all models.</p><p>Neural models using the hierarchical structure (HAN and variants, 3HAN and variants) give a higher accuracy than other baselines. The attention mechanism is a more effective composition operator than average or max pooling. This is demonstrated by the higher accuracy of 3HAN against 3HAN-Ave and 3HAN-Max. Our headline premise is valid since 3HAN which devotes a separate third level in the hierarchy for the headline performs better than HAN. HAN is indifferent to the headline and focuses its two hierarchical levels only on words and sentences. Pre-training helps in better initialization of 3HAN with 3HAN+PT outperforming 3HAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Insights</head><p>The visualization of attention layers provides evidence. An advantage of attention based neural models is the visualization of attention layers which provides insight into the internal classification process. On the other hand, nonattention based models work like a black box. 3HAN provides attention weights to words, sentences and headline of an article. These attention weights are useful for further human fact-checking. A human fact-checker can focus on verifying sentences with high attention weights. Similarly, words with high attention weights can be investigated for inaccuracies. We visualize the attention weights given to words, sentences and the headline for a sample article through a heatmap in Fig. <ref type="figure">2</ref>. The sentences with the top five attention weights and the first eight words in each sentence are shown for clarity. Word attention weights α w are normalized using sentence attention weights α s by α w = √ α s α w . Sentence attention weights are shown on the extreme left edge. We observe that sentence 5 and has been assigned the highest weight (0.287). Interestingly, sentence 5 which states "Even refugee welcoming Canada levies a 12 percent penalty on immigrant money" is a factually incorrect sentence.</p><p>Word count based models perform well. The high accuracy of simple word count based models which do not take into account word ordering or semantics is an indication of vocabulary and patterns of word usage from the vocabulary being a distinguishing feature between fake news and true news.</p><p>The attention mechanism is effective. This is observed through the superior performance of HAN compared to non-attention based 3HAN-Max and 3HAN-Ave.</p><p>Our headline premise is valid. This is observed from the superior perfor- mance of 3HAN to HAN with the third hierarchical level of 3HAN especially designed for our headline premise playing a role.</p><p>The inverted pyramid style of writing is used. Inverted pyramid refers to distributing information in decreasing importance in an article. We inferred the usage of the inverted pyramid through our experiments from the small improvement in accuracy even with higher padding sentence counts. Fake news articles tend to be repetitive in information content <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we presented 3HAN which creates news vector, an effective representation of an article for detection as fake news. We demonstrated the superior accuracy of 3HAN over other state-of-the-art models. We highlighted the use of visualization of the attention layers. We plan to deploy a web application based on 3HAN which provides detection of fake news as a service and learns in a real time online manner from new manually fact-checked articles.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset Statistics: (average words per sentence, average sentences per article)</figDesc><table><row><cell cols="3">Type Sites Articles</cell><cell>Average Words</cell><cell>Average Sentences</cell></row><row><cell>Fake</cell><cell>19</cell><cell>20,372</cell><cell>34.20</cell><cell>16.44</cell></row><row><cell cols="2">Genuine 9</cell><cell>20,932</cell><cell>32.78</cell><cell>27.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Accuracy in Article Classification as Fake or Genuine</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Neural Network Models</cell></row><row><cell cols="2">Word Count Based Models</cell><cell>Model</cell><cell>Accuracy</cell></row><row><cell>Model</cell><cell>Accuracy</cell><cell>GloVe-Ave</cell><cell>93.63%</cell></row><row><cell></cell><cell></cell><cell>GRU</cell><cell>91.11%</cell></row><row><cell>Majority</cell><cell>49.42%</cell><cell>GRU-Ave</cell><cell>95.65%</cell></row><row><cell>Bag-of-words</cell><cell>90.21%</cell><cell>HAN-Ave</cell><cell>94.91%</cell></row><row><cell cols="2">Bag-of-words +TFIDF Bag-of-ngrams 91.41% 91.92%</cell><cell>HAN-Max HAN</cell><cell>94.66% 95.4%</cell></row><row><cell>Bag-of-ngrams +TFIDF</cell><cell>92.47%</cell><cell>3HAN-Ave</cell><cell>94.81%</cell></row><row><cell cols="2">SVM+Bigrams 83.12%</cell><cell cols="2">3HAN-Max 95.25%</cell></row><row><cell></cell><cell></cell><cell>3HAN</cell><cell>96.24%</cell></row><row><cell></cell><cell></cell><cell cols="2">3HAN+PT 96.77%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Visualization of Attention Layers in a Fake News Article with Headline "Trump Defies Left with Brilliant Move -You Will Cheer"</figDesc><table><row><cell></cell><cell cols="2">trump defies</cell><cell>left</cell><cell>with</cell><cell cols="2">brilliant move</cell><cell cols="2">you</cell><cell>will</cell><cell>cheer</cell></row><row><cell>0.12</cell><cell>we</cell><cell>live</cell><cell>in</cell><cell></cell><cell>a</cell><cell>truly</cell><cell>orwellian</cell><cell cols="2">world</cell><cell>or</cell></row><row><cell>0.138</cell><cell>either</cell><cell>way</cell><cell>,</cell><cell></cell><cell>when</cell><cell>a</cell><cell>government</cell><cell cols="2">finds</cell><cell>itself</cell></row><row><cell>0.143</cell><cell>the</cell><cell>bill</cell><cell>,</cell><cell></cell><cell>if</cell><cell>it</cell><cell>becomes</cell><cell>law</cell><cell>,</cell></row><row><cell>0.148</cell><cell>,</cell><cell>before</cell><cell cols="2">any</cell><cell>liberal</cell><cell>reading</cell><cell>this</cell><cell cols="2">decides</cell><cell>to</cell></row><row><cell>0.287</cell><cell>even</cell><cell cols="4">refugee welcoming canada</cell><cell>levies</cell><cell>a</cell><cell>12</cell><cell>percent</cell></row><row><cell cols="2">Fig. 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank the anonymous ICONIP reviewers as well as G. Srinivasaraghavan, Shreyak Upadhyay and Rishabh Manoj for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">As fake news spreads lies, more readers shrug at the truth</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tavernisen</surname></persName>
		</author>
		<ptr target="http://nyti.ms/2lw56HN" />
	</analytic>
	<monogr>
		<title level="j">New York Times</title>
		<imprint>
			<date type="published" when="2016-12-06">December 6, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Identification and verification of simple claims about statistical properties</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d151312</idno>
	</analytic>
	<monogr>
		<title level="m">20th Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015-09">2015. September 2015</date>
			<biblScope unit="page" from="2596" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spread of (mis) information in social networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parandehgheibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games and Economic Behavior</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="227" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting hoaxes, frauds, and deception in writing style online</title>
		<author>
			<persName><forename type="first">S</forename><surname>Afroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greenstadt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">rd IEEE Symposium on Security and Privacy (SP 2012)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-05">May 2012</date>
			<biblScope unit="page" from="461" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th European Conference on Machine Learning</title>
		<imprint>
			<publisher>ECML</publisher>
			<date type="published" when="1998-04">1998. April 1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">50th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012-07">2012. July 2012</date>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015-09">2015. September 2015</date>
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sense and reference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Frege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Philosophical Review</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015-05">2015. May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT</title>
		<imprint>
			<date type="published" when="2016-06">2016. June 2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">This just in: Fake news packs a lot in title, uses simpler, repetitive content in text body, more similar to satire than real news</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of the 11th International AAAI Conference on Web and Social Media</title>
		<imprint>
			<date type="published" when="2017-05">2017. May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014-10">2014. October 2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014-10">2014. October 2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Politifact&apos;s guide to fake news websites and what they peddle</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gillin</surname></persName>
		</author>
		<ptr target="http://bit.ly/2pHYKDV" />
	</analytic>
	<monogr>
		<title level="j">Pundit-Fact</title>
		<imprint>
			<date type="published" when="2017-04-20">April 20, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">10 journalism brands where you find real facts rather than alternative facts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Glader</surname></persName>
		</author>
		<ptr target="http://bit.ly/2sXPpvf" />
	</analytic>
	<monogr>
		<title level="j">Forbes</title>
		<imprint>
			<date type="published" when="2017-02-01">February 1, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014-06">2014. June 2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014-10">2014. October 2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
