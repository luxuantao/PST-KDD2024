<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ENSEMBLE ADVERSARIAL TRAINING: ATTACKS AND DEFENSES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
							<email>tramer@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
							<email>goodfellow@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
							<email>mcdaniel@cse.psu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ENSEMBLE ADVERSARIAL TRAINING: ATTACKS AND DEFENSES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks <ref type="bibr" target="#b19">(Kurakin et al., 2017c)</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine learning (ML) models are often vulnerable to adversarial examples, maliciously perturbed inputs designed to mislead a model at test time <ref type="bibr" target="#b3">(Biggio et al., 2013;</ref><ref type="bibr" target="#b34">Szegedy et al., 2013;</ref><ref type="bibr" target="#b14">Goodfellow et al., 2014b;</ref><ref type="bibr" target="#b27">Papernot et al., 2016a)</ref>. Furthermore, <ref type="bibr" target="#b34">Szegedy et al. (2013)</ref> showed that these inputs transfer across models: the same adversarial example is often misclassified by different models, thus enabling simple black-box attacks on deployed models <ref type="bibr" target="#b30">(Papernot et al., 2017;</ref><ref type="bibr" target="#b21">Liu et al., 2017)</ref>.</p><p>Adversarial training <ref type="bibr" target="#b34">(Szegedy et al., 2013)</ref> increases robustness by augmenting training data with adversarial examples. <ref type="bibr" target="#b23">Madry et al. (2017)</ref> showed that adversarially trained models can be made robust to white-box attacks (i.e., with knowledge of the model parameters) if the perturbations computed during training closely maximize the model's loss. However, prior attempts at scaling this approach to ImageNet-scale tasks <ref type="bibr" target="#b11">(Deng et al., 2009)</ref> have proven unsuccessful <ref type="bibr" target="#b19">(Kurakin et al., 2017b)</ref>.</p><p>It is thus natural to ask whether it is possible, at scale, to achieve robustness against the class of black-box adversaries Towards this goal, <ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> adversarially trained an Inception v3 model <ref type="bibr" target="#b36">(Szegedy et al., 2016b)</ref> on ImageNet using a "single-step" attack based on a linearization of the model's loss <ref type="bibr" target="#b14">(Goodfellow et al., 2014b)</ref>. Their trained model is robust to single-step perturbations but remains vulnerable to more costly "multi-step" attacks. Yet, <ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> found that these attacks fail to reliably transfer between models, and thus concluded that the robustness of their model should extend to black-box adversaries. Surprisingly, we show that this is not the case.</p><p>We demonstrate, formally and empirically, that adversarial training with single-step methods admits a degenerate global minimum, wherein the model's loss can not be reliably approximated by a linear function. Specifically, we find that the model's decision surface exhibits sharp curvature near the data points, thus degrading attacks based on a single gradient computation. In addition to the model of <ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref>, we reveal similar overfitting in an adversarially trained Inception ResNet v2 model <ref type="bibr" target="#b35">(Szegedy et al., 2016a)</ref>, and a variety of models trained on <ref type="bibr">MNIST (LeCun et al., 1998)</ref>.</p><p>We harness this result in two ways. First, we show that adversarially trained models using single-step methods remain vulnerable to simple attacks. For black-box adversaries, we find that perturbations crafted on an undefended model often transfer to an adversarially trained one. We also introduce a simple yet powerful single-step attack that applies a small random perturbation-to escape the nonsmooth vicinity of the data point-before linearizing the model's loss. While seemingly weaker than the Fast Gradient Sign Method of <ref type="bibr" target="#b14">Goodfellow et al. (2014b)</ref>, our attack significantly outperforms it for a same perturbation norm, for models trained with or without adversarial training.</p><p>Second, we propose Ensemble Adversarial Training, a training methodology that incorporates perturbed inputs transferred from other pre-trained models. Our approach decouples adversarial example generation from the parameters of the trained model, and increases the diversity of perturbations seen during training. We train Inception v3 and Inception ResNet v2 models on ImageNet that exhibit increased robustness to adversarial examples transferred from other holdout models, using various single-step and multi-step attacks <ref type="bibr" target="#b14">(Goodfellow et al., 2014b;</ref><ref type="bibr" target="#b6">Carlini &amp; Wagner, 2017a;</ref><ref type="bibr" target="#b18">Kurakin et al., 2017a;</ref><ref type="bibr" target="#b23">Madry et al., 2017)</ref>. We also show that our methods globally reduce the dimensionality of the space of adversarial examples <ref type="bibr" target="#b38">(Tramèr et al., 2017)</ref>. Our Inception ResNet v2 model won the first round of the NIPS 2017 competition on Defenses Against Adversarial Attacks <ref type="bibr" target="#b19">(Kurakin et al., 2017c)</ref>, where it was evaluated on other competitors' attacks in a black-box setting.<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Various defensive techniques against adversarial examples in deep neural networks have been proposed <ref type="bibr" target="#b15">(Gu &amp; Rigazio, 2014;</ref><ref type="bibr" target="#b22">Luo et al., 2015;</ref><ref type="bibr" target="#b29">Papernot et al., 2016c;</ref><ref type="bibr" target="#b26">Nayebi &amp; Ganguli, 2017;</ref><ref type="bibr" target="#b9">Cisse et al., 2017)</ref> and many remain vulnerable to adaptive attackers <ref type="bibr" target="#b6">(Carlini &amp; Wagner, 2017a;</ref><ref type="bibr">b;</ref><ref type="bibr">Baluja &amp; Fischer, 2017)</ref>. Adversarial training <ref type="bibr" target="#b34">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b14">Goodfellow et al., 2014b;</ref><ref type="bibr" target="#b19">Kurakin et al., 2017b;</ref><ref type="bibr" target="#b23">Madry et al., 2017)</ref> appears to hold the greatest promise for learning robust models. <ref type="bibr" target="#b23">Madry et al. (2017)</ref> show that adversarial training on MNIST yields models that are robust to whitebox attacks, if the adversarial examples used in training closely maximize the model's loss. Moreover, recent works by <ref type="bibr" target="#b32">Sinha et al. (2018)</ref>, <ref type="bibr" target="#b31">Raghunathan et al. (2018)</ref> and <ref type="bibr" target="#b17">Kolter &amp; Wong (2017)</ref> even succeed in providing certifiable robustness for small perturbations on MNIST. As we argue in Appendix C, the MNIST dataset is peculiar in that there exists a simple "closed-form" denoising procedure (namely feature binarization) which leads to similarly robust models without adversarial training. This may explain why robustness to white-box attacks is hard to scale to tasks such as Im-ageNet <ref type="bibr" target="#b19">(Kurakin et al., 2017b)</ref>. We believe that the existence of a simple robust baseline for MNIST can be useful for understanding some limitations of adversarial training techniques. <ref type="bibr" target="#b34">Szegedy et al. (2013)</ref> found that adversarial examples transfer between models, thus enabling blackbox attacks on deployed models. <ref type="bibr" target="#b30">Papernot et al. (2017)</ref> showed that black-box attacks could succeed with no access to training data, by exploiting the target model's predictions to extract <ref type="bibr" target="#b37">(Tramèr et al., 2016)</ref> a surrogate model. Some prior works have hinted that adversarially trained models may remain vulnerable to black-box attacks: <ref type="bibr" target="#b14">Goodfellow et al. (2014b)</ref> found that an adversarial maxout network on MNIST has slightly higher error on transferred examples than on white-box examples. <ref type="bibr" target="#b30">Papernot et al. (2017)</ref> further showed that a model trained on small perturbations can be evaded by transferring perturbations of larger magnitude. Our finding that adversarial training degrades the accuracy of linear approximations of the model's loss is as an instance of a gradient-masking phenomenon <ref type="bibr" target="#b28">(Papernot et al., 2016b)</ref>, which affects other defensive techniques <ref type="bibr" target="#b29">(Papernot et al., 2016c;</ref><ref type="bibr" target="#b6">Carlini &amp; Wagner, 2017a;</ref><ref type="bibr" target="#b26">Nayebi &amp; Ganguli, 2017;</ref><ref type="bibr" target="#b4">Brendel &amp; Bethge, 2017;</ref><ref type="bibr" target="#b2">Athalye et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE ADVERSARIAL TRAINING FRAMEWORK</head><p>We consider a classification task with data x ∈ [0, 1] d and labels y true ∈ Z k sampled from a distribution D. We identify a model with an hypothesis h from a space H. On input x, the model outputs class scores h(x) ∈ R k . The loss function used to train the model, e.g., cross-entropy, is L(h(x), y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">THREAT MODEL</head><p>For some target model h ∈ H and inputs (x, y true ) the adversary's goal is to find an adversarial example x adv such that x adv and x are "close" yet the model misclassifies x adv . We consider the wellstudied class of ∞ bounded adversaries <ref type="bibr" target="#b14">(Goodfellow et al., 2014b;</ref><ref type="bibr" target="#b23">Madry et al., 2017)</ref> that, given some budget , output examples x adv where x adv − x ∞ ≤ . As we comment in Appendix C.1, ∞ robustness is of course not an end-goal for secure ML. We use this standard model to showcase limitations of prior adversarial training methods, and evaluate our proposed improvements.</p><p>We distinguish between white-box adversaries that have access to the target model's parameters (i.e., h), and black-box adversaries with only partial information about the model's inner workings. Formal definitions for these adversaries are in Appendix A. Although security against white-box attacks is the stronger notion (and the one we ideally want ML models to achieve), black-box security is a reasonable and more tractable goal for deployed ML models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ADVERSARIAL TRAINING</head><p>Following <ref type="bibr" target="#b23">Madry et al. (2017)</ref>, we consider an adversarial variant of standard Empirical Risk Minimization (ERM), where our aim is to minimize the risk over adversarial examples:  <ref type="bibr" target="#b34">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b14">Goodfellow et al., 2014b)</ref>, which we use in our experiments, trains on both the "clean" examples x and adversarial examples x adv .</p><formula xml:id="formula_0">h * = arg min h∈H E (x,y true )∼D max x adv −x ∞ ≤ L(h(x adv ), y true ) .<label>(1)</label></formula><p>We consider three algorithms to generate adversarial examples with bounded ∞ norm. The first two are single-step (i.e., they require a single gradient computation); the third is iterative-it computes multiple gradient updates. We enforce x adv ∈ [0, 1] d by clipping all components of x adv .</p><p>Fast Gradient Sign Method (FGSM). This method <ref type="bibr" target="#b14">(Goodfellow et al., 2014b)</ref> linearizes the inner maximization problem in (1):</p><formula xml:id="formula_1">x adv FGSM := x + ε • sign (∇ x L(h(x), y true )) .<label>(2)</label></formula><p>Single-Step Least-Likely Class Method (Step-LL). This variant of FGSM introduced by <ref type="bibr" target="#b18">Kurakin et al. (2017a;</ref><ref type="bibr">b)</ref> targets the least-likely class, y LL = arg min{h(x)}:</p><formula xml:id="formula_2">x adv LL := x − ε • sign (∇ x L(h(x), y LL )) .<label>(3)</label></formula><p>Although this attack only indirectly tackles the inner maximization in (1), <ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> find it to be the most effective for adversarial training on ImageNet.</p><p>Iterative Attack (I-FGSM or Iter-LL). This method iteratively applies the FGSM or Step-LL k times with step-size α ≥ /k and projects each step onto the ∞ ball of norm around x. It uses projected gradient descent to solve the maximization in (1). For fixed , iterative attacks induce higher error rates than single-step attacks, but transfer at lower rates <ref type="bibr" target="#b18">(Kurakin et al., 2017a;</ref><ref type="bibr">b)</ref>. • For an input x from D, there is no x adv close to x (in ∞ norm) that induces a high loss. That is,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A DEGENERATE GLOBAL MINIMUM</head><formula xml:id="formula_3">L(h * (x adv FGSM ), y true ) ≈ max x adv −x ∞ ≤ L(h * (x adv ), y true )] ≈ 0 .<label>(5)</label></formula><p>In other words, h * is robust to all ∞ bounded perturbations. • The minimizer h * is a model for which the approximation method underlying the attack (i.e., linearization in our case) poorly fits the model's loss function. That is,</p><formula xml:id="formula_4">L(h * (x adv FGSM ), y true ) max x adv −x ∞ ≤ L(h * (x adv ), y true )] .<label>(6)</label></formula><p>Thus the attack when applied to h * produces samples x adv that are far from optimal.</p><p>Note that this second "degenerate" minimum can be more subtle than a simple case of overfitting to samples produced from single-step attacks. Indeed, we show in Section 4.1 that single-step attacks applied to adversarially trained models create "adversarial" examples that are easy to classify even for undefended models. Thus, adversarial training does not simply learn to resist the particular attack used during training, but actually to make that attack perform worse overall. This phenomenon relates to the notion of Reward Hacking <ref type="bibr" target="#b1">(Amodei et al., 2016)</ref> wherein an agent maximizes its formal objective function via unintended behavior that fails to captures the designer's true intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ENSEMBLE ADVERSARIAL TRAINING</head><p>The degenerate minimum described in Section 3.3 is attainable because the learned model's parameters influence the quality of both the minimization and maximization in (1). One solution is to use a stronger adversarial example generation process, at a high performance cost <ref type="bibr" target="#b23">(Madry et al., 2017)</ref>. Alternatively, <ref type="bibr">Baluja &amp; Fischer (2017)</ref> suggest training an adversarial generator model as in the GAN framework <ref type="bibr" target="#b13">(Goodfellow et al., 2014a)</ref>. The power of this generator is likely to require careful tuning, to avoid similar degenerate minima (where the generator or classifier overpowers the other).</p><p>We propose a conceptually simpler approach to decouple the generation of adversarial examples from the model being trained, while simultaneously drawing an explicit connection with robustness to black-box adversaries.  <ref type="bibr" target="#b24">(Mansour et al., 2009;</ref><ref type="bibr" target="#b41">Zhang et al., 2012)</ref>.</p><p>In Domain Adaptation, a model trained on data sampled from one or more source distributions S 1 , . . . , S k is evaluated on samples x from a different target distribution T .</p><p>Let A i be an adversarial distribution obtained by sampling (x, y true ) from D, computing an adversarial example x adv for some model such that x adv − x ∞ ≤ , and outputting (x adv , y true ). In Ensemble Adversarial Training, the source distributions are D (the clean data) and A 1 , . . . , A k (the attacks overs the currently trained model and the static pre-trained models). The target distribution takes the form of an unseen black-box adversary A * . Standard generalization bounds for Domain Adaptation <ref type="bibr" target="#b24">(Mansour et al., 2009;</ref><ref type="bibr" target="#b41">Zhang et al., 2012)</ref>   Figure <ref type="figure">1</ref>: Gradient masking in single-step adversarial training. We plot the loss of model v3 adv on points</p><formula xml:id="formula_5">x * = x + 1 • g + 2 • g ⊥ ,</formula><p>where g is the signed gradient and g ⊥ is an orthogonal adversarial direction. Plot (b) is a zoom of (a) near x. The gradient poorly approximates the global loss.</p><p>We give a formal statement of this result and of the assumptions on A * in Appendix B. Of course, ideally we would like guarantees against arbitrary future adversaries. For very low-dimensional tasks (e.g., MNIST), stronger guarantees are within reach for specific classes of adversaries (e.g., ∞ bounded perturbations <ref type="bibr" target="#b23">(Madry et al., 2017;</ref><ref type="bibr" target="#b32">Sinha et al., 2018;</ref><ref type="bibr" target="#b31">Raghunathan et al., 2018;</ref><ref type="bibr" target="#b17">Kolter &amp; Wong, 2017)</ref>), yet they also fail to extend to other adversaries not considered at training time (see Appendix C.1 for a discussion). For ImageNet-scale tasks, stronger formal guarantees appear out of reach, and we thus resort to an experimental assessment of the robustness of Ensemble Adversarially Trained models to various non-interactive black-box adversaries in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We show the existence of a degenerate minimum, as described in Section 3.3, for the adversarially trained Inception v3 model of <ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref>. Their model (denoted v3 adv ) was trained on a Step-LL attack with ≤ 16/256. We also adversarially train an Inception ResNet v2 model <ref type="bibr" target="#b35">(Szegedy et al., 2016a)</ref> using the same setup. We denote this model by IRv2 adv . We refer the reader to <ref type="bibr" target="#b19">(Kurakin et al., 2017b)</ref> for details on the adversarial training procedure.</p><p>We first measure the approximation-ratio of the Step-LL attack for the inner maximization in (1). As we do not know the true maximum, we lower-bound it using an iterative attack. For 1,000 random test points, we find that for a standard Inception v3 model, step-LL gets within 19% of the optimum loss on average. This attack is thus a good candidate for adversarial training. Yet, for the v3 adv model, the approximation ratio drops to 7%, confirming that the learned model is less amenable to linearization. We obtain similar results for Inception ResNet v2 models. The ratio is 17% for a standard model, and 8% for IRv2 adv . Similarly, we look at the cosine similarity between the perturbations given by a single-step and multi-step attack. The more linear the model, the more similar we expect both perturbations to be. The average similarity drops from 0.13 for Inception v3 to 0.02 for v3 adv . This effect is not due to the decision surface of v3 adv being "too flat" near the data points: the average gradient norm is larger for v3 adv (0.17) than for the standard v3 model (0.10).</p><p>We visualize this "gradient-masking" effect <ref type="bibr" target="#b28">(Papernot et al., 2016b)</ref> by plotting the loss of v3 adv on examples</p><formula xml:id="formula_6">x * = x + 1 • g + 2 • g ⊥ ,</formula><p>where g is the signed gradient of model v3 adv and g ⊥ is a signed vector orthogonal to g. Looking forward to Section 4.1, we actually chose g ⊥ to be the signed gradient of another Inception model, from which adversarial examples transfer to v3 adv . Figure <ref type="figure">1</ref> shows that the loss is highly curved in the vicinity of the data point x, and that the gradient poorly reflects the global loss landscape. Similar plots for additional data points are in Figure <ref type="figure" target="#fig_6">4</ref>.</p><p>We show similar results for adversarially trained MNIST models in Appendix C.2. On this task, input dropout <ref type="bibr" target="#b33">(Srivastava et al., 2014)</ref> mitigates adversarial training's overfitting problem, in some cases. Presumably, the random input mask diversifies the perturbations seen during training (dropout at intermediate layers does not mitigate the overfitting effect). <ref type="bibr" target="#b25">Mishkin et al. (2017)</ref> find that input dropout significantly degrades accuracy on ImageNet, so we did not include it in our experiments. 13.6 13.5 9.0 13.0 14.5 IRv2 14.1 14.8 9.9 24.0 10.6 IRv2 adv 10.3 10.5 7.7 10.4 5.8</p><p>Top 5</p><p>4.1 ATTACKS AGAINST ADVERSARIALLY TRAINED NETWORKS <ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> found their adversarially trained model to be robust to various single-step attacks. They conclude that this robustness should translate to attacks transferred from other models.</p><p>As we have shown, the robustness to single-step attacks is actually misleading, as the model has learned to degrade the information contained in the model's gradient. As a consequence, we find that the v3 adv model is substantially more vulnerable to single-step attacks than <ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> predicted, both in a white-box and black-box setting. The same holds for the IRv2 adv model.</p><p>In addition to the v3 adv and IRv2 adv models, we consider standard Inception v3, Inception v4 and Inception ResNet v2 models. These models are available in the TensorFlow-Slim library <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>. We describe similar results for a variety of models trained on MNIST in Appendix C.2.</p><p>Black-box attacks. Table <ref type="table" target="#tab_4">1</ref> shows error rates for single-step attacks transferred between models.</p><p>We compute perturbations on one model (the source) and transfer them to all others (the targets).</p><p>When the source and target are the same, the attack is white-box. Adversarial training greatly increases robustness to white-box single-step attacks, but incurs a higher error rate in a black-box setting. Thus, the robustness gain observed when evaluating defended models in isolation is misleading. Given the ubiquity of this pitfall among proposed defenses against adversarial examples <ref type="bibr" target="#b6">(Carlini &amp; Wagner, 2017a;</ref><ref type="bibr" target="#b4">Brendel &amp; Bethge, 2017;</ref><ref type="bibr" target="#b28">Papernot et al., 2016b)</ref>, we advise researchers to always consider both white-box and black-box adversaries when evaluating defensive strategies. Notably, a similar discrepancy between white-box and black-box attacks was recently observed in <ref type="bibr" target="#b5">Buckman et al. (2018)</ref>.</p><p>Attacks crafted on adversarial models are found to be weaker even against undefended models (i.e., when using v3 adv or IRv2 adv as source, the attack transfers with lower probability). This confirms our intuition from Section 3.3: adversarial training does not just overfit to perturbations that affect standard models, but actively degrades the linear approximation underlying the single-step attack.</p><p>A new randomized single-step attack. The loss function visualization in Figure <ref type="figure">1</ref> shows that sharp curvature artifacts localized near the data points can mask the true direction of steepest ascent. We thus suggest to prepend single-step attacks by a small random step, in order to "escape" the non-smooth vicinity of the data point before linearizing the model's loss. Our new attack, called R+FGSM (alternatively, R+Step-LL), is defined as follows, for parameters and α (where α &lt; ):</p><formula xml:id="formula_7">x adv = x + (ε − α) • sign ∇ x J(x , y true ) , where x = x + α • sign(N (0 d , I d )) . (7)</formula><p>Note that the attack requires a single gradient computation. The R+FGSM is a computationally efficient alternative to iterative methods that have high success rates in a white-box setting. Our attack can be seen as a single-step variant of the general PGD method from <ref type="bibr" target="#b23">(Madry et al., 2017)</ref>. We find that the addition of this random step hinders transferability (see Table <ref type="table" target="#tab_14">9</ref>). We also tried adversarial training using R+FGSM on MNIST, using a similar approach as <ref type="bibr" target="#b23">(Madry et al., 2017)</ref>. We adversarially train a CNN (model A in Table <ref type="table" target="#tab_10">5</ref>) for 100 epochs, and attain &gt; 90.0% accuracy on R+FGSM samples. However, training on R+FGSM provides only little robustness to iterative attacks. For the PGD attack of <ref type="bibr" target="#b23">(Madry et al., 2017)</ref> with 20 steps, the model attains 18.0% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ENSEMBLE ADVERSARIAL TRAINING</head><p>We now evaluate our Ensemble Adversarial Training strategy described in Section 3.4. We recall our intuition: by augmenting training data with adversarial examples crafted from static pre-trained models, we decouple the generation of adversarial examples from the model being trained, so as to avoid the degenerate minimum described in Section 3.3. Moreover, our hope is that robustness to attacks transferred from some fixed set of models will generalize to other black-box adversaries.</p><p>We train Inception v3 and Inception ResNet v2 models <ref type="bibr" target="#b35">(Szegedy et al., 2016a)</ref> on ImageNet, using the pre-trained models shown in Table <ref type="table" target="#tab_7">3</ref>  Black-box attacks. Ensemble Adversarial Training significantly boosts robustness to all attacks transferred from the holdout models. For the IRv2 adv-ens model, the accuracy loss (compared to IRv2's accuracy on clean data) is 7.4% (top 1) and 3.1% (top 5). We find that the strongest attacks in our test suite (i.e., with highest transfer rates) are the FGSM attacks. Black-box R+Step-LL or iterative attacks are less effective, as they do not transfer with high probability (see <ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> and Table <ref type="table" target="#tab_14">9</ref>). Attacking an ensemble of all three holdout models, as in <ref type="bibr" target="#b21">Liu et al. (2017)</ref>, did not lead to stronger black-box attacks than when attacking the holdout models individually.</p><p>Our results have little variance with respect to the attack parameters (e.g., smaller ) or to the use of other holdout models for black-box attacks (e.g., we obtain similar results by attacking the v3 adv-ens3 and v3 adv-ens4 models with the IRv2 model). We also find that v3 adv-ens3 is not vulnerable to perturbations transferred from v3 adv-ens4 . We obtain similar results on MNIST (see Appendix C.2), thus demonstrating the applicability of our approach to different datasets and model architectures.</p><p>The NIPS 2017 competition on adversarial examples. Our Inception ResNet v2 model was included as a baseline defense in the NIPS 2017 competition on Adversarial Examples <ref type="bibr" target="#b19">(Kurakin et al., 2017c)</ref>. Participants of the attack track submitted non-interactive black-box attacks that produce adversarial examples with bounded ∞ norm. Models submitted to the defense track were evaluated on all attacks over a subset of the ImageNet test set. The score of a defense was defined as the average accuracy of the model over all adversarial examples produced by all attacks.</p><p>Our IRv2 adv-ens model finished 1 st among 70 submissions in the first development round, with a score of 95.3% (the second placed defense scored 89.9%). The test data was intentionally chosen as an "easy" subset of ImageNet. Our model achieved 97.9% accuracy on the clean test data.</p><p>After the first round, we released our model publicly, which enabled other users to launch white-box attacks against it. Nevertheless, a majority of the final submissions built upon our released model. The winning submission (team "liaofz" with a score of 95.3%) made use of a novel adversarial The dimensionality of the adversarial cone. For 500 correctly classified points x, and for ∈ {4, 10, 16}, we plot the probability that we find at least k orthogonal vectors r i such that r i ∞ = and x + r i is misclassified. For ≥ 10, model v3 adv shows a bimodal phenomenon: most points x either have 0 adversarial directions or more than 90.</p><p>denoising technique. The second placed defense (team "cihangxie" with a score of 92.4%) prepends our IRv2 adv-ens model with random padding and resizing of the input image <ref type="bibr" target="#b40">(Xie et al., 2018)</ref>.</p><p>It is noteworthy that the defenses that incorporated Ensemble Adversarial Training faired better against the worst-case black-box adversary. Indeed, although very robust on average, the winning defense achieved as low as 11.8% accuracy on some attacks. The best defense under this metric (team "rafaelmm" which randomly perturbed images before feeding them to our IRv2 adv-ens model) achieved at least 53.6% accuracy against all submitted attacks, including the attacks that explicitly targeted our released model in a white-box setting.</p><p>Decreasing gradient masking. Ensemble Adversarial Training decreases the magnitude of the gradient masking effect described previously. For the v3 adv-ens3 and v3 adv-ens4 models, we find that the loss incurred on a Step-LL attack gets within respectively 13% and 18% of the optimum loss (we recall that for models v3 and v3 adv , the approximation ratio was respectively 19% and 7%). Similarly, for the IRv2 adv-ens model, the ratio improves from 8% (for IRv2 adv ) to 14%. As expected, not solely training on a white-box single-step attack reduces gradient masking. We also verify that after Ensemble Adversarial Training, a two-step iterative attack outperforms the R+Step-LL attack from Section 4.1, thus providing further evidence that these models have meaningful gradients. Finally, we revisit the "Gradient-Aligned Adversarial Subspace" (GAAS) method of <ref type="bibr" target="#b38">Tramèr et al. (2017)</ref>. Their method estimates the size of the space of adversarial examples in the vicinity of a point, by finding a set of orthogonal perturbations of norm that are all adversarial. We note that adversarial perturbations do not technically form a "subspace" (e.g., the 0 vector is not adversarial). Rather, they may form a "cone", the dimension of which varies as we increase . By linearizing the loss function, estimating the dimensionality of this cone reduces to finding vectors r i that are strongly aligned with the model's gradient g = ∇ x L(h(x), y true ). <ref type="bibr" target="#b38">Tramèr et al. (2017)</ref> give a method that finds k orthogonal vectors r i that satisfy g r i ≥ • g 2 • 1 √ k (this bound is tight). We extend this result to the ∞ norm, an open question in <ref type="bibr" target="#b38">Tramèr et al. (2017)</ref>. In Section E, we give a randomized combinatorial construction <ref type="bibr" target="#b10">(Colbourn, 2010)</ref>, that finds k orthogonal vectors r i satisfying</p><formula xml:id="formula_8">r i ∞ = and E g r i ≥ • g 1 • 1 √ k .</formula><p>We show that this result is tight as well. For models v3, v3 adv and v3 adv-ens3 , we select 500 correctly classified test points. For each x, we search for a maximal number of orthogonal adversarial perturbations r i with r i ∞ = . We limit our search to k ≤ 100 directions per point. The results are in Figure <ref type="figure">2</ref>. For ∈ {4, 10, 16}, we plot the proportion of points that have at least k orthogonal adversarial perturbations. For a fixed , the value of k can be interpreted as the dimension of a "slice" of the cone of adversarial examples near a data point. For the standard Inception v3 model, we find over 50 orthogonal adversarial directions for 30% of the points. The v3 adv model shows a curious bimodal phenomenon for ≥ 10: for most points (≈ 80%), we find no adversarial direction aligned with the gradient, which is consistent with the gradient masking effect. Yet, for most of the remaining points, the adversarial space is very high-dimensional (k ≥ 90). Ensemble Adversarial Training yields a more robust model, with only a small fraction of points near a large adversarial space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>Previous work on adversarial training at scale has produced encouraging results, showing strong robustness to (single-step) adversarial examples <ref type="bibr" target="#b14">(Goodfellow et al., 2014b;</ref><ref type="bibr" target="#b19">Kurakin et al., 2017b</ref>). Yet, these results are misleading, as the adversarially trained models remain vulnerable to simple black-box and white-box attacks. Our results, generic with respect to the application domain, suggest that adversarial training can be improved by decoupling the generation of adversarial examples from the model being trained. Our experiments with Ensemble Adversarial Training show that the robustness attained to attacks from some models transfers to attacks from other models.</p><p>We did not consider black-box adversaries that attack a model via other means than by transferring examples from a local model. For instance, generative techniques (Baluja &amp; Fischer, 2017) might provide an avenue for stronger attacks. Yet, a recent work by <ref type="bibr" target="#b39">Xiao et al. (2018)</ref> found Ensemble Adversarial Training to be resilient to such attacks on MNIST and CIFAR10, and often attaining higher robustness than models that were adversarially trained on iterative attacks.</p><p>Moreover, interactive adversaries (see Appendix A) could try to exploit queries to the target model's prediction function in their attack, as demonstrated in <ref type="bibr" target="#b30">Papernot et al. (2017)</ref>. If queries to the target model yield prediction confidences, an adversary can estimate the target's gradient at a given point (e.g., using finite-differences as in <ref type="bibr" target="#b8">Chen et al. (2017)</ref>) and fool the target with our R+FGSM attack. Note that if queries only return the predicted label, the attack does not apply. Exploring the impact of these classes of black-box attacks and evaluating their scalability to complex tasks is an interesting avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A THREAT MODEL: FORMAL DEFINITIONS</head><p>We provide formal definitions for the threat model introduced in Section 3.1. In the following, we explicitly identify the hypothesis space H that a model belongs to as describing the model's architecture. We consider a target model h ∈ H trained over inputs (x, y true ) sampled from a data distribution D. More precisely, we write h ← train(H, X train , Y train , r) , where train is a randomized training procedure that takes in a description of the model architecture H, a training set X train , Y train sampled from D, and randomness r.</p><p>Given a set of test inputs X, Y = {(x 1 , y 1 ), . . . , (x m , y m )} from D and a budget &gt; 0, an adversary A produces adversarial examples X adv = {x adv 1 , . . . , x adv m }, such that</p><formula xml:id="formula_9">x i − x adv i ∞ ≤ for all i ∈ [1, m].</formula><p>We evaluate success of the attack as the error rate of the target model over X adv :</p><formula xml:id="formula_10">1 m m i=1 1(arg max h(x adv i ) = y i ) .</formula><p>We assume A can sample inputs according to the data distribution D. We define three adversaries. Definition 2 (White-Box Adversary). For a target model h ∈ H, a white-box adversary is given access to all elements of the training procedure, that is train (the training algorithm), H (the model architecture), the training data X train , Y train , the randomness r and the parameters h. The adversary can use any attack (e.g., those in Section 3.2) to find adversarial inputs. Attacks based on transferability <ref type="bibr" target="#b34">(Szegedy et al., 2013)</ref> fall in this category, wherein the adversary selects a procedure train and model architecture H , trains a local model h over D, and computes adversarial examples on its local model h using white-box attack strategies.</p><p>Most importantly, a black-box adversary does not learn the randomness r used to train the target, nor the target's parameters h. The black-box adversaries in our paper are actually slightly stronger than the ones defined above, in that they use the same training data X train , Y train as the target model.</p><p>We provide A with the target's training procedure train to capture knowledge of defensive strategies applied at training time, e.g., adversarial training <ref type="bibr" target="#b34">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b14">Goodfellow et al., 2014b)</ref> or ensemble adversarial training (see Section 4.2). For ensemble adversarial training, A also knows the architectures of all pre-trained models. In this work, we always mount black-box attacks that train a local model with a different architecture than the target model. We actually find that black-box attacks on adversarially trained models are stronger in this case (see Table <ref type="table" target="#tab_4">1</ref>).</p><p>The main focus of our paper is on non-interactive black-box adversaries as defined above. For completeness, we also formalize a stronger notion of interactive black-box adversaries that additionally issue prediction queries to the target model <ref type="bibr" target="#b30">(Papernot et al., 2017)</ref>. We note that in cases where ML models are deployed as part of a larger system (e.g., a self driving car), an adversary may not have direct access to the model's query interface. Definition 4 (Interactive Black-Box Adversary). For a target model h ∈ H, an interactive blackbox adversary only gets access to train (the target model's training procedure) and H (the model architecture). The adversary issues (adaptive) oracle queries to the target model. That is, for arbitrary inputs x ∈ [0, 1] d , the adversary obtains y = arg max h(x) and uses a local algorithm to craft adversarial examples (given knowledge of H, train, and tuples (x, y)). <ref type="bibr" target="#b30">Papernot et al. (2017)</ref> show that such attacks are possible even if the adversary only gets access to a small number of samples from D. Note that if the target model's prediction interface additionally returns class scores h(x), interactive black-box adversaries could use queries to the target model to estimate the model's gradient (e.g., using finite differences) <ref type="bibr" target="#b8">(Chen et al., 2017)</ref>, and then apply the attacks in Section 3.2. We further discuss interactive black-box attack strategies in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B GENERALIZATION BOUND FOR ENSEMBLE ADVERSARIAL TRAINING</head><p>We provide a formal statement of Theorem 1 in Section 3.4, regarding the generalization guarantees of Ensemble Adversarial Training. For simplicity, we assume that the model is trained solely on adversarial examples computed on the pre-trained models (i.e., we ignore the clean training data and the adversarial examples computed on the model being trained). Our results are easily extended to also consider these data points.</p><p>Let D be the data distribution and A 1 , . . . , A k , A * be adversarial distributions where a sample (x, y) is obtained by sampling (x, y true ) from D, computing an x adv such that x adv − x ∞ ≤ and returning (x adv , y true ). We assume the model is trained on N data points Z train , where N k data points are sampled from each distribution A i , for</p><formula xml:id="formula_11">1 ≤ i ≤ k. We denote A train = {A 1 , . . . , A k }. At test time, the model is evaluated on adversarial examples from A * .</formula><p>For a model h ∈ H we define the empirical risk</p><formula xml:id="formula_12">R(h, A train ) := 1 N (x adv ,y true )∈Z train L(h(x adv ), y true ) ,<label>(8)</label></formula><p>and the risk over the target distribution (or future adversary)</p><formula xml:id="formula_13">R(h, A * ) := E (x adv ,y true )∼A * [L(h(x adv ), y true )] .<label>(9)</label></formula><p>We further define the average discrepancy distance <ref type="bibr" target="#b24">(Mansour et al., 2009)</ref> between distributions A i and A * with respect to a hypothesis space H as</p><formula xml:id="formula_14">disc H (A train , A * ) := 1 k k i=1 sup h 1 ,h 2 ∈H E A i [1 {h 1 (x adv )=h 2 (x adv )} ] − E A * [1 {h 1 (x adv )=h 2 (x adv )} ] .<label>(10)</label></formula><p>This quantity characterizes how "different" the future adversary is from the train-time adversaries. Intuitively, the distance disc(A train , A * ) is small if the difference in robustness between two models to the target attack A * is somewhat similar to the difference in robustness between these two models to the attacks used for training (e.g., if the static black-box attacks A i induce much higher error on some model h 1 than on another model h 2 , then the same should hold for the target attack A * ). In other words, the ranking of the robustness of models h ∈ H should be similar for the attacks in A train as for A * .</p><p>Finally, let R N (H) be the average Rademacher complexity of the distributions A 1 , . . . , A k <ref type="bibr" target="#b41">(Zhang et al., 2012)</ref>. Note that R N (H) → 0 as N → ∞. The following theorem is a corollary of <ref type="bibr">Zhang et al. (2012, Theorem 5</ref>.2): Theorem 5. Assume that H is a function class consisting of bounded functions. Then, with probability at least 1 − ,</p><formula xml:id="formula_15">sup h∈H | R(h, A train ) − R(h, A * )| ≤ disc H (A train , A * ) + 2R N (H) + O ln(1/ ) N .<label>(11)</label></formula><p>Compared to the standard generalization bound for supervised learning, the generalization bound for Domain Adaptation incorporates the extra term disc H (A train , A * ) to capture the divergence between the target and source distributions. In our context, this means that the model h * learned by Ensemble Adversarial Training has guaranteed generalization bounds with respect to future adversaries that are not "too different" from the ones used during training. Note that A * need not restrict itself to perturbation with bounded ∞ norm for this result to hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTS ON MNIST</head><p>We re-iterate our ImageNet experiments on MNIST. For this simpler task, <ref type="bibr" target="#b23">Madry et al. (2017)</ref> show that training on iterative attacks conveys robustness to white-box attacks with bounded ∞ norm. Our goal is not to attain similarly strong white-box robustness on MNIST, but to show that our observations on limitations of single-step adversarial training, extend to other datasets than ImageNet. The MNIST dataset is a simple baseline for assessing the potential of a defense, but the obtained results do not always generalize to harder tasks. We suggest that this is because achieving robustness to ∞ perturbations admits a simple "closed-form" solution, given the near-binary nature of the data. Indeed, for an average MNIST image, over 80% of the pixels are in {0, 1} and only 6% are in the range [0.2, 0.8]. Thus, for a perturbation with ≤ 0.3, binarized versions of x and x adv can differ in at most 6% of the input dimensions. By binarizing the inputs of a standard CNN trained without adversarial training, we obtain a model that enjoys robustness similar to the model trained by <ref type="bibr" target="#b23">Madry et al. (2017)</ref>. Concretely, for a white-box I-FGSM attack, we get at most 11.4% error.</p><p>The existence of such a simple robust representation begs the question of why learning a robust model with adversarial training takes so much effort. Finding techniques to improve the performance of adversarial training, even on simple tasks, could provide useful insights for more complex tasks such as ImageNet, where we do not know of a similarly simple "denoising" procedure.</p><p>These positive results on MNIST for the ∞ norm also leave open the question of defining a general norm for adversarial examples. Let us motivate the need for such a definition: we find that if we first rotate an MNIST digit by 20°, and then use the I-FGSM, our rounding model and the model from <ref type="bibr" target="#b23">Madry et al. (2017)</ref> achieve only 65% accuracy (on "clean" rotated inputs, the error is &lt; 5%).</p><p>If we further randomly "flip" 5 pixels per image, the accuracy of both models drops to under 50%. Thus, we successfully evade the model by slightly extending the threat model (see Figure <ref type="figure" target="#fig_3">3</ref>).</p><p>Of course, we could augment the training set with such perturbations (see <ref type="bibr" target="#b12">Engstrom et al. (2017)</ref>).</p><p>An open question is whether we can enumerate all types of "adversarial" perturbations. In this work, we focus on the ∞ norm to illustrate our findings on the limitations of single-step adversarial training on ImageNet and MNIST, and to showcase the benefits of our Ensemble Adversarial Training variant. Our approach can easily be extended to consider multiple perturbation metrics. We leave such an evaluation to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 RESULTS</head><p>We repeat experiments from Section 4 on MNIST. We use the architectures in Table <ref type="table" target="#tab_10">5</ref>. We train a standard model for 6 epochs, and an adversarial model with the FGSM ( = 0.3) for 12 epochs.</p><p>During adversarial training, we avoid the label leaking effect described by <ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> by using the model's predicted class arg max h(x) instead of the true label y true in the FGSM,</p><p>We first analyze the "degenerate" minimum of adversarial training, described in Section 3.3. For each trained model, we compute the approximation-ratio of the FGSM for the inner maximization problem in equation ( <ref type="formula" target="#formula_0">1</ref>). That is, we compare the loss produced by the FGSM with the loss of a If we omit the input dropout (we call this architecture B * ) the single-step attack degrades significantly. We discuss this effect in more detail below. For the fully connected architecture D, we find that the learned model is very close to linear and thus also less prone to the degenerate solution to the min-max problem, as we postulated in Section 3.3.</p><p>Attacks. We evaluate our models on black-box attacks crafted on models A,B,C,D (for a fair comparison, we do not use the same pre-trained models for evaluation, but retrain them with different random seeds). In the majority of cases, we find that using a single pre-trained model produces good results, but that the extra diversity of including three pre-trained models can sometimes increase robustness even further. Our experiments confirm our conjecture that robustness to black-box attacks generalizes across models. Indeed, we find that when training with three external models, we attain very good robustness against attacks initiated from models with the same architecture (as evidenced by the average error on our attack suite), but also increased robustness to attacks initiated from the fourth holdout model D TRANSFERABILITY OF RANDOMIZED SINGLE-STEP PERTURBATIONS.</p><p>In Section 4.1, we introduced the R+Step-LL attack, an extension of the Step-LL method that prepends the attack with a small random perturbation. In Table <ref type="table" target="#tab_14">9</ref>, we evaluate the transferability of R+Step-LL adversarial examples on ImageNet. We find that the randomized variant produces perturbations that transfer at a much lower rate (see Table <ref type="table" target="#tab_4">1</ref> for the deterministic variant).  <ref type="bibr" target="#b38">Tramèr et al. (2017)</ref> consider the following task for a given model h: for a (correctly classified) point x, find k orthogonal vectors {r 1 , . . . , r k } such that r i 2 ≤ and all the x + r i are adversarial (i.e., arg max h(x + r i ) = y true ). By linearizing the model's loss function, this reduces to finding k orthogonal vectors r i that are maximally aligned with the model's gradient g = ∇ x L(h(x), y true ). We provide an optimal construction for the ∞ norm, based on Regular Hadamard Matrices <ref type="bibr" target="#b10">(Colbourn, 2010)</ref>. Given the ∞ constraint, we find orthogonal vectors r i that are maximally aligned with the signed gradient, sign(g). We first prove an analog of <ref type="bibr">(Tramèr et al., 2017, Lemma 1)</ref>.</p><p>Lemma 6. Let v ∈ {−1, 1} d and α ∈ (0, 1). Suppose there are k orthogonal vectors r 1 , . . .</p><formula xml:id="formula_16">r n ∈ {−1, 1} d satisfying v r i ≥ α • d. Then α ≤ k − 1 2 . Proof. Let ri = r i r i 2 = r i √ d .</formula><p>Then, we have</p><formula xml:id="formula_17">d = v 2 2 ≥ k i=1 |v ri | 2 = d −1 k i=1 |v r i | 2 ≥ d −1 • k • (α • d) 2 = k • α 2 • d ,<label>(12)</label></formula><p>from which we obtain α ≤ k − 1 2 .</p><p>This result bounds the number of orthogonal perturbations we can expect to find, for a given alignment with the signed gradient. As a warm-up consider the following trivial construction of k orthogonal vectors in {−1, 1} d that are "somewhat" aligned with sign(g). We split sign(g) into k "chunks" of size d k and define r i to be the vector that is equal to sign(g) in the i th chunk and zero otherwise. We obtain sign(g) r i = d k , a factor √ k worse than the the bound in Lemma 6.</p><p>We now provide a construction that meets this upper bound. We make use of Regular Hadamard Matrices of order k (Colbourn, 2010). These are square matrices H k such that: (1) all entries of H k are in {−1, 1} k ; (2) the rows of H k are mutually orthogonal; (3) All row sums are equal to √ k.</p><p>The order of a Regular Hadamard Matrix is of the form 4u 2 for an integer u. We use known constructions for k ∈ {4, 16, 36, 64, 100}.</p><p>Lemma 7. Let g ∈ R d and k be an integer for which a Regular Hadamard Matrix of order k exists.</p><p>Then, there is a randomized construction of k orthogonal vectors r 1 , . . . r n ∈ {−1, 1} d , such that sign(g)</p><formula xml:id="formula_18">r i = d • k − 1 /2 . Moreover, E[g r i ] = k − 1 /2 • g 1 .</formula><p>Proof. We construct k orthogonal vectors r 1 , . . . , r k ∈ {−1, 1} d , where r i is obtained by repeating the i th row of H k d /k times (for simplicity, we assume that k divides d. Otherwise we pad r i with zeros). We then multiply each r i component-wise with sign(g). By construction, the k vectors r i ∈ {−1, 1} d are mutually orthogonal, and we have sign(g)</p><formula xml:id="formula_19">r i = d k • √ k = d • k − 1 /2</formula><p>, which is tight according to Lemma 6.</p><p>As the weight of the gradient g may not be uniformly distributed among its d components, we apply our construction to a random permutation of the signed gradient. We then obtain</p><formula xml:id="formula_20">E[g r i ] = E d j=1 |g (j) | • sign(g (j) ) • r (j) i (13) = d j=1 |g (j) | • E sign(g (j) ) • r (j) i = k − 1 /2 • g 1 . (<label>14</label></formula><formula xml:id="formula_21">)</formula><p>It can be shown that the bound in Lemma 7 can be attained if and only if the r i are constructed from the rows of a Regular Hadamard Matrix <ref type="bibr" target="#b10">(Colbourn, 2010)</ref>. For general integers k for which no such matrix exists, other combinatorial designs may be useful for achieving looser bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ILLUSTRATIONS OF GRADIENT MASKING IN ADVERSARIAL TRAINING</head><p>In Section 3.3, we show that adversarial training introduces spurious curvature artifacts in the model's loss function around data points. As a result, one-shot attack strategies based on first-order approximations of the model loss produce perturbations that are non-adversarial. In Figures <ref type="figure" target="#fig_6">4 and 5</ref> we show further illustrations of this phenomenon for the Inception v3 adv model trained on ImageNet by <ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> as well as for the model A adv we trained on MNIST.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>in for small 1 , 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure2: The dimensionality of the adversarial cone. For 500 correctly classified points x, and for ∈ {4, 10, 16}, we plot the probability that we find at least k orthogonal vectors r i such that r i ∞ = and x + r i is misclassified. For ≥ 10, model v3 adv shows a bimodal phenomenon: most points x either have 0 adversarial directions or more than 90.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>White-box access to the internal model weights corresponds to a very strong adversarial model. We thus also consider the following relaxed and arguably more realistic notion of a black-box adversary. Definition 3 (Non-Interactive Black-Box Adversary). For a target model h ∈ H, a non-interactive black-box adversary only gets access to train (the target model's training procedure) and H (the model architecture). The adversary can sample from the data distribution D, and uses a local algorithm to craft adversarial examples X adv .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Adversarial Examples on MNIST. (top) clean examples. (middle) inputs are rotated by 20°and 5 random pixels are flipped. (bottom) The I-FGSM with = 0.3 is applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b38">Tramèr et al. (2017)</ref> left a construction for the ∞ norm as an open problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>in of the loss for small 1 , 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Additional illustrations of the local curvature artifacts introduced by adversarial training on ImageNet. We plot the loss of model v3 adv on samples of the form x* = x + 1 • g + 2 • g ⊥ ,where g is the signed gradient of v3 adv and g ⊥ is an orthogonal adversarial direction, obtained from an Inception v4 model. The right-side plots are zoomed in versions of the left-side plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>yield the following result. Theorem 1 (informal). Let h * ∈ H be a model learned with Ensemble Adversarial Training and static black-box adversaries A 1 , . . . , A k . Then, if h * is robust against the black-box adversaries A 1 , . . . A k used at training time, then h * has bounded error on attacks from a future black-box adversary A * , if A * is not "much stronger", on average, than the static adversaries A 1 , . . . , A k .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Error rates (in %) of adversarial examples transferred between models. We use Step-LL with = 16 /256 for 10,000 random test inputs. Diagonal elements represent a white-box attack. The best attack for each target appears in bold. Similar results for MNIST models appear in Table7.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Source</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Source</cell></row><row><cell>Target</cell><cell>v4</cell><cell cols="3">v3 v3 adv IRv2 IRv2 adv</cell><cell>Target</cell><cell>v4</cell><cell cols="2">v3 v3 adv IRv2 IRv2 adv</cell></row><row><cell>v4</cell><cell cols="2">60.2 39.2</cell><cell>31.1 36.6</cell><cell>30.9</cell><cell>v4</cell><cell cols="2">31.0 14.9</cell><cell>10.2 13.6</cell><cell>9.9</cell></row><row><cell>v3</cell><cell cols="2">43.8 69.6</cell><cell>36.4 42.1</cell><cell>35.1</cell><cell>v3</cell><cell cols="2">18.7 42.7</cell><cell>13.0 17.8</cell><cell>12.8</cell></row><row><cell>v3 adv</cell><cell cols="3">36.3 35.6 $ $ 26.6 35.2</cell><cell>35.9</cell><cell>v3 adv</cell><cell></cell><cell></cell></row><row><cell>IRv2</cell><cell cols="2">38.0 38.0</cell><cell>30.8 50.7</cell><cell>31.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">IRv2 adv 31.0 30.3</cell><cell>25.7 30.6</cell><cell>$ $ 21.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Top 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note>compares error rates for the Step-LL and R+Step-LL methods (with = 16/256 and α = /2). The extra random step yields a stronger attack for all models, even those without adversarial training. This suggests that a model's loss function is generally less smooth near the data points. We further compared the R+Step-LL attack to a two-step Iter-LL attack, which computes two gradient steps. Surprisingly, we find that for the adversarially trained Inception v3 model, the R+Step-LL attack is stronger than the two-step Iter-LL attack. That is, the local gradients learned by the adversarially trained model are worse than random directions for finding adversarial examples!</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Error rates (in %) for Step-LL, R+Step-LL and a two-step Iter-LL on ImageNet. We use = 16 /256, α = /2 on 10,000 random test inputs. R+FGSM results on MNIST are in Table7.</figDesc><table><row><cell></cell><cell>v4</cell><cell cols="3">v3 v3 adv IRv2 IRv2 adv</cell><cell>v4</cell><cell cols="2">v3 v3 adv IRv2 IRv2 adv</cell></row><row><cell>Step-LL</cell><cell cols="2">60.2 69.6</cell><cell>26.6 50.7</cell><cell>21.4</cell><cell cols="2">31.0 42.7</cell><cell>9.0 24.0</cell><cell>5.8</cell></row><row><cell>R+Step-LL</cell><cell cols="2">70.5 80.0</cell><cell>64.8 56.3</cell><cell>37.5</cell><cell cols="2">42.8 57.1</cell><cell>37.1 29.3</cell><cell>15.0</cell></row><row><cell>Iter-LL(2)</cell><cell cols="3">78.5 86.3 $ $ 58.3 69.9</cell><cell>41.6</cell><cell cols="3">56.2 70.2 ¨29.6 45.4</cell><cell>16.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Top 1</cell><cell></cell><cell></cell><cell></cell><cell>Top 5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Models used for Ensemble Adversarial Training on ImageNet. The ResNets<ref type="bibr" target="#b16">(He et al., 2016)</ref> use either 50 or 101 layers. IncRes stands for Inception ResNet<ref type="bibr" target="#b35">(Szegedy et al., 2016a)</ref>.</figDesc><table><row><cell>Trained Model</cell><cell>Pre-trained Models</cell><cell>Holdout Models</cell></row><row><cell>Inception v3 (v3 adv-ens3 )</cell><cell>Inception v3, ResNet v2 (50)</cell><cell>Inception v4</cell></row><row><cell cols="2">Inception v3 (v3 adv-ens4 ) Inception v3, ResNet v2 (50), IncRes v2</cell><cell>ResNet v1 (50)</cell></row><row><cell>IncRes v2 (IRv2 adv-ens )</cell><cell>Inception v3, IncRes v2</cell><cell>ResNet v2 (101)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>th batch requires a forward-backward pass to compute adversarial gradients. We use synchronous distributed training on 50 machines, with minibatches of size 16 (we did not pre-compute gradients, and thus lower the batch size to fit all models in memory). Half of the examples in a minibatch are replaced by Step-LL examples. As in<ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref>, we use RMSProp with a learning rate of 0.045, decayed by a factor of 0.94 every two epochs.To evaluate how robustness to black-box attacks generalizes across models, we transfer various attacks crafted on three different holdout models (see Table3), as well as on an ensemble of these models (as in<ref type="bibr" target="#b21">Liu et al. (2017)</ref>). We use the Step-LL, R+Step-LL, FGSM, I-FGSM and the PGD attack from<ref type="bibr" target="#b23">Madry et al. (2017)</ref> using the hinge-loss function from<ref type="bibr" target="#b6">Carlini &amp; Wagner (2017a)</ref>. Our results are in Table4. For each model, we report the worst-case error rate over all black-box attacks transfered from each of the holdout models (20 attacks in total). Results for MNIST are in Table8.</figDesc><table /><note>. In each training batch, we rotate the source of adversarial examples between the currently trained model and one of the pre-trained models. We select the source model at random in each batch, to diversify examples across epochs. The pre-trained models' gradients can be precomputed for the full training set. The per-batch cost of Ensemble Adversarial Training is thus lower than that of standard adversarial training: using our method with n − 1 pre-trained models, only every n Convergence speed. Convergence of Ensemble Adversarial Training is slower than for standard adversarial training, a result of training on "hard" adversarial examples and lowering the batch size.<ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> report that after 187 epochs (150k iterations with minibatches of size 32), the v3 adv model achieves 78% accuracy. Ensemble Adversarial Training for models v3 adv-ens3 and v3 adv-ens4 converges after 280 epochs (450k iterations with minibatches of size 16). The Inception ResNet v2 model is trained for 175 epochs, where a baseline model converges at around 160 epochs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Error rates (in %) for Ensemble Adversarial Training on ImageNet. Error rates on clean data are computed over the full test set. For 10,000 random test set inputs, and = 16 /256, we report error rates on white-box Step-LL and the worst-case error over a series of black-box attacks (Step-LL, R+Step-LL, FGSM, I-FGSM, PGD) transferred from the holdout models in Table3. For both architectures, we mark methods tied for best in bold (based on 95% confidence). For both architectures, the models trained with Ensemble Adversarial Training are slightly less accurate on clean data, compared to standard adversarial training. Our models are also more vulnerable to white-box single-step attacks, as they were only partially trained on such perturbations. Note that for v3 adv-ens4 , the proportion of white-box Step-LL samples seen during training is 1 /4 (instead of 1 /3 for model v3 adv-ens3 ). The negative impact on the robustness to white-box attacks is large, for only a minor gain in robustness to transferred samples. Thus it appears that while increasing the diversity of adversarial examples seen during training can provide some marginal improvement, the main benefit of Ensemble Adversarial Training is in decoupling the attacks from the model being trained, which was the goal we stated in Section 3.4.</figDesc><table><row><cell></cell><cell></cell><cell>Top 1</cell><cell></cell><cell></cell><cell>Top 5</cell><cell></cell></row><row><cell>Model</cell><cell cols="6">Clean Step-LL Max. Black-Box Clean Step-LL Max. Black-Box</cell></row><row><cell>v3</cell><cell>22.0</cell><cell>69.6</cell><cell>51.2</cell><cell>6.1</cell><cell>42.7</cell><cell>24.5</cell></row><row><cell>v3 adv</cell><cell>22.0</cell><cell>26.6</cell><cell>40.8</cell><cell>6.1</cell><cell>9.0</cell><cell>17.4</cell></row><row><cell>v3 adv-ens3</cell><cell>23.6</cell><cell>30.0</cell><cell>34.0</cell><cell>7.6</cell><cell>10.1</cell><cell>11.2</cell></row><row><cell>v3 adv-ens4</cell><cell>24.2</cell><cell>43.3</cell><cell>33.4</cell><cell>7.8</cell><cell>19.4</cell><cell>10.7</cell></row><row><cell>IRv2</cell><cell>19.6</cell><cell>50.7</cell><cell>44.4</cell><cell>4.8</cell><cell>24.0</cell><cell>17.8</cell></row><row><cell>IRv2 adv</cell><cell>19.8</cell><cell>21.4</cell><cell>34.5</cell><cell>4.9</cell><cell>5.8</cell><cell>11.7</cell></row><row><cell>IRv2 adv-ens</cell><cell>20.2</cell><cell>26.0</cell><cell>27.0</cell><cell>5.1</cell><cell>7.6</cell><cell>7.9</cell></row><row><cell cols="2">White-box attacks.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Ensemble Adversarial Training is not robust to white-box Iter-LL and R+Step-LL samples: the error rates are similar to those for the v3 adv model, and omitted for brevity (see<ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> for Iter-LL attacks and Table2for R+Step-LL attacks).<ref type="bibr" target="#b19">Kurakin et al. (2017b)</ref> conjecture that larger models are needed to attain robustness to such attacks. Yet, against black-box adversaries, these attacks are only a concern insofar as they reliably transfer between models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Neural network architectures used in this work for the MNIST dataset.</figDesc><table><row><cell>Conv: convo-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Approximation ratio between optimal loss and loss induced by single-step attack on MNIST. Architecture B' is the same as B without the input dropout layer. The results appear in Table6. As we can see, for all model architectures, adversarial training degraded the quality of a linear approximation to the model's loss.We find that input dropout<ref type="bibr" target="#b33">(Srivastava et al., 2014)</ref> (i.e., randomly dropping a fraction of input features during training) as used in architecture B limits this unwarranted effect of adversarial training. 2</figDesc><table><row><cell>A</cell><cell>A adv</cell><cell>B</cell><cell>B adv</cell><cell>B  *</cell><cell>B  *  adv</cell><cell>C</cell><cell>C adv</cell><cell>D</cell><cell>D adv</cell></row><row><cell cols="2">17% 0%</cell><cell cols="2">25% 8%</cell><cell cols="2">23% 1%</cell><cell cols="2">25% 0%</cell><cell cols="2">49% 16%</cell></row><row><cell cols="2">strong iterative attack.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Table 7 compares error rates of undefended and adversarially trained models on whitebox and black-box attacks, as in Section 4.1. Again, model B presents an anomaly. For all other models, we corroborate our findings on ImageNet for adversarial training: (1) black-box attacks trump white-box single-step attacks; (2) white-box single-step attacks are significantly stronger if prepended by a random step. For model B adv , the opposite holds true. We believe this is because input dropout increases diversity of attack samples similarly to Ensemble Adversarial Training. White-box and black-box attacks against standard and adversarially trained models. For each model, the strongest single-step white-box and black box attacks are marked in bold.While training with input dropout helps avoid the degradation of the single-step attack, it also significantly delays convergence of the model. Indeed, model B adv retains relatively high error on white-box FGSM examples. Adversarial training with input dropout can be seen as comparable to training with a randomized single-step attack, as discussed in Section 4.1.The positive effect of input dropout is architecture and dataset specific: Adding an input dropout layer to models A, C and D confers only marginal benefit, and is outperformed by Ensemble Adversarial Training, discussed below. Moreover,<ref type="bibr" target="#b25">Mishkin et al. (2017)</ref> find that input dropout significantly degrades accuracy on ImageNet. We thus did not incorporate it into our models on ImageNet.</figDesc><table><row><cell></cell><cell cols="2">white-box</cell><cell></cell><cell></cell><cell>black-box</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">FGSM R+FGSM FGSM A FGSM B FGSM B* FGSM C FGSM D</cell></row><row><cell>A</cell><cell>64.7</cell><cell>69.7</cell><cell>-</cell><cell>61.5</cell><cell>53.2</cell><cell>46.8</cell><cell>41.5</cell></row><row><cell>A adv</cell><cell>2.2</cell><cell>14.8</cell><cell>6.6</cell><cell>10.7</cell><cell>8.8</cell><cell>6.5</cell><cell>8.3</cell></row><row><cell>B</cell><cell>85.0</cell><cell>86.0</cell><cell>45.7</cell><cell>-</cell><cell>69.9</cell><cell>59.9</cell><cell>85.9</cell></row><row><cell>B adv</cell><cell>11.6</cell><cell>11.1</cell><cell>6.4</cell><cell>8.9</cell><cell>8.5</cell><cell>4.9</cell><cell>6.1</cell></row><row><cell>B  *  B  *  adv</cell><cell>75.7 4.3</cell><cell>74.1 40.6</cell><cell>44.3 16.1</cell><cell>72.8 14.7</cell><cell>-15.0</cell><cell>46.0 17.9</cell><cell>62.6 9.1</cell></row><row><cell>C</cell><cell>81.8</cell><cell>81.8</cell><cell>40.2</cell><cell>55.8</cell><cell>49.5</cell><cell>-</cell><cell>59.4</cell></row><row><cell>C adv</cell><cell>3.7</cell><cell>17.1</cell><cell>9.8</cell><cell>29.3</cell><cell>21.5</cell><cell>11.9</cell><cell>21.9</cell></row><row><cell>D</cell><cell>92.4</cell><cell>95.4</cell><cell>61.3</cell><cell>74.1</cell><cell>68.9</cell><cell>65.1</cell><cell>-</cell></row><row><cell>D adv</cell><cell>25.5</cell><cell>47.5</cell><cell>32.1</cell><cell>30.5</cell><cell>29.3</cell><cell>28.2</cell><cell>21.8</cell></row><row><cell cols="8">Ensemble Adversarial Training. To evaluate Ensemble Adversarial Training 3.4, we train two</cell></row><row><cell cols="8">models per architecture. The first, denoted [A-D] adv-ens , uses a single pre-trained model of the same</cell></row><row><cell>type (i.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>e., A adv-ens is trained on perturbations from another model A). The second model, denoted [A-D] adv-ens3 , uses 3 pre-trained models ({A, C, D} or {B, C, D}). We train all models for 12 epochs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Ensemble Adversarial Training on MNIST. For black-box robustness, we report the maximum and average error rate over a suite of 12 attacks, comprised of the FGSM, I-FGSM and PGD<ref type="bibr" target="#b23">(Madry et al., 2017)</ref> attacks applied to models A,B,C and D. We use = 16 in all cases. For each model architecture, we mark the models tied for best (at a 95% confidence level) in bold.The attacks we consider are the FGSM, I-FGSM and the PGD attack from<ref type="bibr" target="#b23">Madry et al. (2017)</ref> with the loss function from<ref type="bibr" target="#b6">Carlini &amp; Wagner (2017a)</ref>), all with = 0.3. The results appear in Table8. For each model, we report the worst-case and average-case error rate over all black-box attacks.Ensemble Adversarial Training significantly increases robustness to black-box attacks, except for architecture B, which we previously found to not suffer from the same overfitting phenomenon that affects the other adversarially trained networks. Nevertheless, model B adv-ens achieves slightly better robustness to white-box and black-box attacks than B adv .</figDesc><table><row><cell></cell><cell cols="4">Clean FGSM Max. Black Box Avg. Black Box</cell></row><row><cell>A adv</cell><cell>0.8</cell><cell>2.2</cell><cell>10.8</cell><cell>7.7</cell></row><row><cell>A adv-ens</cell><cell>0.8</cell><cell>7.0</cell><cell>6.6</cell><cell>5.2</cell></row><row><cell>A adv-ens3</cell><cell>0.7</cell><cell>5.4</cell><cell>6.5</cell><cell>4.3</cell></row><row><cell>B adv</cell><cell>0.8</cell><cell>11.6</cell><cell>8.9</cell><cell>5.5</cell></row><row><cell>B adv-ens</cell><cell>0.7</cell><cell>10.5</cell><cell>6.8</cell><cell>5.3</cell></row><row><cell>B adv-ens3</cell><cell>0.8</cell><cell>14.0</cell><cell>8.8</cell><cell>5.1</cell></row><row><cell>C adv</cell><cell>1.0</cell><cell>3.7</cell><cell>29.3</cell><cell>18.7</cell></row><row><cell>C adv-ens</cell><cell>1.3</cell><cell>1.9</cell><cell>17.2</cell><cell>10.7</cell></row><row><cell>C adv-ens3</cell><cell>1.4</cell><cell>3.6</cell><cell>14.5</cell><cell>8.4</cell></row><row><cell>D adv</cell><cell>2.6</cell><cell>25.5</cell><cell>32.5</cell><cell>23.5</cell></row><row><cell>D adv-ens</cell><cell>2.6</cell><cell>21.5</cell><cell>38.6</cell><cell>28.0</cell></row><row><cell>D adv-ens3</cell><cell>2.6</cell><cell>29.4</cell><cell>29.8</cell><cell>15.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Error rates (in %) of randomized single-step attacks transferred between models on ImageNet. We use R+Step-LL with = 16/256, α = /2 for 10,000 random test set samples. The white-box attack always outperforms black-box attacks.E GRADIENT ALIGNED ADVERSARIAL SUBSPACES FOR THE ∞ NORM</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Source</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Source</cell></row><row><cell>Target</cell><cell>v4</cell><cell cols="3">v3 v3 adv IRv2 IRv2 adv</cell><cell>Target</cell><cell>v4</cell><cell cols="3">v3 v3 adv IRv2 IRv2 adv</cell></row><row><cell>v4</cell><cell cols="2">70.5 37.2</cell><cell>23.2 34.0</cell><cell>24.6</cell><cell>v4</cell><cell cols="2">42.8 14.3</cell><cell cols="2">6.3 11.9</cell><cell>6.9</cell></row><row><cell>v3</cell><cell cols="2">42.6 80.0</cell><cell>26.7 38.5</cell><cell>27.6</cell><cell>v3</cell><cell cols="2">18.0 57.1</cell><cell cols="2">8.0 15.6</cell><cell>8.6</cell></row><row><cell>v3 adv</cell><cell cols="2">31.4 30.7</cell><cell>64.8 30.4</cell><cell>34.0</cell><cell>v3 adv</cell><cell cols="2">10.7 10.4</cell><cell cols="2">37.1 10.1</cell><cell>12.9</cell></row><row><cell>IRv2</cell><cell cols="2">36.2 35.7</cell><cell>23.0 56.3</cell><cell>24.6</cell><cell>IRv2</cell><cell cols="2">12.8 13.6</cell><cell cols="2">6.1 29.3</cell><cell>7.0</cell></row><row><cell cols="3">IRv2 adv 26.8 26.3</cell><cell>25.2 26.9</cell><cell>37.5</cell><cell>IRv2 adv</cell><cell>8.0</cell><cell>8.0</cell><cell>7.7</cell><cell>8.3</cell><cell>15.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Top 1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Top 5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We publicly released our model after the first round, and it could thereafter be targeted using white-box attacks. Nevertheless, a majority of the top submissions in the final round, e.g.(Xie et al.,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2018" xml:id="foot_1">) built upon our released model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">We thank Arjun Bhagoji, Bo Li and Dawn Song for this observation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Ben Poole and Jacob Steinhardt for feedback on early versions of this work. Nicolas Papernot is supported by a Google PhD Fellowship in Security. Research was supported in part by the Army Research Laboratory, under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber Security CRA), and the Army Research Office under grant W911NF-13-1-0421. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notation hereon.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavail-ablefromtensorflow.org" />
		<editor>Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Vincent Vanhoucke, Vijay Vasudevan</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<title level="m">Concrete problems in ai safety</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<idno>arXiv:1703.09387</idno>
		<imprint>
			<date type="published" when="2017">2018. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adversarial transformation networks: Learning to generate adversarial examples</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-KDD</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Comment on&quot; biologically inspired protection of deep networks from adversarial attacks</title>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01547</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">S18S</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07263</idno>
		<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojanowski</forename><surname>Piotr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grave</forename><surname>Edouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dauphin</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usunier</forename><surname>Nicolas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08847</idno>
		<title level="m">Parseval networks: Improving robustness to adversarial examples</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">CRC handbook of combinatorial designs</title>
		<author>
			<persName><forename type="first">Colbourn</forename><surname>Charles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A rotation and a translation suffice: Fooling cnns with simple transformations</title>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02779</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014a</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Rigazio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5068</idno>
		<title level="m">Towards deep neural network architectures robust to adversarial examples</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00851</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio ; Ian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017b. 2017. 2017c</date>
		</imprint>
	</monogr>
	<note>Defense against adversarial attack</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Foveation-based mechanisms alleviate adversarial examples</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06292</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.3430</idno>
		<title level="m">Domain adaptation: Learning bounds and algorithms</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Systematic evaluation of convolution neural network advances on the imagenet</title>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Sergievskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Computer Vision and Image Understanding</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Biologically inspired protection of deep networks from adversarial attacks</title>
		<author>
			<persName><forename type="first">Aran</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09202</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE European Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
	<note>Security and Privacy</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Towards the science of security and privacy in machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arunesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wellman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03814</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2016 IEEE Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016c</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia Conference on Computer and Communications Security (ASIACCS)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bys4ob-Rb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Certifiable distributional robustness with principled adversarial training</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hk6kPgZA-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stealing machine learning models via prediction apis</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Usenix Security</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The space of transferable adversarial examples</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03453</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Generating adversarial examples with adversarial networks</title>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HknbyQbC-" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sk9yuql0Z" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generalization bounds for domain adaptation</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
