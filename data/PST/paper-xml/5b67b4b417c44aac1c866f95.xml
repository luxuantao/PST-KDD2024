<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Detection with Deep Learning: A Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhong-Qiu</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shou-Tao</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">China</forename><forename type="middle">Xindong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technol-ogy</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computing and Informatics</orgName>
								<orgName type="institution">University of Louisiana at Lafayette</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object Detection with Deep Learning: A Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">received August xx, 2017; revised xx xx, 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>object detection</term>
					<term>neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network (CNN). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T O gain a complete image understanding, we should not only concentrate on classifying different images, but also try to precisely estimate the concepts and locations of objects contained in each image. This task is referred as object detection <ref type="bibr" target="#b0">[1]</ref>[S1], which usually consists of different subtasks such as face detection <ref type="bibr" target="#b1">[2]</ref>[S2], pedestrian detection <ref type="bibr" target="#b2">[3]</ref>[S2] and skeleton detection <ref type="bibr" target="#b3">[4]</ref>[S3]. As one of the fundamental computer vision problems, object detection is able to provide valuable information for semantic understanding of images and videos, and is related to many applications, including image classification <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, human behavior analysis <ref type="bibr" target="#b6">[7]</ref>[S4], face recognition <ref type="bibr" target="#b7">[8]</ref>[S5] and autonomous driving <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Meanwhile, Inheriting from neural networks and related learning systems, the progress in these fields will develop neural network algorithms, and will also have great impacts on object detection techniques which can be considered as learning systems. <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>[S6]. However, due to large variations in viewpoints, poses, occlusions and lighting conditions, it's difficult to perfectly accomplish object detection with an additional object localization task. So much attention has been attracted to this field in recent years <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>.</p><p>The problem definition of object detection is to determine where objects are located in a given image (object localization) and which category each object belongs to (object classification). So the pipeline of traditional object detection models can be mainly divided into three stages: informative region selection, feature extraction and classification. Informative region selection. As different objects may appear in any positions of the image and have different aspect ratios or sizes, it is a natural choice to scan the whole image with a multi-scale sliding window. Although this exhaustive strategy can find out all possible positions of the objects, its shortcomings are also obvious. Due to a large number of candidate windows, it is computationally expensive and produces too many redundant windows. However, if only a fixed number of sliding window templates are applied, unsatisfactory regions may be produced. Feature extraction. To recognize different objects, we need to extract visual features which can provide a semantic and robust representation. SIFT <ref type="bibr" target="#b18">[19]</ref>, HOG <ref type="bibr" target="#b19">[20]</ref> and Haar-like <ref type="bibr" target="#b20">[21]</ref> features are the representative ones. This is due to the fact that these features can produce representations associated with complex cells in human brain <ref type="bibr" target="#b18">[19]</ref>. However, due to the diversity of appearances, illumination conditions and backgrounds, it's difficult to manually design a robust feature descriptor to perfectly describe all kinds of objects. Classification. Besides, a classifier is needed to distinguish a target object from all the other categories and to make the representations more hierarchical, semantic and informative for visual recognition. Usually, the Supported Vector Machine (SVM) <ref type="bibr" target="#b21">[22]</ref>, AdaBoost <ref type="bibr" target="#b22">[23]</ref> and Deformable Part-based Model (DPM) <ref type="bibr" target="#b23">[24]</ref> are good choices. Among these classifiers, the DPM is a flexible model by combining object parts with deformation cost to handle severe deformations. In DPM, with the aid of a graphical model, carefully designed low-level features and kinematically inspired part decompositions are combined. And discriminative learning of graphical models allows for building high-precision part-based models for a variety of object classes.</p><p>Based on these discriminant local feature descriptors and shallow learnable architectures, state of the art results have been obtained on PASCAL VOC object detection competition <ref type="bibr" target="#b24">[25]</ref> and real-time embedded systems have been obtained with a low burden on hardware. However, small gains are obtained during 2010-2012 by only building ensemble systems and employing minor variants of successful methods <ref type="bibr" target="#b14">[15]</ref>. This fact is due to the following reasons: 1) The generation of candidate bounding boxes with a sliding window strategy is redundant, inefficient and inaccurate. 2) The semantic gap cannot be arXiv:1807.05511v2 [cs.CV] 16 Apr 2019 bridged by the combination of manually engineered low-level descriptors and discriminatively-trained shallow models.</p><p>Thanks to the emergency of Deep Neural Networks (DNNs) <ref type="bibr" target="#b5">[6]</ref>[S7], a more significant gain is obtained with the introduction of Regions with CNN features (R-CNN) <ref type="bibr" target="#b14">[15]</ref>. DNNs, or the most representative CNNs, act in a quite different way from traditional approaches. They have deeper architectures with the capacity to learn more complex features than the shallow ones. Also the expressivity and robust training algorithms allow to learn informative object representations without the need to design features manually <ref type="bibr" target="#b25">[26]</ref>.</p><p>Since the proposal of R-CNN, a great deal of improved models have been suggested, including Fast R-CNN which jointly optimizes classification and bounding box regression tasks <ref type="bibr" target="#b15">[16]</ref>, Faster R-CNN which takes an additional subnetwork to generate region proposals <ref type="bibr" target="#b17">[18]</ref> and YOLO which accomplishes object detection via a fixed-grid regression <ref type="bibr" target="#b16">[17]</ref>. All of them bring different degrees of detection performance improvements over the primary R-CNN and make real-time and accurate object detection become more achievable.</p><p>In this paper, a systematic review is provided to summarise representative models and their different characteristics in several application domains, including generic object detection <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, salient object detection <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, face detection <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref> and pedestrian detection <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Their relationships are depicted in Figure <ref type="figure" target="#fig_0">1</ref>. Based on basic CNN architectures, generic object detection is achieved with bounding box regression, while salient object detection is accomplished with local contrast enhancement and pixel-level segmentation. Face detection and pedestrian detection are closely related to generic object detection and mainly accomplished with multi-scale adaption and multi-feature fusion/boosting forest, respectively. The dotted lines indicate that the corresponding domains are associated with each other under certain conditions. It should be noticed that the covered domains are diversified. Pedestrian and face images have regular structures, while general objects and scene images have more complex variations in geometric structures and layouts. Therefore, different deep models are required by various images.</p><p>There has been a relevant pioneer effort <ref type="bibr" target="#b33">[34]</ref> which mainly focuses on relevant software tools to implement deep learning techniques for image classification and object detection, but pays little attention on detailing specific algorithms. Different from it, our work not only reviews deep learning based object detection models and algorithms covering different application domains in detail, but also provides their corresponding experimental comparisons and meaningful analyses.</p><p>The rest of this paper is organized as follows. In Section 2, a brief introduction on the history of deep learning and the basic architecture of CNN is provided. Generic object detection architectures are presented in Section 3. Then reviews of CNN applied in several specific tasks, including salient object detection, face detection and pedestrian detection, are exhibited in Section 4-6, respectively. Several promising future directions are proposed in Section 7. At last, some concluding remarks are presented in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. A BRIEF OVERVIEW OF DEEP LEARNING</head><p>Prior to overview on deep learning based object detection approaches, we provide a review on the history of deep learning along with an introduction on the basic architecture and advantages of CNN.</p><p>A. The History: Birth, Decline and Prosperity Deep models can be referred to as neural networks with deep structures. The history of neural networks can date back to 1940s <ref type="bibr" target="#b34">[35]</ref>, and the original intention was to simulate the human brain system to solve general learning problems in a principled way. It was popular in 1980s and 1990s with the proposal of back-propagation algorithm by Hinton et al. <ref type="bibr" target="#b35">[36]</ref>. However, due to the overfitting of training, lack of large scale training data, limited computation power and insignificance in performance compared with other machine learning tools, neural networks fell out of fashion in early 2000s.</p><p>Deep learning has become popular since 2006 <ref type="bibr" target="#b36">[37]</ref>[S7] with a break through in speech recognition <ref type="bibr" target="#b37">[38]</ref>. The recovery of deep learning can be attributed to the following factors.</p><p>• The emergence of large scale annotated training data, such as ImageNet <ref type="bibr" target="#b38">[39]</ref>, to fully exhibit its very large learning capacity;</p><p>• Fast development of high performance parallel computing systems, such as GPU clusters;</p><p>• Significant advances in the design of network structures and training strategies. With unsupervised and layerwise pre-training guided by Auto-Encoder (AE) <ref type="bibr" target="#b39">[40]</ref> or Restricted Boltzmann Machine (RBM) <ref type="bibr" target="#b40">[41]</ref>, a good initialization is provided. With dropout and data augmentation, the overfitting problem in training has been relieved <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b41">[42]</ref>.</p><p>With batch normalization (BN), the training of very deep neural networks becomes quite efficient <ref type="bibr" target="#b42">[43]</ref>. Meanwhile, various network structures, such as AlexNet <ref type="bibr" target="#b5">[6]</ref>, Overfeat <ref type="bibr" target="#b43">[44]</ref>, GoogLeNet <ref type="bibr" target="#b44">[45]</ref>, VGG <ref type="bibr" target="#b45">[46]</ref> and ResNet <ref type="bibr" target="#b46">[47]</ref>, have been extensively studied to improve the performance. What prompts deep learning to have a huge impact on the entire academic community? It may owe to the contribution of Hinton's group, whose continuous efforts have demonstrated that deep learning would bring a revolutionary breakthrough on grand challenges rather than just obvious improvements on small datasets. Their success results from training a large CNN on 1.2 million labeled images together with a few techniques <ref type="bibr" target="#b5">[6]</ref> (e.g., ReLU operation <ref type="bibr" target="#b47">[48]</ref> and 'dropout' regularization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture and Advantages of CNN</head><p>CNN is the most representative model of deep learning <ref type="bibr" target="#b25">[26]</ref>. A typical CNN architecture, which is referred to as VGG16, can be found in Fig. <ref type="figure" target="#fig_11">S1</ref>. Each layer of CNN is known as a feature map. The feature map of the input layer is a 3D matrix of pixel intensities for different color channels (e.g. RGB). The feature map of any internal layer is an induced multi-channel image, whose 'pixel' can be viewed as a specific feature. Every neuron is connected with a small portion of adjacent neurons from the previous layer (receptive field). Different types of transformations <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> can be conducted on feature maps, such as filtering and pooling. Filtering (convolution) operation convolutes a filter matrix (learned weights) with the values of a receptive field of neurons and takes a nonlinear function (such as sigmoid <ref type="bibr" target="#b50">[51]</ref>, ReLU) to obtain final responses. Pooling operation, such as max pooling, average pooling, L2-pooling and local contrast normalization <ref type="bibr" target="#b51">[52]</ref>, summaries the responses of a receptive field into one value to produce more robust feature descriptions.</p><p>With an interleave between convolution and pooling, an initial feature hierarchy is constructed, which can be fine-tuned in a supervised manner by adding several fully connected (FC) layers to adapt to different visual tasks. According to the tasks involved, the final layer with different activation functions <ref type="bibr" target="#b5">[6]</ref> is added to get a specific conditional probability for each output neuron. And the whole network can be optimized on an objective function (e.g. mean squared error or cross-entropy loss) via the stochastic gradient descent (SGD) method. The typical VGG16 has totally 13 convolutional (conv) layers, 3 fully connected layers, 3 max-pooling layers and a softmax classification layer. The conv feature maps are produced by convoluting 3*3 filter windows, and feature map resolutions are reduced with 2 stride max-pooling layers. An arbitrary test image of the same size as training samples can be processed with the trained network. Re-scaling or cropping operations may be needed if different sizes are provided <ref type="bibr" target="#b5">[6]</ref>.</p><p>The advantages of CNN against traditional methods can be summarised as follows.</p><p>• Hierarchical feature representation, which is the multilevel representations from pixel to high-level semantic features learned by a hierarchical multi-stage structure <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b52">[53]</ref>, can be learned from data automatically and hidden factors of input data can be disentangled through multi-level nonlinear mappings.</p><p>• Compared with traditional shallow models, a deeper architecture provides an exponentially increased expressive capability.</p><p>• The architecture of CNN provides an opportunity to jointly optimize several related tasks together (e.g. Fast R-CNN combines classification and bounding box regression into a multi-task leaning manner).</p><p>• Benefitting from the large learning capacity of deep CNNs, some classical computer vision challenges can be recast as high-dimensional data transform problems and solved from a different viewpoint. Due to these advantages, CNN has been widely applied into many research fields, such as image super-resolution reconstruction <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, image classification <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b55">[56]</ref>, image retrieval <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, face recognition <ref type="bibr" target="#b7">[8]</ref>[S5], pedestrian detection <ref type="bibr" target="#b58">[59]</ref>- <ref type="bibr" target="#b60">[61]</ref> and video analysis <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GENERIC OBJECT DETECTION</head><p>Generic object detection aims at locating and classifying existing objects in any one image, and labeling them with rectangular bounding boxes to show the confidences of existence. The frameworks of generic object detection methods can mainly be categorized into two types (see Figure <ref type="figure" target="#fig_1">2</ref>). One follows traditional object detection pipeline, generating region proposals at first and then classifying each proposal into different object categories. The other regards object detection as a regression or classification problem, adopting a unified framework to achieve final results (categories and locations) directly. The region proposal based methods mainly include R-CNN <ref type="bibr" target="#b14">[15]</ref>, SPP-net <ref type="bibr" target="#b63">[64]</ref>, Fast R-CNN <ref type="bibr" target="#b15">[16]</ref>, Faster R-CNN <ref type="bibr" target="#b17">[18]</ref>, R-FCN <ref type="bibr" target="#b64">[65]</ref>, FPN <ref type="bibr" target="#b65">[66]</ref> and Mask R-CNN <ref type="bibr" target="#b66">[67]</ref>, some of which are correlated with each other (e.g. SPP-net modifies R-CNN with a SPP layer). The regression/classification based methods mainly includes MultiBox <ref type="bibr" target="#b67">[68]</ref>, AttentionNet <ref type="bibr" target="#b68">[69]</ref>, G-CNN <ref type="bibr" target="#b69">[70]</ref>, YOLO <ref type="bibr" target="#b16">[17]</ref>, SSD <ref type="bibr" target="#b70">[71]</ref>, YOLOv2 <ref type="bibr" target="#b71">[72]</ref>, DSSD <ref type="bibr" target="#b72">[73]</ref> and DSOD <ref type="bibr" target="#b73">[74]</ref>. The correlations between these two pipelines are bridged by the anchors introduced in Faster R-CNN. Details of these methods are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Region Proposal Based Framework</head><p>The region proposal based framework, a two-step process, matches the attentional mechanism of human brain to some extent, which gives a coarse scan of the whole scenario firstly and then focuses on regions of interest. Among the pre-related works <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, the most representative one is Overfeat <ref type="bibr" target="#b43">[44]</ref>. This model inserts CNN into sliding window method, which predicts bounding boxes directly from locations of the topmost feature map after obtaining the confidences of underlying object categories.</p><p>1) R-CNN: It is of significance to improve the quality of candidate bounding boxes and to take a deep architecture to extract high-level features. To solve these problems, R-CNN <ref type="bibr" target="#b14">[15]</ref> was proposed by Ross Girshick in 2014 and obtained a mean average precision (mAP) of 53.3% with more than 30% improvement over the previous best result (DPM HSC <ref type="bibr" target="#b76">[77]</ref>) on PASCAL VOC 2012. Figure <ref type="figure">3</ref> shows the flowchart of R-CNN, which can be divided into three stages as follows. Region proposal generation. The R-CNN adopts selective search <ref type="bibr" target="#b77">[78]</ref> to generate about 2k region proposals for each image. The selective search method relies on simple bottom-up grouping and saliency cues to provide more accurate candidate boxes of arbitrary sizes quickly and to reduce the searching space in object detection <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b38">[39]</ref>. CNN based deep feature extraction. In this stage, each region proposal is warped or cropped into a fixed resolution and the CNN module in <ref type="bibr" target="#b5">[6]</ref> is utilized to extract a 4096dimensional feature as the final representation. Due to large learning capacity, dominant expressive power and hierarchical structure of CNNs, a high-level, semantic and robust feature representation for each region proposal can be obtained. Classification and localization. With pre-trained categoryspecific linear SVMs for multiple classes, different region proposals are scored on a set of positive regions and background (negative) regions. The scored regions are then adjusted with  <ref type="bibr" target="#b63">[64]</ref>, FRCN: Faster R-CNN <ref type="bibr" target="#b15">[16]</ref>, RPN: Region Proposal Network <ref type="bibr" target="#b17">[18]</ref>, FCN: Fully Convolutional Network <ref type="bibr" target="#b64">[65]</ref>, BN: Batch Normalization <ref type="bibr" target="#b42">[43]</ref>, Deconv layers: Deconvolution layers <ref type="bibr" target="#b53">[54]</ref> .</p><p>urate object detection and semantic segmentation onahue 1,2 Trevor Darrell   inspired hierarchical and shift-invariant model for pattern recognition, was an early attempt at just such a process. The neocognitron, however, lacked a supervised training algorithm. LeCun et al. <ref type="bibr" target="#b22">[23]</ref> provided the missing algorithm by showing that stochastic gradient descent, via backpropagation, can train convolutional neural networks (CNNs), a class of models that extend the neocognitron. CNNs saw heavy use in the 1990s (e.g., <ref type="bibr" target="#b23">[24]</ref>), but then fell out of fashion, particularly in computer vision, with the rise of support vector machines. In 2012, Krizhevsky et al. <ref type="bibr" target="#b21">[22]</ref> rekindled interest in CNNs by showing substantially higher image classification accuracy on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Their success resulted from training a large CNN on 1.2 million labeled images, together with a few twists on Le-Cun's CNN (e.g., max(x, 0) rectifying non-linearities and "dropout" regularization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-CNN: Regions with CNN features</head><p>The significance of the ImageNet result was vigorously debated during the ILSVRC 2012 workshop. The central issue can be distilled to the following: To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?</p><p>We answer this question decisively by bridging the chasm between image classification and object detection. This paper is the first to show that a CNN can lead to dra-1 Fig. <ref type="figure">3</ref>. The flowchart of R-CNN <ref type="bibr" target="#b14">[15]</ref>, which consists of 3 stages: (1) extracts bottom-up region proposals, (2) computes features for each proposal using a CNN, and then (3) classifies each region with class-specific linear SVMs.</p><p>bounding box regression and filtered with a greedy nonmaximum suppression (NMS) to produce final bounding boxes for preserved object locations.</p><p>When there are scarce or insufficient labeled data, pretraining is usually conducted. Instead of unsupervised pretraining <ref type="bibr" target="#b78">[79]</ref>, R-CNN firstly conducts supervised pre-training on ILSVRC, a very large auxiliary dataset, and then takes a domain-specific fine-tuning. This scheme has been adopted by most of subsequent approaches <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>In spite of its improvements over traditional methods and significance in bringing CNN into practical object detection, there are still some disadvantages.</p><p>• Due to the existence of FC layers, the CNN requires a fixed-size (e.g., 227×227) input image, which directly leads to the re-computation of the whole CNN for each evaluated region, taking a great deal of time in the testing period.</p><p>• Training of R-CNN is a multi-stage pipeline. At first, a convolutional network (ConvNet) on object proposals is fine-tuned. Then the softmax classifier learned by finetuning is replaced by SVMs to fit in with ConvNet features. Finally, bounding-box regressors are trained.</p><p>• Training is expensive in space and time. Features are extracted from different region proposals and stored on the disk. It will take a long time to process a relatively small training set with very deep networks, such as VGG16. At the same time, the storage memory required by these features should also be a matter of concern.</p><p>• Although selective search can generate region proposals with relatively high recalls, the obtained region proposals are still redundant and this procedure is time-consuming (around 2 seconds to extract 2k region proposals).</p><p>To solve these problems, many methods have been proposed. GOP <ref type="bibr" target="#b79">[80]</ref> takes a much faster geodesic based segmentation to replace traditional graph cuts. MCG <ref type="bibr" target="#b80">[81]</ref> searches different scales of the image for multiple hierarchical segmentations and combinatorially groups different regions to produce proposals. Instead of extracting visually distinct segments, the edge boxes method <ref type="bibr" target="#b81">[82]</ref> adopts the idea that objects are more likely to exist in bounding boxes with fewer contours straggling their boundaries. Also some researches tried to re-rank or refine pre-extracted region proposals to remove unnecessary ones and obtained a limited number of valuable ones, such as DeepBox <ref type="bibr" target="#b82">[83]</ref> and SharpMask <ref type="bibr" target="#b83">[84]</ref>.</p><p>In addition, there are some improvements to solve the problem of inaccurate localization. Zhang et al. <ref type="bibr" target="#b84">[85]</ref> utilized a bayesian optimization based search algorithm to guide the regressions of different bounding boxes sequentially, and trained class-specific CNN classifiers with a structured loss to penalize the localization inaccuracy explicitly. Saurabh Gupta et al. improved object detection for RGB-D images with semantically rich image and depth features <ref type="bibr" target="#b85">[86]</ref>, and learned a new geocentric embedding for depth images to encode each pixel. The combination of object detectors and superpixel classification framework gains a promising result on semantic scene segmentation task. Ouyang et al. proposed a deformable deep CNN (DeepID-Net) <ref type="bibr" target="#b86">[87]</ref> which introduces a novel deformation constrained pooling (def-pooling) layer to impose geometric penalty on the deformation of various object parts and makes an ensemble of models with different settings. Lenc et al. <ref type="bibr" target="#b87">[88]</ref> provided an analysis on the role of proposal generation in CNN-based detectors and tried to replace this stage with a constant and trivial region generation scheme. The goal is achieved by biasing sampling to match the statistics of the ground truth bounding boxes with K-means clustering. However, more candidate boxes are required to achieve comparable results to those of R-CNN.</p><p>2) SPP-net: FC layers must take a fixed-size input. That's why R-CNN chooses to warp or crop each region proposal into the same size. However, the object may exist partly in the cropped region and unwanted geometric distortion may be produced due to the warping operation. These content losses or distortions will reduce recognition accuracy, especially when the scales of objects vary.</p><p>To solve this problem, He et al. took the theory of spatial pyramid matching (SPM) <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref> into consideration and proposed a novel CNN architecture named SPP-net <ref type="bibr" target="#b63">[64]</ref>. SPM takes several finer to coarser scales to partition the image into a number of divisions and aggregates quantized local features into mid-level representations.</p><p>The architecture of SPP-net for object detection can be found in Figure <ref type="figure">4</ref>. Different from R-CNN, SPP-net reuses feature maps of the 5-th conv layer (conv5) to project region </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OBJECT DETECTION</head><p>been used for object detection. e recent state-of-the-art R-CNN first extracts about 2,000 candieach image via selective search egion in each window is warped 27). A pre-trained deep network he feature of each window. A is then trained on these features generates results of compelling ally outperforms previous methe R-CNN repeatedly applies the etwork to about 2,000 windows consuming. Feature extraction is tleneck in testing. lso be used for object detection. re maps from the entire image t multiple scales). Then we apmid pooling on each candidate re maps to pool a fixed-length window (see Figure <ref type="figure" target="#fig_4">5</ref>). Because convolutions are only applied run orders of magnitude faster. cts window-wise features from e maps, while R-CNN extracts regions. In previous works, the del (DPM) <ref type="bibr" target="#b22">[23]</ref> extracts features OG <ref type="bibr" target="#b23">[24]</ref> feature maps, and the method <ref type="bibr" target="#b19">[20]</ref> extracts from win-FT feature maps. The Overfeat also extracts from windows of eature maps, but needs to preze. On the contrary, our method ction in arbitrary windows from al feature maps.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Detection Algorithm</head><p>We use the "fast" mode of selective search <ref type="bibr" target="#b19">[20]</ref> to generate about 2,000 candidate windows per image.</p><p>Then we resize the image such that min(w, h) = s, and extract the feature maps from the entire image.</p><p>We use the SPP-net model of ZF-5 (single-size trained) for the time being. In each candidate window, we use a 4-level spatial pyramid (1×1, 2×2, 3×3, 6×6, totally 50 bins) to pool the features. This generates a 12,800d (256×50) representation for each window. These representations are provided to the fully-connected layers of the network. Then we train a binary linear SVM classifier for each category on these features. Our implementation of the SVM training follows <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b6">[7]</ref>. We use the ground-truth windows to generate the positive samples. The negative samples are those overlapping a positive window by at most 30% (measured by the intersection-over-union (IoU) ratio). Any negative sample is removed if it overlaps another negative sample by more than 70%. We apply the standard hard negative mining <ref type="bibr" target="#b22">[23]</ref> to train the SVM. This step is iterated once. It takes less than 1 hour to train SVMs for all 20 categories. In testing, the classifier is used to score the candidate windows. Then we use non-maximum suppression <ref type="bibr" target="#b22">[23]</ref> (threshold of 30%) on the scored windows.</p><p>Our method can be improved by multi-scale feature extraction. We resize the image such that min(w, h) = s ∈ S = {480, 576, 688, 864, 1200}, and compute the feature maps of conv5 for each scale. One strategy of combining the features from these scales is to pool them channel-by-channel. But we empirically find that another strategy provides better results. For each candidate window, we choose a single scale s ∈ S such that the scaled candidate window has a number of pixels closest to 224×224. Then we only use the feature maps extracted from this scale to compute Fig. <ref type="figure">4</ref>. The architecture of SPP-net for object detection <ref type="bibr" target="#b63">[64]</ref>.  RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets <ref type="bibr" target="#b10">[11]</ref> in which there is only one pyramid level. We use the pooling sub-window calculation given in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Initializing from pre-trained networks</head><p>We experiment with three pre-trained ImageNet <ref type="bibr" target="#b3">[4]</ref> networks, each with five max pooling layers and between five and thirteen conv layers (see Section 4.1 for network details). When a pre-trained network initializes a Fast R-CNN network, it undergoes three transformations.</p><p>First, the last max pooling layer is replaced by a RoI pooling layer that is configured by setting H and W to be compatible with the net's first fully connected layer (e.g., H = W = 7 for VGG16).</p><p>Second, the network's last fully connected layer and softmax (which were trained for 1000-way ImageNet classification) are replaced with the two sibling layers described earlier (a fully connected layer and softmax over K + 1 categories and category-specific bounding-box regressors).</p><p>Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Fine-tuning for detection</head><p>Training all network weights with back-propagation is an important capability of Fast R-CNN. First, let's elucidate why SPPnet is unable to update weights below the spatial pyramid pooling layer.</p><p>The root cause is that back-propagation through the SPP layer is highly inefficient when each training sample (i.e. RoI) comes from a different image, which is exactly how R-CNN and SPPnet networks are trained. The inefficiency 1441 Fig. <ref type="figure" target="#fig_4">5</ref>. The architecture of Fast R-CNN <ref type="bibr" target="#b15">[16]</ref>.</p><p>proposals of arbitrary sizes to fixed-length feature vectors. The feasibility of the reusability of these feature maps is due to the fact that the feature maps not only involve the strength of local responses, but also have relationships with their spatial positions <ref type="bibr" target="#b63">[64]</ref>. The layer after the final conv layer is referred to as spatial pyramid pooling layer (SPP layer). If the number of feature maps in conv5 is 256, taking a 3-level pyramid, the final feature vector for each region proposal obtained after SPP layer has a dimension of 256 × (1 2 + 2 2 + 4 2 ) = 5376.</p><p>SPP-net not only gains better results with correct estimation of different region proposals in their corresponding scales, but also improves detection efficiency in testing period with the sharing of computation cost before SPP layer among different proposals.</p><p>3) Fast R-CNN: Although SPP-net has achieved impressive improvements in both accuracy and efficiency over R-CNN, it still has some notable drawbacks. SPP-net takes almost the same multi-stage pipeline as R-CNN, including feature extraction, network fine-tuning, SVM training and boundingbox regressor fitting. So an additional expense on storage space is still required. Additionally, the conv layers preceding the SPP layer cannot be updated with the fine-tuning algorithm introduced in <ref type="bibr" target="#b63">[64]</ref>. As a result, an accuracy drop of very deep networks is unsurprising. To this end, Girshick <ref type="bibr" target="#b15">[16]</ref> introduced a multi-task loss on classification and bounding box regression and proposed a novel CNN architecture named Fast R-CNN.</p><p>The architecture of Fast R-CNN is exhibited in Figure <ref type="figure" target="#fig_4">5</ref>. Similar to SPP-net, the whole image is processed with conv layers to produce feature maps. Then, a fixed-length feature vector is extracted from each region proposal with a region of interest (RoI) pooling layer. The RoI pooling layer is a special case of the SPP layer, which has only one pyramid level. Each feature vector is then fed into a sequence of FC layers before finally branching into two sibling output layers. One output layer is responsible for producing softmax probabilities for all C + 1 categories (C object classes plus one 'background' class) and the other output layer encodes refined boundingbox positions with four real-valued numbers. All parameters in these procedures (except the generation of region proposals) are optimized via a multi-task loss in an end-to-end way.</p><p>The multi-tasks loss L is defined as below to jointly train classification and bounding-box regression,</p><formula xml:id="formula_0">L(p, u, t u , v) = L cls (p, u) + λ[u ≥ 1]L loc (t u , v)<label>(1)</label></formula><p>where L cls (p, u) = − log p u calculates the log loss for ground truth class u and p u is driven from the discrete probability distribution p = (p 0 , • • • , p C ) over the C + 1 outputs from the last FC layer. L loc (t u , v) is defined over the predicted offsets t u = (t u x , t u y , t u w , t u h ) and ground-truth bounding-box regression targets v = (v x , v y , v w , v h ), where x, y, w, h denote the two coordinates of the box center, width, and height, respectively. Each t u adopts the parameter settings in <ref type="bibr" target="#b14">[15]</ref> to specify an object proposal with a log-space height/width shift and scaleinvariant translation. The Iverson bracket indicator function [u ≥ 1] is employed to omit all background RoIs. To provide more robustness against outliers and eliminate the sensitivity in exploding gradients, a smooth L 1 loss is adopted to fit bounding-box regressors as below</p><formula xml:id="formula_1">L loc (t u , v) = i∈x,y,w,h smooth L1 (t u i − v i )<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">smooth L1 (x) = 0.5x 2 if |x| &lt; 1 |x| − 0.5 otherwise<label>(3)</label></formula><p>To accelerate the pipeline of Fast R-CNN, another two tricks are of necessity. On one hand, if training samples (i.e. RoIs) come from different images, back-propagation through the SPP layer becomes highly inefficient. Fast R-CNN samples mini-batches hierarchically, namely N images sampled randomly at first and then R/N RoIs sampled in each image, where R represents the number of RoIs. Critically, computation and memory are shared by RoIs from the same image in the forward and backward pass. On the other hand, much time is spent in computing the FC layers during the forward pass <ref type="bibr" target="#b15">[16]</ref>. The truncated Singular Value Decomposition (SVD) <ref type="bibr" target="#b90">[91]</ref> can be utilized to compress large FC layers and to accelerate the testing procedure.</p><p>In the Fast R-CNN, regardless of region proposal generation, the training of all network layers can be processed in a single-stage with a multi-task loss. It saves the additional expense on storage space, and improves both accuracy and efficiency with more reasonable training schemes.</p><p>4) Faster R-CNN: Despite the attempt to generate candidate boxes with biased sampling <ref type="bibr" target="#b87">[88]</ref>, state-of-the-art object detection networks mainly rely on additional methods, such as selective search and Edgebox, to generate a candidate pool of isolated region proposals. Region proposal computation is also a bottleneck in improving efficiency. To solve this problem, Ren et al. introduced an additional Region Proposal Network (RPN) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b91">[92]</ref>, which acts in a nearly cost-free way by sharing full-image conv features with detection network.</p><p>RPN is achieved with a fully-convolutional network, which has the ability to predict object bounds and scores at each position simultaneously. Similar to <ref type="bibr" target="#b77">[78]</ref>, RPN takes an image of arbitrary size to generate a set of rectangular object proposals. RPN operates on a specific conv layer with the preceding layers shared with object detection network. feature map. Each sliding window is mapped to a lower-dimensional vector (256-d for ZF and 512-d for VGG). This vector is fed into two sibling fully-connected layers-a box-regression layer (reg) and a box-classification layer (cls). We use n = 3 in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mininetwork is illustrated at a single position in Fig. <ref type="figure" target="#fig_0">1</ref> (left). Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally implemented with an n × n conv layer followed by two sibling 1 × 1 conv layers (for reg and cls, respectively). ReLUs <ref type="bibr" target="#b14">[15]</ref> are applied to the output of the n × n conv layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translation-Invariant Anchors</head><p>At each sliding-window location, we simultaneously predict k region proposals, so the reg layer has 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate probability of object / not-object for each proposal. 2 The k proposals are parameterized relative to k reference boxes, called anchors. Each anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio. We use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a conv feature map of a size W × H (typically ∼2,400), there are W H k anchors in total. An important property of our approach is that it is translation invariant, both in terms of the anchors and the functions that compute proposals relative to the anchors.</p><p>As a comparison, the MultiBox method <ref type="bibr" target="#b19">[20]</ref> uses k-means to generate 800 anchors, which are not translation invariant. If one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location. Moreover, because the MultiBox anchors are not translation invariant, it requires a (4+1)×800-dimensional output layer, whereas our method requires a (4+2)×9-dimensional output layer. Our proposal layers have an order of magnitude fewer parameters (27 million for MultiBox using GoogLeNet <ref type="bibr" target="#b19">[20]</ref> vs. 2.4 million for RPN using VGG-16), and thus have less risk of overfitting on small datasets, like PASCAL VOC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Loss Function for Learning Region Proposals</head><p>For training RPNs, we assign a binary class label (of being an object or not) to each anchor. We assign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersectionover-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors. We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective.</p><p>With these definitions, we minimize an objective function following the multi-task loss in Fast R-CNN <ref type="bibr" target="#b4">[5]</ref>. Our loss function for an image is defined as:</p><formula xml:id="formula_3">L({p i }, {t i }) = 1 N cls i L cls (p i , p * i ) + λ 1 N reg i p * i L reg (t i , t * i ).<label>(1)</label></formula><p>2 For simplicity we implement the cls layer as a two-class softmax layer. Alternatively, one may use logistic regression to produce k scores.</p><p>3 Fig. <ref type="figure">6</ref>. The RPN in Faster R-CNN <ref type="bibr" target="#b17">[18]</ref>. K predefined anchor boxes are convoluted with each sliding window to produce fixed-length vectors which are taken by cls and reg layer to obtain corresponding outputs.</p><p>The architecture of RPN is shown in Figure <ref type="figure">6</ref>. The network slides over the conv feature map and fully connects to an n × n spatial window. A low dimensional vector (512-d for VGG16) is obtained in each sliding window and fed into two sibling FC layers, namely box-classification layer (cls) and box-regression layer (reg). This architecture is implemented with an n × n conv layer followed by two sibling 1 × 1 conv layers. To increase non-linearity, ReLU is applied to the output of the n × n conv layer.</p><p>The regressions towards true bounding boxes are achieved by comparing proposals relative to reference boxes (anchors). In the Faster R-CNN, anchors of 3 scales and 3 aspect ratios are adopted. The loss function is similar to <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_4">L(p i , t i ) = 1 N cls i L cls (p i , p * i ) + λ 1 N reg i p * i L reg (t i , t * i )<label>(4</label></formula><p>) where p i shows the predicted probability of the i-th anchor being an object. The ground truth label p * i is 1 if the anchor is positive, otherwise 0. t i stores 4 parameterized coordinates of the predicted bounding box while t * i is related to the groundtruth box overlapping with a positive anchor. L cls is a binary log loss and L reg is a smoothed L 1 loss similar to <ref type="bibr" target="#b1">(2)</ref>. These two terms are normalized with the mini-batch size (N cls ) and the number of anchor locations (N reg ), respectively. In the form of fully-convolutional networks, Faster R-CNN can be trained end-to-end by back-propagation and SGD in an alternate training manner.</p><p>With the proposal of Faster R-CNN, region proposal based CNN architectures for object detection can really be trained in an end-to-end way. Also a frame rate of 5 FPS (Frame Per Second) on a GPU is achieved with state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012. However, the alternate training algorithm is very time-consuming and RPN produces object-like regions (including backgrounds) instead of object instances and is not skilled in dealing with objects with extreme scales or shapes.</p><p>5) R-FCN: Divided by the RoI pooling layer, a prevalent family <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> of deep networks for object detection are composed of two subnetworks: a shared fully convolutional subnetwork (independent of RoIs) and an unshared RoI-wise subnetwork. This decomposition originates from pioneering classification architectures (e.g. AlexNet <ref type="bibr" target="#b5">[6]</ref> and VGG16 <ref type="bibr" target="#b45">[46]</ref>) which consist of a convolutional subnetwork and several FC layers separated by a specific spatial pooling layer.</p><p>Recent state-of-the-art image classification networks, such as Residual Nets (ResNets) <ref type="bibr" target="#b46">[47]</ref> and GoogLeNets <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b92">[93]</ref>, are fully convolutional. To adapt to these architectures, it's 1 Facebook AI Research (FAIR) 2 Cornell University and Cornell Tech Abstract Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing objects at vastly different scales is a fundamental challenge in computer vision. Feature pyramids built upon image pyramids (for short we call these featurized image pyramids) form the basis of a standard solution <ref type="bibr" target="#b0">[1]</ref> (Fig. <ref type="figure" target="#fig_11">1(a)</ref>). These pyramids are scale-invariant in the sense that an object's scale change is offset by shifting its level in the pyramid. Intuitively, this property enables a model to detect objects across a large range of scales by scanning the model over both positions and pyramid levels.</p><p>Featurized image pyramids were heavily used in the era of hand-engineered features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>. They were so critical that object detectors like DPM <ref type="bibr" target="#b6">[7]</ref> required dense scale sampling to achieve good results (e.g., 10 scales per octave). For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (ConvNets) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Aside from being capable of representing higher-level semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref> (Fig. <ref type="figure" target="#fig_11">1(b)</ref>). But even with this robustness, pyramids are still needed to get the most accurate results. All recent top entries in the ImageNet <ref type="bibr" target="#b32">[33]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref> detection challenges use multi-scale testing on featurized image pyramids (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref>). The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.</p><p>Nevertheless, featurizing each level of an image pyramid has obvious limitations. Inference time increases considerably (e.g., by four times <ref type="bibr" target="#b10">[11]</ref>), making this approach impractical for real applications.  natural to construct a fully convolutional object detection network without RoI-wise subnetwork. However, it turns out to be inferior with such a naive solution <ref type="bibr" target="#b46">[47]</ref>. This inconsistence is due to the dilemma of respecting translation variance in object detection compared with increasing translation invariance in image classification. In other words, shifting an object inside an image should be indiscriminative in image classification while any translation of an object in a bounding box may be meaningful in object detection. A manual insertion of the RoI pooling layer into convolutions can break down translation invariance at the expense of additional unshared region-wise layers. So Li et al. <ref type="bibr" target="#b64">[65]</ref> proposed a region-based fully convolutional networks (R-FCN, Fig. <ref type="figure" target="#fig_1">S2</ref>).</p><p>Different from Faster R-CNN, for each category, the last conv layer of R-FCN produces a total of k 2 position-sensitive score maps with a fixed grid of k × k firstly and a positionsensitive RoI pooling layer is then appended to aggregate the responses from these score maps. Finally, in each RoI, k 2 position-sensitive scores are averaged to produce a C + 1-d vector and softmax responses across categories are computed. Another 4k 2 -d conv layer is appended to obtain class-agnostic bounding boxes.</p><p>With R-FCN, more powerful classification networks can be adopted to accomplish object detection in a fully-convolutional architecture by sharing nearly all the layers, and state-of-theart results are obtained on both PASCAL VOC and Microsoft COCO <ref type="bibr" target="#b93">[94]</ref> datasets at a test speed of 170ms per image.</p><p>6) FPN: Feature pyramids built upon image pyramids (featurized image pyramids) have been widely applied in many object detection systems to improve scale invariance <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b63">[64]</ref> (Figure <ref type="figure" target="#fig_9">7</ref>(a)). However, training time and memory consumption increase rapidly. To this end, some techniques take only a single input scale to represent high-level semantics and increase the robustness to scale changes (Figure <ref type="figure" target="#fig_9">7</ref>(b)), and image pyramids are built at test time which results in an inconsistency between train/test-time inferences <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The in-network feature hierarchy in a deep ConvNet produces feature maps of different spatial resolutions while introduces large semantic gaps caused by different depths (Figure <ref type="figure" target="#fig_9">7(c)</ref>). To avoid using low-level features, pioneer works <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b94">[95]</ref> usually build the pyramid starting from middle layers or just sum transformed feature responses, missing the higher-Fig. <ref type="figure">8</ref>. The Mask R-CNN framework for instance segmentation <ref type="bibr" target="#b66">[67]</ref>.</p><p>resolution maps of the feature hierarchy.</p><p>Different from these approaches, FPN <ref type="bibr" target="#b65">[66]</ref> holds an architecture with a bottom-up pathway, a top-down pathway and several lateral connections to combine low-resolution and semantically strong features with high-resolution and semantically weak features (Figure <ref type="figure" target="#fig_9">7(d)</ref>). The bottom-up pathway, which is the basic forward backbone ConvNet, produces a feature hierarchy by downsampling the corresponding feature maps with a stride of 2. The layers owning the same size of output maps are grouped into the same network stage and the output of the last layer of each stage is chosen as the reference set of feature maps to build the following top-down pathway.</p><p>To build the top-down pathway, feature maps from higher network stages are upsampled at first and then enhanced with those of the same spatial size from the bottom-up pathway via lateral connections. A 1 × 1 conv layer is appended to the upsampled map to reduce channel dimensions and the mergence is achieved by element-wise addition. Finally, a 3×3 convolution is also appended to each merged map to reduce the aliasing effect of upsampling and the final feature map is generated. This process is iterated until the finest resolution map is generated.</p><p>As feature pyramid can extract rich semantics from all levels and be trained end-to-end with all scales, state-of-theart representation can be obtained without sacrificing speed and memory. Meanwhile, FPN is independent of the backbone CNN architectures and can be applied to different stages of object detection (e.g. region proposal generation) and to many other computer vision tasks (e.g. instance segmentation).</p><p>7) Mask R-CNN: Instance segmentation <ref type="bibr" target="#b95">[96]</ref> is a challenging task which requires detecting all objects in an image and segmenting each instance (semantic segmentation <ref type="bibr" target="#b96">[97]</ref>). These two tasks are usually regarded as two independent processes. And the multi-task scheme will create spurious edge and exhibit systematic errors on overlapping instances <ref type="bibr" target="#b97">[98]</ref>. To solve this problem, parallel to the existing branches in Faster R-CNN for classification and bounding box regression, the Mask R-CNN <ref type="bibr" target="#b66">[67]</ref> adds a branch to predict segmentation masks in a pixel-to-pixel manner (Figure <ref type="figure">8</ref>).</p><p>Different from the other two branches which are inevitably collapsed into short output vectors by FC layers, the segmentation mask branch encodes an m × m mask to maintain the explicit object spatial layout. This kind of fully convolutional representation requires fewer parameters but is more accurate than that of <ref type="bibr" target="#b96">[97]</ref>. Formally, besides the two losses in (1) for classification and bounding box regression, an additional loss for segmentation mask branch is defined to reach a multi-task loss. An this loss is only associated with ground-truth class and relies on the classification branch to predict the category.</p><p>Because RoI pooling, the core operation in Faster R-CNN, performs a coarse spatial quantization for feature extraction, misalignment is introduced between the RoI and the features. It affects classification little because of its robustness to small translations. However, it has a large negative effect on pixelto-pixel mask prediction. To solve this problem, Mask R-CNN adopts a simple and quantization-free layer, namely RoIAlign, to preserve the explicit per-pixel spatial correspondence faithfully. RoIAlign is achieved by replacing the harsh quantization of RoI pooling with bilinear interpolation <ref type="bibr" target="#b98">[99]</ref>, computing the exact values of the input features at four regularly sampled locations in each RoI bin. In spite of its simplicity, this seemingly minor change improves mask accuracy greatly, especially under strict localization metrics.</p><p>Given the Faster R-CNN framework, the mask branch only adds a small computational burden and its cooperation with other tasks provides complementary information for object detection. As a result, Mask R-CNN is simple to implement with promising instance segmentation and object detection results. In a word, Mask R-CNN is a flexible and efficient framework for instance-level recognition, which can be easily generalized to other tasks (e.g. human pose estimation <ref type="bibr" target="#b6">[7]</ref>[S4]) with minimal modification.</p><p>8) Multi-task Learning, Multi-scale Representation and Contextual Modelling: Although the Faster R-CNN gets promising results with several hundred proposals, it still struggles in small-size object detection and localization, mainly due to the coarseness of its feature maps and limited information provided in particular candidate boxes. The phenomenon is more obvious on the Microsoft COCO dataset which consists of objects at a broad range of scales, less prototypical images, and requires more precise localization. To tackle these problems, it is of necessity to accomplish object detection with multi-task learning <ref type="bibr" target="#b99">[100]</ref>, multi-scale representation <ref type="bibr" target="#b94">[95]</ref> and context modelling <ref type="bibr" target="#b100">[101]</ref> to combine complementary information from multiple sources.</p><p>Multi-task Learning learns a useful representation for multiple correlated tasks from the same input <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b102">[103]</ref>. Brahmbhatt et al. introduced conv features trained for object segmentation and 'stuff' (amorphous categories such as ground and water) to guide accurate object detection of small objects (StuffNet) <ref type="bibr" target="#b99">[100]</ref>. Dai et al. <ref type="bibr" target="#b96">[97]</ref> presented Multitask Network Cascades of three networks, namely class-agnostic region proposal generation, pixel-level instance segmentation and regional instance classification. Li et al. incorporated the weakly-supervised object segmentation cues and region-based object detection into a multi-stage architecture to fully exploit the learned segmentation features <ref type="bibr" target="#b103">[104]</ref>.</p><p>Multi-scale Representation combines activations from multiple layers with skip-layer connections to provide semantic information of different spatial resolutions <ref type="bibr" target="#b65">[66]</ref>. Cai et al. proposed the MS-CNN <ref type="bibr" target="#b104">[105]</ref> to ease the inconsistency between the sizes of objects and receptive fields with multiple scale-independent output layers. Yang et al. investigated two strategies, namely scale-dependent pooling (SDP) and layerwise cascaded rejection classifiers (CRC), to exploit appropriate scale-dependent conv features <ref type="bibr" target="#b32">[33]</ref>. <ref type="bibr">Kong et al.</ref> proposed the HyperNet to calculate the shared features between RPN and object detection network by aggregating and compressing hierarchical feature maps from different resolutions into a uniform space <ref type="bibr" target="#b100">[101]</ref>.</p><p>Contextual Modelling improves detection performance by exploiting features from or around RoIs of different support regions and resolutions to deal with occlusions and local similarities <ref type="bibr" target="#b94">[95]</ref>. <ref type="bibr">Zhu et al.</ref> proposed the SegDeepM to exploit object segmentation which reduces the dependency on initial candidate boxes with Markov Random Field <ref type="bibr" target="#b105">[106]</ref>. Moysset et al. took advantage of 4 directional 2D-LSTMs <ref type="bibr" target="#b106">[107]</ref> to convey global context between different local regions and reduced trainable parameters with local parameter-sharing <ref type="bibr" target="#b107">[108]</ref>. Zeng et al. proposed a novel GBD-Net by introducing gated functions to control message transmission between different support regions <ref type="bibr" target="#b108">[109]</ref>.</p><p>The Combination incorporates different components above into the same model to improve detection performance further. Gidaris et al. proposed the Multi-Region CNN (MR-CNN) model <ref type="bibr" target="#b109">[110]</ref> to capture different aspects of an object, the distinct appearances of various object parts and semantic segmentation-aware features. To obtain contextual and multiscale representations, Bell et al. proposed the Inside-Outside Net (ION) by exploiting information both inside and outside the RoI <ref type="bibr" target="#b94">[95]</ref> with spatial recurrent neural networks <ref type="bibr" target="#b110">[111]</ref> and skip pooling <ref type="bibr" target="#b100">[101]</ref>. Zagoruyko et al. proposed the MultiPath architecture by introducing three modifications to the Fast R-CNN <ref type="bibr" target="#b111">[112]</ref>, including multi-scale skip connections <ref type="bibr" target="#b94">[95]</ref>, a modified foveal structure <ref type="bibr" target="#b109">[110]</ref> and a novel loss function summing different IoU losses. 9) Thinking in Deep Learning based Object Detection: Apart from the above approaches, there are still many important factors for continued progress.</p><p>There is a large imbalance between the number of annotated objects and background examples. To address this problem, Shrivastava et al. proposed an effective online mining algorithm (OHEM) <ref type="bibr" target="#b112">[113]</ref> for automatic selection of the hard examples, which leads to a more effective and efficient training.</p><p>Instead of concentrating on feature extraction, Ren et al. made a detailed analysis on object classifiers <ref type="bibr" target="#b113">[114]</ref>, and found that it is of particular importance for object detection to construct a deep and convolutional per-region classifier carefully, especially for ResNets <ref type="bibr" target="#b46">[47]</ref> and GoogLeNets <ref type="bibr" target="#b44">[45]</ref>.</p><p>Traditional CNN framework for object detection is not skilled in handling significant scale variation, occlusion or truncation, especially when only 2D object detection is involved. To address this problem, Xiang et al. proposed a novel subcategory-aware region proposal network <ref type="bibr" target="#b59">[60]</ref>, which guides the generation of region proposals with subcategory information related to object poses and jointly optimize object detection and subcategory classification.</p><p>Ouyang et al. found that the samples from different classes follow a longtailed distribution <ref type="bibr" target="#b114">[115]</ref>, which indicates that different classes with distinct numbers of samples have different degrees of impacts on feature learning. To this end, objects are firstly clustered into visually similar class groups, and then a hierarchical feature learning scheme is adopted to learn deep representations for each group separately.</p><p>In order to minimize computational cost and achieve the state-of-the-art performance, with the 'deep and thin' design principle and following the pipeline of Fast R-CNN, Hong et al. proposed the architecture of PVANET <ref type="bibr" target="#b115">[116]</ref>, which adopts some building blocks including concatenated ReLU <ref type="bibr" target="#b116">[117]</ref>, Inception <ref type="bibr" target="#b44">[45]</ref>, and HyperNet <ref type="bibr" target="#b100">[101]</ref> to reduce the expense on multi-scale feature extraction and trains the network with batch normalization <ref type="bibr" target="#b42">[43]</ref>, residual connections <ref type="bibr" target="#b46">[47]</ref>, and learning rate scheduling based on plateau detection <ref type="bibr" target="#b46">[47]</ref>. The PVANET achieves the state-of-the-art performance and can be processed in real time on Titan X GPU (21 FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Regression/Classification Based Framework</head><p>Region proposal based frameworks are composed of several correlated stages, including region proposal generation, feature extraction with CNN, classification and bounding box regression, which are usually trained separately. Even in recent end-to-end module Faster R-CNN, an alternative training is still required to obtain shared convolution parameters between RPN and detection network. As a result, the time spent in handling different components becomes the bottleneck in realtime application.</p><p>One-step frameworks based on global regression/classification, mapping straightly from image pixels to bounding box coordinates and class probabilities, can reduce time expense. We firstly reviews some pioneer CNN models, and then focus on two significant frameworks, namely You only look once (YOLO) <ref type="bibr" target="#b16">[17]</ref> and Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b70">[71]</ref>.</p><p>1) Pioneer Works: Previous to YOLO and SSD, many researchers have already tried to model object detection as a regression or classification task.</p><p>Szegedy et al. formulated object detection task as a DNNbased regression <ref type="bibr" target="#b117">[118]</ref>, generating a binary mask for the test image and extracting detections with a simple bounding box inference. However, the model has difficulty in handling overlapping objects, and bounding boxes generated by direct upsampling is far from perfect.</p><p>Pinheiro et al. proposed a CNN model with two branches: one generates class agnostic segmentation masks and the other predicts the likelihood of a given patch centered on an object <ref type="bibr" target="#b118">[119]</ref>. Inference is efficient since class scores and segmentation can be obtained in a single model with most of the CNN operations shared.</p><p>Erhan et al. proposed regression based MultiBox to produce scored class-agnostic region proposals <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b119">[120]</ref>. A unified loss was introduced to bias both localization and confidences of multiple components to predict the coordinates of classagnostic bounding boxes. However, a large quantity of additional parameters are introduced to the final layer.</p><p>Yoo et al. adopted an iterative classification approach to handle object detection and proposed an impressive end-toend CNN architecture named AttentionNet <ref type="bibr" target="#b68">[69]</ref>. Starting from the top-left (TL) and bottom-right (BR) corner of an image, AttentionNet points to a target object by generating quantized weak directions and converges to an accurate object boundary box with an ensemble of iterative predictions. However, the model becomes quite inefficient when handling multiple categories with a progressive two-step procedure.</p><p>Najibi et al. proposed a proposal-free iterative grid based object detector (G-CNN), which models object detection as Fig. <ref type="figure">9</ref>. Main idea of YOLO <ref type="bibr" target="#b16">[17]</ref>.</p><p>finding a path from a fixed grid to boxes tightly surrounding the objects <ref type="bibr" target="#b69">[70]</ref>. Starting with a fixed multi-scale bounding box grid, G-CNN trains a regressor to move and scale elements of the grid towards objects iteratively. However, G-CNN has a difficulty in dealing with small or highly overlapping objects.</p><p>2) YOLO: Redmon et al. <ref type="bibr" target="#b16">[17]</ref> proposed a novel framework called YOLO, which makes use of the whole topmost feature map to predict both confidences for multiple categories and bounding boxes. The basic idea of YOLO is exhibited in Figure <ref type="figure">9</ref>. YOLO divides the input image into an S × S grid and each grid cell is responsible for predicting the object centered in that grid cell. Each grid cell predicts B bounding boxes and their corresponding confidence scores. Formally, confidence scores are defined as P r(Object) * IOU truth pred , which indicates how likely there exist objects (P r(Object) ≥ 0) and shows confidences of its prediction (IOU truth pred ). At the same time, regardless of the number of boxes, C conditional class probabilities (P r(Class i |Object)) should also be predicted in each grid cell. It should be noticed that only the contribution from the grid cell containing an object is calculated.</p><p>At test time, class-specific confidence scores for each box are achieved by multiplying the individual box confidence predictions with the conditional class probabilities as follows.</p><p>P r(Object) * IOU truth pred * P r(Class i |Object) = P r(Class i ) * IOU truth pred <ref type="bibr" target="#b4">(5)</ref> where the existing probability of class-specific objects in the box and the fitness between the predicted box and the object are both taken into consideration.</p><p>During training, the following loss function is optimized,</p><formula xml:id="formula_5">λ coord S 2 i=0 B j=0 1 obj ij (x i − xi ) 2 + (y i − ŷi ) 2 +λ coord S 2 i=0 B j=0 1 obj ij √ w i − ŵi ) 2 + ( h i − ĥi 2 + S 2 i=0 B j=0 1 obj ij C i − Ĉi 2 +λ noobj S 2 i=0 B j=0 1 noobj ij C i − Ĉi 2 + S 2 i=0 1 obj i c∈classes (p i (c) − pi (c)) 2<label>(6)</label></formula><p>In a certain cell i, (x i , y i ) denote the center of the box relative to the bounds of the grid cell, (w i , h i ) are the normalized width and height relative to the image size, C i represents confidence scores, 1 obj i indicates the existence of objects and 1 obj ij denotes that the prediction is conducted by the jth bounding box predictor. Note that only when an object is present in that grid cell, the loss function penalizes classification errors. Similarly, when the predictor is 'responsible' for the ground truth box (i.e. the highest IoU of any predictor in that grid cell is achieved), bounding box coordinate errors are penalized.</p><p>The YOLO consists of 24 conv layers and 2 FC layers, of which some conv layers construct ensembles of inception modules with 1 × 1 reduction layers followed by 3 × 3 conv layers. The network can process images in real-time at 45 FPS and a simplified version Fast YOLO can reach 155 FPS with better results than other real-time detectors. Furthermore, YOLO produces fewer false positives on background, which makes the cooperation with Fast R-CNN become possible. An improved version, YOLOv2, was later proposed in <ref type="bibr" target="#b71">[72]</ref>, which adopts several impressive strategies, such as BN, anchor boxes, dimension cluster and multi-scale training.</p><p>3) SSD: YOLO has a difficulty in dealing with small objects in groups, which is caused by strong spatial constraints imposed on bounding box predictions <ref type="bibr" target="#b16">[17]</ref>. Meanwhile, YOLO struggles to generalize to objects in new/unusual aspect ratios/ configurations and produces relatively coarse features due to multiple downsampling operations.</p><p>Aiming at these problems, Liu et al. proposed a Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b70">[71]</ref>, which was inspired by the anchors adopted in MultiBox <ref type="bibr" target="#b67">[68]</ref>, RPN <ref type="bibr" target="#b17">[18]</ref> and multi-scale representation <ref type="bibr" target="#b94">[95]</ref>. Given a specific feature map, instead of fixed grids adopted in YOLO, the SSD takes advantage of a set of default anchor boxes with different aspect ratios and scales to discretize the output space of bounding boxes. To handle objects with various sizes, the network fuses predictions from multiple feature maps with different resolutions .</p><p>The architecture of SSD is demonstrated in Figure <ref type="figure" target="#fig_10">10</ref>. Given the VGG16 backbone architecture, SSD adds several feature layers to the end of the network, which are responsible for predicting the offsets to default boxes with different scales and aspect ratios and their associated confidences. The network is trained with a weighted sum of localization loss (e.g. Smooth L1) and confidence loss (e.g. Softmax), which is similar to <ref type="bibr" target="#b0">(1)</ref>. Final detection results are obtained by conducting NMS on multi-scale refined bounding boxes.</p><p>Integrating with hard negative mining, data augmentation and a larger number of carefully chosen default anchors, SSD significantly outperforms the Faster R-CNN in terms of accuracy on PASCAL VOC and COCO, while being three times faster. The SSD300 (input image size is 300 × 300) runs at 59 FPS, which is more accurate and efficient than YOLO. However, SSD is not skilled at dealing with small objects, which can be relieved by adopting better feature extractor backbone (e.g. ResNet101), adding deconvolution layers with skip connections to introduce additional large-scale context <ref type="bibr" target="#b72">[73]</ref> and designing better network structure (e.g. Stem Block and Dense Block) <ref type="bibr" target="#b73">[74]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Evaluation</head><p>We compare various object detection methods on three benchmark datasets, including PASCAL VOC 2007 <ref type="bibr" target="#b24">[25]</ref>, PASCAL VOC 2012 <ref type="bibr" target="#b120">[121]</ref> and Microsoft COCO <ref type="bibr" target="#b93">[94]</ref>. The evaluated approaches include R-CNN <ref type="bibr" target="#b14">[15]</ref>, SPP-net <ref type="bibr" target="#b63">[64]</ref>, Fast R-CNN <ref type="bibr" target="#b15">[16]</ref>, NOC <ref type="bibr" target="#b113">[114]</ref>, Bayes <ref type="bibr" target="#b84">[85]</ref>, MR-CNN&amp;S-CNN <ref type="bibr" target="#b104">[105]</ref>, Faster R-CNN <ref type="bibr" target="#b17">[18]</ref>, HyperNet <ref type="bibr" target="#b100">[101]</ref>, ION <ref type="bibr" target="#b94">[95]</ref>, MS-GR <ref type="bibr" target="#b103">[104]</ref>, StuffNet <ref type="bibr" target="#b99">[100]</ref>, SSD300 <ref type="bibr" target="#b70">[71]</ref>, SSD512 <ref type="bibr" target="#b70">[71]</ref>, OHEM <ref type="bibr" target="#b112">[113]</ref>, SDP+CRC <ref type="bibr" target="#b32">[33]</ref>, GCNN <ref type="bibr" target="#b69">[70]</ref>, SubCNN <ref type="bibr" target="#b59">[60]</ref>, GBD-Net <ref type="bibr" target="#b108">[109]</ref>, PVANET <ref type="bibr" target="#b115">[116]</ref>, YOLO <ref type="bibr" target="#b16">[17]</ref>, YOLOv2 <ref type="bibr" target="#b71">[72]</ref>, R-FCN <ref type="bibr" target="#b64">[65]</ref>, FPN <ref type="bibr" target="#b65">[66]</ref>, Mask R-CNN <ref type="bibr" target="#b66">[67]</ref>, DSSD <ref type="bibr" target="#b72">[73]</ref> and DSOD <ref type="bibr" target="#b73">[74]</ref>. If no specific instructions for the adopted framework are provided, the utilized model is a VGG16 <ref type="bibr" target="#b45">[46]</ref> pretrained on 1000-way ImageNet classification task <ref type="bibr" target="#b38">[39]</ref>. Due to the limitation of paper length, we only provide an overview, including proposal, learning method, loss function, programming language and platform, of the prominent architectures in Table <ref type="table" target="#tab_4">I</ref>. Detailed experimental settings, which can be found in the original papers, are missed. In addition to the comparisons of detection accuracy, another comparison is provided to evaluate their test consumption on PASCAL VOC 2007.  <ref type="table" target="#tab_5">II</ref> and III, from which the following remarks can be obtained.</p><p>• If incorporated with a proper way, more powerful backbone CNN models can definitely improve object detection performance (the comparison among R-CNN with AlexNet, R-CNN with VGG16 and SPP-net with ZF-Net <ref type="bibr" target="#b121">[122]</ref>).</p><p>• With the introduction of SPP layer (SPP-net), end-toend multi-task architecture (FRCN) and RPN (Faster R-CNN), object detection performance is improved gradually and apparently.</p><p>• Due to large quantities of trainable parameters, in order to obtain multi-level robust features, data augmentation is very important for deep learning based models (Faster R-CNN with '07' ,'07+12' and '07+12+coco').</p><p>• Apart from basic models, there are still many other factors affecting object detection performance, such as multi-scale and multi-region feature extraction (e.g. MR-CNN), modified classification networks (e.g. NOC), additional information from other correlated tasks (e.g. StuffNet, HyperNet), multi-scale representation (e.g. ION) and mining of hard negative samples (e.g. OHEM).</p><p>• As YOLO is not skilled in producing object localizations of high IoU, it obtains a very poor result on VOC 2012. However, with the complementary information from Fast R-CNN (YOLO+FRCN) and the aid of other strategies, such as anchor boxes, BN and fine grained features, the localization errors are corrected (YOLOv2).</p><p>• By combining many recent tricks and modelling the whole network as a fully convolutional one, R-FCN achieves a more obvious improvement of detection performance over other approaches.</p><p>2) Microsoft COCO: Microsoft COCO is composed of 300,000 fully segmented images, in which each image has an average of 7 object instances from a total of 80 categories. As there are a lot of less iconic objects with a broad range of scales and a stricter requirement on object localization, this dataset is more challenging than PASCAL 2012. Object detection performance is evaluated by AP computed under different degrees of IoUs and on different object sizes. The results are shown in Table <ref type="table" target="#tab_7">IV</ref>.</p><p>Besides similar remarks to those of PASCAL VOC, some other conclusions can be drawn as follows from Table <ref type="table" target="#tab_7">IV</ref>.</p><p>• Multi-scale training and test are beneficial in improving object detection performance, which provide additional information in different resolutions (R-FCN). FPN and DSSD provide some better ways to build feature pyramids to achieve multi-scale representation. The complementary information from other related tasks is also helpful for accurate object localization (Mask R-CNN with instance segmentation task).</p><p>• Overall, region proposal based methods, such as Faster R-CNN and R-FCN, perform better than regression/classfication based approaches, namely YOLO and SSD, due to the fact that quite a lot of localization errors are produced by regression/classfication based approaches.</p><p>• Context modelling is helpful to locate small objects, which provides additional information by consulting nearby objects and surroundings (GBD-Net and multi-path).</p><p>• Due to the existence of a large number of nonstandard small objects, the results on this dataset are much worse than those of VOC 2007/2012. With the introduction of other powerful frameworks (e.g. ResNeXt <ref type="bibr" target="#b122">[123]</ref>) and useful strategies (e.g. multi-task learning <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b123">[124]</ref>), the performance can be improved.</p><p>• The success of DSOD in training from scratch stresses the importance of network design to release the requirements for perfect pre-trained classifiers on relevant tasks and large numbers of annotated samples.</p><p>3) Timing Analysis: Timing analysis (Table <ref type="table" target="#tab_8">V</ref>) is conducted on Intel i7-6700K CPU with a single core and NVIDIA Titan     X GPU. Except for 'SS' which is processed with CPU, the other procedures related to CNN are all evaluated on GPU.</p><p>From Table <ref type="table" target="#tab_8">V</ref>, we can draw some conclusions as follows.</p><p>• By computing CNN features on shared feature maps (SPP-net), test consumption is reduced largely. Test time is further reduced with the unified multi-task learning (FRCN) and removal of additional region proposal generation stage (Faster R-CNN). It's also helpful to compress the parameters of FC layers with SVD <ref type="bibr" target="#b90">[91]</ref> (PAVNET and FRCN). • It takes additional test time to extract multi-scale features and contextual information (ION and MR-RCNN&amp;S-RCNN).</p><p>• It takes more time to train a more complex and deeper network (ResNet101 against VGG16) and this time consumption can be reduced by adding as many layers into shared fully convolutional layers as possible (FRCN).</p><p>• Regression based models can usually be processed in real-time at the cost of a drop in accuracy compared with region proposal based models. Also, region proposal based models can be modified into real-time systems with the introduction of other tricks <ref type="bibr" target="#b115">[116]</ref> (PVANET), such as BN <ref type="bibr" target="#b42">[43]</ref>, residual connections <ref type="bibr" target="#b122">[123]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SALIENT OBJECT DETECTION</head><p>Visual saliency detection, one of the most important and challenging tasks in computer vision, aims to highlight the most dominant object regions in an image. Numerous applications incorporate the visual saliency to improve their performance, such as image cropping <ref type="bibr" target="#b124">[125]</ref> and segmentation <ref type="bibr" target="#b125">[126]</ref>, image retrieval <ref type="bibr" target="#b56">[57]</ref> and object detection <ref type="bibr" target="#b65">[66]</ref>.</p><p>Broadly, there are two branches of approaches in salient object detection, namely bottom-up (BU) <ref type="bibr" target="#b126">[127]</ref> and top-down (TD) <ref type="bibr" target="#b127">[128]</ref>. Local feature contrast plays the central role in BU salient object detection, regardless of the semantic contents of the scene. To learn local feature contrast, various local and global features are extracted from pixels, e.g. edges <ref type="bibr" target="#b128">[129]</ref>, spatial information <ref type="bibr" target="#b129">[130]</ref>. However, high-level and multi-scale semantic information cannot be explored with these low-level features. As a result, low contrast salient maps instead of salient objects are obtained. TD salient object detection is taskoriented and takes prior knowledge about object categories to guide the generation of salient maps. Taking semantic segmentation as an example, a saliency map is generated in the segmentation to assign pixels to particular object categories via a TD approach <ref type="bibr" target="#b130">[131]</ref>. In a word, TD saliency can be viewed as a focus-of-attention mechanism, which prunes BU salient points that are unlikely to be parts of the object <ref type="bibr" target="#b131">[132]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep learning in Salient Object Detection</head><p>Due to the significance for providing high-level and multiscale feature representation and the successful applications in many correlated computer vision tasks, such as semantic segmentation <ref type="bibr" target="#b130">[131]</ref>, edge detection <ref type="bibr" target="#b132">[133]</ref> and generic object detection <ref type="bibr" target="#b15">[16]</ref>, it is feasible and necessary to extend CNN to salient object detection.</p><p>The early work by Eleonora Vig et al. <ref type="bibr" target="#b27">[28]</ref> follows a completely automatic data-driven approach to perform a largescale search for optimal features, namely an ensemble of deep networks with different layers and parameters. To address the problem of limited training data, Kummerer et al. proposed the Deep Gaze <ref type="bibr" target="#b133">[134]</ref> by transferring from the AlexNet to generate a high dimensional feature space and create a saliency map. A similar architecture was proposed by Huang et al. to integrate saliency prediction into pre-trained object recognition DNNs <ref type="bibr" target="#b134">[135]</ref>. The transfer is accomplished by fine-tuning DNNs' weights with an objective function based on the saliency evaluation metrics, such as Similarity, KL-Divergence and Normalized Scanpath Saliency. Some works combined local and global visual clues to improve salient object detection performance. Wang et al. trained two independent deep CNNs (DNN-L and DNN-G) to capture local information and global contrast and predicted saliency maps by integrating both local estimation and global search <ref type="bibr" target="#b135">[136]</ref>. Cholakkal et al. proposed a weakly supervised saliency detection framework to combine visual saliency from bottom-up and top-down saliency maps, and refined the results with a multi-scale superpixel-averaging <ref type="bibr" target="#b136">[137]</ref>. Zhao et al. proposed a multi-context deep learning framework, which utilizes a unified learning framework to model global and local context jointly with the aid of superpixel segmentation <ref type="bibr" target="#b137">[138]</ref>. To predict saliency in videos, Bak et al. fused two static saliency models, namely spatial stream net and temporal stream net, into a two-stream framework with a novel empirically grounded data augmentation technique <ref type="bibr" target="#b138">[139]</ref>.</p><p>Complementary information from semantic segmentation and context modeling is beneficial. To learn internal representations of saliency efficiently, He et al. proposed a novel superpixelwise CNN approach called SuperCNN <ref type="bibr" target="#b139">[140]</ref>, in which salient object detection is formulated as a binary labeling problem. Based on a fully convolutional neural network, Li et al. proposed a multi-task deep saliency model, in which intrinsic correlations between saliency detection and semantic segmentation are set up <ref type="bibr" target="#b140">[141]</ref>. However, due to the conv layers with large receptive fields and pooling layers, blurry object boundaries and coarse saliency maps are produced. Tang et al. proposed a novel saliency detection framework (CRPSD) <ref type="bibr" target="#b141">[142]</ref>, which combines region-level saliency estimation and pixel-level saliency prediction together with three closely related CNNs. Li et al. proposed a deep contrast network to combine segment-wise spatial pooling and pixel-level fully convolutional streams <ref type="bibr" target="#b142">[143]</ref>.</p><p>The proper integration of multi-scale feature maps is also of significance for improving detection performance. Based on Fast R-CNN, Wang et al. proposed the RegionNet by performing salient object detection with end-to-end edge preserving and multi-scale contextual modelling <ref type="bibr" target="#b143">[144]</ref>. Liu et al. <ref type="bibr" target="#b26">[27]</ref> proposed a multi-resolution convolutional neural network (Mr-CNN) to predict eye fixations, which is achieved by learning both bottom-up visual saliency and top-down visual factors from raw image data simultaneously. Cornia et al. proposed an architecture which combines features extracted at different levels of the CNN <ref type="bibr" target="#b144">[145]</ref>. Li et al. proposed a multiscale deep CNN framework to extract three scales of deep contrast features <ref type="bibr" target="#b145">[146]</ref>, namely the mean-subtracted region, the bounding box of its immediate neighboring regions and the masked entire image, from each candidate region.</p><p>It is efficient and accurate to train a direct pixel-wise CNN architecture to predict salient objects with the aids of RNNs and deconvolution networks. Pan et al. formulated saliency prediction as a minimization optimization on the Euclidean distance between the predicted saliency map and the ground truth and proposed two kinds of architectures <ref type="bibr" target="#b146">[147]</ref>: a shallow one trained from scratch and a deeper one adapted from deconvoluted VGG network. As convolutionaldeconvolution networks are not expert in recognizing objects of multiple scales, Kuen et al. proposed a recurrent attentional convolutional-deconvolution network (RACDNN) with several spatial transformer and recurrent network units to conquer this problem <ref type="bibr" target="#b147">[148]</ref>. To fuse local, global and contextual information of salient objects, Tang et al. developed a deeplysupervised recurrent convolutional neural network (DSRCNN) to perform a full image-to-image saliency detection <ref type="bibr" target="#b148">[149]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Evaluation</head><p>Four representative datasets, including ECSSD <ref type="bibr" target="#b155">[156]</ref>, HKU-IS <ref type="bibr" target="#b145">[146]</ref>, PASCALS <ref type="bibr" target="#b156">[157]</ref>, and SOD <ref type="bibr" target="#b157">[158]</ref>, are used to evaluate several state-of-the-art methods. ECSSD consists of 1000 structurally complex but semantically meaningful natural images. HKU-IS is a large-scale dataset containing over 4000 challenging images. Most of these images have more than one salient object and own low contrast. PASCALS is a subset chosen from the validation set of PASCAL VOC 2010 segmentation dataset and is composed of 850 natural images. The SOD dataset possesses 300 images containing multiple salient objects. The training and validation sets for different datasets are kept the same as those in <ref type="bibr" target="#b151">[152]</ref>.</p><p>Two standard metrics, namely F-measure and the mean absolute error (MAE), are utilized to evaluate the quality of a saliency map. Given precision and recall values pre-computed on the union of generated binary mask B and ground truth Z, F-measure is defined as below</p><formula xml:id="formula_6">F β = (1 + β 2 )P resion × Recall β 2 P resion + Recall (7)</formula><p>where β 2 is set to 0.3 in order to stress the importance of the precision value.</p><p>The MAE score is computed with the following equation</p><formula xml:id="formula_7">M AE = 1 H × W H i=1 W j=1 Ŝ(i, j) = Ẑ(i, j)<label>(8)</label></formula><p>where Ẑ and Ŝ represent the ground truth and the continuous saliency map, respectively. W and H are the width and height of the salient area, respectively. This score stresses the importance of successfully detected salient objects over detected non-salient pixels <ref type="bibr" target="#b158">[159]</ref>.</p><p>The following approaches are evaluated: CHM <ref type="bibr" target="#b149">[150]</ref>, RC <ref type="bibr" target="#b150">[151]</ref>, DRFI <ref type="bibr" target="#b151">[152]</ref>, MC <ref type="bibr" target="#b137">[138]</ref>, MDF <ref type="bibr" target="#b145">[146]</ref>, LEGS <ref type="bibr" target="#b135">[136]</ref>, DSR <ref type="bibr" target="#b148">[149]</ref>, MTDNN <ref type="bibr" target="#b140">[141]</ref>, CRPSD <ref type="bibr" target="#b141">[142]</ref>, DCL <ref type="bibr" target="#b142">[143]</ref>, ELD <ref type="bibr" target="#b152">[153]</ref>, NLDF <ref type="bibr" target="#b153">[154]</ref> and DSSC <ref type="bibr" target="#b154">[155]</ref>. Among these methods, CHM, RC and DRFI are classical ones with the best performance <ref type="bibr" target="#b158">[159]</ref>, while the other methods are all associated with CNN. F-measure and MAE scores are shown in Table <ref type="table" target="#tab_9">VI</ref>.</p><p>From Table <ref type="table" target="#tab_9">VI</ref>, we can find that CNN based methods perform better than classic methods. MC and MDF combine the information from local and global context to reach a more accurate saliency. ELD refers to low-level handcrafted features for complementary information. LEGS adopts generic region proposals to provide initial salient regions, which may be insufficient for salient detection. DSR and MT act in different ways by introducing recurrent network and semantic segmentation, which provide insights for future improvements. CPRSD, DCL, NLDF and DSSC are all based on multi-scale representations and superpixel segmentation, which provide robust salient regions and smooth boundaries. DCL, NLDF and DSSC perform the best on these four datasets. DSSC earns the best performance by modelling scale-to-scale shortconnections.</p><p>Overall, as CNN mainly provides salient information in local regions, most of CNN based methods need to model visual saliency along region boundaries with the aid of superpixel segmentation. Meanwhile, the extraction of multiscale deep CNN features is of significance for measuring local conspicuity. Finally, it's necessary to strengthen local connections between different CNN layers and as well to utilize complementary information from local and global context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. FACE DETECTION</head><p>Face detection is essential to many face applications and acts as an important pre-processing procedure to face recognition <ref type="bibr" target="#b159">[160]</ref>- <ref type="bibr" target="#b161">[162]</ref>, face synthesis <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b163">[164]</ref> and facial expression analysis <ref type="bibr" target="#b164">[165]</ref>. Different from generic object detection, this task is to recognize and locate face regions covering a very large range of scales (30-300 pts vs. 10-1000 pts). At the same time, faces have their unique object structural configurations (e.g. the distribution of different face parts) and characteristics (e.g. skin color). All these differences lead to special attention to this task. However, large visual variations of faces, such as occlusions, pose variations and illumination changes, impose great challenges for this task in real applications.</p><p>The most famous face detector proposed by Viola and Jones <ref type="bibr" target="#b165">[166]</ref> trains cascaded classifiers with Haar-Like features and AdaBoost, achieving good performance with real-time efficiency. However, this detector may degrade significantly in real-world applications due to larger visual variations of human faces. Different from this cascade structure, Felzenszwalb et al. proposed a deformable part model (DPM) for face detection <ref type="bibr" target="#b23">[24]</ref>. However, for these traditional face detection methods, high computational expenses and large quantities of annotations are required to achieve a reasonable result. Besides, their performance is greatly restricted by manually designed features and shallow architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep learning in Face Detection</head><p>Recently, some CNN based face detection approaches have been proposed <ref type="bibr" target="#b166">[167]</ref>- <ref type="bibr" target="#b168">[169]</ref>.As less accurate localization results from independent regressions of object coordinates, Yu et al. <ref type="bibr" target="#b166">[167]</ref> proposed a novel IoU loss function for predicting the four bounds of box jointly. Farfade et al. <ref type="bibr" target="#b167">[168]</ref> proposed a Deep Dense Face Detector (DDFD) to conduct multi-view face detection, which is able to detect faces in a wide range of orientations without requirement of pose/landmark annotations. Yang et al. proposed a novel deep learning based face detection framework <ref type="bibr" target="#b168">[169]</ref>, which collects the responses from local facial parts (e.g. eyes, nose and mouths) to address face detection under severe occlusions and unconstrained pose variations. Yang et al. <ref type="bibr" target="#b169">[170]</ref> proposed a scale-friendly detection network named ScaleFace, which splits a large range of target scales into smaller sub-ranges. Different specialized sub-networks are constructed on these sub-scales and combined into a single one to conduct end-to-end optimization. Hao et al. designed an efficient CNN to predict the scale distribution histogram of the faces and took this histogram to guide the zoom-in and zoomout of the image <ref type="bibr" target="#b170">[171]</ref>. Since the faces are approximately in uniform scale after zoom, compared with other state-ofthe-art baselines, better performance is achieved with less computation cost. Besides, some generic detection frameworks </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Metrics CHM <ref type="bibr" target="#b149">[150]</ref> RC <ref type="bibr" target="#b150">[151]</ref> DRFI <ref type="bibr" target="#b151">[152]</ref> MC <ref type="bibr" target="#b137">[138]</ref> MDF <ref type="bibr" target="#b145">[146]</ref> LEGS <ref type="bibr" target="#b135">[136]</ref> DSR <ref type="bibr" target="#b148">[149]</ref> MTDNN <ref type="bibr" target="#b140">[141]</ref> CRPSD <ref type="bibr" target="#b141">[142]</ref> DCL <ref type="bibr" target="#b142">[143]</ref> ELD <ref type="bibr" target="#b152">[153]</ref> NLDF <ref type="bibr" target="#b153">[154]</ref> DSSC <ref type="bibr" target="#b154">[155]</ref> PASCAL are extended to face detection with different modifications, e.g. Faster R-CNN <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b171">[172]</ref>, <ref type="bibr" target="#b172">[173]</ref>. Some authors trained CNNs with other complementary tasks, such as 3D modelling and face landmarks, in a multitask learning manner. Huang et al. proposed a unified endto-end FCN framework called DenseBox to jointly conduct face detection and landmark localization <ref type="bibr" target="#b173">[174]</ref>. Li et al. <ref type="bibr" target="#b174">[175]</ref> proposed a multi-task discriminative learning framework which integrates a ConvNet with a fixed 3D mean face model in an end-to-end manner. In the framework, two issues are addressed to transfer from generic object detection to face detection, namely eliminating predefined anchor boxes by a 3D mean face model and replacing RoI pooling layer with a configuration pooling layer. Zhang et al. <ref type="bibr" target="#b175">[176]</ref> proposed a deep cascaded multi-task framework named MTCNN which exploits the inherent correlations between face detection and alignment in unconstrained environment to boost up detection performance in a coarse-to-fine manner.</p><p>Reducing computational expenses is of necessity in real applications. To achieve real-time detection on mobile platform, Kalinovskii and Spitsyn proposed a new solution of frontal face detection based on compact CNN cascades <ref type="bibr" target="#b176">[177]</ref>. This method takes a cascade of three simple CNNs to generate, classify and refine candidate object positions progressively. To reduce the effects of large pose variations, Chen et al. proposed a cascaded CNN denoted by Supervised Transformer Network <ref type="bibr" target="#b30">[31]</ref>. This network takes a multi-task RPN to predict candidate face regions along with associated facial landmarks simultaneously, and adopts a generic R-CNN to verify the existence of valid faces. Yang et al. proposed a three-stage cascade structure based on FCNs <ref type="bibr" target="#b7">[8]</ref>, while in each stage, a multi-scale FCN is utilized to refine the positions of possible faces. Qin et al. proposed a unified framework which achieves better results with the complementary information from different jointly trained CNNs <ref type="bibr" target="#b177">[178]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Evaluation</head><p>The FDDB <ref type="bibr" target="#b178">[179]</ref> dataset has a total of 2,845 pictures in which 5,171 faces are annotated with elliptical shape. Two types of evaluations are used: the discrete score and continuous score. By varying the threshold of the decision rule, the ROC curve for the discrete scores can reflect the dependence of the detected face fractions on the number of false alarms. Compared with annotations, any detection with an IoU ratio exceeding 0.5 is treated as positive. Each annotation is only associated with one detection. The ROC curve for the continuous scores is the reflection of face localization quality.</p><p>The evaluated models cover DDFD <ref type="bibr" target="#b167">[168]</ref>, CascadeCNN <ref type="bibr" target="#b179">[180]</ref>, ACF-multiscale <ref type="bibr" target="#b180">[181]</ref>, Pico <ref type="bibr" target="#b181">[182]</ref>, HeadHunter <ref type="bibr" target="#b182">[183]</ref>, Joint Cascade <ref type="bibr" target="#b29">[30]</ref>, SURF-multiview <ref type="bibr" target="#b183">[184]</ref>, Viola-Jones <ref type="bibr" target="#b165">[166]</ref>, NPDFace <ref type="bibr" target="#b184">[185]</ref>, Faceness <ref type="bibr" target="#b168">[169]</ref>, CCF <ref type="bibr" target="#b185">[186]</ref>, MTCNN <ref type="bibr" target="#b175">[176]</ref>, Conv3D <ref type="bibr" target="#b174">[175]</ref>, Hyperface <ref type="bibr" target="#b186">[187]</ref>, UnitBox <ref type="bibr" target="#b166">[167]</ref>, LDCF+ [S2], DeepIR <ref type="bibr" target="#b172">[173]</ref>, HR-ER <ref type="bibr" target="#b187">[188]</ref>, Face-R-CNN <ref type="bibr" target="#b171">[172]</ref> and Scale-Face <ref type="bibr" target="#b169">[170]</ref>. ACF-multiscale, Pico, HeadHunter, Joint Cascade, SURF-multiview, Viola-Jones, NPDFace and LDCF+ are built on classic hand-crafted features while the rest methods are based on deep CNN features. The ROC curves are shown in Figure <ref type="figure" target="#fig_13">11</ref>.</p><p>From Figure <ref type="figure" target="#fig_13">11</ref>(a), in spite of relatively competitive results produced by LDCF+, it can be observed that most of classic methods perform with similar results and are outperformed by CNN based methods by a significant margin. From Figure <ref type="figure" target="#fig_13">11</ref>(b), it can be observed that most of CNN based methods earn similar true positive rates between 60% and 70% while DeepIR and HR-ER perform much better than them. Among classic methods, Joint Cascade is still competitive. As earlier works, DDFD and CCF directly make use of generated feature maps and obtain relatively poor results. CascadeCNN builds cascaded CNNs to locate face regions, which is efficient but inaccurate. Faceness combines the decisions from different part detectors, resulting in precise face localizations while being time-consuming. The outstanding performance of MTCNN, Conv3D and Hyperface proves the effectiveness of multi-task learning. HR-ER and ScaleFace adaptively detect faces of different scales, and make a balance between accuracy and efficiency. DeepIR and Face-R-CNN are two extensions of the Faster R-CNN architecture to face detection, which validate the significance and effectiveness of Faster R-CNN. Unitbox provides an alternative choice for performance improvements by carefully designing optimization loss.</p><p>From these results, we can draw the conclusion that CNN based methods are in the leading position. The performance can be improved by the following strategies: designing novel optimization loss, modifying generic detection pipelines, building meaningful network cascades, adapting scale-aware detection and learning multi-task shared CNN features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. PEDESTRIAN DETECTION</head><p>Recently, pedestrian detection has been intensively studied, which has a close relationship to pedestrian tracking <ref type="bibr" target="#b188">[189]</ref>, <ref type="bibr" target="#b189">[190]</ref>, person re-identification <ref type="bibr" target="#b190">[191]</ref>, <ref type="bibr" target="#b191">[192]</ref> and robot navigation <ref type="bibr" target="#b192">[193]</ref>, <ref type="bibr" target="#b193">[194]</ref>. Prior to the recent progress in DCNN based methods <ref type="bibr" target="#b194">[195]</ref>, <ref type="bibr" target="#b195">[196]</ref>, some researchers combined boosted decision forests with hand-crafted features to obtain pedestrian detectors <ref type="bibr" target="#b196">[197]</ref>- <ref type="bibr" target="#b198">[199]</ref>. At the same time, to explicitly model the deformation and occlusion, part-based models <ref type="bibr" target="#b199">[200]</ref> and explicit occlusion handling <ref type="bibr" target="#b200">[201]</ref>, <ref type="bibr" target="#b201">[202]</ref> are of concern.</p><p>As there are many pedestrian instances of small sizes in typical scenarios of pedestrian detection (e.g. automatic driving and intelligent surveillance), the application of RoI pooling layer in generic object detection pipeline may result in 'plain' features due to collapsing bins. In the meantime, the main source of false predictions in pedestrian detection is the confusion of hard background instances, which is in contrast to the interference from multiple categories in generic object detection. As a result, different configurations and components are required to accomplish accurate pedestrian detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep learning in Pedestrian Detection</head><p>Although DCNNs have obtained excellent performance on generic object detection <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b71">[72]</ref>, none of these approaches have achieved better results than the best hand-crafted feature based method <ref type="bibr" target="#b197">[198]</ref> for a long time, even when part-based information and occlusion handling are incorporated <ref type="bibr" target="#b201">[202]</ref>. Thereby, some researches have been conducted to analyze the reasons. Zhang et al. attempted to adapt generic Faster R-CNN <ref type="bibr" target="#b17">[18]</ref> to pedestrian detection <ref type="bibr" target="#b202">[203]</ref>. They modified the downstream classifier by adding boosted forests to shared, highresolution conv feature maps and taking a RPN to handle small instances and hard negative examples. To deal with complex occlusions existing in pedestrian images, inspired by DPM <ref type="bibr" target="#b23">[24]</ref>, Tian et al. proposed a deep learning framework called DeepParts <ref type="bibr" target="#b203">[204]</ref>, which makes decisions based an ensemble of extensive part detectors. DeepParts has advantages in dealing with weakly labeled data, low IoU positive proposals and partial occlusion.</p><p>Other researchers also tried to combine complementary information from multiple data sources. CompACT-Deep adopts a complexity-aware cascade to combine hand-crafted features and fine-tuned DCNNs <ref type="bibr" target="#b194">[195]</ref>. Based on Faster R-CNN, Liu et al. proposed multi-spectral deep neural networks for pedestrian detection to combine complementary information from color and thermal images <ref type="bibr" target="#b204">[205]</ref>. Tian et al. <ref type="bibr" target="#b205">[206]</ref> proposed a taskassistant CNN (TA-CNN) to jointly learn multiple tasks with  <ref type="bibr" target="#b206">[207]</ref>. Based on the candidate bounding boxes generated with SSD detectors <ref type="bibr" target="#b70">[71]</ref>, multiple binary classifiers are processed parallelly to conduct soft-rejection based network fusion (SNF) by consulting their aggregated degree of confidences. However, most of these approaches are much more sophisticated than the standard R-CNN framework. CompACT-Deep consists of a variety of hand-crafted features, a small CNN model and a large VGG16 model <ref type="bibr" target="#b194">[195]</ref>. DeepParts contains 45 fine-tuned DCNN models, and a set of strategies, including bounding box shifting handling and part selection, are required to arrive at the reported results <ref type="bibr" target="#b203">[204]</ref>. So the modification and simplification is of significance to reduce the burden on both software and hardware to satisfy real-time detection demand. Tome et al. proposed a novel solution to adapt generic object detection pipeline to pedestrian detection by optimizing most of its stages <ref type="bibr" target="#b58">[59]</ref>. Hu et al. <ref type="bibr" target="#b207">[208]</ref> trained an ensemble of boosted decision models by reusing the conv feature maps, and a further improvement was gained with simple pixel labelling and additional complementary hand-crafted features. Tome et al. <ref type="bibr" target="#b208">[209]</ref> proposed a reduced memory region based deep CNN architecture, which fuses regional responses from both ACF detectors and SVM classifiers into R-CNN. Ribeiro et al. addressed the problem of Human-Aware Navigation <ref type="bibr" target="#b31">[32]</ref> and proposed a vision-based person tracking system guided by multiple camera sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Evaluation</head><p>The evaluation is conducted on the most popular Caltech Pedestrian dataset <ref type="bibr" target="#b2">[3]</ref>. The dataset was collected from the videos of a vehicle driving through an urban environment and consists of 250,000 frames with about 2300 unique pedestrians and 350,000 annotated bounding boxes (BBs). Three kinds of labels, namely 'Person (clear identifications)', 'Person? (unclear identifications)' and 'People (large group of individuals)', are assigned to different BBs. The performance is measured with the log-average miss rate (L-AMR) which is computed evenly spaced in log-space in the range 10 −2 to 1 by averaging miss rate at the rate of nine false positives per image (FPPI) <ref type="bibr" target="#b2">[3]</ref>. According to the differences in the height and visible part of the BBs, a total of 9 popular settings are adopted to evaluate different properties of these models. Details of these settings are as <ref type="bibr" target="#b2">[3]</ref>.</p><p>Evaluated methods include Checkerboards+ <ref type="bibr" target="#b197">[198]</ref>, LDCF++ [S2], SCF+AlexNet <ref type="bibr" target="#b209">[210]</ref>, SA-FastRCNN <ref type="bibr" target="#b210">[211]</ref>, MS-CNN <ref type="bibr" target="#b104">[105]</ref>, DeepParts <ref type="bibr" target="#b203">[204]</ref>, CompACT-Deep <ref type="bibr" target="#b194">[195]</ref>, RPN+BF <ref type="bibr" target="#b202">[203]</ref> and F-DNN+SS <ref type="bibr" target="#b206">[207]</ref>. The first two methods are based on hand-crafted features while the rest ones rely on deep CNN features. All results are exhibited in Table <ref type="table" target="#tab_11">VII</ref>. From this table, we observe that different from other tasks, classic handcrafted features can still earn competitive results with boosted decision forests <ref type="bibr" target="#b202">[203]</ref>, ACF <ref type="bibr" target="#b196">[197]</ref> and HOG+LUV channels [S2]. As an early attempt to adapt CNN to pedestrian detection, the features generated by SCF+AlexNet are not so discriminant and produce relatively poor results. Based on multiple CNNs, DeepParts and CompACT-Deep accomplish detection tasks via different strategies, namely local part integration and cascade network. The responses from different local part detectors make DeepParts robust to partial occlusions. However, due to complexity, it is too time-consuming to achieve real-time detection. The multi-scale representation of MS-CNN improves accuracy of pedestrian locations. SA-FastRCNN extends Fast R-CNN to automatically detecting pedestrians according to their different scales, which has trouble when there are partial occlusions. RPN+BF combines the detectors produced by Faster R-CNN with boosting decision forest to accurately locate different pedestrians. F-DNN+SS, which is composed of multiple parallel classifiers with soft rejections, performs the best followed by RPN+BF, SA-FastRCNN and MS-CNN.</p><p>In short, CNN based methods can provide more accurate candidate boxes and multi-level semantic information for identifying and locating pedestrians. Meanwhile, handcrafted features are complementary and can be combined with CNN to achieve better results. The improvements over existing CNN methods can be obtained by carefully designing the framework and classifiers, extracting multi-scale and part based semantic information and searching for complementary information from other related tasks, such as segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. PROMISING FUTURE DIRECTIONS AND TASKS</head><p>In spite of rapid development and achieved promising progress of object detection, there are still many open issues for future work.</p><p>The first one is small object detection such as occurring in COCO dataset and in face detection task. To improve localization accuracy on small objects under partial occlusions, it is necessary to modify network architectures from the following aspects.</p><p>• Multi-task joint optimization and multi-modal information fusion. Due to the correlations between different tasks within and outside object detection, multi-task joint optimization has already been studied by many researchers <ref type="bibr">[16] [18]</ref>. However, apart from the tasks mentioned in Subs. III-A8, it is desirable to think over the characteristics of different sub-tasks of object detection (e.g. superpixel semantic segmentation in salient object detection) and extend multi-task optimization to other applications such as instance segmentation <ref type="bibr" target="#b65">[66]</ref>, multi-object tracking <ref type="bibr" target="#b201">[202]</ref> and multi-person pose estimation [S4]. Besides, given a specific application, the information from different modalities, such as text <ref type="bibr" target="#b211">[212]</ref>, thermal data <ref type="bibr" target="#b204">[205]</ref> and images <ref type="bibr" target="#b64">[65]</ref>, can be fused together to achieve a more discriminant network.</p><p>• Scale adaption. Objects usually exist in different scales, which is more apparent in face detection and pedestrian detection. To increase the robustness to scale changes, it is demanded to train scale-invariant, multi-scale or scaleadaptive detectors. For scale-invariant detectors, more powerful backbone architectures (e.g. ResNext <ref type="bibr" target="#b122">[123]</ref>), negative sample mining <ref type="bibr" target="#b112">[113]</ref>, reverse connection <ref type="bibr" target="#b212">[213]</ref> and subcategory modelling <ref type="bibr" target="#b59">[60]</ref> are all beneficial. For multi-scale detectors, both the FPN <ref type="bibr" target="#b65">[66]</ref> which produces multi-scale feature maps and Generative Adversarial Network <ref type="bibr" target="#b213">[214]</ref> which narrows representation differences between small objects and the large ones with a low-cost architecture provide insights into generating meaningful feature pyramid. For scale-adaptive detectors, it is useful to combine knowledge graph <ref type="bibr" target="#b214">[215]</ref>, attentional mechanism <ref type="bibr" target="#b215">[216]</ref>, cascade network <ref type="bibr" target="#b179">[180]</ref> and scale distribution estimation <ref type="bibr" target="#b170">[171]</ref> to detect objects adaptively.</p><p>• Spatial correlations and contextual modelling. Spatial distribution plays an important role in object detection. So region proposal generation and grid regression are taken to obtain probable object locations. However, the correlations between multiple proposals and object categories are ignored. Besides, the global structure information is abandoned by the position-sensitive score maps in R-FCN.</p><p>To solve these problems, we can refer to diverse subset selection <ref type="bibr" target="#b216">[217]</ref> and sequential reasoning tasks <ref type="bibr" target="#b217">[218]</ref> for possible solutions. It is also meaningful to mask salient parts and couple them with the global structure in a joint-learning manner <ref type="bibr" target="#b218">[219]</ref>.</p><p>The second one is to release the burden on manual labor and accomplish real-time object detection, with the emergence of large-scale image and video data. The following three aspects can be taken into account.</p><p>• Cascade network. In a cascade network, a cascade of detectors are built in different stages or layers <ref type="bibr" target="#b179">[180]</ref>, <ref type="bibr" target="#b219">[220]</ref>.</p><p>And easily distinguishable examples are rejected at shallow layers so that features and classifiers at latter stages can handle more difficult samples with the aid of the decisions from previous stages. However, current cascades are built in a greedy manner, where previous stages in cascade are fixed when training a new stage. So the optimizations of different CNNs are isolated, which stresses the necessity of end-toend optimization for CNN cascade. At the same time, it is also a matter of concern to build contextual associated cascade networks with existing layers.</p><p>• Unsupervised and weakly supervised learning. It's very time consuming to manually draw large quantities of bounding boxes. To release this burden, semantic prior <ref type="bibr" target="#b54">[55]</ref>, unsupervised object discovery <ref type="bibr" target="#b220">[221]</ref>, multiple instance learning <ref type="bibr" target="#b221">[222]</ref> and deep neural network prediction <ref type="bibr" target="#b46">[47]</ref> can be integrated to make best use of image-level supervision to assign object category tags to corresponding object regions and refine object boundaries. Furthermore, weakly annotations (e.g. center-click annotations <ref type="bibr" target="#b222">[223]</ref>) are also helpful for achieving high-quality detectors with modest annotation efforts, especially aided by the mobile platform.</p><p>• Network optimization. Given specific applications and platforms, it is significant to make a balance among speed, memory and accuracy by selecting an optimal detection architecture <ref type="bibr" target="#b115">[116]</ref>, <ref type="bibr" target="#b223">[224]</ref>. However, despite that detection accuracy is reduced, it is more meaningful to learn compact models with fewer number of parameters <ref type="bibr" target="#b208">[209]</ref>. And this situation can be relieved by introducing better pre-training schemes <ref type="bibr" target="#b224">[225]</ref>, knowledge distillation <ref type="bibr" target="#b225">[226]</ref> and hint learning <ref type="bibr" target="#b226">[227]</ref>. DSOD also provides a promising guideline to train from scratch to bridge the gap between different image sources and tasks <ref type="bibr" target="#b73">[74]</ref>. The third one is to extend typical methods for 2D object detection to adapt 3D object detection and video object detection, with the requirements from autonomous driving, intelligent transportation and intelligent surveillance.</p><p>• 3D object detection. With the applications of 3D sensors (e.g. LIDAR and camera), additional depth information can be utilized to better understand the images in 2D and extend the image-level knowledge to the real world. However, seldom of these 3D-aware techniques aim to place correct 3D bounding boxes around detected objects. To achieve better bounding results, multi-view representation <ref type="bibr" target="#b180">[181]</ref> and 3D proposal network <ref type="bibr" target="#b227">[228]</ref> may provide some guidelines to encode depth information with the aid of inertial sensors (accelerometer and gyrometer) <ref type="bibr" target="#b228">[229]</ref>.</p><p>• Video object detection. Temporal information across different frames play an important role in understanding the behaviors of different objects. However, the accuracy suffers from degenerated object appearances (e.g., motion blur and video defocus) in videos and the network is usually not trained end-to-end. To this end, spatiotemporal tubelets <ref type="bibr" target="#b229">[230]</ref>, optical flow <ref type="bibr" target="#b198">[199]</ref> and LSTM <ref type="bibr" target="#b106">[107]</ref> should be considered to fundamentally model object associations between consecutive frames.</p><p>VIII. CONCLUSION Due to its powerful learning ability and advantages in dealing with occlusion, scale transformation and background switches, deep learning based object detection has been a research hotspot in recent years. This paper provides a detailed review on deep learning based object detection frameworks which handle different sub-problems, such as occlusion, clutter and low resolution, with different degrees of modifications on R-CNN. The review starts on generic object detection pipelines which provide base architectures for other related tasks. Then, three other common tasks, namely salient object detection, face detection and pedestrian detection, are also briefly reviewed. Finally, we propose several promising future directions to gain a thorough understanding of the object detection landscape. This review is also meaningful for the developments in neural networks and related learning systems, which provides valuable insights and guidelines for future progress.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The application domains of object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Two types of frameworks: region proposal based and regression/classification based. SPP: Spatial Pyramid Pooling<ref type="bibr" target="#b63">[64]</ref>, FRCN: Faster R-CNN<ref type="bibr" target="#b15">[16]</ref>, RPN: Region Proposal Network<ref type="bibr" target="#b17">[18]</ref>, FCN: Fully Convolutional Network<ref type="bibr" target="#b64">[65]</ref>, BN: Batch Normalization<ref type="bibr" target="#b42">[43]</ref>, Deconv layers: Deconvolution layers<ref type="bibr" target="#b53">[54]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Object detection system overview. Our system (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVMs. R-CNN achieves a mean average precision (mAP) of 53.7% on PASCAL VOC 2010. For comparison, [32] reports 35.1% mAP using the same region proposals, but with a spatial pyramid and bag-of-visual-words approach. The popular deformable part models perform at 33.4%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Pooling features from arbitrary windows on feature maps. The feature maps are computed from the entire image. The pooling is performed in candidate windows.</figDesc><graphic url="image-13.png" coords="5,88.61,178.89,57.49,51.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Fast R-CNN architecture. An input image and multiple regions of interest (RoIs) are input into a fully convolutional network. Each RoI is pooled into a fixed-size feature map and then mapped to a feature vector by fully connected layers (FCs). The network has two output vectors per RoI: softmax probabilities and per-class bounding-box regression offsets. The architecture is trained end-to-end with a multi-task loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals on PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) Using an image pyramid to build a feature pyramid. Features are computed on each of the image scales independently, which is slow. (b) Recent detection systems have opted to use only single scale features for faster detection. (c) An alternative is to reuse the pyramidal feature hierarchy computed by a ConvNet as if it were a featurized image pyramid. (d) Our proposed Feature Pyramid Network (FPN) is fast like (b) and (c), but more accurate. In this figure, feature maps are indicate by blue outlines and thicker outlines denote semantically stronger features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Moreover, training deep 1 arXiv:1612.03144v2 [cs.CV] 19 Apr 2017</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid to build a feature pyramid. (b) Only single scale features is adopted for faster detection. (c) An alternative to the featurized image pyramid is to reuse the pyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both (b) and (c). Blue outlines indicate feature maps and thicker outlines denote semantically stronger features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. The architecture of SSD 300<ref type="bibr" target="#b70">[71]</ref>. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor boxes and their associated confidences. Final detection results are obtained by conducting NMS on multi-scale refined bounding boxes.</figDesc><graphic url="image-27.png" coords="10,127.29,56.07,358.64,93.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 )</head><label>1</label><figDesc>PASCAL VOC 2007/2012: PASCAL VOC 2007 and 2012 datasets consist of 20 categories. The evaluation terms are Average Precision (AP) in each single category and mean Average Precision (mAP) across all the 20 categories. Comparative results are exhibited in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0 * FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN with multi-scale training and testing, Mask: Mask R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The ROC curves of state-of-the-art methods on FDDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1,2 Jitendra Malik 1 UC Berkeley and 2 ICSI ,trevor,malik}@eecs.berkeley.edu</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>aeroplane? no.</cell></row><row><cell>ed on the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>in the last</cell><cell></cell><cell></cell><cell></cell></row><row><cell>mplex en-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>low-level</cell><cell>1. Input</cell><cell>2. Extract region</cell><cell>3. Compute</cell></row><row><cell>paper, we</cell><cell>image</cell><cell>proposals (~2k)</cell><cell>CNN features</cell></row><row><cell>m that im-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>than 30%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>achieving</cell><cell></cell><cell></cell><cell></cell></row><row><cell>y insights:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>neural net-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>n order to</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ed training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>iliary task,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a signifi-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>on propos-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>gions with</cell><cell></cell><cell></cell><cell></cell></row><row><cell>at provide</cell><cell></cell><cell></cell><cell></cell></row><row><cell>rich hier-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>plete sys-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ey.edu/</cell><cell></cell><cell></cell><cell></cell></row><row><cell>on various ably on the at perfor-, PASCAL nowledged with small d employ-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>istograms, h complex isual path-urs several ht be hier-atures that . ologically-</cell><cell></cell><cell></cell><cell></cell></row></table><note>. . . person? yes. tvmonitor? no.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>82.44 93.42±0.5</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>9</cell></row><row><cell></cell><cell>VOC 2007</cell><cell>Caltech101</cell></row><row><cell></cell><cell>56.07</cell><cell>74.41±1.0</cell></row><row><cell></cell><cell>57.66</cell><cell>76.95±0.4</cell></row><row><cell></cell><cell>61.69</cell><cell>77.78±0.6</cell></row><row><cell></cell><cell>-</cell><cell>86.91±0.7</cell></row><row><cell>]</cell><cell>75.90  ‡</cell><cell>86.5±0.5</cell></row><row><cell></cell><cell>77.7</cell><cell>-</cell></row><row><cell>]</cell><cell>82.42</cell><cell>88.54±0.3</cell></row><row><cell cols="3">n results for Pascal VOC 2007 1 (accuracy).  † numbers reported entation as in Table 6 (a).</cell></row><row><cell cols="3">s our results compared with the ods on Caltech101. Our result previous record (88.54%) by a .88%).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I AN</head><label>I</label><figDesc>OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES. '+' denotes that corresponding techniques are employed while '-' denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the other architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions by the referenced authors.</figDesc><table><row><cell>Framework</cell><cell>Proposal</cell><cell>Multi-scale Input</cell><cell>Learning Method</cell><cell>Loss Function</cell><cell>Softmax Layer</cell><cell>End-to-end Train</cell><cell>Platform</cell><cell>Language</cell></row><row><cell>R-CNN [15] SPP-net [64] Fast RCNN [16] Faster R-CNN [18] R-FCN [65] Mask R-CNN [67] FPN [66] YOLO [17] SSD [71] YOLOv2 [72]</cell><cell>Selective Search EdgeBoxes Selective Search RPN RPN RPN RPN ---</cell><cell>-+ + + + + + ---</cell><cell>SGD,BP SGD SGD SGD SGD SGD Synchronized SGD SGD SGD SGD</cell><cell>Hinge loss (classification),Bounding box regression Hinge loss (classification),Bounding box regression Class Log loss+bounding box regression Class Log loss+bounding box regression Class Log loss+bounding box regression Class Log loss+bounding box regression +Semantic sigmoid loss Class Log loss+bounding box regression Class sum-squared error loss+bounding box regression +object confidence+background confidence Class softmax loss+bounding box regression Class sum-squared error loss+bounding box regression +object confidence+background confidence</cell><cell>+ + + + -+ + + -+</cell><cell>---+ + + + + + +</cell><cell>Caffe Caffe Caffe Caffe Caffe TensorFlow/Keras TensorFlow Darknet Caffe Darknet</cell><cell>Matlab Matlab Python Python/Matlab Matlab Python Python C C++ C</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II COMPARATIVE</head><label>II</label><figDesc>RESULTS ON VOC 2007 TEST SET (%).</figDesc><table><row><cell>Methods</cell><cell>Trained on</cell><cell>areo</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>mbike</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mAP</cell></row><row><cell>R-CNN (Alex) [15] R-CNN(VGG16) [15] SPP-net(ZF) [64] GCNN [70] Bayes [85] Fast R-CNN [16] SDP+CRC [33] SubCNN [60] StuffNet30 [100] NOC [114] MR-CNN&amp;S-CNN [110] HyperNet [101] MS-GR [104] OHEM+Fast R-CNN [113] ION [95] Faster R-CNN [18] Faster R-CNN [18] Faster R-CNN [18] SSD300 [71] SSD512 [71]</cell><cell>07 07 07 07 07 07+12 07 07 07 07+12 07+12 07+12 07+12 07+12 07+12+S 07 07+12 07+12+COCO 07+12+COCO 07+12+COCO</cell><cell>68.1 73.4 68.5 68.3 74.1 77.0 76.1 70.2 72.6 76.3 80.3 77.4 80.0 80.6 80.2 70.0 76.5 84.3 80.9 86.6</cell><cell>72.8 77.0 71.7 77.3 83.2 78.1 79.4 80.5 81.7 81.4 84.1 83.3 81.0 85.7 85.2 80.6 79.0 82.0 86.3 88.3</cell><cell>56.8 63.4 58.7 68.5 67.0 69.3 68.2 69.5 70.6 74.4 78.5 75.0 77.4 79.8 78.8 70.1 70.9 77.7 79.0 82.4</cell><cell>43.0 45.4 41.9 52.4 50.8 59.4 52.6 60.3 60.5 61.7 70.8 69.1 72.1 69.9 70.9 57.3 65.5 68.9 76.2 76.0</cell><cell>36.8 44.6 42.5 38.6 51.6 38.3 46.0 47.9 53.0 60.8 68.5 62.4 64.3 60.8 62.6 49.9 52.1 65.7 57.6 66.3</cell><cell>66.3 75.1 67.7 78.5 76.2 81.6 78.4 79.0 81.5 84.7 88.0 83.1 88.2 88.3 86.6 78.2 83.1 88.1 87.3 88.6</cell><cell>74.2 78.1 72.1 79.5 81.4 78.6 78.4 78.7 83.7 78.2 85.9 87.4 88.1 87.9 86.9 80.4 84.7 88.4 88.2 88.9</cell><cell>67.6 79.8 73.8 81.0 77.2 86.7 81.0 84.2 83.9 82.9 87.8 87.4 88.4 89.6 89.8 82.0 86.4 88.9 88.6 89.1</cell><cell>34.4 40.5 34.7 47.1 48.1 42.8 46.7 48.5 52.2 53.0 60.3 57.1 64.4 59.7 61.7 52.2 52.0 63.6 60.5 65.1</cell><cell>63.5 73.7 67.0 73.6 78.9 78.8 73.5 73.9 78.9 79.2 85.2 79.8 85.4 85.1 86.9 75.3 81.9 86.3 85.4 88.4</cell><cell>54.5 62.2 63.4 64.5 65.6 68.9 65.3 63.0 70.7 69.2 73.7 71.4 73.1 76.5 76.5 67.2 65.7 70.8 76.7 73.6</cell><cell>61.2 79.4 66.0 77.2 77.3 84.7 78.6 82.7 85.0 83.2 87.2 85.1 87.3 87.1 88.4 80.3 84.8 85.9 87.5 86.5</cell><cell>69.1 78.1 72.5 80.5 78.4 82.0 81.0 80.6 85.7 83.2 86.5 85.1 87.4 87.3 87.5 79.8 84.6 87.6 89.2 88.9</cell><cell>68.6 73.1 71.3 75.8 75.1 76.6 76.7 76.0 77.0 78.5 85.0 80.0 85.1 82.4 83.4 75.0 77.5 80.1 84.5 85.3</cell><cell>58.7 64.2 58.9 66.6 70.1 69.9 77.3 70.2 78.7 68.0 76.4 79.1 79.6 78.8 80.5 76.3 76.7 82.3 81.4 84.6</cell><cell>33.4 35.6 32.8 34.3 41.4 31.8 39.0 38.2 42.2 45.0 48.5 51.2 50.1 53.7 52.4 39.1 38.8 53.6 55.0 59.1</cell><cell>62.9 66.8 60.9 65.2 69.6 70.1 65.1 62.4 73.6 71.6 76.3 79.1 78.4 80.5 78.1 68.3 73.6 80.4 81.9 85.0</cell><cell>51.1 67.2 56.1 64.4 60.8 74.8 67.2 67.7 69.2 76.7 75.5 75.7 79.5 78.7 77.2 67.3 73.9 75.8 81.5 80.4</cell><cell>62.5 70.4 67.9 75.6 70.2 80.4 77.5 77.7 79.2 82.2 85.0 80.9 86.9 84.5 86.9 81.1 83.0 86.6 85.9 87.4</cell><cell>68.6 71.1 68.8 66.4 73.7 70.4 70.3 60.5 73.8 75.7 81.0 76.5 75.5 80.7 83.5 67.6 72.6 78.9 78.9 81.2</cell><cell>58.5 66.0 60.9 66.8 68.5 70.0 68.9 68.5 72.7 73.3 78.2 76.3 78.6 78.9 79.2 69.9 73.2 78.8 79.6 81.6</cell></row></table><note>* '07': VOC2007 trainval, '07+12': union of VOC2007 and VOC2012 trainval, '07+12+COCO': trained on COCO trainval35k at first and then fine-tuned on 07+12. The S in ION '07+12+S' denotes SBD segmentation labels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III COMPARATIVE</head><label>III</label><figDesc>RESULTS ON VOC 2012 TEST SET (%).</figDesc><table><row><cell>Methods</cell><cell>Trained on</cell><cell>areo</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>mbike</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mAP</cell></row><row><cell>R-CNN(Alex) [15] R-CNN(VGG16) [15] Bayes [85] Fast R-CNN [16] SutffNet30 [100] NOC [114] MR-CNN&amp;S-CNN [110] HyperNet [101] OHEM+Fast R-CNN [113] ION [95] Faster R-CNN [18] Faster R-CNN [18] YOLO [17] YOLO+Fast R-CNN [17] YOLOv2 [72] SSD300 [71] SSD512 [71] R-FCN (ResNet101) [16]</cell><cell>12 12 12 07++12 12 07+12 07++12 07++12 07++12+coco 07+12+S 07++12 07++12+coco 07++12 07++12 07++12+coco 07++12+coco 07++12+coco 07++12+coco</cell><cell>71.8 79.6 82.9 82.3 83.0 82.8 85.5 84.2 90.1 87.5 84.9 87.4 77.0 83.4 88.8 91.0 91.4 92.3</cell><cell>65.8 72.7 76.1 78.4 76.9 79.0 82.9 78.5 87.4 84.7 79.8 83.6 67.2 78.5 87.0 86.0 88.6 89.9</cell><cell>52.0 61.9 64.1 70.8 71.2 71.6 76.6 73.6 79.9 76.8 74.3 76.8 57.7 73.5 77.8 78.1 82.6 86.7</cell><cell>34.1 41.2 44.6 52.3 51.6 52.3 57.8 55.6 65.8 63.8 53.9 62.9 38.3 55.8 64.9 65.0 71.4 74.7</cell><cell>32.6 41.9 49.4 38.7 50.1 53.7 62.7 53.7 66.3 58.3 49.8 59.6 22.7 43.4 51.8 55.4 63.1 75.2</cell><cell>59.6 65.9 70.3 77.8 76.4 74.1 79.4 78.7 86.1 82.6 77.5 81.9 68.3 79.1 85.2 84.9 87.4 86.7</cell><cell>60.0 66.4 71.2 71.6 75.7 69.0 77.2 79.8 85.0 79.0 75.9 82.0 55.9 73.1 79.3 84.0 88.1 89.0</cell><cell>69.8 84.6 84.6 89.3 87.8 84.9 86.6 87.7 92.9 90.9 88.5 91.3 81.4 89.4 93.1 93.4 93.9 95.8</cell><cell>27.6 38.5 42.7 44.2 48.3 46.9 55.0 49.6 62.4 57.8 45.6 54.9 36.2 49.4 64.4 62.1 66.9 70.2</cell><cell>52.0 67.2 68.6 73.0 74.8 74.3 79.1 74.9 83.4 82.0 77.1 82.6 60.8 75.5 81.4 83.6 86.6 90.4</cell><cell>41.7 46.7 55.8 55.0 55.7 53.1 62.2 52.1 69.5 64.7 55.3 59.0 48.5 57.0 70.2 67.3 66.3 66.5</cell><cell>69.6 82.0 82.7 87.5 85.7 85.0 87.0 86.0 90.6 88.9 86.9 89.0 77.2 87.5 91.3 91.3 92.0 95.0</cell><cell>61.3 74.8 77.1 80.5 81.2 81.3 83.4 81.7 88.9 86.5 81.7 85.5 72.3 80.9 88.1 88.9 91.7 93.2</cell><cell>68.3 76.0 79.9 80.8 80.3 79.5 84.7 83.3 88.9 84.7 80.9 84.7 71.3 81.0 87.2 88.6 90.8 92.1</cell><cell>57.8 65.2 68.7 72.0 79.5 72.2 78.9 81.8 83.6 82.3 79.6 84.1 63.5 74.7 81.0 85.6 88.5 91.1</cell><cell>29.6 35.6 41.4 35.1 44.2 38.9 45.3 48.6 59.0 51.4 40.1 52.2 28.9 41.8 57.7 54.7 60.9 71.0</cell><cell>57.8 65.4 69.0 68.3 71.8 72.4 73.4 73.5 82.0 78.2 72.6 78.9 52.2 71.5 78.1 83.8 87.0 89.7</cell><cell>40.9 54.2 60.0 65.7 61.0 59.5 65.8 59.4 74.7 69.2 60.9 65.5 54.8 68.5 71.0 77.3 75.4 76.0</cell><cell>59.3 67.4 72.0 80.4 78.5 76.7 80.3 79.9 88.2 85.2 81.2 85.4 73.9 82.1 88.5 88.3 90.2 92.0</cell><cell>54.1 60.3 66.2 64.2 65.4 68.1 74.0 65.7 77.3 73.5 61.5 70.2 50.8 67.2 76.8 76.5 80.4 83.4</cell><cell>53.3 62.4 66.4 68.4 70.0 68.8 73.9 71.4 80.1 76.4 70.4 75.9 57.9 70.7 78.2 79.3 82.2 85.0</cell></row></table><note>* '07++12': union of VOC2007 trainval and test and VOC2012 trainval. '07++12+COCO': trained on COCO trainval35k at first then fine-tuned on 07++12.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV COMPARATIVE</head><label>IV</label><figDesc>RESULTS ON MICROSOFT COCO TEST DEV SET (%).</figDesc><table><row><cell>Methods</cell><cell>Trained on 0.5:0.95 0.5 0.75 S</cell><cell>M L</cell><cell>1 10 100 S</cell><cell>M L</cell></row><row><cell cols="5">Fast R-CNN [16] ION [95] NOC+FRCN(VGG16) [114] NOC+FRCN(Google) [114] NOC+FRCN (ResNet101) [114] GBD-Net [109] OHEM+FRCN [113] OHEM+FRCN* [113] OHEM+FRCN* [113] Faster R-CNN [18] YOLOv2 [72] SSD300 [71] SSD512 [71] R-FCN (ResNet101) [65] R-FCN*(ResNet101) [65] R-FCN**(ResNet101) [65] Multi-path [112] FPN (ResNet101) [66] Mask (ResNet101+FPN) [67] trainval35k 38.2 60.3 41.7 20.1 41.1 50.2 -train 20.5 39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0 train 23.6 43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6 train 21.2 41.5 19.7 ---------train 24.8 44.4 25.2 ---------train 27.2 48.4 27.6 ---------train 27.0 45.8 ----------train 22.6 42.5 22.2 5.0 23.7 34.6 ------train 24.4 44.4 24.8 7.1 26.4 37.9 ------trainval 25.5 45.9 26.1 7.4 27.7 38.5 ------trainval 24.2 45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4 trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4 trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5 trainval35k 26.8 46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0 trainval 29.2 51.5 -10.8 32.8 45.0 ------trainval 29.9 51.9 -10.4 32.4 43.3 ------trainval 31.5 53.2 -14.3 35.5 44.2 ------trainval 33.2 51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4 trainval35k 36.2 59.1 39.0 18.2 39.0 48.2 -----------Mask (ResNeXt101+FPN) [67] trainval35k 39.8 62.3 43.4 22.1 43.2 51.2 ------DSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4 DSOD300 [74] trainval 29.3 47.3 30.6 9.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF TESTING CONSUMPTION ON VOC 07 TEST SET.</figDesc><table><row><cell>Methods</cell><cell>Trained on</cell><cell>mAP(%)</cell><cell>Test time(sec/img)</cell><cell>Rate(FPS)</cell></row><row><cell>SS+R-CNN [15] SS+SPP-net [64] SS+FRCN [16] SDP+CRC [33] SS+HyperNet* [101] MR-CNN&amp;S-CNN [110] ION [95] Faster R-CNN(VGG16) [18] Faster R-CNN(ResNet101) [18] YOLO [17] SSD300 [71] SSD512 [71] R-FCN(ResNet101) [65] YOLOv2(544*544) [72] DSSD321(ResNet101) [73] DSOD300 [74] PVANET+ [116] PVANET+(compress) [116]</cell><cell>07 07 07+12 07 07+12 07+12 07+12+S 07+12 07+12 07+12 07+12 07+12 07+12+coco 07+12 07+12 07+12+coco 07+12+coco 07+12+coco</cell><cell>66.0 63.1 66.9 68.9 76.3 78.2 79.2 73.2 83.8 63.4 74.3 76.8 83.6 78.6 78.6 81.7 83.8 82.9</cell><cell>32.84 2.3 1.72 0.47 0.20 30 1.92 0.11 2.24 0.02 0.02 0.05 0.17 0.03 0.07 0.06 0.05 0.03</cell><cell>0.03 0.44 0.6 2.1 5 0.03 0.5 9.1 0.4 45 46 19 5.9 40 13.6 17.4 21.7 31.3</cell></row><row><cell cols="5">* SS: Selective Search [15], SS*: 'fast mode' Selective Search [16], HyperNet*: the speed up version of HyperNet and PAVNET+ (compresss): PAVNET with additional bounding box voting and compressed fully convolutional layers.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>BETWEEN STATE OF THE ART METHODS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The bigger wF β is or the smaller MAE is, the better the performance is.</figDesc><table><row><cell>-S</cell><cell>wF β MAE</cell><cell>0.631 0.222</cell><cell>0.640 0.225</cell><cell>0.679 0.221</cell><cell>0.721 0.147</cell><cell>0.764 0.145</cell><cell>0.756 0.157</cell><cell>0.697 0.128</cell><cell>0.818 0.170</cell><cell>0.776 0.063</cell><cell>0.822 0.108</cell><cell>0.767 0.121</cell><cell>0.831 0.099</cell><cell>0.830 0.080</cell></row><row><cell>ECSSD</cell><cell>wF β MAE</cell><cell>0.722 0.195</cell><cell>0.741 0.187</cell><cell>0.787 0.166</cell><cell>0.822 0.107</cell><cell>0.833 0.108</cell><cell>0.827 0.118</cell><cell>0.872 0.037</cell><cell>0.810 0.160</cell><cell>0.849 0.046</cell><cell>0.898 0.071</cell><cell>0.865 0.098</cell><cell>0.905 0.063</cell><cell>0.915 0.052</cell></row><row><cell>HKU-IS</cell><cell>wF β MAE</cell><cell>0.728 0.158</cell><cell>0.726 0.165</cell><cell>0.783 0.143</cell><cell>0.781 0.098</cell><cell>0.860 0.129</cell><cell>0.770 0.118</cell><cell>0.833 0.040</cell><cell>--</cell><cell>0.821 0.043</cell><cell>0.907 0.048</cell><cell>0.844 0.071</cell><cell>0.902 0.048</cell><cell>0.913 0.039</cell></row><row><cell>SOD</cell><cell>wF β MAE</cell><cell>0.655 0.249</cell><cell>0.657 0.242</cell><cell>0.712 0.215</cell><cell>0.708 0.184</cell><cell>0.785 0.155</cell><cell>0.707 0.205</cell><cell>--</cell><cell>0.781 0.150</cell><cell>--</cell><cell>0.832 0.126</cell><cell>0.760 0.154</cell><cell>0.810 0.143</cell><cell>0.842 0.118</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII DETAILED</head><label>VII</label><figDesc>BREAKDOWN PERFORMANCE COMPARISONS OF STATE-OF-THE-ART MODELS ON CALTECH PEDESTRIAN DATASET. ALL NUMBERS ARE REPORTED IN L-AMR.</figDesc><table><row><cell>Method</cell><cell>Reasonable</cell><cell>All</cell><cell>Far</cell><cell>Medium</cell><cell>Near</cell><cell>none</cell><cell>partial</cell><cell>heavy</cell></row><row><cell>Checkerboards+ [198] LDCF++[S2] SCF+AlexNet [210] SA-FastRCNN [211] MS-CNN [105] DeepParts [204] CompACT-Deep [195] RPN+BF [203] F-DNN+SS [207]</cell><cell>17.1 15.2 23.3 9.7 10.0 11.9 11.8 9.6 8.2</cell><cell>68.4 67.1 70.3 62.6 61.0 64.8 64.4 64.7 50.3</cell><cell>100 100 100 100 97.2 100 100 100 77.5</cell><cell>58.3 58.4 62.3 51.8 49.1 56.4 53.2 53.9 33.2</cell><cell>5.1 5.4 10.2 0 2.6 4.8 4.0 2.3 2.8</cell><cell>15.6 13.3 20.0 7.7 8.2 10.6 9.6 7.7 6.7</cell><cell>31.4 33.3 48.5 24.8 19.2 19.9 25.1 24.2 15.1</cell><cell>78.4 76.2 74.7 64.3 60.0 60.4 65.8 74.2 53.4</cell></row><row><cell cols="9">multiple data sources and to combine pedestrian attributes with semantic scene attributes together. Du et al. proposed a deep neural network fusion architecture for fast and robust pedestrian detection</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by the National Natural Science Foundation of China (No.61672203 &amp; 61375047 &amp; 91746209), the National Key Research and Development Program of China (2016YFB1000901), and Anhui Natural Science Funds for Distinguished Young Scholar (No.170808J08).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1627</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Example-based learning for view-based human face detection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">743</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detection of spicules on mammogram based on skeleton analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kobatake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoshinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multi-scale cascade fully convolutional network face detector</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Embedded streaming deep neural networks accelerator with applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. &amp; Learning Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1572" to="1583" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Low-complexity approximate convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Cintra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. &amp; Learning Syst</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. &amp; Learning Syst</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature extraction with deep neural networks by a generalized discriminant analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stuhlsatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zielke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. &amp; Learning Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="596" to="608" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An extended set of haar-like features for rapid object detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maydt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Support vector machine</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A desicion-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Comput. &amp; Sys. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="663" to="671" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge 2007 (voc 2007) results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting eye fixations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face detection with the faster r-cnn</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised transformer network for efficient face detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A real-time pedestrian detector using deep learning for human-aware navigation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miraldo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04441</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey of deep learning methods and software tools for image classification and object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Druzhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kustikova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition and Image Anal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How we know universals the perception of auditory and visual forms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bulletin of Mathematical Biophysics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="147" />
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning internal representation by back-propagation of errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Binary coding of speech spectrograms using a deep autoencoder</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Phone recognition with the mean-covariance restricted boltzmann machine</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weakly supervised object recognition with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Probit analysis: a statistical treatment of the sigmoid response curve</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Wadley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the Entomological Soc. of America</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="549" to="553" />
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning invariant features through topographic filter maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierarchies for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Plant leaf identification via a growing convolution neural network with progressive sample learning</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep learning for content-based image retrieval: A comprehensive study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for pedestrian detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tomè</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baroffio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process.: Image Commun</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="482" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Subcategory-aware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<editor>WACV</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Pedestrian detection based on fast r-cnn and batch normalization</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<editor>ICIC</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Modeling spatialtemporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Attentionnet: Aggregating weak directions for accurate object detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">G-cnn: an iterative grid based object detector</title>
		<author>
			<persName><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Dssd: Deconvolutional single shot detector</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Transforming autoencoders</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<editor>ICANN</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning invariance through imitation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Spiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Histograms of sparse codes for object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Deepbox: Learning objectness with convolutional networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">R-cnn minus r</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06981</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Restructuring of deep neural network acoustic models with singular value decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Fully convolutional instanceaware semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Stuffnet: Using stuffto improve object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<editor>WACV</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Curriculum learning of multiple tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pentina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Rotating your face using multi-task deep neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Multi-stage object detection with group recursive learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05159</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">segdeepm: Exploiting segmentation and context in deep neural networks for object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Learning to detect and localize many objects from few examples</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moysset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05664</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Gated bidirectional cnn for object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware cnn model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">A multipath network for object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02135</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Object detection networks on convolutional feature maps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1476" to="1481" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Pvanet: Lightweight deep neural networks for real-time object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08588</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Scalable, high-quality object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge 2012 (voc2012) results (2012)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2011/workshop/index.html" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06211</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Autocollage</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bordeaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hamadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="847" to="852" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A unified spectral-domain approach for saliency detection and its application to automatic object segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1272" to="1283" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Real-time salient object detection with a minimum spanning tree</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Top-down visual saliency via joint crf and dictionary learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="576" to="588" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">A simple method for detecting salient regions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2363" to="2371" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="989" to="1005" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1045</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Weakly supervised top-down salient object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05345</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for dynamic saliency prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04730</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Supercnn: A superpixelwise convolutional neural network for salient object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="330" to="344" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3919" to="3930" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Saliency detection via combining region-level and pixel-level predictions with cnns</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Edge preserving and multi-scale contextual neural network for salient object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08029</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">A deep multi-level network for saliency prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Visual saliency detection based on multiscale deep cnn features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5012" to="5024" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Recurrent attentional networks for saliency detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Deeply-supervised recurrent convolutional neural network for saliency detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Contextual hypergraph modeling for salient object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04849</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<title level="m" type="main">Design and perceptual validation of performance measures for salient object segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<editor>CVPRW</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Graphical representation for heterogeneous face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="312" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Face recognition from multiple stylistic sketches: Scenarios, datasets, and evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Face sketchcphoto synthesis and retrieval using sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1213" to="1226" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">A comprehensive survey to face hallucination</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="30" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Multiple representations-based face sketch-photo synthesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. &amp; Learning Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2201" to="2215" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition system using deep network-based data fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Behera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="103" to="114" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Multi-view face detection using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Farfade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<editor>ICMR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Face detection through scale-friendly deep convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Scale-aware face detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Face r-cnn</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01061</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Face detection using deep learning: An improved faster rcnn approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08289</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">face detection with end-to-end integration of a convnet and a 3d model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Compact convolutional neural network cascadefor face detection</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kalinovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Spitsyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEUR Workshop</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Joint training of cascaded cnn for face detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<editor>IJCB</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<monogr>
		<title level="m" type="main">Object detection with pixel intensity comparisons organized in decision trees</title>
		<author>
			<persName><forename type="first">N</forename><surname>Markuš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frljak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Pandžić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1305.4537</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Learning surf cascade for fast and accurate object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">A fast and accurate unconstrained face detector</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="223" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multitask learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01249</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Multiple pedestrian tracking from monocular videos in an interacting multiple model framework</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1361" to="1375" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Multi-cue pedestrian detection and tracking from a moving vehicle</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Munder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="41" to="59" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Jointly attentive spatial-temporal pooling networks for video-based person reidentification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Stepwise metric promotion for unsupervised video person re-identification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Cooperative robots to observe moving targets: Review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="187" to="198" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Robotics Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Pedestrian detection with spatially pooled features and structured ensemble learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1243" to="1257" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Discriminatively trained and-or graph models for object shape detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="959" to="972" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Handling occlusions with franken-classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Detection and tracking of occluded people</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="58" to="69" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Multispectral deep neural networks for pedestrian detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02644</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">Fused dnn: A deep neural network fusion approach to fast and robust pedestrian detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<editor>WACV</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Pushing the limits of deep cnns for pedestrian detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m" type="main">Reduced memory region based deep convolutional neural network detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tomé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baroffio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Plebani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pau</surname></persName>
		</author>
		<editor>ICCE-Berlin</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<monogr>
		<title level="m" type="main">Scale-aware fast r-cnn for pedestrian detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08160</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Visual-textual joint relevance learning for tag-based social image search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Ron: Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Object detection meets knowledge graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Saliency-based sequential image attention with multiset prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Learning detection with diverse proposals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Real time image saliency for black box classifiers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dabkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Craft objects from images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Unsupervised learning from video to detect foreground objects in single images</title>
		<author>
			<persName><forename type="first">I</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-V</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Training object class detectors with click supervision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Mimicking very efficient network for object detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="38" to="39" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>3d object proposals for accurate object class detection,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Visual-inertial-semantic scene representation for 3d object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Peng Zheng is a Ph.D. candidate at Hefei University of Technology since 2010. He received his Bachelor&apos;s degree in 2010 from Hefei University of Technology. His interests cover pattern recognition, image processing and computer vision. Shou-tao Xu is a Master student at Hefei University of Technology. His research interests cover pattern recognition, image processing, deep learning and computer vision</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hefei University of Technology, China. He obtained the Ph.D. degree in Pattern Recognition &amp; Intelligent System at University of Science and Technology</title>
				<meeting><address><addrLine>China; Hongkong, China</addrLine></address></meeting>
		<imprint>
			<publisher>TKDE</publisher>
			<date type="published" when="2005">2017. 2007. April 2008 to November 2009. January 2013 to December 2014. 2005. 2008</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science of Hongkong Baptist University</orgName>
		</respStmt>
	</monogr>
	<note>He is the Steering Committee Chair of the IEEE International Conference on Data Mining (ICDM), the Editor-in-Chief of Knowledge and Information Systems. AI&amp;KP). He was the Editor-in-Chief of the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
