<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An artificial neural network as a troubled-cell indicator</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-04-17">April 17, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Deep</forename><surname>Ray</surname></persName>
							<email>deep.ray@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><forename type="middle">S</forename><surname>Hesthaven</surname></persName>
							<email>jan.hesthaven@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An artificial neural network as a troubled-cell indicator</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-04-17">April 17, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">90FBF163F7EE605456D8944884479E17</idno>
					<idno type="DOI">10.1016/j.jcp.2018.04.029</idno>
					<note type="submission">Received date: 14 November 2017 Revised date: 1 March 2018 Accepted date: 13 April 2018 Preprint submitted to Elsevier</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conservation laws</term>
					<term>Discontinuous Galerkin</term>
					<term>Limiting</term>
					<term>Troubled-cell indicator</term>
					<term>Artificial neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• Design of a multilayer perceptron serving as a troubled-cell indicator.</p><p>• The proposed indicator is free of problem-dependent parameters.</p><p>• Correct classification of cells with smooth extrema.</p><p>• Testing with scalar and systems of conservation laws in one-dimension.</p><p>• Comparison with minmod-type TVB limiter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is well known that solutions to conservation laws often develop discontinuities in finite time, even for smooth initial data <ref type="bibr" target="#b0">[1]</ref>. Thus, numerical methods need to be carefully corrected near discontinuities to avoid spurious Gibbs oscillations. The approach used to handle discontinuities numerically can be broken into two key steps. The first step involves detecting the (mesh) cells where the solution loses regularity. Such cells are termed as troubled-cells. In the second step, the solution in these cells are corrected to avoid spurious oscillations. Several correction techniques are available for use in numerical schemes, such as slope limiting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, adaptively choosing the stencil for reconstruction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, or adding artificial diffusion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. For most numerical methods, the two stages are usually combined into a single step. However, it is useful to consider them separately for Runge-Kutta discontinuous Galerkin (RKDG) schemes <ref type="bibr" target="#b10">[11]</ref>.</p><p>The correction (or limiting) of the numerical solution in troubled-cells can be computationally expensive. To ensure cost-efficiency of numerical schemes, it is essential to use troubled-cell indicators that only flag the genuine troubled-cells. In <ref type="bibr" target="#b11">[12]</ref>, a thorough numerical study was performed to assess the performance of various limiter-based troubled-cell indicators for RKDG schemes. It was observed that the classical minmod limiter flags more cells than necessary, including cells with smooth extrema, which can lead to an unnecessary increase in computational cost. As an alternative, the minmod-type TVB limiter <ref type="bibr" target="#b12">[13]</ref> seeks to correctly identify troubled-cells, provided its problem-dependent parameter M is chosen appropriately. In general, however, it is difficult to estimate M a priori. Thus, there is a need to construct a troubled-cell indicator that flags genuine troubled-cells, and is independent of problem-dependent parameters. Recently, an automatic parameter selection strategy for troubled-cell indicators was proposed in <ref type="bibr" target="#b13">[14]</ref>, which is based on Tukey's boxplot method of outlier-detection <ref type="bibr" target="#b14">[15]</ref>. This approach has also been tested with compact-WENO finite difference methods in <ref type="bibr" target="#b15">[16]</ref>. The outlier-detection algorithm requires the input solution vector to be sorted, which in general scales as O(N log N ) on a mesh with N elements. A new indicator was proposed for RKDG schemes in <ref type="bibr" target="#b16">[17]</ref>, which is shown to perform well with the Euler equations in one-and two-dimensions, provided an empirically determined parameter is chosen. However, this parameter only seems to depend on the degree of the approximating polynomial. In the present paper, we propose a new type of indicator by constructing an artificial neural network (ANN) that serves as a troubled-cell indicator, without requiring the prescription of any problem-dependent parameters.</p><p>In principle, ANNs can be seen as function approximators capable of capturing a high-degree of complexity and non-linearity. The design of ANNs is based on the architecture of their biological counterpart, and have the capacity to learn <ref type="bibr" target="#b18">[18]</ref>. Once suitably trained on a given dataset, ANNs are able to recover key features of the underlying model, and accurately predict the output for data points lying outside the training dataset. Although the training of the network can be computationally-intensive and time consuming, it is done offline and only once for a given application. Thereafter, the trained network is used as a black-box, with the involved computations being inexpensive. For this reason, ANNs are popular in applications such as image identification <ref type="bibr" target="#b19">[19]</ref> and speech recognition <ref type="bibr" target="#b20">[20]</ref>, as a substitute for complex rule-based algorithms which are often difficult to program. ANNs have also been used to solve certain classes of ordinary and partial differential equations <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>In this paper, we consider a specific type of ANN, know as a multilayer perceptron (MLP), which consists of neurons stacked in a series of layers. Under mild assumptions on the network design, it has been shown that MLPs with one or two hidden layers, i.e., layers between the first and the last, can approximate any continuous function <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b26">25]</ref>. The approximation capabilities of simple MLP networks have been studied quite extensively (see <ref type="bibr" target="#b27">[26]</ref> and references therein). However, rigorous results for more general and complex networks remain elusive.</p><p>We propose a deep MLP network, which is trained to inherit the properties of a troubled-cell indicator. The training is performed offline on a robust dataset, and the final network is used within the framework of RKDG schemes. The network is independent of any problem-dependent parameter, thus making it attractive as a universal troubled-cell indicator for general conservation laws.</p><p>The rest of the paper is structured as follows. In Section 2 we introduce the RKDG formulation, followed by a brief discussion of a few existing troubled-cell indicators. We motivate the construction of MLPs in Section 3, and give an overview of the key ingredients required to construct and train such networks. In Section 4, we present the construction of an MLP based troubled-cell indicator. Several numerical results are presented in Section 5 to demonstrate the capability of the proposed network, as compared to existing limiter-based troubled-cell indicators. We make a few concluding remarks in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Discontinuous-Galerkin Formulation</head><p>We consider the following one-dimensional scalar conservation law</p><formula xml:id="formula_0">∂u ∂t + ∂f (u) ∂x = 0 ∀ (x, t) ∈ [a, b] × [0, T ], u(x, 0) = u 0 (x) ∀ x ∈ [a, b], (2.1)</formula><p>where u is the conserved variable with the smooth flux f (u). To approximate the solution of (2.1), the computational domain is discretized using N non-overlapping cells, with cell-interfaces</p><formula xml:id="formula_1">a = x 1 2 &lt; x 3 2 &lt; ... &lt; x N + 1 2 = b. The center of cell I i = [x i-1 2 , x i+ 1 2 ] is denoted by x i = (x i-1 2 + x i+ 1 2 )/2.</formula><p>For the rest of this paper, we assume the discretization to be uniform, with the mesh size denoted by h = x i+ 1  2x i-1 2 . We define the space of broken polynomials</p><formula xml:id="formula_2">V h r = {v ∈ L 2 ([a, b]) : v Ii ∈ P r (I i )}</formula><p>, where P r (I i ) is the space of polynomials with degree d r on the cell I i . The semi-discrete DG scheme can be formulated as follows.</p><formula xml:id="formula_3">Definition 2.1. Find u h (., t) ∈ V h r such that the following semi-discrete relation is satisfied for all v h ∈ V h r , Ii ∂u h ∂t v h -f (u h ) dv h dx dx + fi+ 1 2 (t)v h (x - i+ 1 2 ) -fi-1 2 (t)v h (x + i-1 2 ) = 0,<label>(2.2</label></formula><p>)</p><formula xml:id="formula_4">where v h (x ± ) = lim ↓0 v h (x ± ) and fi+ 1 2 (t) = f u h (x - i+ 1 2 , t), u h (x + i+ 1 2 , t) is a consistent numerical flux.</formula><p>In practice, the solution in each cell I i is represented using a suitable local basis {φ ij (x), j = 0, ..., r} as</p><formula xml:id="formula_5">u h (x, t) = r j=0 u ij (t)φ ij (x) ∀ x ∈ I i ,</formula><p>where the coefficients u i0 (t), ...u ir (t) are the degrees of freedom to be determined using the numerical scheme. Defining the vector U i = (u i0 , ..., u ir ) and taking</p><formula xml:id="formula_6">v h = φ ij gives us Ii ∂u h ∂t φ ij dx = r k=0 du ik dt Ii φ ik φ ij dx = r k=0 M (i) jk du ik dt , (<label>2.3)</label></formula><p>where</p><formula xml:id="formula_7">M (i)</formula><p>jk are the elements of the mass-matrix M (i) , corresponding to cell I i . Note that the mass-matrix can be computed using a quadrature rule which is exact for polynomials of degree 2r. The remaining terms of the (2.2) can be approximated as</p><formula xml:id="formula_8">R (i) (U(t)) j = Ii f (u h ) dφ ij dx dx -fi+ 1 2 (t)φ ij (x - i+ 1 2 ) + fi-1 2 (t)φ ij (x + i-1 2 ) ≈ q w iq f u h (x iq , t) dφ ij dx (x iq )dx -fi+ 1 2 (t)φ ij (x - i+ 1 2 ) + fi-1 2 (t)φ ij (x + i-1 2 ),<label>(2.4)</label></formula><p>where x iq and w iq are the nodes and weights, respectively, for a suitable q-point quadrature. Using (2.3), (2.4) and the fact that the mass-matrix is invertible, we obtain the following system of ordinary differential equations corresponding to cell I i</p><formula xml:id="formula_9">dU i dt = M (i) -1 R (i) (U(t)),</formula><p>which is solved using a suitable time-marching scheme, such as the third-order strong stability preserving Runge-Kutta (SSP-RK3) scheme <ref type="bibr" target="#b28">[27]</ref>. While the scheme described above is capable of approximating smooth solutions with a highdegree of accuracy, it suffers from Gibbs oscillations near discontinuities. A commonly used technique to mitigate this issue is by correcting the approximating polynomial in troubled-cells after each Runge-Kutta stage. This procedure comprises two steps: i) detection of troubled-cells, and ii) a suitable limited reconstruction of the polynomial solution in the troubled-cells.</p><p>In general, troubled-cell indicators analyze the smoothness of the data in the cell I i based on the information extracted from a 3-cell compact stencil centered at i, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. In each cell, the solution is approximated by a polynomial (solid lines), with corresponding cell-averaged values u i-1 , u i , u i+1 (dashed lines). In addition, we require the left and the right cell-interface values of the polynomial in I i , i.e., u</p><formula xml:id="formula_10">+ i-1 2 and u - i+ 1 2</formula><p>. Using these five quantities, we construct the following forward and backward differences,</p><formula xml:id="formula_11">Δ -u i = u i -u i-1 , Δ + u i = u i+1 -u i , ǔi = u i -u + i-1 2 , ûi = u - i+ 1 2 -u i . (<label>2.5)</label></formula><p>A troubled-cell indicator uses the differences (2.5) to modify the cell-interface values as</p><formula xml:id="formula_12">u + i-1 2 = u i + M ǔi , Δ -u i , Δ + u i ; Z , u - i+ 1 2 = u i -M ûi , Δ -u i , Δ + u i ; Z , (2.6)</formula><p>where M is a slope-limiter and Z denotes additional parameters that the slope-limiter may depend upon. The cell I i is marked as a troubled-cell, if the modifications in (2.6) changes either of the two cell-interface values, i.e., if</p><formula xml:id="formula_13">u + i-1 2 = u + i-1 2 or u - i+ 1 2 = u - i+ 1 2</formula><p>. A thorough numerical comparison of various limiter-based indicator functions has been performed in <ref type="bibr" target="#b11">[12]</ref>. We consider the following two commonly used limiters: 1. The minmod limiter : This slope-limiter modifies the cell-interface values to ensure that the solution is total variation diminishing in the mean (TVDM) <ref type="bibr" target="#b7">[8]</ref>. It is given by</p><formula xml:id="formula_14">I i-1 I i I i+1 ūi-1 ūi ūi+1 u + i-1 2 u - i+ 1 2 i + 1 2 i + 3 2 i -1 2 i -3 2</formula><formula xml:id="formula_15">M mm (a, b, c) = sign(a) min(|a|, |b|, |c|) i fs i g n ( a) = sign(b) = sign(c), 0 otherwise. (<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.7)</head><p>A disadvantage with the minmod limiter is that it also flags cells containing a smooth extrema, which can lead to an increased computational cost, and limits local accuracy to first-order. 2. The minmod-type TVB limiter : In order to overcome the problems of the minmod limiter, one can relax the TVDM condition by requiring the solution to be total variation bounded (TVB). This can be achieved by the following modified slope-limiter</p><formula xml:id="formula_16">M tvb (a, b, c; h, M ) = a if |a| Mh 2 , M mm (a, b, c) otherwise,<label>(2.8)</label></formula><p>where the limiter now depends on the local mesh-size h and a problem-dependent parameter M . For scalar conservation laws, M is proportional to the curvature of the initial condition near smooth extrema <ref type="bibr" target="#b12">[13]</ref>. However, it is difficult to estimate M for a general system of conservation laws. If M is chosen too small, (2.8) essentially reduces to the minmod limiter (2.7). On the other hand, if M is chosen too large, the indicator does not flag all the troubledcells, leading to the re-appearance of Gibbs oscillations.</p><p>Based on the two limiters discussed above, we seek a troubled-cell indicator which i) does not flag cells with smooth extrema, and ii) is independent of problem specific parameters. To accommodate both these traits, we propose a new approach to the problem of discontinuity detection, via an artificial neural network (see Section 3).</p><p>For the reconstruction step, we can replace the polynomial in the troubled-cells by the cellaverage or a limited linear polynomial <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b7">8]</ref>. This leads to a loss in accuracy if cells are incorrectly flagged by the indicator. To overcome this issue, one can use WENO limiters to rebuild the polynomial in flagged cells by extending the polynomial in neighboring cells <ref type="bibr" target="#b10">[11]</ref>. However, finding the WENO weights is computationally expensive, re-emphasizing the need to not flag more cells than necessary as troubled-cells.</p><p>Since the focus of this paper will be on the constructing a suitable troubled-cell indicator, we use the classical MUSCL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> reconstruction procedure for the second step. Specifically, if the unlimited polynomial approximation in a troubled-cell I i is written as</p><formula xml:id="formula_17">u h (x) = u i + (x -x i )s i + O (x -x i ) 2 ,</formula><p>then the limited linear polynomial is given by ũh</p><formula xml:id="formula_18">(x) = u i + (x -x i )M s i , u i -u i-1 h , u i+1 -u i h ,</formula><p>where M is some slope-limiter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Artificial Neural Networks</head><p>Our goal is to find a suitable approximation to an unknown function</p><formula xml:id="formula_19">G : Ω ∈ R N I → R N O , given T = {(X p , Y p ) | Y p = G(X p ) ∀ p ∈ Λ}. (3.1)</formula><p>For our problem, this corresponds to the underlying true indicator function that correctly flags the genuine troubled-cells. Simple approaches such as linear-regression or least-squares polynomial fitting, are unsuitable if G is highly non-linear. In fact, we have no information about the qualitative features of the troubled-cell indicator. Thus, it is fruitful to look for an approximation which is capable of learning these unknown features based on the dataset T. This is precisely what we hope to accomplish using artificial neural networks, inspired by biological networks. The biological nervous system in vertebrates is responsible for transmitting information through the organism, facilitating the coordination between different body parts. A specialized nerve cell, known as the neuron, forms the fundamental building block of the nervous system. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, a simplified model of the neuron consists of three main components, namely the dentrites, the nucleus and the axon. The dentrites are nerve fibers through which the neuron receives signals from several input neurons, with the connection between two neurons being termed as a synapsis. The signal from the input neuron is pre-processed in the synapsis, before being transmitted to the receiving neuron. Thus, the synapsis can be seen as a weighted connection. The weighted accumulation of the signals received by the neuron is stored in the nucleus. Once the accumulation crosses a certain threshold, the nucleus fires an electrical signal through the axon to other connecting neurons. A complex network of neurons is involved in passing electrical impulses throughout the body <ref type="bibr" target="#b30">[29]</ref>. Various studies have shown that knowledge is gained through a training process, in which synaptic connections are created or modified when exposed to different environmental situations. Based on this learning, the organism is able adapt and react appropriately to more general situations. An artificial neural network (ANN) can be seen as a numerical black-box, designed to mimic the training procedure of the biological network. Mathematically, an ANN is described by the triplet (N , V, W). Here, N is the set of all artificial neurons in the network, while V represents the set of all directed connection (i, j), i, j ∈ N , where i is the sending neuron and j is the receiving neuron. The neural connections are weighted, with W being the set of weights w i,j for the connections (i, j). A single neuron j, receiving signals y s1 , ..., y s k from s 1 , ..., s k sending neurons, is depicted in Figure <ref type="figure" target="#fig_2">3</ref>. The weighted accumulation q j stored in the neuron can be expressed in terms of a propagation function</p><formula xml:id="formula_20">q j = f prop (y s1 , ..., y s k , w s1,j , ..., w s k ,j ).</formula><p>In practice, the propagation function is chosen to be linear,</p><formula xml:id="formula_21">f prop (y s1 , ..., y s k , w s1,j , ..., w s k ,j ) = k i=1 w si,j y si ,</formula><p>which is also the choice we adhere to in this paper. The neuron j transmits a scalar-valued signal, provided the accumulation crosses a certain threshold or bias -b j (in literature, the bias is often taken as the negative of the threshold value). This aspect is modeled using a non-linear activation function y j = f act (q j + b j ).</p><p>The simplest example of an activation function is the Heaviside function</p><formula xml:id="formula_22">h(x) = 0 if x &lt; 0 1 if x &gt; 0 , (<label>3.2)</label></formula><p>which leads to the McCulloch-Pitts neuron model <ref type="bibr" target="#b31">[30]</ref>. However, (3.2) is not preferred in practice since its derivative vanishes everywhere (see discussion in Section 3.3).</p><formula xml:id="formula_23">w s 1 ,j w s 2 ,j w s k-1 ,j w s k ,j y s 1 y s 2 y s k-1 y s k q j b j</formula><p>Activation function </p><formula xml:id="formula_24">y j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-layer perceptron</head><p>Several architectures have been proposed for ANNs <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b32">31]</ref>, describing the arrangement and connectivity of neurons in the network. We focus on a specific architecture known as a multi-layer perceptron (MLP), in which the neurons arranged in several layers. The first layer with N I source neurons is called the input layer, while the last layer with N 0 neurons is termed as the output layer. The remaining layers lying in between are called the hidden layers, with the k-th hidden layer consisting of N k neurons. The structure of an MLP with two hidden layers is shown in Figure <ref type="figure" target="#fig_3">4</ref>. The neurons in a given layer receive signals from the layer preceding it, and send signals to the succeeding layer. Furthermore, the neurons within a single layer do not communicate with each other. No computations occur inside the input layer, and it simply provides the source signal to the network. The signals from the output layer do not pass through an activation function, but may pass through an output function to convert the signals to a meaningful form. For instance, for the classification problem, the output values should lie between 0 and 1 to indicate the probability of the input belonging to a particular class. Since the signal transmitted by each neuron is scalarvalued, the input and output of the MLP matches the dimensions of the domain and range spaces of the function (3.1) being approximated.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training the network</head><p>The appropriate weights and biases of the network are obtained by training the MLP on a given dataset T. The training procedure is essentially an iterative algorithm which tunes the network weights to accurately predict the responses corresponding to the set T. In addition, the trained network must be capable of predicting the responses for data outside T, with a suitable degree of accuracy. This property of the network is termed as generalization.</p><p>There are several training strategies that one can opt for, a detailed description of which can be found in <ref type="bibr" target="#b30">[29]</ref>. For our model, we consider the training paradigm termed as supervised learning, where the true responses for the points in the training set are known a priori. The training aims to minimize the error between the prediction and the truth. More precisely, a cost function C is defined</p><formula xml:id="formula_25">C := C(Y, Ŷ), Y = G(X), Ŷ = Ĝ(X), ∀ X ∈ R N I ,</formula><p>where Ĝ represents the approximation of G by the neural network. The function C can be seen as a measure of the discrepancy between the predicted response Ŷ and the true response Y. Supervised learning aims to find the optimal values for the weights and biases of the network to minimize C over the training set T.</p><p>Remark 3.1. Even if C is a convex function of Y, Ŷ, it need not be convex in terms of the weights and biases. Thus, the optimization algorithm may converge to a local minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Activation function</head><p>The activation function used in the MLP network is responsible for introducing non-linearity into the model. As mentioned earlier, the most obvious choice for the activation function is the Heaviside function (3.2). To understand why this may not be a suitable choice, we note that most optimization algorithms, such as gradient descent, update the weight of the connection between the neuron pair (i, j) iteratively as</p><formula xml:id="formula_26">w s+1 i,j = w s i,j + Δw i,j , Δw i,j = -η ∂C ∂w i,j ,</formula><p>where Δw i,j is the update step and η &gt; 0 is the learning rate. The evaluation of the gradient of the cost function involves the gradient of the activation function, which is essentially zero for the Heaviside function. Thus, the updates in each iteration would be very small, leading to a slow convergence of the algorithm. Other non-linear choices for the activation function include the logistic function and the hyperbolic tangent (see Figure <ref type="figure">5</ref>), which are smooth functions capturing the qualitative features of the Heaviside function. However, these too suffer from vanishing gradients as we move away from the origin. A popular activation function used by most practitioners, is the rectified linear unit (ReLU) <ref type="bibr" target="#b33">[32]</ref>, shown in Figure <ref type="figure">5(c)</ref>. Training with ReLU is often faster, when compared to models using the logistic or hyperbolic tangent activation function <ref type="bibr" target="#b34">[33]</ref>. This can been justified by noting the linear, non-saturating form of the ReLU function, and the inexpensive evaluation of the function itself. Unfortunately, the ReLU function can suffer from the issue of dying neurons during the training process. If an update of weights and the bias for a particular neuron j leads to its deactivation, i.e., q j + b j &lt; 0, then the neuron may never activate again for the rest of the training. Thus, by the end of the training, most of the neurons are left inactive in the network. A proposed fix to this problem is to consider the leaky ReLU function <ref type="bibr" target="#b35">[34]</ref>, which modifies the original ReLU by adding a small negative slope ν when x &lt; 0, as shown in Figure <ref type="figure">5</ref>(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">An MLP trouble-cell indicator</head><p>We now describe the design of an MLP-based troubled-cell indicator. The function we wish to approximate is the true troubled-cell indicator. As is the case with the slope-limiters discussed in Section 2, the input for our MLP is the vector</p><formula xml:id="formula_27">(u i-1 , u i , u i+1 , u + i-1 2 , u - i+ 1 2</formula><p>) ∈ R 5 , i.e., the input layer has N I = 5 neurons. We use 5 hidden layers, whose widths vary as 256, 128, 64, 32 and 16 as we move from the input layer to the output layer. Based on the discussion in Section 3.3, we choose the leaky ReLU as our activation function. The output layer has a width of 2 neurons, giving the output X O ∈ R 2 . Finally, the vector X O is put through the softmax output function to give the output Ŷ</p><formula xml:id="formula_28">Ŷ 1 = e X 1 e X 1 + e X 2 , Ŷ 2 = e X 2 e X 1 + e X 2 , X O = (X 1 , X 2 ) . (4.1)</formula><p>The softmax transforms the vector of arbitrary real numbers into a vector of real values in [0, 1] which sum up to unity. Thus, the final output can be viewed as the probability that the cell I i falls into either of two classes: a troubled-cell or a good-cell. Specifically, Ŷ 1 represents the probability that the cell in question is a troubled-cell.</p><p>The cost functional used to train the model is given by the cross entropy function</p><formula xml:id="formula_29">C = - 1 S S k=1 Y 1 k log( Ŷ 1 k ) + Y 2 k log( Ŷ 2 k ) ,</formula><p>where S is the number of samples used for training, while values 0 or 1. The cross-entropy function is closely related to the Kullback-Leibler divergence, which measures the discrepancy between two probability distributions <ref type="bibr" target="#b36">[35]</ref>. In our case, we aim to measure and minimize the discrepancy between the probability distribution about the type of a given cell predicted by the MLP and the true boolean distribution.</p><formula xml:id="formula_30">Y k = (Y 1 k , Y 2 k ) is</formula><p>The training is performed using a stochastic optimization algorithm, which uses mini-batches of size S b from the training set, to take a single optimization step. More specifically, the full training set with S data-points is shuffled, following which mini-batches with S b &lt; S samples are sequentially extracted to take S/S b optimization steps. Once the entire training set is exhausted, the training is said to have completed one full epoch. The training set is then reshuffled and the process is repeated for several epochs. The shuffling introduces stochasticity in the training data set, and has been observed to lead to faster convergence <ref type="bibr" target="#b37">[36]</ref>.</p><p>The network topology and the choice of cost function can lead to overfitting of the network to the training dataset, which can severely effect the ability of the network to generalize. A commonly used method to avoid overfitting involves the regularization of the cost functional <ref type="bibr" target="#b38">[37]</ref> by penalizing the weights W of the network</p><formula xml:id="formula_31">C = C + β W 2 2 , β 0,<label>(4.2)</label></formula><p>where W 2 2 represents the squared sum of all the weights in the MLP. The early stopping of the training is yet another commonly used approach, which makes use of a validation data set V that is independent of the training set T <ref type="bibr" target="#b38">[37]</ref>. After each epoch, the responses of the MLP is evaluated over V and the accuracy of the output is evaluated. As the training evolves, the accuracy on the validation set ideally increases, signifying that the MLP is capable of generalization. However, the validation accuracy starts decreasing after a point due to over-fitting. We evaluate the accuracy of the MLP responses over the set V as</p><formula xml:id="formula_32">V acc = #{X ∈ V | Ŷ = Ĝ(X), Ŷ 1 0.5} #V × 100,<label>(4.3)</label></formula><p>where Ŷ 1 is the probability of the cell being a troubled-cell. Based on this evaluation, an early stopping criteria is used, wherein the training is terminated if the accuracy on the validation set decreases for L consecutive epochs. Since the cost-function is not convex, the choice of initial conditions can strongly influence the local minima to which the optimization converges. Thus, we restart the training process R times with different initializations of the weights and biases, and select the model with the best generalization (measured in terms of the final validation accuracy). The training is performed using TensorFlow, which is an open-source software library for machine learning <ref type="bibr" target="#b39">[38]</ref>. The offline training procedure to obtain the optimal wights and biases for the MLP, is outlined in Algorithms 1 and 2. The MLP with the following structure of weights W and biases b</p><formula xml:id="formula_33">W = {W 1 , W 2 , W 3 , W 4 , W 5 , W O } with W 1 ∈ R 256×5 , W 2 ∈ R 128×256 , W 3 ∈ R 64×128 , W 4 ∈ R 32×64 , W 5 ∈ R 16×32 , W O ∈ R 2×16 , b = b 1 ∈ R 256 , b 2 ∈ R 128 , b 3 ∈ R 64 , b 4 ∈ R 32 , b 5 ∈ R 16 , b O ∈ R 2 . (4.4)</formula><p>is implemented in Algorithm 3. To obtain the gradient of the cost function (4.2), we also need to differentiate the network itself. In Tensorflow, this is achieved by using automatic differentiation <ref type="bibr" target="#b40">[39]</ref> on the MLP algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generating the training and validation sets</head><p>We briefly discuss the generation of the sets T and V used to train the MLP network. The methodology can be described via the following steps:</p><p>1. Choose a function u(x) such that complete information about its regularity is available in the interval [a, b]. For instance we can choose the sine-wave or a step-function. 2. Pick a point x i ∈ [a, b] and set a mesh size h ensuring that a x i - 3  2 h &lt; x i + 3 2 h b. Thus, we can construct a 3-cell stencil centered at x i , which is contained in the interval [a, b] (see Figure <ref type="figure" target="#fig_8">6</ref>). 3. In each cell, project the solution onto the space of polynomials of degree r. We use the Legendre polynomials for the projection. 4. Find the cell-averages of the approximating polynomials in each of the three cells to obtain the values</p><formula xml:id="formula_34">u i-1 , u i , u i+1 . Also extract u + i-1 2 , u - i+ 1 2</formula><p>using the polynomial in the cell I i . Thus, we have generated the input vector X corresponding to the cell I i . 5. The true output corresponding to X depends on the regularity of the solution in the stencil.</p><p>Ideally, if the solution in cell I i loses regularity, i.e., has a discontinuity, or is continuous but not differentiable, the cell is flagged as a troubled-cell with the true output Y = (1, 0) . Otherwise, the output is Y = (0, 1) .   In practice, we construct true outputs by flagging the cell I i as a troubled-cell if regularity is lost in the wider zone [x i - 3  2 h, x i + 3 2 h]. This ensures the the indicator is more robust in terms of capturing the discontinuities.</p><formula xml:id="formula_35">V acc = EVAL ACC(N , V, W(t + 1), b(t + 1), V) 9 if V acc &lt; V r acc then l ← l + 1 else l = 0 end t ← t + 1 end V r acc = EVAL ACC(N , V, W(t -l), b(t -l), V) if V r acc &gt; V opt acc then W opt ← W(t -l)</formula><formula xml:id="formula_36">x i x i-1</formula><p>x i+1 a b In each cell of the stencil, the function is approximated using a polynomial (dashed line).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">A word on the computational cost</head><p>We briefly discuss the computational cost associated with the MLP indicator, which can be broken into two part: the training cost and the cost involved with using the trained MLP. We first elaborate on the latter, by analyzing the key components of Algorithm 3. In each operational layer i of the MLP with W{i} ∈ R m×n , the complexities of the propagation and activation steps are O(mn) and O(m), respectively. Thus, for our MLP determined by (4.4), the complexity for a single data point X ∈ R 5 will be dominated by the computations in the second hidden layer, which is given by = O(128 × 256). Note that is independent of the mesh size, or the number of elements. For a mesh with N elements, the total computational cost for using the MLP indicator on the whole mesh is O( N ). In other words, the algorithm scales linearly with the N .</p><p>Let us now estimate the training cost by considering Algorithms 1 and 2. The evaluation of the function MINIBATCH OPT is the most expensive component of the training procedure. Let us denote by ϑ, the complexity of evaluating ΔW and Δb for each mini-batch. Note that ϑ depends on the choice of the optimizer, the cost function C, the size of the network being trained, as well as the mini-batch size S b . Since this has to be evaluated for each mini-batch, the complexity of Algorithm 2 is O(ϑN T /S b ), where N T = #T. We have an additional cost of O( N V ) for evaluating the function EVAL ACC in each epoch of Algorithm 1, where N V = #V. We get the maximal complexity by assuming that the stopping criteria does not come into play for any of the R training cycles. Thus, the complexity of the full training algorithm can be estimated as O (RT (ϑN T /S b + N V )). Although the training procedure can be expensive for large data sets T and V, it needs to be done just once, following which the trained MLP is used as a black-box indicator for all one-dimensional conservation laws.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Numerical results</head><p>We now demonstrate the capability of the MLP network when used as a troubled-cell indicator in the RKDG framework. The MLP is trained offline by setting the slope parameter for leaky ReLU as ν = 10 -3 , and the cost regularization parameter as β = 10 -2 . We also use an early stopping criteria, by setting L = 10. The momentum based Adam stochastic optimizer <ref type="bibr" target="#b41">[40]</ref> is used to minimize the cost function (4.2), with an initial learning rate of η = 10 -3 . The training and validation datasets are constructed using a number of functions, which are listed in Tables 1(a)-(b). For each function, the mesh size h and the approximating polynomial degree r are varied. Some of the functions have additional parameters, which are also varied to generate additional sample points. Furthermore, we use mini-batches of size S b = 500 after reshuffling the training set for each epoch. The training is restarted R = 10 times, with the weights and biases randomly initialized using a normal distribution at the beginning of each training. The model with the highest validation accuracy at the end, is chosen as the MLP indicator.  The DG scheme is evaluated using the local Lax-Friedrichs flux. We compare the performance of the trained MLP indicator with that of the minmod-limiter and the minmod-type TVB limiter. The notation TVB-1, TVB-2, and TVB-3 are used to refer to the TVB limiter with the parameter M = 10, M = 100, and M = 1000, respectively. The limited reconstruction in troubled-cells is performed using the MUSCL-scheme with the minmod limiter. The semi-discrete DG scheme is integrated in time using SSP-RK3. The computational cost of the scheme depends greatly on the number of cells flagged as troubled-cells. In order to analyze this cost, we also plot the troubled-cells detected by the various indicators at each time-step. All test cases considered in this paper have been simulated for r = 1,2,3 and 4. However, only the results with r = 4 are presented for most problems, due to paucity of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>u(x) Domain Additional parameters varied</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Good cells</head><formula xml:id="formula_37">Troubled cells sin(4πx) [0, 1] - 4470 0 ax [-1, 1] a ∈ R 10000 0 a|x| [-1, 1] a ∈ R 800 3200 ul.(x &lt; x 0 ) + ur.(x &gt; x 0 ) (only troubled-cells selected) [-1, 1] (u l , u r ) ∈ [-1, 1] 2 x 0 ∈ [-0.</formula><p>In addition to uniform grids, we also compute the results on randomly perturbed meshes obtained by modifying the (interior) cell-interfaces of a uniform mesh as follows</p><formula xml:id="formula_38">x i+ 1 2 -→ x i+ 1 2 + θhω i+ 1 2 , ω i+ 1 2 ∈ U([-0.5, 0.5]), i= 1, ..., N -1,<label>(5.1)</label></formula><p>where θ controls the magnitude of perturbation. We choose θ = 10% for all simulations.</p><p>We also demonstrate the performance of the MLP indicator on systems of conservation laws, by considering the one-dimensional shallow water equations, as well as the Euler equations. In the case of systems, we have the freedom to choose the variables used to detect the troubled-cells, which we refer to as the indicator-variables. Additionally, we need to choose the limited-variables, i.e., the variables which are limited in the troubled-cells. The importance of limiting the local characteristic variables to avoid spurious oscillations, especially for high-order schemes, has been demonstrated in <ref type="bibr" target="#b42">[41]</ref>. Thus, we adhere to this choice of limited-variables for the results of systems of conservation laws.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Linear advection</head><p>We first consider the linear scalar advection equation</p><formula xml:id="formula_39">∂u ∂t + c ∂u ∂x = 0,</formula><p>and set the advection speed c = 1 for all test cases. The time step is obtained by setting CFL = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Sine-wave</head><p>This test case describes the advection of a smooth sine wave. The initial condition is given by</p><formula xml:id="formula_40">u 0 (x) = sin(10πx), x∈ [0, 1],</formula><p>with periodic boundary conditions. At the end of the final time T = 1, the wave completes one full revolution and returns to its original position. The solution is evaluated with various troubledcell indicators on a mesh with N = 100 cells. Ideally, none of the cells should be flagged as troubled-cells, since the solution is smooth. However, the minmod, TVB-1 and TVB-2 indicators incorrectly mark cells with smooth extrema as troubled-cells, as can be seen in Figure <ref type="figure" target="#fig_11">7</ref>. On the other hand, no cells are flagged by the TVB-3 and MLP indicators. Note that, for the given problem, the parameter M corresponding TVB-3 is close to the curvature of the solution near the smooth extrema, which explains the performance of the limiter. Furthermore, the results with the TVB-3 and MLP indicators remain unchanged when the mesh is perturbed in accordance to 5.1.  This test case corresponds to a solution consisting of waves with different degrees of regularity. The initial condition is given by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Multi-wave</head><formula xml:id="formula_41">u 0 (x) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 10(x -0.2) if 0.2 &lt; x 0.3 , 10(0.4 -x) i f0 .3 &lt; x 0.4 , 1 i f 0 .6 &lt; x 0.8 , 100(x -1)(1.2 -x) if 1&lt; x 1.2 , 0 otherwise ,</formula><p>on the domain [0, <ref type="bibr">1.4]</ref> with periodic boundary conditions. The solution completes one full revolution at the end of time T = 1.4. The minmod and TVB-1 limiters give very dissipative results, as can be seen in Figure <ref type="figure">8</ref>, while TVB-2 performs significantly better. Note that the TVB and MLP indicators give rise to overshoots near the second wave, especially for r = 1. However, the overshoots are reduced as the polynomial degree is increased, as shown in Figure <ref type="figure" target="#fig_14">9</ref>. Although TVB-3 is better at resolving the discontinuity, it also gives the largest overshoots. The MLP indicator gives milder overshoots, with its resolution capabilities lying between TVB-2 and TVB-3.</p><p>The time-history of the troubled-cells marked by the various indicators for r = 4, is shown in Figure <ref type="figure" target="#fig_16">10</ref>. Since the minmod limiter marks the most number of cells, it leads to the most dissipative solution. The MLP indicator seems to mark the requisite number cells to control the overshoots, without being over-dissipative. Furthermore, the result with the MLP indicator on a randomly perturbed mesh is almost identical to the result obtained on a uniform mesh, as shown in Figure <ref type="figure" target="#fig_0">11</ref> and 10(f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Burgers equation</head><p>We compare the performance of the indicator functions for the non-linear Burgers equation</p><formula xml:id="formula_42">∂u ∂t + ∂ ∂x u 2 2 = 0.</formula><p>The time step is obtained by setting CFL = 0.2.     Figure <ref type="figure" target="#fig_0">11</ref>: Comparing the solution for the linear advection of the multi-wave, computed on a uniform mesh and a randomly perturbed mesh with N=100 and r = 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Shock collision</head><p>This test describes the collision of three shocks of varying strengths and speeds, which eventually move as a single shock wave to the right. The initial condition is given by</p><formula xml:id="formula_43">u 0 (x) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ 10 if x 0.2 , 6 i f0 .2 &lt; x 0.4 , 0 i f0 .4 &lt; x 0.6 , -4 if 0.6 &lt; x ,</formula><p>on the domain [0, 1] with open boundary conditions. The solutions are evaluated at time T = 0.1.</p><p>The results with the various indicators on a mesh with N = 100 cells and r = 4 are indistinguishable, as shown in Figure <ref type="figure" target="#fig_12">12</ref>(a). To compare and assess the performance of the indicators, we consider the time-history of the troubled-cells. We notice from the results in Figure <ref type="figure" target="#fig_19">13</ref> that the minmod limiter marks the most cells. Comparatively, the TVB limiter has thinner zones of marked cells, which decay as the TVB parameter M is increased. Thus, TVB-3 ensures the most cost-efficient simulation for this problem, while the MLP lies between TVB-2 and TVB-3. We observe from Figure <ref type="figure" target="#fig_12">12</ref>(b) that the MLP indicator works equally well when the mesh is randomly perturbed, with a similar marking of the troubled-cells as shown in Figure <ref type="figure" target="#fig_19">13</ref>(f).   The initial condition for this test is a composition of smooth and discontinuous data, expressed as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Compound wave</head><formula xml:id="formula_44">u 0 (x) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ sin(πx) if|x| 1 , 3 i f-1 &lt; x -0.5 , 1 i f-0.5 &lt; x 0 , 3 i f0&lt; x 0.5 , 2 i f0 .5 &lt; x 1</formula><p>on the domain [0, 1] with periodic boundary conditions. The initial condition is plotted in Figure <ref type="figure" target="#fig_20">14</ref>(a), and the solution is simulated until time T = 0.4. As the solution evolves, alternating shock and rarefaction waves begin to develop. In all the previous tests, the solutions obtained with TVB-3 were superior to those obtained with TVB-1 and TVB-2, ignoring minor oscillations. However, TVB-3 leads to large Gibbs oscillations near the discontinuities for the current test case, as shown in Figure <ref type="figure" target="#fig_20">14(b)</ref>. This demonstrates that the same TVB parameter does not work uniformly for all problems. On the other hand, the results with minmod, TVB-1, TVB-2 and the MLP are equally well resolved. Once again, the minmod limiter marks most number of cells, as shown in Figure <ref type="figure" target="#fig_0">15</ref>, while the count with the MLP indicator lies between TVB-1 and TVB-2. The existence of Gibbs oscillations with TVB-3 is further supported by the fact that a negligible number of cells are flagged by the indicator. The performance of the MLP remains unchanged when the mesh is randomly perturbed, as shown in Figures <ref type="figure" target="#fig_20">14(d</ref>) and 15(f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Buckley-Leverett</head><p>In order to demonstrate the performance of the MLP indicator for a non-convex flux, we consider the Buckley-Leverett equations with the flux function</p><formula xml:id="formula_45">f (u) = u 2 u 2 + 0.5(1 -u) 2 ,</formula><p>where u represents the water saturation in a mixture of oil and water <ref type="bibr" target="#b43">[42]</ref>. We consider the initial condition u 0 (x) = 0.95 if x 0.5 , 0.1 ifx &gt; 0.5 , on the domain [0,1.5], which evolves into a compound wave consisting of a shock and a rarefaction. The numerical solutions are evaluated at time T = 0.4 with CFL=0.4, on a mesh with N = 150 cells and open boundary conditions. As shown in Figure <ref type="figure" target="#fig_8">16</ref>(a), the solutions with the various indicators are indistinguishable. From the point of cost-efficiency, TVB-3 marks the least number of cells, while MLP lies between TVB-2 and TVB-3 (see Figure <ref type="figure" target="#fig_24">17</ref>). Similar to the previous test cases, the MLP performs equally well on a perturbed mesh, as shown in Figures <ref type="figure" target="#fig_8">16(b</ref>) and 17(f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Shallow-water equations</head><p>We consider the one-dimensional shallow-water equations   where D represents the depth of the fluid, u denotes the fluid velocity and g denotes the acceleration due to gravity. We solve the following Riemann problem simulating a dam-break</p><formula xml:id="formula_46">∂ ∂t D Du + ∂ ∂x Du Du 2 + 1 2 gD ,<label>(a) (b) (c) (d)</label></formula><formula xml:id="formula_47">D 0 (x) = 3 ifx &lt; 0 1 ifx &gt; 0 , u 0 (x) = 0,<label>g= 1,</label></formula><p>on a mesh with N = 100 cells until the time T = 1 with CFL=0.4. The primitive variables D and u are chosen as the indicator-variables. The results in Figure <ref type="figure" target="#fig_25">18</ref> show that TVB-2 and TVB-3 lead to solutions contaminated with low amplitude oscillations, making them unsuitable for the current problem. While TVB-2 marks an insufficient number of troubled-cells (see 19(c)), TVB-3 does not flag any cell. This once again demonstrates the issue of choosing the TVB parameter M . The solutions with the minmod, TVB-1 and MLP indicators are non-oscillatory and indistinguishable, with the MLP marking the least number of cells among the three (see Figure <ref type="figure" target="#fig_27">19</ref>). Randomly perturbing the mesh does not alter the performance of the MLP indicator, as shown in Figures <ref type="figure" target="#fig_25">18(d</ref>) and 19(e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Euler equations</head><p>We consider the one-dimensional Euler equations</p><formula xml:id="formula_48">∂ ∂t ⎡ ⎣ ρ ρu E ⎤ ⎦ + ∂ ∂x ⎡ ⎣ ρu p + ρu 2 (E + p)u ⎤ ⎦ ,</formula><p>where ρ, u, p represents the fluid density, velocity and pressure, respectively. The quantity E is the total energy per unit volume            </p><formula xml:id="formula_49">E = ρ u 2 2 + e ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4.">Shock-entropy test</head><p>This test case proposed in <ref type="bibr" target="#b47">[46]</ref>, describes the interaction of a right moving shock with an oscillatory smooth wave. Its initial condition is given by   The solution is simulated on a mesh with N = 256 cells, until the time T = 1.8 and CFL=0.1. While TVB-3 gives the most accurate results for r = 4, especially in the smooth high-frequency among all the indicators. Unlike the TVB indicators, the MLP only flags the cells close the shock and contact discontinuities (barring the first few time steps), which seems to be sufficient to avoid the appearance of spurious oscillations.  Remark 5.1. The limiting of characteristic variables can lead a loss of positivity of density and/or pressure, which explains why the TVB limiter failed in the above experiments, especially for cases when a sufficient number of cells were flagged. The MLP indicator did not face this issue for any of the Euler test cases considered in this paper. However, it might be useful to add a positivity preserving limiter <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b50">49]</ref> for more complex test cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we propose a new approach to tackle the issue of troubled-cell detection, by designing an MLP. The network is trained offline on a robust dataset consisting of canonical samples characterizing the local solution structure of conservation laws. The final trained network is used as a black-box troubled-cell indicator in a RKDG solver. An advantage of the proposed MLP indicator over traditional troubled-cell indicators, is that it does not depend on any tunable problemdependent parameters.</p><p>The proposed indicator has been successfully tested on both scalar and systems of conservations laws in one-dimension. The numerical results presented in this paper demonstrate the ability of the network to try and mimic the behavior of the TVB limiter with an optimally chosen parameter M for a given problem. This ensures that the network correctly classifies cells with smooth extrema as good-cells. Since the network attempts not to flag more troubled-cells than necessary, the computational cost of the simulations can be significantly reduced. We have also demonstrated that despite being trained on uniform grids, the MLP indicator is able to perform well on grids obtained by randomly perturbing uniform meshes.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Stencil used by troubled-cell indicators. The polynomial approximation in each cell is denoted by solid line, while the corresponding cell-average values denoted by dashed lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A simplified model of the biological neuron. This image has been taken from [29].</figDesc><graphic coords="8,177.29,348.50,257.35,137.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A single artificial neuron receiving signals from k neurons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: An MLP with 2 hidden layers. The input layer transmits the signal X to the first hidden layer. The final output of the network is Ŷ, which is the prediction to the true output Y corresponding to X.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 ν = 10 νFigure 5 :</head><label>1105</label><figDesc>Figure 5: Choices for the activation function used in the MLP network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 : 2 V opt acc = 0 3 for r ← 1 to R do 4 t5 6 while t &lt;= T and l &lt; L do 7 [</head><label>123467</label><figDesc>Offline training of the neural network Input : Neural network (N , V), learning rate η, leaky ReLU parameter ν, cost function C, cost regularization parameter β, training set T, validation set V, maximum number of epochs T, stopping parameter L, mini-batch size S b , number of restarts R. Output: Optimal weights W opt and biases b opt 1 Function [W opt , b opt ] = TRAIN MLP(N , V, η, ν, C, β, T, V, T, L, S b , R): = 1, V r acc = 0, l = 0 Randomly initialize W(1) and b(1) W(t + 1), b(t + 1)] = MINIBATCH OPT(N , V, W(t), b(t), η, ν, C, β, T, S b ) // Evaluate the accuracy according to 4.3 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>b 2 : 1 N 10 W ← W + ΔW 11 b ← b + Δb 12 i 3 : 2 ) ∈ R 5 . 5 X 1 ) 6 .</head><label>21101112325516</label><figDesc>opt ← b(t -Mini-batch optimization. This is called by the function TRAIN MLP Input : Neural network (N , V), old weights W 0 , old biases b 0 , learning rate η, leaky ReLU parameter ν, cost function C, cost regularization parameter β, training set T, mini-batch size S b . Output: Updated weights W and biases b Function [W, b] = MINIBATCH OPT(N , V, W 0 , b 0 , η, ν, C, β, T, S b ): Shuffle data set T i = = size(T) W ← W 0 b ← b 0 while i &lt;= N do Data = T(i : i + S b -1) // Evaluate step update using any optimizer [ΔW, Δb] = OPT ITERATION(N , V, W 0 , b 0 , η, ν, C, β, Data) ← min (i + S b , N) MLP network used as a troubled-cell indicator Input : MLP weights W and biases b (see (4.4)), leaky ReLU parameter ν, solution data X = (u i-1 , u i , u i+1 , u + Output: 1 if the data corresponds to a troubled-cell, else 0. Function [ind] = MLP INDICATOR(W, b, ν, X): // Scale data to lie in [-1, 1] = X/ max(abs(X), 1) ind = 0 // i = 6 corresponds to the output layer for i ← 1 to 6 do X ← W{i} * X + b{i} // Propagation X ← LEAKY RELU(X , ν) // Activation: Component-wise leaky ReLU end Y ← SOFTMAX(X) // See (4.Repeat steps 2-5 by varying the choice of the point x i , the mesh size h and the polynomial degree r. 7. Repeat steps 1-6 for different known functions u(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Remark 4 . 1 .</head><label>41</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Construction of a training sample for a known function u(x) (solid line) in the interval [a, b].In each cell of the stencil, the function is approximated using a polynomial (dashed line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>x &lt; x 0 ) + ur.(x &gt; x 0 ) (only troubled-cells selected) [-1, 1] (u l , u r ) ∈ [-20, 20] 2x 0 ∈ [-0.76, 0Functions used to create V.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The time-history of flagged troubled-cells for the linear advection of a sine-wave, simulated until T = 1 with N = 100 cells and r = 4.</figDesc><graphic coords="19,113.87,116.12,107.33,107.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>(a) r = 1 (b) r = 2 (</head><label>12</label><figDesc>c) r = 3 (d) r = 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 : 1 (b) r = 2 (</head><label>812</label><figDesc>Figure 8: Solution for the linear advection of the multi-wave, obtained at T = 1.4 with N = 100 cells and polynomial degree r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Solution for the linear advection of the multi-wave, obtained at T = 1.4 with N = 100 cells and polynomial degree r. The plots are zoomed to focus on the central step wave.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The time-history of flagged troubled-cells for the linear advection of the multi-wave, simulated until T = 1 with N = 100 cells and r = 4. The plots (a)-(e) are obtained on a uniform mesh, while (f) is obtained on a randomly perturbed mesh.</figDesc><graphic coords="22,420.29,296.36,77.08,78.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Solution of the shock collision problem for the Burgers equation, computed at T = 0.1 with N = 100 cells and r = 4. (a) solution evaluated on a uniform mesh; (b) comparison of the solution with the MLP indicator on a uniform mesh and a randomly perturbed mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>MLP (perturbed mesh)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: The time-history of flagged troubled-cells of the shock collision problem for the Burgers equation, simulated until T = 0.1 with N = 100 cells and r = 4. The plots (a)-(e) are obtained on a uniform mesh, while (f) is obtained on a randomly perturbed mesh.</figDesc><graphic coords="24,137.03,369.74,54.20,104.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Solution of the compound wave problem for the Burgers equation, computed at T = 0.4 with N = 200 cells and r = 4. (a) the initial condition; (b) the oscillatory solution obtained with TVB-3; (c) the solution with the remaining indicators; (d) comparison of the solution with the MLP indicator on a uniform mesh and a randomly perturbed mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>MLP (perturbed mesh)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 15 :Figure 16 :</head><label>1516</label><figDesc>Figure 15: The time-history of flagged troubled-cells of the compound wave problem for the Burgers equation, simulated until T = 0.4 with N = 200 cells and r = 4. The plots (a)-(e) are obtained on a uniform mesh, while (f) is obtained on a randomly perturbed mesh.</figDesc><graphic coords="27,415.73,370.28,88.87,115.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>MLP (perturbed mesh)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: The time-history of the Riemann problem for the Buckley-Leverett equations, simulated until T = 0.4 with N = 150 cells and r = 4. The plots (a)-(e) are obtained on a uniform mesh, while (f) is obtained on a randomly perturbed mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Solution of the dam-break problem for the shallow water equations, computed at T = 1 with N = 100 cells and r = 4. (a) the oscillatory solution obtained with TVB-2; (b) the oscillatory solution obtained with TVB-3; (c) the solution with the remaining indicators; (d) comparison of the solution with the MLP indicator on a uniform mesh and a randomly perturbed mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>MLP (perturbed mesh)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: The time-history of flagged troubled-cells of the dam-break problem for the shallow water equations, simulated until T = 1 with N = 100 cells and r = 4. The plots (a)-(d) are obtained on a uniform mesh, while (e) is obtained on a randomly perturbed mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Solution (density) of the Lax problem for the Euler equations, computed at T = 1.3 with N = 200 cells. (b) and (d) are zoomed near the contact discontinuity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: The time-history of flagged troubled-cells of the Lax problem for the Euler equations, computed at T = 1.3 with N = 200 cells and r = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Solution (density) of the double rarefaction problem for the Euler equations, computed at T = 0.6 with N = 128 cells and r = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: The time-history of flagged troubled-cells of the double rarefaction problem for the Euler equations, computed at T = 0.6 with N = 128 cells and r = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: Solution (density) for the shock-entropy problem, computed at T = 1.8 with N = 256 cells and r = 4. (b) zoomed near the smooth high-frequency region of the solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 28 :</head><label>28</label><figDesc>Figure 28: Solution (density) for the left half of the blast wave problem, computed at T = 0.012 with N = 256 cells and r = 4. (b) the solution zoomed near the contact discontinuity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 29 :</head><label>29</label><figDesc>Figure 29: The time-history of flagged troubled-cells for the left half of the blast wave problem, computed at T = 0.012 with N = 256 cells and r = 4.</figDesc><graphic coords="39,199.13,371.18,77.67,102.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Functions used to create the training and validation datasets. The last two columns of each table indicate the number of good cell and troubled-cell sample points extracted from each function. The total number of good cell and troubled-cell sample points present in each dataset are listed in the last row of each table. The total number of samples in each set is obtained by adding the corresponding good and troubled-cells.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the present work, we have restricted our focus to one-dimensional conservation laws. Future work will investigate the performance of similar MLP-based indicators trained for multi-dimensional systems.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where e is the specific internal energy given by a caloric equation of state, e = e(ρ, p). We chose the equation of state for ideal gas given by e = p (γ -1)ρ , with γ = c p /c v denoting the ratio of specific heats. We set γ = 1.4 for all the test cases below.</p><p>Based on the numerical results in Sections 5.1 -5.4, we only compare the performance of the TVB and MLP indicators with the Euler equations, since the solutions with the minmod indicator are both diffusive and expensive. The primitive variables ρ, u, p are chosen as the limited-variables. The time-step is evaluated by setting CFL=0.4, except when indicated otherwise.</p><p>We have also simulated the various Euler tests with the MLP indicator on a randomly perturbed mesh (using (5.1)) and found them to be very similar to the results obtained on the uniform mesh. Thus, we refrain from presenting them in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1.">Sod test</head><p>This problem describes a mild shock tube test proposed by Sod <ref type="bibr" target="#b44">[43]</ref>, whose initial condition is given by</p><p>The solution is simulated on a mesh with N = 100 cells, until the time T = 2. The simulations with TVB-2 for r = 1 and TVB-3 for r = 1, 2, 3, 4 fail due to loss of positivity of density. Furthermore, TVB-2 leads to highly oscillatory results for r = 2, 3, 4, with the results for r = 4 shown in Figure <ref type="figure">20</ref>. This is substantiated by the fact that an insufficient number of cells are flagged by TVB-2 (see Figure <ref type="figure">21</ref>). The solutions with TVB-1 and MLP are indistinguishable, with the latter marking a few more cells as compared to the former.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2.">Lax test</head><p>We consider the Lax shock tube problem <ref type="bibr" target="#b45">[44]</ref>, whose initial condition is given by</p><p>The solution is simulated on a mesh with N = 200 cells, until the time T = 1.3. The simulations with TVB-3 fail due to loss of positivity of density. While TVB-2 leads to the most accurate solution for r = 4, it gives the largest oscillation near to contact discontinuity for r = 1, as shown in Figure <ref type="figure">22</ref>. On the other hand, the MLP indicator gives much milder undershoots for r = 1, and lies between TVB-1 and TVB-2 in term of its resolution capabilities and the number of cells flagged as troubled-cells (see Figure <ref type="figure">23</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3.">Double rarefaction</head><p>We consider the Riemann problem given by the following initial condition <ref type="bibr" target="#b46">[45]</ref> (ρ,</p><p>simulated on a mesh with N = 128 cells, until the time T = 0.6. The solution consists of two rarefaction waves pulling away from each other, creating a near-vacuum region at x = 0. This test case is used to assess the ability of numerical schemes to preserve the positivity of density and pressure. All three TVB indicators fail for r = 1. Additionally, TVB-3 fails for r = 4, and is observed to give rise to spurious oscillations near x = 0 for r = 2, 3 (not shown here). The solution with TVB-1, TVB-2 and MLP are indistinguishable for r = 4, as can be seen in Figure <ref type="figure">24</ref>. Furthermore, the results in Figure <ref type="figure">24</ref> demonstrate the superiority of the MLP indicator, which marks the least number of cells among the three.</p><p>region of the solution (see Figure <ref type="figure">26</ref>), we note that it fails to preserve positivity of density for r = 1.</p><p>The solution is very dissipative with TVB-1, while the solutions with TVB-2 and MLP are equally well resolved. However, the MLP indicator marks fewer cells than TVB-2, as shown in Figure <ref type="figure">27</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.5.">Left half of the blast-wave</head><p>We consider a severe test case, describing the left half of the blast-wave problem <ref type="bibr" target="#b48">[47]</ref>. The initial condition is given by</p><p>and is simulated on a mesh with N = 256 cells, until the time T = 0.012. The solution with TVB-3 fails for r = 1. The results for this problem show the most significant improvement with the MLP indicator, which leads to the best approximation of the wave peak, as shown in Figure <ref type="figure">28</ref>. Furthermore, the results in Figure <ref type="figure">29</ref> show that the MLP marks the fewest number of cells</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hyperbolic conservation laws in continuum physics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Dafermos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Grundlehren der Mathematischen Wissenschaften</title>
		<imprint>
			<biblScope unit="volume">325</biblScope>
			<date type="published" when="2010">2010</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>Fundamental Principles of Mathematical Sciences. third ed.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards the ultimate conservative difference scheme. V -A second-order sequel to Godunov&apos;s method</title>
		<author>
			<persName><forename type="first">B</forename><surname>Van Leer</surname></persName>
		</author>
		<idno type="DOI">10.1016/0021-9991</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="90145" to="90146" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tvb runge-kutta local projection discontinuous galerkin finite element method for conservation laws iii: One-dimensional systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.1016/0021-9991(89)90183-6</idno>
		<ptr target="https://doi.org/10.1016/0021-9991(89)90183-6" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="90" to="113" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The runge-kutta discontinuous galerkin method for conservation laws v: Multidimensional systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.1006/jcph.1998.5892</idno>
		<ptr target="https://doi.org/10.1006/jcph.1998.5892" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="199" to="224" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uniformly high order accurate essentially non-oscillatory schemes, iii</title>
		<author>
			<persName><forename type="first">A</forename><surname>Harten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Engquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Chakravarthy</surname></persName>
		</author>
		<idno type="DOI">10.1006/jcph.1996.5632</idno>
		<ptr target="https://doi.org/10.1006/jcph.1996.5632" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="3" to="47" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weighted essentially non-oscillatory schemes</title>
		<author>
			<persName><forename type="first">X.-D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1006/jcph.1994.1187</idno>
		<ptr target="https://doi.org/10.1006/jcph.1994.1187" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Essentially non-oscillatory and weighted essentially non-oscillatory schemes for hyperbolic conservation laws</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.1007/BFb0096355</idno>
		<idno>doi:10.1007/BFb0096355</idno>
		<ptr target="https://doi.org/10.1007/BFb0096355" />
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="325" to="432" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hesthaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Warburton</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-72067-8</idno>
		<idno>doi:10.1007/978-0-387-72067-8</idno>
		<ptr target="http://dx.doi.org/10.1007/978-0-387-72067-8" />
	</analytic>
	<monogr>
		<title level="m">Nodal discontinuous Galerkin methods</title>
		<title level="s">Texts in Applied Mathematics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Numerical solution of the euler equations by finite volume methods using runge-kutta time stepping schemes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jameson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Turkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIAA Paper</title>
		<imprint>
			<biblScope unit="page" from="81" to="1259" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Entropy stability theory for difference approximations of nonlinear conservation laws and related time-dependent problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tadmor</surname></persName>
		</author>
		<editor>Acta Numerica</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">512</biblScope>
			<biblScope unit="page">451</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Runge-kutta discontinuous galerkin method using weno limiters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.1137/S1064827503425298</idno>
		<idno>doi:10.1137/S1064827503425298</idno>
		<ptr target="https://doi.org/10.1137/S1064827503425298" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="907" to="929" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comparison of troubled-cell indicators for Runge-Kutta discontinuous Galerkin methods using weighted essentially nonoscillatory limiters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.1137/04061372X</idno>
		<idno>doi:10.1137/04061372X</idno>
		<ptr target="http://dx.doi.org/10.1137/04061372X" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="995" to="1013" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TVB Runge-Kutta local projection discontinuous Galerkin finite element method for conservation laws</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.2307/2008474</idno>
		<idno>doi:10.2307/2008474</idno>
		<ptr target="http://dx.doi.org/10.2307/2008474" />
	</analytic>
	<monogr>
		<title level="j">II. General framework, Math. Comp</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="411" to="435" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated parameters for troubled-cell indicators using outlier detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vuik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ryan</surname></persName>
		</author>
		<idno type="DOI">10.1137/15M1018393</idno>
		<idno>doi:10.1137/15M1018393</idno>
		<ptr target="https://doi.org/10.1137/15M1018393" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="84" to="A104" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<title level="m">Exploratory Data Analysis</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced robustness of the hybrid compact-weno finite difference scheme for hyperbolic conservation laws with multi-resolution analysis and tukey&apos;s boxplot method</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Don</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10915-017-0465-0</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Journal of Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="736" to="752" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new troubled-cell indicator for discontinuous galerkin methods for hyperbolic conservation laws</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<biblScope unit="page">305</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.jcp.2017.06.046</idno>
		<ptr target="https://doi.org/10.1016/j.jcp.2017.06.046" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Neural Networks: A Comprehensive Foundation</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall PTR</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Cristea</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-90-481-2899-0_5</idno>
		<ptr target="https://doi.org/10.1007/" />
		<title level="m">Application of Neural Networks In Image Processing and Visualization</title>
		<meeting><address><addrLine>Netherlands, Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="59" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speech recognition with artificial neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Sazli</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dsp.2009.10.004</idno>
		<ptr target="https://doi.org/10.1016/j.dsp.2009.10.004" />
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="763" to="768" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Artificial neural networks for solving ordinary and partial differential equations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Lagaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Fotiadis</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.712178</idno>
		<idno>doi:10.1109/72.712178</idno>
		<ptr target="http://dx.doi.org/10.1109/72.712178" />
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="987" to="1000" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A MLP Solver for First and Second Order Partial Differential Equations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Golak</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-74695-9_81</idno>
		<idno>doi:10.1007/978-3-540-74695-9_81</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-74695-9_81" />
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="789" to="797" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A constrained integration (cint) approach to solving partial differential equations using artificial neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rudd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page">277</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.neucom.2014.11.058</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2014.11.058" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Continuous Valued Neural Networks with Two Hidden Layers Are Sufficient</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Medford, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Tufts University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02551274</idno>
		<idno>doi:10.1007/BF02551274</idno>
		<ptr target="https://doi.org/10.1007/BF02551274" />
	</analytic>
	<monogr>
		<title level="m">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A single hidden layer feedforward network with only one neuron in the hidden layer can approximate any univariate function</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Guliyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Ismailov</surname></persName>
		</author>
		<idno type="DOI">10.1162/NECO_a_00849</idno>
		<idno>doi:10.1162/NECO_a_00849</idno>
		<ptr target="https://doi.org/10.1162/NECO_a_00849" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1289" to="1304" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Strong stability-preserving high-order time discretization methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tadmor</surname></persName>
		</author>
		<idno type="DOI">10.1137/S003614450036757</idno>
		<idno>doi:10.1137/S003614450036757X</idno>
		<ptr target="http://dx.doi.org/10.1137/S003614450036757" />
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="89" to="112" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Runge-kutta discontinuous galerkin methods for convectiondominated problems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1012873910884</idno>
		<idno>doi:10.1023/A:1012873910884</idno>
		<ptr target="https://doi.org/10.1023/A:1012873910884" />
	</analytic>
	<monogr>
		<title level="j">Journal of Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="173" to="261" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A Brief Introduction to Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kriesel</surname></persName>
		</author>
		<ptr target="http://www.dkriesel.com" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A logical calculus of the ideas immanent in nervous activity</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02478259</idno>
		<idno>doi:10.1007/BF02478259</idno>
		<ptr target="https://doi.org/10.1007/BF02478259" />
	</analytic>
	<monogr>
		<title level="j">The bulletin of mathematical biophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2014.09.003</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2014.09.003" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.icml2010.org/papers/432.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</editor>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Rectifier nonlinearities improve neural network acoustic models</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177729694</idno>
		<idno>doi:10.1214/aoms/1177729694</idno>
		<ptr target="https://doi.org/10.1214/aoms/1177729694" />
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Practical Recommendations for Gradient-Based Training of Deep Architectures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-35289-8_26</idno>
		<idno>doi:10.1007/978-3-642-35289-8_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-35289-8_26" />
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="437" to="478" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simplifying neural networks by soft weight-sharing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1992.4.4.473</idno>
		<idno>doi:10. 1162/neco.1992.4.4.473</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1992.4.4.473" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="473" to="493" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic differentiation of algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartholomew-Biggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Christianson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dixon</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0377-0427(00)00422-2</idno>
		<ptr target="https://doi.org/10.1016/S0377-0427(00)00422-2" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="171" to="190" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
	<note>Analysis. IV: Optimization and Nonlinear Equations</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the construction, comparison, and local characteristic decomposition for high-order central weno schemes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.1006/jcph.2002.7191</idno>
		<ptr target="https://doi.org/10.1006/jcph.2002.7191" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page" from="187" to="209" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Finite Volume Methods for Hyperbolic Problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leveque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Cambridge Texts in Applied Mathematics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A survey of several finite difference methods for systems of nonlinear hyperbolic conservation laws</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Sod</surname></persName>
		</author>
		<idno type="DOI">10.1016/0021-9991(78)90023-2</idno>
		<ptr target="https://doi.org/10.1016/0021-9991(78)90023-2" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Weak solutions of nonlinear hyperbolic equations and their numerical computation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Lax</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpa.3160070112</idno>
		<idno>doi:10.1002/cpa.3160070112</idno>
		<ptr target="http://dx.doi.org/10.1002/cpa.3160070112" />
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="159" to="193" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roe</surname></persName>
		</author>
		<idno type="DOI">10.2514/6.1997-2098</idno>
		<ptr target="https://doi.org/10.2514/6.1997-2098" />
		<title level="m">Robust Euler Codes, AIAA Paper-97-2098</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient implementation of essentially non-oscillatory shock-capturing schemes, ii</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<idno type="DOI">10.1016/0021-9991</idno>
		<ptr target="https://doi.org/10.1016/0021-9991" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="90222" to="90222" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The numerical simulation of two-dimensional fluid flow with strong shocks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Woodward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Colella</surname></persName>
		</author>
		<idno type="DOI">10.1016/0021-9991</idno>
		<ptr target="https://doi.org/10.1016/0021-9991" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="90142" to="90146" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On maximum-principle-satisfying high order schemes for scalar conservation laws</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jcp.2009.12.030</idno>
		<ptr target="https://doi.org/10.1016/j.jcp.2009.12.030" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page" from="3091" to="3120" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On positivity-preserving high order discontinuous galerkin schemes for compressible euler equations on rectangular meshes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jcp.2010.08.016</idno>
		<ptr target="https://doi.org/10.1016/j.jcp.2010.08.016" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page" from="8918" to="8934" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
