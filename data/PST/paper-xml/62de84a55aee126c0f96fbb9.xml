<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Are Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
							<email>h.yin1@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
							<email>tong.chen@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
							<email>x.xia@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Quoc</forename><surname>Viet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hung</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shandong University Jinan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Griffith University Gold Coast</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Are Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3477495.3531937</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Self-Supervised Learning</term>
					<term>Recommendation</term>
					<term>Contrastive Learning</term>
					<term>Data Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning (CL) recently has spurred a fruitful line of research in the field of recommendation, since its ability to extract self-supervised signals from the raw data is well-aligned with recommender systems' needs for tackling the data sparsity issue. A typical pipeline of CL-based recommendation models is first augmenting the user-item bipartite graph with structure perturbations, and then maximizing the node representation consistency between different graph augmentations. Although this paradigm turns out to be effective, what underlies the performance gains is still a mystery. In this paper, we first experimentally disclose that, in CL-based recommendation models, CL operates by learning more uniform user/item representations that can implicitly mitigate the popularity bias. Meanwhile, we reveal that the graph augmentations, which used to be considered necessary, just play a trivial role. Based on this finding, we propose a simple CL method which discards the graph augmentations and instead adds uniform noises to the embedding space for creating contrastive views. A comprehensive experimental study on three benchmark datasets demonstrates that, though it appears strikingly simple, the proposed method can smoothly adjust the uniformity of learned representations and has distinct advantages over its graph augmentation-based counterparts in terms of recommendation accuracy and training efficiency. The code is released at https://github.com/Coder-Yu/QRec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, a resurgence of contrastive learning (CL) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> has been witnessed in deep representation learning. Due to the ability to extract the general features from massive unlabeled data and regularize representations in a self-supervised manner, CL has led to major advances in multiple research fields <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. As data annotation is not required in CL, it is a natural antidote to the data sparsity issue in recommender systems <ref type="bibr">[22?</ref> ]. An increasing number of very recent studies <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> have sought to harness CL for improving recommendation performance and have demonstrated significant gains. A typical way <ref type="bibr" target="#b27">[28]</ref> to apply CL to recommendation is first augmenting the user-item bipartite graph with structure perturbations (e.g., stochastic edge/node dropout at a certain ratio), and then maximizing the consistency of representations under different views learned via a graph encoder. In this setting, the CL task acts as the auxiliary task, and is jointly optimized with the recommendation task (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Despite the encouraging results achieved by CL, however, what underlies the performance gains still remains unclear. Intuitively, we presume that contrasting different graph augmentations can capture the essential information existing in the original user-item interactions, by randomly removing the redundancy and impurity with the edge/node dropout. Unexpectedly, a few latest works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref> have reported that even extremely sparse graph augmentations (with edge dropout rate 0.9) in CL can bring desired performance gains. Such a phenomenon is quite elusive and counterintuitive because a large dropout rate will result in a huge loss of the raw information and a highly skewed graph structure. It naturally raises a meaningful question: Do we really need graph augmentations when integrating CL with recommendation?</p><p>To answer this question, we first conduct experiments with and without the graph augmentations respectively for a performance comparison. The results show that the when the graph augmentations are absent, the performance is also comparable to those with graph augmentations. We then investigate the embedding space learned by non-CL and CL-based recommendation methods. By visualizing the distributions of the representations and associating them with their performances, we find that what really matters for the recommendation performance is the CL loss, rather than the graph augmentation. Optimizing the contrastive loss InfoNCE <ref type="bibr" target="#b17">[18]</ref> learns more uniform user/item representations no matter if graph augmentations are applied, which implicitly plays a role in mitigating the popularity bias <ref type="bibr" target="#b3">[4]</ref>. Meanwhile, despite not as effective as expected, graph augmentations are not utterly useless in the sense that the properly perturbed versions of the original graph help learn representations invariant to the disturbance factors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. However, generating hand-crafted graph augmentations requires constant reconstruction of the graph adjacency matrix during training, which is quite time-consuming. In addition, dropping a critical edge/node (e.g., a cut edge) may split a connected graph into a few disconnected components, at the risk of making the augmented graph and the original graph share little learnable invariance. In view of these defects, a follow-up question then arises: Are there more effective and efficient augmentation approaches?</p><p>In this paper, we give an affirmative answer to the question. On top of our finding that the uniformity of the representation distribution is the key point, we develop a graph-augmentation-free CL method in which the uniformity is more controllable. Technically, we follow the graph CL framework presented in Fig. <ref type="figure" target="#fig_0">1</ref>, but we discard the dropout-based graph augmentation and instead add random uniform noises to the original representations for a representation-level data augmentation. Imposing different random noises creates variance between contrastive views, while the learnable invariance is still retained due to the controlled magnitude. Compared with the graph augmentation, the noise version directly regularizes the embedding space towards a more uniform distribution, which is easy-to-implement and far more efficient.</p><p>The major contributions of this paper are summarized as follows:</p><p>• We experimentally unravel why CL can boost recommendation performance and illustrate that the InfoNCE loss, rather than the graph augmentation, is the decisive factor. • We propose a simple yet effective graph-augmentation-free CL method for recommendation that can regulate the uniformity in a smooth way. It can be an ideal alternative of cumbersome graph augmentation-based CL methods.</p><p>• We conduct a comprehensive experimental study on three benchmark datasets showing that the proposed method has distinct advantages over its graph augmentation-based counterparts in terms of recommendation accuracy and model training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INVESTIGATION OF GRAPH CONTRASTIVE LEARNING IN RECOMMENDATION 2.1 Graph CL for Recommendation</head><p>CL is often applied to recommendation with a particular set of presumed representational invariances to data augmentations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45]</ref>. In this paper, we revisit the most commonly used dropoutbased augmentation on graphs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref> which assumes that the representations are invariant to partial structure perturbations. An investigation is launched into a state-of-the-art CL-based recommendation model, SGL <ref type="bibr" target="#b27">[28]</ref>, which performs node and edge dropout to augment the original graph and adopts InfoNCE <ref type="bibr" target="#b17">[18]</ref> for CL. Formally, the joint learning scheme in SGL is defined as:</p><formula xml:id="formula_0">L 𝑗𝑜𝑖𝑛𝑡 = L 𝑟𝑒𝑐 + 𝜆L 𝑐𝑙 ,<label>(1)</label></formula><p>which consists of two losses: recommendation loss L 𝑟𝑒𝑐 and CL loss L 𝑐𝑙 . The InfoNCE in SGL is formulated as:</p><formula xml:id="formula_1">L 𝑐𝑙 = ∑︁ 𝑖 ∈B − log exp(z ′⊤ 𝑖 z ′′ 𝑖 /𝜏) 𝑗 ∈B exp(z ′⊤ 𝑖 z ′′ 𝑗 /𝜏) ,<label>(2)</label></formula><p>where 𝑖, 𝑗 are users/items in a sampled batch B, z ′ (z ′′ ) are 𝐿 2 normalized 𝑑-dimensional node representations learned from two different dropout-based graph augmentations, and 𝜏 &gt; 0 (e.g., 0.2) is the temperature. The CL loss encourages consistency between z ′ 𝑖 and z ′′ 𝑖 which are the augmented representations of the same node 𝑖 and are the positive sample of each other, while minimizing the agreement between 𝑧 ′ 𝑖 and z ′′ 𝑗 , which are the negative samples of each other. To learn the representations from the user-item graph, SGL employs a popular and effective graph encoder LightGCN <ref type="bibr" target="#b8">[9]</ref> as its backbone, whose message passing process is defined as:</p><formula xml:id="formula_2">E = 1 1 + 𝐿 (E (0) + ÃE (0) + ... + Ã𝐿 E (0) ),<label>(3)</label></formula><p>where and e ′ 𝑖 is the corrupted version of e 𝑖 in E. For conciseness, here we just abstract the core ingredients of SGL and LightGCN. More technical details can be found in the original papers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>.</p><formula xml:id="formula_3">E (0) ∈ R |𝑁 |×𝑑</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Necessity of Graph Augmentation</head><p>To demystify how CL-based recommendation methods work, we first investigate the necessity of the graph augmentation in SGL. We construct a new variant of SGL, termed SGL-WA (WA stands for 'without augmentation'), in which the CL loss is:</p><formula xml:id="formula_4">L 𝑐𝑙 = ∑︁ 𝑖 ∈B − log exp(1/𝜏) 𝑗 ∈B exp(z ⊤ 𝑖 z 𝑗 /𝜏) . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Topic 18: Recommender System SIGIR '22, July 11-15, 2022, Madrid, Spain </p><formula xml:id="formula_6">= z ′′ 𝑖 = z 𝑖 .</formula><p>The experiments for the performance comparison are conducted on two benchmark datasets: Yelp2018 and Amazon-Book <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref>. A three-layer setting is adopted and the hyperparameters are tuned according to the original paper of SGL (more experimental details in Section 4.1). The results are presented in Table <ref type="table" target="#tab_1">1</ref>. Three variants of SGL (with dropout rate 0.1) proposed in the paper are evaluated (-ND denotes node dropout, -ED is short for edge dropout, and -RW means random walk (i.e., multi-layer edge dropout). CL Only means that only the CL loss in SGL is minimized).</p><p>As can be observed, all the variants of SGL outperform LightGCN by large margins, which demonstrates the effectiveness of CL in improving recommendation performance. To our surprise, when the graph augmentation is detached, the performance gains are still so remarkable that SGL-WA even exhibits superiority over SGL-ND and SGL-RW. We conjecture that the node dropout and random walk (especially the former) are very likely to drop the key nodes and associated edges and hence break the correlated subgraphs into disconnected pieces, which highly distort the original graph. Such graph augmentations share little learnable invariance, and encouraging consistency between them probably has a negative impact. By contrast, one-time edge dropout is at a lower risk to largely disturb the semantics of the original graph, so that SGL-ED can maintain a trivial advantage over SGL-WA, which suggests the potential of a proper graph augmentation. However, considering the time-consuming reconstruction of the adjacency matrices in each epoch, we should rethink the necessity of graph augmentations and search for better alternatives. Besides, we wonder what underlies the outstanding performance of SGL-WA since no variances are provided in its CL part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">InfoNCE Loss Influences More</head><p>Wang and Isola <ref type="bibr" target="#b23">[24]</ref> have identified that optimizing the contrastive loss intensifies two properties in the visual representation learning: alignment of features from positive pairs, and uniformity of the normalized feature distribution on the unit hypersphere. It is unclear if the CL-based recommendation methods exhibit similar patterns that can explain the results in Section 2.2. Since top-N recommendation is a one-class problem, we only investigate the uniformity by following the visualization method in <ref type="bibr" target="#b23">[24]</ref>.</p><p>We first map the learned representations (randomly sample 2,000 users for each dataset) to 2-dimensional normalized vectors on the unit hypersphere S 1 (i.e., circle with radius 1) by using t-SNE <ref type="bibr" target="#b22">[23]</ref>.</p><p>All the representations are obtained when the methods reach their best performance. Then we plot the feature distributions with the nonparametric Gaussian kernel density estimation <ref type="bibr" target="#b2">[3]</ref> in R 2 (shown in Fig. <ref type="figure">2</ref>). For a clearer presentation, the density estimations on angles for each point on S 1 are also visualized. According to Fig. <ref type="figure">2</ref>, we can observe notably different feature/density distributions. In the leftmost column, LightGCN shows highly clustered features that mainly reside on some narrow arcs. While in the second and the third columns, the distributions become more uniform, and the density estimation curves are less sharp, no matter if the graph augmentations are applied. In the forth column, we plot the features learned only by the contrastive loss in Eq. ( <ref type="formula" target="#formula_1">2</ref>). The distributions are almost completely uniform.</p><p>We think that two reasons may explain the highly clustered feature distributions. The first is the message passing mechanism in LightGCN. With the increase of layers, node embeddings become locally similar. The second is the popularity bias <ref type="bibr" target="#b3">[4]</ref> in the recommendation data. Recall the BPR loss <ref type="bibr" target="#b20">[21]</ref> used in LightGCN:</p><formula xml:id="formula_7">L 𝑟𝑒𝑐 = − log(𝜎 (e ⊤ 𝑢 e 𝑖 − e ⊤ 𝑢 e 𝑗 )),<label>(5)</label></formula><p>which is with a triplet input (𝑢, 𝑖, 𝑗). To optimize the BPR loss, we can get the gradients w.r.t e 𝑢 :∇ e 𝑢 = −𝜂 (1−𝑠)(e 𝑖 −e 𝑗 ), where 𝜂 is the learning rate, 𝜎 is the sigmoid function, 𝑠 = 𝜎 (e ⊤ 𝑢 e 𝑖 −e ⊤ 𝑢 e 𝑗 ), e 𝑢 is the user embedding, and e 𝑖 and e 𝑗 denote the positive and negative item embeddings, respectively. Since the recommendation data usually follows a long-tail distribution, when 𝑖 is a popular item with a large number of interactions, the user embedding will be constantly updated towards 𝑖's direction (i.e., −∇ e 𝑢 ). The message passing mechanism further exacerbates the clustering problem (i.e., e 𝑢 and e 𝑖 aggregate information from each other in the graph convolution) and causes the representation degeneration <ref type="bibr" target="#b19">[20]</ref>.</p><p>As for the distributions in other columns, by rewriting Eq. ( <ref type="formula" target="#formula_4">4</ref>), we can derive</p><formula xml:id="formula_8">L 𝑐𝑙 = ∑︁ 𝑖 ∈B −1/𝜏 + log exp(1/𝜏) + ∑︁ 𝑗 ∈B/{𝑖 } exp(z ⊤ 𝑖 z 𝑗 /𝜏) .<label>(6)</label></formula><p>Because 1/𝜏 is a constant, optimizing the CL loss is actually minimizing the cosine similarity between different nodes embeddings e 𝑖 and e 𝑗 , which will push connected nodes away from the highdegree hubs in the representation space and lead to a more uniform distribution even under the influence of the recommendation loss. By associating the results in Table <ref type="table" target="#tab_1">1</ref> with the distributions in Fig. <ref type="figure">2</ref>, we can easily draw a conclusion that the uniformity of the distribution is the underlying factor that has a decisive impact on the recommendation performance in SGL, rather than the dropoutbased graph augmentations. Optimizing the CL loss can be seen as an implicit way to debias (discussed in section 4.2) because a more uniform representation distribution can preserve the intrinsic characteristics of nodes and improve the generalization ability. This can be a persuasive explanation for the unexpected performance of SGL-WA. It also should be noted that, by only minimizing the CL loss in Eq. ( <ref type="formula" target="#formula_1">2</ref>), a poor performance will be reached, which means that a positive correlation between the uniformity and the performance only holds in a limited scope. The excessive pursuit to the uniformity will overlook the closeness of interacted pairs and similar users/items, and impairs recommendation performance. Figure <ref type="figure">2</ref>: We plot feature distributions with Gaussian kernel density estimation (KDE) in R 2 (the darker the color is, the more points fall in that area.) and KDE on angles (i.e., arctan2(y, x) for each point (x,y) ∈ S 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SIMGCL: SIMPLE GRAPH CONTRASTIVE LEARNING FOR RECOMMENDATION</head><p>Based on the findings in Section 2, we speculate that by adjusting the uniformity of the learned representation in a certain scope, the optimal performance can be reached. In this section, we aim to develop a Simple Graph Contrastive Learning method (SimGCL) for recommendation that can smoothly regulate the uniformity and provide informative variance to maximize the benefit from CL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation and Formulation</head><p>Since manipulating the graph structure for the uniform representation distribution is intractable and time-consuming, we shift our attention to the embedding space. Inspired by the adversarial examples <ref type="bibr" target="#b7">[8]</ref> which are constructed by adding imperceptibly small perturbation to the input images, we directly add random noises to the representation for an efficient and effective augmentation. Formally, given a node 𝑖 and its representation e 𝑖 in the 𝑑dimensional embedding space, we can implement the following representation-level augmentation:</p><formula xml:id="formula_9">e ′ 𝑖 = e 𝑖 + Δ ′ 𝑖 , e ′′ 𝑖 = e 𝑖 + Δ ′′ 𝑖 ,<label>(7)</label></formula><p>where the added noise vectors Δ ′ 𝑖 and Δ ′′ 𝑖 are subject to ∥Δ∥ 2 = 𝜖 and Δ = Δ ⊙ sign(e 𝑖 ), Δ ∈ R 𝑑 ∼ 𝑈 (0, 1). The first constraint controls the magnitude of Δ, and Δ is numerically equivalent to points on a hypersphere with the radius 𝜖. The second constraint requires that e 𝑖 , Δ ′ and Δ ′′ should be in the same hyperoctant, so that adding the noises will not cause a large deviation of e 𝑖 , making less valid positive samples. In Fig. <ref type="figure" target="#fig_2">3</ref>, we illustrate Eq. ( <ref type="formula" target="#formula_9">7</ref>) in R 2 . By adding the scaled noise vectors to the original representation, we rotate e 𝑖 by two small angles (𝜃 1 and 𝜃 2 ). Each rotation corresponds to a deviation of e 𝑖 , and leads to an augmented representation (e ′ 𝑖 and e ′′ 𝑖 ). Since the rotation is small enough, the augmented representation retains most information of the original representation and meanwhile also keeps some variance. Note that, for each node representation, the added random noises are different. Following SGL, we adopt LightGCN as the graph encoder to propagate node information and amplify the impact of the variance due to its simple structure and effectiveness. At each layer, different scaled random noises are imposed on the current node embeddings. The final perturbed node representations are learned by:</p><formula xml:id="formula_10">r = 𝜖 𝒆 𝑖 𝒆 j ∆ ′ ∆ ′′ 𝒆 𝑖 ′′ 𝒆 𝑖 ′ 𝜃 1 𝜃 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Space</head><formula xml:id="formula_11">E ′ = 1 𝐿 ( ÃE (0) + ∆ (1)</formula><p>) + ( Ã( ÃE (0) + ∆ (1) ) + Δ (2) )) + ...</p><formula xml:id="formula_12">+( Ã𝐿 E (0) + Ã𝐿−1 ∆ (1) + ... + Ã∆ (𝐿−1) + ∆ (𝐿) )<label>(8)</label></formula><p>It should be mentioned that we skip the input embedding E (0) in all the three encoders when calculating the final representations, because we experimentally find that skipping it can lead to slight performance improvement in our setting. However, without the CL task, this operation will result in a performance drop of LightGCN. Finally, we also unify the BPR loss (Eq. ( <ref type="formula" target="#formula_7">5</ref>)) and the CL loss (Eq. ( <ref type="formula" target="#formula_1">2</ref>)), and then use Adam to optimize the joint loss presented in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Regulating Uniformity</head><p>In SimGCL, two hyperparameters 𝜆 and 𝜖 can influence the uniformity of the representations which are critical to the performance. But 𝜖 can explicitly and smoothly regulate the uniformity beyond by only tuning 𝜆. By adjusting the value of 𝜖, we can directly control how far the augmented representations deviate from the original. Intuitively, a larger 𝜖 will lead to a more roughly uniform distribution of the learned representation, because when the augmented representations are enough far away from the original, the information lying in their representations is also considerably influenced by the noises. As the noises are sampled from the uniform distribution, by contrasting the augmented representations, the original representation is regularized towards higher uniformity. We present the following experimental analysis to demonstrate it.</p><p>In <ref type="bibr" target="#b23">[24]</ref>, a metric is proposed to measure the uniformity of the representation, which is the logarithm of the average pairwise Gaussian potential (a.k.a. the Radial Basis Function (RBF) kernel):</p><formula xml:id="formula_13">L uniform (𝑓 ) = log E 𝑖.𝑖.𝑑 𝑢,𝑣 ∼ 𝑝 node 𝑒 −2 ∥𝑓 (𝑢)−𝑓 (𝑣) ∥ 2 2 . (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>where 𝑓 (𝑢) outputs the 𝐿 2 normalized embedding of 𝑢. We choose the popular items (with more than 200 interactions) and randomly sample 5,000 users in the dataset of Yelp2018 to form the user-item pairs, and then compute the uniformity of their representations in the SGL variants and SimGCL with Eq. ( <ref type="formula" target="#formula_13">9</ref>). For a fair comparison, a three-layer setting is applied to all the compared methods with 𝜆 = 0.1. We then tune 𝜖 to observe how the uniformity changes.</p><p>The uniformity is checked after every epoch, and we record the values in the first 30 epochs during which the compared methods all converge to their optimal solutions. As clearly shown in Fig. <ref type="figure">4</ref>, similar trends are observed on all the curves. At the initial stage, all the methods have highly uniformlydistributed representations because we use Xavier initialization, which is a special uniform distribution. With the training proceeding, the uniformity declines (L uniform gets higher), and after reaching the peak, the uniformity improves till convergence and maintains this tendency. As for SimGCL, with the increase of 𝜖, it tends to learn more uniform representations, and even a very small 𝜖 = 0.01 leads to higher uniformity compared with the SGL variants. As a result, users (especially the long-tail users) are less affected by the popular items. In the rightmost column in Fig. <ref type="figure">2</ref>, we also plot the representation distributions of SimGCL with 𝜖 = 0.1. We can clearly see that the distributions are evidently more uniform than those learned by SGL variants and LightGCN. All these results can support our claim that by replacing graph augmentations with the noise-based augmentations, SimGCL is more capable of controlling the uniformity of learned representations so as to debias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complexity</head><p>In this section, we analyze the time complexity of SimGCL, and compare it with that of LightGCN and its graph-augmentation based counterpart SGL-ED. We hereby discuss the batch time complexity since the in-batch negative sampling is a widely used trick in CL <ref type="bibr" target="#b4">[5]</ref>. Let |𝐸| be the edge number in the graph, 𝑑 be the embedding size, 𝐵 denote the batch size, 𝑀 represent the node number in a batch, and 𝜌 denote the edge keep rate in SGL-ED. We can derive: • For LightGCN and SimGCL, no graph augmentations are required, so they just need to normalize the original adjacency matrix which has 2|𝐸| non-zero elements. For SGL-ED, two graph augmentations are used and each has 2𝜌 |𝐸| non-zero elements. • In the graph convolution stage, a three-encoder architecture (see Fig. <ref type="figure" target="#fig_0">1</ref>) is employed in both SGL-ED and SimGCL to learn augmented node representations. So, the time costs of SGL-ED and SimGCL are almost three times that of LightGCN. • As for the recommendation loss, three methods all use the BPR loss and each batch contains 𝐵 interactions, so they have the same time cost in this component. • When calculating the CL loss, the computation costs between the positive/negative samples are O (𝐵𝑑) and O (𝐵𝑀𝑑), respectively, because each node only considers itself as the positive, while the other nodes all are negatives. Comparing SimGCL with SGL-ED, we can clearly see that SGL-ED theoretically spends less time for graph convolution, and this bonus may offset SimGCL's advantage for the adjacency matrix construction. However, when putting them into practice, we actually observe that SimGCL is much more time-efficient. That is because, the computation for the graph convolution is mostly finished on GPUs, while the graph perturbation is performed on CPUs. Besides, in each epoch, the adjacency matrices of graph augmentations in SGL-ED need to be reconstructed. While in SimGCL, the adjacency matrix of the original graph only needs to be generated once before the training. In a nutshell, SimGCL is far more efficient than SGL, beyond what we can observe from the theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets. Three public benchmark datasets: Douban-Book <ref type="bibr" target="#b37">[38]</ref> (#user 13,024, #item 22,347, #interaction 792,062), Yelp2018 <ref type="bibr" target="#b8">[9]</ref> (#user 31,668 #item 38,048, #interaction 1,561,406), and Amazon-Book <ref type="bibr" target="#b27">[28]</ref> (#user 52,463, #item 91,599, #interaction 2,984,108) are used in our experiments to evaluate SimGCL. Because we focus on the Top-N recommendation, following the convention in the previous research <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b39">40]</ref>, we discard ratings less than 4 in Douban-Book, which is with a 1-5 rating scale, and reset the rest to 1. We split the datasets into three parts (training set, validation set, and test set) with a ratio of 7:1:2. Two common metrics: Recall@𝐾 and NDCG@𝐾 are used and we set 𝐾=20. For a rigorous and unbiased evaluation, each experiment in this section is conducted 5 times with ranking all the items and we then report the average result. Baselines. Besides LightGCN and the SGL variants, the following recent data augmentation-based methods are compared.</p><p>• Mult-VAE <ref type="bibr" target="#b14">[15]</ref> is a variational autoencoder-based recommendation model. It can be seen as a special self-supervised recommendation model because it has a reconstruction objective. • DNN+SSL <ref type="bibr" target="#b33">[34]</ref> is a recent DNN-based recommendation method which adopts the similar architecture in Fig. <ref type="figure" target="#fig_0">1</ref>, and conducts feature masking for CL.</p><p>• BUIR <ref type="bibr" target="#b13">[14]</ref> has a two-branch architecture which consists of a target network and an online network, and only uses positive examples for self-supervised learning.</p><p>• MixGCL <ref type="bibr" target="#b9">[10]</ref> designs the hop mixing technique to synthesize hard negatives for graph collaborative filtering by embedding interpolation.</p><p>Hyperparameters. For a fair comparison, we refer to the best hyperparameter settings reported in the original papers of the baselines and then fine-tune all the hyperparameters of the baselines with the grid search. As for the general settings of all the baselines, the Xavier initialization is used on all the embeddings. The embedding size is 64, the parameter for 𝐿 2 regularization is 10 −4 and the batch size is 2048. We use Adam with the learning rate 0.001 to optimize all the models. In SimGCL and SGL, we empirically let the temperature 𝜏 = 0.2, and this value is also reported as the best in the original paper of SGL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SGL vs. SimGCL: From a Comprehensive Perspective</head><p>As one of the core claims of this paper is that graph augmentations are not indispensable and inefficient in CL-based recommendation, in this part, we conduct a comprehensive comparison between SGL and SimGCL in terms of the recommendation performance, convergence speed, running time, and ability to debias. • SGL-ED is the most effective variant of SGL while SGL-ND is the least effective. When a 2-layer or 3-layer setting is used, SGL-WA outperforms SGL-ND in most cases and shows advantages over SGL-RW in a few cases. These results demonstrate that the CL loss is the main driving force of the performance improvement while intuitive graph augmentations may not be as effective as expected, and some of them may even lower the performance. • SimGCL shows the best performance in all the cases, which proves the effectiveness of the random noised-based data augmentation. Particularly, on the two larger datasets: Yelp2018 and Amazon-Book, SimGCL significantly outperforms the SGL variants by large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Convergence Speed Comparison.</head><p>In this part, we show that SimGCL converges much faster than SGL does. A 2-layer setting is used in this part and the other parameters remain unchanged. According to Fig. <ref type="figure" target="#fig_5">5</ref> and Fig. <ref type="figure" target="#fig_6">6</ref>, we can observe that, SimGCL reaches its best performance on the test set at the 25 th , the 11 th , and the 10 th epoch on Douban-Book, Yelp2018, and Amazon-Book, respectively. By contrast, SGL-ED peaks at the 38 th , the 17 th , and the 14 th epoch, respectively. SimGCL only spends 2/3 epochs that the SGL variants need. Besides, the curve of SGL-WA almost overlaps that of SGL-ED on Yelp2018 and Amazon-Book, and exhibits the same tendency to convergence. It seems that the dropout-based graph augmentations cannot speed up the model for a faster convergence. Despite that, all the CL-based methods show advantages over LightGCN at the convergence speed. When the other three methods begin to get overfitted, LightGCN is still hundreds of epochs distant from getting converged.</p><p>In the paper of SGL, the authors guess that the multiple negatives in the CL loss may contribute to the fast convergence. However, with almost infinite negative samples created by dropout, SGL-ED is basically on par with SGL-WA in speeding up the training, though the latter only has a certain number of negative samples. As for SimGCL, we consider that the remarkable convergence speed stems from the noises. By analyzing the gradients from the CL loss, we find that the noises averagely provide a constant increment, working like a momentum. In addition to the results in Fig. <ref type="figure" target="#fig_6">5 and  6</ref>, we also find that the training accelerates with 𝜖 getting larger. But when it is overlarge (e.g., greater than 1), despite the rapid decrease of BPR loss, SimGCL requires more time to get converged. A large 𝜖 acts like a large learning rate, causing the progressive zigzag optimization that will overshoot the minimum. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amazon-Book</head><p>LightGCN SGL-WA SGL-ED SimGCL  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Running Time Comparison.</head><p>In this part, we report the real running time that the compared methods cost for one epoch. The results in Table <ref type="table" target="#tab_5">4</ref> are collected on an Intel(R) Xeon(R) Gold 5122 CPU and a GeForce RTX 2080Ti GPU. As shown in Table <ref type="table" target="#tab_5">4</ref>, we calculate how many times slower the other methods are when compared with LightGCN. Because there is no graph augmentation in SGL-WA, we can see its running speed is very close to that of LightGCN. For SGL-ED, two graph augmentations are required and the computation in this part is mostly finished on CPUs, so that it is even 5.7 times slower than LightGCN on Amazon-Book. The running time increases with the volume of the datasets. By contrast, despite not as fast as SGL-WA, SimGCL is only 2.4 times slower than LightGCN on Amazon-Book, and the growth trend is far lower than that of SGL-ED. Considering that SimGCL only needs 2/3 the epochs that SGL-ED spends, it outperforms SGL in all aspects w.r.t efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Ability to Debias.</head><p>The InfoNCE loss is found to have the ability to implicitly alleviate the popularity bias by learning more uniform representations. To verify that SimGCL upgrades this ability with the noise-based representation augmentation, we divide the test set into three subsets in proportion to the popularity of items. 80% items with the fewest number of clicks/purchases are labelled 'Unpopular', 5% items which are most clicked/purchased are labelled 'Popular', and the rest items are labelled 'Normal'. We then conduct experiments to check the Recall@20 value that each group contributes (overall Recall@20 value is the sum of the values from three groups). The results are illustrated in Fig. <ref type="figure">7</ref>.</p><p>We can clearly see that the SimGCL's improvements all come from the items with lower popularity. Its prominent advantage on recommending long-tail items largely compensates for its loss on the 'Popular' group. By contrast, LightGCN is inclined to recommend popular items and achieves the highest recall value on the last two datasets. The SGL variants fall between LightGCN and SimGCL on exploring long-tail items and exhibit similar recommendation preference. Combining Fig. <ref type="figure">2</ref> with Fig. <ref type="figure">7</ref>, we can easily find that there is a positive correlation between the uniformity of representations and the ability to debias. Since the popular items probably have been exposed to users from other sources, recommending them may no be a good choice. On this point, SimGCL significantly outperforms SGL, and its extraordinary performance on discovering long-tail items fits the real need of users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Sensitivity Analysis</head><p>In this part, we investigate the impact of the two important hyperparameters in SimGCL. Here we adopt the experimental settings used in section 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Impact of 𝜆.</head><p>By fixing 𝜖 at 0.1, we change 𝜆 to a set of predetermined representative values presented in Fig. <ref type="figure" target="#fig_7">8</ref>. As can be observed, with the increase of 𝜆, the performance of SimGCL starts to increase at the beginning, and it gradually reaches its peak when 𝜆 is 0.2 on Douban-Book, 0.5 on Yelp2018, and 2 on Amazon-Book. Afterwards, it starts to decline. Besides, in contrast to Fig. <ref type="figure" target="#fig_8">9</ref>, more dramatic changes are observed in Fig. <ref type="figure" target="#fig_7">8</ref> though 𝜖 and 𝜆 are tuned in the same scope, which demonstrates that 𝜖 can provide a finergrained regulation beyond that provided only by tuning 𝜆. 4.3.2 Impact of 𝜖. We think a larger 𝜖 leads to a more uniform distribution that can help to debias. However, when it is too large, the recommendation task will be hindered because the high similarity between connected nodes cannot be reflected by an over-uniform distribution. We fix 𝜆 at the best values on the three datasets as reported in Fig. <ref type="figure" target="#fig_7">8</ref>, and then adjust 𝜖 to see the performance change. As shown in Fig. <ref type="figure" target="#fig_8">9</ref>, the shapes of the curves are as expected. On all the datasets, when 𝜖 is near 0.1, SimGCL achieves the best performance. We also find that initializing embeddings with uniform distributions (including Xavier initialization) leads to 3%-4% performance improvement compared with Gaussian initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison with Other Methods</head><p>To further confirm the outstanding competence of SimGCL, we compare it with other four recently proposed data augmentationbased methods. According to Table <ref type="table" target="#tab_8">5</ref>, SimGCL outperforms all the baselines, and MixGCF is the runner-up. Meanwhile, we find that some data augmentation-based recommendation methods are not as powerful as expected, and even surpassed by LightGCN in many cases. We attribute their failure to: <ref type="bibr" target="#b0">(1)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amazon-Book</head><p>Recall NDCG  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amazon-Book</head><p>Recall NDCG (2). DNNs are proved effective when user/item features are available.</p><p>In our datasets, no features are provided and we mask embeddings learned by DNN to conduct self-supervised learning, so it cannot fulfill itself in this situation. (3). In the paper of BUIR, it removes long-tail nodes to achieve a good performance, but we use all the users and items. Besides, its siamese network may also collapse to a trivial solution on some long-tail nodes because it does not use negative examples, which may account for its incompetence. Graph Neural Networks (GNNs) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> now have become widely acknowledged powerful architectures for modeling recommendation data. This new neural network paradigm ends the regime of MLP-based recommendation models in the academia, and boosts the neural recommender systems to a new level. A large number of recommendation models, which adopt GNNs as their bases, claim that they have achieved state-of-the-art performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> in different subfields. Particularly, GCN <ref type="bibr" target="#b12">[13]</ref>, as the most prevalent variant of GNNs, further fuels the development of the graph neural recommendation models like GCMC <ref type="bibr" target="#b1">[2]</ref>, NGCF <ref type="bibr" target="#b24">[25]</ref>, LightGCN <ref type="bibr" target="#b8">[9]</ref>, and LCF <ref type="bibr" target="#b41">[42]</ref>. Despite the different implementations in details, these GCN-driven models share a common idea that is to acquire the information from the neighbors in the user-item graph layer by layer to refine the target node's embeddings and fulfill graph reasoning <ref type="bibr" target="#b28">[29]</ref>. Among these methods, LightGCN is the most popular one due to its simple structure and decent performance. Following <ref type="bibr" target="#b26">[27]</ref>, it removes the redundant operations including transformation matrices and nonlinear activation functions. Such a design is proved efficient and effective, and inspires a lot of follow-up CL-based recommendation models like SGL <ref type="bibr" target="#b27">[28]</ref> and MHCN <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Contrastive Learning in Recommendation</head><p>As CL works in a self-supervised manner <ref type="bibr" target="#b40">[41]</ref>, it is inherently a possible solution to the data sparsity issue <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> in recommender systems. Inspired by the achievements of CL in other fields, there has been a wave of new research that integrates CL with recommendation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45]</ref>. Zhou et al. <ref type="bibr" target="#b44">[45]</ref> adopted random masking on attributes and items to create sequence augmentations for sequential model pretraining with mutual information maximization. Wei et al. <ref type="bibr" target="#b25">[26]</ref> reformulated the cold-start item representation learning from an information-theoretic standpoint and maximized the mutual dependencies between item content and collaborative signals to alleviate the data sparsity issue. Similar ideas are also found in <ref type="bibr" target="#b33">[34]</ref>, where a two-tower DNN architecture is developed for recommendation, in which the item tower is also shared for contrasting augmented item features. SEPT <ref type="bibr" target="#b37">[38]</ref> and COTREC <ref type="bibr" target="#b30">[31]</ref> further propose to mine multiple positive samples with semisupervised learning on the perturbed graph for social/session-based recommendation. In addition to the dropout, CL4Rec <ref type="bibr" target="#b32">[33]</ref> proposes to reorder and crop item segments for sequential data augmentation. Yu et al. <ref type="bibr" target="#b39">[40]</ref>, Zhang et al. <ref type="bibr" target="#b42">[43]</ref> and Xia et al. <ref type="bibr" target="#b31">[32]</ref> leveraged hypergraph to model recommendation data, and proposed to contrast different hypergraph structures for representation regularization. In addition to the data sparsity problem, Zhou et al. <ref type="bibr" target="#b43">[44]</ref> theoretically proved that CL can also mitigate the exposure bias in recommendation, and developed a method named CLRec to improve deep match in terms of fairness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we revisit the dropout-based CL in recommendation, and investigate how it improves recommendation performance. We reveal that, in CL-based recommendation models, the CL loss is the core and the graph augmentation only plays a secondary role.</p><p>Optimizing the CL loss leads to a more uniform representation distribution, which helps to debias in the scenario of recommendation.</p><p>We then develop a simple graph-augmentation-free CL method to regulate the uniformity of the representation distribution in a more straightforward way. By adding directed random noises to the representation for different data augmentations and contrast, the proposed method can significantly enhance recommendation.</p><p>The extensive experiments demonstrate that the proposed method outperforms its graph augmentation-based counterparts and meanwhile the training time is dramatically reduced.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graph contrastive learning with edge dropout for recommendation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Distribution of item representations learned from the dataset of Yelp2018. Distribution of item representations learned from the dataset of Amazon-Book.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of the proposed random noise-based data augmentation in R 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Topic 18 :</head><label>18</label><figDesc>Recommender System SIGIR '22, July 11-15, 2022, Madrid, Spain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>Figure Trends of uniformity. The star indicates the epoch where the best recommendation performance is reached. Lower L uniform numbers are better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The performance curves in the first 50 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The loss curves in the first 50 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Influence of the magnitude 𝜆 of CL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Influence of the noise magnitude 𝜖.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of different SGL variants.</figDesc><table><row><cell>Method</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Book</cell></row><row><cell></cell><cell cols="4">Recall@20 NDCG@20 Recall@20 NDCG@20</cell></row><row><cell>LightGCN</cell><cell>0.0639</cell><cell>0.0525</cell><cell>0.0410</cell><cell>0.0318</cell></row><row><cell>SGL-ND</cell><cell>0.0644</cell><cell>0.0528</cell><cell>0.0440</cell><cell>0.0346</cell></row><row><cell>SGL-ED</cell><cell>0.0675</cell><cell>0.0555</cell><cell>0.0478</cell><cell>0.0379</cell></row><row><cell>SGL-RW</cell><cell>0.0667</cell><cell>0.0547</cell><cell>0.0457</cell><cell>0.0356</cell></row><row><cell>SGL-WA</cell><cell>0.0671</cell><cell>0.0550</cell><cell>0.0466</cell><cell>0.0373</cell></row><row><cell>CL Only</cell><cell>0.0245</cell><cell>0.0190</cell><cell>0.0314</cell><cell>0.0258</cell></row><row><cell cols="5">Because we only learn representations over the original user-item</cell></row><row><cell cols="2">graph, then we have z ′ 𝑖</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The comparison of time complexity</figDesc><table><row><cell>Component</cell><cell>LightGCN</cell><cell>SGL-ED</cell><cell>SimGCL</cell></row><row><cell>Adjacency Matrix</cell><cell>O (2|𝐸 |)</cell><cell>O (2|𝐸 | + 4𝜌 |𝐸 |)</cell><cell>O (2|𝐸 |)</cell></row><row><cell>Graph Convolution</cell><cell>O (2|𝐸 |𝐿𝑑)</cell><cell>O ( (2 + 4𝜌) |𝐸 |𝐿𝑑)</cell><cell>O (6|𝐸 |𝐿𝑑)</cell></row><row><cell>BPR Loss</cell><cell>O (2𝐵𝑑)</cell><cell>O (2𝐵𝑑)</cell><cell>O (2𝐵𝑑)</cell></row><row><cell>CL Loss</cell><cell>-</cell><cell>O (𝐵𝑑 + 𝐵𝑀𝑑)</cell><cell>O (𝐵𝑑 + 𝐵𝑀𝑑)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance Comparison for different CL methods on three benchmarks.</figDesc><table><row><cell cols="2">Method</cell><cell cols="2">Douban-Book</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Book</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell></row><row><cell></cell><cell>LightGCN</cell><cell>0.1288</cell><cell>0.1081</cell><cell>0.0631</cell><cell>0.0515</cell><cell>0.0384</cell><cell>0.0298</cell></row><row><cell></cell><cell>SGL-ND</cell><cell>0.1619 (+25.7%)</cell><cell>0.1448 (+34.0%)</cell><cell>0.0643 (+1.9%)</cell><cell>0.0529 (+2.7%)</cell><cell>0.0432 (+12.5%)</cell><cell>0.0334 (+12.1%)</cell></row><row><cell>1-Layer</cell><cell>SGL-ED SGL-RW</cell><cell>0.1658 (+28.7%) 0.1658 (+28.7%)</cell><cell>0.1491 (+37.9%) 0.1491 (+37.9%)</cell><cell>0.0637 (+1.0%) 0.0637 (+1.0%)</cell><cell>0.0526 (+2.1%) 0.0526 (+2.1%)</cell><cell>0.0451 (+17.4%) 0.0451 (+17.4%)</cell><cell>0.0353 (+18.5%) 0.0353 (+18.5%)</cell></row><row><cell></cell><cell>SGL-WA</cell><cell>0.1628 (+26.4%)</cell><cell>0.1454 (+34.5%)</cell><cell>0.0628 (-0.4%)</cell><cell>0.0525 (+1.9%)</cell><cell>0.0403 (+4.9%)</cell><cell>0.0320 (+7.4%)</cell></row><row><cell></cell><cell>SimGCL</cell><cell cols="2">0.1720 (+33.5%) 0.1519 (+40.5%)</cell><cell>0.0689 (+9.2%)</cell><cell cols="3">0.0572 (+11.1%) 0.0453 (+18.0%) 0.0358 (+20.1%)</cell></row><row><cell></cell><cell>LightGCN</cell><cell>0.1485</cell><cell>0.1272</cell><cell>0.0622</cell><cell>0.0504</cell><cell>0.0411</cell><cell>0.0315</cell></row><row><cell></cell><cell>SGL-ND</cell><cell>0.1622 (+9.2%)</cell><cell>0.1434 (+12.7%)</cell><cell>0.0658 (+5.8%)</cell><cell>0.0538 (+6.7%)</cell><cell>0.0427 (+3.9%)</cell><cell>0.0335 (+6.3%)</cell></row><row><cell>2-Layer</cell><cell>SGL-ED SGL-RW</cell><cell>0.1721 (+15.9%) 0.1710 (+15.2%)</cell><cell>0.1525 (+19.9%) 0.1516 (+19.2%)</cell><cell>0.0668 (+7.4%) 0.0644 (+3.5%)</cell><cell>0.0549 (+8.9%) 0.0530 (+5.2%)</cell><cell>0.0468 (+13.9%) 0.0453 (+10.2%)</cell><cell>0.0371 (+17.8%) 0.0358 (+13.7%)</cell></row><row><cell></cell><cell>SGL-WA</cell><cell>0.1687 (+13.6%)</cell><cell>0.1501 (+18.0%)</cell><cell>0.0653 (+5.0%)</cell><cell>0.0544 (+7.9%)</cell><cell>0.0453 (+10.2%)</cell><cell>0.0358 (+13.7%)</cell></row><row><cell></cell><cell>SimGCL</cell><cell cols="6">0.1770 (+19.2%) 0.1582 (+24.4%) 0.0719 (+15.6%) 0.0601 (+19.2%) 0.0507 (+23.4%) 0.0405 (+28.6%)</cell></row><row><cell></cell><cell>LightGCN</cell><cell>0.1392</cell><cell>0.1188</cell><cell>0.0639</cell><cell>0.0525</cell><cell>0.0410</cell><cell>0.0318</cell></row><row><cell></cell><cell>SGL-ND</cell><cell>0.1626 (+16.8%)</cell><cell>0.1450 (+22.1%)</cell><cell>0.0644 (+0.8%)</cell><cell>0.0528 (+0.6%)</cell><cell>0.0440 (+7.3%)</cell><cell>0.0346 (+8.8%)</cell></row><row><cell>3-Layer</cell><cell>SGL-ED SGL-RW</cell><cell>0.1732 (+24.4%) 0.1730 (+24.3%)</cell><cell>0.1551 (+30.6%) 0.1546 (+30.1%)</cell><cell>0.0675 (+5.6%) 0.0667 (+4.4%)</cell><cell>0.0555 (+5.7%) 0.0547 (+4.2%)</cell><cell>0.0478 (+16.6%) 0.0457 (+11.5%)</cell><cell>0.0379 (+19.2%) 0.0356 (+12.0%)</cell></row><row><cell></cell><cell>SGL-WA</cell><cell>0.1705 (+22.5%)</cell><cell>0.1525 (+28.4%)</cell><cell>0.0671 (+5.0%)</cell><cell>0.0550 (+4.8%)</cell><cell>0.0466 (+13.7%)</cell><cell>0.0373 (+18.4%)</cell></row><row><cell></cell><cell>SimGCL</cell><cell cols="6">0.1772 (+27.3%) 0.1583 (+33.2%) 0.0721 (+12.8%) 0.0601 (+14.5%) 0.0515 (+25.6%) 0.0414 (+30.2%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>4.2.1 Performance Comparison.We first further compare SGL with SimGCL on three different datasets with different layer settings. We do not report the results with deeper layers because both SGL and SimGCL reach their best performances with shallow structures. The used hyperparameters are reported in section 4.3. We thicken the figures representing the best performance and underline the second best. The improvements are calculated by using LightGCN as the baseline. According to Table3, we can draw the following observations and conclusions: • All the SGL variants and SimGCL are effective in improving LightGCN under different settings. The largest improvements are observed on Douban-Book. When the layer number is 1, SimGCL can remarkably improve LightGCN by 33.5% on Recall, and 40.5% on NDCG.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Running time for per epoch (x in the brackets represents times).</figDesc><table><row><cell></cell><cell cols="3">Method</cell><cell></cell><cell cols="13">Douban-Book Yelp2018 Amazon-Book Time (s) Time (s) Time (s)</cell></row><row><cell></cell><cell cols="3">LightGCN</cell><cell></cell><cell></cell><cell></cell><cell>3.6</cell><cell></cell><cell></cell><cell cols="2">13.6</cell><cell></cell><cell></cell><cell></cell><cell>41.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">SGL-WA</cell><cell></cell><cell cols="3">4.4 (1.2x)</cell><cell></cell><cell cols="4">16.3 (1.2x)</cell><cell></cell><cell cols="3">47.0 (1.1x)</cell><cell></cell></row><row><cell></cell><cell cols="3">SGL-ED</cell><cell></cell><cell cols="3">13.3 (3.7x)</cell><cell></cell><cell cols="4">62.3 (4.6x)</cell><cell cols="5">235.3 (5.7x)</cell></row><row><cell></cell><cell cols="3">SimGCL</cell><cell></cell><cell cols="3">6.1 (1.7x)</cell><cell></cell><cell cols="4">27.9 (2.1x)</cell><cell></cell><cell cols="3">98.4 (2.4x)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Douban-Book</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Yelp2018</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>17.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>15.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>12.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>7.5% 10.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.0% 4.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.0% 3.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.5% 5.0%</cell><cell></cell><cell></cell><cell></cell><cell cols="2">LightGCN SGL-WA SGL-ED</cell><cell>1.0% 2.0%</cell><cell></cell><cell></cell><cell cols="3">LightGCN SGL-WA SGL-ED</cell><cell>1.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0%</cell><cell></cell><cell></cell><cell></cell><cell>SimGCL</cell><cell></cell><cell>0.0%</cell><cell></cell><cell></cell><cell cols="2">SimGCL</cell><cell></cell><cell>0.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. LightGCN, MixGCF, and SimGCL are all based on the graph convolution mechanism, which are more capable of modeling graph data compared with Mult-VAE.</figDesc><table><row><cell></cell><cell>0.16</cell><cell></cell><cell cols="3">Douban-Book</cell><cell>0.05</cell><cell></cell><cell></cell><cell>Yelp2018</cell><cell></cell><cell>0.030</cell><cell>Amazon-Book</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">LightGCN</cell><cell></cell><cell></cell><cell cols="3">LightGCN</cell><cell></cell><cell></cell><cell>LightGCN</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SGL-WA</cell><cell></cell><cell></cell><cell cols="3">SGL-WA</cell><cell></cell><cell></cell><cell>SGL-WA</cell></row><row><cell></cell><cell>0.14</cell><cell></cell><cell cols="2">SGL-ED</cell><cell></cell><cell></cell><cell cols="3">SGL-ED</cell><cell></cell><cell></cell><cell>SGL-ED</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SimGCL</cell><cell></cell><cell>0.04</cell><cell cols="3">SimGCL</cell><cell></cell><cell>0.025</cell><cell>SimGCL</cell></row><row><cell></cell><cell>0.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.020</cell></row><row><cell>Recall@20</cell><cell>0.06 0.08 0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.02 0.03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.015</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.010</cell></row><row><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.005</cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell cols="2">Un po pu la r</cell><cell>No rm al</cell><cell>Po pu la r</cell><cell>0.00</cell><cell cols="2">Un po pu la r</cell><cell>No al</cell><cell>Po pu la r</cell><cell>0.000</cell><cell>Un po pu la r</cell><cell>No rm al</cell><cell>Po pu la r</cell></row><row><cell cols="13">Figure 7: Performance comparison over different item groups</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Douban-Book</cell><cell></cell><cell></cell><cell></cell><cell>Yelp2018</cell><cell></cell><cell></cell></row><row><cell cols="2">18.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.0%</cell></row><row><cell cols="2">16.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.5%</cell></row><row><cell cols="2">14.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">12.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.0%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">10.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.5%</cell></row><row><cell cols="2">8.0%</cell><cell></cell><cell cols="2">Recall NDCG</cell><cell></cell><cell>5.0% 4.5%</cell><cell></cell><cell cols="2">Recall NDCG</cell><cell></cell><cell>3.0%</cell></row><row><cell></cell><cell></cell><cell>0 0 .0 1</cell><cell>0 .0 5</cell><cell cols="2">0 .1 0 .2 0 .5 1 2 5</cell><cell></cell><cell>0 0 .0 1</cell><cell>0 .0 5</cell><cell cols="2">0 .1 0 .2 0 .5 1 2 5</cell><cell></cell><cell>0 0 .0 1</cell><cell>0 .0 5</cell><cell>0 .1 0 .2 0 .5 1 2 5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison with other SOTA models.In SimGCL, we use the random noises sampled from a uniform distribution to implement data augmentation. However, there are other types of noises including the Gaussian noises and adversarial noises. Here we also test different noises and report their best results in</figDesc><table><row><cell>Method</cell><cell cols="2">Douban-Book</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Book</cell></row><row><cell></cell><cell cols="6">Recall NDCG Recall NDCG Recal NDCG</cell></row><row><cell cols="2">LightGCN 0.1485</cell><cell>0.1272</cell><cell>0.0639</cell><cell>0.0525</cell><cell>0.0411</cell><cell>0.0315</cell></row><row><cell>Mult-VAE</cell><cell>0.1310</cell><cell>0.1103</cell><cell>0.0584</cell><cell>0.0450</cell><cell>0.0407</cell><cell>0.0315</cell></row><row><cell cols="2">DNN+SSL 0.1366</cell><cell>0.1148</cell><cell>0.0483</cell><cell>0.0382</cell><cell>0.0438</cell><cell>0.0337</cell></row><row><cell>BUIR</cell><cell>0.1533</cell><cell>0.1317</cell><cell>0.0578</cell><cell>0.0461</cell><cell>0.0423</cell><cell>0.0326</cell></row><row><cell>MixGCF</cell><cell>0.1731</cell><cell>0.1552</cell><cell>0.0713</cell><cell>0.0589</cell><cell>0.0485</cell><cell>0.0378</cell></row><row><cell>SimGCL</cell><cell cols="6">0.1772 0.1583 0.0721 0.0601 0.0515 0.0410</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 (</head><label>6</label><figDesc>𝑢 denotes uniform noises sampled from 𝑈 (0, 1), 𝑝 represents the positive uniform noises which differ from 𝑢 in not satisfying the second constraint in section 3.1, 𝑔 denotes Gaussian noises generated by the standard Gaussian distribution, and 𝑎 denotes adversarial noises generated by FGSM<ref type="bibr" target="#b7">[8]</ref>). According to Table6, SimGCL 𝑔 shows comparable performance while SimGCL 𝑎 is less effective. The possible reason is that we apply 𝐿 2 normalization to the noises. The normalized noises generated by the standard Gaussian distribution can fit a much flatter Gaussian distribution (can be easily proved) which approximates a uniform distribution. So, the comparable results are observed. As for SimGCL 𝑎 , the adversarial noises are generated by only targeting maximizing the CL loss while the recommendation loss has a dominant status that impacts the performance more during optimization. As for SimGCL 𝑝 , we notice a slight performance drop compared with SimGCL 𝑢 in most cases, which suggests the necessity of the directional constraint for creating more informative augmentations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison between different SimGCL variants.</figDesc><table><row><cell>Method</cell><cell cols="2">Douban-Book</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Book</cell></row><row><cell></cell><cell cols="6">Recall NDCG Recall NDCG Recal NDCG</cell></row><row><cell cols="2">LightGCN 0.1485</cell><cell>0.1272</cell><cell>0.0639</cell><cell>0.0525</cell><cell>0.0411</cell><cell>0.0315</cell></row><row><cell>SimGCL 𝑎</cell><cell>0.1561</cell><cell>0.1379</cell><cell>0.0604</cell><cell>0.0505</cell><cell>0.0455</cell><cell>0.0358</cell></row><row><cell>SimGCL 𝑝</cell><cell>0.1751</cell><cell>0.1565</cell><cell>0.0708</cell><cell>0.0593</cell><cell>0.0514</cell><cell>0.0409</cell></row><row><cell>SimGCL 𝑔</cell><cell cols="3">0.1773 0.1586 0.0718</cell><cell>0.0599</cell><cell>0.0511</cell><cell>0.0408</cell></row><row><cell>SimGCL 𝑢</cell><cell>0.1772</cell><cell cols="5">0.1583 0.0721 0.0601 0.0515 0.0414</cell></row><row><cell cols="3">5 RELATED WORK</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">5.1 Graph Neural Recommendation Models</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This work is supported by Australian Research Council Future Fellowship (Grant No. FT210100624), Discovery Project (Grant No. DP190101985) and Discovery Early Career Research Award (Grant No. DE200101465). Topic 18: Recommender System SIGIR '22, July 11-15, 2022, Madrid, Spain</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kernel density estimation via diffusion</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">F</forename><surname>Zdravko I Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><forename type="middle">P</forename><surname>Grotowski</surname></persName>
		</author>
		<author>
			<persName><surname>Kroese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2916" to="2957" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hande</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03240</idno>
		<title level="m">Bias and debias in recommender system: A survey and future directions</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingrong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghua</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12843</idno>
		<title level="m">Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<title level="m">SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems</title>
		<author>
			<persName><forename type="first">Tinglin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on contrastive self-supervised learning</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Zaki</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debapriya</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fillia</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><surname>Makedon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technologies</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised Contrastive Learning</title>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrapping User and Item Representations for One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Dongha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongku</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjun</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<editor>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</editor>
		<meeting><address><addrLine>Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1513" to="1522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Rahul G Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 world wide web conference</title>
				<meeting>the 2018 world wide web conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Disentangled Self-Supervision in Sequential Recommenders</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="483" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Memory Augmented Multi-Instance Contrastive Predictive Coding for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="813" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
				<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sparsity, scalability, and distribution in recommender systems</title>
		<author>
			<persName><forename type="first">Badrul</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarwar</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</title>
				<meeting>the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contrastive learning for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
				<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5382" to="5390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-supervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02260</idno>
		<title level="m">Graph Neural Networks in Recommender Systems: A Survey</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-Supervised Graph Co-Training for Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2180" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-Supervised Hypergraph Convolutional Networks for Sessionbased Recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4503" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bolin Ding, and Bin Cui</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14395</idno>
	</analytic>
	<monogr>
		<title level="m">Contrastive Learning for Sequential Recommendation</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-supervised Learning for Large-scale Item Recommendations</title>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Tjoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieqi</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4321" to="4330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning with Augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive Implicit Friends Identification over Heterogeneous Network for Social Recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generating reliable friends via adversarial training to improve social recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="768" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Socially-Aware Self-Supervised Tri-Training for Recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
				<editor>
			<persName><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Beng</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chin</forename><surname>Ooi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2084" to="2092" />
		</imprint>
	</monogr>
	<note>Xin Xia, Xiangliang Zhang, and Nguyen Quoc Viet Hung</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Enhance Social Recommendation with Adversarial Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02340</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
	<note>Nguyen Quoc Viet Hung, and Xiangliang Zhang</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Self-Supervised Learning for Recommender Systems: A Survey</title>
				<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph convolutional network for recommendation with low-pass collaborative filters</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10936" to="10945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Double-Scale Self-Supervised Hypergraph Learning for Group Recommendation</title>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2557" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Contrastive learning for debiased candidate generation in large-scale recommender systems</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3985" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07873</idno>
		<title level="m">Sˆ3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03019</idno>
		<title level="m">SelfCF: A Simple Framework for Self-supervised Collaborative Filtering</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
