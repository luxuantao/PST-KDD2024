<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Correlation Particle Filter for Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
							<email>tzzhang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
							<email>liusi@iie.ac.cn.</email>
						</author>
						<author>
							<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
							<email>csxu@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<email>liubin@dress-plus.com</email>
						</author>
						<author>
							<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>mhyang@ucmerced.edu.</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Recognition, Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100093</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Moshanghua Tech Co., Ltd. Beijing</orgName>
								<address>
									<settlement>Bin Liu</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>95344</postCode>
									<settlement>Merced</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Correlation Particle Filter for Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1AC69716979259FCA20833DF7DCE4BC1</idno>
					<idno type="DOI">10.1109/TIP.2017.2781304</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2781304, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel correlation particle filter (CPF) for robust visual tracking. Instead of a simple combination of a correlation filter and a particle filter, we exploit and complement the strength of each one. Compared with existing tracking methods based on correlation filters and particle filters, the proposed tracker has four major advantages:</p><p>(1) It is robust to partial and total occlusions, and can recover from lost tracks by maintaining multiple hypotheses. (2) It can effectively handle large-scale variation via a particle sampling strategy. (3) It can efficiently maintain multiple modes in the posterior density using fewer particles than conventional particle filters, resulting in low computational cost. ( <ref type="formula">4</ref>) It can shepherd the sampled particles toward the modes of the target state distribution using a mixture of correlation filters, resulting in robust tracking performance. Extensive experimental results on challenging benchmark datasets demonstrate that the proposed CPF tracking algorithm performs favorably against the state-ofthe-art methods. Manuscript received *** **, 2016; revised *** **, 2016; accepted *** **, 2017.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual tracking is one of the most important tasks in computer vision because of its wide range of applications such as video surveillance, motion analysis, human-computer interaction, and vehicle navigation, to name a few. The main challenge for robust visual tracking is to handle large appearance changes of target objects over time. Although significant progress has been made in recent years, it remains a difficult task to develop robust algorithms for object state estimation for tracking scenarios with challenging factors such as illumination changes, fast motions, pose variations, partial occlusions and background clutter.</p><p>Visual tracking algorithms can be generally categorized as either generative or discriminative approaches. Generative trackers typically formulate tracking problem as searching for the best image regions which are similar to the tracked targets. <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Black et al. <ref type="bibr" target="#b3">[4]</ref> utilize an off-line subspace model to represent target object for tracking. In <ref type="bibr" target="#b5">[6]</ref>, the mean shift tracking algorithm models a target with nonparametric distributions of color features and locates the object with mode shifts. In <ref type="bibr" target="#b6">[7]</ref>, an adaptive appearance model is developed based on the mixture Fig. <ref type="figure">1</ref>: Comparisons of the proposed CPF tracker with the state-of-the-art correlation filter trackers (DSST <ref type="bibr" target="#b0">[1]</ref> and KCF <ref type="bibr" target="#b1">[2]</ref>) in challenging situations of heavy occlusions on the jogging-1 sequence <ref type="bibr" target="#b2">[3]</ref>. The proposed tracker uses a particle sampling strategy to maintain multiple hypotheses. It is robust to heavy occlusions and can recover from lost tracks.</p><p>of Gaussians to model objects with three components. The Frag tracker <ref type="bibr" target="#b12">[13]</ref> addresses the partial occlusion problem by modeling object appearance with histograms of local patches. However, this method is less effective for handling large appearance changes as the model is not updated. Kwon et al. <ref type="bibr" target="#b8">[9]</ref> decompose the observation model into multiple basic observation models to cover a wide range of pose and illumination variations. The sparse tracker <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref> utilizes a sparse representation to model target appearance. Different from generative trackers, discriminative approaches cast tracking as a classification problem that distinguishes tracked targets from backgrounds <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In <ref type="bibr" target="#b15">[16]</ref>, a target confidence map is constructed by determining the most discriminative features based on color pixels. Avidan <ref type="bibr" target="#b16">[17]</ref> combines a set of weak classifiers into a strong one to do ensemble tracking. Grabner et al. <ref type="bibr" target="#b17">[18]</ref> propose an online boosting tracking method to update discriminative features. Babenko et al. <ref type="bibr" target="#b18">[19]</ref> introduce multiple instance learning into online tracking where samples are considered within positive and negative bags or sets. The TLD tracker <ref type="bibr" target="#b19">[20]</ref> explicitly decomposes the long-term tracking task into tracking, learning and detection. In <ref type="bibr" target="#b20">[21]</ref>, Hare et al. use an online structured output support vector machine for adaptive visual tracking. Zhang et al. <ref type="bibr" target="#b21">[22]</ref> utilize multiple experts using entropy minimization to address the model drift problem in online tracking.</p><p>Recently, tracking methods based on correlation filters have been proved to be able to achieve fairly high speed and robust performance <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. For tracking, a correlation filter evaluates the similarity degree by computing the dot product for each possible alignment of a learned template (or filter) relative to a test image sample. The computation of correlation filters can be sped up based on the convolution theorem, which states that the convolution in the spatial domain can be calculated as the element-wise multiplication of the Fourier transforms in the frequency domain. Due to its computational efficiency, correlation filters have attracted considerable attention in visual tracking <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Although the CSK <ref type="bibr" target="#b24">[25]</ref> and KCF <ref type="bibr" target="#b1">[2]</ref> methods achieve the state-of-the-art performance both in accuracy and robustness, these correlation filter based trackers do not deal with scale variation and occlusion well.</p><p>To handle the scale variation of target objects, Danelljan et al.</p><p>propose a novel DSST tracker <ref type="bibr" target="#b0">[1]</ref> with adaptive multi-scale correlation filters using HOG features. Although the DSST tracker performs well for robust scale estimation by learning discriminative correlation filters based on a scale pyramid representation, it does not deal with partial occlusion or total occlusion well. Figure <ref type="figure">1</ref> shows some tracking results on the jogging-1 sequence by two correlation filter based trackers, DSST <ref type="bibr" target="#b0">[1]</ref> and KCF <ref type="bibr" target="#b1">[2]</ref>, which have achieved state-of-theart results in terms of accuracy in the VOT challenge <ref type="bibr" target="#b33">[34]</ref>. However, these two trackers drift off when the target objects undergo heavy occlusions. These correlation filter based trackers do not handle occlusions well because they use only one single hypothesis, which means these trackers search for the current state of target object only around the previous state. As a result, these trackers are likely to fail when partial occlusion or fast motion occurs. On the other hand, particle filters <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> can be used to handle large-scale variation and partial occlusion. A particle filter is based on the Bayesian formulation where the samples are propagated over time to maintain multiple hypotheses, and uses a stochastic motion model to predict the states in the next time instance. In a particle-based tracking method, maintaining multiple hypotheses allows the tracker to handle clutters in the background, partial and total occlusions, and recovery from failure or temporary distraction. Thus, particle filters have been widely used in tracking methods for the strength to handle nonlinear target motion and flexibility to incorporate with different object representations. In general, when more particles are sampled and a robust target representation is constructed, particle filter based tracking algorithms are more likely to perform reliably in cluttered and noisy environments. However, the computational cost of particle filter based trackers tend to increase linearly with the number of particles, which is the bottleneck for its use in visual tracking. Moreover, particle filter based trackers determine each target object state based on the sampled particles. If the sampled particles do not cover target object states well as shown in Figure <ref type="figure" target="#fig_0">2</ref>(a), the predicted target state may be not correct. To overcome this problem, it is better to shepherd the sampled particles toward the modes of the target state distribution.</p><p>In this work, we aim to exploit the strength of correlation filters and particle filters and complement each other: (1) Particle filters provide a probabilistic framework for tracking objects by propagating the posterior density over time based on a factored sampling technique. With dense sampling, the states for target objects undergoing large-scale variation can be covered. Moreover, particle filters can also handle multimodal problems by maintaining multiple hypotheses. Therefore, particle filters can effectively help correlation filter handle scale variation and partial occlusion problems. (2) For each sampled particle, a correlation filter can be applied such that particles are shepherded to the local modes of the target object as shown in Figure <ref type="figure" target="#fig_0">2</ref>. Here, the sampled states by a standard particle filter are shown in Figure <ref type="figure" target="#fig_0">2(a)</ref>. With the correlation filter, these particles are shepherded the modes of the target state distribution as shown in Figure <ref type="figure" target="#fig_0">2(b)</ref>. Therefore, we do not need to draw particles densely to maintain multiple modes, because particles are moved to the local maxima actively after the correlation filter analysis. As a result, we can maintain multiple modes using fewer particles in comparison to the conventional particle filter. Since the computational load of a particle-based tracking method depends heavily on the number of drawn particles, correlation filters can be used in these methods for efficient and effective visual tracking.</p><p>Motivated by the above observations, we propose a novel Correlation Particle Filter (CPF) for robust visual tracking, which has the advantages of both particle filters and correlation filters, e.g., robustness to partial occlusion as well as scale variation, and computational efficiency. The merits of the proposed CPF tracking method are as follows. (1) The proposed CPF tracker is robust to partial and total occlusions, and can recover from lost tracks by maintaining multiple hypotheses. (2) The proposed CPF tracker can effectively overcome the scale variation problem via a particle sampling strategy as in traditional particle filter. (3) The proposed CPF tracker can efficiently maintain multiple modes in the posterior density using fewer particles than conventional particle filters do, resulting in low computational cost. (4) The proposed CPF tracker can shepherd the sampled particles toward the modes of the target state distribution using a mixture of correlation filters, resulting in robust tracking performance. During tracking, target object state is estimated as a weighted average of all particles. We evaluate the proposed tracking algorithm on a large-scale benchmark with 50 challenging image sequences <ref type="bibr" target="#b2">[3]</ref>. Extensive experimental results show that the proposed CPF tracking algorithm performs favorably against the state-of-the-art methods in terms of accuracy, efficiency, and robustness.</p><p>The paper is organized as follows. In Section II, we summarize the methods that are most related to our work. Section III gives a detailed description of the proposed correlation particle filter tracking approach. Experimental results are reported and analyzed in Section IV. We conclude the paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Visual tracking has been studied extensively with numerous applications. A comprehensive review of the tracking methods is beyond the scope of the paper, and surveys of this filed can be found in <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. In this section, we discuss the methods closely related to this work including trackers based on correlation filter and particle filters in details.</p><p>Correlation Filter Trackers: Correlation filters have attracted considerable attention recently in visual tracking due to its computational efficiency and robustness. Bolme et al. encode target appearance by learning an adaptive correlation filter which is optimized by minimizing the output sum of squared error (MOSSE) <ref type="bibr" target="#b22">[23]</ref>. With the use of correlation filters, the MOSSE tracker is computationally efficient with a speed capability several hundreds frames per second. Heriques et al. exploit the circulant structure of adjacent image patches in a kernel space and propose the CSK method <ref type="bibr" target="#b24">[25]</ref>, which achieves the highest speed in a recent benchmark <ref type="bibr" target="#b2">[3]</ref>. The CSK method builds on illumination intensity features and is further improved by using HOG features in the KCF tracking algorithm <ref type="bibr" target="#b1">[2]</ref>. Danelljan et al. exploit adaptive color attributes <ref type="bibr" target="#b25">[26]</ref> by mapping multi-channel features into a Gaussian kernel space, and propose the DSST tracker <ref type="bibr" target="#b0">[1]</ref> with adaptive multiscale correlation filters using HOG features to handle the scale change of target objects. Zhang et al. <ref type="bibr" target="#b23">[24]</ref> incorporate context information into filter learning and model the scale change based on consecutive correlation responses. Hong et al. <ref type="bibr" target="#b26">[27]</ref> propose a biology-inspired framework (MUSTer) where shortterm processing and long-term processing are cooperated with each other. In <ref type="bibr" target="#b27">[28]</ref>, Ma et al. introduce an online random fern classifier as a re-detection component for long-term tracking. Li et al. <ref type="bibr" target="#b30">[31]</ref> introduce reliable local patches to exploit the use of local contexts and treat the KCF as the base tracker. In <ref type="bibr" target="#b28">[29]</ref>, a real-time part-based visual object tracker is proposed to handle partial occlusion and other challenging factors.</p><p>The key idea is to use the correlation filters <ref type="bibr" target="#b1">[2]</ref> as part classifiers, and a structural constraint mask to handle various appearance changes. In contrast, the proposed method can shepherd the sampled particles toward the modes of the target state distribution as shown in Figure <ref type="figure" target="#fig_0">2</ref>, and does not need to draw particles densely as in <ref type="bibr" target="#b28">[29]</ref> to maintain multiple modes.</p><p>Particle Filters: Particle filters or Sequential Monte Carlo (SMC) methods, developed based on the Monte Carlo methodologies, have been widely used in visual tracking in <ref type="bibr" target="#b41">[42]</ref>. To guarantee the robustness of particle filters, particle sampling must be sufficient to capture the variations in the state space. A dense sampling of particles brings high computation load, and it conflicts with the low cost demand of real-time visual tracking. Consequently, numerous techniques have been presented to improve the sampling efficiency of particle filtering <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Importance sampling <ref type="bibr" target="#b35">[36]</ref> is introduced to obtain better proposal by combining prediction based on the previous configuration with additional knowledge from auxiliary measurements. In <ref type="bibr" target="#b42">[43]</ref>, the observation likelihood is computed in a coarse-to-fine manner, which allows the computation to quickly focus on the more promising regions. In <ref type="bibr" target="#b43">[44]</ref>, subspace representations are adopted in the particle filter, which is made efficiently by applying Rao-Blackwellization to the subspace coefficients in the state vector. In <ref type="bibr" target="#b44">[45]</ref>, the number of particle samples is adjusted according to an adaptive noise component. Different from the existing methods, we adopt correlation filter to shepherd particles toward the modes of the target state distribution and reduce the number of particles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CORRELATION PARTICLE FILTER TRACKING</head><p>To effectively incorporate object appearance variations over time, rather than explicitly modeling object appearance by one correlation filter as existing methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b24">[25]</ref>, we model object appearance variation as a mixture of correlation filters. We first introduce the use of a mixture of correlation filters and discuss the correlation particle filter model in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mixture of Correlation Filters</head><p>The basic idea of the KCF tracker <ref type="bibr" target="#b1">[2]</ref> is that many negative samples are employed to enhance the discriminative ability of the track-by-detector scheme while exploring the structure of the circulant matrix for the high efficiency. The KCF tracker models the appearance of a target object using a correlation filter w trained on an image patch x of P × Q pixels, where all the circular shifts of x p,q , (p, q) ∈ {0, 1, . . . , P -1} × {0, 1, . . . , Q -1}, are generated as training samples with a Gaussian function label r p,q . The goal is to find the optimal weights w, w = arg min</p><formula xml:id="formula_0">w p,q | φ(x p,q ), w -r p,q | 2 + λ w 2 ,<label>(1)</label></formula><p>where φ denotes the mapping to a kernel space and λ is a regularization parameter. Using the fast Fourier transformation (FFT) to compute the correlation, this objective function is minimized as w = p,q α(p, q)φ(x p,q ), and the coefficient α is calculated as</p><formula xml:id="formula_1">α = F -1 ( F(r) F( φ(x), φ(x) ) + λ ),<label>(2)</label></formula><p>where r = {r(p, q)}, F and F -1 denote the Fourier transform and its inverse, respectively. Given the learned α and target appearance model x, the tracking task is carried out on an image patch u in the new frame with the search window size P × Q by computing the response map as</p><formula xml:id="formula_2">r = F -1 (F(α) F( φ(u), φ(x) )),<label>(3)</label></formula><p>where is the Hadamard product. Then, the new position of the target is detected by searching for the location of the maximal value of r.</p><p>The KCF tracker only uses one correlation filter w for object appearance modeling. In tracking, object appearance may have significant changes over time because of a number of factors such as illumination and pose changes. To effectively model object appearance distribution, we propose to use a mixture of correlation filters. Here, we adopt K correlation filters to handle appearance variation, and the proposed model consists of the learned target appearance xk and the classifier coefficients α k , k = 1, . . . , K. Note that, if we simply combine the K correlation filters with the same weights, their importance may be unfairly emphasized. Therefore, we have the weight π k to emphasize the importance of each correlation filter. The proposed mixture correlation filter at time t is updated as in <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_3">F(α k ) t = (1 -η)F(α k ) t-1 + ηF(α k ) F(x k ) t = (1 -η)F(x k ) t-1 + ηF(x k ) π t k = (1 -η)π t k + ηπ k .<label>(4)</label></formula><p>Here, the index k means that the k-th correlation filter has the maximum response among the K filters at time t; η is a learning rate parameter; and α k as well as xk are updated by taking the current values into account, and the π k is the maximal response value of the k-th correlation filter. After each update, the weights π k are normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Correlation Particle Filter Model</head><p>The proposed correlation particle filter is based on a Bayesian sequential importance sampling technique, which recursively approximates the posterior distribution using a finite set of weighted samples for estimating the posterior distribution of state variables characterizing a dynamic system. It provides a convenient framework for estimating and propagating the posterior probability density function of state variables regardless of the underlying distribution through a sequence of prediction and update steps. Let s t and y t denote the state variable describing the parameters of an object at time t (e.g. location and scale) and its observation respectively. The problem consists in calculating the posterior probability density function p(s t |y 1:t-1 ) at each time instant t. This posterior density function can be obtained recursively in two steps, namely prediction and update. The prediction stage uses the probabilistic system transition model p(s t |s t-1 ) to predict the posterior distribution of s t given all available observations y 1:t-1 = {y 1 , y 2 , • • • , y t-1 } up to time t -1, and is recursively computed as p(s t |y 1:t-1 ) = p(s t |s t-1 )p(s t-1 |y 1:t-1 )ds t-1 , (5) where p(s t-1 |y 1:t-1 ) is known from the previous iteration. When the observation y t is available, it is possible to perform the update step using the Bayes' rule as in <ref type="bibr" target="#b5">(6)</ref> </p><formula xml:id="formula_4">w i t δ(s t -s i t ),<label>(7)</label></formula><p>where w i t are the importance weights associated to the particles and are calculated as</p><formula xml:id="formula_5">w i t ∝ w i t-1 p(y t |s i t )p(s i t |s i t-1 ) q(s i t |s i t-1 , y t ) ,<label>(8)</label></formula><p>where q(•) is the importance density function which is chosen to be p(s t |s i t-1 ) and this leads to w i t ∝ w i t-1 p(y t |s i t ). Then, a re-sampling algorithm is applied to avoid the degeneracy problem <ref type="bibr" target="#b34">[35]</ref>. In this case, the weights are set to w i t-1 = 1/n ∀i. Therefore, we can rewrite the importance weights in <ref type="bibr" target="#b8">(9)</ref>, which are proportional to the likelihood function p(y t |s i t ),</p><formula xml:id="formula_6">w i t ∝ p(y t |s i t ).<label>(9)</label></formula><p>The above re-sampling step derives the particles depending on the weights of the previous step, then all the new particles receive a starting weight equal to 1/n which will be updated by the next frame likelihood function. With mixture correlation filter, each particle can be shepherded toward the local modes of the target object by use of its circular shift information. For simplicity, we define the mixture correlation filter operator for state calculation S mcf : R d → R d , where d is the state space dimensionality. Note that S mcf is a d-dimension operator, but in the proposed algorithm it operates on the bidimensional position sub-space only. Each S mcf procedure guides the generated particle over the subspace independently from all the others. The S mcf (s i t ) is the new state of particle s i t , which is obtained via the mixture correlation filters. We define the response of the mixture correlation filter for particle</p><formula xml:id="formula_7">s i t as R mcf (s i t ) by R mcf (s i t ) = k π k F -1 (F(α k ) F( φ(y i t ), φ(x) ),<label>(10)</label></formula><p>where y i t is the observation of particle i. Based on the above response, we define p(y t |s i t ) = R mcf (s i t ), and the particle weights are proportional to the response of mixture correlation filter as</p><formula xml:id="formula_8">w i t ∝ R mcf (s i t ).<label>(11)</label></formula><p>After the proposed mixture correlation filter analysis, the state of each particle is shifted s i t → S mcf (s i t ). The best state of target object is estimated as</p><formula xml:id="formula_9">E[s t |y 1:t ] ≈ n i=1 w i t S mcf (s i t ).<label>(12)</label></formula><p>As shown in Algorithm 1, we divide the proposed tracker into four main steps. The first step generates particles using the transition model p(s t |s t-1 ) and re-samples them. The second step applies the proposed mixture correlation filter to each particle to make it have shifted and reached a stable position. The third step updates the weights using the responses of the mixture correlation filter. Finally, the fourth step calculates the Algorithm 1: The proposed correlation particle filter tracking algorithm.</p><p>Input : Image Sequences and Initialization.</p><p>Output: Tracking Results s t ∀t. 1 for each frame do 2 Generate particles using the transition model p(s t |s t-1 ) and re-sample them.</p><p>3 Shift particles with a mixture correlation filter s i t → S mcf (s i t ).</p><p>4</p><p>Update particle importance weights using <ref type="bibr" target="#b10">(11)</ref>.</p><p>5</p><p>Predict target object state using <ref type="bibr" target="#b11">(12)</ref>. 6 end weighted average to obtain the best state as in <ref type="bibr" target="#b11">(12)</ref>. Note that, the proposed correlation particle filter is different from the traditional particle filter, because it operates on particles that are close to the local maxima. Therefore, the approximation of ( <ref type="formula" target="#formula_9">12</ref>) is performed using particles better representing the state space than those generated by the traditional particle filter. For this reason, these particles are more efficient than those in the traditional particle filter. Moreover, as particles are drawn at different scales, the proposed model can effectively handle scale variation. The proposed CPF algorithm is more than a simple running mixture correlation filter for multiple particles since the particles are filtered and selected depending on their likelihood. This feature gives to the proposed CPF the possibility to treat multi-modal posterior density functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we first introduce an experimental setup. We then show the effect of particle numbers on tracking performance. Finally, we provide extensive comparisons with state-of-the-art trackers. Our algorithm is implemented in Matlab without optimization and runs at 2.5 frames per second on a 2.80 GHz Intel Core2 Duo machine with 16 GB RAM. We will make the source code available to the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>Features and Parameters: We use HOG features for image representation as in the KCF tracker <ref type="bibr" target="#b1">[2]</ref>. In the proposed mixture correlation filter, K is set to 3, η is set to 0.02, π k is initialized as 1/K and updated over time, and all the other parameters are set to the same values as the KCF tracker. We use the same parameter values for all the experiments. Moreover, all the parameter settings are available in the source code to be released for accessible reproducible research. Datasets and Evaluation Metrics: We evaluate the proposed method on the tracking benchmark <ref type="bibr" target="#b2">[3]</ref> that includes 50 videos with comparisons to state-of-the-art trackers. The performance of our approach is quantitatively validated by three metrics used in <ref type="bibr" target="#b2">[3]</ref> including center location error (CLE), distance precision (DP), and overlap precision (OP). The CLE is computed as the average Euclidean distance between the ground-truth and the estimated center location of the target. The DP is computed as the relative number of frames in the sequence where the center location error is smaller than a certain threshold. As in <ref type="bibr" target="#b2">[3]</ref>, the DP values at a threshold of 20 pixels are reported. The OP is defined as the percentage of frames where the bounding box overlap surpasses a threshold of 0.5, which correspond to the PASCAL evaluation criterion. We report results using median CLE, DP and OP over all 50 sequences. In addition, the results are also presented using precision and success plots <ref type="bibr" target="#b2">[3]</ref>. In the legend, we report the average distance precision score at 20 pixels for each method. The average overlap precision is plotted in the success plot. We report the results at a threshold of 0.5, which corresponds to the PASCAL evaluation criteria. The area under the curve (AUC) is included in the legend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of Particle Number on Visual Tracking</head><p>In this section, we evaluate how the particle numbers effect the tracking performance as shown in Table <ref type="table" target="#tab_2">II</ref>. The proposed CPF tracker is evaluated with different particle numbers on the 50 benchmark sequences, and the success rate and precision scores are shown for each testing. Moreover, we also report the speed of the trackers in average frames per second (FPS) over all the 50 sequences. Based on the results, it is clear that increasing the number of particles can improve tracking performance. However, the tracker becomes slower. To balance the accuracy and efficiency, we use 40 particles through all the experimental evaluations.</p><p>In the benchmark <ref type="bibr" target="#b2">[3]</ref>, there are several particle filters based sparse trackers, which have achieved the state-of-theart performances, including L1APG <ref type="bibr" target="#b50">[51]</ref>, ASLA <ref type="bibr" target="#b51">[52]</ref>, and MTT <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b52">[53]</ref>. The details of the 3 trackers in the benchmark evaluation can be found in <ref type="bibr" target="#b2">[3]</ref>. Since randomization is involved in all the above trackers, we repeat the experiments 5 times using the same initial initializations to evaluate the effects of randomization and report the average and standard deviation of tracking accuracy. The average distance precision and overlap success rate of these trackers are L1APG (48.1%, 37.8%), ASLA (60.2%, 47.5%), and MTT (47.8%, 37.2%), respectively. In addition, the FPS are 2.4, 4.5, and 1.0. Compared with these trackers, the proposed CPF method improves the tracking performance significantly with comparable FPS. For the proposed CPF method with 10 particles, both the speed and the accuracy are better than these particle filter based trackers <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Note that, in these trackers <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b52">[53]</ref>, hundreds of particles are used. These results show that the correlation filter can enhance and complement the particle filter, and help cover target object states well. Even with a fewer number of particles, the proposed CPF method can achieve better object tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mixture Correlation Filters vs Particle Filters</head><p>Both mixture correlation filters and particle filters can improve object tracking performance. As in Figure <ref type="figure" target="#fig_1">3</ref>, we show the experimental results with different number of correlation filters (denoted as CPF-k, k=1, 2, 3, 4). Here, the CPF-k method uses particle filtering with k correlation filters. We also add another tracker CPF-SP, which uses a mixture of TABLE I: Comparison with state-of-the-art trackers on the 50 benchmark sequences. Our approach performs favorably against existing methods in median overlap precision (OP) (%) at an overlap threshold 0.5, median distance precision (DP) (%) at a threshold of 20 pixels and median center location error (CLE) (in pixels). The top rank 2 values are highlighted by bold with red and blue colors, respectively.   correlation filters (k = 3) without particle filtering. The CPF-SP method is based on the KCF tracker with a mixture of correlation filters (k = 3), and the CPF-1 method is based on the KCF tracker with particle filtering strategy.</p><p>From the ablation study, we have the following observations. (1) Mixture correlation filters are useful: The tracking performances of the CPF-k (k=1, 2, 3, 4) are improved with increasing number of correlation filters. Compared with CPF-1, CPF-4 achieves about 3.6% and 2.2% improvement on both the distance precision and overlap success rate. Furthermore, compared with KCF, CPF-SP achieves about 0.3% and 0.7% improvement on precision and overlap metrics. These results show that the mixture correlation filters can improve tracking performance and the improvement of the mixture correlation Fig. <ref type="figure">4</ref>: Precision and success plots over all the sequences using one-pass evaluation (OPE). The legend contains the area-under-the-curve score for each tracker. Among all the trackers, the proposed CPF method achieves the second best results on both the distance precision and overlap success rate. Compared with the MUSTer, the proposed CPF tracker achieves comparable results. filters with particle filters (CPF-k) is higher than the mixture correlation filters without particle filters (CPF-SP). (2) Particle filters are useful: Compared with CPF-SP, CPF-3 achieves much better performance with about 10.0% and 10.6% improvement on precision and overlap metrics. Compared with KCF, CPF-1 has about 6.8% and 9.1% improvement. These results show particle filters can complement correlation filters and significantly improve tracking performance. The above results show that particle filters are more important than mixture correlation filters for improving tracking performance. Let's take the trackers KCF, CPF-SP, and CPF-1 as an example. The CPF-SP and CPF-1 adopt a mixture of correlation filter (k = 3) and particle filtering strategy to improve the base tracker KCF, respectively. Compared with the KCF tracker, the CPF-SP achieves about 0.3% and 0.7% improvement on precision and overlap metrics. However, the CPF-1 has about 6.8% and 9.1% improvement on these two metrics. These results agree with the empirical tracking results that KCF are less effective in handling scale variations and occlusions, which can be effectively dealt with particle filters by maintaining multiple hypotheses. As such, we propose CPF to exploit and complement the strength of each one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with State-of-the-Art</head><p>For more thorough evaluations, we test the proposed algorithm on the benchmark with comparisons to 34 trackers including 29 trackers in <ref type="bibr" target="#b2">[3]</ref>, and other 5 recently published stateof-the-art trackers with their shared source code: MEEM <ref type="bibr" target="#b21">[22]</ref>, TGPR <ref type="bibr" target="#b48">[49]</ref>, RPT <ref type="bibr" target="#b30">[31]</ref>, MUSTer <ref type="bibr" target="#b26">[27]</ref>, DSST <ref type="bibr" target="#b0">[1]</ref>. The details of the 29 trackers in the benchmark evaluation can be found 1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2781304, IEEE Transactions on Image Processing in <ref type="bibr" target="#b2">[3]</ref>. We present the results using median OP, DP, and CLE over all sequences in Table <ref type="table">I</ref>, and report the results in one-pass evaluation (OPE) using the distance precision and overlap success rate in Figure <ref type="figure">4</ref>. The attribute-based evaluation is shown in Figure <ref type="figure">5</ref>. Furthermore, we compare the center location error and the overlap score frame-by-frame in Figure <ref type="figure" target="#fig_2">6</ref> and Figure <ref type="figure" target="#fig_3">7</ref>. Finally, we show the qualitative comparison on several video sequences in Figure <ref type="figure" target="#fig_4">8</ref>.</p><p>Table <ref type="table">I</ref> shows that our algorithm performs favorably against state-of-the-art methods. Among the trackers in the literature, the MUSTer and MEEM methods achieve the first best results with median OP of 94.8%, median DP of 97.9% and median CLE of 6.9 pixels, and the second best results with median OP of 83.8%, median DP of 96.5%, and median CLE of 7.5 pixels, respectively. The proposed algorithm preforms well with median OP of 94.9%, median DP of 97.9%, and median CLE of 5.3 pixels. These results show that the proposed CPF tracker has slightly better tracking performance than the MUSTer and MEEM trackers. Overall, the proposed method achieves significant improvement than other existing trackers. The details are as follows. (1) The Struck, TLD, and SCM trackers are the top 3 methods among the 29 trackers in the benchmark evaluation <ref type="bibr" target="#b2">[3]</ref>. The proposed tracking algorithm performs well against the (by 37.4%, 22.8%, TLD (by 43.2%, 36.2%, 22.2), and SCM (by 14.2%, 16.6%, 7.7) methods in terms of median OP, DP, and CLE, respectively.</p><p>(2) Compared with the correlation filter trackers CSK, KCF, and DSST, the proposed CPF method performs well against the CSK (by 58.1%), DSST (by 16%) and KCF (by 25.7%) methods in terms of median OP, and achieves performance gain of 41.2%, 6.1%, and 10.2% in terms of median DP. In terms of average CLE, the proposed CPF method has about 20.3 pixels, 5.8 pixels, and 6.6 pixels improvement.</p><p>Figure <ref type="figure">4</ref> shows the precision and success plots illustrating the mean distance and overlap precision over all the 50 sequences. In both precision and success plots, our approach shows comparable results compared to the MUSTer and significantly outperforms the existing correlation filter methods (DSST and KCF). For example, the proposed CPF method performs well against the DSST (by 7.3%) and KCF (by 11.3%) methods in terms of success rate. In addition, the proposed tracking algorithm performs well against the DSST (by 10.6%) and KCF (by 10.3%) methods in terms of precision. These results show that particle filter strategy can significantly improve the tracking performances of the correlation filter based trackers. Moreover, in the VOT challenge <ref type="bibr" target="#b33">[34]</ref>, the DSST <ref type="bibr" target="#b0">[1]</ref> and KCF <ref type="bibr" target="#b1">[2]</ref> trackers achieve much better performances than existing trackers including particle filters based methods. However, the proposed CPF tracker achieves much better tracking performance than the DSST and KCF trackers. It shows that correlation filter strategy can also improve particle filter based trackers. In summary, the precision and success plots demonstrate that our approach performs well against the existing methods.</p><p>In Figure <ref type="figure">5</ref>, we show the tracking performance based on attributes of image sequences <ref type="bibr" target="#b2">[3]</ref>, which annotates 11 attributes to describe the different challenges in the tracking problem, e.g., scale variation, out of view, occlusion, deformation. These attributes are useful for analyzing the performance of trackers in different aspects. Here, we present the success plots of OPE for the 11 attributes in Figure <ref type="figure">5</ref>. For presentation clarity, we present the top 10 performing methods in each plot. We note that the proposed tracking method performs well in dealing with challenging factors including scale variation, occlusion, deformation, fast motion, and out of view. As shown in Figure <ref type="figure">5</ref>, the KCF tracker does not handle scale variation well, which achieves worse performances than the DSST and CPF methods. The proposed CPF performs better than the DSST tracker by 5.5% in terms of success rate. These results show that the proposed CPF tracker can handle scale variation well. For the occlusion attribute, the proposed CPF achieves the second best among all the trackers, and much better than the DSST and KCF trackers, which demonstrates that the proposed CPF method can handle occlusion problem much better than the two correlation filter based trackers. In details, the proposed CPF performs better than the DSST tracker by 6.5% in terms of success rate. Overall, these results demonstrate that the particle sampling strategy can improve correlation filter based trackers well in handling scale variation and partial occlusion.</p><p>We show the center location error and the overlap score frame-by-frame on the 12 sequences in Figure <ref type="figure" target="#fig_2">6</ref> and Figure <ref type="figure" target="#fig_3">7</ref>, respectively. The average center location errors and the overlap scores of the top 10 trackers for each image sequence are shown in the legend. These results show our method performs well against the state-of-the-art trackers. Moreover, we compare our algorithm with the top 9 existing trackers in our evaluation (TGPR <ref type="bibr" target="#b48">[49]</ref>, SCM <ref type="bibr" target="#b49">[50]</ref>, Struck <ref type="bibr" target="#b20">[21]</ref>, RPT <ref type="bibr" target="#b30">[31]</ref>, KCF <ref type="bibr" target="#b1">[2]</ref>, MEEM <ref type="bibr" target="#b21">[22]</ref>, DSST <ref type="bibr" target="#b0">[1]</ref>, MUSTer <ref type="bibr" target="#b26">[27]</ref>, and TLD <ref type="bibr" target="#b19">[20]</ref>) on the 12 challenging sequences in Figure <ref type="figure" target="#fig_4">8</ref>. Overall, these trackers perform well, but the existing trackers have the following issues: The TGPR tracker does not perform well in partial occlusion (jogging-1, suv) and fast motion (couple). The SCM method is not robust for fast motion (jumping, couple), and the Struck scheme does not handle partial occlusion well (jogging-1, jogging-2, and suv). On the other hand, the RPT approach does not perform well in scale variation (singer1, walking2. The correlation filter based trackers KCF and DSST drift when target objects undergo heavy occlusion (jogging-1, jogging-2) and fast motion (couple, jumping). The MEEM tracker does not handle partial occlusion well (suv, walking2, and jogging-2). We note that the MUSTer tacker drifts off the target object objects when fast motion occurs (tiger1, couple). The TLD method does not track targets undergoing significant deformation and fast motion (tiger1 and shaking), and scale variation (singer1). Overall, our tracker performs well in tracking objects on these challenging sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion</head><p>The above results clearly demonstrate the effectiveness and efficiency of the proposed CPF tracker, and correlation filters and particle filters can enhance and complement each other well. Here, we highlight the following conclusions.</p><p>(1) Particle filters based trackers need to densely sample particles to cover target object states as shown in Figure <ref type="figure" target="#fig_0">2</ref>(a) Fig. <ref type="figure">5</ref>: Success plots over 11 tracking challenges of scale variation, out of view, out-of-plane rotation, low resolution, in-plane rotation, illumination, motion blur, background clutter, occlusion, deformation, and fast motion. The legend contains the AUC score for each tracker. Our CPF method performs favorably against the state-of-the-art trackers. and have high computational cost, which can be improved by correlation filters. Correlation filters can particles to cover target object states, thus, effectively reducing the number of particles needed for accurate tracking. This involves two strategies: search region padding and particle refinement. As shown in Figure <ref type="figure" target="#fig_5">9</ref> (a), for a particle (denoted in red bounding box), its search region (denoted in red bounding box with dashed line) is two times the size of the particle and determines the total number of possible circulant shifts of correlation filters. Even though this particle cannot cover the target object (denoted in green bounding box), its search region can do well. With the correlation filter, the particle can be shepherded toward the modes of the target state distribution. As a result, though the sampled particles do not cover target object states well as shown in Figure <ref type="figure" target="#fig_0">2</ref>(a), they will be shepherded toward the modes of the target state distribution to obtain much better object states as shown in Figure <ref type="figure" target="#fig_0">2(b)</ref>. Therefore, we do not need to draw particles densely to maintain multiple modes, be- cause particles are refined by moving to local maxima actively after correlation filters analysis. Thanks to correlation filters, we are able to not only maintain multiple modes to cover object state well using fewer particles in comparison to the conventional particle filter, but also reduce the computational load of a particle-based tracking method depending heavily on the number of drawn particles.</p><p>(2) Correlation filters based trackers cannot deal with scale variation and partial occlusion, which can be improved by particle filters. Scale Variation Handling: Particle filters adopt dense sampling technique to cover the states for target object undergoing large-scale variation. Therefore, particle filters can effectively help correlation filter handle scale variation, which has been demonstrated in Figure <ref type="figure">5</ref> over tracking challenge of scale variation. Occlusion Handling: Particle filters can maintain multiple hypotheses to allow the tracker to handle clutters in the background, partial and total occlusions, and recovery from failure or temporary distraction. Therefore, particle filters can effectively help correlation filter handle occlusion problem, which has been demonstrated in Figure <ref type="figure">5</ref> over tracking challenge of occlusion. As shown in Figure <ref type="figure" target="#fig_5">9</ref>(b), for correlation filters based trackers, if their search region is only the search region of particle i, these trackers are prone to drift because target object is outside of the search region. However, particle filters can maintain multiple hypotheses with multiple particles, and recovery from failure by using the search region of particle j.</p><p>(3) The proposed CPF has the advantages of both particle filters and correlation filters, e.g., robustness to partial occlusion as well as scale variation, and computational efficiency. (a) As shown in Figure <ref type="figure">5</ref> for scale variation attribute, the proposed CPF tracker is robust to partial and total occlusions, and can recover from lost tracks by maintaining multiple hypotheses. (b) As shown in Figure <ref type="figure">5</ref> for occlusion attribute, the proposed CPF tracker can effectively overcome the scale variation problem via a particle sampling strategy as in traditional particle filter. (c) As shown in Table <ref type="table" target="#tab_2">II</ref>, our CPF tracker can efficiently maintain multiple modes in the posterior density using fewer particles than conventional particle filters do, resulting in low computational cost. (d) As shown in Figure <ref type="figure">4</ref>, our CPF tracker has robust tracking performance overall and achieves much better performance than correlation filters based trackers and particle filters based trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a novel correlation particle filter for robust visual tracking. The proposed correlation particle filter can effectively handle partial occlusion and scale variation, and efficiently maintain multiple modes in the posterior density using fewer particles than conventional particle filters. Moreover, it can shepherd particles toward the modes of the target state distribution to obtain robust tracking performance.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: A correlation filter can be used to shepherd particles toward the modes of the target state distribution. The numbers in (b) are the output scores of correlation filter for the particles. Different colored boxes are used to indicate the respective location and scores.</figDesc><graphic coords="2,51.41,58.12,120.51,89.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Precision and success plots over all the 50 sequences [36] using one-pass evaluation (OPE). The legend contains the area-under-the-curve score for each tracker. Here, the CPF-k method uses particle filtering with k correlation filters, and the tracker CPF-SP uses a mixture of correlation filters (k = 3) without particle filtering. These results show that correlation filters and particle filters can enhance and complement each other. Please see the text for more details.</figDesc><graphic coords="6,51.11,342.38,246.74,112.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Fame-by-frame comparison of center location errors (in pixels) on 12 challenging sequences. The average center location errors of the top 10 trackers (highlighted in different colors) for each image sequence are shown in the legend. Generally, our method is able to track targets accurately and stably.</figDesc><graphic coords="9,48.96,53.14,514.06,316.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Fame-by-frame comparison of overlap scores on 12 challenging sequences. The average overlap scores of the top 10 trackers (highlighted in different colors) for each image sequence are shown in the legend. Generally, our method is able to track targets accurately and stably.</figDesc><graphic coords="10,48.96,53.14,514.06,313.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Tracking results of the top 10 trackers (denoted in different colors and lines) in our evaluation on 12 challenging sequences (from left to right and top to down are couple, jogging-1, jumping, liquor, shaking, singer1, skating1, jogging-2, singer2, tiger1, suv, walking2.).</figDesc><graphic coords="11,48.96,53.14,514.05,304.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Correlation filters and particle filters can enhance and complement each other. (a) Correlation filters can shepherd particles toward toward the modes of the target state distribution and make particle filters cover object state well using fewer particles. (b) Particle filters can improve correlation filters by handling scale variation, and make correlation filters maintain multiple hypotheses with multiple particles for occlusion handling.</figDesc><graphic coords="11,51.11,418.07,246.76,113.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Effect of particle numbers on visual tracking performance. The results are evaluated on the 50 sequences in comparison with particle filters based sparse trackers (L1APG<ref type="bibr" target="#b50">[51]</ref>, ASLA<ref type="bibr" target="#b51">[52]</ref>, and MTT<ref type="bibr" target="#b52">[53]</ref>). Here, the success rate, precision, and FPS are shown with different particle numbers. Increasing the number of particles can improve tracking performance. However, the tracker becomes slower.</figDesc><table><row><cell>Method</cell><cell># Particles</cell><cell>Success Rate</cell><cell>Precision</cell><cell>FPS</cell></row><row><cell></cell><cell>10</cell><cell>58.5 ± 0.83</cell><cell>79.4 ± 0.82</cell><cell>8.3</cell></row><row><cell>CPF</cell><cell>30 40</cell><cell>60.9 ± 0.81 62.6 ± 0.81</cell><cell>81.6 ± 0.81 84.4 ± 0.82</cell><cell>2.8 2.5</cell></row><row><cell></cell><cell>50</cell><cell>62.8 ± 0.82</cell><cell>84.6 ± 0.80</cell><cell>1.9</cell></row><row><cell>L1APG</cell><cell>600</cell><cell>37.8 ± 0</cell><cell>48.1 ± 0</cell><cell>2.4</cell></row><row><cell>ASLA</cell><cell>600</cell><cell>47.5 ± 0.66</cell><cell>60.2 ± 1.26</cell><cell>4.5</cell></row><row><cell>MTT</cell><cell>600</cell><cell>37.2 ± 0.63</cell><cell>47.8 ± 0.76</cell><cell>1.0</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61432019, Grant 61572498, Grant 61532009, Grant 61572493, Grant U1536203, Beijing Natural Science Foundation (4172062), and Key Research Program of Frontier Sciences, CAS, Grant NO. QYZDJ-SSW-JSC039. Tianzhu Zhang and Changsheng Xu are with National Lab of Pattern</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2007">2014. 1, 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>1, 2, 3, 5</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2013. 1, 3, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eigentracking: Robust matching and tracking of articulated objects using a view-based representation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="63" to="84" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Partial occlusion handling for visual tracking via robust part matching</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel-Based Object Tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="575" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust on-line appearance models for visual tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>El-Maraghi</surname></persName>
		</author>
		<idno>1296-1311. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust multi-object tracking via crossdomain contextual information for sports video analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual tracking decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust Visual Tracking via Consistent Low-Rank Sparse Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno>171-190. 1</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust Visual Tracking and Vehicle Classification via Sparse Representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2259" to="2272" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Low-rank sparse learning for robust visual tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust fragments-based tracking using the integral histogram</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="798" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Structural sparse tracking</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object tracking by occlusion detection via structured sparse learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVsports workshop in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On-line selection of discriminative tracking features</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="346" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ensemble tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-Time Tracking via On-line Boosting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tracking-learning-detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno>1409-1422. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MEEM: Robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast visual tracking via dense spatio-temporal context learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2012. 2, 3, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive color attributes for real-time visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-store tracker (muster): A cognitive psychology inspired approach to object tracking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time part-based visual tracking via adaptive correlation filters</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-task correlation particle filter for robust object tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reliable patch trackers: Robust visual tracking by exploiting reliable patches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structural correlation filter for robust visual tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">In defense of sparse tracking: Circulant sparse tracker</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Liris</surname></persName>
		</author>
		<title level="m">The visual object tracking vot2014 challenge results</title>
		<imprint>
			<date type="published" when="2007">2014. 2, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A tutorial on particle filters for online nonlinear/non-gaussian bayesian tracking</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Arulampalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maskell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Condensation -conditional density propagation for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>13. 3</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive appearance modeling for video tracking: Survey and evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4334" to="4348" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The visual object tracking vot 2013 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV2013 Workshops, Workshop on Visual Object Tracking Challenge</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Finding the best from the second bests -inhibiting subjective bias in evaluation of visual tracking algorithms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual tracking: an experimental survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>1442-1468. 3</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Condensation -conditional density propagation for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast multiple object tracking via a hierarchical particle filter</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A rao-blackwellized particle filter for eigentracking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visual tracking and recognition using appearance-adaptive models in particle filters</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<idno>1491-1506. 3</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Sequential monte carlo methods in practice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Locally orderless tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bar-Hillel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1940" to="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Distribution fields for tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1910" to="1917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Transfer learning based visual tracking with gaussian process regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust object tracking via sparsity-based collaborative model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Real time robust l 1 tracker using accelerated proximal gradient approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visual tracking via adaptive structural local sparse appearance model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Robust visual tracking via multitask sparse learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust visual tracking via structured multi-task sparse learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="383" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Currently, he is an Associate Professor at the Institute of Automation, Chinese Academy of Sciences. His current research interests include computer vision and multimedia, especially action recognition, object classification, object tracking, and social event analysis. Si Liu (M&apos;12) is an Associate</title>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006, and the Ph.D. degree in pattern recognition and intelligent systems from the Institute of Automation, Chinese Academy of Sciences</title>
		<meeting><address><addrLine>Beijing, China; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Professor in Institute of Information Engineering, Chinese Academy of Sciences ; and Computer Engineering, National University of Singapore. She obtained the Ph.D. degree from Institute of Automation, Chinese Academy of Sciences (CASIA</orgName>
		</respStmt>
	</monogr>
	<note>She used to be a Research Fellow in Learning and Vision Re-search Group at the Department of Electrical. Her current research interests include Object Categorization, Object Detection, Image Parsing and Human Pose Estimation</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">He has served as associate editor, guest editor, general chair, program chair, area/track chair, special session organizer, session chair and TPC member for over 20 IEEE and ACM prestigious multimedia journals, conferences and workshops</title>
	</analytic>
	<monogr>
		<title level="m">He received the Best Associate Editor Award of ACM Trans. on Multimedia Computing, Communications and Applications in 2012 and the Best Editorial Member Award of ACM/Springer Multimedia Systems Journal in 2008. He served as Program Chair of ACM Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Professor in National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences and Executive Director of China-Singapore Institute of Digital Media</orgName>
		</respStmt>
	</monogr>
	<note>He is IEEE Fellow, IAPR Fellow and ACM Distinguished Scientist</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
