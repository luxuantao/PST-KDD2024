<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Margin Transductive Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Brian</forename><surname>Quanz</surname></persName>
							<email>bquanz@ku.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information and Telecommunication Technology Center Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Kansas Lawrence</orgName>
								<address>
									<postCode>66045</postCode>
									<settlement>Kansas</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Huan</surname></persName>
							<email>jhuan@ku.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information and Telecommunication Technology Center Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Kansas Lawrence</orgName>
								<address>
									<postCode>66045</postCode>
									<settlement>Kansas</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large Margin Transductive Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D1C32F2D6D93E9FD1A09EF838D9FF150</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Applications]: Data Mining; I.5 [Pattern Recognition]: Models -Statistical Transfer learning</term>
					<term>large margin classifier</term>
					<term>transductive learning</term>
					<term>kernel method</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently there has been increasing interest in the problem of transfer learning, in which the typical assumption that training and testing data are drawn from identical distributions is relaxed. We specifically address the problem of transductive transfer learning in which we have access to labeled training data and unlabeled testing data potentially drawn from different, yet related distributions, and the goal is to leverage the labeled training data to learn a classifier to correctly predict data from the testing distribution. We have derived efficient algorithms for transductive transfer learning based on a novel viewpoint and the Support Vector Machine (SVM) paradigm, of a large margin hyperplane classifier in a feature space. We show that our method can out-perform some recent state-of-the-art approaches for transfer learning on several data sets, with the added benefits of model and data separation and the potential to leverage existing work on support vector machines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Constructing mining and learning algorithms for data that may not be identically and independently distributed (i.i.d.) is one of the emergent research topics in data mining and machine learning <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39]</ref>. Non-i.i.d. data occur naturally in applications such as cross-language text mining, bioinformatics, distributed sensor networks and sensor-based security <ref type="bibr" target="#b29">[29]</ref>, social network studies, low quality data mining <ref type="bibr" target="#b41">[41]</ref>, and ones found in multi-task learning <ref type="bibr" target="#b25">[25]</ref>. The key challenge of these applications is that accurately-labeled task-specific data are scarce while task-relevant data are abundant. Learning with non-i.i.d. data in such scenarios helps build accurate models by leveraging relevant data to perform new learning tasks, identifying the true connections among samples and their labels, and expediting the knowledge discovery process by simplifying the expensive data collection process.</p><p>Transfer learning aims to learn classification models with training and testing data sampled from possibly different distributions. The common assumption in transfer learning is that the training and testing data sets share a certain level of commonality and identifying such common structures is of key importance. For data that have well-separated structures, exploring the common cluster structure of training and testing sets is a widely used technique <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b37">37]</ref>. Instance based methods assume a common relationship between the class label and samples and use weighting or sampling strategies to correct differences between training and testing distributions <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b35">35]</ref>. In feature based methods, shared feature structure is learned in order to transfer knowledge in training data to testing data <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref>. In addition, Xue et al. used a hierarchical Bayesian model and developed a matrix stick-breaking process to learn shared prior information across a group of related tasks <ref type="bibr" target="#b39">[39]</ref>. From a multi-task learning framework, if we assume that the testing data is coming from a new task and that the new task belongs to a parameterized task family, we can learn the structure of such a parameterized task family and use that information for transfer learning, as demonstrated in the zero-data learning algorithm <ref type="bibr" target="#b25">[25]</ref>.</p><p>In this paper, we explore a research direction motivated by manifold regularization which assumes that data distribute on a low dimensional manifold embedded in a high dimensional space <ref type="bibr" target="#b3">[3]</ref>. The learning task is to find a low complexity decision function that well separates the data and that varies smoothly on the manifold. Following the same intuition, we approach the non-i.i.d. data learning problem by learning a decision function with low empirical error, regularized by the complexity of the function and the difference between training and testing data distributions, evaluated against the decision function. The idea is to in effect find a manifold for which the training and testing data distributions are brought together so that the labeled training data can be used to learn a model for the testing data. In particular, we aim to obtain a linear classifier, in a reproducing kernel Hilbert space, such that it achieves a trade-off between the large margin class separation and the minimization of training and testing distribution discrepancy, as projected along the linear classifier. Our hypothesis is that unlabeled testing data reveal information about testing data distribution and help build accurate classification models. Though large margin classifiers have been investigated in similar contexts including semi-supervised learning and transductive learning <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b36">36]</ref>, applying large margin classifiers to transfer learning by incorporating a regularization component measuring the distances between training and testing data is new and worth a careful investigation. We illustrate our hypothesis in Figure <ref type="figure" target="#fig_0">1</ref> where we show an artificial data set in a 2D space where training and testing data sets have different distributions. As shown in the figure, the support vector machine builds a decision boundary that fits the training data well. Clearly the decision boundary is not the optimal one as evaluated on the testing data set. Clustering based methods are widely used in designing transfer learning algorithms. In this example, there is no obvious clustering structure for the positive and negative samples and clustering based techniques will not be very helpful. Yet another class of widely used methods is ones that are based on feature extraction and feature selection. These methods will not be very useful since in this case we only have two features and both of them are important. The key observation, as illustrated in this example, is that we need to integrate feature weighting (in order to handle distribution mismatches between training and testing samples) and model selection in a unified framework.</p><p>The major advantage of adopting the regularized empirical error minimization paradigm such as the SVM is the potential to exploit many algorithms designed specifically for SVMs with only slight modifications, if any. For example, there have been fast algorithms designed for handling large data sets <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b24">24]</ref>, anomaly detection with one-class SVM, and multi-class SVM for multi-category classification. Other advantages are the rigorous mathematical foundation such as the Representer Theorem, global optimization with polynomial running time using convex optimization, and geometric interpretations through generalized singular value decomposition. We discuss these properties of SVM based transfer learning in detail in the Algorithmic study section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Notations and Problem Statement</head><p>In supervised learning, we aim to derive ("learn") a mapping for a sample x ∈ X to an output y ∈ Y. Towards that end we collect a set of n training samples Ds = {{ x1, y1}, . . . , { xn, yn}} sampled from X × Y following a (unknown) probability distribution P r( x, y). We also have a set of m testing samples Dt = { z1, . . . , zm} sampled from X following a (unknown) probability distribution P r ( x, y), where the corresponding outputs from Y are unavailable, or hidden, and must be predicted. We assume that Ds are i.i.d. sampled according to the distribution P r( x, y) and Dt are i.i.d. sampled according to the distribution P r ( x, y). In standard supervised learning, we assume that P r( x, y) = P r ( x, y). The problem of large margin transductive transfer learning is to learn a classifier that accurately predicts the outputs (class labels) for the unlabeled testing data set when P r( x, y) and P r ( x, y) are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>There are two main approaches to transfer learning that have been considered, inductive transfer learning, where a small number of labeled test data are used along with labeled training data <ref type="bibr" target="#b1">[1]</ref>, and transductive transfer learning, where a significant number of unlabeled testing samples are used along with the labeled training data. In this paper we focus on transductive transfer learning.</p><p>A common approach to transfer learning is a model-based approach in which the different distributions are incorporated in a model, e.g. through domain specific priors <ref type="bibr" target="#b11">[11]</ref> or through a model with general and domain-specific components <ref type="bibr" target="#b12">[12]</ref>. Several approaches have also been developed for transductive transfer learning which consider the local structure of the unlabeled data, utilizing some unsupervised learning methods, such as clustering <ref type="bibr" target="#b14">[14]</ref> or co-clustering <ref type="bibr" target="#b37">[37]</ref>. Our approach is most similar to feature-based approaches to transfer learning, which include such approaches as weighting features to find feature subsets <ref type="bibr" target="#b31">[31]</ref> or feature subspaces <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b28">28]</ref> that generalize well across distributions. The difference is that we do so in a regularization framework, which aims to avoid over fitting and minimize the generalization error. Another approach that is similar to ours is that of Bickel et al. <ref type="bibr" target="#b7">[7]</ref>. They address the problem of covariate shift through a likelihood model approach that takes into account the discrepancy between train and test distributions. However their method results in a logistic regression based classifier from a non-convex problem, whereas our approach results in an SVM classifier from a convex problem.</p><p>At the heart of our approach is the goal of finding a feature transform such that the distance between the testing and training data distributions, based on some distribution distance measure, is minimized, while at the same time maximizing a class distance or classification performance criterion for the training data. There has also been work describing how to measure the distance between distributions. A key idea is that the distance between two distributions can be measured with respect to how well they can be separated, given some function class. For instance, Ben-David et al. <ref type="bibr" target="#b4">[4]</ref> used as an example the class of hyperplane classifiers and showed that the performance of the hyperplane classifier that could best separate the data could provide a good method for measuring distribution distance for different data representations. Along these same lines, Gretton et al. <ref type="bibr" target="#b18">[18]</ref> showed that for a specific function class, the measure simplifies to a form that can be easily computed, the distance between the two means of the distributions, resulting in the maximum mean discrepancy (MMD) measure, which we use in this paper. The particular form of this measurement makes it easier to incorporate into optimization problems, and so we chose this formulation to estimate distribution distances.</p><p>All the methods cited previously, including transfer learning, are closely related to multi-task learning and may be viewed as a special case of semi-supervised learning where unlabeled data is used to enhance the learning of a decision function. The difference is that in transfer learning, there is an assumed bias between training and testing samples. A recent review of semi-supervised learning may be found in <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b40">40]</ref>. A discussion of possible sample bias, in a multi-task learning framework, may be found in <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b33">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Large Margin Classifier</head><p>Here we briefly discuss the formulation of the standard support vector machine (SVM), since it forms the basis for our transductive transfer support vector machine. Given ( x1, y1), . . . , ( xn, yn) ∈ X ×{±1} the supervised binary classification learning task is to learn a function f * ( x) for any x ∈ X that correctly predicts its corresponding class label y; of particular interest is generalization accuracy the accuracy of the function on predicting unseen future data. For hyperplane classifiers such as the SVM, the decision function is given by the function f * ( x) = sign(f ( x) + b), where f ( x) = w T x, and w controls the orientation of the hyperplane, and b the offset. For the separable case, in which the two classes of data can be separated by a hyperplane, the SVM method tries to find the hyperplane with the maximum margin of separation, where the margin is the distance to the hyperplane of a point closest to the hyperplane. For the non-separable case, the SVM method tries to identify the hyperplane with the maximal margin with slack variables called the soft-margin. It can be shown that selecting the hyperplane with the largest margin minimizes a bound on expected generalization error <ref type="bibr" target="#b36">[36]</ref>.</p><p>The binary soft-margin SVM formulation aims to learn a decision function f specified below:</p><formula xml:id="formula_0">f = arg min f ∈H K C n i=1 V ( xi, yi, f) + 1 2 ||f || 2 K (1)</formula><p>where If the decision function f is a linear function represented by a vector w, equation 1 can be represented as:</p><formula xml:id="formula_1">K( x, x ) : X × X → R</formula><formula xml:id="formula_2">min. 1 2 || w|| 2 + C n i=1 i s.t. i ≥ 0 yi( w T φ( xi) + b) ≥ 1 -i ∀i = 1, ..., n<label>(2)</label></formula><p>Where an unregularized bias term b is included and φ( xi) is the kernel feature vector of xi. Following common terminology (e.g. <ref type="bibr" target="#b32">[32]</ref>) we refer to this as the 1-norm soft margin SVM, and if squared slack variables are penalized instead, i.e. C n i=1 2 i , the 2-norm soft margin SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distribution Distance and MMD</head><p>For our formulation, it is necessary to choose a convenient distribution distance measure. One popular distribution "distance" measure is the Kullback-Leibler divergence, based on entropy calculations. However for our approach we need a nonparametric method suitable for a reproducing kernel Hilbert space (RKHS) that is both efficient to compute and relatively easy to incorporate into optimization problems while still allowing accurate distance measurement. One method that has recently been shown to be both efficient and effective for estimating the distance between two distributions in a reproducing kernel Hilbert space is the maximum mean discrepancy (MMD) measure <ref type="bibr" target="#b18">[18]</ref>. The measure derives from computing the distribution distance by finding the function from a given class of functions that can best separate the two distributions, with the function class restricted to a unit ball in the RKHS. Additionally the particular form of this measure fits quite well into our support vector formulation, as shown in Section 4.</p><p>Here we briefly overview the MMD measure for estimating the distance between two distributions. Given a set of n training samples D s = {{ x1, y1}, . . . , { xn, yn}} and a set of m testing samples Dt = { z1, . . . , zm}. The (squared) maximum mean discrepancy distance of the training and testing distributions is given by the following formula:</p><formula xml:id="formula_3">MMD 2 = || 1 n n i=1 φ( xi) -1 m m i=1 φ( zi)|| 2 = 1 n 2 n i,j=1 K( xi, xj) + 1 m 2 m i,j=1 K( zi, zj) -2 1 nm n,m i,j=1 K( xi, zj) (3)</formula><p>The MMD measure has also recently been used in the context of transfer learning, e.g. for kernel learning <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ALGORITHM</head><p>Our general approach is as follows. We want to find a feature transform that minimizes the between-distribution distance, but at the same time maximizes the performance of a classifier on data from the training distribution. The latter criterion could also be considered a distribution distance measure (along the lines of <ref type="bibr" target="#b4">[4]</ref>) in this case the distance between the distributions of the classes of the training data distribution. Thus in essence our general transfer learning approach is described with Equation <ref type="formula" target="#formula_4">4</ref>.</p><formula xml:id="formula_4">f = arg min f ∈H K C n i=1 V ( xi, yi, f) + 1 2 ||f || 2 K + λd f,K (P r, P r )<label>(4</label></formula><p>) where P r is the distribution of the training samples, P r the distribution of the testing samples, d f,K (P r, P r ) is a distance measure of the two distributions, as evaluated against the decision function f and the kernel function K. λ controls the trade-off between the three components in the objective function. Other symbols such as C, V, HK are the same as explained in Equation <ref type="formula">1</ref>.</p><p>Following convention, we only consider linear decision functions f in the format f ( x) = w T φ( x) where w is the direction vector of f . Also following convention, we introduce an unregularized bias term, b, so that the final function is given by f ( x) + b and the label is assigned as sign(f ( x) + b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Projected Distribution Distance</head><p>One approach we take to measure the distance between two distributions is to estimate how well the two distributions are separated as explored in the maximum mean discrepancy distance <ref type="bibr" target="#b18">[18]</ref>, mentioned previously. We define the projected maximum mean discrepancy distance measure, using a set of training samples Ds = {{ x1, y1}, . . . , { xn, yn}} and a set of m testing samples Dt = { z1, . . . , zm} below. Here we take the squared projected maximum mean discrepancy measure for our distribution distance measure, to estimate the distribution distance under a given projection w:</p><formula xml:id="formula_5">d f,K (P r, P r ) 2 = || 1 n n i=1 f ( xi) -1 m m j=1 f ( zj )|| 2 = 1 n 2 ( n i=1 w T φ( xj)) 2 + 1 m 2 ( m j=1 w T φ( zj)) 2 -2 1 nm n,m i,j=1 w T φ( xi) w T φ( zj) (5)</formula><p>With the given decision and distance functions, we can rewrite Equation 4 in vector format below:</p><formula xml:id="formula_6">min. 1 2 || w|| 2 + C n i=1 i + λd f,K (P r, P r ) 2 s.t. i ≥ 0, yi( w T φ( xi) + b) ≥ 1 -i ∀i = 1, ..., n<label>(6)</label></formula><p>where d f,K (P r, P r ) 2 is estimated using Equation <ref type="formula">5</ref>.</p><p>The major difficulty in solving Equation <ref type="formula" target="#formula_6">6</ref>is that w is a vector in the Hilbert space defined by the kernel function K and hence may have infinite dimensionality. The Representer Theorem, which states that any vector w that minimizes Equation 6 should be a linear combination of the kernel feature vectors of the training and testing samples, provides a useful remedy.</p><formula xml:id="formula_7">w = n i=1 βiφ( xi) + m j=1 β j φ( zj) (7)</formula><p>where βi and β j are coefficients and w is the vector that optimizes Equation <ref type="formula" target="#formula_6">6</ref>. For simplicity, we denote φ(S) = (φ( s1), . . . , φ( sn+m)) = (φ( x1), . . . , φ( xn), φ( z1), . . . , φ( zm)) is a list of kernel feature vectors for training and testing samples and β = (β1, . . . , βn, β 1 , . . . , β m ) T is a (n + m) column vector. Hence we have w = φ(S) β.</p><p>The key observation of the Representer Theorem is that if w has a component that is not in the span of column vectors in φ(S), that component must be orthogonal to the linear space spanned by the training and testing samples. In that case, the value of f , evaluated on training and testing samples will remain unchanged but the L2 norm of f will increase <ref type="bibr" target="#b3">[3]</ref>. The details of the formal proof in this case can be found in the appendix. With the Representer Theorem, we state our algorithm for large margin transductive transfer learning below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Large Margin Transductive Transfer Learning Algorithm</head><p>With the Representer Theorem, we learn the decision boundary without explicitly learning the vector w. We have the following observations.</p><formula xml:id="formula_8">|| w|| 2 = β T φ(S) T φ(S) β = β T Λ β (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where Λ is a (n + m) by (n + m) positive semi-definite matrix and Λ i,j = K(φ( si), φ( sj)). Our projected distribution distance measure can then be expressed as:</p><formula xml:id="formula_10">d f,K (P r, P r ) 2 = 1 n 2 ( n i=1 w T φ( xi)) 2 + 1 m 2 ( m j=1 w T φ( zj)) 2 -2 nm n,m i,j=1 w T φ( xi) w T φ( zj) = 1 n 2 n i,j=1 β T φ(S) T φ( xi) β T φ(S) T φ( xj)+ 1 m 2 m i,j=1 β T φ(S) T φ( zi) β T φ(S) T φ( zj)- 2 nm n,m i,j=1 β T φ(S) T φ( xi) β T φ(S) T φ( zj) = 1 n 2 β T [ n i,j=1 (φ(S) T φ( xi)φ( xj) T φ(S))] β+ 1 m 2 β T [ m i,j=1 φ(S) T φ( zi)φ( zj) T φ(S)] β- 2 nm β T [ n,m i,j=1 φ(S) T φ( xi)φ( zj) T φ(S)] β = 1 n 2 β T KTrain[1] n×n K T Train β + 1 m 2 β T KTest[1] m×m K T Test β -1 nm β T (KTrain[1] n×m K T Test + KTest[1] m×n K T Train ) β = β T Ω β (9)</formula><p>where Ω is a (n + m) × (n + m) symmetric positive semidefinite matrix. KTrain is the (n + m) × n kernel matrix for the training data, KTest the (n + m) × m kernel matrix for the testing data, and <ref type="bibr" target="#b1">[1]</ref> k×l is a k × l matrix of all ones.</p><p>With these two equations, Equation 6 is expressed using β in the following way:</p><formula xml:id="formula_11">min. β T ( 1 2 Λ + λΩ) β + C n i=1 i s.t. i ≥ 0 yi( β T Ki + b) ≥ 1 -i ∀i = 1, ..., n<label>(10)</label></formula><p>where Ki = φ(S) T φ( xi) is an (n + m) column vector.</p><p>It is easy to show that the optimization problem of Equation 10 has an objective with a quadratic form of β and is a standard convex quadratic program, and hence can be solved using quadratic program solvers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Regularization of the Hilbert space basis coefficients</head><p>We can view the problem of Equation 10 as performing regression in the Hilbert space with a hinge loss function and parameters β. Thus we propose adding an L2 penalty to the β parameters to shrink the selection of the data points used for the classifier and to add numerical stability to the algorithm in practical implementations -particularly with large matrices this can correct for slight negative eigenvalues from calculating Ω. Thus our final objective to minimize is:</p><formula xml:id="formula_12">β T ( 1 2 Λ + λΩ + λ2I) β + C n i=1 i, (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where I is the (n + m) × (n + m) identity matrix. In our experiments we found that generally a moderate amount of such L2 regularization improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Simplification with Linear Kernel, Linear Feature Weighting</head><p>Below we show a special case with linear kernels and a feature weighting as opposed to a projection for measuring the distribution distance and demonstrate that in this case our algorithm can be viewed as a processing technique, following by a regular SVM model construction. We arrive at this simplification if we consider the target projection w as representing a linear feature weighting transform W = diag( w) that does not project a data point but re-weights it, and measure the MMD with respect to the feature weighting introduced for a given w and the resulting W . With linear kernels, w is a vector in the original feature space, rather than in the kernel feature space, and the MMD measure under this linear transform is given by equation 12.</p><formula xml:id="formula_14">MMD 2 = ( 1 n n i=1 W xi -1 m m j=1 W zj) 2<label>(12)</label></formula><p>We can rearrange the MMD measure to sum across each feature:</p><formula xml:id="formula_15">MMD 2 = p k=1 w 2 k ( 1 n n i=1 x ik -1 m m j=1 z jk ) 2 = w T Q w (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>where p is the dimensionality of x and Q is a p × p diagonal matrix with</p><formula xml:id="formula_17">Q k,k = ( 1 n n i=1 x ik -1 m m j=1 z jk ) 2 for k ∈ [1, p].</formula><p>Plugging this back into our 1-norm soft-margin SVM formulation, we can combine the MMD 2 term with the maximum margin term, resulting in the objective:</p><formula xml:id="formula_18">min. 1 2 w T Q w + C n i=1 i (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>where I is a p × p identity matrix and Q = λQ + 1 2 I. We could derive a similar quadratic programming for computing w but it is unnecessary. The problem presented in Equation 14 can be solved using a pre-processing step, followed by any off-the-shelf SVM solver. To see this, notice that since Q is diagonal it can be expressed as</p><formula xml:id="formula_20">U T U with U = Q 1 2 so that w T Q w becomes w T U T U w = (U w) T (U w).</formula><p>Thus by defining w = U w and re-scaling the data by 1/U (i.e. x i = xi(1/U )), we obtain the standard SVM problem. To obtain w from the solution w * we simply divided by U . Note that we can incoporate nonlinearity in this case through basis expansion; we simply define the feature fj for a given x as the output of the kernel function between x and the data instance (from the training and testing sets) sj , j ∈ {1, . . . , n + m}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">2-Norm Soft Margin Transductive Transfer Learning with Generalized Singular Value Decomposition</head><p>In the previous sections, we discussed the SVM with 1norm soft margin for transductive transfer learning. In this section, we introduce a similar formalization for 2-norm soft margin transductive transfer learning that is equivalent for the case of the standard SVM, in which we fix the hyperplane norm || w|| and find the hyperplane direction that gives maximum separation, measured by γ. This formalization reveals a geometric interpretation for the regularization. We discuss the geometric interpretation using a technique known as generalized singular value decomposition (GSVD).</p><p>The 2-norm transductive transfer learning is an optimization problem specified below:</p><formula xml:id="formula_21">min. -γ + λ MMD 2 +C n i=1 2 i s.t. yi( w T xi + b) ≥ γ -i ∀i = 1, ..., n || w|| = 1<label>(15)</label></formula><p>With the Representer Theorem we have w = β T φ(S) where φ(S) = (φ( x1), . . . , φ( xn), φ( z1), . . . , φ( zm)).</p><p>Using the expression of MMD from Equation <ref type="formula">9</ref>and the L2 norm of w in Equation <ref type="formula" target="#formula_8">8</ref>, we have the following optimization problem:</p><formula xml:id="formula_22">min. -γ + λ β T Ω β + C n i=1 2 i s.t. yi( β T Ki + b) ≥ γ -i ∀i = 1, ..., n β T Λ β = 1<label>(16)</label></formula><p>The Lagrangian of Equation <ref type="formula" target="#formula_22">16</ref>is L(w, b, γ, α, λ, λ0) =</p><formula xml:id="formula_23">-1 4C n i=1 α 2 i - 1 4 αiyiK T i M -1 Kiyiαi -λ0 where M = λΩ + λ0Λ.</formula><p>Clearly, if the value of λ0 is known, the Lagrangian is a quadratic programming problem for α. The difficulty here is that we have to optimize two variables λ0 and α. In regular SVM with 2-norm soft margin, the optimal value of λ0 can be determined analytically once we know α and the optimization problem adopts the quadratic programming format. In transductive transfer learning, we do not have this convenience anymore. However, we may use a technique called generalized singular value decomposition to show the effect of the distribution distance measure Ω in the optimization.</p><p>For the kernel matrix Λ we obtain a matrix Γc such that K = Γ T c Γc. Similarly for the kernel matrix Ω we obtain a matrix Γ d such that K = Γ T d Γ d . Given two square-matrix Γc and Γ d with the same size, if we apply the generalized singular value decomposition we have Γc = U Σ1RQ T and Γ d = V Σ2RQ T where U, Q are orthogonal matrices and R is an upper-triangular matrix. Then we have the following formula:</p><formula xml:id="formula_24">M = λ0Λ + λΩ = λ0Γ T c Γc + λΓ T d Γ d = λ0QR T Σ 2 1 RQ T + λQR T Σ 2 2 RQ T = QR T (λ0Σ 2 1 + λΣ 2 2 )RQ T<label>(17)</label></formula><p>We have</p><formula xml:id="formula_25">M -1 = QR (-1) 1 (λ0Σ 2 1 + λΣ 2 2 )</formula><p>R (-1)T Q T . Hence M -1 is a shrinkage operator, penalizing smaller generalized singular values and the penalization is controlled by the two parameters λ0 and λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SYNTHETIC DATA EXPERIMENTS</head><p>Here we give a synthetic 2D example to illustrate our approach. The training data distribution is shown as the green dots or squares (for the negative class) and the black plus symbols (as the positive class), generated by sampling from Gaussians for each feature with σ 2 = 1, centered at (0, -2) and (2, 0) respectively. The testing distribution is generated in a similar fashion, designed to be similar to the training distribution particularly along one dimension, with the negative class, depicted with upside-down red triangles generated from a Gaussian distribution centered at (0, 2) and the positive class, depicted as blue circles, generated with a Gaussian centered at (2, 0).</p><p>The transductive support vector machine is a widely used method that handled to some extent the possible difference between training and testing data sets. The transductive SVM tries to minimize the decision function norm and the errors on both the training and testing data, taking the unknown labels as variables of the optimization problem, so that these labels must be solved for along with the decision function. One of the key disadvantages of the transductive SVM is that the underlying optimization problem is an NPhard problem and hence an iterative approximation has been used to solve it, which can take a very long time to finish. Our formalization of the transductive transfer SVM utilizes a quadratic programming optimization which is guaranteed to identify the global minimum in worst-case polynomial time.</p><p>The results for three versions of the support vector classifier are shown in Figure <ref type="figure" target="#fig_1">2</ref>. The first is the standard support vector machine (green line), which performs the worst, obtaining an accuracy of .60, the second is the transductive SVM <ref type="bibr" target="#b23">[23]</ref> (magenta line). The accuracy here improves to .72. Finally, the results of our transductive transfer SVM with a 1-norm soft margin are shown and the linear featureweighting simplification (LMFW -red line), which tries to take into account the distance between the testing and training distributions. In this case it achieves the best accuracy, .84, and comes closest to finding the underlying ideal separation for a linear transform, a vertical line between the two classes.  The next example we give is for a nonlinear classification task. Here data of the negative class are generated around the origin by sampling 100 points from a Gaussian distribution that is stretched in one dimension and shrunken in the other, for the training data it is stretched along the x2 axis, and for the test data along the x1 axis. The positive class is then generated in each case by randomly sampling points from a uniform distribution in the box region around the negative class distributions. Points that are less than a fixed threshold when evaluated in the Gaussian function for the negative data distribution are discarded, and points are sampled until 100 are obtained. For all three methods we use default parameters of σ = 0.5 for the RBF kernel width and regularization parameter C = 1. The resulting classification boundaries learned by each of the three methods are shown in Figure <ref type="figure" target="#fig_0">1</ref>, this time for our large-margin projection algorithm (LMPROJ). Our algorithm again achieves superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">REAL-WORLD DATA EXPERIMENTS</head><p>Here we evaluate our methods using collections of realworld data. We use data from four different classification tasks, forming a combined total of 24 transfer learning data sets. Three of these tasks are commonly used in the literature and are related to text classification (work that used all or some of these data sets include <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b28">28]</ref>). We include a fourth data set for transfer learning, related to protein-chemical interaction prediction.</p><p>Besides baseline methods of the standard support vector machine (SVM) and the transductive support vector machine (TSVM), we choose for comparison two recent stateof-the-art algorithms from KDD'08 that showed impressive results, out-performing baseline methods and some previous transfer learning methods in their experiments. The first comparison method is the Cross Domain Spectral Classifier (CDSC) <ref type="bibr" target="#b26">[26]</ref> (out-performing the methods of <ref type="bibr" target="#b37">[37]</ref> and <ref type="bibr" target="#b33">[33]</ref> in their experiments). We implemented their method in Matlab, directly following the algorithm as presented in the paper. The second is the Locally-Weighted Ensemble (LWE) classifier of <ref type="bibr" target="#b14">[14]</ref>. We used the same three methods that they used in their experiments for the ensemble, namely the Winnow algorithm from the SNoW Learning Architecture <ref type="bibr" target="#b8">[8]</ref>, a logistic regression algorithm from the BBR package <ref type="bibr" target="#b15">[15]</ref> and the LIBSVM implementation of a support vector machine classifier <ref type="bibr">[9]</ref>. We obtained parts of the code for their algorithm from an author's website http://ews.uiuc.edu/~jinggao3/kdd08transfer.htm and implemented the rest following the algorithm in their paper.</p><p>We obtained three pre-processed text classification data sets from the paper <ref type="bibr" target="#b14">[14]</ref> for our experimental study: the Reuters data sets, 20 newsgroups text classification data sets, and the spam filtering data sets. We follow the sampling strategy in <ref type="bibr" target="#b26">[26]</ref> to sample 500 instances each from the testing and training distribution to form our training and testing data sets.</p><p>We confirmed the correctness of our implementation by obtaining similar results to the performance reported in the respective papers (in some cases slightly more and in some cases slightly less accuracy). The methods we compared to did not list the type of normalization used, so we tried three different ways to normalize the non-binary features, no normalization, [0, 1] normalization using both the training and testing data, and [0, 1] normalization separately on the training and testing data. Interestingly, the performance of all the methods except LWE improved slightly using normalization, since normalization may interrupt the clustering structure in a data set. The difference between the second and the third normalization methods is negligible and hence we only report results on [0, 1] normalization separately on the training and testing data.</p><p>From our methods, we tested both the large-margin projection approach as described in Section 4.2 and Equation <ref type="formula" target="#formula_11">10</ref>and the large margin feature-weighting approach as described in Section 4.3. We denote the two approaches as LMPROJ and LMFW, respectively. We tested these two approaches as well as the basic SVM using a linear kernel and a cosine similarity measure, K( x, y) = ( x T y)/(|| x|||| y||) the same similarity measure used by the CDSC method and commonly used in text mining. We only show results using the cosine similarity since they were slightly better than with the linear kernel. We used Matlab and a convex solver, CVX <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>, to solve the quadratic programs of the LM-PROJ methods. For transductive transfer learning no labeled testing data can be used in the training, and since the testing and training distributions are different there is no easy way to use typical model selection approaches such as cross-validation to select appropriate parameters <ref type="bibr" target="#b14">[14]</ref>. Thus we give the best performance for each method over a range of parameters, for the LWE and CDSC methods we center this range around the best performing parameters reported in their respective papers. Because of this, the base line SVM method and the transductive SVM method have higher accuracy as compared to those reported in the literature when default parameter values are used. We also perform detailed parameter sensitivity analysis to show how the performance is affected by each of the parameters in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation Criteria</head><p>To compare the performance of the different methods, the first evaluation criterion we use is the F1 score, which is commonly used in information retrieval tasks such as document classification. The F1 score is the harmonic mean of the precision (P ) and recall (R): 2P R P +R , where P is given by tp tp+f p and R by tp tp+f n . tp denotes the number of true positive predictions, fp the number of false positives, fn false negatives, and tn true negatives. The F1 score is particularly appropriate for the spam filtering and chemical-protein interaction prediction data sets where predicting the positive class, the existence of spam and chemical-protein interaction respectively, is of particular interest. The second criterion we present results for is accuracy, commonly used to evaluate classification performance in general. Accuracy is given by tp+tn tp+tn+f p+f n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data Sets</head><p>A brief description of each data set and its set-up is given here. Table <ref type="table">3</ref> in the Appendix summarizes the data sets and gives the indexes by which we will refer to each in our results. For example, data set 10 is an email spam filtering data set where the training data set is a set of public messages and the testing data set is the set of emails collected from a specific user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Reuters and 20 Newsgroups (Data sets 1 -9)</head><p>These data sets both represent text categorization tasks, Reuters is made up of news articles with 5 top-level categories, among which, Orgs, Places, and People are the largest, and the 20 Newsgroups data set contains 20 newsgroup categories each with approximately 1000 documents. For these text categorization data, in each case the goal is to correctly discriminate between articles at the top level, e.g. "sci" articles vs. "talk" articles, using different sets of subcategories within each top-category for training and testing, e.g. sci.electronics and sci.med vs. talk.politics.misc and talk.religion.misc for training and sci.crypt and sci.space vs. talk.politics.guns and talk.politics.mideast for testing. For more details about the sub-categories, see <ref type="bibr" target="#b37">[37]</ref>. Each set of sub-categories represents a different domain in which different words will be more common. Features are given by converting the documents into bag-of-word representations which are then transformed into feature vectors using term frequency, details to this procedure can also be found in <ref type="bibr" target="#b37">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Spam Filtering (Data sets 10 -12)</head><p>For this task, there is a large quantity of public email messages available, but an individual's emails are generally kept private, and these messages will have different word distributions. The goal is to use the publicly available email messages to learn to detect spam messages, and transfer this The features for this data set are also made using term frequency from bag-of-word representations for the messages, details can be found in <ref type="bibr" target="#b6">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Protein-Chemical Interaction (Data sets 13 -24)</head><p>For this data set, we test the ability of the algorithms to transfer learning across protein families for protein-chemical interaction prediction. The goal is to be able to use the known protein-chemical interactions for a given protein family to help predict which chemicals the proteins of another protein family will interact with, for which no interaction information is known. We obtained a data set from Jacob et al. <ref type="bibr" target="#b22">[22]</ref> which includes all chemicals and their G protein-coupled receptor (GPCR) targets, built from an exhaustive search of the GPCR ligand database GLIDA <ref type="bibr" target="#b27">[27]</ref>. The data set contains 80 GPCR proteins across 5 protein families, 2687 compounds, and a total of 4051 protein-chemical interactions. One family we discard since it has too few proteins and interactions. For the proteins we extracted features using the signature molecular descriptors <ref type="bibr" target="#b13">[13]</ref>, for the chemicals we used a frequent subgraph feature representation approach <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b34">34]</ref>, and we used a threshold on the feature frequencies to obtain about 100 features each. We then built the feature vector for a given protein-chemical pair by taking the tensor product between the protein and chemical feature vectors.</p><p>For each protein family we then built a data set by sampling 500 pairs of proteins from the family and chemicals that are known to interact (or took all available interactions for a given family if there were less than 500). Since we had no "negative interaction" data we randomly sampled the same number of protein-chemical pairs among the proteins of the given family and the chemicals for which there was no known interaction, the assumption being that the positive interactions are scarce. We then constructed 12 transfer learning tasks by using each protein family in turn as the training domain and each other protein family for the testing domain. The break-down of the protein families is shown in Table <ref type="table">3</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental Results</head><p>First, we show an overall comparison of our method with the two state-of-the-art methods we compared with as well as the baseline of a SVM classifier with a cosine similarity kernel and the off-the-shelf transductive SVM. For easy visualization we show a plot of the F1 scores in Figure <ref type="figure" target="#fig_4">3</ref> with the data set index on the x-axis and the F1 score on the y-axis for the different methods, only showing here our method LMPROJ with the cosine similarity kernel (though the LMFW method was comparable, as seen in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>) marked by blue circles, the LWE method marked by upside-down purple triangles, the CDSC method marked by green crosses, transductive SVM (TSVM) by a dashed orange line, and traditional SVM by the dotted black line. The results for accuracy are reported in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>.</p><p>In Figure <ref type="figure" target="#fig_4">3</ref>, we observe that there is a general agreement of all 5 different methods that we compared in the first 12 data sets. The chemical-protein interaction data sets are harder and there is a large performance gap between different methods. Specifically comparing different methods, the base-line SVM works almost always the worst. This is not surprising since we know there are differences between training and testing samples and ignoring such differences usually does not lead to optimal modeling.</p><p>The cross-domain spectral classifier method (CDSC) has competitive performance, as compared to other methods. For some reasons that we do not fully understand, we observe a large performance variation of the CDSC method across different data sets. The locally weighted ensemble method (LWE) and the transductive SVM (TSVM) method have competitive performance in the first 12 data sets but they do not perform very well in the chemical-protein data sets. The results may suggest that the chemical-protein interaction data do not follow the clustering assumption well.</p><p>We observe that the LMPROJ method delivers stable results across the 24 data sets. For both accuracy and F1 score LMPROJ achieves the best score in 11 out of 24 data sets and is competitive with the best methods for the majority of the other data sets. It obtains the best score more times than any of the other methods.</p><p>We also note that we obtained somewhat better results for the SVM and TSVM methods than typically reported in the literature (e.g. <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b26">26]</ref>) on the same data sets that we use. This is because in our study instead of selecting a default parameter or allowing an internal cross-validation on the training data to be performed, to allow a fair comparison with the transfer learning approaches we reported the best results over a set of parameters for the baseline methods.</p><p>Next we give parameter sensitivity results in Figure <ref type="figure" target="#fig_5">4</ref>, for the accuracy criterion and the three parameters λ, λ 2, and C. For each plot, two parameters are fixed at the best values while the third parameter is varied to generate the plots. Here we show representative results for a couple of data sets, the 2nd Reuters data set -a text data set, and the second chemical-protein interaction data set. In the last three subfigures we also show the sensitivity results for the three parameters averaged over all 24 data sets. While the base accuracy was different for different data sets, the general trends are captured by averaging the results together. In general we see that as we suspected larger values of λ tend to improve performance; as λ is increased, the performance increases from the base standard SVM performance, and levels off to a maximum for a wide range of parameters. The results for λ2 show that in general the L2 regularization slightly improves performance up to moderate amounts, but past a certain point, i.e. too much regularization, the Finally the full results including a comparison of all the methods tested in terms of accuracy are given in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION AND FUTURE WORK</head><p>We have addressed the problem of transductive transfer learning using regularization with the goal of maximizing a classification margin while at the same time minimizing a distance between training and testing distributions. With extensive experimental study we demonstrated the effectiveness of our approach, comparing it with some recent state-ofthe-art methods. Our results demonstrate the effectiveness of this viewpoint of using regularization to find a decision function that brings the training and testing distributions together so that the training data can be effectively utilized.</p><p>One key idea for future work is incorporate an L1 penalty on β of the projection method to encourage a sparse solution. Also, an open problem for transductive transfer learning in general is how to perform parameter selection, since no labeled testing data is available. Another area of future work is to experiment with different loss functions for our large-margin classifier, in particular, a truncated hingeloss function (e.g. <ref type="bibr" target="#b38">[38]</ref>), to avoid situations where errors on the training data effectively prevent the transfer to the test domain. Finally, from our results we have seen that two schools of thought for considering transfer learning problems, one which tries to match the structure of the testing data and the other which tries to find some type of transform/embedding that brings the testing and training data together, seem to some extent to provide complementary results. Forming a hybrid method could potentially result in a more powerful classifier. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Decision boundaries for the standard support vector classifier (black) and our method (red) on a simple generated 2-D transfer learning problem. This example is discussed in detail in Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 K</head><label>2</label><figDesc>is a kernel function which defines an inner product (dot product) between samples in X , HK is the set of functions in the kernel space, ||f || is the L2 norm of the function f , and C is a regularization coefficient. V measures the fitness of the function in terms of predicting the class labels for training samples and is called a risk function. The hinge loss function is a commonly used risk function in the form of V = (1yif ( xi))+ and x+ = x if x ≥ 0 and zero otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of different support vector classifiers on a simple generated 2-D transfer learning problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Prediction F1 score on all 24 data sets learning to individual users' email messages. There are three different users with associated email messages. The features for this data set are also made using term frequency from bag-of-word representations for the messages, details can be found in<ref type="bibr" target="#b6">[6]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Parameter Sensitivity performance deteriorates. Also the performance is relatively insensitive to C for a wide range of values.Finally the full results including a comparison of all the methods tested in terms of accuracy are given in Table1and Table2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Accuracies for All Methods on Text Classification Datasets</head><label>1</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>1</cell><cell>Reuters 2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell cols="2">20 Newsgroup 6 7</cell><cell>8</cell><cell>9</cell><cell cols="3">Spam Filtering 10 11 12</cell></row><row><cell>SVM</cell><cell>0.80</cell><cell>0.70</cell><cell>0.68</cell><cell>0.79</cell><cell>0.76</cell><cell>0.78</cell><cell>0.76</cell><cell>0.84</cell><cell>0.91</cell><cell>0.77</cell><cell>0.77</cell><cell>0.85</cell></row><row><cell>TSVM</cell><cell>0.82</cell><cell>0.78</cell><cell>0.73</cell><cell>0.76</cell><cell>0.73</cell><cell>0.84</cell><cell>0.80</cell><cell>0.84</cell><cell>0.90</cell><cell>0.81</cell><cell>0.84</cell><cell>0.91</cell></row><row><cell>CDSC</cell><cell>0.86</cell><cell>0.75</cell><cell>0.67</cell><cell>0.71</cell><cell>0.87</cell><cell>0.66</cell><cell>0.73</cell><cell>0.83</cell><cell>0.90</cell><cell>0.68</cell><cell>0.82</cell><cell>0.56</cell></row><row><cell>LWE</cell><cell>0.81</cell><cell>0.71</cell><cell>0.66</cell><cell>0.87</cell><cell>0.79</cell><cell>0.84</cell><cell>0.70</cell><cell>0.87</cell><cell>0.92</cell><cell>0.84</cell><cell>0.91</cell><cell>0.95</cell></row><row><cell>LMFW</cell><cell>0.81</cell><cell>0.75</cell><cell>0.70</cell><cell>0.79</cell><cell>0.76</cell><cell>0.82</cell><cell>0.78</cell><cell>0.85</cell><cell>0.92</cell><cell>0.77</cell><cell>0.78</cell><cell>0.87</cell></row><row><cell>LMPROJ</cell><cell>0.83</cell><cell>0.78</cell><cell>0.71</cell><cell>0.81</cell><cell>0.77</cell><cell>0.85</cell><cell>0.84</cell><cell>0.87</cell><cell>0.93</cell><cell>0.84</cell><cell>0.82</cell><cell>0.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Accuracies for All Methods on Protein-Chemical Datasets</head><label>2</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell cols="4">Protein-Chemical Interaction 17 18 19 20</cell><cell>21</cell><cell>22</cell><cell>23</cell><cell>24</cell></row><row><cell>SVM</cell><cell>0.50</cell><cell>0.53</cell><cell>0.51</cell><cell>0.55</cell><cell>0.49</cell><cell>0.46</cell><cell>0.66</cell><cell>0.50</cell><cell>0.54</cell><cell>0.61</cell><cell>0.49</cell><cell>0.52</cell></row><row><cell>TSVM</cell><cell>0.56</cell><cell>0.56</cell><cell>0.61</cell><cell>0.51</cell><cell>0.60</cell><cell>0.45</cell><cell>0.72</cell><cell>0.55</cell><cell>0.72</cell><cell>0.66</cell><cell>0.48</cell><cell>0.57</cell></row><row><cell>CDSC</cell><cell>0.54</cell><cell>0.60</cell><cell>0.78</cell><cell>0.72</cell><cell>0.54</cell><cell>0.50</cell><cell>0.70</cell><cell>0.53</cell><cell>0.80</cell><cell>0.70</cell><cell>0.49</cell><cell>0.52</cell></row><row><cell>LWE</cell><cell>0.50</cell><cell>0.50</cell><cell>0.50</cell><cell>0.51</cell><cell>0.52</cell><cell>0.50</cell><cell>0.56</cell><cell>0.50</cell><cell>0.50</cell><cell>0.52</cell><cell>0.51</cell><cell>0.50</cell></row><row><cell>LMFW</cell><cell>0.56</cell><cell>0.63</cell><cell>0.74</cell><cell>0.60</cell><cell>0.54</cell><cell>0.56</cell><cell>0.66</cell><cell>0.54</cell><cell>0.75</cell><cell>0.57</cell><cell>0.49</cell><cell>0.63</cell></row><row><cell>LMPROJ</cell><cell>0.58</cell><cell>0.69</cell><cell>0.69</cell><cell>0.66</cell><cell>0.58</cell><cell>0.61</cell><cell>0.69</cell><cell>0.56</cell><cell>0.69</cell><cell>0.64</cell><cell>0.53</cell><cell>0.63</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>This work has been partially supported by ONR award # N00014-07-1-1042 and an NSF Graduate Research Fellowship (for BQ).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Representer Theorem</head><p>The major difficulty in solving Equation <ref type="formula">6</ref>is that w is a vector in the Hilbert space defined by the kernel function K and hence may have infinite dimensionality. Fortunately we have the following theorem, known as the Representer Theorem, which states that w is always a linear combination of φ(xi) and φ(zj) where xi in Ds and zj in Dt. Below we prove that the Representer Theorem is correct in our case.</p><p>Theorem 10.1. The vector w that minimizes the Equation 6 can be represented as</p><p>where βi and β j are coefficients.</p><p>Proof. We prove the theorem by showing contradiction. </p><p>And || w1|| 2 = || w0|| 2 + || w ⊥ || 2 ≥ || w0|| 2 . If we compare w1 and w0, we claim that the hinge loss function values are exactly the same and the MMD regularizer values are exactly the same. The only difference is that the norm of w1 is larger than w0. This claim contradicts the original assumption that w1 optimizes Equation <ref type="formula">6</ref>. Hence w ⊥ = 0.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting feature hierarchy for transfer learning in named entity recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL:HLT)</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transfer learning by distribution matching for targeted advertising</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sawade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ecml-pkdd discovery challenge 2006 overview</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECML/PKDD Discovery Challenge Workshop</title>
		<meeting>ECML/PKDD Discovery Challenge Workshop</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative learning for differing training and test distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brückner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th Int. Conf. on Machine Learning (ICML)</title>
		<meeting>of the 24th Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The snow learning architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cumby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rizzolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="http://l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=SNOW" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<title level="m">Semi-Supervised Learning (Adaptive Computation and Machine Learning)</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptation of maximum entropy capitalizer: Little data can help a lot</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="382" to="399" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical classifers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="101" to="126" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Genome scale enzyme-metabolite and drug-target interaction predictions using the signature molecular descriptor</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Faulon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="233" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge transfer via multiple model local structure mapping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><surname>Bbr</surname></persName>
		</author>
		<ptr target="http://www.stat.rutgers.edu/~madigan/BBR/" />
		<title level="m">Bayesian Logistic Regression Software. Software available at</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">CVX: Matlab software for disciplined convex programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<ptr target="http://stanford.edu/~boyd/cvx" />
		<imprint>
			<date type="published" when="2008-12">December 2008</date>
		</imprint>
	</monogr>
	<note>Web page and software</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph implementations for nonsmooth convex programs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Control and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS 19</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Dual Coordinate Descent Method for Large-scale Linear SVM</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Fifth International Conference on Machine Learning (ICML)</title>
		<meeting>the Twenty Fifth International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient mining of frequent subgraph in the presence of isomorphism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the 3rd IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="549" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Twentieth Annual Conference on Neural Information Processing Systems</title>
		<meeting>Twentieth Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Virtual screening of gpcrs: an in silico chemogenomics approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<idno>HAL-00220396</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>French Center for Computational Biology</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML-99, 16th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Bratko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</editor>
		<meeting>ICML-99, 16th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training linear svms in linear time</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-data learning of new tasks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spectral domain-transfer learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GLIDA: GPCR-ligand database for chemical genomic drug discovery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Taneishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yabuuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsujimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transfer learning via dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yangpan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 23rd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="677" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Determining object safety using a multiagent, collaborative system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Quanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tsatsoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Environment-Mediated Coordination in Self-Organizing and Self-Adaptive Systems (ECOSOA 2008) Workshop</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">October 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Domain adaptation of conditional probability models via feature subsetting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Satpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases</title>
		<meeting>the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Kernel methods for pattern analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving predictive inference under convariance shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Structure-based pattern mining for chemical compound classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lushington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Asia Pacific Bioinformatics Conference</title>
		<meeting>the 6th Asia Pacific Bioinformatics Conference</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Co-clustering based classification for out-of-domain documents</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Y Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Thirteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007-08">August 2007</date>
			<biblScope unit="page" from="210" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust truncated hinge loss support vector machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">479</biblScope>
			<biblScope unit="page" from="974" to="983" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The matrix stick-breaking process for flexible multi-task learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Madison</publisher>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Editorial: Special issue on mining low-quality data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="136" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
