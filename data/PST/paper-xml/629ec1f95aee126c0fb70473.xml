<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HYCEDIS: HYbrid Confidence Engine for Deep Document Intelligence System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-01">1 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bao-Sinh</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Quang-Bach</forename><surname>Tran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tuan-Anh</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dang</forename><forename type="middle">Duc</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hung</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tuan-Anh</forename><surname>Nguyen Dang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Duc</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cinnamon AI Inc</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Cinnamon AI Inc</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HYCEDIS: HYbrid Confidence Engine for Deep Document Intelligence System</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-01">1 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.02628v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>uncertainty</term>
					<term>neural networks</term>
					<term>supervised learning</term>
					<term>information extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Measuring the confidence of AI models is critical for safely deploying AI in real-world industrial systems. One important application of confidence measurement is information extraction from scanned documents. However, there exists no solution to provide reliable confidence score for current state-of-the-art deep-learning-based information extractors. In this paper, we propose a complete and novel architecture to measure confidence of current deep learning models in document information extraction task. Our architecture consists of a Multi-modal Conformal Predictor and a Variational Cluster-oriented Anomaly Detector, trained to faithfully estimate its confidence on its outputs without the need of host models modification. We evaluate our architecture on real-wold datasets, not only outperforming competing confidence estimators by a huge margin but also demonstrating generalization ability to out-of-distribution data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Document structure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent advances in machine learning enables creations of automatic information extractors that can read the input document in image format, locate and understand relevant text lines before organizing the information into computer-readable format for further analysis <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref>. Despite these successes, in critical domains such as healthcare and banking, humans still have to involve to scrutinize AI outputs as there is no room for AI errors in making important decisions that can affect human life. Confidence score estimation is one critical step towards implementing practical industrial systems wherein AI automates most of the operations yet human will intervene if necessary <ref type="bibr" target="#b38">[39]</ref>.</p><p>Unfortunately, to the best of our knowledge, there exists no holistic solution to reliably estimate the confidence score for the task of document information extraction. Current confidence score approaches are either generic methods verified only for simple image classification tasks <ref type="bibr" target="#b5">[6]</ref> or applied only for part of the information extraction process <ref type="bibr" target="#b24">[25]</ref>. * Both authors contributed equally to this research.</p><p>In this paper, we introduce a novel neural architecture that can judge the result of extracted structured information from documents provided by the information extracting neural networks (hereafter referred to as the IE Networks). Our architecture is hybrid, consisting of two models, which are a Multi-modal Conformal Predictor (MCP) and an Variational Cluster-oriented Anomaly Detector (VCAD). The former aims to combine the neural signals from 3 main stages of information extraction processes including textbox localization, OCR, and key-value recognition to predict the confidence level for each extracted key-value output. The later computes anomaly scores for the raw input document image, providing the MCP with additional features to produce better confidence estimation. The VCAD works on global, low-level features and plays a critical role in lifting the burden of detecting outliers off the MCP, which focuses more on local, high-level features.</p><p>We demonstrate the capacity of our proposed architecture on real-world invoice datasets: SROIE <ref type="bibr" target="#b13">[14]</ref>, CORD <ref type="bibr" target="#b27">[28]</ref>) and 2 inhouse datasets. The experimental results demonstrate that our method outperforms various confidence estimator baselines (including Droupout <ref type="bibr" target="#b5">[6]</ref>, temperature scaling <ref type="bibr" target="#b7">[8]</ref>). In short, we summarize our contribution as follows:</p><p>? We propose a Multi-modal Conformal Predictor (MCP) using a Feature Fusion module over 3 Feature Encoders to fuse signals extracted from IE Networks and compute the confidence score of the IE Networks' outputs. ? We provide a Variational Cluster-oriented Anomaly Detector (VCAD) to equip the MCP with an ability to handle out-ofdistribution data. ? We unify the proposed MCP and VCAD in a single hybrid confidence engine, dubbed as HYCEDIS, that for the first time, can well estimate the confidence of document intelligent system. ? We conduct intensive experiments on 4 datasets with detailed ablation studies to show the effectiveness and generalization of our hybrid architecture on real-world problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>A typical Document Intelligence System consists of multiple smaller steps: text detection, text recognition and information extraction (IE). Given a document image, the usual first step is to detect text lines, using segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref> or object detection method <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. The detected text line images can each go through an OCR model to transcribe into text <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref>. After all text contents are transcribed, the relevant text entities can be extracted, using entity recognition (sequence tagging) method <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>, segmentationbased method <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref>, or graph-based method <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> which formulates the document layout as a graph of text-lines/words. In this paper, we adopt a common IE Network that consisted of 3 main modules: text detection (Layout Analysis), text recognition (CRNN) and graph-based information extraction model (Graph KV). The text detection model shares the same architecture with <ref type="bibr" target="#b2">[3]</ref> which utilizes segmentation masks to detect text-lines in the document image. The text recognition (CRNN) uses popular CNN+Bi-LSTM+CTC-loss architecture to transcribe each text-line images into text. Finally, the GCN model <ref type="bibr" target="#b20">[21]</ref> performs the node classification tasks from the input document graph constructed from the text-lines' location and text to extract relevant information. Here, for our problems, we classify each node into different key types that represent the categories of the text-line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Multi-modal Conformal Predictor (MCP)</head><p>Given extracted intermediate features of IE Networks, our Multimodal Conformal Predictor aims to estimate the confidence score through predicting whether the final output is true or false. The MCP architecture (see Figure <ref type="figure" target="#fig_2">1</ref>(a)) contains two main components which are Feature Encoding and Feature Fusion. The Feature Encoding extracts features from different layers of trained IE Networks while the Feature Fusion combine them for predicting the final output.</p><p>Feature Encoding. Motivated by designs of late-fusion multi-view sequential learning approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>, three components of the Feature Encoding layers are independent processing streams including visual, lingual and structural feature encoders. In particular, the visual feature encoder is a many-to-one LSTM ? ? ? (?) that captures the visual information embedded in the CRNN of the IE Network. It takes the CRNN's logits (whose shape is ? ? ? ??_??? where ? is the number of timesteps) as input and outputs a vector of size ? ??? _??? , which represents the knowledge of the IE Networks on its OCR model's neural activations given the input image. In particular, for the ?-th extracted text-line image ? ? , we compute it as:</p><formula xml:id="formula_0">? ??? ? = ? ? ? (??? ? (? ? )).</formula><p>The lingual feature encoder, which is also implemented as a many-to-one LSTM ? ?? (?), processes the predicted OCR texts of the IE Networks. Each OCR-ed character in the text is represented as an one-hot vector with the size that equals to the size of the corpus. For the ?-th extracted OCR-ed text, the LSTM takes a sequence of these one-hot vectors (denoted by ???? ? , whose shape is? ?? ??_??? ) and produces an output vector of size ? ??? _??? , representing the knowledge of the IE Networks on the linguistic meaning and the syntactic pattern of its OCR-ed outputs. We compute it as:</p><formula xml:id="formula_1">? ??? ? = ? ?? (???? ? ).</formula><p>The structural feature encoding ? ?? (?) is a feed-forward neural network that accesses the information from the final layer of the IE Networks (node classification) -the Graph KV module. Here, the logits before softmax layer of the ?-th node in the graph (corresponding to the ?-th text box extracted from the document), denoted by ????? ).</p><p>Feature Fusion. The Feature Fusion network ? ?????? takes the three outputs from the Feature Encoding module and produces the ultimate feature vector. We use simple concatenation and Bi-linear pooling <ref type="bibr" target="#b37">[38]</ref> as two options for Feature Fusion. Bi-linear pooling use outer-product to combine inputs of different modalities. For simple concatenation, we just concatenate three vectors. For Bilinear Pooling, we first pool the pair of ? ??? ? and ? ??? ? , and then pool the resulting vector with ? ???? ? to get the pooled output ? ? :</p><formula xml:id="formula_2">? ? = ? ?????? (? ??? ? , ? ??? ? , ? ???? ? )<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variational Cluster-oriented Anomaly Detector (VCAD)</head><p>The anomaly detector aims to detect which input image is normal or abnormal, thus bolsters the MCP by a measurement of the normality that the input has. Specifically, the input to the anomaly detector is a compressed representation of the document image, and the output is a score in the range [0, 1] indicating the level of anomaly of the input. This score serves as an additional input to the confidence estimator.</p><p>Representing image data with cluster-oriented embeddings. In this section, we describe the representation learning of document images. Firstly, the training dataset was classified into some categories based on the appearance and the layout structure of the document image. Then we train a CNN-based image encoder to map each document image into a lower-dimensional vector representation. Here, the CNN architecture is MobileNet <ref type="bibr" target="#b30">[31]</ref>. We adopt the triplet loss <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref> to learn the compressed representation, wherein the embeddings of images from the same category tend to form a cluster in the embedding space.</p><p>Anomaly detector training. After constructing embeddings for training images, we build a Variational Auto Encoder (VAE) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref> as our anomaly detector (Figure <ref type="figure" target="#fig_2">1(b)</ref>). The VAE outlier detector is first trained on a set of normal (inlier) data to reconstruct the input it receives, with the standard VAE loss function which is the sum of KL term and reconstruction loss:</p><formula xml:id="formula_3">L VAE (?; ?, ?) = -??(? ? (?|?)||? ? (?)) + 1 ? ? ?? ?=1 log ? ? (? |? (?) ) (2)</formula><p>where ?, ? and ? denote the VAE's input, latent variable, and number of samples, respectively. ? ? represents the encoder and ? ? the decoder of VAE.</p><p>If the input data cannot be reconstructed well, the reconstruction error (implemented as L1 loss between VAE's input and output) is high and the data can be flagged as an outlier. We apply the min-max normalization <ref type="bibr" target="#b0">[1]</ref> to the reconstruction losses in order to get the corresponding abnormal scores in the range of [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hybrid confidence estimation</head><p>After getting the scalar output from our VCAD, we simply concatenate this scalar with the output of the Feature Fusion module in the MCP. The resulting vector is fed to a confidence estimator (CE),  Let ? ?? denote the input of our CE corresponding to prediction v?? , the CE is represented by the function ? ?? (?) yielding the softmax output ? ?? = ? ?? (? ?? ). The label for confidence estimation task is</p><formula xml:id="formula_4">? ?? = 1{?? ? {1 : ? ? } | ? ? ? match = v?? }<label>(3)</label></formula><p>The IE's output is considered to match the ground truth element if both the text contents and the keys match and the locations' IoU is greater than a threshold ( 0.3 in this paper). ? ?? is 1 if IE's prediction matches a ground truth element (be correct) and vice versa. Then the loss function is the standard binary cross-entropy loss with label ? ?? and probability ? ?? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets and evaluation metrics</head><p>4.1.1 Datasets. We collect 4 Invoice-like datasets and divide them into 2 tasks, corresponding to English and Japanese language used in the data. For each task, we use the bigger dataset as the main one, and the smaller as the out-of-distribution (OOD) dataset with respect to the main dataset.</p><p>We first use pre-trained IE Networks (see Sec. In-house 2 -OOD dataset. In-house 2 consists of 68 invoice documents from another Japanese company. The document pattern is quite different to the In-house 1 dataset. The two in-house dataset share 4 key types in common, resulting in 3,887 IE's output fields.</p><p>4.1.2 Evaluation metrics. We use the popular Area Under the Receiver Operating Characteristic Curve (AUC) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> and Expected Calibration Error (ECE) <ref type="bibr" target="#b25">[26]</ref> metrics for measuring the performance of confidence predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental baselines</head><p>Softmax Threshold. Our IE pipeline consists of multiple sequential models, so we adapted <ref type="bibr" target="#b11">[12]</ref> by combining both softmax probabilities from OCR and KV models using multiplication (i.e: ? ? ???? = ? ??? * ? ?? ). We then specify a threshold score and considered examples with higher-than-threshold softmax probability as correctly predicted one, and vice versa. The threshold score is tuned on the training dataset. Temperature Scaling. Temperature scaling <ref type="bibr" target="#b7">[8]</ref> is a technique that post-processes the neural networks to make them calibrated in term of confidence. Temperature scaling divides the logits (inputs to the softmax function) by a learned scalar parameter ? (temperature). We learn this parameter on a validation set, where ? is chosen to minimize negative log-likelihood. Softmax Classifier. Instead of only utilizing the softmax probability of the predicted class as Softmax Threshold, Softmax Classifier is more advanced by making use of the whole softmax vector. Particularly, we build a simple classifier using a feed-forward neural network. The input for the network is the concatenation of the OCR model's softmax vector and the KV model's one. Monte Carlo Dropout. MC Dropout <ref type="bibr" target="#b5">[6]</ref> belongs to the class of Bayesian/variational approaches. By keeping the dropout enabled at test time, we can obtain the variance of the neural network's outputs, and this variance indicates the level of uncertainty. We apply MC Dropout on our KV model, which is the final model in the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benchmarking results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>3.1 Ablation study. We ablate the effect of VCAD and MCP on the whole hybrid system. Table <ref type="table">1</ref> reports the results on SROIE dataset. Without VCAD, the proposed model achieves best AUC score of 86.90% using bi-linear pooling fusion strategy. Simpler concatenation method underperforms by about 3% demonstrating the importance of using outer-product to retain bit-level relationships among 3 modalities. When the VCAD is integrated, it consistently improves the performance of all fusion methods. Hence, the full hybrid HYCEDIS architecture can reach 88.12% AUC. Similar behaviors can be found with measurement using ECE metric.  <ref type="table">3</ref>: Performance comparison of baselines and proposed methods on In-house datasets images, OCR-ed text and graph structure help improve the accuracy of the softmax-based methods which only rely on some softmax layers of the IE Networks. In addition, when combined with VCAD, the AUC score is further increased and the ECE also downgrades. That manifests the contribution of our VCAD model. We can see a significant performance drop from baselines such as MC-Dropout when being tested on OOD CORD data. Our methods alleviate this issue, maintaining a moderate generalization to strange data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Public English datasets result.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.3</head><p>In-house Japanese datasets result. We also benchmark the models on two in-house datasets. In Table <ref type="table">3</ref>, our model continues to show the superior performance compared with other baselines. Our MCP model improves about 14.89% and 2% AUC score and reduces 0.0503 and 0.0273 ECE score in In-house 1 and In-house 2 datasets, respectively. When adding VCAD, the performance is improved around 3.82% on In-house 1 dataset and 2.78% on In-house dataset, which again validates our hypothesis on using anomaly detector to enhance conformal predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have introduced a holistic confidence score architecture that aims to verify the result of IE Networks in document understanding tasks. Our architecture takes advantages of a Multi-modal Conformal Predictor and a Variational Cluster-oriented Anomaly Detector to predict whether the IE Networks' output correct or not using features of different granularity. Our hybrid approach surpasses prior confidence estimation methods by a huge margin in benchmarks with invoice datasets. Remarkably, it demonstrates a capability of generalization to out-of-distribution datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(?? ) ?, is the input of the structural feature encoding, and the corresponding output is node embedding vector ? ???? ? representing the knowledge of the IE networks on its final decision (node classification): ? ???? ? = ? ?? (????? (?? ) ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: HYCEDIS architecture. (a) The Multi-modal Conformal Predictor (MCP). (b) The Variational Cluster-oriented Anomaly Detector (VCAD). (c) Confidence estimator (CE). MCP's output vector, plus the VCAD's abnormal score, is fed to fully-connected layers to produce the final output of HYCEDIS, indicating whether the extracted field true or false.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2) to generate the intermediate features for the MCP as mentioned in Sec. 3.1. The outputs of the IE Networks and the ground-truth IE outputs are used to produce labels for the confidence estimation task (Sec. 3.3). We only train the confidence models on the training dataset and benchmark them on the testing and corresponding OOD datasets. The evaluation on OOD data is a challenging benchmark since the OOD dataset is totally different from the main one in terms of layout, background and writing styles. Moreover, since the OOD datasets can have different type of keys from those in the main one, we only test the models on fields that share common keys with the main dataset. (a) Public datasets (English) SROIE -Main dataset. SROIE [14] is a dataset of scanned receipts. There are 4 keys: address, company, date, total. The training set has 626 files corresponding to 3859 IE's output key-value fields. We further hold 10% of the training as the validation set. The statistics for the test set are 341 files and 1,640 fields, respectively. CORD -OOD dataset. CORD [28] contains receipts collected from Indonesian shops and restaurants. Compared to SROIE, CORD document images are captured in the wild, thus the data is noisy and low in quality. CORD field shares only one key with SROIE, which is total. We use the CORD-dev set which contains 100 files correspoding to 103 IE's output fields. (b) In-house datasets (Japanese) In-house 1 -Main dataset. In-house 1 is a dataset containing Japanese invoice documents collected from several vendors. There are 25 keys. Example keys are issued_date, to-tal_amount, tax, item_name, item_amount. The training set has 835 files corresponding to 24,697 IE's output fields, and the test set has 338 files and 10,898 fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Table 2 shows the performance of all models on public datasets. On both SROIE and its OOD CORD dataset, our full HYCEDIS is consistently the best performer regarding both ECE and AUC scores. Our MCP is the runner-up under ECE metric. The improvements of MCP in AUC and ECE suggests that the signals from intermediate features extracted from text-line Performance comparison of baselines and proposed methods on SROIE and CORD datasets</figDesc><table><row><cell cols="2">Methods</cell><cell cols="2">ECE</cell><cell>AUC</cell></row><row><cell cols="2">MCP (concatenation)</cell><cell></cell><cell>0.1525 83.75</cell></row><row><cell cols="2">MCP (bilinear pooling)</cell><cell></cell><cell>0.1175 86.90</cell></row><row><cell cols="2">MCP (concatenation) + VCAD</cell><cell></cell><cell>0.1385 84.37</cell></row><row><cell cols="4">MCP (bilinear pooling) + VCAD 0.1002 88.12</cell></row><row><cell cols="4">Table 1: Ablation study on SROIE dataset</cell></row><row><cell>Methods</cell><cell cols="2">SROIE ECE AUC</cell><cell>CORD ECE AUC</cell></row><row><cell>Softmax threshold</cell><cell cols="3">0.1525 83.75 0.1731 66.91</cell></row><row><cell>Softmax classifier</cell><cell cols="3">0.1400 85.50 0.3289 54.91</cell></row><row><cell>MC Dropout</cell><cell cols="3">0.1175 86.90 0.5446 43.52</cell></row><row><cell cols="4">Temperature scaling 0.1385 84.37 0.3787 74.58</cell></row><row><cell>MCP</cell><cell cols="3">0.1124 86.40 0.1432 75.12</cell></row><row><cell>HYCEDIS</cell><cell cols="3">0.1002 88.12 0.1259 77.45</cell></row><row><cell>Methods</cell><cell cols="2">In-house 1 ECE AUC</cell><cell>In-house 2 ECE AUC</cell></row><row><cell>Softmax threshold</cell><cell cols="2">0.1285 68.79</cell><cell>0.5885 53.38</cell></row><row><cell>Softmax classifier</cell><cell cols="2">0.2810 71.43</cell><cell>0.3945 51.22</cell></row><row><cell>MC Dropout</cell><cell cols="2">0.3733 66.14</cell><cell>03621 48.20</cell></row><row><cell cols="3">Temperature scaling 0.1728 64.00</cell><cell>0.5879 58.18</cell></row><row><cell>MCP</cell><cell cols="3">0.0782 86.32 0.3348 60.12</cell></row><row><cell>HYCEDIS</cell><cell cols="3">0.0712 90.12 0.3019 61.90</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ganomaly: Semisupervised anomaly detection via adversarial training</title>
		<author>
			<persName><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Amir Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Murat Seckin Ayhan and Philipp Berens</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Character region awareness for text detection</title>
		<author>
			<persName><forename type="first">Youngmin</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bado</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9365" to="9374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end information extraction by character-level embedding and multi-stage attentional u-net</title>
		<author>
			<persName><forename type="first">Tuan</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04599</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qile</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3047" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An end-to-end textspotter with explicit alignment and attention</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changming</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5020" to="5029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Bitterwolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bounding the probability of error for high precision optical character recognition</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gary B Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Kae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="387" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Icdar2019 competition on scanned receipt ocr and information extraction</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1516" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual memory neural computer for asynchronous two-view sequential learning</title>
		<author>
			<persName><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rotationsensitive regression for oriented scene text detection</title>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Graph convolution for multimodal information extraction from visually rich documents</title>
		<author>
			<persName><forename type="first">Xiaojing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huasha</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11279</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graph Convolution for Multimodal Information Extraction from Visually Rich Documents</title>
		<author>
			<persName><forename type="first">Xiaojing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huasha</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-03">mar 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1962" to="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName><forename type="first">Shangbang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distance-based confidence score for neural network classifiers</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Mandelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.09844</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Confidence prediction for lexicon-free ocr</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Mor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="218" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Mahdi Pakdaman Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milos</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving long handwritten text line recognition with convolutional multi-way associative memory</title>
		<author>
			<persName><forename type="first">Duc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nhan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01577</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cord: A consolidated receipt dataset for post-ocr parsing</title>
		<author>
			<persName><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bado</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeheung</forename><surname>Surh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">GraphIE: A Graph-Based Framework for Information Extraction</title>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An Invoice Reading System Using a Graph Convolutional Network</title>
		<author>
			<persName><forename type="first">Della</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Vedova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrick</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Orchard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="434" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Layoutlm: Pre-training of text and layout for document image understanding</title>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1192" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to extract semantic structure from documents using multimodal fully convolutional neural networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Asente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Kraley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5315" to="5324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hybrid-augmented intelligence: collaboration and cognition</title>
		<author>
			<persName><forename type="first">Nan-Ning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Ju</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Qiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi-Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si-Yu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Ru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ba-Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Yue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="179" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
