<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HuatuoGPT, towards Taming Language Model to Be a Doctor</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-24">24 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongbo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junying</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianquan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guiming</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangbo</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingying</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
							<email>wangbenyou@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institue of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HuatuoGPT, towards Taming Language Model to Be a Doctor</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-24">24 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.15075v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present HuatuoGPT, a large language model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both distilled data from ChatGPT and real-world data from doctors in the supervised fine-tuned stage. The responses of ChatGPT are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis. We argue that real-world data from doctors would be complementary to distilled data in the sense the former could tame a distilled language model to perform like doctors. To better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an RLAIF (reinforced learning from AI feedback) fashion. To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics). Experimental results demonstrate that HuatuoGPT achieves stateof-the-art results in performing medical consultation among open-source LLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It is worth noting that by using additional real-world data and RLAIF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model ChatGPT in most cases. Our code, data, and models are publicly available at https://github.com/ FreedomIntelligence/HuatuoGPT. The online demo is available at https: //www.HuatuoGPT.cn/.</p><p>Medicine is a profoundly human endeavor where language plays a crucial role in facilitating interactions among clinicians, researchers, and patients. Coincidentally, the emergence of large language models (LLMs) in artificial intelligence is language-driven. This presents a remarkable opportunity for LLMs to contribute significantly to medicine. By bridging the gap between Medicine and LLMs, referred to as LLM for Medicine or LLM4Med, large language models can bring about transformative changes in human lives. One such impact is the ability to provide equitable access to high-quality</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>LLMs for Medicine Medicine stands as a paramount pillar in human existence, encompassing profound significance. Medicine relies heavily on experiential knowledge, wherein seasoned physicians outperform their novice counterparts. However, the advent of generative artificial intelligence (AI) systems, such as ChatGPT and DALLE, which also can learn from past experiences and external sources, heralds a transformative era for experience-driven professions. It is increasingly evident that intelligent (or, say, 'data-driven') medicine is an inexorable trend destined to materialize soon, albeit with ethical quandaries that demand consideration. medical resources to people worldwide through online means. This aligns with the original vision of the internet era and fulfills the aspirations of AI.</p><p>It is distressing to envision the thousands of lives lost each day, particularly in underdeveloped areas, due to the unavailability of medical resources, untimely medical care, or exorbitant medical costs. Given the substantial disparities in medical resources across countries and even within a single country, LLMs for Medicine have the potential to address these imbalances and promote equality among all human beings.</p><p>Is ChatGPT Enough in Medicine? The short answer is 'NO'. According to the recent study <ref type="bibr" target="#b0">[1]</ref>, it has been observed that ChatGPT, and even GPT-4, exhibit relatively poorer performance in vertical domains such as medicine. One contributing factor to this phenomenon is the potential lack of proficiency in medical knowledge among annotators. Consequently, there exist significant opportunities for further exploration and improvement in this domain.</p><p>On the other hand, online medicine often presents customized and localized challenges. For instance, Chinese medicine differs fundamentally from Western medicine, as does Indian medicine and many others. However, ChatGPT, being a general language model, lacks the capability for extensive customization. Additionally, entrusting private companies with users' medical data raises concerns, emphasizing the need for private deployment to ensure local data storage. Developing a medical ChatGPT that is fully open-sourced and commercially viable would be advantageous for the wellbeing of individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use Cases and Advantages of LLM4Med</head><p>The intended purposes of LLM4Med could be medical and health advice, triage, diagnosis, prescribing drugs, interpretation of medical reports, etc. In general, any medical or health information could be consolidated into an online chat process, similar to utilizing ChatGPT. Online medical consultation offers numerous advantages, including:</p><p>? Cost-effectiveness: The marginal cost of serving multiple users in an online manner is not linearly proportional to that of serving a single user. This scalability allows for cost-efficient expansion once the model is trained.</p><p>? Reducing hospital crowding: The recent pandemic highlighted the risks associated with overcrowded hospitals, as many individuals sought offline consultations even when not requiring immediate medical treatment. By providing online alternatives, the strain on hospitals can be alleviated in order to mitigate the risks of future pandemics.</p><p>? Addressing psychological barriers: Some individuals may refrain from seeking medical help or treatment due to fear or superstition, a phenomenon known as '????' in Chinese. Online chatting platforms may provide a more comfortable environment for such individuals to discuss their concerns.</p><p>Our Practices for Chinese Medical LLM As widely recognized, healthcare inequality in China is a significant issue. Disparities in medical conditions between residents in first-tier cities and those in small cities and rural areas are striking. For instance, the average life expectancy in Shanghai stands at approximately 82 years, whereas in regions such as Guizhou, characterized by relative economic disadvantage, life expectancy drops significantly to 73 years. <ref type="foot" target="#foot_0">2</ref>Here, we present a new Chinese medical LLM called 'HuatuoGPT' to commemorate the renowned Chinese physician Hua Tuo <ref type="foot" target="#foot_1">3</ref> . Rather than training from real-world medical data as many previous language models did, a straightforward way is to distill from ChatGPT <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> as it could quickly equip a language model with fluent chat and well-formatted responses. However, distilling from ChatGPT in medical domain is problematic, since the teacher model (i.e. ChatGPT) has the following issues:</p><p>? ChatGPT does not perform well in medical domain, especially in Chinese <ref type="bibr" target="#b0">[1]</ref>.</p><p>? ChatGPT refuses to diagnose and prescribe drugs due to ethical and safety issues.</p><p>? ChatGPT does not perform as a doctor does. For example, it never ask questions even though the patients' situation is incomplete for medical decision-making while doctors usually ask for further details. In this case, ChatGPT gives a general response instead of a specialized one. ? ChatGPT struggles with hallucination due to the auto-regressive fashion.</p><p>To overcome the above issues, the core recipe of HuatuoGPT is to leverage both real-world data from doctors and distilled data from ChatGPT in the Supervised Fine-Tuned (SFT) stage; both data consist of medical instruction data and medical conversation data <ref type="bibr" target="#b2">[3]</ref>. The distilled data from ChatGPT is used to tame language models to follow medical instructions and talk fluently. The additional real-world medical data not only inject medical knowledge into language models but also tame the language models to perform medical diagnoses or prescribe medications, act like a doctor and provide accurate information. The complementarity between real-world medical data and distilled data is further discussed in Sec. 2.</p><p>To leverage the strengths of both data (i.e., the real-world and distilled data) and meanwhile mitigate their weaknesses, we design a well-defined RL from AI Feedback (RLAIF) <ref type="bibr" target="#b3">[4]</ref> method after the SFT stage. It is used to reward the generated responses that are not only patient-friendly (learned from ChatGPT with better presentation quality, lengthy and informative contents, instruction-following abilities and fluent chat), but also doctor-like (learned from doctors with professional and interactive diagnosis.). Technically, we employ LLMs to score generated responses based on their correctness, richness, logical consistency, and diagnostic ability to align our model with the both merits of ChatGPT and doctors.</p><p>In assessing the performance of our model in the medical consultations, we meticulously crafted an evaluation schema encompassing both automated and manual assessments. HuatuoGPT, when assessed using GPT-4 in automatic evaluations on a series of 100 questions sourced from CBLUE with ten distinct medical intents, consistently outperformed incumbent Chinese medical models. More impressively, our model surpassed the performance of GPT-3.5-turbo in a majority of the evaluated cases. For the more complex multi-turn conversation evaluations, our HuatuoGPT model notably outshone ChatGPT in over 60% of the instances in 20 departments, showcasing our proficiency in fusing real-world and distilled data and effectively applying reinforcement learning techniques to them. Furthermore, HuatuoGPT also achieved state-of-the-art (SOTA) performance in several medical benchmarks such as CmedQA, webmedQA, and Huatuo26M datasets.</p><p>To ensure the integrity and precision of our assessment, we incorporated manual evaluations of our model's performance in both single-turn and multi-turn conversation scenarios. The results from these manual evaluations corroborated the findings from our automated evaluations, thus reinforcing the reliability and consistency of our model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significance of HuatuoGPT</head><p>The contributions of HuatuoGPT are manyfold:</p><p>? HuatuoGPT is the first medical language model to use RLAIF to leverage the merits of both real data and distilled data (including instruction and conversation data). ? This is among the first work that conducts systematic evaluation in medical LLMs. Training language models from purely real-world conversation was a common practice <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. However, this suffers from low-quality data. For example, the responses in real-world conversations might be uninformative, short, and poorly presented. More importantly, the values in these data are not aligned and even contradictory. Learning from purely humans usually result in an unsatisfied chat-based language model <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> compared to ChatGPT. User: ?????????????? ?????(Eating some crayfish last night, could that be the cause?) Recent work tends to distill a language model from ChatGPT, either imitating ChatGPT responses from single-turn instructions <ref type="bibr" target="#b11">[12]</ref> or learning the ChatGPT responses when interactively chatting with humans <ref type="bibr" target="#b1">[2]</ref>. By distilling output from ChatGPT, a model can quickly acquire impressive instructionfollowing capabilities and seamless dialogue skills. In addtion, characterized by its diversity and rapid generation, ChatGPT-distilled data can span various medical dialogues, encompassing various diseases, symptoms, and treatment modalities. This breadth and diversity substantially enhance the predictive performance and generalizability of the model.</p><formula xml:id="formula_0">Doctor: ? ? ? ??????? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning From Both Doctors and Chatgpt in Medicine</head><p>Complementarity Between ChatGPT and Doctors However, distillation from ChatGPT might not work for medical LLMs since there exists a fundamental gap between ChatGPT responses and doctor responses, as shown in Figure <ref type="figure">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Doctors' Responses</head><p>When consulting with doctors about our medical conditions, their responses typically exhibit professionalism that meets the personalized consultation. They are adept at inquiring about the symptoms and providing accurate diagnoses. However, due to time constraint <ref type="foot" target="#foot_2">4</ref> , their replies are often informal and concise in nature, and sometimes incoherent. Our preliminary study shows that training from purely patient-doctor interaction data is not satirised: 1) it cannot fluently follow diverse instructions or chats ; 2) the responses are short, poorly-presented, and sometimes uninformative, which are not patient-friendly.</p><p>ChatGPT Responses On the other side, although ChatGPT usually generates informative, wellpresented and logical responses, it usually tends to enumerate multiple possibilities and provides general and high-level advice. Since ChatGPT does not raise questions and guide patients to describe their symptoms, it lacks patients' input that can be used to generate specialized responses. In general, its responses often lack the contextual understanding that a doctor possesses, resulting in abstract responses that offer little substantial help to patients. In Conclusion, ChatGPT does not perform like doctors that conduct interactive diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Our Solution</head><p>Considering these challenges, we propose to combine the strengths of both distilled data (from ChatGPT) and real-world data (from Doctors), as illustrated in Table <ref type="table" target="#tab_4">2</ref>. The objective is to tame the medical LLM to perform like doctor. For example, it is expected to not only provide detailed, informative, and well-presented content but also conduct accurate and interactive diagnostic (usually posing clarifying questions) like doctors. To this end, our approach first mix distilled and real-world data in the Supervised Fine-Tuning stage (SFT). Furthermore, we employ RL from AI Feedback (RLAIF) to leverage the strengths of both data and meanwhile mitigate their weaknesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our approach focuses on integrating the characteristics of both doctor and ChatGPT to enhance the quality of responses in medical consultations through a two-stage training strategy: SFT with hybrid data and RL with AI feedback. We first utilize well-selected hybrid data to train the model through supervised fine-tuning and subsequently reinforce the generation of desired responses through feedback from AI, as illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Language Instruction Data Conversation Data</head><p>Training Method Distilled Real-world Distilled Real-world  </p><formula xml:id="formula_1">ChatDoctor English ? ? - - SFT MEDALPACA English ? ? - - SFT Visual Med-Alpaca English ? ? - ? SFT BenTsao Chinese ? - - - SFT DoctorGLM Chinese ? ? - ? SFT HuatuoGPT (Ours) Chinese ? ? ? ? SFT+RLAIF</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SFT with Hybrid Data</head><p>In the first stage, we employ a blend of distilled data and real-world data, capitalizing on both strengths to endow the model with Doctor-like and Patient-friendly characteristics. Within each data category, we have collected instruction data and conversation data to imbue the model with the capacity for instruction-following and interactive diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distilled Instructions from ChatGPT</head><p>We follow the work of self-instruct <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3]</ref> to construct a set of medical instruction data aiming to enable the model to follow user's medical instructions.</p><p>The difference is that we have employed top-down manner to create more natural and comprehensive responses. We design a taxonomy to collect or manually create seed instructions based on the roles and use cases. Based on each role or use case, we generate instructions separately using self-instruct <ref type="bibr" target="#b12">[13]</ref>. This could provide a wide range of instructions and meanwhile keep enough instructions for each role or use cases. Finally, we mix all seed instructions together and conduct self-instruct; this might be helpful to generate more diverse instructions. Details refer to Appendix A.1.  sometimes suffers from being overly concise and too colloquial. To address this, we utilized language models to enhance and refine the data based on the original content, which yields a high-quality real conversation dataset. Details refer to Appendix A.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-world</head><formula xml:id="formula_2">User????????????????. Doctor??????????????????? ????????????????????? ?????????????????????? ???????? User???????????????????? ?????????????????????? ??????????AHC?????????? ???? Doctor??????????????????? ????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ??????e??. . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RL with AI Feedback</head><p>In the Supervised Fine-Tuning (SFT) phase, we introduced a diverse dataset with the aim of enabling HuatuoGPT to emulate the inquiry and diagnosing strategy of doctors, while maintaining the rich, logical, coherent characteristics of LLMs' responses. In order to further align the model's generation preferences to our needs, we propose reinforcement learning with AI feedback to improve the quality of models' responses. Previously, OpenAI introduced reinforcement learning with human feedback <ref type="bibr" target="#b13">[14]</ref> to align LLMs with human preference but at a significant time and labor cost. <ref type="bibr" target="#b3">[4]</ref> demonstrated that with a carefully designed prompt, AI is able to imitate human preferences and to give relatively consistent scores on generated responses. Inspired by these alignment methods, we design a new pipeline to force the model to generate informative and logical responses without deviating from doctor's diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward Model</head><p>We train a reward model to align with the characteristics of doctors and LLMs. We use real instructions and conversations as training data, sampling multiple responses from our fine-tuned model. For multi-turn conversations, we provide the dialogue history to align our model's response generation. These responses are then scored by an LLM, such as ChatGPT, considering informativeness, coherence, adherence to human preferences, and factual accuracy based on given real doctors' diagnoses. The scoring LLM evaluates each response and assigns a score. We use this paired response data to train the reward model, using the fine-tuned model as its backbone for better generalization.</p><p>Reinforcement Learning In RL process, we sample k different responses {y 1 , . . . , y k } of a given query x by current policy ?. Each response y i is fed to our reward model to provide a reward score r RM . To ensure that the model does not deviate too far from the initial state ? 0 , we add the empirically-estimated KL penalty term, and the final reward function is as follows:</p><formula xml:id="formula_3">r = r RM -? KL D KL (?||? 0 )<label>(1)</label></formula><p>where ? KL is a hyperparameter for KL penalty, D KL is the KL penalty function. In our experiment, ? KL is set to 0.05. Input queries are de-duplicated and sampled from the remaining SFT hybrid data. This ensures a diverse range of inputs while retaining the model's response preferences in both the single-turn instruction and the multi-turn conversation scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the training implementation (Section 4.1) and then present the evaluation manners and results including automatic evaluation (Section 4.2) and manual evaluation (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Details</head><p>Our model is implemented in PyTorch using the Accelerate<ref type="foot" target="#foot_3">5</ref> and trlx <ref type="foot" target="#foot_4">6</ref> packages with Bloomz-7b1mt <ref type="bibr" target="#b14">[15]</ref> as the base architecture. <ref type="foot" target="#foot_5">7</ref> We leverage ZeRO-3 <ref type="bibr" target="#b16">[17]</ref> to distribute the model across 8 A100 GPUs for training. In the supervised fine-tuning process, we set the learning rate, batch size, and maximum context length to 2e -5, 128, and 2048, respectively. All models are trained for 3 epochs and weights performed the best on the validation set are saved. During the reinforcement learning process, we only update the parameters of the last two layers. The total number of steps is 16, 000, with a learning rate of 8e -6. In addition, to enhance the model's conversational and instructionfollowing capabilities in the general domain, we have incorporated Chinese instruction data (the Chinese Alpaca dataset <ref type="bibr" target="#b17">[18]</ref> and conversation data (ShareGPT<ref type="foot" target="#foot_6">8</ref> ). This enhances the model's ability to effectively understand and generate responses in various conversational scenarios and accurately follow instructions across different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Medical Benchmarks</head><p>We select three existing Chinese medical QA datasets as examples, namely cMedQA2 <ref type="bibr" target="#b18">[19]</ref>, webMedQA <ref type="bibr" target="#b19">[20]</ref> and Huatuo-26M <ref type="bibr" target="#b20">[21]</ref>, and compare the results with the existing baselines. cMedQA2 is a publicly available dataset based on Chinese medical questions and answers consisting of 108,000 questions and 203,569 answers. webMedQA is a real-world Chinese medical QA dataset collected from online health consultancy websites consisting of 63,284 questions. Huatuo-26M <ref type="bibr" target="#b20">[21]</ref> is the largest Chinese medical QA dataset which has 26M QA pairs from online medical consultation, knowledge bases and encyclopedias.  <ref type="bibr" target="#b20">[21]</ref>. GPT-3.5-turbo (ChatGPT) and HuatuoGPT are zero-shot while T5 is finetuned.</p><p>Evaluation Metrics Following the previous works <ref type="bibr" target="#b20">[21]</ref>, we utilize evaluation metrics such as BLEU, ROUGE, GLEU, and Distinct. BLEU computes the k-gram overlap between generated and reference sentences to measure similarity. ROUGE-N assesses the N-gram overlap, and ROUGE-L gauges the longest common subsequence of word matches. GLEU auto-evaluates sentence-level fluency. Distinct-1/2 aids in assessing textual diversity of the generated response by determining distinct n-grams count. However, these reference-based metrics may not suit medical QA scenarios due to diverse potential reference answers; more sound metrics should be paid more attention.</p><p>Baselines We compare our model to the best reported zero-shot model ChatGPT (GPT-3.5-turbo) and an in-domain fine-tuned model Chinese T5<ref type="foot" target="#foot_7">9</ref> respectively, which is continuously trained for 1 epoch on the full training set using batch-size 8, with a learning rate of 10 -4 using Adam, linear scheduling with a warm-up rate of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>HuatuoGPT demonstrates impressive performance across various Chinese medical benchmarks, achieves consistently high scores across all metrics, and demonstrates a high level of accuracy, fluency, and diversity in its generated responses. In cMedQA2 and webMedQA, HuatuoGPT even outperforms fine-tuned T5, suggesting that it has a robust generalization capability and is able to effectively handle a wide range of medical question-answering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Evaluation with GPT4</head><p>We conduct an automated evaluation on single-turn questions with different intents and multiturn conversations from different departments to observe the performance of the model in various scenarios.</p><p>Evaluation dataset For the single-turn questions, we extract 100 questions representing 10 intents (condition diagnosis, etiological analysis, treatment plan, medical advice, indicators interpretation, disease description, consequences description, precautions, efficacy, medical expenses) from the validation set of the Knowledge-based Universal Automated Knowledge Extraction for Query Intent Classification (KUAKE-QIC) in Chinese Biomedical Language Understanding Evaluation (CBLUE <ref type="bibr" target="#b21">[22]</ref>) <ref type="foot" target="#foot_8">10</ref> . KUAKE-QIC is collected from search engine queries, which makes it suitable for single-turn questions. To filter the noisy data, these questions were initially scored by ChatGPT, and a manual filtering process was conducted to select higher quality candidate questions for the test set.</p><p>For the multi-turn questions, we used the patient cases from <ref type="bibr" target="#b7">[8]</ref>. We selected 20 departments and randomly sampled 5 patient cases from each department, resulting in a total of 100 real patient cases. These cases were provided to ChatGPT, which played the role of the patient, interacting with each doctor model to obtain the diagnosis results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation aspects and metrics</head><p>We use GPT-4 as the referees to review the quality of model outputs. We prompt it to consider doctor-like language, symptom inquiry capability, the effect and reliability of the treatment recommendations and prescriptions, and the helpfulness to the patient. Given the question and the corresponding two answers from two models, GPT-4 is asked to first compare the advantages of each output and analyze the helpfulness to the patient, then it is requested to provide a score to each response respectively. In this way, we can get the evaluation scores of the 100 questions for each model comparison pair. We take the average scores over all the questions and calculate the performance ratio for each compared model (i.e. the overall score of the compared model divided by that of HuatuoGPT in a comparison pair).</p><p>Baselines We mainly compare HuatuoGPT to the two most popular general models ChatGPT and GPT4 <ref type="foot" target="#foot_9">11</ref> , and the two most representative open-source Chinese medical large language models: BenTsao (tuned from LLaMA) 12 , DoctorGLM (tuned from ChatGLM) 13 . For single-turn questions evaluation, we compare to all the mentioned four models. For multi-turn conversations evaluation, we only compare our model to DoctorGLM and GPT-3.5-turbo due to the quote limit of GPT-4. We report the performance ratio of all models over all single-turn questions and multi-turn conversations respectively.  Results For the single-turn questions evaluation, all the model performance results are shown in Table <ref type="table" target="#tab_9">5</ref>. The comparison among models for each category is shown in Figure <ref type="figure">3</ref> and the comparison among their overall performance is shown in Figure <ref type="figure" target="#fig_1">4</ref>, where the performance of HuatuoGPT is set to 1.0. According to GPT-4, HuatuoGPT is much better than BenTsao and DoctorGPT in all categories. Compared to GPT-3.5-turbo, HuatuoGPT outperforms it in three categories (Indicators Interpretation, Condition Diagnosis, and Medical Expenses) and performs similarly to it in two categories (Efficacy and Disease Description). However, HuatuoGPT is still worse than GPT4 in almost all categories, where it attains similar performance to GPT4 in two categories (Efficacy and Medical Expenses). Overall, HuatuoGPT achieves higher scores than DoctorGLM, BenTsao, and GPT-3.5-turbo. For the multi-turn conversations evaluation, similarly, the overall performance of HuatuoGPT surpasses GPT-3.5-turbo in over 60% of cases. The comparison for each category and for overall performance are shown in Table <ref type="table" target="#tab_10">6</ref>, Figure <ref type="figure" target="#fig_2">5</ref>, and Figure <ref type="figure" target="#fig_3">6</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Figure <ref type="figure">3</ref>: The model performance ratio for each category on 100 single-turn questions, reviewed by GPT4. The performance of HuatuoGPT is set to 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Manual Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Evaluation dataset and metrics</head><p>We utilize the 100 KUAKE-QIC questions (the same as those in automated evaluation) as the test set for single-turn question evaluation and randomly sample 50 patient cases from 100 test cases used in automated evaluation for multi-turn conversations manually evaluation.</p><p>In the manual evaluation of the HuatuoGPT, we think that the following three aspects should be considered, particularly in medical consultation and medication prescription and take them as the guidelines for evaluation:</p><p>Diagnosis accuracy. This aspect evaluates the model's accuracy and comprehensiveness in diagnosing patient symptoms. Evaluators are provided a set of medical cases or symptom descriptions and assess the correctness, relevance, and reasonableness of the model's diagnosis. Comparisons can be made with assessments made by medical professionals to ensure the model's accuracy.</p><p>Treatment recommendation accuracy. This aspect assesses the accuracy and appropriateness of the model's treatment recommendations for patients. Evaluators are provided a set of medical cases or symptom descriptions and evaluate whether the model's treatment recommendations align with medical knowledge and real-world applications that are effective and reliable to the patient's main condition and problem.</p><p>Medication knowledge and prescription accuracy. This aspect evaluates the model's understanding of medications and the accuracy of its prescription recommendations. Evaluators are provided a set of medical cases or symptom descriptions and assess the accuracy and reliability of the medication recommendations based on medical knowledge and guidelines.</p><p>We provide physicians with above considerations, enabling them to align their evaluation guidelines. This allows for a meticulous comparison of the good and bad outputs of different models for the same scenario.</p><p>During evaluation, medical experts are asked to provide assessments on different responses. Each physician is solely responsible for evaluating the output of a single pair of models, ensuring that each response data is scrambled and anonymized with the utmost strictness. Consistent with automatic evaluation, we take BenTsao, DoctorGLM, ChatGPT and GPT4 as the baselines in single-turn question evaluation and select DoctorGLM, ChatGPT as the baselines in multi-turn conversation evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>As shown in Table <ref type="table" target="#tab_12">7</ref>, HuatuoGPT performs exceptionally well against BenTsao, DoctorGLM, and it even slightly outperforms ChatGPT, highlighting its robust diagnosis accuracy, treatment recommendations, and medication knowledge. The results of multi-turn evaluation are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison</head><p>Win Lose HuatuoGPT vs DoctorGLM 86% 14% HuatuoGPT vs ChatGPT 58% 42% Table <ref type="table" target="#tab_11">8</ref>: Manual evaluation of model performance in multi-turn medical consultations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>In this section, we explore the impact of two types of data on the model. We trained two distinct models, namely HuatuoGPT (w/ real data) and HuatuoGPT (w/ distilled data), using exclusively real-world data or distilled data, respectively. We thoroughly compare the variations in responses between the two models for the same set of questions as shown in Table <ref type="table" target="#tab_13">9</ref>. HuatuoGPT (w/ real data) has a tendency to ask clarifying questions to patients, performing as expected, similar to a doctor. However, a minor flaw is that the response is brief and the content appears less well-organized for reading. On the other hand, HuatuoGPT (w/ distilled data) generates well-organized, detailed, and informative content. Nevertheless, its responses are more focused on providing suggestions rather than making a diagnostic decision. Thus, HuatuoGPT (w/ distilled data) resembles a "non-doctor friend" rather than a doctor.</p><p>To assess the impact of RLAIF (Reinforced Learning with Auxiliary Information Feedback), we also compare two models: the default model called HuatuoGPT and a variant called HuatuoGPT (w/o RLAIF) which does not utilize RLAIF. It is worth noting that the latter model, HuatuoGPT (w/o RLAIF), did not ask additional questions to patients. This might be attributed to the fact that its training data could be biased towards the ChatGPT data, while real-world data may have been overlooked. In contrast, our default model, HuatuoGPT with RLAIF, can function like a doctor by asking follow-up questions to patients to get more accurate diagnoses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Limitation</head><p>We emphasize the potential risks associated with generation-based medical consultation. The main concern lies in the challenge of verifying the accuracy and correctness of the generated content. In the medical domain, the dissemination of misleading information can have severe ethical implications. Although generative QA has shown promise, especially with the success of models like ChatGPT, they are not yet fully prepared for real-world deployment in the biomedical domain.</p><p>While generation methods currently hold great potential, it is important to exercise caution and prudence before deploying them in real-world applications. Further research and development are necessary to refine these models, enhance their accuracy, and establish robust mechanisms for accurateness-checking and error correction. Only through careful scrutiny and continual improvement can we minimize the risks and ethical concerns associated with generation-based medical QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LLMs in Medicine</head><p>The language model in the medical field has always been a concern for researchers. The early models were mainly based on the GPT-2 series models to continue pre-training in the domain. BioMedLM<ref type="foot" target="#foot_11">14</ref> is a domain-specific large language model for biomedicine, trained from 2.7B GPT-2. It is trained on the PubMed Abstracts and PubMed Central portions of the Pile dataset, which contains around 50B tokens and spans a collection of 16 million abstracts and 5 million full-text articles from the biomedical literature. Similarly, BioGPT <ref type="bibr" target="#b22">[23]</ref> is a medium GPT-2 model pre-training in medical data collected from the official PubMed website <ref type="foot" target="#foot_12">15</ref> . For downstream tasks, it uses the soft prompt for fine-tuning training.</p><p>Recently, many efforts have attempted to use instruction fine-tuning to enhance the ability for medical consultation on large-scale language models (&gt;6B), as shown in Table <ref type="table" target="#tab_4">2</ref>. MEDALPACA <ref type="bibr" target="#b23">[24]</ref> <ref type="foot" target="#foot_13">16</ref> is a LLaMA model trained on the Medical Meadow, consisting of two main categories, a collection of established medical NLP tasks reformatted in instruction tuning formats, as well as a crawl of various internet resources. ChatDoctor <ref type="bibr" target="#b24">[25]</ref> <ref type="foot" target="#foot_14">17</ref> is also a medical LLM trained on the HealthCareMagic-100k</p><p>User: ??????????????????????????????????? (I recently developed small blisters on my hands and palms, and my ten fingers are peeling. What is this disease and do I need to go to the hospital?)</p><p>HuatuoGPT (w/t real data): ???????????????????????????????????????????????????????? ??????????????????????????????????????? (Hello, based on the description of your symptoms, you may be suffering from an allergic reaction caused by a skin condition such as eczema or contact dermatitis. In order to diagnose the condition more accurately, I need to know some additional information. Are these small blisters accompanied by itching? Are there any other areas of the skin that show similar symptoms?)</p><p>HuatuoGPT (w/t distilled data): ???????????????????????????????????????????????????????? ?????????????????????????????? ?????????????????????? 1.????????? 2.????????????????????????????? 3.?????????????????????? 4.??????????????????????? 5.???????????? (Hello, according to the symptoms you describe, it may be an inflammatory skin reaction caused by eczema. However, in order to determine the cause and treatment plan more precisely, we recommend that you visit a hospital. Your doctor can make a definitive diagnosis by carefully examining your symptoms and asking for information about your history. While waiting to seek medical attention, you can take the following steps to relieve your symptoms: 1. Keep your hands clean and dry. 2. Avoid washing your hands with detergents or hand sanitizers that contain harsh ingredients or chemicals. 3. Wear loose and breathable clothes and try to avoid contact with allergenic substances. 4. Drink more water and eat more fresh vegetables and fruits to replenish vitamins and minerals. 5. Pay attention to psychological adjustment and reduce stress.)</p><formula xml:id="formula_4">HuatuoGPT (w/o RLAIF): ???????????????????????????????????????????????????????? ????????? ???????????????????????????????????????????????????????? ???????????????????????????? ???????????????????????????????????????????????????????? ??????????????????(???)????????????????</formula><p>(Hello, according to your description, you may be suffering from a skin condition such as eczema or contact dermatitis. This condition is usually associated with skin allergies to certain substances, such as detergents, cosmetics, metal products, etc. I recommend that you first avoid any items or environmental stimuli that may cause allergic reactions, such as perfumes, detergents, cosmetics, etc. Also pay attention to keeping your hands hygienically clean and dry, wear soft and breathable gloves, and avoid scratching the affected area with your hands to avoid aggravating the infection. If the symptoms persist and cannot be relieved, you can go to the hospital, the doctor will further examine to determine the cause of the disease and give the appropriate treatment plan. In general, you can use topical medications for treatment, such as hydrocortisone cream (Euthyrox), but the specific dosage and application methods need to follow medical advice.)</p><formula xml:id="formula_5">HuatuoGPT: ??????????????????????????????????????????????????????? ?? ???????????????????????????????????????????????????????? ????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ??</formula><p>(Hello, according to your description, you may have eczema with sweating blisters. This is a skin condition caused by a viral infection that usually results in blisters and itching after rubbing or heat on the skin. In order to diagnose the condition, I need to know more information to be sure of the diagnosis. How long have these symptoms been present? Are they accompanied by other uncomfortable sensations, such as pain, fever or other skin changes? Is there a history of allergies or exposure to harmful substances? If you are not experiencing significant physical discomfort, you can try the following measures to relieve your symptoms for the time being: avoid wearing tight clothing or using chemical substances, keep your skin clean and dry; use a moisturizing cream or lotion with natural ingredients on the affected area to reduce the symptoms of itching and blisters; if your symptoms are severely affecting your daily life, seek medical advice promptly.)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In conclusion, this paper presents a comprehensive approach to training a reliable and conversational healthcare model by leveraging complementary data sources and incorporating AI model feedback through reinforcement learning. The proposed approach addresses the limitations of relying solely on real or synthetic data and allows for the creation of a model that combines the strengths of both sources. By continuously refining its responses based on feedback, the model can improve its conversational abilities while maintaining the reliability necessary for healthcare applications. Further research in this area holds significant potential for advancing the field of AI in healthcare and improving patient outcomes.</p><p>A Methodology details The translation is as below:</p><p>You are asked to design 20 different triplets of&lt;characters, instructions, input&gt;. The first line is the character, the second line is the instruction that the character wants GPT to help him improve his work efficiency, and the third line is the corresponding input for the instruction. requirement:</p><p>1. The role can be very specific and needs to be related to the medical scene.</p><p>If it is a doctor, it can even be refined to the medical department, such as "respiratory physician".</p><p>2. The description of each instruction should be diverse, and the types of instructions should be diverse. Verbs should be avoided as much as possible to maximize diversity. Each instruction should be something that the GPT language model can accomplish, unable to generate and draw images, unable to read audio and webpage links; Instructions should be 1-2 sentences in length, which can be either command sentences or interrogative sentences; Instructions usually have a placeholder, placeholder, such as "this below" or "some", and the "input" field will be specified.</p><p>3. The input should be a specific example of the instruction, providing real substantive content, as the instruction may be empty and need to be qualified with a specific input. The input should not be just a link or file name, or an unspecified 'paper', but rather specific content. It is recommended to input no more than 200 words. 4. The roles, instructions, and inputs are mostly in Chinese, and the roles, instructions, and inputs should not be repeated. Instructions are mandatory, please provide roles and inputs as much as possible.</p><p>Input can be empty The list of 20 triples is as follows:</p><p>Different from the original self-instruction, we generated role-enhanced instructions and it will be used to generate the output with the following prompt. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>???????????[Medical</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic of HuatuoGPT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The overall model performance ratio 100 single-turn questions, reviewed by GPT-4. The performance of HuatuoGPT is set to 100%.</figDesc><graphic url="image-16.png" coords="12,167.40,72.00,277.22,222.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The model performance ratio for category on 100 multi-turn dialogues, reviewed by GPT4. The performance of HuatuoGPT is set to 1.0.</figDesc><graphic url="image-17.png" coords="13,147.60,72.00,316.82,273.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The overall model performance ratio on 100 multi-turn dialogues, reviewed by GPT-4. The performance of HuatuoGPT is set to 100%.</figDesc><graphic url="image-18.png" coords="14,167.40,72.00,277.22,222.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-15.png" coords="11,147.60,194.61,316.80,278.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? Human evaluation shows that HuatuoGPT outperforms existing open-sourced LLMs and ChatGPT(GPT-3.5-turbo). Its performance is most similar to that of a doctor. ? We open-source our training data, code, HuatuoGPT model and the reward model at https: //github.com/FreedomIntelligence/HuatuoGPT.</figDesc><table><row><cell>2 Motivations</cell></row><row><cell>2.1 Learning from Humans or ChatGPT?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>and Table1. The quality of distilled data can fluctuate, manifesting as incorrect or ambiguous information in the generated conversations. Contrastingly, real-world data, harvested from authentic doctor-patient interactions, provide an indispensable perspective into the complexities of actual medical scenarios. It can accurately reflect the true intention distribution of patients and has accurate diagnoses from doctors. The primary strength of real-world data lies in its high accuracy and professionalism. Behavior difference between ChatGPT and Doctors in various aspects. We argue that learning from ChatGPT and Doctors are complementary.</figDesc><table><row><cell></cell><cell>Aspects</cell><cell cols="2">Responses ChatGPT Doctor</cell><cell>How ChatGPT works</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Example:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>User: ???????????????(Doctor, do I have</cell></row><row><cell></cell><cell>diagnostic ability</cell><cell>?</cell><cell>?</cell><cell>gastroenteritis as I have a stomachache?)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ChatGPT: ?????????????????? (As a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>language model, I am unable to provide medical diagnoses.)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Example:</cell></row><row><cell>doctor-like</cell><cell>expert-level accuracy</cell><cell>low</cell><cell>high</cell><cell>User: ???'??????'?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ChatGPT: "??????"????????????????</cell></row><row><cell></cell><cell>raising questions</cell><cell>?</cell><cell>?</cell><cell>ChatGPT usually cannot raise questions in response to patients</cell></row><row><cell></cell><cell>informativeness</cell><cell>high</cell><cell>-</cell><cell>ChatGPT usually provides informative responses</cell></row><row><cell>patient-friendly</cell><cell>patience</cell><cell>high</cell><cell>-</cell><cell>ChatGPT has patience to provide lengthy responses</cell></row><row><cell></cell><cell>presentation quality</cell><cell>high</cell><cell>-</cell><cell>ChatGPT usually provide well-formatted responses (like using bullets)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Data Sources and Training Method Across Popular Medical Models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Instructions from Doctors Real-world instruction data are derived from questionanswering between doctors and patients. Responses from doctors are expertise, with high relevance and conciseness. Therefore, we further enhance the quality and reliability of the single-turn instruction data by refining authentic doctor-patient question-answer pairs. Details refer to Appendix A.2.Distilled Conversations from ChatGPT Distilled conversations are generated by two ChatGPTs, each ChatGPT is associated with a role (either doctor or patient) using a well-designed prompt. First, we leverage a third-party medical diagnosis database as a valuable source of medical knowledge and expertise for generating synthetic dialogue data. Based on the basic background of patients and the final diagnosis from doctors, two ChatGPTs are asked to generate dialogue utterances one by one. In these conversations, the responses generated by LLMs usually are informative, detailed, well-presented, and adhere to a consistent style; the format and information are usually friendly to</figDesc><table><row><cell>Data</cell><cell>Sources</cell><cell>Scale</cell><cell>Description</cell><cell></cell><cell>Example</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Medical instructions</cell><cell>User:?????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">generated with self-</cell><cell>Doctor:???????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">instruction. Seed in-</cell><cell>?????</cell></row><row><cell></cell><cell cols="2">ChatGPT 61,400</cell><cell cols="2">structions are manu-ally written.</cell><cell>1. ????????????????????? ?????????????????????? ????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2. ?????????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>??????????????. . .</cell></row><row><cell>Instruction</cell><cell></cell><cell></cell><cell cols="2">Single-turn medical</cell><cell>User: ???????160-90.??????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">consultants in real-</cell><cell>??8?????11??????????160-90.?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">world scenarios, in</cell><cell>???????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">which the doctor's re-</cell><cell>Doctor: ???????????????????</cell></row><row><cell></cell><cell>Doctors</cell><cell>69,768</cell><cell cols="2">sponse is more spe-</cell><cell>??????160-90.?????????8????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">cific and can address</cell><cell>?11????????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">the user's medical</cell><cell>??????????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell>concerns.</cell><cell></cell><cell>??????????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Medical multi-turn</cell><cell>User: ????????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell>conversations</cell><cell>are</cell><cell>????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">created using two</cell><cell>Doctor: ???????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ChatGPTs:</cell><cell>one</cell><cell>??????????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">acting as the patient</cell><cell>??????6??7?????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">and the other as</cell><cell>1. ?? n2. ???????</cell></row><row><cell></cell><cell cols="2">ChatGPT 68,888</cell><cell>the doctor. conversations</cell><cell>The are</cell><cell>3. ??????????? 4. ?????3-5???...</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">based on real-world</cell><cell>User: ????????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">patient cases and</cell><cell>???????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">their corresponding</cell><cell>Doctor: ???????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell>treatment.</cell><cell></cell><cell>??????????????????????</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>??????????????????????</cell></row><row><cell>Conversation</cell><cell></cell><cell></cell><cell cols="2">Medical multi-turn</cell><cell>??????????...</cell></row><row><cell></cell><cell></cell><cell></cell><cell>conversations</cell><cell>in</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">real-world scenarios.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Conversations</cell><cell>in-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">clude multiple rounds</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">of questioning by</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">doctors and continu-</cell></row><row><cell></cell><cell>Doctors</cell><cell>25,986</cell><cell cols="2">ous questioning by patients.</cell></row></table><note><p><p>patients. Details refer to Appendix A.3</p>Real-world Conversations with Doctors Real-world conversations are collected from genuine scenarios, where doctors' responses often demand diverse abilities, including long-range reasoning and raising questions to guide patients in describing their symptoms. However, this type of data</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The Details of Hybrid SFT Data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Benchmark on Chinese medical QA dataset</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="10">BLEU-1 BLEU-2 BLEU-3 BLEU-4 GLEU ROUGE-1 ROUGE-2 ROUGE-L Distinct-1 Distinct-2</cell></row><row><cell></cell><cell>GPT-3.5-turbo</cell><cell>19.21</cell><cell>7.43</cell><cell>3.14</cell><cell>1.24</cell><cell>5.06</cell><cell>20.13</cell><cell>3.10</cell><cell>12.57</cell><cell>0.69</cell><cell>0.99</cell></row><row><cell>cMedQA2</cell><cell cols="2">T5 (fine-tuned)  ? 20.88 HuatuoGPT 25.37</cell><cell>11.87 13.16</cell><cell>7.69 7.39</cell><cell>5.09 4.25</cell><cell>7.62 8.30</cell><cell>27.16 27.75</cell><cell>9.30 7.31</cell><cell>20.11 17.36</cell><cell>0.41 0.74</cell><cell>0.52 0.93</cell></row><row><cell></cell><cell>GPT-3.5-turbo</cell><cell>18.06</cell><cell>6.74</cell><cell>2.73</cell><cell>1.09</cell><cell>4.71</cell><cell>20.01</cell><cell>2.81</cell><cell>12.58</cell><cell>0.65</cell><cell>0.87</cell></row><row><cell>webMedQA</cell><cell cols="2">T5 (fine-tuned)  ? 21.42 HuatuoGPT 24.61</cell><cell>13.79 12.84</cell><cell>10.06 7.23</cell><cell>7.38 4.19</cell><cell>8.94 7.73</cell><cell>31.00 27.38</cell><cell>13.85 7.09</cell><cell>25.78 17.66</cell><cell>0.37 0.71</cell><cell>0.46 0.93</cell></row><row><cell></cell><cell>GPT-3.5-turbo</cell><cell>18.44</cell><cell>6.95</cell><cell>2.87</cell><cell>1.13</cell><cell>4.87</cell><cell>19.60</cell><cell>2.82</cell><cell>12.46</cell><cell>0.69</cell><cell>0.89</cell></row><row><cell>Huatuo-26M</cell><cell>T5 (fine-tuned) HuatuoGPT</cell><cell>26.63 25.16</cell><cell>16.74 13.21</cell><cell>11.77 7.54</cell><cell cols="2">8.46 11.38 4.40 8.37</cell><cell>33.21 27.76</cell><cell>13.26 7.45</cell><cell>24.85 17.99</cell><cell>0.51 0.73</cell><cell>0.68 0.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Scores for each category and the overall scores of each model pair on 100 single-turn questions reviewed by GPT-4.</figDesc><table><row><cell>Category</cell><cell cols="4">HuatuoGPT v.s. DoctorGLM HuatuoGPT v.s. ChatGPT</cell></row><row><cell>Traditional Chinese Medicine</cell><cell>8.80</cell><cell>5.70</cell><cell>8.60</cell><cell>8.30</cell></row><row><cell>Obstetrics</cell><cell>9.00</cell><cell>5.20</cell><cell>8.90</cell><cell>7.50</cell></row><row><cell>Pediatrics</cell><cell>8.90</cell><cell>6.50</cell><cell>8.50</cell><cell>7.80</cell></row><row><cell>Internal Medicine</cell><cell>8.80</cell><cell>5.60</cell><cell>8.70</cell><cell>7.70</cell></row><row><cell>Stomatology</cell><cell>8.60</cell><cell>6.60</cell><cell>8.30</cell><cell>8.40</cell></row><row><cell>Surgery</cell><cell>8.00</cell><cell>5.20</cell><cell>8.60</cell><cell>7.60</cell></row><row><cell>Obstetrics and Gynecology</cell><cell>8.70</cell><cell>5.80</cell><cell>8.60</cell><cell>7.60</cell></row><row><cell>Gynecology</cell><cell>8.50</cell><cell>6.40</cell><cell>8.10</cell><cell>8.00</cell></row><row><cell>Cardiovascular Medicine</cell><cell>8.60</cell><cell>5.20</cell><cell>8.40</cell><cell>8.00</cell></row><row><cell>General Surgery</cell><cell>9.00</cell><cell>5.00</cell><cell>8.80</cell><cell>7.70</cell></row><row><cell>Urology</cell><cell>8.70</cell><cell>5.80</cell><cell>8.50</cell><cell>7.40</cell></row><row><cell>Gastroenterology</cell><cell>8.70</cell><cell>5.50</cell><cell>8.70</cell><cell>7.60</cell></row><row><cell>Andrology</cell><cell>8.90</cell><cell>5.20</cell><cell>8.20</cell><cell>7.50</cell></row><row><cell cols="2">Dermatology and Venereology 8.70</cell><cell>5.80</cell><cell>8.40</cell><cell>7.60</cell></row><row><cell>Dermatology</cell><cell>8.80</cell><cell>5.30</cell><cell>7.90</cell><cell>7.70</cell></row><row><cell>Ophthalmology</cell><cell>8.50</cell><cell>6.00</cell><cell>7.60</cell><cell>8.40</cell></row><row><cell>Neurology</cell><cell>8.80</cell><cell>5.90</cell><cell>9.00</cell><cell>8.10</cell></row><row><cell>Neurosurgery</cell><cell>8.80</cell><cell>4.20</cell><cell>8.10</cell><cell>8.40</cell></row><row><cell>Psychiatry</cell><cell>8.70</cell><cell>5.80</cell><cell>8.20</cell><cell>7.70</cell></row><row><cell>Orthopedics</cell><cell>8.80</cell><cell>5.90</cell><cell>8.30</cell><cell>8.00</cell></row><row><cell>Overall</cell><cell>8.72</cell><cell>5.63</cell><cell>8.42</cell><cell>7.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Scores for each category and the overall scores of each model pair on 100 multi-turn dialogues reviewed by GPT-4.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 ,</head><label>8</label><figDesc>which reveals that HuatuoGPT excels in extended dialogue contexts, evidenced by an 86% win rate against DoctorGLM and 58% against ChatGPT. It indicates that HuatuoGPT has a more prominent interactive diagnostic capability in patient consultation scenarios.</figDesc><table><row><cell>Comparison</cell><cell>Win</cell><cell>Lose</cell></row><row><cell>HuatuoGPT vs BenTsao</cell><cell>100%</cell><cell>0%</cell></row><row><cell>HuatuoGPT vs DoctorGLM</cell><cell>98%</cell><cell>2%</cell></row><row><cell>HuatuoGPT vs ChatGPT</cell><cell>52%</cell><cell>48%</cell></row><row><cell>HuatuoGPT vs GPT-4</cell><cell cols="2">10.5% 89.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Manual evaluation of model performance in single-turn medical consultations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Sample responses from four models. dataset based on the LLaMA model. The HealthCareMagic-100k dataset consists of 100k real-world patient-physician conversations from an online medical consultation site. ChatDoctor has autonomous knowledge retrieval capabilities by accessing real-time and authoritative information and answering patient questions based on databases such as Wikipedia to improve the accuracy of the model's response. Baize-healthcare 18 is a variant of Baize that is fine-tuned on Medical data (Quora Dialogs and Medical Dialogs). The technique report associated with it has not been published, resulting in limited details being available, as only the model weights were released. Visual Med-Alpaca 19 is</figDesc><table><row><cell>Model Name</cell><cell>Access</cell><cell cols="2">Language Base Model</cell><cell>Training Dataset</cell><cell>Dataset Size</cell></row><row><cell>Med-PaML2</cell><cell>-</cell><cell>English</cell><cell>PaML2 (340B)</cell><cell>MultiMedQA</cell><cell>193k</cell></row><row><cell>ChatDoctor</cell><cell cols="2">Need request English</cell><cell>LLaMA (7B)</cell><cell>HealthCareMagic-100K + iCliniq-10K</cell><cell>110K</cell></row><row><cell cols="3">Visual Med-Alpaca Need request English</cell><cell>LLaMA (7B)</cell><cell>various medical QA datasets</cell><cell>54K</cell></row><row><cell>MEDALPACA</cell><cell>Access</cell><cell>English</cell><cell cols="2">LLaMA (7B/13B) Medical Meadow</cell><cell>160K</cell></row><row><cell>Baize-healthcare</cell><cell>Access</cell><cell>English</cell><cell>LLaMA (7B)</cell><cell>Quora Dialogs + Medical Dialogs</cell><cell>101K</cell></row><row><cell>DoctorGLM</cell><cell>Access</cell><cell>Chinese</cell><cell>ChatGLM (6B)</cell><cell cols="2">CMD. + MedDialog + ChatDoctor + HearlthcareMagic 4.4M</cell></row><row><cell>BenTsao</cell><cell>Access</cell><cell>Chinese</cell><cell>LLaMA (7B)</cell><cell>CMeKG-8K</cell><cell>8K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>The Popular Medical Language Models Training Data Sources and Model Specifications. fine-tuned on LLaMA-7B model using a model-generated dataset comprising of manual filtering 54,000 biomedical examples for instruction-tuning purposes, plus the fine-tuned Microsoft GIT model on the Radiology Objects in Context (ROCO) dataset to incorporate visual modality. Recently, Med-PaLM2 [26] was published, which is based on PaLM2 and finetuned in MultiMedQA for Expert-Level Medical Question Answering. In Chinese, DoctorGLM [27] 20 is a Chinese Medical LLM trained on Multiple Medical QA datasets based on ChatGLM. It utilizes the training data from ChatDoctor through translation and incorporates Chinese medical dialogues encompassing five departments' QA and MedDialog chat data as part of the training data. BenTsao [28] 21 is a knowledge-enhanced Chinese Medical LLM trained on over 8K instructions. The instruction is generated from CMeKG 22 by ChatGPT API. MedicalGPTzh [29] is a Chinese medical general model based on ChatGLM-6B LoRA with 16-bit instruction fine-tuning. The dataset for training the model was obtained from Chinese medical knowledge question-and-answer pairs and clinical guideline texts from 28 medical departments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>A.1 Distilled Instructions from ChatGPTFollowing previous work, we use self-instruction to generated the instructions from ChatGPT with the medical seed instructions we manually build and the prompt is shown below:</figDesc><table><row><cell>??????20????&lt;????????&gt;???????????????????</cell></row><row><cell>?GPT????????????????????????????</cell></row><row><cell>???</cell></row><row><cell>1.??????????????????????????????????????</cell></row><row><cell>??"??????"?</cell></row><row><cell>2.??????????????????????????????????????</cell></row><row><cell>?????????????????GPT?????????????????????</cell></row><row><cell>???????????; ?????1?2????????????????????</cell></row><row><cell>?;???????????placeholder???"????"??"??"?"??"????</cell></row><row><cell>??</cell></row><row><cell>3.??????????????????????????????????????</cell></row><row><cell>??????????????????????????,???????"????"?</cell></row><row><cell>??????????????????200??</cell></row><row><cell>4.??????????????????????????????????????</cell></row><row><cell>?????????? ??????</cell></row><row><cell>20??????????</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Assuming you are an experienced [Medical Role], you will provide very patient and comprehensive answers to patients, and your tone will be gentle and friendly, which is very popular with patients. If the patient does not provide you with enough information to make a judgment, you will ask them relevant questions. And at the end of the diagnosis, you will also give him some additional advice. If the patient asks: {Question} you will answer:A.2 Real-world Instructions from DoctorsIn the experiment, we collect real-world question answering data from web and sample a set of high quality question-answering pairs used for training. Every pair is refined by LLMs. The prompt is shown below:You are HuatuoGPT AI model, based on the patient's question, please refer to the response and then give a reply to the patient's question, speak like a doctor and have a gentle and kind tone, reply to the patient's query in a more detailed and helpful way, and ask the patient for more information if necessary if you cannot clearly diagnose the patient's disease. Real-world Conversations with Doctors We show prompts used for patient LLM and doctor LLM. Prompt for patient LLM:</figDesc><table><row><cell>{Question}</cell></row><row><cell>????????</cell></row><row><cell>The translation is as below:</cell></row><row><cell>&lt;????&gt;:{Patient_Question}</cell></row><row><cell>&lt;????&gt;:{Doctor_Response}</cell></row><row><cell>??HuatuoGPT????????????????????????????????</cell></row><row><cell>???????????????????????????????????????</cell></row><row><cell>?????????????????????????????</cell></row><row><cell>&lt;HuatuoGPT??&gt;:</cell></row><row><cell>The translation is as below:</cell></row><row><cell>&lt;Patient Question&gt;:{Patient_Question}</cell></row><row><cell>&lt;Response Reference&gt;:{Doctor_Response}</cell></row><row><cell>&lt;HuatuoGPT Response&gt;:</cell></row><row><cell>A.3 ???????????????????HuatuoGPT????????????????</cell></row><row><cell>???????????????????????????????????????</cell></row><row><cell>??</cell></row><row><cell>{medical_case}</cell></row><row><cell>????????????????????</cell></row><row><cell>Prompt for doctor LLM:</cell></row></table><note><p>Role]????????????????????? ??????????????????????????????????????? ???????????????????????????????: ??????????????????????????????????????? ??????????????????????????????????????? ???????????????????????????????????????</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://en.wikipedia.org/wiki/List_of_Chinese_administrative_divisions_by_life_ expectancy</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://en.wikipedia.org/wiki/Hua_Tuo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Interestingly, ChatGPT does not has a sense of time and life, it does not need to save time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://huggingface.co/docs/accelerate/index</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/CarperAI/trlx</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>We adopt BLOOMZ-7b1-mt as our backbone, which stands out with its exceptional multilingual capabilities and suitable for open-source applications. BLOOMZ model family is trained with the PILE corpus<ref type="bibr" target="#b15">[16]</ref>, which contains varied medical texts, including resources like PubMed Central and PubMed Abstracts. These valuable texts significantly enrich the BLOOMZ models with an extensive body of medical knowledge, subsequently enables our models to perform better in the medical domain.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>https://huggingface.co/datasets/philschmid/sharegpt-raw</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://huggingface.co/imxly/t5-pegasus</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>https://github.com/CBLUEbenchmark/CBLUE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>ChatGPT and GPT-4 version is the online one on</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_10"><p>12th May 2023<ref type="bibr" target="#b11">12</ref> https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main 13 https://github.com/xionghonglin/DoctorGLM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11"><p>https://www.mosaicml.com/blog/introducing-pubmed-gpt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_12"><p>https://ftp.ncbi.nlm.nih.gov/pubmed/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_13"><p>https://github.com/kbressem/medAlpaca</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_14"><p>https://github.com/Kent0n-Li/ChatDoctor</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_15"><p>https://huggingface.co/project-baize/baize-healthcare-lora-7B</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_16"><p>https://github.com/cambridgeltl/visual-med-alpaca</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Prof. Zhi-Quan Luo</rs> and <rs type="person">Dr. Ping Li</rs> for their support in SRIBD.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majia</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuseng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Z-bench</title>
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10453</idno>
		<title level="m">Democratizing chatgpt across languages</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandipan</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Mckinnon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08073</idno>
		<title level="m">Constitutional ai: Harmlessness from ai feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Task-oriented dialogue system for automatic diagnosis</title>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianlong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiao</forename><surname>Tou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Dai</surname></persName>
		</author>
		<ptr target="https://github.com/xionghonglin/DoctorGLM21https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese22https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="207" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end knowledgerouted relational dialogue system for automatic diagnosis</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7346" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial regularized mutual information policy gradient framework for automatic diagnosis</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1062" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meddialog: Large-scale medical dialogue datasets</title>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenmian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqian</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruisi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9241" to="9250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dialogpt: Large-scale generative pre-training for conversational response generation</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00536</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pangubot: Efficient generative dialogue pre-training from pre-trained language model</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.17090</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Self-instruct: Aligning language model with self generated instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10560</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Crosslingual generalization through multitask finetuning</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<editor>Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Xiangru Tang, Dragomir Radev, Alham Fikri Aji</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03277</idno>
		<title level="m">Instruction tuning with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale attentive interaction networks for chinese medical question answer selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="74061" to="74071" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Applying deep matching networks to chinese medical question answering: a study and a dataset</title>
		<author>
			<persName><forename type="first">Junqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manshu</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medical informatics and decision making</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Jianquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prayag</forename><surname>Tiwari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01526</idno>
		<title level="m">Xiang Wan, and Benyou Wang. Huatuo-26m, a large-scale chinese medical qa dataset</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangping</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08087</idno>
		<title level="m">A chinese biomedical language understanding evaluation benchmark</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">BioGPT: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-09">sep 2022</date>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Medalpaca -an open-source collection of medical conversational ai models and training data</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens-Michalis</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Oberhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>L?ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Truhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keno</forename><forename type="middle">K</forename><surname>Bressem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge</title>
		<author>
			<persName><forename type="first">Yunxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruilong</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards expert-level medical question answering with large language models</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juraj</forename><surname>Gottweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rory</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellery</forename><surname>Wulczyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darlene</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Lachgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushant</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewa</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Aguera Y Arcas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenad</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renee</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Semturs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Barral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Doctorglm: Fine-tuning your chinese doctor is not a herculean task</title>
		<author>
			<persName><forename type="first">Honglin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Haochun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuwa</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Huatuo</surname></persName>
		</author>
		<title level="m">Tuning llama model with chinese medical knowledge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Hongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/MediaBrain-SJTU/MedicalGPT-zh" />
	</analytic>
	<monogr>
		<title level="j">Medicalgpt-zh????? ??????</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
