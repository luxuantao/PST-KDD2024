<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">with Correlated Sources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-10-03">3 Oct 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">João</forename><surname>Barros</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sergio</forename><forename type="middle">D</forename><surname>Servetto</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Network Information Flow</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute for Communications Engineering</orgName>
								<orgName type="institution">Munich University of Technology</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Porto</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">with Correlated Sources</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-10-03">3 Oct 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">D6DFD1DCE010C2C0FDE9D7F58090DBD3</idno>
					<idno type="arXiv">arXiv:cs.IT/0504014v2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consider the following network communication setup, originating in a sensor networking application we refer to as the "sensor reachback" problem. We have a directed graph G = (V, E), where</p><p>The channels are independent. Each node v i gets to observe a source of information U i (i = 0...M ), with joint distribution p(U 0 U 1 ...U M ). Our goal is to solve an incast problem in G: nodes exchange messages with their neighbors, and after a finite number of communication rounds, one of the M + 1 nodes (v 0 by convention) must have received enough information to reproduce the entire field of observations (U 0 U 1 ...U M ), with arbitrarily small probability of error. In this paper, we prove that such perfect reconstruction is possible if and only if</p><p>for all S ⊆ {0...M }, S = ∅, 0 ∈ S c . Our main finding is that in this setup a general source/channel separation theorem holds, and that Shannon information behaves as a classical network flow, identical in nature to the flow of water in pipes. At first glance, it might seem surprising that separation holds in a fairly general network situation like the one we study. A closer look, however, reveals that the reason for this is that our model allows only for independent point-to-point channels between pairs of nodes, and not multiple-access and/or broadcast channels, for which separation is well known not to hold [5, pp. 448-49]. This "information as flow" view provides an algorithmic interpretation for our results, among which perhaps the most important one is the optimality of implementing codes using a layered protocol stack.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Sensor Reachback Problem</head><p>Wireless sensor networks made up of small, cheap, and mostly unreliable devices equipped with limited sensing, processing and transmission capabilities, have recently sparked a fair amount of interest in communications problems involving multiple correlated sources and large-scale wireless networks <ref type="bibr" target="#b5">[6]</ref>. It is envisioned that an important class of applications for such networks involves a dense deployment of a large number of sensors over a fixed area, in which a physical process unfolds-the task of these sensors is then to collect measurements, encode them, and relay them to some data collection point where this data is to be analyzed, and possibly acted upon. This scenario is illustrated in Fig. <ref type="figure" target="#fig_6">1</ref>. Fig. <ref type="figure" target="#fig_6">1</ref>. A large number of sensors is deployed over a target area. After collecting the data of interest, the sensors must reach back and transmit this information to a single receiver (e.g., an overflying plane) for further processing.</p><p>There are several aspects that make this communications problem interesting:</p><p>• Correlated Observations: If we have a large number of nodes sensing a physical process within a confined area, it is reasonable to assume that their measurements are correlated. This correlation may be exploited for efficient encoding/decoding.</p><p>• Cooperation among Nodes: Before transmitting data to the remote receiver, the sensor nodes may establish a conference to exchange information over the wireless medium and increase their efficiency or flexibility through cooperation.</p><p>• Channel Interference: If multiple sensor nodes use the wireless medium at the same time (either for conferencing or reachback), their signals will necessarily interfere with each other. Consequently, reliable communication in a reachback network requires a set of rules that control (or exploit) the interference in the wireless medium.</p><p>In order to capture some of these key aspects, while still being able to provide complete results, we make some modeling assumptions, discussed next. <ref type="bibr" target="#b35">October 2, 2005</ref>. DRAFT 1) Source Model: We assume that the sources are memoryless, and thus consider only the spatial correlation of the observed samples and not their temporal dependence (since the latter dependencies could be dealt with by simple extensions of our results to the case of ergodic sources). Furthermore, each sensor node v i observes only one component U i and must transmit enough information to enable the sink node v 0 to reconstruct the whole vector U 1 U 2 . . . U M . This assumption is the most natural one to make for scenarios in which data is required at a remote location for fusion and further processing, but the data capture process is distributed, with sensors able to gather local measurements only, and deeply embedded in the environment.</p><p>A conceptually different approach would be to assume that all sensor nodes get to observe independently corrupted noisy versions of one and the same source of information U , and it is this source (and not the noisy measurements) that needs to be estimated at a remote location. This approach seems better suited for applications involving non-homogeneous sensors, where each one of the sensors gets to observe different characteristics of the same source (e.g., multispectral imaging), and therefore leads to a conceptually very different type of sensing applications from those of interest in this work. Such an approach leads to the so called CEO problem studied by Berger, Zhang and Viswanathan in <ref type="bibr" target="#b6">[7]</ref>.</p><p>2) Independent Channels: Our motivation to consider a network of independent DMCs is twofold.</p><p>From a pure information-theoretic point of view independent channels are interesting because, as shown in this paper, this assumption gives rise to long Markov chains which play a central role in our ability to prove the converse part of our coding theorem, and thus obtain conclusive results in terms of capacity.</p><p>Moreover, a corollary of said coding theorem does provide a conclusive answer for a special case of the multiple access channel with correlated sources, a problem for which no general converse is known.</p><p>From a more practical point of view, the assumption of independent channels is valid for any network that controls interference by means of a reservation-based medium-access control protocol (e.g., TDMA).</p><p>This option seems perfectly reasonable for sensor networking scenarios in which sensors collect data over extended periods of time, and must then transmit their accumulated measurements simultaneously. In this case, a key assumption in the design of standard random access techniques for multiaccess communication breaks down-the fact that individual nodes will transmit with low probability <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">Chapter 4</ref>]. As a result, classical random access would result in too many collisions and hence low throughput. Alternatively, instead of mitigating interference, a medium access control (MAC) protocol could attempt to exploit it, in the form of using cooperation among nodes to generate waveforms that add up constructively at the receiver (cf. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>). Providing an information-theoretic analysis of such cooperation mechanisms would be very desirable, but since it entails dealing with correlated sources and a general multiple access channel, dealing with correlated sources and an array of independent channels constitutes a reasonable first step towards <ref type="bibr" target="#b35">October 2, 2005</ref>. DRAFT that goal, and is also interesting in its own right, since it provides the ultimate performance limits for an important class of sensor networking problems.</p><p>3) Perfect Reconstruction at the Receiver: In our formulation of the sensor reachback problem, the far receiver is interested in reconstructing the entire field of sensor measurements with arbitrarily small probability of error. This formulation leads us to a natural capacity problem, in the classical sense of Shannon.</p><p>Alternatively, one could relax the condition of perfect reconstruction, and tolerate some distortion in the reconstruction of the field of measurements at the far receiver, thus leading to the so called Multiterminal Source Coding problem studied by Berger <ref type="bibr" target="#b11">[12]</ref>. This condition could be further relaxed, to require a faithful reproduction of the image of some function f of the sources, leading to a problem studied extensively by Csiszar, Körner and Marton <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. An Information Theoretic View of Architectural Issues</head><p>For large-scale, complex systems of the type of interest in this work, the complexity of basic questions of design and performance analysis appears daunting:</p><p>• How should nodes cooperate to relay messages to the data collector node v 0 ? Should they decode received messages, re-encode them, and forward to other nodes? Should they map channel outputs to channel inputs without attempting to decode? Should they do something else?</p><p>• How should redundancy among the sources be exploited? Should we compress the information as much as possible? Should we leave some of that redundancy to combat noise in the channels? Is there a source/channel separation theorem in these networks?</p><p>• How do we measure performance of these networks, what are appropriate cost metrics? How do we design networks that are efficient under an appropriate cost metric?</p><p>In <ref type="bibr" target="#b14">[15]</ref>, a number of examples are identified in which the existence of a simple architecture has played an enabling role in the proliferation of technology: the von Neuman computer architecture, separation of source and channel coding in communications, separation of plant and controller in control systems, and the OSI layered architecture model. So what all these questions boil down to is an issue similar to those considered in <ref type="bibr" target="#b14">[15]</ref>: what are appropriate abstractions of the network, similar to the IP protocol stack for the Internet, based on which we can break the design task into independent reusable components, optimize the design of these components, and obtain an efficient system as a result? In this work, we show how information theory is indeed capable of providing very meaningful answers to this problem.</p><p>Information theory, in one of its applications, deals with the analysis of performance of communication systems. So, to some it may seem the natural theory to turn to for guidance in the task of searching for October 2, 2005. DRAFT a suitable network architecture. However, to others it may seem unnatural to do so: it is well known that information theory and communication networks have not had fruitful interactions in the past, as explained by Ephremides and Hajek <ref type="bibr" target="#b15">[16]</ref>. Thus, in the presence of these mixed indicators, we take the stand that indeed information theory has a great deal to offer in the task at hand. And to justify our position, consider Shannon's model for a communications system, as illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>. For this setup, Shannon established that reliable communication of a source over a noisy channel is possible if and only if the entropy rate of the source is less than the capacity of the channel <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Ch. 8.13]</ref>. This result, known as the source/channel separation theorem, has a double significance. On one hand, it provides an exact single-letter characterization of conditions under which reliable communication is possible. On the other hand, and of particular interest to the task at hand for us, it is a statement about the architecture of an optimal communication system: the encoder/decoder design task can be split into the design and optimization of two independent components. So it is inspired by Shannon's teachings for point-to-point systems that we ask in this work, and answer in the affirmative, the question of whether it is possible or not to derive similar useful architectural guidelines for the class of networks under consideration.</p><formula xml:id="formula_0">Noise ÖÖÓÖ ÓÒØÖÓÐ Ó Ò ÖÖÓÖ ÓÖÖ Ø ÓÒ ÒÒ Ð ´Ô´Ý Üµµ Ò Ò ËÓÙÖ Ê ÓÒ×ØÖÙØ ËÓÙÖ Í Ò Ø ÓÑÔÖ ×× ÓÒ Ø ¹ ÓÑÔÖ ×× ÓÒ Ø× ËÓÙÖ ÒÓ Ö ÒÒ Ð Ó Ö Í× Ö Í Ò Ò Ò Í Ò ÒÓ Ö Ó Ö</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Related Work</head><p>The problem of communicating distributed correlated sources over a network of point-to-point links is closely related to several classical problems in network information theory. To set the stage for the main contributions of this paper, we now review related previous work.</p><p>October <ref type="bibr" target="#b1">2,</ref><ref type="bibr">2005</ref>. DRAFT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Distributed Correlated Sources and Multiple Access:</head><p>The concept of separate encoding of correlated sources was studied by Slepian and Wolf in their seminal paper <ref type="bibr" target="#b16">[17]</ref>, where they proved that two correlated sources (U 1 U 2 ) drawn i.i.d. ∼ p(u 1 u 2 ) can be compressed at rates (R 1 , R 2 ) if and only if</p><formula xml:id="formula_1">R 1 ≥ H(U 1 |U 2 ) R 2 ≥ H(U 2 |U 1 ) R 1 + R 2 ≥ H(U 1 U 2 ).</formula><p>Assume now that (U 1 U 2 ) are to be transmitted with arbitrarily small probability of error to a joint receiver over a multiple access channel with transition probability p(y|x 1 x 2 ). Knowing that the capacity of the multiple access channel with independent sources is given by the convex hull of the set of points</p><formula xml:id="formula_2">(R 1 , R 2 ) satisfying [5, Ch. 14.3] R 1 &lt; I(X 1 ; Y |X 2 ) R 2 &lt; I(X 2 ; Y |X 1 ) R 1 + R 2 &lt; I(X 1 X 2 ; Y ),</formula><p>it is not difficult to prove that Slepian-Wolf source coding of (U 1 U 2 ) followed by separate channel coding yields the following sufficient conditions for reliable communication</p><formula xml:id="formula_3">H(U 1 |U 2 ) &lt; I(X 1 ; Y |X 2 ) H(U 2 |U 1 ) &lt; I(X 2 ; Y |X 1 ) H(U 1 U 2 ) &lt; I(X 1 X 2 ; Y ).</formula><p>These conditions, which basically state that the Slepian-Wolf region and the capacity region of the multiple access channel have a non-empty intersection, are sufficient but not necessary for reliable communication, as shown by Cover, El Gamal, and Salehi with a simple counterexample in <ref type="bibr" target="#b17">[18]</ref>. In that same paper, the authors introduce a class of correlated joint source/channel codes, which enables them to increase the region of achievable rates to</p><formula xml:id="formula_4">H(U 1 |U 2 ) &lt; I(X 1 ; Y |X 2 U 2 )<label>(1)</label></formula><formula xml:id="formula_5">H(U 2 |U 1 ) &lt; I(X 2 ; Y |X 1 U 1 )<label>(2)</label></formula><formula xml:id="formula_6">H(U 1 U 2 ) &lt; I(X 1 X 2 ; Y ),<label>(3)</label></formula><p>for some p(u</p><formula xml:id="formula_7">1 u 2 x 1 x 2 y) = p(u 1 u 2 ) • p(x 1 |u 1 ) • p(x 2 |u 2 ) • p(y|x 1 x 2 )</formula><p>. Also in <ref type="bibr" target="#b17">[18]</ref>, the authors generalize this set of sufficient conditions to sources (U 1 U 2 ) with a common part W = f (U 1 ) = g(U 2 ), but they were not <ref type="bibr" target="#b35">October 2, 2005</ref>. DRAFT able to prove a converse, i.e., they were not able to show that their region is indeed the capacity region of the multiple access channel with correlated sources. Later, it was shown with a carefully constructed example by Dueck in <ref type="bibr" target="#b18">[19]</ref> that indeed the region defined by eqns. ( <ref type="formula" target="#formula_4">1</ref>)-( <ref type="formula" target="#formula_6">3</ref>) is not tight. Related problems were considered by Slepian and Wolf <ref type="bibr" target="#b19">[20]</ref>, and Ahlswede and Han <ref type="bibr" target="#b20">[21]</ref>. To this date however, the general problem still remains open.</p><p>Assuming independent sources, Willems investigated a cooperative scenario, in which encoders exchange messages over conference links of limited capacity prior to transmission over the multiple access channel <ref type="bibr" target="#b21">[22]</ref>.</p><p>In this case, the capacity region is given by</p><formula xml:id="formula_8">R 1 &lt; I(X 1 ; Y |X 2 Z) + C 12 R 2 &lt; I(X 2 ; Y |X 1 Z) + C 21 R 1 + R 2 &lt; min{ I(X 1 X 2 ; Y |Z) + C 12 + C 21 , I(X 1 X 2 ; Y ) },</formula><p>for some auxiliary random variable</p><formula xml:id="formula_9">Z such that |Z| ≤ min(|X 1 |•|X 2 |+2, |Y|+3</formula><p>), and for a joint distribution</p><formula xml:id="formula_10">p(zx 1 x 2 y 1 y 2 ) = p(z) • p(x 1 |z) • p(x 2 |z) • p(y|x 1 x 2 ).</formula><p>2) Correlated Sources and Networks of DMCs: Very recently, an early paper was brought to our attention, in which Han considers the transmission of correlated sources to a common sink over a network of independent channels <ref type="bibr" target="#b22">[23]</ref>. Although the problem setup is less general than ours, in that (a) each source block and each transmitted codeword partipate only once in the encoding process, and (b) the intermediate nodes are assumed to decode the data before passing it on, Theorem 3.1 of <ref type="bibr" target="#b22">[23]</ref> is very similar to our Theorem 1.</p><p>Our work, done independently of Han's, differs from it and complements it in the following ways:</p><p>• Our setup is more general. We allow for arbitrary forms of joint source-channel coding to take place inside the network while data flows towards the decoder, and then prove that a one-step encoding process, pure routing, and separate source/channel coding are sufficient. Han assumes decode-and-forward in his problem statement, as well as a one-step encoding process.</p><p>• The proof techniques are different. Han takes a purely combinatorial approach to the problem: he thoroughly exploits the polymatroidal structure of the capacity function for the network of channels, and the co-polymatroidal structure for the Slepian-Wolf region. We establish our achievability result by explicitly constructing a routing algorithm for the Slepian-Wolf indices, and our converse by standard methods based on Fano's inequality.</p><p>Furthermore our work, being motivated by a concrete sensor networking application, establishes connections and relevance to practical engineering problems (see Section III) that are not a concern in <ref type="bibr" target="#b22">[23]</ref>.</p><p>3) Network Coding: Another closely related problem is the well known network coding problem, introduced by Ahlswede, Cai, Li and Yeung <ref type="bibr" target="#b23">[24]</ref>. In that work, the authors establish the need for applying coding operations at intermediate nodes to achieve the max-flow/min-cut bound of a general multicast network. A converse proof for this problem was provided by Borade <ref type="bibr" target="#b24">[25]</ref>. Linear codes were proposed by Li, Yeung and Cai in <ref type="bibr" target="#b25">[26]</ref>, and Koetter and Médard in <ref type="bibr" target="#b26">[27]</ref>.</p><p>Effros, Médard et al. have developed a comprehensive study on separate and joint design of linear source, channel and network codes for networks with correlated sources under the assumption that all operations are defined over a common finite field <ref type="bibr" target="#b27">[28]</ref>. For this particular case, optimality of separate linear source and channel coding was observed in the one-receiver instance, but the result of <ref type="bibr" target="#b27">[28]</ref> does not prove that it holds for general networks and channels with arbitrary input and output alphabets. Error exponents for multicasting of correlated sources over a network of noiseless channels were given by Ho, Médard et al.</p><p>in <ref type="bibr" target="#b28">[29]</ref>, and networks with undirected links were considered by Li and Li in <ref type="bibr" target="#b29">[30]</ref>.</p><p>Another problem in which network flow techniques have been found useful is that of finding the maximum stable throughput in certain networks. In this problem, posed by Gupta and Kumar in <ref type="bibr" target="#b30">[31]</ref>, it is sought to determine the maximum rate at which nodes can inject bits into a network, while keeping the system stable.</p><p>This problem was reformulated by Peraki and Servetto as a multicommodity flow problem, for which tight bounds were obtained using elementary counting techniques <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Main Contributions and Organization of the Paper</head><p>Our main original contributions can be summarized as follows:</p><p>• A general coding theorem yielding necessary and sufficient conditions for reliable communication of M + 1 correlated sources to a common sink over a network of independent DMCs.</p><p>• An achievability proof which combines classical coding arguments with network flow methods and a converse proof that establishes the optimality of separate source and channel coding.</p><p>• A detailed discussion on the engineering implications of our main result, and the concepts of informationtheoretically optimal network architectures and protocol stacks.</p><p>The rest of the paper is organized as follows. In Section II we give formal definitions, to then state and prove our main theorem. We also look at three special cases: a network with three nodes, the non-cooperative case, and an array of orthogonal Gaussian channels. In Section III we address the practical implications of our main result, by describing an information-theoretically optimal protocol stack, elaborating on the tractability of related network architecture and network optimization problems, and discussing the suboptimality of correlated codes for orthogonal channels. The paper concludes with Section IV.</p><p>October 2, 2005. DRAFT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. A CODING THEOREM FOR NETWORK INFORMATION FLOW WITH CORRELATED SOURCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Formal Definitions and Statement of the Main Theorem</head><p>A network is modeled as the complete graph on M + 1 nodes. For each</p><formula xml:id="formula_11">(v i , v j ) ∈ E (0 ≤ i, j ≤ M ), there is a discrete memoryless channel (X ij , p ij (y|x), Y ij ), with capacity C ij = max pij(x) I(X ij ; Y ij ). 1 At each node v i ∈ V , a random variable U i is observed (i = 0...M ), drawn i.i.d. from a known joint distribution p(U 0 U 1 ...U M )</formula><p>. Node v 0 is the decoder -the goal in this problem is to find conditions under which U 1 ...U M can be reproduced reliably at v 0 . We now make this statement more precise, by describing how the nodes communicate and by giving the formal definitions of code, probability of error and reliable communication.</p><p>Time is discrete. Every N time steps, node v i collects a block U N i of source symbols -we refer to the collection of all blocks</p><formula xml:id="formula_12">[U N 0 (k)U N 1 (k)...U N M (k)] collected at time kN (k ≥ 1)</formula><p>as a block of snapshots. Node v i then sends a codeword X N ij to node v j . This codeword depends on a window of K previous blocks of source sequences U N i observed at node v i , and of T previously received blocks of channel outputs, corresponding to noisy versions of the codewords sent by all nodes to node v i in the previous T communications steps (corresponding to N T time steps).</p><p>For a block of snapshots observed at time kN , at time (k + W )N (that is, after allowing for a finite but otherwise arbitrary amount of time to elapse,<ref type="foot" target="#foot_2">2</ref> in which the information injected by all nodes reaches v 0 ), an attempt is made to decode at v 0 . The decoder produces an estimate of the block of snapshots</p><formula xml:id="formula_13">U N 0 (k)U N 1 (k)...U N M (k) based on the local observations U N 0 (k)</formula><p>, and the previous W blocks of N channel outputs generated by codewords sent to v 0 by the other nodes.</p><p>Thus, a code for this network consists of:</p><p>• four integers N , K, T and W ;</p><p>• encoding functions at each node</p><formula xml:id="formula_14">g ij : K l=1 U N i × T t=1 M m=0 Y N mi -→ X N ij ,</formula><p>for 0 ≤ i, j ≤ M .</p><p>• the decoding function at node v 0 :</p><formula xml:id="formula_15">h : U N 0 × W w=1 M m=1 Y N m0 -→ M m=1 ÛN m .</formula><p>• the block probability of error:</p><formula xml:id="formula_16">P (N ) e = P (U N 1 ...U N M = Û N 1 ... Û N M ).</formula><p>We say that blocks of snapshots U N 1 ...U N M can be reliably communicated to v 0 if there exists a sequence of codes as above, with P (N ) e → 0 as N → ∞, for some finite values K, T and W , all independent of N .</p><p>With these definitions, we are now ready to state our main theorem.</p><p>Theorem 1: Let S denote a non-empty subset of node indices that does not contain node 0: S ⊆ {0...M }, S = ∅, 0 ∈ S c . Then, it is possible to communicate U 1 ...U M reliably to v 0 if and only if, for all S as above,</p><formula xml:id="formula_17">H(U S |U S c ) &lt; i∈S,j∈S c C ij .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Achievability Proof</head><p>Our coding strategy is based on separate source and channel coding. We first use capacity attaining channel codes to turn the noisy network into a network of noiseless links (of capacity C ij ). Then, we use Slepian-Wolf source codes, jointly with a custom designed routing algorithm, to deliver all this data to destination.</p><p>Since the channel coding aspects of the proof are rather straightforward extensions of classical point-to-point arguments, in the following we only focus on the less obvious source coding and routing aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Mechanics of the Coding Strategy:</head><p>Consider a "noise-free" version of the problem formulated above:</p><p>we still have a complete graph, now with noiseless links of capacity C ij . Variables U i are still observed at each node v i , and the goal remains to reproduce all of these at v 0 . Each node uses a classical Slepian-Wolf code: there is a source encoder at node v i that maps a sequence U N i to an index from the random binning set {1, 2, . . . , 2 N Ri }, thus compressing the block of observations U N i using codes as in [5, Thm. 14.4.2]. Let (R 1 ...R M ) denote the rate allocation to each of the nodes. To achieve perfect reconstruction, these bits must be delivered to node v 0 .</p><p>• Set K = T = 1 -each block of source symbols and each block of codewords participates in the encoding process only once.</p><p>• To deliver the bin indices produced by the Slepian-Wolf codes to destination, the noise-free network is regarded as a flow network <ref type="bibr" target="#b33">[34,</ref><ref type="bibr">Ch. 26</ref>]. Let ϕ(v i , v j ) be a feasible flow in this network, with M sources v 1 ...v M , supply R i at source v i , and a single sink v 0 . If no such feasible flow exists, the code construction fails.</p><p>• If there is a feasible flow ϕ then this ϕ uniquely determines, at each node v i , the number of bits that need to be sent to each of its neighbors -thus from ϕ we derive the encoding functions g ij as follows:</p><p>-Consider the directed acyclic graph G ′ of G induced by ϕ, by taking V (G ′ ) = V (G), and</p><formula xml:id="formula_18">E(G ′ ) = {(v i , v j ) ∈ E : ϕ(v i , v j ) &gt; 0}. Define a permutation π : {0...M } → {0...M }, such that [v π(0) v π(1) ...v π(M )</formula><p>] is a topological sort of the nodes in G, as illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>. </p><formula xml:id="formula_19">v 1 ...v M such that if (v i , v j ) is an edge, then i &lt; j. -Consider a block of snapshots U(k) = [U N 0 (k)U N 1 (k)...U N M (k)] captured at time kN . At time (k + l)N (for l = 0...M ), node v π(l)</formula><p>will have received all bits with portions of the encodings of U(k) generated by nodes upstream in the topological order -thus, together with its own encoding of U N π(l) (k), all the bits for U(k) up to and including node v π(l) will be available there, and thus can be routed to nodes downstream in the topological order.</p><p>-Consider now all edges of the form</p><formula xml:id="formula_20">(v π(k) , v ′ ) for which ϕ(v π(k) , v ′ ) &gt; 0: 1) Collect the m = v ′ ϕ(v ′ , v π(k)</formula><p>) information bits sent by the upstream nodes v ′ .</p><p>2) Consider now the set of all downstream nodes v ′′ , for which</p><formula xml:id="formula_21">ϕ(v π(k) , v ′′ ) &gt; 0. Due to flow conservation for ϕ, v ′′ ϕ(v π(k) , v ′′ ) = m + R π(k) , where R π(k) is the rate allocated to node v π(k) .</formula><p>3) For each v ′′ as above, define g To illustrate the operations performed at each node. In this example, five bits come into node v π(k) from neighbouring nodes, two on the top link and three on the bottom link. The information bits from other nodes come in the form of noisy codewords -they need to be decoded from the received channel outputs. Now, because flow conservation holds for ϕ, we know that the aggregate capacity of the three output links will be at least five bits plus some local bits (the encoding of a block of local observations U N π(k) , denoted by b 6 and b 7 here). So at this point we split those bits in a way such that the individual capacity constraints of the output links are not violated, and then they are sent on their way to v 0 .</p><formula xml:id="formula_22">(k) π(k)v ′′ to be a message such that |g (k) π(k)v ′′ | = ϕ(v π(k) , v ′ ). Partition the m + R π(k) available</formula><p>-Perform typical set decoding (as in <ref type="bibr">[5, pg. 411]</ref>), to recover the block of snapshot</p><formula xml:id="formula_23">[U N 1 (k)...U N M (k)].</formula><p>An important observation is that, in this setup, network coding (in the sense of <ref type="bibr" target="#b23">[24]</ref>) is not needed. This is because we have a case of M sources and a single sink interested in collecting all messages, a case for which it was shown in <ref type="bibr" target="#b34">[35]</ref> that routing alone suffices.</p><p>Our next task is to find conditions under which this coding strategy results in P (N ) e → 0 as N → ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Analysis of the Probability of Error:</head><p>The coding strategy proposed above hinges on two main elements:</p><p>• Slepian-Wolf codes: in this case, we know that provided the rate vector (R</p><formula xml:id="formula_24">1 ...R M ) is such that, for all partitions S of {0...M }, S = ∅, 0 ∈ S c , i∈S R i &gt; H(U S |U S c ),<label>(5)</label></formula><p>then there exist Slepian-Wolf codes with arbitrarily low probability of error <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Ch. 14.4</ref>].</p><p>• Network flows: from elementary flow concepts we know that if a flow ϕ is feasible in a network G,</p><formula xml:id="formula_25">then for all S ⊆ {0...M }, S = ∅, 0 ∈ S c , i∈S R i (a) = i∈S,j∈V ϕ(v i , v j ) (b) = i∈S,j∈S c ϕ(v i , v j ) (c) ≤ i∈S,j∈S c C ij ,<label>(6)</label></formula><p>where (a) and (b) follow from the flow conservation properties of a feasible flow (all the flow injected by the sources has to go somewhere in the network, and in particular all of it has to go across a network October 2, 2005. DRAFT cut with the destination on the other side); and (c) follows from the fact that in any flow network, the capacity of any cut is an upper bound to the value of any flow.</p><p>Thus, from ( <ref type="formula" target="#formula_24">5</ref>) and ( <ref type="formula" target="#formula_25">6</ref>), we conclude that if, for all partitions S as above, we have that</p><formula xml:id="formula_26">H(U S |U S c ) &lt; i∈S,j∈S c C ij ,<label>(7)</label></formula><p>then</p><formula xml:id="formula_27">P (N ) e → 0 as N → ∞.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Converse Proof</head><p>The converse proof is fairly long and tedious, but by virtue of being based on Fano's inequality and standard information-theoretic arguments, it is relatively straightforward -therefore, we omit it here and provide the technical details in Appendix A. At this point however, we would like to sketch out an informal argument on why this converse should hold.</p><p>Consider an arbitrary network partition S of {0...M }, S = ∅, 0 ∈ S c . For each such partition we define a two-terminal system, with a "supersource" that has access to the whole vector of observations U 1 ...U M , and a "supersink" that has access only to U S c . The supersource and supersink are connected by an array of parallel DMCs: if i ∈ S and j ∈ S c , then (X ij , p ij (y|x), Y ij ) from the network is one of the channels in the array. This is illustrated in Fig. <ref type="figure">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ú¼ Ë Ë</head><p>Fig. <ref type="figure">5</ref>. An artificial two-terminal system: all sources in S are treated as a supersource, connected to a supersink made of all the sinks in S c by an array of DMCs (those going across the cut). Intuitively, any necessary condition for this system should also be necessary for our system (although this requires a formal statement and proof). The interesting statement thus is to show that the set of all conditions obtained in this form (by considering all possible cuts) is also sufficient.</p><p>Clearly, H(U S |U S c ) &lt; i∈S,j∈S c C ij is an outer bound for this two-terminal system (follows directly from the source/channel separation theorem, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Sec. 8.13]</ref>). And intuitively, it is also clear that any outer bound for this two-terminal system provides necessary conditions for reliable communication to be possible in our network. Thus, by considering all possible partitions (S, S c ) as above, we obtain a set of necessary conditions matching those of the achievability result. <ref type="foot" target="#foot_3">3</ref>We would also like to highlight that, because of the correlation between sources, a simple max-flow/mincut bounding argument as suggested in [5, Section 14.10]) is not sufficient to establish the source-channel separation result we seek -proving said result requires all the steps of a typical converse.</p><p>A formal proof for this converse is provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Special Cases 1) A Network with Three Nodes:</head><p>To provide an illustration of the meaning of Theorem 1, and of the optimality of the flow-based solution, we specialize Theorem 1 to the case of a network with three nodes.</p><p>In this case, those conditions become:</p><formula xml:id="formula_28">H(U 1 |U 2 U 0 ) &lt; C 10 + C 12<label>(8)</label></formula><formula xml:id="formula_29">H(U 2 |U 1 U 0 ) &lt; C 20 + C 21<label>(9)</label></formula><formula xml:id="formula_30">H(U 1 U 2 |U 0 ) &lt; C 10 + C 20 .<label>(10)</label></formula><p>A network with three nodes as considered here is illustrated in Fig. <ref type="figure">6</ref>.</p><formula xml:id="formula_31">½¼ Ú ½ Ú ¼ Ú ¾ ¾¼ ¾½ ½¾</formula><p>Fig. <ref type="figure">6</ref>. A network with three nodes.</p><p>Next, we regard the network in Fig. <ref type="figure">6</ref> as a flow network [34, Ch. 26]: a flow network with two sources (v 1 and v 2 ) and a single sink (v 0 ). Encodings of U 1 injected at source v 1 at rate R 1 , and of U 2 injected at v 2 at rate R 2 , are the "objects" that flow in this network and are to be delivered to the sink v 0 . This is illustrated in Fig. <ref type="figure" target="#fig_3">7</ref>.</p><p>In the simple flow network of Fig. <ref type="figure" target="#fig_3">7</ref>, any feasible flow ϕ must satisfy some conservation equations: where the last equality follows from the fact that flow conservation holds: the total amount of flow injected (R 1 + R 2 ) must equal the total amount of flow received by the sink (ϕ(v 1 , v 0 ) + ϕ(v 2 , v 0 )) <ref type="bibr" target="#b33">[34]</ref>. Similarly, any feasible flow must also satisfy all capacity constraints:</p><formula xml:id="formula_32">R 1 = ϕ(v 1 , v 0 ) + ϕ(v 1 , v 2 ), R 2 = ϕ(v 2 , v 0 ) + ϕ(v 2 , v 1 ), R 1 + R 2 = ϕ(v 1 , v 0 ) + ϕ(v 1 , v 2 ) + ϕ(v 2 , v 0 ) + ϕ(v 2 , v 1 ) = ϕ(v 1 , v 0 ) + ϕ(v 2 , v 0 ), ½¼ Ú ½ Ú ¼ Ú ¾ ¾¼ ¾½ ½¾ ÒÓ Ö Í ½ Í ¾ ÒÓ Ö Ê ½ Ê ¾</formula><formula xml:id="formula_33">ϕ(v 1 , v 0 ) + ϕ(v 1 , v 2 ) ≤ C 10 + C 12 , ϕ(v 2 , v 0 ) + ϕ(v 2 , v 1 ) ≤ C 20 + C 21 , ϕ(v 1 , v 0 ) + ϕ(v 2 , v 0 ) ≤ C 10 + C 20 .</formula><p>Combining these last two sets of constraints, and the conditions from the Slepian-Wolf theorem on feasible (R 1 , R 2 ) pairs, we immediately get</p><formula xml:id="formula_34">H(U 1 |U 2 U 0 ) &lt; R 1 ≤ C 10 + C 12 , H(U 2 |U 1 U 0 ) &lt; R 2 ≤ C 20 + C 21 , H(U 1 U 2 |U 0 ) &lt; R 1 + R 2 ≤ C 10 + C 20 .</formula><p>It is interesting to observe in this argument that the region of achievable rates forms a convex polytope, in which three of its faces come from the Slepian-Wolf conditions, and three come from the capacity constraints. This polytope is illustrated in Fig. <ref type="figure" target="#fig_4">8</ref>. This polytope plays a central role in our analysis: reliable communication is possible if and only if R = ∅. Thus, the view of "information as a flow" in this class of networks is complete.</p><formula xml:id="formula_35">À´Í ½ Í ¾ Í ¼ µ ½¼ • ¾¼ ½¼ • ½¾ À´Í ¾ Í ½ Í ¼ µ ¾¼ • ¾½ Ê À´Í ½ Í ¾ Í ¼ µ Ê ½ Ê ¾</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) No Cooperation and No Side Information at v 0 :</head><p>We consider now the special case of M noncooperating nodes and one sink, as illustrated in Fig. <ref type="figure">9</ref>. Necessary and sufficient conditions for reliable communication under this scenario follow naturally from our main theorem by setting C ij = 0 for all j = 0,</p><formula xml:id="formula_36">and |U 0 | = 1. ½¼ Å¼ Ú ½ Ú ¾ Ú Å Ú ¼ ¾¼ Fig. 9. M non-cooperating nodes.</formula><p>Corollary 1: The sources U 1 , U 2 , . . . , U M can be communicated reliably over an array of independent channels of capacity C i0 , i = 1 . . . M , if and only if</p><formula xml:id="formula_37">H(U S |U S c ) &lt; i∈S C i0 ,</formula><p>for all subsets S ⊆ {1, 2, . . . , M }, S = ∅.</p><p>An illustration of this corollary for two sources U 1 and U 2 is shown in Fig. <ref type="figure" target="#fig_5">10</ref>. When we have two 00 00 00 00 <ref type="bibr">11 11 11 11</ref> Slepian-Wolf capacity region region Assuming that this is the case, consider now the following possibilities:</p><formula xml:id="formula_38">Ê ½ À´Í ½ Í ¾ µ À´Í ½ Í ¾ µ À´Í ¾ µ À´Í ½ Í ¾ µ À´Í ¾ Í ½ µ ¾¼ ½¼ • ¾¼ À´Í ½ µ ½¼ Ê ¾ capacity region Slepian-Wolf region Ê ½ À´Í ½ Í ¾ µ À´Í ½ Í ¾ µ ¾¼ ½¼ • ¾¼ Ê ¾ À´Í ¾ Í ½ µ À´Í ¾ µ À´Í ½ Í ¾ µ ½¼ À´Í ½ µ</formula><p>• H(U 1 ) &lt; C 10 and H(U 2 ) &lt; C 20 . The Slepian-Wolf region and the capacity region intersect, so any point (R 1 , R 2 ) in this intersection makes reliable communication possible. Alternatively, we can argue that reliable transmission of U 1 and U 2 is possible even with independent decoders, therefore a joint decoder will also achieve an error-free reconstruction of the source.</p><p>• H(U or not (see Fig. <ref type="figure" target="#fig_5">10</ref>), since examples are known in which the intersection between the capacity region of the multiple access channel and the Slepian-Wolf region of the correlated sources is empty and still reliable communication is possible <ref type="bibr" target="#b17">[18]</ref>.</p><p>Corollary 1 gives a definite answer to this last question: in the special case of correlated sources and independent channels an intersection between the capacity region and the Slepian-Wolf rate regions is not only sufficient, but also a necessary condition for reliable communication to be possible-in this case, separation holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Arrays of Gaussian Channels:</head><p>We should also mention that Theorem 1 applies to other channel models that are relevant in practice, for instance Gaussian channels with orthogonal multiple access. For simplicity, we illustrate this issue in the context of Corollary 1. The capacity of the Gaussian multiple access channel with M independent sources is given by</p><formula xml:id="formula_39">i∈S R i ≤ 1 2 log 1 + i∈S P i σ 2 ,</formula><p>for all S ⊆ {1...M }, S = ∅, and where σ 2 and P i are the noise power and the power of the i-th user respectively <ref type="bibr">[5, pp. 378-379</ref>]. If we use orthogonal accessing (e.g. TDMA), and assign different time slots to each of the transmitters, then the Gaussian multiple access channel is reduced to an array of M independent single-user Gaussian channels each with capacity</p><formula xml:id="formula_40">C i0 = τ i0 • 1 2 log 1 + P i0 σ 2 τ i0 , 1 ≤ i ≤ M,</formula><p>where τ i0 is the time fraction allocated to source user i to communicate with the data collector node v 0 , and P i0 is the corresponding power allocation.</p><p>Applying Theorem 1, we obtain the reachback capacity of the Gaussian channel with orthogonal accessing. <ref type="foot" target="#foot_4">4</ref> Then, reliable communication is possible if and only if</p><formula xml:id="formula_41">H(U S |U S c ) ≤ i∈S τ i0 2 log 1 + P i0 σ 2 τ i0 ,</formula><p>for all subsets S ⊆ {1, 2, . . . , M }, S = ∅.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRACTICAL/ENGINEERING IMPLICATIONS OF THEOREM 1</head><p>A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. An Information Theoretically Optimal Protocol Stack</head><p>We believe that the fact that in networks of point-to-point noisy links with one sink Shannon information has the exact same properties of classical network flows is of particular practical relevance. This is so because there is a rich algorithmic theory associated with it, which allows us to cast standard information theoretic problems into the language of flows and optimization. Perhaps most relevant among these is is the optimality of implementing codes using a layered protocol stack, as illustrated in Fig. <ref type="figure" target="#fig_6">11</ref>.</p><p>As discussed in the Introduction, the decision to turn a wireless network into a network of point-to-point links is an arbitrary one. But, due to complexity and/or economic considerations, this arbitrary decision is one made very often, and thus we believe it is of great practical interest to understand what are appropriate design criteria for such networks. And our Theorem 1 offers valuable insights in this regard -if we decide to define a link-layer based on a MAC protocol that deals with interference by suppressing it, then all remaining layers in Fig. <ref type="figure" target="#fig_6">11</ref> follow from the achievability proof of Theorem 1. We see therefore that indeed, in this class of networks, Fig. <ref type="figure" target="#fig_6">11</ref> provides a set of abstractions analogous to those of Fig. <ref type="figure" target="#fig_0">2</ref> for classical two-terminal systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithmic/Computational Issues</head><p>As an illustration of the benefits of the "information as flow" interpretation for our results, in this subsection we outline some initial results on an optimal routing problem. This topic however will be developed in full depth elsewhere.</p><formula xml:id="formula_42">¾¼ ¾½ ½¼ ½¾ Ú½ Ú¾ Ú¼ Ú½ Ú¾ Ú¼ Ä Ò × ³½¾ ³½¼ Ú½ Ú¾ Ú¼ ÐÓÛ× Ê ÓÒ×ØÖÙØ Ø ¬ Ð Ú¼ Ú¾ Ú½ Ê ½ Ø ×ØÖ Ñ× ÓÒÒ Ø ÓÒ× Ê ½ Ú¼ Ú¾ Ú½ Í ½ Í ¾ Í ¼ Ø Û Ý AEÓ Ø ¬ Ð ÈÓÛ Ö ÓÒ×ØÖ Ò AEÓ × Ê ¾ ³¾¼ Ê ¾ Ä Ò Ä Ý Ö ´Å »ÈÓÛ Ö» ÖÖÓÖ ÓÒØÖÓÐµ AE ØÛÓÖ Ä Ý Ö ´ × Ð ÐÓÛ ÓÑÔÙØ Ø ÓÒµ ÌÖ Ò×ÔÓÖØ Ä Ý Ö ´ÊÓÙØ Ò µ ÈÖ × ÒØ Ø ÓÒ Ä Ý Ö ´ ×ØÖ ÙØ Ë ÑÔÐ Ò » ÓÑÔÖ ×× ÓÒµ ´ÁÒØ ÖÔÓÐ Ø ÓÒµ ´Í× Ö Ó Ø Ø µ ÔÔÐ Ø ÓÒ Ä Ý Ö È Ý× Ð Ä Ý Ö Fig. 11.</formula><p>Abstractions that follow from the achievability proof, illustrated here for three nodes. At the physical layer there are nodes with power constraints, a data field of which these nodes collect samples in space and time, and a gateway node that will deliver all this data to destination. On top of this physical substrate, we construct a sequence of abstractions: noiseless point-to-point links of a given capacity (the Link Layer); a flow network (the Network Layer); a set of connections (the Transport Layer); and a set of distributed signal processing algorithms for sampling, compression and interpolation of the space/time continuous process (the Presentation Layer). In the end, an approximate representation of the underlying data field is delivered to applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Optimization Aspects of Protocol Design:</head><p>A natural question that follows from our previous developments is one of optimization: given a non-empty feasibility polytope R, we have the freedom of choosing among multiple assignments of values to flow variables, and thus it is only natural to ask if there is an optimal flow. To this end, we define a cost function κ as follows:</p><formula xml:id="formula_43">κ(ϕ) = (vi,vj )∈E c(v i , v j ) • ϕ(v i , v j ),</formula><p>where c(v i , v j ) is a constant that, multiplied by the total number of bits ϕ(v i , v j ) that a flow ϕ assigns to an edge (v i , v j ), determines the cost of sending all that information over the channel (X ij , p ij (y|x), Y ij ).</p><p>The resulting optimization problem is shown in Fig. <ref type="figure" target="#fig_6">12</ref>.</p><formula xml:id="formula_44">min (vi,vj)∈E c(v i , v j ) • ϕ(v i , v j ) subject to:</formula><p>Standard flow constraints (capacity / skew symmetry / flow conservation)</p><formula xml:id="formula_45">ϕ(v i , v j ) ≤ C ij , 0 ≤ i, j ≤ M. ϕ(v i , v j ) = -ϕ(v j , v i ), 0 ≤ i, j ≤ M. v∈V ϕ(v i , v) = 0, 1 ≤ i ≤ M.</formula><p>Rate admissibility constraints</p><formula xml:id="formula_46">H(U S |U S c ) &lt; i∈S ϕ(s, v i ) ≤ i∈S,j∈S c C ij , S ⊆ {1...M }, S = ∅. ϕ(s, v i ) = R i , 1 ≤ i ≤ M.</formula><p>Fig. <ref type="figure" target="#fig_6">12</ref>. Linear programming formulation for the assignment of values to flow variables (observe the introduction of a "supersource" s, which supplies R i units of flow to v i ). A solution to this problem provides optimal routes (those with positive flow assignment) and loads on each link. Note as well that, by choosing c(v i , v j ) = 0 for all</p><formula xml:id="formula_47">(v i , v j ) ∈ E,</formula><p>this LP is solvable if and only if R = ∅ -that is, the decision problem for reliable communication (i.e., for whether a given load p(U 0 U 1 ...U M ) can be carried over a given network G) admits a linear programming formulation too.</p><p>The choice of a linear cost model in this setup can be justified based on a number of reasons. First of all, linearity is a very natural assumption: in simple language, it says that it costs twice as much to double the amount of information sent on any channel. For example, we could take c(v i , v j ) to be the minimum energy per information bit required for reliable communication over the DMC from v i to v j <ref type="bibr" target="#b35">[36]</ref>, and then κ(ϕ) would give us the sum of the energy consumed by all nodes when transporting data as dictated by a particular flow ϕ. Specifically in the context of routing problems, another important consideration is that the main drawback often cited for solving optimal routing problems based on network flow formulations is given by the fact that cost functions such as κ only optimize average levels of link traffic, ignoring other traffic statistics <ref type="bibr">[8, pg. 436</ref>]. But this is not at all an issue here, since the values of flow variables (i.e., Shannon information) are already average quantities themselves.</p><p>2) A Routing Example: As one example of the usefulness of the LP formulation in Fig. <ref type="figure" target="#fig_6">12</ref>, we consider next the problem of designing efficient mechanisms for data aggregation, as motivated in <ref type="bibr" target="#b36">[37]</ref>. There has been a fair amount of work reported in the networking literature, on the design and performance analysis of tree structures for aggregation-for example, the work of Goel and Estrin on the construction of trees that perform well simultaneously under multiple concave costs <ref type="bibr" target="#b37">[38]</ref>. Based on our LP formulation, we construct two examples which show the extent to which trees could give rise to suboptimalities, as opposed to other topological structures. And we start by showing an example in which, although R = ∅, there are no feasible trees. This case is illustrated in Fig. <ref type="figure" target="#fig_6">13</ref>.</p><formula xml:id="formula_48">½ ½ ½ ½ ½ Ú ½ Ú ¾ ¼ Ú ¼ Ú ¼ ½ Ú ½ Ú ¾ ¼ ½ ¼ ¼ ½ ½ Ú ¼ Ú ½ Ú ¾ ¼ ½ ¼ ¼ ¼ ¼ ¼ Fig. 13.</formula><p>To illustrate a solvable problem that cannot be solved using trees. Left: a flow network; middle/right: the decomposition of a feasible flow into two single flows, showing how much of the flow injected at each source is sent over which link (x/c next to an edge means that the edge carries x units of flow, and has capacity c).</p><p>As illustrated in Fig. <ref type="figure" target="#fig_6">13</ref>, a solution to the transport problem exists. However, it is easy to check that if we constrain data to flow along trees, none of the three possible trees ({(v</p><formula xml:id="formula_49">1 , v 0 ); (v 2 , v 0 )}, or {(v 1 , v 2 ); (v 2 , v 0 )},</formula><p>or {(v 2 , v 1 ); (v 1 , v 0 )}) are feasible: in all cases, there is one link for which the capacity constraint is violated.</p><p>Next we consider a case where feasible trees exist, but the lowest cost of any tree differs from the optimal cost by an arbitrarily large factor. This case is illustrated in Fig. <ref type="figure" target="#fig_6">14</ref>.</p><formula xml:id="formula_50">Ú ¼ ½ ´Ú ¾ Ú ½ µ ½ ½ • ¯Ú½ Ú ¾ ½ ¾ ´Ú ¾ Ú ¼ µ ½ ½ • ¯ ´Ú ½ Ú ¼ µ ½ ½ ´Ú ½ Ú ¾ µ ½ Ú ¼ ½ • ¯Ú½ Ú ¾ ½ ½ ½ ½ ¯ ½ • ¯Ú¼ Ú ½ Ú ¾ ¼ ½ ½ ½ ¼ ¼ ½ Fig. 14.</formula><p>To illustrate a problem in which trees are very expensive. Left: a flow network with costs; right: an optimal solution to the linear program in Fig. <ref type="figure" target="#fig_6">12</ref>. Such a case could arise, e.g., in a situation where there is heavy interference in the direct path from v1 to v0.</p><p>In this case, there exists only one feasible tree: {(v 1 , v 0 ); (v 2 , v 0 )}, with cost ℓ(1+ǫ)+1. However, because of the "expensive" link (v 1 , v 0 ) along which the tree is forced to send all its data, the cost is significantly increased: by splitting the encoding of U 1 as illustrated in Fig. <ref type="figure" target="#fig_6">14</ref>, the cost incurred into by this structure would be ǫℓ + 3. Hence, we see that in this case, the cost of the best feasible tree is ℓ(1+ǫ)+1 ǫℓ+3 times larger October 2, 2005. DRAFT than that of an optimal solution allowing splits. And this "overpayment factor" could be significant: when ℓ is large, this is ≈ 1 + 1 ǫ , and it grows unbound for small ǫ. Note as well that any time that a network is operated close to capacity, it will be necessary to split flows.</p><p>And that is a situation likely to be encountered often in power-constrained networks, since minimum energy designs will necessarily result in links being allocated the least amount of power needed to carry a given traffic load. Thus, we see that these examples above are not pathological cases of limited practical interest, but instead, they are good representatives of situations likely to be encountered often in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Suboptimality of Correlated Codes for Orthogonal Channels</head><p>The key ingredient of the achievability proof presented by Cover, El Gamal and Salehi for the multiple access channel with correlated sources is the generation of random codes, whose codewords X N i are statistically dependent on the source sequences U N i <ref type="bibr" target="#b17">[18]</ref>. This property, which is achieved by drawing the codewords according to N j=1 p(x ij |u ij ) with u ij and x ij denoting the j-th element of U N i and X N i , respectively, implies that U N i and X N i are jointly typical with high probability. Since the source sequences</p><formula xml:id="formula_51">U N 1 and U N 2 are correlated, the codewords X N 1 (U N 1 ) and X N 2 (U N</formula><p>2 ) are also correlated, and so we speak of correlated codes. This class of random codes, which is treated in more general terms in <ref type="bibr" target="#b20">[21]</ref>, can be viewed as joint source and channel codes that preserve the given correlation structure of the source sequences, based upon which the decoder can lower the probability of error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The class of correlated codes is of interest to us because of two main reasons:</head><p>• From a practical point of view, correlated codes have a very strong appeal: sensor nodes with limited processing capabilities may be forced to use very simple codes that do not eliminate correlations between measurements prior to transmission <ref type="bibr" target="#b38">[39]</ref> (e.g., a simple scalar quantizer and simple BPSK modulation).</p><p>• From a theoretical point of view, since these codes yield the largest known admissibility region for the problem of communicating distributed sources over multiple-access channels, it would be interesting to know how these codes fare in our context, where we know separate source and channel coding to achieve optimality. Thus, specializing the achievability proof of <ref type="bibr" target="#b17">[18]</ref> to the case of M independent channels, we get the following result.</p><p>Corollary 2 (From Theorem 1 of <ref type="bibr" target="#b17">[18]</ref>): A set of correlated sources [U 1 U 2 ...U M ] can be communicated reliably over independent channels (X 1 , p(y</p><formula xml:id="formula_52">1 |x 1 ), Y 1 ) . . . (X M , p(y M |x M ), Y M ) to a sink v 0 , if H(U S |U S c ) &lt; i∈S I(X i ; Y 0 |U S c ), October 2, 2005.</formula><p>DRAFT for all subsets S ⊆ {1, 2, . . . , M }, S = ∅.</p><p>Proof: This result can be obtained from the M -source version of the main theorem in <ref type="bibr" target="#b17">[18]</ref>, by specializing it to a multiple access channel with conditional probability distribution</p><formula xml:id="formula_53">p(y|x 1 x 2 ...x M ) = p(y 1 y 2 . . . y M |x 1 x 2 . . . x M ) = M i=1 p(y i |x i ).</formula><p>Part of the reason why we feel this is an interesting result is that the main theorem in <ref type="bibr" target="#b17">[18]</ref> does not immediately specialize to Corollary 1: whereas the achievability results do coincide, <ref type="bibr" target="#b17">[18]</ref> does not provide a converse. To illustrate this point better, we focus now on the case of M = 2:</p><p>• In general, we have that</p><formula xml:id="formula_54">I(X 1 X 2 ; Y 1 Y 2 ) ≤ I(X 1 ; Y 1 )+I(X 2 ; Y 2 ), for any p(u 1 u 2 x 1 x 2 )p(y 1 |x 1 )p(y 2 |x 2 );</formula><p>but for this upper bound on the sum-rate to be achieved, we must take p(u</p><formula xml:id="formula_55">1 u 2 x 1 x 2 ) = p(u 1 u 2 )p(x 1 )p(x 2 )</formula><p>-that is, the codewords must be drawn independently of the source. And for this special case, our Theorem 1 does provide a converse.</p><p>• As argued earlier, due to practical considerations it may not be feasible to remove correlations in the source before choosing channel codewords, in which case we face a situation where correlated codes are used, despite their obvious suboptimality. In this case, it is of interest to determine the rate losses resulting from the use of correlated codes, defined as ∆ 1 = I(X 1 ; Y 1 ) -I(X 1 ; Y 1 |U 2 ), ∆ 2 = I(X 2 ; Y 2 ) -I(X 2 ; Y 2 |U 1 ), and ∆ 0 = I(X 1 ; Y 1 ) + I(X 2 ; Y 2 ) -I(X 1 X 2 ; Y 1 Y 2 ). Straightforward manipulations show that ∆ 1 = I(Y 1 ; U 2 ), ∆ 2 = I(Y 2 ; U 1 ), and ∆ 0 = I(Y 1 ; Y 2 ).</p><p>• Since ∆ i ≥ 0, i ∈ {0, 1, 2} (mutual information is always nonnegative), we conclude that the region of achievable rates given by Corollary 2 is contained in the region defined by Corollary 1. Furthermore, we find that the rate loss terms have a simple, intuitive interpretation: ∆ 0 is the loss in sum rate due to the dependencies between the outputs of different channels, and ∆ 1 (or ∆ 2 ) represent the rate loss due to the dependencies between the outputs of channel 1 (or 2) and the source transmitted over channel 2</p><p>(or 1). All these terms become zero if, instead of using correlated codes, we fix p(x 1 )p(x 2 ) and remove the correlation between the source blocks before transmission over the channels.</p><p>At first glance, this observation may seem somewhat surprising, since the problem addressed by Corollary 1 is a special case of the multiple access channel with correlated sources considered in <ref type="bibr" target="#b17">[18]</ref>, where it is shown that in the general case correlated codes outperform the concatenation of Slepian-Wolf codes (independent codewords) and optimal channel codes. The crucial difference between the two problems is the presence (or absence) of interference in the channel. Albeit somewhat informally, we can state that correlated codes are advantageous when the transmitted codewords are combined in the channel through interference, which is October 2, 2005. DRAFT obviously not the case in our problem. Practical code constructions built around this observation have been reported in <ref type="bibr" target="#b38">[39]</ref>.</p><p>IV. CONCLUSIONS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Summary</head><p>In this paper we have considered the problem of encoding a set of distributed correlated sources for delivery to a single data collector node over a network of DMCs. For this setup we were able to obtain single-letter information theoretic conditions that provide an exact characterization of the admissibility problem. Two important conclusions follow from the achievability proof:</p><p>• Separate source/channel coding is optimal in any network with one sink in which interference is dealt with at the MAC layer by creating independent links among nodes.</p><p>• In such networks, the properties of Shannon information are exactly identical to those of water in pipes -information is a flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion</head><p>A few interesting observations follow from our results:</p><p>• It is a well known fact that turning a multiple access channel into an array of orthogonal channels by using a suitable MAC protocol is a suboptimal strategy in general, in the sense that the set of rates that are achievable with orthogonal access is strictly contained in the Ahlswede-Liao capacity region <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Ch. 14.3]</ref>. However, despite its inherent suboptimality, there are strong economic incentives for the deployment of networks based on such technologies, related to the low complexity and cost of existing solutions, as well as experience in the fabrication and operation of such systems. As a result, most existing standard implementations we are aware of (e.g., the IEEE 802.11 and 802.15.* families, or Bluetooth), are based on variants of protocols like TDMA/FDMA/CDMA or Aloha, that treat interference among users as noise or collisions, and deal with it by creating orthogonal links. We feel therefore that some of the interest in our results stems from the fact that they provide a thorough</p><p>analysis for what we deem to be, with high likelihood, the vast majority of wireless communication networks to be deployed for the foreseeable future.</p><p>• A basic question follows from the results in this paper: when exactly does Shannon information act like a classical flow in a network setup? In this paper, we showed that far more often than common wisdom would suggest: for any network made up of independent links and one sink, Shannon information is a flow. The assumption of independence among channels is crucial, since well known counterexamples <ref type="bibr" target="#b35">October 2, 2005</ref>. DRAFT hold without it <ref type="bibr" target="#b17">[18]</ref>. But, as argued before, far from being just some technical assumption needed for the theory to hold, independent channels arise naturally in practical applications. In establishing the flow properties of information, we showed how some well understood network flow tools can be applied to address network design problems that have traditionally been difficult to deal with using standard tools in network information theory, and we illustrated this with a simple example involving optimal routing. In particular we showed that, at least from an information theoretic point of view, there is little justification for the common practice of designing trees for collecting data picked up by a sensor network, thus opening up interesting problems of protocol design.</p><p>• In retrospect, perhaps the results we prove in this paper should not have been surprising. In the context of two-terminal networks, we do know the following:</p><p>-Feedback does not increase the capacity. Therefore, the capacity of individual links is unaffected by the ability of our codes to establish a conference mechanism among nodes.</p><p>-Compression rates are not reduced by explicit cooperation, as it follows from the Slepian-Wolf theorem: the minimum rate required to communicate U 1 to a decoder that has access to sideinformation U 0 is H(U 1 |U 0 ), and knowledge of U 0 does not reduce the rates needed for coding U 1 . Therefore, the amount of information that needs to flow through our network is not reduced either by the ability of nodes to establish conferences.</p><p>Of course the statements above only hold for individual links, and a proof was needed to carry that intuition to the general network setup considered in this work. But those observations we think are the key to understanding why our results hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Future Work</head><p>After having established coding theorems for the problem of network information flow with correlated sources, a natural question that arises: what if, in a given scenario, R = ∅? In that case, the best we can hope for is to reconstruct an approximation to the original source message -and the answer is given by rate-distortion theory <ref type="bibr" target="#b39">[40]</ref>. The rate-distortion formulation of our problem in the case of non-cooperating encoders is equivalent to the well known (and still open) Multiterminal Source Coding problem <ref type="bibr" target="#b11">[12]</ref>. Our current efforts are focused on completing work on the rate/distortion problem, and on fully developing the ideas outlined in Section III-B (e.g., to deal with problems of the type considered in <ref type="bibr" target="#b40">[41]</ref>).</p><p>where h(•) denotes the binary entropy function, and</p><formula xml:id="formula_56">Û LN i = ( Û N i (1), Û N i (2), . . . , Û N i (L)</formula><p>) denotes L blocks of N snapshots reconstructed at v 0 . For convenience, we define also</p><formula xml:id="formula_57">δ(P (LN ) e ) = P (LN ) e log |U LN 1 × U LN 2 × • • • × U LN M | + h(P (LN ) e ) /LN.</formula><p>It follows from eqn. <ref type="bibr" target="#b10">(11)</ref> that</p><formula xml:id="formula_58">H(U LN 1 U LN 2 . . . U LN M |U LN 0 Y BN 10 Y BN 20 . . . Y BN M 0 ) (a) = H(U LN 1 U LN 2 . . . U LN M |U LN 0 Y BN 10 Y BN 20 . . . Y BN M 0 Û LN 1 Û LN 2 . . . Û LN M ) ≤ H(U LN 1 U LN 2 . . . U LN M | Û LN 1 Û LN 2 . . . Û LN M ) ≤ LN δ(P (LN ) e ),</formula><p>where </p><formula xml:id="formula_59">Y BN ij = (Y N ij (1), Y N ij (2), . . . , Y N ij (B)) denotes B = W + (L<label>-</label></formula><formula xml:id="formula_60">H(U LN S |U LN S c Y BN S→S c Y BN S c →S c ) ≤ H(U LN S |U LN S c Y BN S→0 Y BN S c \{0}→0 ) ≤ LN δ(P (LN ) e ).<label>(12)</label></formula><p>Let the set of B codewords sent by the nodes in a subset A to the nodes in a subset D be</p><formula xml:id="formula_61">X BN A→D = {X BN ij : i ∈ A and j ∈ D},</formula><p>and, likewise, the corresponding channel outputs be denoted as</p><formula xml:id="formula_62">Y BN A→D = {Y BN ij : i ∈ A and j ∈ D}.</formula><p>We will make use of the following lemmas.</p><p>Lemma 1: Let X S→S c be a set of channel inputs and Y S→S c be a set of channel outputs of an array of independent channels {X ij , p ij (y|x), Y ij }, ∀i ∈ S and ∀j ∈ S c . Then,</p><formula xml:id="formula_63">I(X S→S c ; Y S→S c ) ≤ i∈S,j∈S c I(X ij ; Y ij ).<label>(13)</label></formula><p>Proof: Without loss of generality, assume that S = {1, . . . , x 0 } and S c = {x 0 + 1, . . . , M }. From the definition of mutual information, it follows that</p><formula xml:id="formula_64">I(X S→S c ; Y S→S c ) = H(Y S→S c ) -H(Y S→S c |X S→S c ).</formula><p>October 2, 2005. DRAFT</p><p>Expanding the first term on the right handside, we get</p><formula xml:id="formula_65">H(Y S→S c ) = H(Y 1→S c Y 2→S c . . . Y x0→S c ) ≤ i∈S H(Y i→S c ) = i∈S H(Y i→x0+1 Y i→x0+2 . . . Y i→M ) ≤ i∈S,j∈S c H(Y ij )</formula><p>Similarly, the second term reduces to</p><formula xml:id="formula_66">H(Y S→S c |X S→S c ) = H(Y 1→S c Y 2→S c . . . Y x0→S c |X 1→S c X 2→S c . . . X x0→S c ) = H(Y 1→S c |X 1→S c X 2→S c . . . X x0→S c ) + x0 i=2 H(Y i→S c |X 1→S c X 2→S c . . . X x0→S c Y 1→S c . . . Y i-1→S c ) = H(Y 1→S c |X 1→S c ) + x0 i=2 H(Y i→S c |X i→S c ) = i∈S H(Y i→S c |X i→S c ) = i∈S H(Y i→x0+1 Y i→x0+2 . . . Y i→M |X i→x0+1 X i→x0+2 . . . X i→M ) = i∈S H(Y i→x0+1 |X i→x0+1 X i→x0+2 . . . X i→M ) + M j=x0+2 H(Y i→j |X i→x0+1 X i→x0+2 . . . X i→M )Y i→x0+1 . . . Y i→j-1 ) = i∈S H(Y i→x0+1 |X i→x0+1 ) + M j=x0+2 H(Y i→j |X i→j ) = i∈S,j∈S c H(Y ij |X ij ).</formula><p>Combining the two expressions, we get</p><formula xml:id="formula_67">I(X S→S c ; Y S→S c ) ≤ i∈S,j∈S c H(Y ij ) -H(Y ij |X ij ) = i∈S,j∈S c I(X ij ; Y ij ),</formula><p>thus proving the lemma. To prove that U LN S can be removed from the last factor in the previous expression, we will use an induction argument on the length of the pipeline, L, and window sizes, K and T .</p><p>Fix (S, S c ) and i, j ∈ S c . Let L = K = T = 1. The encoding functions produce g ij (U N i ) = X N i→j , which result in the channel outputs Y N i→j after transmission over the DMC between nodes i and j. In shorthand, we write</p><formula xml:id="formula_68">g ij (U N i ) = X N i→j DMC -→ Y N i→j .</formula><p>Thus, the first block of channel inputs X 1...N S c →S c generated in the node set S c depends only on source symbols U 1...N S c available in S c . Moreover, since the channels are DMCs, the channel outputs depend only on the channel inputs. Thus, we conclude that U 1...N S and Y 1...N S c →S c are independent given U 1...N S c . Since we consider a pipeline of length L = 1, there are no more blocks to inject, but not all data may have arrived to destination, so we have to allow for a few (W , to be precise) extra transmissions. By "flushing the pipeline", we have , 5 Since W is the delay used to allow data to flow to the destination, it would not be reasonable to perform induction on W a given fixed network. Instead we take W as a parameter, which must be greater or equal to the diameter of the network.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Shannon's model for a point-to-point system. Top figure: abstract view, consisting of a source, an encoder from source symbols to channel symbols, a conditional probability distribution to model the random dependence of outputs on inputs, and a decoder to map from received messages back to source symbols; bottom figure: a capacity-achieving architecture for this system, in which error control codes are used to create an illusion of a noiseless bit pipe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A topological sort of the nodes of a directed acyclic graph is a linear orderingv 1 ...v M such that if (v i , v j ) is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>b 4 b 5 b 1 b 5 b 6 b 7 b 2 b 3 b 4 b 1 b 2 Fig. 4 .</head><label>17424</label><figDesc>Fig. 4. To illustrate the operations performed at each node. In this example, five bits come into node v π(k) from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. A flow network with three nodes, supplies R 1 and R 2 and nodes v 1 and v 2 , and a sink v 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The polytope R of admissible rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Relationship between the Slepian-Wolf region and the capacity region for two independent channels. In the left figure, as H(U1|U2) &lt; C10 and H(U2|U1) &lt; C20 the two regions intersect and therefore reliable communication is possible. The figure on the right shows the case in which H(U2|U1) &gt; C20 and there is no intersection between the two regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 )</head><label>1</label><figDesc>blocks of N channel outputs observed by node v j while communicating with node v i , and (a) follows from the fact that the estimates Û LN i , i = 1 . . . M , are functions of U LN 0 and of the received channel outputs Y BN i0 , i = 1 . . . M . From the chain rule for entropy, from the fact that conditioning does not increase entropy, and for any S ⊆ M = {0...M }, S = ∅, 0 ∈ S c , it follows that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>g.S</head><label></label><figDesc>ij (Y 1...N S→i Y 1...N S c →i ) = X N +1...2N i→j DMC -→ Y N +1...2N i→j It follows that Y N +1...2N S c →S c is independent of U 1...N Sgiven Y 1...N S→S c and U 1...N S c . Similarly, we haveg ij (Y (W -2)N +1...(W -1)N S→i Y (W -2)N +1...(W -1)N S c →i ) = X (W -1)N +1...W N i→j DMC -→ Y (W -1)N +1...W N i→j , from which we conclude that Y (W -1)N +1...W N S c →S c is independent of U 1...N S given Y (W -2)N +1...(W -1)N S→S cand U 1...N S c . Thus, for K = T = L = 1, and W arbitrary,5 the Markov chain in the lemma holds (with B = L + W -1).To proceed with the inductive proof, we still take K = T = 1, (S, S c ) fixed, i, j ∈ S c , but L is now arbitrary. By inductive hypothesis, we have the following Markov chainU c →S c .Encoding and transmission of the last block of each source yieldsg ij (U</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>S+</head><label></label><figDesc>c →S c . This is not yet the sought Markov chain, as we still need to flush the pipe. But similarly to how it was done for the base case of this inductive argument, we have thatg ij (Y LN +1...(1)N +1...BN i→j ,and therefore, now yes, we have thatY BN S c →S c is independent of U 1...NSgiven Y BN S→S c and U 1...N S c . The proof of the lemma is completed by performing the exact same induction steps on K and T as done on L. For brevity, those same are omitted from this proof.2) Main Proof: We now take an arbitrary non-empty subset S ⊆ M = {0...M }, S = ∅, 0 ∈ S c . and start by bounding H(U LN S ) according toH(U LN S ) = I U LN S ; U LN S c Y BN S→S c Y BN S c →S c + H U LN S |U LN S c Y BN S→S c Y BN S c →S c (a) ≤ I U LN S ; U LN S c Y BN S→S c Y BN S c →S c + LN δ(P (LN ) I(U LN S ; Y BN S→S c |U LN S c ) + I(U LN S ; Y BN S c →S c |U LN S c Y BN S→S c ) + LN δ(P (LN ) e ),where (a) follows from<ref type="bibr" target="#b11">(12)</ref>. From Lemma 2, we have thatI(U LN S ; Y BN S c →S c |U LN S c Y BN S→S c ) = 0, and so we getH(U LN S ) ≤ I(U LN S ; U LN S c ) + I(U LN S ; Y BN S→S c |U LN S c ) + LN δ(P (LN ) e ).(14)Developing the second term on the right handside yields:I(U LN S ; Y BN S→S c |U LN S c ) = BN k=1 I(U LN S ; Y S→S c (k)|U LN S c Y k-1 S→S c ) S ; Y S→S c (k)|U LN S c Y k-1 S→S c ) + BN k=1 I(X S→S c (k); Y S→S c (k)|U LN S c Y k-1 S→S c S→S c (k)U LN S ; Y S→S c (k)|U LN S c Y k-1 S→S c ) = BN k=1 I(X S→S c (k); Y S→S c (k)|U LN S c Y k-1 S→S c ) + I(U LN S ; Y S→S c (k)|U LN S c Y k-1 S→S c X S→S c (k))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>)</head><label></label><figDesc>S→S c (k); Y S→S c (k)|U LN S c Y k-1 S→S c ) = BN k=1 H(Y S→S c (k)|U LN S c Y k-1 S→S c ) -H(Y S→S c (k)|U LN S c Y k-1 S→S c X S→S c (k)) S→S c (k)|U LN S c Y k-1 S→S c ) -H(Y S→S c (k)|X S→S c (k)) S→S c (k)) -H(Y S→S c (k)|X S→S c (k)) = BN k=1 I(X S→S c (k); Y S→S c (k)) (d) ≤ BN k=1 i∈S,j∈S c I(X ij (k); Y ij (k)) = i∈S,j∈S c BN k=1 I(X ij (k); Y ij (k)) ≤ i∈S,j∈S c BN C ijwhere we use the following arguments:(a) given the channel inputs X S→S c (i) the channel outputs Y S→S c (i) are independent of all other random variables;(b) same as (a);(c) conditioning does not increase the entropy;(d) direct application of lemma 1.Substituting in<ref type="bibr" target="#b13">(14)</ref> yieldsH(U LN S ) ≤ I(U LN S ; U LN S c ) + i∈S,j∈S c BN C ij + LN δ(P (LN ) e ).Using the fact that the sources are drawn i.i.d., this last expression can be rewritten asLN H(U S ) ≤ LN I(U S ; U S c ) + i∈S,j∈S c BN C ij + LN δ(P (LN ) S |U S c ) ≤ B L i∈S,j∈S c C ij + δ(P (LN ) e ) ≤ (W + L -1) L i∈S,j∈S c C ij +δ(P (LN ) e October 2, 2005. DRAFT Finally, we observe that this inequality holds for all finite values of L. Thus, it must also be the case that H(U S |U S c ) &lt; inf L=1,2,... (W + L -1) L i∈S,j∈S c C ij + δ(P (LN ) we get H(U S |U S c ) &lt; i∈S,j∈S c C ij , thus concluding the proof.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 ) &gt; C 10 and H(U 2 ) &gt; C 20 . Since H(U 1 U 2 ) &lt; C 10 + C 20 there is always at least one point of intersection between the Slepian-Wolf region and the capacity region, so reliable communication is possible.</figDesc><table /><note><p>• H(U 1 ) &lt; C 10 and H(U 2 ) &gt; C 20 (or vice versa). If H(U 2 |U 1 ) &lt; C 20 (or if H(U 1 |U 2 ) &lt; C 10 ) then the two regions will intersect. On the other hand, if H(U 2 |U 1 ) &gt; C 20 (or if H(U 1 |U 2 ) &gt; C 10 ), then there are no intersection points, but it is not immediately clear whether reliable communication is possible</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We begin by expanding p(u LN S u LN S c y BN S→S c y BN S c →S c ) according to p(u LN S u LN S c y BN S→S c y BN S c →S c ) = p(u LN S ) • p(u LN S c y BN S→S c |u LN S ) • p(y BN S c →S c |u LN S u LN S c y BN S→S c ).</figDesc><table><row><cell>Lemma 2: U LN S</cell><cell>→ (U LN S c Y BN S→S</cell></row></table><note><p><p>c ) → Y BN S c →S c forms a Markov chain.</p>Proof:</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>October 2, 2005.DRAFT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Note that Cij could potentially be zero, thus assuming a complete graph does not mean necessarily that any node can send messages to any other node in one hop.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>During the time that a block of snapshots spends within the network, arbitrarily complex coding operations are allowed within the pipeline: nodes can exchange information, redistribute their load, and in general perform any form of joint source-channel coding operations. The only constraint imposed is that all information eventually be delivered to destination, within a finite time horizon. October 2, 2005. DRAFT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>We thank our Reviewer B, for suggesting this simple and very clear interpretation for the converse. October 2, 2005. DRAFT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>The generalization of Theorem 1 for channels with real-valued output alphabets can be easily obtained using the techniques in [5, Sec. 9.2 &amp; Ch. 10]. October 2, 2005. DRAFT</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors most gratefully acknowledge discussions with Neri Merhav, whose insightful comments on an earlier version of this manuscript led to substantial improvements, as well as the valuable feedback from all reviewers (and particularly from reviewer B). They also wish to thank Toby Berger and Te Sun Han for helpful discussions, and Joachim Hagenauer for financial support without which they would have not been able to work together. The second author is also grateful to Mung Chiang, Eric Friedman, Éva Tardos and Sergio Verdú, for useful discussions and feedback on this work.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Work supported by a scholarship from the Fulbright commission, and by the National Science Foundation, under awards CCR-0238271 (CAREER), CCR-0330059, and ANR-0325556. Previous conference publications: <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>October 2, 2005. DRAFT</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>A. Converse Proof for Theorem 1 1) Preliminaries: Assume there exists a sequence of codes such that the decoder at v 0 is capable of producing a perfect reconstruction of blocks of</p><p>Consider now decoding L blocks of N snapshots (indexed by l = 0...L -1):</p><p>• The 1-st block of snapshots (l = 0) is computed based on messages Y N i0 received by v 0 from all nodes v i at times kN (k = 0 ... W -1).</p><p>• The 2-nd block of snapshots (l = 1) is computed based on messages Y N i0 received by v 0 from all nodes v i at times kN (k = 1 ... W ). . . .</p><p>• The L-th block of snapshots (l = L -1) is computed based on messages Y N i0 received by v 0 from all nodes v i at times kN (k = L-1 ... W +(L-2)).</p><p>Thus, we regard the network as a pipeline, in which "packets" (i.e., blocks of N source symbols injected by each source) take N W units of time to flow, and each source gets to inject L packets total. We are interested in the behavior of this pipeline in the regime of large L.</p><p>For any fixed L, the probability of at least one of the L blocks being decoded in error is P</p><p>) L . Thus, from the existence of a code with low block probability of error we can infer the existence of codes for which the probability of error for the entire pipeline is low as well, by considering a large enough block length N .</p><p>We begin with Fano's inequality. If there is a suitable code as defined in the problem statement, then we must have</p><p>October 2, 2005. DRAFT</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the Capacity of the Reachback Channel in Wireless Sensor Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Servetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop Multimedia Sig. Proc., US Virgin Islands</title>
		<meeting>IEEE Int. Workshop Multimedia Sig. ., US Virgin Islands</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Invited paper to the special session on Signal Processing for Wireless Networks</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reachback Capacity with Non-Interfering Nodes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Servetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Inform. Theory (ISIT)</title>
		<meeting>IEEE Int. Symp. Inform. Theory (ISIT)<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coding Theorems for the Sensor Reachback Problem with Partially Cooperating Nodes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Servetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discrete Mathematics and Theoretical Computer Science (DIMACS) series on Network Information Theory</title>
		<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Coding Theorem for Network Information Flow with Correlated Sources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Servetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Inform. Theory (ISIT)</title>
		<meeting>IEEE Int. Symp. Inform. Theory (ISIT)<address><addrLine>Adelaide, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>John Wiley and Sons, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Survey on Sensor Networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Akyildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sankarasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cayirci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Mag</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="102" to="114" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The CEO Problem</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Viswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="887" to="902" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gallager</surname></persName>
		</author>
		<title level="m">Data Networks</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>nd ed</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">dFSK: Distributed Frequency Shift Keying Modulation in Dense Sensor Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Servetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Commun. (ICC)</title>
		<meeting>IEEE Int. Conf. Commun. (ICC)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algorithmic Aspects of the Time Synchronization Problem in Large-Scale Sensor Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Servetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM/Kluwer Mobile Networks and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="491" to="503" />
			<date type="published" when="2003">2005. 2003</date>
		</imprint>
	</monogr>
	<note>Special issue with selected (and revised) papers from ACM WSNA</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the Scalability of Cooperative Time Synchronization in Pulse-Connected Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Servetto</surname></persName>
		</author>
		<ptr target="http://cn.ece.cornell.edu/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<title level="m">The Information Theory Approach to Communications</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Longo</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
	<note>chapter Multiterminal Source Coding</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards a General Theory of Source Networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Csiszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Körner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How to Encode the Modulo-Two Sum of Binary Sources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Körner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="221" />
			<date type="published" when="1979-10-02">1979. October 2, 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A Cautionary Perspective on Cross-Layer Design</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kawadia</surname></persName>
			<affiliation>
				<orgName type="collaboration">IEEE Wireless Comm. Mag</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
			<affiliation>
				<orgName type="collaboration">IEEE Wireless Comm. Mag</orgName>
			</affiliation>
		</author>
		<ptr target="http://decision.csl.uiuc.edu/˜prkumar/" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Information Theory and Communication Networks: An Unconsummated Union</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ephremides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hajek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2416" to="2434" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Noiseless Coding of Correlated Information Sources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Slepian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory, IT</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="471" to="480" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiple Access Channels with Arbitrarily Correlated Sources</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A El</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="648" to="657" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Note on the Multiple Access Channel with Correlated Sources</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory, IT</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="232" to="235" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Coding Theorem for Multiple Access Channels with Correlated Sources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Slepian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1037" to="1076" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On Source Coding with Side Information via a Multiple-Access Channel, and Related Problems in Multi-User Information Theory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ahlswede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="396" to="411" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Discrete Memoryless Multiple Access Channel with Partially Cooperating Encoders</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M J</forename><surname>Willems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="445" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Slepian-Wolf-Cover Theorem for a Network of Channels</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Contr</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="83" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Ahlswede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><forename type="middle">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network Information Flow. IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1204" to="1216" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Network Information Flow: Limits and Achievability</title>
		<author>
			<persName><forename type="first">S</forename><surname>Borade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Inform. Theory (ISIT)</title>
		<meeting>IEEE Int. Symp. Inform. Theory (ISIT)<address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linear Network Coding</title>
		<author>
			<persName><forename type="first">S.-Y</forename><forename type="middle">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An Algebraic Approach to Network Coding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Médard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Networking</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="782" to="795" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linear Network Codes: A Unified Framework for Source, Channel, and Network Coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Effros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Médard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discrete Mathematics and Theoretical Computer Science (DIMACS) series on Network Information Theory</title>
		<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Network Coding for Correlated Sources</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Médard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Effros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 38th Annual Conf</title>
		<meeting>38th Annual Conf<address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-03">March 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Network Coding in Undirected Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 38th Annual Conf</title>
		<meeting>38th Annual Conf<address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-03">March 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Capacity of Wireless Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="388" to="404" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the Maximum Stable Throughput Problem in Random Networks with Directional Antennas</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Servetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MobiHoc</title>
		<meeting>ACM MobiHoc<address><addrLine>Annapolis, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Capacity, Stability and Flows in Large-Scale Random Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Servetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Inform. Theory Workshop (ITW)</title>
		<meeting>IEEE Inform. Theory Workshop (ITW)<address><addrLine>San Antonio, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to Algorithms</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>nd ed</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Complexity Classification of Network Information Flow Problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/SIAM Symp. Discr. Alg. (SODA)</title>
		<meeting>ACM/SIAM Symp. Discr. Alg. (SODA)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spectral Efficiency in the Wideband Regime</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verdú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1319" to="1343" />
			<date type="published" when="2002-10-02">2002. October 2, 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Directed Diffusion for Wireless Sensor Networking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Intanagonwiwat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Estrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heidemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Networking</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="16" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simultaneous Optimization for Concave Costs: Single Sink Aggregation or Single Source Buy-at-Bulk</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/SIAM Symp. Discr. Alg. (SODA)</title>
		<meeting>ACM/SIAM Symp. Discr. Alg. (SODA)<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable Source/Channel Decoding for Large-Scale Sensor Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tüchler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Commun. (ICC)</title>
		<meeting>Int. Conf. Commun. (ICC)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rate Distortion Theory: A Mathematical Basis for Data Compression</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Balancing Transport and Physical Layers in Wireless Multihop Jointly Optimal Congestion Control and Power Control</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE. J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="116" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
