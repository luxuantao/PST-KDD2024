<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interaction on the Edge: Offset Sensing for Small Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ian</forename><surname>Oakley</surname></persName>
							<email>ian.r.oakley@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Design and Human Engineering</orgName>
								<orgName type="institution">UNIST UNIST-gil 50</orgName>
								<address>
									<postCode>689-798</postCode>
									<settlement>Ulsan</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Doyoung</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Design and Human Engineering</orgName>
								<orgName type="institution">UNIST UNIST-gil 50</orgName>
								<address>
									<postCode>689-798</postCode>
									<settlement>Ulsan</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interaction on the Edge: Offset Sensing for Small Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6501440105227740E2BC6990F0B3238C</idno>
					<idno type="DOI">10.1145/2556288.2557138</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Touch</term>
					<term>Pointing</term>
					<term>Mobile Devices</term>
					<term>Edge-of-device input H.5.2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The touch screen interaction paradigm, currently dominant in mobile devices, begins to fail when very small systems are considered. Specifically, "fat fingers", a term referring to the fact that users' extremities physically obstruct their view of screen content and feedback, become particularly problematic. This paper presents a novel solution for this issue based on sensing touches to the perpendicular edges of a device featuring a front-mounted screen. The use of such offset contact points ensures that both a user's fingers and the device screen remain clearly in view throughout a targeting operation. The configuration also supports a range of novel interaction scenarios based on the touch, grip and grasp patterns it affords. To explore the viability of this concept, this paper describes EdgeTouch, a small (6 cm) hardware prototype instantiating this multi-touch functionality. User studies characterizing targeting performance, typical user grasps and exploring input affordances are presented. The results show that targets of 7.5-22.5 degrees in angular size are acquired in 1.25-1.75 seconds and with accuracy rates of 3%-18%, promising results considering the small form factor of the device. Furthermore, grasps made with between two and five fingers are robustly identifiable. Finally, we characterize the types of input users envisage performing with EdgeTouch, and report occurrence rates for key interactions such as taps, holds, strokes and multi-touch and compound input. The paper concludes with a discussion of the interaction scenarios enabled by offset sensing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Technological advances are enabling us to embed more and more computational power into smaller and smaller packages. This is valuable and, indeed, very small devices (e.g. with dimensions of around 6 cm and below <ref type="bibr" target="#b1">[2]</ref>) have already opened up new application areas in the domains of mobile and wearable computing. These include systems that support unobtrusive health monitoring (www.fitbit.com), tangible gaming (www.sifteo.com) or serve as fully-fledged mobile media or communication devices (www.imwatch.it) or remotes that interface with such tools <ref type="bibr" target="#b8">[9]</ref>. The benefits of miniaturization are reported to be substantial and include wearability <ref type="bibr" target="#b6">[7]</ref>, comfort, portability and aesthetics.</p><p>However, interaction with small devices presents novel challenges. Most prototypes currently take the form of flat slabs sporting touch-screens [e.g. 8], a practical setup also used in larger devices such as smartphones. Although sophisticated display technology allows such systems to provide high-resolution and expressive output despite their diminutive dimensions, standard touch-screen interaction techniques do not scale-down so well <ref type="bibr" target="#b10">[11]</ref>. A key reason for this is fat-finger problem <ref type="bibr" target="#b12">[13]</ref>, a phrase referring to the fact that touching an interactive screen inevitably obscures the targeted content, lowering selection accuracy and hiding graphical feedback. A lack of screen real estate also means that effective solutions to this problem on larger devices, such as providing offset graphical cues <ref type="bibr" target="#b16">[17]</ref> are ineffective on very small systems.</p><p>Inspired by everyday devices such as watches and the side mounted controls (e.g. dials, buttons) they typically feature, this paper introduces a novel solution to the problem of interacting with small devices based around incorporating an array of touch sensors into the edge of a system with a front-mounted screen. As it is fully offset, at 90° to the screen, touches to such a sensor system inherently avoid obscuring graphical content. The remainder of this paper assesses the novelty and viability of this idea by positioning it against related work, by describing EdgeTouch (see Figure <ref type="figure" target="#fig_0">1</ref>), a prototype that realizes this functionality and by presenting a series of studies on this device and concept. These studies assess basic targeting performance, capture the kinds of grips and grasps used to hold the device and explore how users conceive of interacting with this kind of functionality. The paper closes with a discussion of the contributions and limitations of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>The problems of interaction with miniature or wearable computational devices have attracted considerable research attention. Much work has focused on supporting target selection tasks without obscuring graphical contents. For instance, Baudisich and Chu <ref type="bibr" target="#b1">[2]</ref> present a highly effective system that incorporates touch input on the rear of a device, a setup that requires the system be held in free space (e.g. it cannot be resting against the body) and obscures the user's finger but ensures that the screen remains visible at all times. The concept has been extended by a range of other authors to encompass the design of more sophisticated interaction techniques, such as those based on pinches <ref type="bibr" target="#b17">[18]</ref>, and to domains such as authentication <ref type="bibr" target="#b5">[6]</ref>. Another approach has been to explore techniques that enable pointing input that is fully decoupled from the device itself. Examples include Butler et al.'s optical system <ref type="bibr" target="#b3">[4]</ref> and Harrison and Hudson's <ref type="bibr" target="#b7">[8]</ref> use of a magnetic peripheral to enable cursor control to take place in the area adjacent to a device's screen. A final strand of work has looked at extending input from the device screen to an adjacent area using physical controllers that transmit and route a user's touches to particular optimized zones on the display <ref type="bibr" target="#b18">[19]</ref>.</p><p>This paper tackles the same problem space as this prior work, but aims to derive a solution capable of operating in a greater variety of situations. These include typical use cases such as when a device is worn, as in the watch scenario shown in Figure <ref type="figure" target="#fig_1">2</ref> (left). In such scenarios the back of the device is inaccessible and it is undesirable to rely on additional handheld equipment such as styli -arguably, the very point of such systems is that they do not require a user to hold equipment in their hands in order to interact. As such, we believe the offset sensing paradigm has the potential to add value to this design space -indeed in some ways it can be seen as an extension of the buttons that traditionally adorn the edge of watches to include high resolution, multi-touch position input. A second motivating scenario comes in the form of small computers that can be fully held in the hand. The form factor of such devices might resemble a coin, or a loosely attached wearable object such as a pendant. We argue that touches to the edge of such devices, as shown in Figure <ref type="figure" target="#fig_1">2</ref> (right), represent a natural, confortable way to interact without occluding content on a front-mounted screen. With devices this small, we suggest that around-device interaction is infeasible -the devices are held in the hand -and that interaction on the back of the device will likely require grasps that either involve two hands (one to hold and the other to interact, such as those showcased in Baudisch and Chu <ref type="bibr" target="#b1">[2]</ref>) or that occlude the screen -essentially pinches the front and back surfaces (such as those discussed by Wolf et al. <ref type="bibr" target="#b17">[18]</ref>).</p><p>Other work on small or wearable devices has looked at how physical structures around the edges of screens can support gestural and targeting tasks. For example, both Blasko and Feiner <ref type="bibr" target="#b2">[3]</ref> and Ashbrook et al. <ref type="bibr" target="#b0">[1]</ref> present detailed examinations of how a physical bevel can support pointing activity around the front-facing perimeter of watch-like devices. These studies differ from the current investigation in that they focus on how raised ridges can support interaction with a touchscreen, whereas the current study looks at the unexplored potential of offset touches to the sides, rather than the front-facing extremities, of a device. Edge based interaction has also been recently explored in the context of pressure based input. For example, Spelmezan et al. <ref type="bibr" target="#b14">[15]</ref> describe a system in which a small number of pressure sensors are mounted around a mobile phone in order to detect squeeze-based input. The work in this paper complements this promising input modality by investigating a relatively high-resolution position sensor and general pointing operations rather than pressure input.</p><p>Scholars have also explored the use of touch sensors mounted around a mobile device to support context-sensing interactions. For instance, Taylor and Bove describe Graspables <ref type="bibr" target="#b15">[16]</ref>, a system in which the pattern of touches on an array of sensors spread all over a device is used to infer user intent. Song et al. <ref type="bibr" target="#b13">[14]</ref> present a broadly similar grasp sensing system built into a stylus for and capable of tasks such as automatically changing the tool in a paint program depending on how the stylus is held. More recently, this notion has being extended, refined and applied to current mobile device form factors. Cheng et al. <ref type="bibr" target="#b4">[5]</ref>, for example, describe a system in which capacitive sensors mounted along two edges of a tablet computer can be used to sense users' hand positions and adjust the location of an on-screen virtual keyboard so that it is always appropriately positioned under their fingers. Finally, in a radically different approach to the capacitive sensing paradigm, Sato et al. <ref type="bibr" target="#b11">[12]</ref> introduced swept frequency capacitive sensing, a technique capable of (among other things) inferring the number of fingers engaged in a touch from a single sensor. The work in this paper differs from this literature in that it uses of an array of capacitive sensors to detect highly localized touches in directed targeting tasks and other intentional (rather than inferred) interaction scenarios. In summary, much research attention has been directed to developing novel interaction techniques for small devices.</p><p>One key theme has been on extending touch-sensing capabilities to regions of the device other than the screen. This paper builds on this existing body of work by examining a novel configuration: a high-resolution position sensor situated around the edge of a device featuring a front mounted screen. It argues this arrangement is highly suitable for very small handheld or wearable devices and sets out to explore the validity of the claim by describing a prototype that realizes this functionality and a series of user studies that characterize how users interact with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EDGETOUCH SYSTEM</head><p>The EdgeTouch prototype is shown in Figure <ref type="figure" target="#fig_0">1</ref>. It is a 3D printed hollow disc with a removable lid, 1.8 cm in height, 6 cm in diameter and with a resultant circumference of 18.85 cm. A 2.7 cm square full color OLED screen with a resolution of 128 by 128 pixels (a 4DSystems µOLED-128-G1) was secured to the center of the lid using M3 bolts through integrated fixtures; it sat proud from the surface by 7 mm. The screen features an on-board graphics processor that can be controlled remotely via commands delivered over an RS232 serial link and is capable of rendering limited amounts of text and simple graphical primitives to the screen in real time. Although the screen is square, a disc shaped housing was chosen to ensure an unambiguous oneto-one mapping between touches to the edge of the unit and positions on a circular region around the center of the screen. This ensured the relationship between touches to the edge of the device and the corresponding position on the front-mounted screen was as clear as possible.</p><p>Capacitive sensing was implemented with two Sparkfun breakout boards featuring the MPR121 capacitive sensing microprocessor from Freescale. These were fully enclosed within the prototype. Each three-by-two cm board has the ability to read 12 individual binary sensors and features inbuilt auto-calibration functionality that optimizes sensing parameters on power-up. The 24 sensing electrodes were positioned in holes situated equidistantly (e.g. with a 7.8 mm inter-sensor spacing) around the mid-point of the rim of the disc. They took the form of simple eight mm M3 bolts (five mm head diameter) secured to the plastic shell by nuts and wired directly to the jumper points on the two MPR121 boards. The bolts were screwed as flush as possible to the surface of the disc and, in order to provide a smoother texture, a layer of electrical tape was wound around the rim of the device. The MPR121 sensor boards communicate using the I2C protocol. Figure <ref type="figure" target="#fig_0">1 (b)</ref> shows the internals of this hardware setup; the ultimate dimensions of the device were selected to minimize size while robustly enclosing all elements of the sensing and display hardware.</p><p>A remotely situated Arduino Mega 1280 interfaced with the prototype and communicated, via a second RS232 serial link, to a host PC as and when required. In order to minimize latency, most computation took place on the Arduino and the link to the PC was used primarily to log data and issue high-level commands. The Arduino polled the sensor boards 100 times a second and distributed commands to screen sporadically and in response to feedback requirements and application logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EdgeTouch Sensor Software</head><p>Data from EdgeTouch's 24 individual sensors were interpolated to create 48 uniquely touchable locations -if two adjacent sensors were simultaneously active, a touch was recorded at the mid-point, leading to a uniquely identifiable location every 7.5 degrees (or 3.9 mm). As this paper is largely concerned with intentional input rather than naturalistic grasp patterns [e.g. <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref> the sensor data was also processed to more reliably identify individual touches. Essentially, small blocks of adjacently selected sensor locations (1-3) were resolved to a single central touch while larger blocks (4-7) were treated as two touches and still larger blocks simply ignored. All detected touches were marked as small round brightly colored cursors drawn on the edge of a 2.7 cm diameter circle centered at the midpoint of the screen. The rim of this circle was always 1.65 cm distant from touches made on the rim of the EdgeTouch device; see Figure <ref type="figure" target="#fig_0">1 (c)</ref> and<ref type="figure">(d</ref>) for examples of this feedback. Finally, in order to provide a more consistent experience during targeting operations that involved movement on the device surface, changing patterns of sensor activation were processed and cursors that animated smoothly to match such dynamic, persistent touches were presented to users. This simple tracking process stored current contact points as cursor locations and, in subsequent sensor readings, associated adjacent sensor activations with these cursors and smoothly animated their on-screen representations towards these new positions. Depending on movement speed over the device surface, this smoothing process led to a small latency (approx. 100ms) in the accuracy of the cursor positions rendered on the screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USER PERFORMANCE STUDY</head><p>They were two stages and goals to this study. In the first stage the objective was to investigate targeting performance -whether touches to the side of the EdgeTouch device would allow users to rapidly and reliably select targets displayed on the front mounted screen. In the second stage, conducted immediately after the completion of the first, the goal was to characterize the basic set of grips and grasps used to hold the device. In contrast to the single-touch input in the first stage, the second stage of the study was primarily descriptive and looked at multiple simultaneous contact points. These studies are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>The same twelve participants (nine male, mean age 28) completed both stages of the study. All were students or staff at an affiliated research institute and none were compensated. Two were left-handed and, on ratings out of 5, all reported they were experienced with computers (5), smartphones (4.5) and touch-screens (4.3). None had prior exposure to the EdgeTouch prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeting Study: Experimental Design</head><p>This investigated targeting performance using the EdgeTouch prototype. To achieve this aim meaningfully, two aspects of the targets were systematically varied: their size and polar position. Three size conditions were considered: small, medium and large respectively spanning one, two and three sensor locations (representing 7.5, 15 and 22.5 degrees of angular space). These were presented in a fully balanced repeated measures design -all participants completed all conditions in one of six possible orders. Furthermore, for each condition targets centered on every one of the 48 uniquely detectable locations was presented in a random order. There were three runs through this set of locations for each size condition, the first of which was discarded as practice. This led to an experimental data set composed of 288 selections (48 targets by 2 presentations by 3 conditions) for each participant. Completing this stage of the study typically took 45 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeting Study: Procedure</head><p>All instructions in this stage of the study were shown on the EdgeTouch device. In order to minimize confounding behaviors that might emerge from different postures and grips, participants were instructed to start each trial with the device (screen upright) in both hands and held between their thumb and middle fingers, as shown in Figure <ref type="figure" target="#fig_0">1 (d)</ref>. Each trial then began by requesting that the participant tap the side of the device. After releasing this touch, a fixation spot was displayed for 500ms, followed by a target in the form of a 10 pixel deep red polygon occupying an appropriate position and portion of the polar space. Participants then had to select this target. This was achieved by touching the edge of the device, at which point a cursor was displayed, either green (if the over the target) or red (if not). Movements across the surface of the device adjusted the cursor position and a selection event was recorded only when the finger was released. In the case of erroneous selections, data were noted, but participants were required to complete the trial again before finishing the condition. Figure <ref type="figure" target="#fig_0">1</ref> (d) shows a trial in progress -a participant has correctly touched the device over a target with their right index finger and simply needs release the touch to successfully complete the trial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeting Study: Hypothesis and Measures</head><p>The targeting experiment had multiple objectives. First and foremost it sought to capture data expressing the viability of touching the rim of a handheld device for target selection tasks. Secondly, it aimed to explore this input space in more depth by recording variations in performance caused by changes to target size. A third goal was to explore the impact of the polar position of targets. We expected that smaller targets would incur longer selection times and increased errors and also that particular angular portions of the device (e.g. those under the index fingers) would afford more rapid and reliable access. Consequently, measures used in the study included touch time, the point from the presentation of a target until an initial touch of the sensors and hold time, the time until the sensors were released. These time data were calculated only for correct trials. The location of both the initial and final touches were also recorded and used to classify each trial into one of four categories: a miss (when neither initial nor final touch was over the target); a slip-off (when the initial touch, but not  the final touch matched the target); a slide-on (when the final, but not the initial, touch was on target) and its complement in the 96 correct trials in each condition, a tap (both touches hit the target).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeting Study: Results</head><p>The mean timing and outcome data from the three target size conditions are presented in Figures <ref type="figure" target="#fig_3">3</ref> and<ref type="figure" target="#fig_2">4</ref>. In order to reduce the data from the 48 target locations to a manageable size for analysis, it was clustered into eight angular segments, each aggregating trials from six targets centered on a 45-degree region aligned along one of the eight cardinal or ordinal directions. This summary data for both time measures and trial outcomes is shown in the five radar diagrams in Figure <ref type="figure">5</ref>. Data for taps are not shown, as these are simply the complement of the slide-on data.</p><p>All data were analyzed using two-way repeated measures ANOVAs followed by post-hoc t-tests incorporating Bonferroni confidence interval corrections. In cases when the data violated the sphericity assumption, Greenhouse-Geisser corrections were used. Effect sizes are reported in the form of partial eta squared (η p 2 ), a figure that represents the proportion of otherwise unaccounted for variance that can be attributed to the effects of each variable. The error data for the angular position variable was too sparse and irregularly distributed to support formal analysis -several conditions resulted in zero errors, while others exhibited high error rates indicative of outlier performance such as, in one condition, a single participant recording eight errors on the same target (from a total of 13 in whole experiment). Consequently, the angular position variable was collapsed for the miss and slip-off error measures and these data were analyzed with one-way repeated measures ANOVAs.</p><p>In terms of the time data, no significant interactions were uncovered in either metric (both at p&gt;0.37). However, both touch time (F (2, 22) = 5.95, p&lt;0.01, η p 2 = 0.351) and hold time F (2, 22) = 122.34, p&lt;0.001, η p 2 = 0.918) led to significant main effects of target size. The pair-wise tests revealed all differences to be significant for hold time (all p&lt;0.01), but that only the small condition differed from the medium and large conditions in terms of touch time (both at p&lt;0.05). Similarly, in terms of angular target position, both touch time (F (7, 77) = 50.37, p&lt;0.001, η p 2 = 0.821) and hold time (F (3.74, 41.13) = 3.852, p&lt;0.05, η p 2 = 0.259) resulted in significant main effects. However, the post-hoc tests showed few differences between angular locations in terms of hold time -only the top-right segment (which would have been positioned directly under a participant's right index finger) showed faster performance than the top and left segments (both p&lt;0.05). Table <ref type="table">1</ref> shows the significant pairwise comparisons from the touch time data. These findings can be summarized with the observation that targets towards the top of the device, close to participants' index fingers, led to more rapid targeting performance.</p><p>Data describing the ratio of taps/slide-on trials did not result in a significant interaction (F (14, 154), = 1.67, p=0.07, η p 2 = 0.13), but both main effects attained significance: target size (F (2, 22), = 268.5, p&lt;0.01, η p 2 = 0.961) and angular segment (F (3.48, 38.25), = 4.48, p&lt;0.01, η p 2 = 0.29). Posthoc tests showed all size conditions to significantly differ (all p&lt;0.01), but, in terms of angular location, that only the top right and bottom segments were more readily targeted (e.g. had a higher proportion of taps) that the rightmost segment. Based on the hand pose used in the study, these are locations that are positioned, respectively, in easy reach of the right index finger and either thumb. The error outcomes also led to significant trends in the size variable: slip-off (F (1.15, 12.6) = 15.3, p&lt;0.01, η p 2 = 0.75) and miss (F (1.17, 12.83) = 32.07, p&lt;0.01, η p 2 = 0.58). Post-hoc tests showed the small condition led to greater errors (misses and slip-offs) than the other two conditions (both at p&lt;0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeting Study: Discussion</head><p>The targeting study had three objectives: to determine the viability of the underlying concept of input on the edge of a device and explore how performance varied with both target size and polar position. In regards to the first objective, the results are positive: they indicate that participants were able to select targets rapidly and effectively using EdgeTouch. Total task completion times and accuracy rates in the large condition, which enables 16 non-overlapping polar targets each sized at 22.5°, were fast (at 1.25 seconds) and accurate (3.8% error rate). These figures speak for themselves and also compare favorably to data reporting in prior studies of targeting on small devices.</p><p>For example, in Ashbrook et al.'s <ref type="bibr" target="#b0">[1]</ref> study of radial targets on the surface of a touch screen error rates of 28% are reported for 15° targets and 13% for 30° targets (when targets are sized such that they occupy 50% of the total display area). These rates are substantially worse than those observed in the large condition with EdgeTouch -a minimum of three times greater. Furthermore, as Ashbrook et al.'s system relied on targets that were both displayed on, and interacted with via, the surface of a touch screen, performance also varied with the depth of the targetsbasically how far they graphically extended to the center of the screen. With shallow targets situated only around the rim of the device, performance dropped dramatically -the smallest targets considered, approximately three mm in width, led to projected error rates of 62%. EdgeTouch, by relying on input around its edge rather than its front surface, avoids this problem and performance does not depend on the depth of on-screen targets. Indeed, the targets used in the current study were only 2.1mm (10 pixels) deep, freeing up much of the limited device screen space for other content. This evidence suggests that that the offset sensing paradigm used in EdgeTouch offers advantages over radial targets on the front of a touch screen: it improves considerably on accuracy and frees up screen real estate.</p><p>It is also useful to contrast performance in the current study with Baudisch and Chu's <ref type="bibr" target="#b1">[2]</ref> work examining item selection via touches to the back of a 2.4-inch (6 cm) device featuring 12 targets. In this work, target selection times and error rates are reported to be between one and three seconds and five and 30 percent, depending on input style and target size.</p><p>Looking at the optimal conditions from these data, we can conclude these are broadly comparable to those from EdgeTouch. This is an encouraging result that suggests that both back and edge make equally viable surfaces for pointing input on small-screen devices. Furthermore the edge of a device is available in situations, such as those involving a worn object like a watch, when the back is not. Finally, Harrison and Hudson <ref type="bibr" target="#b7">[8]</ref> describe polar targeting performance in free space in a system that senses the angle between a screen and a magnet mounted on a user's finger. With 16 targets, task completion time is approximately two seconds, 60% greater than that recorded with EdgeTouch while error rates remain broadly comparable at five percent. This suggests that the physical contact required to select radial targets in EdgeTouch may facilitate more rapid use than the free floating near-device interaction space investigated by Harrison and Hudson.</p><p>Moving beyond these comparisons, it is unsurprising to note that as target sizes decreased, performance dropped. In the medium condition targets of 15° led to modest decreases in performance, suggesting that arrangements of 24 unique targets may well be viable. In the small condition, with 48 separate 7.5° targets, performance lowered further to 1.75 seconds and 18% errors, figures that still compare favorably to those in the literature -both Ashbrook et al.</p><p>[1] and Harrison and Hudson <ref type="bibr" target="#b7">[8]</ref> report error rates of approximately 40%-60% in such situations. This suggests that EdgeTouch's offset pointing technique scales better than these prior approaches.</p><p>Performance also varied significantly according to polar position -given the hand pose participants were asked to adopt, areas within easy reach of the index fingers and thumbs tended to result in the fastest and most error free performance. This highlights a close relationship between hand posture and performance that needs to be taken into account during the physical design of any system implementing the kind of offset targeting functionality discussed in this paper. One simple design strategy would be to adjust the angular size of targets according to expected hand pose -basically to deploy small targets in easy to reach locations and use larger targets elsewhere.</p><p>Finally, in an exploratory analysis to better understand the rise in hold times between conditions, we examined this data on a target-by-target basis (Figure <ref type="figure" target="#fig_5">6</ref>, top left). It shows a prominent zig-zag in the small condition. To explain this, we conducted a two-way repeated measures ANOVA on the hold time data from the small condition with variables of angular region (eight levels, as in the main analysis) and the parity of each target location -basically whether it was even (based on contact with a single sensor) or odd (an interpolated location requiring a simultaneous touch to two sensors). This led to a significant interaction effect (F (3.3, 36.17 A candidate explanation for this effect is based on the size of contact area involved in typical touches to the device: left and right touches likely involved the relatively small finger tip while top and bottom touches involved the larger finger or thumb pad, as illustrated in Figure <ref type="figure" target="#fig_5">6</ref> (bottom). The different sizes of these contact areas facilitated activation of different numbers of EdgeTouch sensors -with a small finger tip contact region, selecting an interpolated twosensor target was challenging, while the inverse was true for larger finger pad touches. While this effect is largely due to the resolution limitations of the current sensor system, these results also demonstrate the importance of considering finger and hand posture in the design of curved touch input surfaces. Different positions result in widely different reach and contact profiles, variations that need be considered and incorporated into the design of interfaces.</p><p>Overall, the results of the study endorse the idea that offset input on the sides of a device is well suited to target selection tasks on very small computers. It shows good basic performance and demonstrates advantages over a range of prior approaches. These include that high accuracy can be maintained with small visual targets, that edge interaction is available in scenarios involving worn devices and finally that the physical contact implied by touching the device may lead to higher performance that in near-device interaction scenarios. We also determined that limitations of the sensor arrangement in the current hardware prototype affected the results. However, these helped highlight important aspects of the physical act of touching a curved edge surface and mainly affected the smallest targets studied. The main conclusions of this study are drawn from the larger targets that were immune to such issues and maintain their validity -showing that edge interaction is a rapid, accurate and effective way to conduct targeting tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grasp Study: Experimental Design and Procedure</head><p>The goal of this stage of the study was to explore the how the EdgeTouch prototype is grasped and the viability of detecting such grasps. This activity was descriptive rather than inferential -it sought to capture data regarding the grips and grasps participants performed rather than compare between any particular conditions. Accordingly, it was simply structured and composed of three repeating sets of four trials, the first set of which was considered practice and not analyzed. In each set, participants picked up the EdgeTouch prototype by its rim with two, three, four and finally all five digits on their dominant hand. When it was held comfortably, they pressed a key on an adjacent laptop with their non-dominant hand. This logged the grasp information and they then put the device back down. Within these constraints, they were instructed to hold the device in any way they liked -including grasps that obscured the device screen. To facilitate this process, all instructions for this stage of the study were presented on a laptop computer in front of the participants. In total 96 grasps were captured, eight from each participant. Participants were allowed to rest before starting this stage of the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grasp Study: Results and Discussion</head><p>EdgeTouch records relative touch positions around its rim. This data was aggregated as follows. Firstly, eight trials (from 96) in which the requested number of fingers were not detected were discarded. These cases fell into two error categories. Either the user made contact with a large number of adjacent sensors, making it impossible to determine the number and placement of a user's digits (two trials) or there was one too few digits recorded (six trials). This latter category is likely due to the discrepancy between the physical depth of the EdgeTouch device (1.8cm) compared with the depth of the sensor electrodes (0.5cm)basically one finger failed to reach the sensors. As these errors all occurred in trials when participants needed to grip the device with all five digits, the limited reach of users' baby (or pinky) fingers is the likely cause of this effect.</p><p>After removing these trials, the rest of the touch data was transformed to inter-touch intervals -the angular gap between each touch. These were then sorted into descending order and used to calculate sequential inter-touch intervals for the dataset as a whole (e.g. the mean largest interval, the mean second-largest interval, and so on). These data are plotted in Figure <ref type="figure" target="#fig_7">7</ref>, arranged such that the largest two distances originate at the base of each diagram. This figure suggests that the noticeably isolated bottommost touch represents the thumb and detecting this digit would be trivial. Indeed, one-way ANOVAs and posthoc t-tests on the distances generated in the three, four and five digit conditions show the largest two inter-touch distances are always greater than the others. All main effects and t-tests were significant at p&lt;0.01 or lower, and the F values were as follows: three touches (F (2, 44) = 237, η p 2 =0.915), four touches (F (2.1, 48.23) = 234, η p 2 =0.934) and five touches (F (2.28, 36.52) = 272, η p 2 =0.944).</p><p>In sum, the results of this experiment suggest that the EdgeTouch is suitable for interaction techniques based on detecting grips -study participants were able to produce simple, highly distinctive grasps with a high level of accuracy. It was also possible to extract higher-level features, such as determining which contact point represents the thumb with a trivial analysis. These represent valuable interaction primitives by themselves and, in the future, we suggest that combining grip detection with inertial sensing could disambiguate further information about hand pose. For example, specific device angles with respect to gravity could be combined with the current system to infer which hand is holding a device, or to identify index fingers in a multi-finger grasp, increasing the richness of interaction space available for devices featuring offset sensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USER MAPPINGS STUDY</head><p>Encouraged by these results, we ran a follow-up study to elicit the kinds of gestures and interactions that users naively perform on the EdgeTouch hardware. In order to achieve this objective, we deployed a variation on Lee et al's <ref type="bibr" target="#b9">[10]</ref> methodology to capture and describe interactions with imaginary future devices. Essentially, participants were asked to perform a range of activities, such as menu navigation or setting adjustment, on a simulated media player interface shown on the EdgeTouch prototype. Their task was to devise interactions they would use to achieve these activities naturally and effectively. In this way, we sought to understand how users conceive of interacting with EdgeTouch in terms of both the range and frequency of the proposed ideas. We believe the results of this user-led study will help inform the design of effective interaction techniques for the offset sensing paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants, Experimental Design and Tasks</head><p>Ten participants (five male, mean age 23.3, all righthanded) completed this experiment. All were either students or recent graduates and they were compensated with approx. $10. On average, they indicated they had been smartphone users for more than 3 years and had experience with (or owned) four small digital devices such as mp3 players.</p><p>None had any prior exposure to the EdgeTouch prototype.</p><p>To provide a context for the tasks performed in this experiment, an interactive image-based prototype of an interface to a personal media player was implemented. It featured three types of menu interface (list, grid and pie) as well as volume, radio and setting control screens. Example on-device screen shots can be seen in Figure <ref type="figure" target="#fig_8">8</ref>. Commands were based on those in existing media players and, in total, 14 tasks were modeled, encompassing menu navigation (variously up/down, left/right, and rotate left/rotate right), media playback (play/pause, rewind/fast-forward, previous/next and volume up/down), radio use (volume up/down and frequency up/down) and toggling settings (set/unset). In line with similar studies in prior work <ref type="bibr" target="#b9">[10]</ref>, participants' task was to report on two types of input they would make to achieve the operations naturally. To create a richer data set they were asked to consider both one-handed and two-handed operation of the device. After defining gestures they were also rated their preference for each of their gestures on a five-point scale. In total each participant defined 112 gestures (14 tasks by two directions by two repetitions with one and two hands) in an hour-long session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>At the start of the study participants completed a brief demographics questionnaire, then watched a 60 second video showing basic EdgeTouch gestures like tapping, double tapping or holding a touch and swiping (or stroking) over the device's edge. Movements of the device such as traditional tilting (used in many current mobile devices) and spinning clockwise or anti-clockwise were also shown. Finally, the multi-touch capabilities of the device and example input combinations (e.g. hold and tilt, two finger swipe, etc.) were demonstrated. Participants were free to ask questions and the experimenter also gave a brief inperson introduction to the device and its features.</p><p>The study then started. This took the form of a wizard of Oz simulation in which the experimenter remotely manipulated on-device content in accordance with the input the users stated they were making. Users were given freedom to select and define input and gestures composed of one or more touches and movements on the device edge and any sequence of device rotations. There were no instructions or restrictions regarding grasp posture and participants verbally reported the input techniques they were devising. Using a notation format composed of outlines of the EdgeTouch device and shorthand codes T for Tap, H for hold, for index-finger, R for ring-finger, etc.) the experimenter transcribed this information live, verifying it with participants when necessary. In total 1120 interaction techniques were recorded over the entire study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>This descriptive study provided a detailed summary of the diverse ways participants sought to interact with EdgeTouch. Physical contact with the device edge was integral to the vast majority of inputs -only 18% of inputs involved no contact and were based solely on device movements such as tilting. In contrast, 55% of gestures involved a single finger, 17% two fingers and, respectively, 5%, 3% and 2% were made with three, four and five fingers -a total of 27% of inputs were multi-touch. Participants also favored their dominant hands and use of their thumb and index fingers. Specifically, the right index finger accounted for 43% of inputs while the right thumb (21%) and the left index finger (17%) were also frequently employed and the right middle (7%) and left thumb (5%) made up much of the remaining usage. The input types selected were diverse -the most common input types were taps (16%) and strokes (16%) with a specific finger followed by taps at a specific device location (10%) and one finger holds (7%) and double-taps (5%). The remaining 45% of inputs were widely distributed over other action types. Participants also used compound techniques composed of different types of input regularly -20% of interactions involved two or more simultaneous or overlapping inputs, with the most common combinations being one and two finger hold plus stroke (11% and 9%) followed by a diverse set of other combinations at lower frequencies. Finally, overall, participants reported they were satisfied with their gestures -ratings of 3.8 out of five.</p><p>In terms of the different interaction tasks presented to participants, these can be broadly categorized as requiring translational movements (menu up/down or left/right), rotations (as in a pie menu) or issuing commands (select/set, etc.). Where possible, participants focused on generating spatial mappings between the tasks and their inputs -they made taps to appropriate edges to move in particular directions or issued strokes in the desired direction of movement. Device motions, such as tilting, were also used to achieve these operations and rotating the device around its screen was the most popular mechanism to navigate in the pie menu. To issue commands, when there was a clear correspondence between displayed items and the device edge (as in the pie menu), participants directly touched areas relating to desired targets. In other situations, specific finger combinations (e.g. right or left index finger) or device locations (e.g. top-right or top-left) were assigned functionality such as entering or returning from a menu.</p><p>Synthesizing this data in order to inform design, we conclude that users relied heavily on relating edge-based touches to spatial aspects of desired on-screen transitions. They particularly focused on taps and touches with individual fingers and grips and grasps with multiple fingers. In terms of such holds, the number of digits (or the specific fingers used) was frequently intended to signify input -e.g. a pinch between index finger and thumb might signify a volume change command, while a pinch with middle finger and thumb changes track. Finally, participants often combined inputs, such as holding and stroking. This analysis provides concrete recommendations for the design of offset sensing interaction techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION AND INTERACTION SCENARIOS</head><p>In sum, the results of the studies described in this paper demonstrate that offset sensing enables rapid, accurate targeting performance. It also offers a range of qualitative advantages over prior work: it frees up screen real estate, does not require additional equipment to support pointing and is accessible in situations when a device is worn against the skin. In addition, compared to existing physical controls (such as those on the rims of watches) it offers the advantages of spatial multi-touch input -users can touch input areas that clearly and unambiguously map to dynamic screen content. An examination of device grasp poses also suggests key properties of this modality, such as basic digit identification, can be readily determined. Finally, a userelicitation study provided insights into the ways naïve users conceive of interacting with offset sensing systems.</p><p>Drawing together these results, we argue that offset sensing has the potential to enable a wide range of novel interaction techniques and be applicable to a wide range of devices and scenarios. To support this point, we provide examples inspired by the study results at three levels: interaction technique, interface and application scenario. In terms of interaction technique, Figure <ref type="figure" target="#fig_9">9</ref> (top) illustrates how touches to the edge of a circular device could be used to detect a novel -rotations due to the synchronized movements of two or more fingers around the device surface, similar to how we typically operate a fixed dial or knob. As it is not reliant on inertial sensing, this technique may be relatively high precision and immune to noise from other bodily activities and movements <ref type="bibr" target="#b4">[5]</ref>. In terms of interface, figure <ref type="figure" target="#fig_9">9</ref> (bottom left) shows a simple menu design in which the thumb selects a top-level menu (shown in green) and the index finger completes a pinch gesture to select a menu item (shown in red) -this technique seems ideal for worn devices, such as watches [e.g. 9]. Finally, in terms of application, Figure <ref type="figure" target="#fig_9">9</ref> (bottom right) highlights how edge sensing functionality could be integrated into larger and more advanced devices such as a camera. Inspired by the Lytro camera (www.lytro.com), a small device with a capacitive slider built into the edge just above the LCD viewfinder that controls zoom level, this example highlights the potential of offset touches to support interaction with a wide range of digital devices and tools.</p><p>Moving beyond such scenarios to practical limitations and avenues for future work, one key weakness of the current system is its resolution. Although cheap and effective, the results of targeting study suggest that a higher resolution system would improve performance. Further avenues for development include adding pressure-sensors to combine the interaction techniques discussed in this paper with those of Spelmezan et al. <ref type="bibr" target="#b14">[15]</ref> or integrating with Sato et al's <ref type="bibr" target="#b11">[12]</ref> advanced finger detection capabilities. Additional studies to characterize gesturing and stroking performance on edgemounted sensors would also complement the data reported in this paper. Another key limitation relates to the preliminary nature of our current efforts to design meaningful interfaces. To address this issue, we plan to develop and extend the UI concepts from this paper into prototypes combining context inference <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> with explicit interaction. Ultimately, we believe that the offset sensing proposed in this paper will help enable rich expressive interactions on very small mobile and wearable computers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Four views of the EdgeTouch prototype, showing: (a) the top mounted OLED screen; (b) the ring of 24 metallic capacitive sensors around the perpendicular edge of the device; (c) a three finger multi-touch grip with contact points highlighted with red cursors (left) and; (d) a target selection task with the target shown as a hollow red polygon and a green cursor marking the position of the right index finger.</figDesc><graphic coords="1,317.60,274.65,240.65,110.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Motivating scenarios for the EdgeTouch. Left shows multi-touch interaction with the edge of a bodymounted device such as a watch while right shows a device held in the hand -three contact points support interaction.</figDesc><graphic coords="2,334.10,62.90,99.06,123.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Mean number of trials completed as miss, slip-off, slide-on and tap events in user study.</figDesc><graphic coords="4,323.60,215.03,227.35,167.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Mean task completion time from three size conditions in user study. Bars show standard error.</figDesc><graphic coords="4,326.73,59.55,218.26,128.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Table 1 .</head><label>51</label><figDesc>Figure 5. Mean touch time, mean hold time, and mean number of slide-on, slip-off and miss trials shown according to targets in each of eight 45° angular segments. Participants' hand pose was such that their index fingers hovered over the upper left and right regions.</figDesc><graphic coords="5,312.39,331.10,250.32,134.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Top left chart shows mean hold times for 48 different sensor locations in three conditions in the user study. Top right chart shows mean hold times in the small condition by angular segment and parity -whether each location is addressed by contact with one or two sensors. Bottom images show typical fingers touches to side and top of device, with red highlighting differences in contact area (and green the sensor activation) that can explain this interaction.</figDesc><graphic coords="6,318.20,179.45,236.83,85.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>) = 11.154, p&lt;0.01, η p 2 = 0.503) shown in Figure 6 (top right), while neither main effect attained significance: angular region (F (3.4, 37.38) = 2.01, p=0.12, η p 2 = 0.154) or parity (F (1, 11) = 1.39, p=0.27, η p 2 = 0.11). These results indicate that it was quicker to select targets involving a single sensor in the left and right regions of the device while more rapid selections of targets in the top and bottom regions occurred when contact with two interpolated sensors was required.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Grasps of the EdgeTouch prototype with two (top left), three (top right), four (bottom left) and five (bottom right) digits of one hand. Fingers are shown on the outside of the circles. Positions are plotted from mean angular intervals between finger touches (after arrangement into descending order). Bars show standard deviation.</figDesc><graphic coords="7,77.93,161.68,95.04,95.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Images used in user mappings study showing three menu designs, music player, radio station and options UI</figDesc><graphic coords="8,61.75,585.35,225.40,112.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Interaction techniques and scenarios for offset sensing. Top: a roll gesture, in which a device is spun clockwise between the fingers (left to right). Bottom left: a menu and sub-menu accessed by touches to either side of a device. Bottom right: offset sensing in a camera-based scenario with sensors mounted around the rim of the lens.</figDesc><graphic coords="9,65.35,162.35,108.70,96.25" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the year of 2013 Research Fund of the Ulsan National Institute of Science and Technology (UNIST). Additional thanks to colleagues, staff and participants at the University of Madeira and M-ITI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An investigation into round touchscreen wristwatch interaction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ashbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MobileHCI&apos;08</title>
		<meeting>of MobileHCI&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="311" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Back-of-device interaction allows creating very small touch devices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI &apos;09</title>
		<meeting>of CHI &apos;09<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1923" to="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Interaction System for Watch Computers Using Tactile Guidance and Bidirectional Segmented Strokes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Blasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISWC&apos;04</title>
		<meeting>ISWC&apos;04</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SideSight: multi-&quot;touch&quot; interaction around small devices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST &apos;08</title>
		<meeting>of UIST &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="201" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">iGrasp: grasp-based adaptive keyboard for mobile devices</title>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings CHI &apos;13</title>
		<meeting>CHI &apos;13</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3037" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Back-of-device authentication on smartphones</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Von Zezschwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rubegni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Scipioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Langheinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI &apos;13</title>
		<meeting>CHI &apos;13</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Design for Wearability</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gemperle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kasabach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stivoric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISWC &apos;98</title>
		<meeting>ISWC &apos;98</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abracadabra: wireless, high-precision, and unpowered finger input for very small mobile devices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;09</title>
		<meeting>UIST &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="121" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Samsung unveils Galaxy Gear smartwatch accessory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kelion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="http://www.bbc.co.uk/news/technology-23961692" />
	</analytic>
	<monogr>
		<title level="j">BBC News, accessed Sept</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How users manipulate deformable displays as input devices</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI &apos;10</title>
		<meeting>CHI &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TapTap and MagStick: improving one-handed target acquisition on small touch-screens</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roudaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lecolinet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AVI &apos;08</title>
		<meeting>of AVI &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="146" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Touché: enhancing touch interaction on humans, screens, liquids, and everyday objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Poupyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI &apos;12</title>
		<meeting>of CHI &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fat Finger Worries: How Older and Younger Users Physically Interact with PDAs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Siek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Connelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERACT&apos;05</title>
		<meeting>INTERACT&apos;05</meeting>
		<imprint>
			<biblScope unit="page" from="267" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Grips and gestures on a multi-touch pen</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Benko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guimbretiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI &apos;11</title>
		<meeting>of CHI &apos;11<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Side pressure for bidirectional navigation on small devices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Spelmezan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Appert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chapuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietriga</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MobileHCI &apos;13</title>
		<meeting>MobileHCI &apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graspables: grasp-recognition as a user interface</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename><forename type="middle">V M</forename><surname>Bove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;09</title>
		<meeting>CHI &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="917" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shift: A Technique for Operating Pen-Based Interfaces Using Touch</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;07</title>
		<meeting>CHI&apos;07</meeting>
		<imprint>
			<biblScope unit="page" from="657" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PinchPad: performance of touchbased gestures while grasping devices</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Müller-Tomfelde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wechsung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TEI &apos;12</title>
		<meeting>TEI &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clip-on gadgets: expanding multi-touch interaction area with unpowered</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">P</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST &apos;11</title>
		<meeting>of UIST &apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="367" to="372" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
