<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-07-31">31 Jul 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
							<email>mandar90@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
							<email>danqic@cs.princeton.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Seattle</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
							<email>yinhanliu@fb.com</email>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Seattle</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
							<email>weld@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Seattle</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
							<email>omerlevy@fb.com</email>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Seattle</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-07-31">31 Jul 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1907.10529v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. Span-BERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-large, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0, respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even show gains on GLUE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-training methods like BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering <ref type="bibr" target="#b36">(Rajpurkar et al., 2016)</ref>, determining that the "Denver Broncos" is a type of "NFL team" is critical for answering the question "Which NFL team won Super Bowl 50?" Such spans provide a more challenging target for self supervision tasks, for example predict-ing "Denver Broncos" is much harder than predicting only "Denver" when you know the next word is "Broncos". In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution.</p><p>We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) to train the model to predict the entire masked span from the observed tokens at its boundary. Spanbased masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during fine tuning. Figure <ref type="figure" target="#fig_6">1</ref> illustrates our approach.</p><p>To implement SpanBERT, we build on a welltuned replica of BERT, which already outperforms the original BERT. While building on our baseline, we find that pre-training on single segments, instead of two half-length segments with the next sentence prediction (NSP) objective, significantly improves performance on most downstream tasks. Therefore, we add our modifications on top of the tuned single-sequence BERT baseline.</p><p>Together, our pre-training process yields models that outperform all BERT baselines on a wide variety of tasks, and reach substantially better performance on span selection tasks in particular. Specifically, our method reaches 94.6% and x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z Y + R s J i l E 2 D s + 9 + V G 7 h S 6 g 3 W Y i I = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R S 0 G X B j c s K 9 g F t K J P p p B 0 6 m Y S Z G 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e T a i F g 9 4 C z h f k T H S o S C U b T S Y B B R n A R h 9 j Q f e s N q z a 2 7 C 5 B 1 4 h W k B g V a w + r X Y B S z N O I K m a T G 9 D 0 3 Q T + j G g W T f F 4 Z p I Y n l E 3 p m P c t V T T i x s 8 W m e f k w i o j E s b a P o V k o f 7 e y G h k z C w K 7 G S e 0 a x 6 u f i f 1 0 8 x v P E z o Z I U u W L L Q 2 E q C c Y k L 4 C M h O Y M 5 c w S y r S w W Q m b U E 0 Z 2 p o q t g R v 9 c v r p H N V 9 9 y 6 d 9 + o N R t F H W U 4 g 3 O 4 B A + u o Q l 3 0 I I 2 M E j g G V 7 h z U m d F + f d + V i O l p x i 5 x T + w P n 8 A S Y q k b I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z Y + R s J i l E 2 D s + 9 + V G 7 h S 6 g 3 W Y i I = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R S 0 G X B j c s K 9 g F t K J P p p B 0 6 m Y S Z G 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e T a i F g 9 4 C z h f k T H S o S C U b T S Y B B R n A R h 9 j Q f e s N q z a 2 7 C 5 B 1 4 h W k B g V a w + r X Y B S z N O I K m a T G 9 D 0 3 Q T + j G g W T f F 4 Z p I Y n l E 3 p m P c t V T T i x s 8 W m e f k w i o j E s b a P o V k o f 7 e y G h k z C w K 7 G S e 0 a x 6 u f i f 1 0 8 x v P E z o Z I U u W L L Q 2 E q C c Y k L 4 C M h O Y M 5 c w S y r S w W Q m b U E 0 Z 2 p o q t g R v 9 c v r p H N V 9 9 y 6 d 9 + o N R t F H W U 4 g 3 O 4 B A + u o Q l 3 0 I I 2 M E j g G V 7 h z U m d F + f d + V i O l p x i 5 x T + w P n 8 A S Y q k b I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z Y + R s J i l E 2 D s + 9 + V G 7 h S 6 g 3 W Y i I = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R S 0 G X B j c s K 9 g F t K J P p p B 0 6 m Y S Z G 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e T a i F g 9 4 C z h f k T H S o S C U b T S Y B B R n A R h 9 j Q f e s N q z a 2 7 C 5 B 1 4 h W k B g V a w + r X Y B S z N O I K m a T G 9 D 0 3 Q T + j G g W T f F 4 Z p I Y n l E 3 p m P c t V T T i x s 8 W m e f k w i o j E s b a P o V k o f 7 e y G h k z C w K 7 G S e 0 a x 6 u f i f 1 0 8 x v P E z o Z I U u W L L Q 2 E q C c Y k L 4 C M h O Y M 5 c w S y r S w W Q m b U E 0 Z 2 p o q t g R v 9 c v r p H N V 9 9 y 6 d 9 + o N R t F H W U 4 g 3 O 4 B A + u o Q l 3 0 I I 2 M E j g G V 7 h z U m d F + f d + V i O l p x i 5 x T + w P n 8 A S Y q k b I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z Y + R s J i l E 2 D s + 9 + V G 7 h S 6 g 3 W Y i I = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R S 0 G X B j c s K 9 g F t Q o H W s P I 1 G M U s j V A a J q j W f c 9 N j J 9 R Z T g T O C 8 P U o 0 J Z V M 6 x r 6 l k k a o / W y R e U 4 u r T I i Y a z s k 4 Y s 1 N 8 b G Y 2 0 n k W B n c w z 6 l U v F / / z + q k J b / y M y y Q 1 K N n y U J g K Y m K S F 0 B G X C E z Y m Y J Z Y r b r I R N q K L M 2 J r K t g R v 9 c v r p F O v e W 7 N u 2 9 U m 4 2 i j h K c w w V c g Q f X 0 I Q 7 a E E b G C T w D K / w 5 q T O i / P u f C x H N 5 x i 5 w z + w P n 8 A S e u k b M = &lt; / l a t e x i t &gt;</p><p>x 3</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 M Z D 8 j u V E m l 7 F F u F D p m 0 t n o B j f Q = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q L u i y 4 c V n B P q A p Z T K d t E M n k z B z I 5 b Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K</p><formula xml:id="formula_0">D k 3 U R z G g W S d 4 L J b e 5 3 H r k 2 I l Y P O E 1 4 P 6 I j J U L B K F r J 9 y O K 4 y D M n m a D q 0 G l 6 t b c O c g q 8 Q p S h Q L N Q e X L H 8 Y s j b h C J q k x P c 9 N s J 9 R j Y J J P i v 7 q e E J Z R M 6 4 j 1 L F Y 2 4 6 W f z z D N y b p U h C W N t n 0 I y V 3 9 v Z D Q y Z h o F d j L P a J a 9 X P z P 6 6 U Y 3 v Q z o Z I U u W K L Q 2 E q C c Y k L 4 A M h e Y M 5 d Q S y r S w W Q k b U 0 0 Z 2 p r K t g R v + c u r p H 1 Z 8 9 y a d 1 + v N u p F H S U 4 h T O 4 A A + u o Q F 3 0 I Q W M E j g G V 7 h z U m d F + f d + V i M r j n F z g n 8 g f P 5 A y k y k b Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 M Z D 8 j u V E m l 7 F F u F D p m 0 t n o B j f Q = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q L u i y 4 c V n B P q A p Z T K d t E M n k z B z I 5 b Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 H r k 2 I l Y P O E 1 4 P 6 I j J U L B K F r J 9 y O K 4 y D M n m a D q 0 G l 6 t b c O c g q 8 Q p S h Q L N Q e X L H 8 Y s j b h C J q k x P c 9 N s J 9 R j Y J J P i v 7 q e E J Z R M 6 4 j 1 L F Y 2 4 6 W f z z D N y b p U h C W N t n 0 I y V 3 9 v Z D Q y Z h o F d j L P a J a 9 X P z P 6 6 U Y 3 v Q z o Z I U u W K L Q 2 E q C c Y k L 4 A M h e Y M 5 d Q S y r S w W Q k b U 0 0 Z 2 p r K t g R v + c u r p H 1 Z 8 9 y a d 1 + v N u p F H S U 4 h T O 4 A A + u o Q F 3 0 I Q W M E j g G V 7 h z U m d F + f d + V i M r j n F z g n 8 g f P 5 A y k y k b Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 M Z D 8 j u V E m l 7 F F u F D p m 0 t n o B j f Q = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q L u i y 4 c V n B P q A p Z T K d t E M n k z B z I 5 b Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 H r k 2 I l Y P O E 1 4 P 6 I j J U L B K F r J 9 y O K 4 y D M n m a D q 0 G l 6 t b c O c g q 8 Q p S h Q L N Q e X L H 8 Y s j b h C J q k x P c 9 N s J 9 R j Y J J P i v 7 q e E J Z R M 6 4 j 1 L F Y 2 4 6 W f z z D N y b p U h C W N t n 0 I y V 3 9 v Z D Q y Z h o F d j L P a J a 9 X P z P 6 6 U Y 3 v Q z o Z I U u W K L Q 2 E q C c Y k L 4 A M h e Y M 5 d Q S y r S w W Q k b U 0 0 Z 2 p r K t g R v + c u r p H 1 Z 8 9 y a d 1 + v N u p F H S U 4 h T O 4 A A + u o Q F 3 0 I Q W M E j g G V 7 h z U m d F + f d + V i M r j n F z g n 8 g f P 5 A y k y k b Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 M Z D 8 j u V E m l 7 F F u F D p m 0 t n o B j f Q = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q L u i y 4 c V n B P q A p Z T K d t E M n k z B z I 5 b Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 H r k 2 I l Y P O E 1 4 P 6 I j J U L B K F r J 9 y O K 4 y D M n m a D q 0 G l 6 t b c O c g q 8 Q p S h Q L N Q e X L H 8 Y s j b h C J q k x P c 9 N s J 9 R j Y J J P i v 7 q e E J Z R M 6 4 j 1 L F Y 2 4 6 W f z z D N y b p U h C W N t n 0 I y V 3 9 v Z D Q y Z h o F d j L P a J a 9 X P z P 6 6 U Y 3 v Q z o Z I U u W K L Q 2 E q C c Y k L 4 A M h e Y M 5 d Q S y r S w W Q k b U 0 0 Z 2 p r K t g R v + c u r p H 1 Z 8 9 y a d 1 + v N u p F H S U 4 h T O 4 A A + u o Q F 3 0 I Q W M E j g G V 7 h z U m d F + f d + V i M r j n F z g n 8 g f P 5 A y k y k b Q = &lt; / l a t e x i t &gt; x 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l L b j s G 9 M A Z d q Q r 7 I v d q C I a i l b g 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R S 0 G X B j c s K 9 g F t K J P p p B 0 6 m Y S Z G 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e T a i F g 9 4 C z h f k T H S o S C U b T S Y B B R n A R h 9 j Q f N o b V m l t 3 F y D r x C t I D Q q 0 h t W v w S h m a c Q V M k m N 6 X t u g n 5 G N Q o m + b w y S A 1 P K J v S M e 9 b q m j E j Z 8 t M s / J h V V G J I y 1 f Q r J Q v 2 9 k d H I m F k U 2 M k 8 o 1 n 1 c v E / r 5 9 i e O N n Q i U p c s W W h 8 J U E o x J X g A Z C c 0 Z y p k l l G l h s x I 2 o Z o y t D V V b A n e 6 p f X S e e q 7 r l 1 7 7 5 R a z a K O s p w B u d w C R 5 c Q x P u o A V t Y J D A M 7 z C m 5 M 6 L 8 6 7 8 7 E c L T n F z i n 8 g f P 5 A y q 2 k b U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l L b j s G 9 M A Z d q Q r 7 I v d q C I a i l b g 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R S 0 G X B j c s K 9 g F t K J P p p B 0 6 m Y S Z G 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e T a i F g 9 4 C z h f k T H S o S C U b T S Y B B R n A R h 9 j Q f N o b V m l t 3 F y D r x C t I D Q q 0 h t W v w S h m a c Q V M k m N 6 X t u g n 5 G N Q o m + b w y S A 1 P K J v S M e 9 b q m j E j Z 8 t M s / J h V V G J I y 1 f Q r J Q v 2 9 k d H I m F k U 2 M k 8 o 1 n 1 c v E / r 5 9 i e O N n Q i U p c s W W h 8 J U E o x J X g A Z C c 0 Z y p k l l G l h s x I 2 o Z o y t D V V b A n e 6 p f X S e e q 7 r l 1 7 7 5 R a z a K O s p w B u d w C R 5 c Q x P u o A V t Y J D A M 7 z C m 5 M 6 L 8 6 7 8 7 E c L T n F z i n 8 g f P 5 A y q 2 k b U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l L b j s G 9 M A Z d q Q r 7 I v d q C I a i l b g 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R S 0 G X B j c s K 9 g F t K J P p p B 0 6 m Y S Z G 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e T a i F g 9 4 C z h f k T H S o S C U b T S Y B B R n A R h 9 j Q f N o b V m l t 3 F y D r x C t I D Q q 0 h t W v w S h m a c Q V M k m N 6 X t u g n 5 G N Q o m + b w y S A 1 P K J v S M e 9 b q m j E j Z 8 t M s / J h V V G J I y 1 f Q r J Q v 2 9 k d H I m F k U 2 M k 8 o 1 n 1 c v E / r 5 9 i e O N n Q i U p c s W W h 8 J U E o x J X g A Z C c 0 Z y p k l l G l h s x I 2 o Z o y t D V V b A n e 6 p f X S e e q 7 r l 1 7 7 5 R a z a K O s p w B u d w C R 5 c Q x P u o A V t Y J D A M 7 z C m 5 M 6 L 8 6 7 8 7 E c L T n F z i n 8 g f P 5 A y q 2 k b U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l L b j s G 9 M A Z d q Q r 7 I v d q C I a i l b g 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R S 0 G X B j c s K 9 g F t K J P p p B 0 6 m Y S Z G 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e T a i F g 9 4 C z h f k T H S o S C U b T S Y B B R n A R h 9 j Q f N o b V m l t 3 F y D r x C t I D Q q 0 h t W v w S h m a c Q V M k m N 6 X t u g n 5 G N Q o m + b w y S A 1 P K J v S M e 9 b q m j E j Z 8 t M s / J h V V G J I y 1 f Q r J Q v 2 9 k d H I m F k U 2 M k 8 o 1 n 1 c v E / r 5 9 i e O N n Q i U p c s W W h 8 J U E o x J X g A Z C c 0 Z y p k l l G l h s x I 2 o Z o y t D V V b A n e 6 p f X S e e q 7 r l 1 7 7 5 R a z a K O s p w B u d w C R 5 c Q x P u o A V t Y J D A M 7 z C m 5 M 6 L 8 6 7 8 7 E c L T n F z i n 8 g f P 5 A y q 2 k b U = &lt; / l a t e x i t &gt; x 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y F R N r j c M N I 5 Q T j D D n 6 0 P E f 0 9 F t Y = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i S i 6 L L g x m U F + 4 C m l M n 0 p h 0 6 m Y S Z i V h C f 8 O N C 0 X c + j P u / B s n b R b a e m D g c M 6 9 3 D M n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d R 1 S a x / L B T B P s R 3 Q k e c g Z N V b y / Y i a c R B m T 7 P B 1 a B a c + v u H G S V e A W p Q Y H m o P r l D 2 O W R i g N E 1 T r n u c m p p 9 R Z T g T O K v 4 q c a E s g k d Y c 9 S S S P U / W y e e U b O r D I k Y a z s k 4 b M 1 d 8 b G Y 2 0 n k a B n c w z 6 m U v F / / z e q k J b / o Z l 0 l q U L L F o T A V x M Q k L 4 A M u U J m x N Q S y h S 3 W Q k b U 0 W Z s T V V b A n e 8 p d X S f u i 7 r l 1 7 / 6 y 1 r g s 6 i j D C Z z C O X h w D Q 2 4 g y a 0 g E E C z / A K b 0 7 q v D j v z s d i t O Q U O 8 f w B 8 7 n D y w 6 k b Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y F R N r j c M N I 5 Q T j D D n 6 0 P E f 0 9 F t Y = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i S i 6 L L g x m U F + 4 C m l M n 0 p h 0 6 m Y S Z i V h C f 8 O N C 0 X c + j P u / B s n b R b a e m D g c M 6 9 3 D M n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d R 1 S a x / L B T B P s R 3 Q k e c g Z N V b y / Y i a c R B m T 7 P B 1 a B a c + v u H G S V e A W p Q Y H m o P r l D 2 O W R i g N E 1 T r n u c m p p 9 R Z T g T O K v 4 q c a E s g k d Y c 9 S S S P U / W y e e U b O r D I k Y a z s k 4 b M 1 d 8 b G Y 2 0 n k a B n c w z 6 m U v F / / z e q k J b / o Z l 0 l q U L L F o T A V x M Q k L 4 A M u U J m x N Q S y h S 3 W Q k b U 0 W Z s T V V b A n e 8 p d X S f u i 7 r l 1 7 / 6 y 1 r g s 6 i j D C Z z C O X h w D Q 2 4 g y a 0 g E E C z / A K b 0 7 q v D j v z s d i t O Q U O 8 f w B 8 7 n D y w 6 k b Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y F R N r j c M N I 5 Q T j D D n 6 0 P E f 0 9 F t Y = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i S i 6 L L g x m U F + 4 C m l M n 0 p h 0 6 m Y S Z i V h C f 8 O N C 0 X c + j P u / B s n b R b a e m D g c M 6 9 3 D M n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d R 1 S a x / L B T B P s R 3 Q k e c g Z N V b y / Y i a c R B m T 7 P B 1 a B a c + v u H G S V e A W p Q Y H m o P r l D 2 O W R i g N E 1 T r n u c m p p 9 R Z T g T O K v 4 q c a E s g k d Y c 9 S S S P U / W y e e U b O r D I k Y a z s k 4 b M 1 d 8 b G Y 2 0 n k a B n c w z 6 m U v F / / z e q k J b / o Z l 0 l q U L L F o T A V x M Q k L 4 A M u U J m x N Q S y h S 3 W Q k b U 0 W Z s T V V b A n e 8 p d X S f u i 7 r l 1 7 / 6 y 1 r g s 6 i j D C Z z C O X h w D Q 2 4 g y a 0 g E E C z / A K b 0 7 q v D j v z s d i t O Q U O 8 f w B 8 7 n D y w 6 k b Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y F R N r j c M N I 5 Q T j D D n 6 0 P E f 0 9 F t Y = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i S i 6 L L g x m U F + 4 C m l M n 0 p h 0 6 m Y S Z i V h C f 8 O N C 0 X c + j P u / B s n b R b a e m D g c M 6 9 3 D M n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d R 1 S a x / L B T B P s R 3 Q k e c g Z N V b y / Y i a c R B m T 7 P B 1 a B a c + v u H G S V e A W p Q Y H m o P r l D 2 O W R i g N E 1 T r n u c m p p 9 R Z T g T O K v 4 q c a E s g k d Y c 9 S S S P U / W y e e U b O r D I k Y a z s k 4 b M 1 d 8 b G Y 2 0 n k a B n c w z 6 m U v F / / z e q k J b / o Z l 0 l q U L L F o T A V x M Q k L 4 A M u U J m x N Q S y h S 3 W Q k b U 0 W Z s T V V b A n e 8 p d X S f u i 7 r l 1 7 / 6 y 1 r g s 6 i j D C Z z C O X h w D Q 2 4 g y a 0 g E E C z / A K b 0 7 q v D j v z s d i t O Q U O 8 f w B 8 7 n D y w 6 k b Y = &lt; / l a t e x i t &gt;</formula><p>x 6</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z j P F 9 0</p><formula xml:id="formula_1">A F V Z b d Q h y M Y 2 m 3 z h S 3 k e 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 r L g x m U F + 4 C m l M n 0 p h 0 6 m Y S Z i V h C f 8 O N C 0 X c + j P u / B s n b R b a e m D g c M 6 9 3 D M n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d R 1 S a x / L B T B P s R 3 Q k e c g Z N V b y / Y i a c R B m T 7 P B 1 a B a c + v u H G S V e A W p Q Y H m o P r l D 2 O W R i g N E 1 T r n u c m p p 9 R Z T g T O K v 4 q c a E s g k d Y c 9 S S S P U / W y e e U b O r D I k Y a z s k 4 b M 1 d 8 b G Y 2 0 n k a B n c w z 6 m U v F / / z e q k J b / o Z l 0 l q U L L F o T A V x M Q k L 4 A M u U J m x N Q S y h S 3 W Q k b U 0 W Z s T V V b A n e 8 p d X S f u i 7 r l 1 7 / 6 y 1 r g s 6 i j D C Z z C O X h w D Q 2 4 g y a 0 g E E C z / A K b 0 7 q v D j v z s d i t O Q U O 8 f w B 8 7 n D y 2 + k b c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z j P F 9 0 A F V Z b d Q h y M Y 2 m 3 z h S 3 k e 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 r L g x m U F + 4 C m l M n 0 p h 0 6 m Y S Z i V h C f 8 O N C 0 X c + j P u / B s n b R b a e m D g c M 6 9 3 D M n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d R 1 S a x / L B T B P s R 3 Q k e c g Z N V b y / Y i a c R B m T 7 P B 1 a B a c + v u H G S V e A W p Q Y H m o P r l D 2 O W R i g N E 1 T r n u c m p p 9 R Z T g T O K v 4 q c a E s g k d Y c 9 S S S P U / W y e e U b O r D I k Y a z s k 4 b M 1 d 8 b G Y 2 0 n k a B n c w z 6 m U v F / / z e q k J b / o Z l 0 l q U L L F o T A V x M Q k L 4 A M u U J m x N Q S y h S 3 W Q k b U 0 W Z s T V V b A n e 8 p d X S f u i 7 r l 1 7 / 6 y 1 r g s 6 i j D C Z z C O X h w D Q 2 4 g y a 0 g E E C z / A K b 0 7 q v D j v z s d i t O Q U O 8 f w B 8 7 n D y 2 + k b c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z j P F 9 0 A F V Z b d Q h y M Y 2 m 3 z h S 3 k e 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 r L g x m U F + 4 C m l M n 0 p h 0 6 m Y S Z i V h C f 8 O N C 0 X c + j P u / B s n b R b a e m D g c M 6 9 3 D M n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d R 1 S a x / L B T B P s R 3 Q k e c g Z N V b y / Y i a c R B m T 7 P B 1 a B a c + v u H G S V e A W p Q Y H m o P r l D 2 O W R i g N E 1 T r n u c m p p 9 R Z T g T O K v 4 q c a E s g k d Y c 9 S S S P U / W y e e U b O r D I k Y a z s k 4 b M 1 d 8 b G Y 2 0 n k a B n c w z 6 m U v F / / z e q k J b / o Z l 0 l q U L L F o T A V x M Q k L 4 A M u U J m x N Q S y h S 3 W Q k b U 0 W Z s T V V b A n e 8 p d X S f u i 7 r l 1 7 / 6 y 1 r g s 6 i j D C Z z C O X h w D Q 2 4 g y a 0 g E E C z / A K b 0 7 q v D j v z s d i t O Q U O 8 f w B 8 7 n D y 2 + k b c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z j P F 9 0 A F V Z b d Q h y M Y 2 m 3 z h S 3 k e 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 r L g x m U F + 4 C m l M n 0 p h 0 6 m Y S Z i V h C f 8 O N C 0 X c + j P u / B s n b R b a e m D g c M 6 9 3 D M n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d R 1 S a x / L B T B P s R 3 Q k e c g Z N V b y / Y i a c R B m T 7 P B 1 a B a c + v u H G S V e A W p Q Y H m o P r l D 2 O W R i g N E 1 T r n u c m p p 9 R Z T g T O K v 4 q c a E s g k d Y c 9 S S S P U / W y e e U b O r D I k Y a z s k 4 b M 1 d 8 b G Y 2 0 n k a B n c w z 6 m U v F / / z e q k J b / o Z l 0 l q U L L F o T A V x M Q k L 4 A M u U J m x N Q S y h S 3 W Q k b U 0 W Z s T V V b A n e 8 p d X S f u i 7 r l 1 7 / 6 y 1 r g s 6 i j D C Z z C O X h w D Q 2 4 g y a 0 g E E C z / A K b 0 7 q v D j v z s d i t O Q U O 8 f w B 8 7 n D y 2 + k b c = &lt; / l a t e x i t &gt;</formula><p>x 8</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y s K c S C + e 8 M b M h / J 6 6 6 O 9 c 8 k w o G   </p><formula xml:id="formula_2">I = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 2 G X B j c s K 9 g F t K J P p T T t 0 M g k z E 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q G L Z Z L G L V C 6 h G w S W 2 D T c C e 4 l C G g U C u 8 H 0 N v e 7 j 6 g 0 j + W D m S X o R 3 Q s e c g Z N V Y a D C J q J k G Y P c 2 H j W G 1 5 t b d B c g 6 8 Q p S g w K t Y f V r M I p Z G q E 0 T F C t + 5 6 b G D + j y n A m c F 4 Z p B o T y q Z 0 j H 1 L J Y 1 Q + 9 k i 8 5 x c W G V E w l j Z J w 1 Z q L 8 3 M h p p P Y s C O 5 l n 1 K t e L v 7 n 9 V M T N v y M y y Q 1 K N n y U J g K Y m K S F 0 B G X C E z Y m Y J Z Y r b r I R N q K L M 2 J o q t g R v 9 c v r p H N V 9 9 y 6 d 3 9 d a 1 4 X d Z T h D M 7 h E j y 4 g S b c Q Q v a w C C B Z 3 i F N y d 1 X p x 3 5 2 M 5 W n K K n V P 4 A + f z B z D G k b k = &lt; / l a t</formula><formula xml:id="formula_3">G I = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 2 G X B j c s K 9 g F t K J P p T T t 0 M g k z E 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q G L Z Z L G L V C 6 h G w S W 2 D T c C e 4 l C G g U C u 8 H 0 N v e 7 j 6 g 0 j + W D m S X o R 3 Q s e c g Z N V Y a D C J q J k G Y P c 2 H j W G 1 5 t b d B c g 6 8 Q p S g w K t Y f V r M I p Z G q E 0 T F C t + 5 6 b G D + j y n A m c F 4 Z p B o T y q Z 0 j H 1 L J Y 1 Q + 9 k i 8 5 x c W G V E w l j Z J w 1 Z q L 8 3 M h p p P Y s C O 5 l n 1 K t e L v 7 n 9 V M T N v y M y y Q 1 K N n y U J g K Y m K S F 0 B G X C E z Y m Y J Z Y r b r I R N q K L M 2 J o q t g R v 9 c v r p H N V 9 9 y 6 d 3 9 d a 1 4 X d Z T h D M 7 h E j y 4 g S b c Q Q v a w C C B Z 3 i F N y d 1 X p x 3 5 2 M 5 W n K K n V P 4 A + f z B z D G k b k = &lt; / l a t</formula><formula xml:id="formula_4">G I = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 2 G X B j c s K 9 g F t K J P p T T t 0 M g k z E 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q G L Z Z L G L V C 6 h G w S W 2 D T c C e 4 l C G g U C u 8 H 0 N v e 7 j 6 g 0 j + W D m S X o R 3 Q s e c g Z N V Y a D C J q J k G Y P c 2 H j W G 1 5 t b d B c g 6 8 Q p S g w K t Y f V r M I p Z G q E 0 T F C t + 5 6 b G D + j y n A m c F 4 Z p B o T y q Z 0 j H 1 L J Y 1 Q + 9 k i 8 5 x c W G V E w l j Z J w 1 Z q L 8 3 M h p p P Y s C O 5 l n 1 K t e L v 7 n 9 V M T N v y M y y Q 1 K N n y U J g K Y m K S F 0 B G X C E z Y m Y J Z Y r b r I R N q K L M 2 J o q t g R v 9 c v r p H N V 9 9 y 6 d 3 9 d a 1 4 X d Z T h D M 7 h E j y 4 g S b c Q Q v a w C C B Z 3 i F N y d 1 X p x 3 5 2 M 5 W n K K n V P 4 A + f z B z D G k b k = &lt; / l a t</formula><formula xml:id="formula_5">G I = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 2 G X B j c s K 9 g F t K J P p T T t 0 M g k z E 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q G L Z Z L G L V C 6 h G w S W 2 D T c C e 4 l C G g U C u 8 H 0 N v e 7 j 6 g 0 j + W D m S X o R 3 Q s e c g Z N V Y a D C J q J k G Y P c 2 H j W G 1 5 t b d B c g 6 8 Q p S g w K t Y f V r M I p Z G q E 0 T F C t + 5 6 b G D + j y n A m c F 4 Z p B o T y q Z 0 j H 1 L J Y 1 Q + 9 k i 8 5 x c W G V E w l j Z J w 1 Z q L 8 3 M h p p P Y s C O 5 l n 1 K t e L v 7 n 9 V M T N v y M y y Q 1 K N n y U J g K Y m K S F 0 B G X C E z Y m Y J Z Y r b r I R N q K L M 2 J o q t g R v 9 c v r p H N V 9 9 y 6 d 3 9 d a 1 4 X d Z T h D M 7 h E j y 4 g S b c Q Q v a w C C B Z 3 i F N y d 1 X p x 3 5 2 M 5 W n K K n V P 4 A + f z B z D G k b k = &lt; / l a t e x i t &gt; x 9 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c v + N e Q L Y b 9 E 4 v 8 d 6 a 3 g c V A O T m b c = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i q L u C G 5 c V 7 A O a U i b T m 3 b o Z B J m J m I J / Q 0 3 L h R x 6 8 + 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I B F c G 9 f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f C o r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J b e 5 3 H l F p H s s H M 0 2 w H 9 G R 5 C F n 1 F j J 9 y N q x k G Y P c 0 G N 4 N q z a 2 7 c 5 B V 4 h W k B g W a g + q X P 4 x Z G q E 0 T F C t e 5 6 b m H 5 G l e F M 4 K z i p x o T y i Z 0 h D 1 L J Y 1 Q 9 7 N 5 5 h k 5 s 8 q Q h L G y T x o y V 3 9 v Z D T S e h o F d j L P q J e 9 X P z P 6 6 U m v O 5 n X C a p Q c k W h 8 J U E B O T v A A y 5 A q Z E V N L K F P c Z i V s T B V l x t Z U s S V 4 y 1 9 e J e 2 L u u f W v f v L W u O y q K M M J 3 A K 5 + D B F T T g D p r Q A g Y J P M M r v D m p 8 + K 8 O x + L 0 Z J T 7 B z D H z i f P z J K k b o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c v + N e Q L Y b 9 E 4 v 8 d 6 a 3 g c V A O T m b c = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i q L u C G 5 c V 7 A O a U i b T m 3 b o Z B J m J m I J / Q 0 3 L h R x 6 8 + 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I B F c G 9 f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f C o r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J b e 5 3 H l F p H s s H M 0 2 w H 9 G R 5 C F n 1 F j J 9 y N q x k G Y P c 0 G N 4 N q z a 2 7 c 5 B V 4 h W k B g W a g + q X P 4 x Z G q E 0 T F C t e 5 6 b m H 5 G l e F M 4 K z i p x o T y i Z 0 h D 1 L J Y 1 Q 9 7 N 5 5 h k 5 s 8 q Q h L G y T x o y V 3 9 v Z D T S e h o F d j L P q J e 9 X P z P 6 6 U m v O 5 n X C a p Q c k W h 8 J U E B O T v A A y 5 A q Z E V N L K F P c Z i V s T B V l x t Z U s S V 4 y 1 9 e J e 2 L u u f W v f v L W u O y q K M M J 3 A K 5 + D B F T T g D p r Q A g Y J P M M r v D m p 8 + K 8 O x + L 0 Z J T 7 B z D H z i f P z J K k b o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c v + N e Q L Y b 9 E 4 v 8 d 6 a 3 g c V A O T m b c = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i q L u C G 5 c V 7 A O a U i b T m 3 b o Z B J m J m I J / Q 0 3 L h R x 6 8 + 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I B F c G 9 f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f C o r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J b e 5 3 H l F p H s s H M 0 2 w H 9 G R 5 C F n 1 F j J 9 y N q x k G Y P c 0 G N 4 N q z a 2 7 c 5 B V 4 h W k B g W a g + q X P 4 x Z G q E 0 T F C t e 5 6 b m H 5 G l e F M 4 K z i p x o T y i Z 0 h D 1 L J Y 1 Q 9 7 N 5 5 h k 5 s 8 q Q h L G y T x o y V 3 9 v Z D T S e h o F d j L P q J e 9 X P z P 6 6 U m v O 5 n X C a p Q c k W h 8 J U E B O T v A A y 5 A q Z E V N L K F P c Z i V s T B V l x t Z U s S V 4 y 1 9 e J e 2 L u u f W v f v L W u O y q K M M J 3 A K 5 + D B F T T g D p r Q A g Y J P M M r v D m p 8 + K 8 O x + L 0 Z J T 7 B z D H z i f P z J K k b o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c v + N e Q L Y b 9 E 4 v 8 d 6 a 3 g c V A O T m b c = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i q L u C G 5 c V 7 A O a U i b T m 3 b o Z B J m J m I J / Q 0 3 L h R x 6 8 + 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I B F c G 9 f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f C o r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J b e 5 3 H l F p H s s H M 0 2 w H 9 G R 5 C F n 1 F j J 9 y N q x k G Y P c 0 G N 4 N q z a 2 7 c 5 B V 4 h W k B g W a g + q X P 4 x Z G q E 0 T F C t e 5 6 b m H 5 G l e F M 4 K z i p x o T y i Z 0 h D 1 L J Y 1 Q 9 7 N 5 5 h k 5 s 8 q Q h L G y T x o y V 3 9 v Z D T S e h o F d j L P q J e 9 X P z P 6 6 U m v O 5 n X C a p Q c k W h 8 J U E B O T v A A y 5 A q Z E V N L K F P c Z i V s T B V l x t Z U s S V 4 y 1 9 e J e 2 L u u f W v f v L W u O y q K M M J 3 A K 5 + D B F T T g D p r Q A g Y J P M M r v D m p 8 + K 8 O x + L 0 Z J T 7 B z D H z i f P z J K k b o = &lt; / l a t e x i t &gt; x 10 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 m b t u r c i G w x 6 I V P r w m u h 4 2 k Z f 5 c = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R g i 4 L b l x W s A 9 o Q 5 h M J + 3 Q y S T M T M Q a 8 i V u X C j i 1 k 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 f X T c U 3 E q C e 2 S m M d y E G B F O R O 0 q 5 n m d J B I i q O A 0 3 4 w u y n 8 / g O V i s X i X s 8 T 6 k V 4 I l j I C N Z G 8 u 3 6 K M J 6 G o T Z Y + 5 n r p P 7 d s N p O g u g d e K W p A E l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e G 6 W K J p j M 8 I Q O D R U 4 o s r L F s F z d G 6 U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V c 2 U 4 K 5 + e Z 3 0 L p u u 0 3 T v W o 1 2 q 6 y j C q d w B h f g w h W 0 4 R Y 6 0 A U C K T z D K 7 x Z T 9 a L 9 W 5 9 L E c r V r l z A n 9 g f f 4 A 3 W u T K Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 m b t u r c i G w x 6 I V P r w m u h 4 2 k Z f 5 c = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R g i 4 L b l x W s A 9 o Q 5 h M J + 3 Q y S T M T M Q a 8 i V u X C j i 1 k 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 f X T c U 3 E q C e 2 S m M d y E G B F O R O 0 q 5 n m d J B I i q O A 0 3 4 w u y n 8 / g O V i s X i X s 8 T 6 k V 4 I l j I C N Z G 8 u 3 6 K M J 6 G o T Z Y + 5 n r p P 7 d s N p O g u g d e K W p A E l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e G 6 W K J p j M 8 I Q O D R U 4 o s r L F s F z d G 6 U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V c 2 U 4 K 5 + e Z 3 0 L p u u 0 3 T v W o 1 2 q 6 y j C q d w B h f g w h W 0 4 R Y 6 0 A U C K T z D K 7 x Z T 9 a L 9 W 5 9 L E c r V r l z A n 9 g f f 4 A 3 W u T K Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 m b t u r c i G w x 6 I V P r w m u h 4 2 k Z f 5 c = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R g i 4 L b l x W s A 9 o Q 5 h M J + 3 Q y S T M T M Q a 8 i V u X C j i 1 k 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 f X T c U 3 E q C e 2 S m M d y E G B F O R O 0 q 5 n m d J B I i q O A 0 3 4 w u y n 8 / g O V i s X i X s 8 T 6 k V 4 I l j I C N Z G 8 u 3 6 K M J 6 G o T Z Y + 5 n r p P 7 d s N p O g u g d e K W p A E l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e G 6 W K J p j M 8 I Q O D R U 4 o s r L F s F z d G 6 U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V c 2 U 4 K 5 + e Z 3 0 L p u u 0 3 T v W o 1 2 q 6 y j C q d w B h f g w h W 0 4 R Y 6 0 A U C K T z D K 7 x Z T 9 a L 9 W 5 9 L E c r V r l z A n 9 g f f 4 A 3 W u T K Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 m b t u r c i G w x 6 I V P r w m u h 4 2 k Z f 5 c = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R g i 4 L b l x W s A 9 o Q 5 h M J + 3 Q y S T M T M Q a 8 i V u X C j i 1 k 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 f X T c U 3 E q C e 2 S m M d y E G B F O R O 0 q 5 n m d J B I i q O A 0 3 4 w u y n 8 / g O V i s X i X s 8 T 6 k V 4 I l j I C N Z G 8 u 3 6 K M J 6 G o T Z Y + 5 n r p P 7 d s N p O g u g d e K W p A E l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e G 6 W K J p j M 8 I Q O D R U 4 o s r L F s F z d G 6 U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V c 2 U 4 K 5 + e Z 3 0 L p u u 0 3 T v W o 1 2 q 6 y j C q d w B h f g w h W 0 4 R Y 6 0 A U C K T z D K 7 x Z T 9 a L 9 W 5 9 L E c r V r l z A n 9 g f f 4 A 3 W u T K Q = = &lt; / l a t e x i t &gt; x 11 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T Z z n U P D W L P 2 t j L y 1 O I 3 E a f U w d Q A = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R g i 4 L b l x W s A 9 o Q 5 h M J + 3 Q y S T M T M Q a 8 i V u X C j i 1 k 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 f X T c U 3 E q C e 2 S m M d y E G B F O R O 0 q 5 n m d J B I i q O A 0 3 4 w u y n 8 / g O V i s X i X s 8 T 6 k V 4 I l j I C N Z G 8 u 3 6 K M J 6 G o T Z Y + 5 n r p v 7 d s N p O g u g d e K W p A E l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e G 6 W K J p j M 8 I Q O D R U 4 o s r L F s F z d G 6 U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V c 2 U 4 K 5 + e Z 3 0 L p u u 0 3 T v W o 1 2 q 6 y j C q d w B h f g w h W 0 4 R Y 6 0 A U C K T z D K 7 x Z T 9 a L 9 W 5 9 L E c r V r l z A n 9 g f f 4 A 3 v C T K g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T Z z n U P D W L P 2 t j L y 1 O I 3 E a f U w d Q A = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R g i 4 L b l x W s A 9 o Q 5 h M J + 3 Q y S T M T M Q a 8 i V u X C j i 1 k 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 f X T c U 3 E q C e 2 S m M d y E G B F O R O 0 q 5 n m d J B I i q O A 0 3 4 w u y n 8 / g O V i s X i X s 8 T 6 k V 4 I l j I C N Z G 8 u 3 6 K M J 6 G o T Z Y + 5 n r p v 7 d s N p O g u g d e K W p A E l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e G 6 W K J p j M 8 I Q O D R U 4 o s r L F s F z d G 6 U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V c 2 U 4 K 5 + e Z 3 0 L p u u 0 3 T v W o 1 2 q 6 y j C q d w B h f g w h W 0 4 R Y 6 0 A U C K T z D K 7 x Z T 9 a L 9 W 5 9 L E c r V r l z A n 9 g f f 4 A 3 v C T K g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T Z z n U P D W L P 2 t j L y 1 O I 3 E a f U w d Q A = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R g i 4 L b l x W s A 9 o Q 5 h M J + 3 Q y S T M T M Q a 8 i V u X C j i 1 k 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 f X T c U 3 E q C e 2 S m M d y E G B F O R O 0 q 5 n m d J B I i q O A 0 3 4 w u y n 8 / g O V i s X i X s 8 T 6 k V 4 I l j I C N Z G 8 u 3 6 K M J 6 G o T Z Y + 5 n r p v 7 d s N p O g u g d e K W p A E l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e G 6 W K J p j M 8 I Q O D R U 4 o s r L F s F z d G 6 U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V c 2 U 4 K 5 + e Z 3 0 L p u u 0 3 T v W o 1 2 q 6 y j C q d w B h f g w h W 0 4 R Y 6 0 A U C K T z D K 7 x Z T 9 a L 9 W 5 9 L E c r V r l z A n 9 g f f 4 A 3 v C T K g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T Z z n U P D W L P 2 t j L y 1 O I 3 E a f U w d Q A = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R g i 4 L b l x W s A 9 o Q 5 h M J + 3 Q y S T M T M Q a 8 i V u X C j i 1 k 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 f X T c U 3 E q C e 2 S m M d y E G B F O R O 0 q 5 n m d J B I i q O A 0 3 4 w u y n 8 / g O V i s X i X s 8 T 6 k V 4 I l j I C N Z G 8 u 3 6 K M J 6 G o T Z Y + 5 n r p v 7 d s N p O g u g d e K W p A E l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e G 6 W K J p j M 8 I Q O D R U 4 o s r L F s F z d G 6 U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V c 2 U 4 K 5 + e Z 3 0 L p u u 0 3 T v W o 1 2 q 6 y j C q d w B h f g w h W 0 4 R Y 6 0 A U C K T z D K 7 x Z T 9 a L 9 W 5 9 L E c r V r l z A n 9 g f f 4 A 3 v C T K g = = &lt; / l a t e x i t &gt; x 12 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B / n N x h / 4 9 4 s i M w V Y q Y w h h F / T w G k = " &gt; A A A B + H i c b V D L S s N A F L 3 x W e u j U Z d u B o v g q i S l o M u C G 5 c V 7 A P a E C b T S T t 0 M g k z E 7 G G f I k b F 4 q 4 9 V P c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k H C m t O N 8 W x u b W 9 s 7 u 5 W 9 6 v 7 B 4 V H N P j 7 p q T i V h H Z J z G M 5 C L C i n A n a 1 U x z O k g k x V H A a T + Y 3 R R + / 4 F K x W J x r + c J 9 S I 8 E S x k B G s j + X Z t F G E 9 D c L s M f c z t 5 n 7 d t 1 p O A u g d e K W p A 4 l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e H a W K J p j M 8 I Q O D R U 4 o s r L F s F z d G G U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V d W U 4 K 5 + e Z 3 0 m g 3 X a b h 3 r X q 7 V d Z R g T M 4 h 0 t w 4 Q r a c A s d 6 A K B F J 7 h F d 6 s J + v F e r c + l q M b V r l z C n 9 g f f 4 A 4 H W T K w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B / n N x h / 4 9 4 s i M w V Y q Y w h h F / T w G k = " &gt; A A A B + H i c b V D L S s N A F L 3 x W e u j U Z d u B o v g q i S l o M u C G 5 c V 7 A P a E C b T S T t 0 M g k z E 7 G G f I k b F 4 q 4 9 V P c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k H C m t O N 8 W x u b W 9 s 7 u 5 W 9 6 v 7 B 4 V H N P j 7 p q T i V h H Z J z G M 5 C L C i n A n a 1 U x z O k g k x V H A a T + Y 3 R R + / 4 F K x W J x r + c J 9 S I 8 E S x k B G s j + X Z t F G E 9 D c L s M f c z t 5 n 7 d t 1 p O A u g d e K W p A 4 l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e H a W K J p j M 8 I Q O D R U 4 o s r L F s F z d G G U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V d W U 4 K 5 + e Z 3 0 m g 3 X a b h 3 r X q 7 V d Z R g T M 4 h 0 t w 4 Q r a c A s d 6 A K B F J 7 h F d 6 s J + v F e r c + l q M b V r l z C n 9 g f f 4 A 4 H W T K w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B / n N x h / 4 9 4 s i M w V Y q Y w h h F / T w G k = " &gt; A A A B + H i c b V D L S s N A F L 3 x W e u j U Z d u B o v g q i S l o M u C G 5 c V 7 A P a E C b T S T t 0 M g k z E 7 G G f I k b F 4 q 4 9 V P c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k H C m t O N 8 W x u b W 9 s 7 u 5 W 9 6 v 7 B 4 V H N P j 7 p q T i V h H Z J z G M 5 C L C i n A n a 1 U x z O k g k x V H A a T + Y 3 R R + / 4 F K x W J x r + c J 9 S I 8 E S x k B G s j + X Z t F G E 9 D c L s M f c z t 5 n 7 d t 1 p O A u g d e K W p A 4 l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e H a W K J p j M 8 I Q O D R U 4 o s r L F s F z d G G U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V d W U 4 K 5 + e Z 3 0 m g 3 X a b h 3 r X q 7 V d Z R g T M 4 h 0 t w 4 Q r a c A s d 6 A K B F J 7 h F d 6 s J + v F e r c + l q M b V r l z C n 9 g f f 4 A 4 H W T K w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B / n N x h / 4 9 4 s i M w V Y q Y w h h F / T w G k = " &gt; A A A B + H i c b V D L S s N A F L 3 x W e u j U Z d u B o v g q i S l o M u C G 5 c V 7 A P a E C b T S T t 0 M g k z E 7 G G f I k b F 4 q 4 9 V P c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k H C m t O N 8 W x u b W 9 s 7 u 5 W 9 6 v 7 B 4 V H N P j 7 p q T i V h H Z J z G M 5 C L C i n A n a 1 U x z O k g k x V H A a T + Y 3 R R + / 4 F K x W J x r + c J 9 S I 8 E S x k B G s j + X Z t F G E 9 D c L s M f c z t 5 n 7 d t 1 p O A u g d e K W p A 4 l O r 7 9 N R r H J I 2 o 0 I R j p Y a u k 2 g v w 1 I z w m l e H a W K J p j M 8 I Q O D R U 4 o s r L F s F z d G G U M Q p j a Z 7 Q a K H + 3 s h w p N Q 8 C s x k E V O t e o X 4 n z d M d X j t Z U w k q a a C L A + F K U c 6 R k U L a M w k J Z r P D c F E M p M V k S m W m G j T V d W U 4 K 5 + e Z 3 0 m g 3 X a b h 3 r X q 7 V d Z R g T M 4 h 0 t w 4 Q r a c A s d 6 A K B F J 7 h F d 6 s J + v F e r c + l q M b V r l z C n 9 g f f 4 A 4 H W T K w = = &lt; / l a t e x i t &gt; x 7 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 p i a X O Z Q g z 8 m S B c U f J W c H e d 9 3 T o = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I U J c F N y 4 r 2 A e 0 Y 8 m k m T Y 0 y Q x J R i 3 D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n i D n T x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T z q 6 C h R h L Z J x C P V C 7 C m n E n a N s x w 2 o s V x S L g t B t M r 3 O / + 0 C V Z p G 8 M 7 O Y + g K P J Q s Z w c Z K 9 w O B z S Q I 0 6 d s m D a y Y b X m 1 t 0 5 0 C r x C l K D A q 1 h 9 W s w i k g i q D S E Y 6 3 7 n h s b P 8 X K M M J p V h k k m s a Y T P G Y 9 i 2 V W F D t p / P U G T q z y g i F k b J P G j R X f 2 + k W G g 9 E 4 G d z F P q Z S 8 X / / P 6 i Q m v / J T J O D F U k s W h M O H I R C i v A I 2 Y o s T w m S W Y K G a z I j L B C h N j i 6 r Y E r z l L 6 + S z k X d c + v e 7 W W t e V n U U Y Y T O I V z 8 K A B T b i B F r S B g I J n e I U 3 5 9 F 5 c d 6 d j 8 V o y S l 2 j u E P n M 8 f + 4 6 S x A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 p i a X O Z Q g z 8 m S B c U f J W c H e d 9 3 T o = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I U J c F N y 4 r 2 A e 0 Y 8 m k m T Y 0 y Q x J R i 3 D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n i D n T x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T z q 6 C h R h L Z J x C P V C 7 C m n E n a N s x w 2 o s V x S L g t B t M r 3 O / + 0 C V Z p G 8 M 7 O Y + g K P J Q s Z w c Z K 9 w O B z S Q I 0 6 d s m D a y Y b X m 1 t 0 5 0 C r x C l K D A q 1 h 9 W s w i k g i q D S E Y 6 3 7 n h s b P 8 X K M M J p V h k k m s a Y T P G Y 9 i 2 V W F D t p / P U G T q z y g i F k b J P G j R X f 2 + k W G g 9 E 4 G d z F P q Z S 8 X / / P 6 i Q m v / J T J O D F U k s W h M O H I R C i v A I 2 Y o s T w m S W Y K G a z I j L B C h N j i 6 r Y E r z l L 6 + S z k X d c + v e 7 W W t e V n U U Y Y T O I V z 8 K A B T b i B F r S B g I J n e I U 3 5 9 F 5 c d 6 d j 8 V o y S l 2 j u E P n M 8 f + 4 6 S x A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 p i a X O Z Q g z 8 m S B c U f J W c H e d 9 3 T o = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I U J c F N y 4 r 2 A e 0 Y 8 m k m T Y 0 y Q x J R i 3 D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n i D n T x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T z q 6 C h R h L Z J x C P V C 7 C m n E n a N s x w 2 o s V x S L g t B t M r 3 O / + 0 C V Z p G 8 M 7 O Y + g K P J Q s Z w c Z K 9 w O B z S Q I 0 6 d s m D a y Y b X m 1 t 0 5 0 C r x C l K D A q 1 h 9 W s w i k g i q D S E Y 6 3 7 n h s b P 8 X K M M J p V h k k m s a Y T P G Y 9 i 2 V W F D t p / P U G T q z y g i F k b J P G j R X f 2 + k W G g 9 E 4 G d z F P q Z S 8 X / / P 6 i Q m v / J T J O D F U k s W h M O H I R C i v A I 2 Y o s T w m S W Y K G a z I j L B C h N j i 6 r Y E r z l L 6 + S z k X d c + v e 7 W W t e V n U U Y Y T O I V z 8 K A B T b i B F r S B g I J n e I U 3 5 9 F 5 c d 6 d j 8 V o y S l 2 j u E P n M 8 f + 4 6 S x A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 p i a X O Z Q g z 8 m S B c U f J W c H e d 9 3 T o = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I U J c F N y 4 r 2 A e 0 Y 8 m k m T Y 0 y Q x J R i 3 D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n i D n T x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T z q 6 C h R h L Z J x C P V C 7 C m n E n a N s x w 2 o s V x S L g t B t M r 3 O / + 0 C V Z p G 8 M 7 O Y + g K P J Q s Z w c Z K 9 w O B z S Q I 0 6 d s m D a y Y b X m 1 t 0 5 0 C r x C l K D A q 1 h 9 W s w i k g i q D S E Y 6 3 7 n h s b P 8 X K M M J p V h k k m s a Y T P G Y 9 i 2 V W F D t p / P U G T q z y g i F k b J P G j R X f 2 + k W G g 9 E 4 G d z F P q Z S 8 X / / P 6 i Q m v / J T J O D F U k s W h M O H I R C i v A I 2 Y o s T w m S W Y K G a z I j L B C h N j i 6 r Y E r z l L 6 + S z k X d c + v e 7 W W t e V n U U Y Y T O I V z 8 K A B T b i B F r S B g I J n e I U 3 5 9 F 5 c d 6 d j 8 V o y S l 2 j u E P n M 8 f + 4 6 S x A = = &lt; / l a t e x i t &gt; p 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X s 1 a E h D h k n k g x 4 X D n I 9 8 b g 6 n 3 S 8 = " &gt; A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g x m U F + 4 A 2 l s l 0 0 g 6 d T M L M j V J C / 8 O N C 0 X c + i / u / B s n b R b a e m D g c M 6 9 3 D M n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T m 9 z v P H J t R K z u c Z p w P 6 I j J U L B K F r p o R 9 R H A d h l s w G m T c b V G t u 3 Z 2 D r B K v I D U o 0 B x U v / r D m K U R V 8 g k N a b n u Q n 6 G d U o m O S z S j 8 1 P K F s Q k e 8 Z 6 m i E T d + N k 8 9 I 2 d W G Z I w 1 v Y p J H P 1 9 0 Z G I 2 O m U W A n 8 5 R m 2 c v F / 7 x e i u G 1 n w m V p M g V W x w K U 0 k w J n k F Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t A W V b E l e M t</formula><p>f X i X t i 7 r n 1 r 2 7 y 1 r j s q i j D C d w C u f g w R U 0 4 B a a 0 A I G G p 7 h F d 6 c J + f F e X c + F q M l p 9 g 5 h j 9 w P n 8 A 5 i i S t g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X s 1 a</p><formula xml:id="formula_6">E h D h k n k g x 4 X D n I 9 8 b g 6 n 3 S 8 = " &gt; A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g x m U F + 4 A 2 l s l 0 0 g 6 d T M L M j V J C / 8 O N C 0 X c + i / u / B s n b R b a e m D g c M 6 9 3 D M n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T m 9 z v P H J t R K z u c Z p w P 6 I j J U L B K F r p o R 9 R H A d h l s w G m T c b V G t u 3 Z 2 D r B K v I D U o 0 B x U v / r D m K U R V 8 g k N a b n u Q n 6 G d U o m O S z S j 8 1 P K F s Q k e 8 Z 6 m i E T d + N k 8 9 I 2 d W G Z I w 1 v Y p J H P 1 9 0 Z G I 2 O m U W A n 8 5 R m 2 c v F / 7 x e i u G 1 n w m V p M g V W x w K U 0 k w J n k F Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t A W V b E l e M t</formula><p>f X i X t i 7 r n 1 r 2 7 y 1 r j s q i j D C d w C u f g w R U 0 4 B a a 0 A I G G p 7 h F d 6 c J + f F e X c + F q M l p 9 g 5 h j 9 w P n 8 A 5 i i S t g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X s 1 a</p><formula xml:id="formula_7">E h D h k n k g x 4 X D n I 9 8 b g 6 n 3 S 8 = " &gt; A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g x m U F + 4 A 2 l s l 0 0 g 6 d T M L M j V J C / 8 O N C 0 X c + i / u / B s n b R b a e m D g c M 6 9 3 D M n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T m 9 z v P H J t R K z u c Z p w P 6 I j J U L B K F r p o R 9 R H A d h l s w G m T c b V G t u 3 Z 2 D r B K v I D U o 0 B x U v / r D m K U R V 8 g k N a b n u Q n 6 G d U o m O S z S j 8 1 P K F s Q k e 8 Z 6 m i E T d + N k 8 9 I 2 d W G Z I w 1 v Y p J H P 1 9 0 Z G I 2 O m U W A n 8 5 R m 2 c v F / 7 x e i u G 1 n w m V p M g V W x w K U 0 k w J n k F Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t A W V b E l e M t</formula><p>f X i X t i 7 r n 1 r 2 7 y 1 r j s q i j D C d w C u f g w R U 0 4 B a a 0 A I G G p 7 h F d 6 c J + f F e X c + F q M l p 9 g 5 h j 9 w P n 8 A 5 i i S t g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X s 1 a</p><formula xml:id="formula_8">E h D h k n k g x 4 X D n I 9 8 b g 6 n 3 S 8 = " &gt; A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g x m U F + 4 A 2 l s l 0 0 g 6 d T M L M j V J C / 8 O N C 0 X c + i / u / B s n b R b a e m D g c M 6 9 3 D M n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T m 9 z v P H J t R K z u c Z p w P 6 I j J U L B K F r p o R 9 R H A d h l s w G m T c b V G t u 3 Z 2 D r B K v I D U o 0 B x U v / r D m K U R V 8 g k N a b n u Q n 6 G d U o m O S z S j 8 1 P K F s Q k e 8 Z 6 m i E T d + N k 8 9 I 2 d W G Z I w 1 v Y p J H P 1 9 0 Z G I 2 O m U W A n 8 5 R m 2 c v F / 7 x e i u G 1 n w m V p M g V W x w K U 0 k w J n k F Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t A W V b E l e M t</formula><p>f X i X t i 7 r n 1 r 2 7 y 1 r j s q i j D C d w C u f g w R U 0 4 B a a 0 A I G G p 7 h F d 6 c J + f F e X c + F q M l p 9 g 5 h j 9 w P n 8 A 5 i i S t g = = &lt; / l a t e x i t &gt; p 2</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><formula xml:id="formula_9">i p / k Y G L 3 z W y c W l m r i O E u p + q d / O 0 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y U g i 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S h v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I T X f s p k n B g q y f J Q m H B k I p R X g M Z M U W L 4 3 B J M F L N Z E Z l i h Y m x R V V s C d 7 q l 9 d J t 1 H 3 3 L p 3 1 6 y 1 m k U d Z T i D c 7 g E D 6 6 g B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 5 6 2 S t w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i p / k Y G L 3 z W y c W l m r i O E u p + q d / O 0 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y U g i 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S h v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I T X f s p k n B g q y f J Q m H B k I p R X g M Z M U W L 4 3 B J M F L N Z E Z l i h Y m x R V V s C d 7 q l 9 d J t 1 H 3 3 L p 3 1 6 y 1 m k U d Z T i D c 7 g E D 6 6 g B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 5 6 2 S t w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i p / k Y G L 3 z W y c W l m r i O E u p + q d / O 0 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y U g i 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S h v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I T X f s p k n B g q y f J Q m H B k I p R X g M Z M U W L 4 3 B J M F L N Z E Z l i h Y m x R V V s C d 7 q l 9 d J t 1 H 3 3 L p 3 1 6 y 1 m k U d Z T i D c 7 g E D 6 6 g B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 5 6 2 S t w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i p / k Y G L 3 z W y c W l m r i O E u p + q d / O 0 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y U g i 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S h v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I T X f s p k n B g q y f J Q m H B k I p R X g M Z M U W L 4 3 B J M F L N Z E Z l i h Y m x R V V s C d 7 q l 9 d J t 1 H 3 3 L p 3 1 6 y 1 m k U d Z T i D c 7 g E D 6 6 g B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 5 6 2 S t w = = &lt; / l a t e x i t &gt; p 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H r X 7 j u 2 L M Z N T b D B H g l W R 5 6 v / m R 8 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x o Q Z c F N y 4 r 2 A e 0 Y 8 m k m T Y 0 k w l J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n k J x p 4 7 r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w o Q t s k 5 r H q B V h T z g R t G 2 Y 4 7 U l F c R R w 2 g 2 m N 7 n f f a R K s 1 j c m 5 m k f o T H g o W M Y G O l h 0 G E z S Q I U 5 k N 0 8 t s W K 2 5 d X c O t E q 8 g t S g Q G t Y / R q M Y p J E V B j C s d Z 9 z 5 X G T 7 E y j H C a V Q a J p h K T K R 7 T v q U C R 1 T 7 6 T x 1 h s 6 s M k J h r O w T B s 3 V 3 x s p j r S e R Y G d z F P q Z S 8 X / / P 6 i Q m v / Z Q J m R g q y O J Q m H B k Y p R X g E Z M U W L 4 z B J M F L N Z E Z l g h Y m x R V V s C d 7 y l 1 d J 5 6 L u u X X v r l F r N o o 6 y n A C p 3 A O H l x B E 2 6 h B W 0 g o O A Z X u H N e X J e n H f n Y z F a c o q d Y / g D 5 / M H 6 T K S u A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H r X 7 j u 2 L M Z N T b D B H g l W R 5 6 v / m R 8 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x o Q Z c F N y 4 r 2 A e 0 Y 8 m k m T Y 0 k w l J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n k J x p 4 7 r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w o Q t s k 5 r H q B V h T z g R t G 2 Y 4 7 U l F c R R w 2 g 2 m N 7 n f f a R K s 1 j c m 5 m k f o T H g o W M Y G O l h 0 G E z S Q I U 5 k N 0 8 t s W K 2 5 d X c O t E q 8 g t S g Q G t Y / R q M Y p J E V B j C s d Z 9 z 5 X G T 7 E y j H C a V Q a J p h K T K R 7 T v q U C R 1 T 7 6 T x 1 h s 6 s M k J h r O w T B s 3 V 3 x s p j r S e R Y G d z F P q Z S 8 X / / P 6 i Q m v / Z Q J m R g q y O J Q m H B k Y p R X g E Z M U W L 4 z B J M F L N Z E Z l g h Y m x R V V s C d 7 y l 1 d J 5 6 L u u X X v r l F r N o o 6 y n A C p 3 A O H l x B E 2 6 h B W 0 g o O A Z X u H N e X J e n H f n Y z F a c o q d Y / g D 5 / M H 6 T K S u A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H r X 7 j u 2 L M Z N T b D B H g l W R 5 6 v / m R 8 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x o Q Z c F N y 4 r 2 A e 0 Y 8 m k m T Y 0 k w l J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n k J x p 4 7 r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w o Q t s k 5 r H q B V h T z g R t G 2 Y 4 7 U l F c R R w 2 g 2 m N 7 n f f a R K s 1 j c m 5 m k f o T H g o W M Y G O l h 0 G E z S Q I U 5 k N 0 8 t s W K 2 5 d X c O t E q 8 g t S g Q G t Y / R q M Y p J E V B j C s d Z 9 z 5 X G T 7 E y j H C a V Q a J p h K T K R 7 T v q U C R 1 T 7 6 T x 1 h s 6 s M k J h r O w T B s 3 V 3 x s p j r S e R Y G d z F P q Z S 8 X / / P 6 i Q m v / Z Q J m R g q y O J Q m H B k Y p R X g E Z M U W L 4 z B J M F L N Z E Z l g h Y m x R V V s C d 7 y l 1 d J 5 6 L u u X X v r l F r N o o 6 y n A C p 3 A O H l x B E 2 6 h B W 0 g o O A Z X u H N e X J e n H f n Y z F a c o q d Y / g D 5 / M H 6 T K S u A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H r X 7 j u 2 L M Z N T b D B H g l W R 5 6 v / m R 8 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x o Q Z c F N y 4 r 2 A e 0 Y 8 m k m T Y 0 k w l J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n k J x p 4 7 r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w o Q t s k 5 r H q B V h T z g R t G 2 Y 4 7 U l F c R R w 2 g 2 m N 7 n f f a R K s 1 j c m 5 m k f o T H g o W M Y G O l h 0 G E z S Q I U 5 k N 0 8 t s W K 2 5 d X c O t E q 8 g t S g Q G t Y / R q M Y p J E V B j C s d Z 9 z 5 X G T 7 E y j H C a V Q a J p h K T K R 7 T v q U C R 1 T 7 6 T x 1 h s 6 s M k J h r O w T B s 3 V 3 x s p j r S e R Y G d z F P q Z S 8 X / / P 6 i Q m v / Z Q J m R g q y O J Q m H B k Y p R X g E Z M U W L 4 z B J M F L N Z E Z l g h Y m x R V V s C d 7 y l 1 d J 5 6 L u u X X v r l F r N o o 6 y n A C p 3 A O H l x B E 2 6 h B W 0 g o O A Z X u H N e X J e n H f n Y z F a c o q d Y / g D 5 / M H 6 T K S u A = = &lt; / l a t e x i t &gt; p 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N f 1 T S 8 X x w K W R / u G p r E A X T Y u V 8 L 8 = " &gt; A A A B 9 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q K u i y 4 c V n B P q C N Z T K 9 a Y d O J m F m o p T Q / 3 D j Q h G 3 / o s 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 y f 3 O I y r N Y 3 l v p g n 6 E R 1 J H n J G j Z U e + h E 1 4 y D M k t k g q 8 8 G l a p b c + c g q 8 Q r S B U K N A e V r / 4 w Z m m E 0 j B B t e 5 5 b m L 8 j C r D m c B Z u Z 9 q T C i b 0 B H 2 L J U 0 Q u 1 n 8 9 Q z c m 6 V I Q l j Z Z 8 0 Z K 7 + 3 s h o p P U 0 C u x k n l I v e 7 n 4 n 9 d L T X j t Z 1 w m q U H J F o f C V B A T k 7 w C M u Q K m R F T S y h T 3 G Y l b E w V Z c Y W V b Y l e M t f X i X t y 5 r n 1 r y 7 e r V R L + o o w S m c w Q V 4 c A U N u I U m t I C B g m d 4</formula><p>h T f n y X l x 3 p 2 P x e i a U + y c w B 8 4 n z / q t 5 K 5 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N f 1 T S 8 X</p><formula xml:id="formula_10">x w K W R / u G p r E A X T Y u V 8 L 8 = " &gt; A A A B 9 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q K u i y 4 c V n B P q C N Z T K 9 a Y d O J m F m o p T Q / 3 D j Q h G 3 / o s 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 y f 3 O I y r N Y 3 l v p g n 6 E R 1 J H n J G j Z U e + h E 1 4 y D M k t k g q 8 8 G l a p b c + c g q 8 Q r S B U K N A e V r / 4 w Z m m E 0 j B B t e 5 5 b m L 8 j C r D m c B Z u Z 9 q T C i b 0 B H 2 L J U 0 Q u 1 n 8 9 Q z c m 6 V I Q l j Z Z 8 0 Z K 7 + 3 s h o p P U 0 C u x k n l I v e 7 n 4 n 9 d L T X j t Z 1 w m q U H J F o f C V B A T k 7 w C M u Q K m R F T S y h T 3 G Y l b E w V Z c Y W V b Y l e M t f X i X t y 5 r n 1 r y 7 e r V R L + o o w S m c w Q V 4 c A U N u I U m t I C B g m d 4</formula><p>h T f n y X l x 3 p 2 P x e i a U + y c w B 8 4 n z / q t 5 K 5 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N f 1 T S 8 X</p><formula xml:id="formula_11">x w K W R / u G p r E A X T Y u V 8 L 8 = " &gt; A A A B 9 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q K u i y 4 c V n B P q C N Z T K 9 a Y d O J m F m o p T Q / 3 D j Q h G 3 / o s 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 y f 3 O I y r N Y 3 l v p g n 6 E R 1 J H n J G j Z U e + h E 1 4 y D M k t k g q 8 8 G l a p b c + c g q 8 Q r S B U K N A e V r / 4 w Z m m E 0 j B B t e 5 5 b m L 8 j C r D m c B Z u Z 9 q T C i b 0 B H 2 L J U 0 Q u 1 n 8 9 Q z c m 6 V I Q l j Z Z 8 0 Z K 7 + 3 s h o p P U 0 C u x k n l I v e 7 n 4 n 9 d L T X j t Z 1 w m q U H J F o f C V B A T k 7 w C M u Q K m R F T S y h T 3 G Y l b E w V Z c Y W V b</formula><p>Y l e M t f X i X t y 5 r n 1 r y 7 e r V R L + o o w S m c w Q V 4 c A U N u I U m t I C B g m d 4 h T f n y X l x 3 p 2 P x e i a U + y c w B 8 4 n z / q t 5 K 5 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N f 1 T S 8 X</p><formula xml:id="formula_12">x w K W R / u G p r E A X T Y u V 8 L 8 = " &gt; A A A B 9 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q K u i y 4 c V n B P q C N Z T K 9 a Y d O J m F m o p T Q / 3 D j Q h G 3 / o s 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 y f 3 O I y r N Y 3 l v p g n 6 E R 1 J H n J G j Z U e + h E 1 4 y D M k t k g q 8 8 G l a p b c + c g q 8 Q r S B U K N A e V r / 4 w Z m m E 0 j B B t e 5 5 b m L 8 j C r D m c B Z u Z 9 q T C i b 0 B H 2 L J U 0 Q u 1 n 8 9 Q z c m 6 V I Q l j Z Z 8 0 Z K 7 + 3 s h o p P U 0 C u x k n l I v e 7 n 4 n 9 d L T X j t Z 1 w m q U H J F o f C V B A T k 7 w C M u Q K m R F T S y h T 3 G Y l b E w V Z c Y W V b</formula><p>Y l e M t f X i X t y 5 r n 1 r y 7 e r V R L + o o w S m c w Q V 4 c A U N u I U m t I C B g m d 4 h T f n y X l x 3 p 2 P x e i a U + y c w B 8 4 n z / q t 5 K 5 &lt; / l a t e x i t &gt; p 5</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d v a + t p r j v r I a z J</p><formula xml:id="formula_13">1 3 T 2 2 M i S U x g Q Q = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k o s u C G 5 c V 7 A P a s W T S T B u a y Q x J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S f H j w X X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J Q o y t o 0 E p H q + U Q z w S V r G 2 4 E 6 8 W K k d A X r O t P b 3 K / + 8 i U 5 p G 8 N 7 O Y e S E Z S x 5 w S o y V H g Y h M R M / S O N s m F 5 m w 2 r N q T t z 4 F X i F q Q G B V r D 6 t d g F N E k Z N J Q Q b T u u 0 5 s v J Q o w 6 l g W W W Q a B Y T O i V j 1 r d U k p B p L 5 2 n z v C Z V U Y 4 i J R 9 0 u C 5 + n s j J a H W s 9 C 3 k 3 l K v e z l 4 n 9 e P z H B t Z d y G S e G S b o 4 F C Q C m w j n F e A R V 4 w a M b O E U M V t V k w n R B F q b F E V W 4 K 7 / O V V 0 r m o u 0 7 d v W v U m o 2 i j j K c w C m c g w t X 0 I R b a E E b K C h 4 h l d 4 Q 0 / o B b 2 j j 8 V o C R U 7</formula><p>x / A H 6 P M H 7 D y S u g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d v a + t p r j v r I a z J</p><formula xml:id="formula_14">1 3 T 2 2 M i S U x g Q Q = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k o s u C G 5 c V 7 A P a s W T S T B u a y Q x J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S f H j w X X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J Q o y t o 0 E p H q + U Q z w S V r G 2 4 E 6 8 W K k d A X r O t P b 3 K / + 8 i U 5 p G 8 N 7 O Y e S E Z S x 5 w S o y V H g Y h M R M / S O N s m F 5 m w 2 r N q T t z 4 F X i F q Q G B V r D 6 t d g F N E k Z N J Q Q b T u u 0 5 s v J Q o w 6 l g W W W Q a B Y T O i V j 1 r d U k p B p L 5 2 n z v C Z V U Y 4 i J R 9 0 u C 5 + n s j J a H W s 9 C 3 k 3 l K v e z l 4 n 9 e P z H B t Z d y G S e G S b o 4 F C Q C m w j n F e A R V 4 w a M b O E U M V t V k w n R B F q b F E V W 4 K 7 / O V V 0 r m o u 0 7 d v W v U m o 2 i j j K c w C m c g w t X 0 I R b a E E b K C h 4 h l d 4 Q 0 / o B b 2 j j 8 V o C R U 7</formula><p>x / A H 6 P M H 7 D y S u g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d v a + t p r j v r I a z J</p><formula xml:id="formula_15">1 3 T 2 2 M i S U x g Q Q = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k o s u C G 5 c V 7 A P a s W T S T B u a y Q x J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S f H j w X X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J Q o y t o 0 E p H q + U Q z w S V r G 2 4 E 6 8 W K k d A X r O t P b 3 K / + 8 i U 5 p G 8 N 7 O Y e S E Z S x 5 w S o y V H g Y h M R M / S O N s m F 5 m w 2 r N q T t z 4 F X i F q Q G B V r D 6 t d g F N E k Z N J Q Q b T u u 0 5 s v J Q o w 6 l g W W W Q a B Y T O i V j 1 r d U k p B p L 5 2 n z v C Z V U Y 4 i J R 9 0 u C 5 + n s j J a H W s 9 C 3 k 3 l K v e z l 4 n 9 e P z H B t Z d y G S e G S b o 4 F C Q C m w j n F e A R V 4 w a M b O E U M V t V k w n R B F q b F E V W 4 K 7 / O V V 0 r m o u 0 7 d v W v U m o 2 i j j K c w C m c g w t X 0 I R b a E E b K C h 4 h l d 4 Q 0 / o B b 2 j j 8 V o C R U 7</formula><p>x / A H 6 P M H 7 D y S u g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d v a + t p r j v r I a z J</p><formula xml:id="formula_16">1 3 T 2 2 M i S U x g Q Q = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k o s u C G 5 c V 7 A P a s W T S T B u a y Q x J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S f H j w X X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J Q o y t o 0 E p H q + U Q z w S V r G 2 4 E 6 8 W K k d A X r O t P b 3 K / + 8 i U 5 p G 8 N 7 O Y e S E Z S x 5 w S o y V H g Y h M R M / S O N s m F 5 m w 2 r N q T t z 4 F X i F q Q G B V r D 6 t d g F N E k Z N J Q Q b T u u 0 5 s v J Q o w 6 l g W W W Q a B Y T O i V j 1 r d U k p B p L 5 2 n z v C Z V U Y 4 i J R 9 0 u C 5 + n s j J a H W s 9 C 3 k 3 l K v e z l 4 n 9 e P z H B t Z d y G S e G S b o 4 F C Q C m w j n F e A R V 4 w a M b O E U M V t V k w n R B F q b F E V W 4 K 7 / O V V 0 r m o u 0 7 d v W v U m o 2 i j j K c w C m c g w t X 0 I R b a E E b K C h 4 h l d 4 Q 0 / o B b 2 j j 8 V o C R U 7</formula><p>x / A H 6 P M H 7 D y S u g = = &lt; / l a t e x i t &gt; p 6</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e o Q a F n k u F r G S e R r 5 j o</p><formula xml:id="formula_17">x S f J W 3 2 8 Y = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k q M u C G 5 c V 7 A P a s W T S T B u a y Q x J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S f H j w X X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J Q o y t o 0 E p H q + U Q z w S V r G 2 4 E 6 8 W K k d A X r O t P b 3 K / + 8 i U 5 p G 8 N 7 O Y e S E Z S x 5 w S o y V H g Y h M R M / S O N s m F 5 m w 2 r N q T t z 4 F X i F q Q G B V r D 6 t d g F N E k Z N J Q Q b T u u 0 5 s v J Q o w 6 l g W W W Q a B Y T O i V j 1 r d U k p B p L 5 2 n z v C Z V U Y 4 i J R 9 0 u C 5 + n s j J a H W s 9 C 3 k 3 l K v e z l 4 n 9 e P z H B t Z d y G S e G S b o 4 F C Q C m w j n F e A R V 4 w a M b O E U M V t V k w n R B F q b F E V W 4 K 7 / O V V 0 r m o u 0 7 d v W v U m o 2 i j j K c w C m c g w t X 0 I R b a E E b K C h 4 h l d 4 Q 0 / o B b 2 j j 8 V o C R U 7 x / A H 6 P M H 7 c G S u w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e o Q a F n k u F r G S e R r 5 j o x S f J W 3 2 8 Y = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k q M u C G 5 c V 7 A P a s W T S T B u a y Q x J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S f H j w X X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J Q o y t o 0 E p H q + U Q z w S V r G 2 4 E 6 8 W K k d A X r O t P b 3 K / + 8 i U 5 p G 8 N 7 O Y e S E Z S x 5 w S o y V H g Y h M R M / S O N s m F 5 m w 2 r N q T t z 4 F X i F q Q G B V r D 6 t d g F N E k Z N J Q Q b T u u 0 5 s v J Q o w 6 l g W W W Q a B Y T O i V j 1 r d U k p B p L 5 2 n z v C Z V U Y 4 i J R 9 0 u C 5 + n s j J a H W s 9 C 3 k 3 l K v e z l 4 n 9 e P z H B t Z d y G S e G S b o 4 F C Q C m w j n F e A R V 4 w a M b O E U M V t V k w n R B F q b F E V W 4 K 7 / O V V 0 r m o u 0 7 d v W v U m o 2 i j j K c w C m c g w t X 0 I R b a E E b K C h 4 h l d 4 Q 0 / o B b 2 j j 8 V o C R U 7 x / A H 6 P M H 7 c G S u w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e o Q a F n k u F r G S e R r 5 j o x S f J W 3 2 8 Y = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k q M u C G 5 c V 7 A P a s W T S T B u a y Q x J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S f H j w X X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J Q o y t o 0 E p H q + U Q z w S V r G 2 4 E 6 8 W K k d A X r O t P b 3 K / + 8 i U 5 p G 8 N 7 O Y e S E Z S x 5 w S o y V H g Y h M R M / S O N s m F 5 m w 2 r N q T t z 4 F X i F q Q G B V r D 6 t d g F N E k Z N J Q Q b T u u 0 5 s v J Q o w 6 l g W W W Q a B Y T O i V j 1 r d U k p B p L 5 2 n z v C Z V U Y 4 i J R 9 0 u C 5 + n s j J a H W s 9 C 3 k 3 l K v e z l 4 n 9 e P z H B t Z d y G S e G S b o 4 F C Q C m w j n F e A R V 4 w a M b O E U M V t V k w n R B F q b F E V W 4 K 7 / O V V 0 r m o u 0 7 d v W v U m o 2 i j j K c w C m c g w t X 0 I R b a E E b K C h 4 h l d 4 Q 0 / o B b 2 j j 8 V o C R U 7 x / A H 6 P M H 7 c G S u w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e o Q a F n k u F r G S e R r 5 j o x S f J W 3 2 8 Y = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k q M u C G 5 c V 7 A P a s W T S T B u a y Q x J R i n D / I c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S f H j w X X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J Q o y t o 0 E p H q + U Q z w S V r G 2 4 E 6 8 W K k d A X r O t P b 3 K / + 8 i U 5 p G 8 N 7 O Y e S E Z S x 5 w S o y V H g Y h M R M / S O N s m F 5 m w 2 r N q T t z 4 F X i F q Q G B V r D 6 t d g F N E k Z N J Q Q b T u u 0 5 s v J Q o w 6 l g W W W Q a B Y T O i V j 1 r d U k p B p L 5 2 n z v C Z V U Y 4 i J R 9 0 u C 5 + n s j J a H W s 9 C 3 k 3 l K v e z l 4 n 9 e P z H B t Z d y G S e G S b o 4 F C Q C m w j n F e A R V 4 w a M b O E U M V t V k w n R B F q b F E V W 4 K 7 / O V V 0 r m o u 0 7 d v W v U m o 2 i j j K c w C m c g w t X 0 I R b a E E b K C h 4 h l d 4 Q 0 / o B b 2 j j 8 V o C R U 7 x / A H 6 P M H 7 c G S u w = = &lt; / l a t e x i t &gt; p 7</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 q W n 6 g t + l U g w M s F s U 8 Q j P I 0 p l 1</p><formula xml:id="formula_18">E = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I o S 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S p v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I T X f s p k n B g q y f J Q m H B k I p R X g M Z M U W L 4 3 B J M F L N Z E Z l i h Y m x R V V s C d 7 q l 9 d J 9 6 r u u X X v r l F r N Y o 6 y n A G 5 3 A J H j S h B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 7 0 a S v A = = &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 q W n 6 g t + l U g w M s F s U 8 Q j P I 0 p l 1</p><formula xml:id="formula_19">E = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I o S 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S p v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I T X f s p k n B g q y f J Q m H B k I p R X g M Z M U W L 4 3 B J M F L N Z E Z l i h Y m x R V V s C d 7 q l 9 d J 9 6 r u u X X v r l F r N Y o 6 y n A G 5 3 A J H j S h B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 7 0 a S v A = = &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 q W n 6 g t + l U g w M s F s U 8 Q j P I 0 p l 1</p><formula xml:id="formula_20">E = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I o S 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S p v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I T X f s p k n B g q y f J Q m H B k I p R X g M Z M U W L 4 3 B J M F L N Z E Z l i h Y m x R V V s C d 7 q l 9 d J 9 6 r u u X X v r l F r N Y o 6 y n A G 5 3 A J H j S h B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 7 0 a S v A = = &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 q W n 6 g t + l U g w M s F s U 8 Q j P I 0 p l 1</p><formula xml:id="formula_21">E = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I o S 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S p v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I T X f s p k n B g q y f J Q m H B k I p R X g M Z M U W L 4 3 B J M F L N Z E Z l i h Y m x R V V s C d 7 q l 9 d J 9 6 r u u X X v r l F r N Y o 6 y n A G 5 3 A J H j S h B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 7 0 a S v A = = &lt; / l a t e x i t &gt; p 8 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O 8 0 z O + R 6 u X f 3 A d O y v 8 0 v A 9 P s T l 0 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I w S 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S p v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I R N P 2 U y T g y V Z H k o T D g y E c o r Q G O m K D F 8 b g k m i t m s i E y x w s T Y o i q 2 B G / 1 y + u k e 1 X 3 3 L p 3 1 6 i 1 G k U d Z T i D c 7 g E D 6 6 h B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 8 M u S v Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O 8 0 z O + R 6 u X f 3 A d O y v 8 0 v A 9 P s T l 0 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I w S 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S p v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I R N P 2 U y T g y V Z H k o T D g y E c o r Q G O m K D F 8 b g k m i t m s i E y x w s T Y o i q 2 B G / 1 y + u k e 1 X 3 3 L p 3 1 6 i 1 G k U d Z T i D c 7 g E D 6 6 h B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 8 M u S v Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O 8 0 z O + R 6 u X f 3 A d O y v 8 0 v A 9 P s T l 0 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I w S 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S p v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I R N P 2 U y T g y V Z H k o T D g y E c o r Q G O m K D F 8 b g k m i t m s i E y x w s T Y o i q 2 B G / 1 y + u k e 1 X 3 3 L p 3 1 6 i 1 G k U d Z T i D c 7 g E D 6 6 h B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 8 M u S v Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O 8 0 z O + R 6 u X f 3 A d O y v 8 0 v A 9 P s T l 0 = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I w S 4 L b l x W s A 9 o x 5 J J M 2 1 o k h m S j F K G + Q 8 3 L h R x 6 7 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O E H O m j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 1 V G i C O 2 Q i E e q H 2 B N O Z O 0 Y 5 j h t B 8 r i k X A a S + Y 3 e R + 7 5 E q z S J 5 b + Y x 9 Q W e S B Y y g o 2 V H o Y C m 2 k Q p n E 2 S p v Z q F p z 6 + 4 C a J 1 4 B a l B g f a o + j U c R y Q R V B r C s d Y D z 4 2 N n 2 J l G O E 0 q w w T T W N M Z n h C B 5 Z K L K j 2 0 0 X q D F 1 Y Z Y z C S N k n D V q o v z d S L L S e i 8 B O 5 i n 1 q p e L / 3 m D x I R N P 2 U y T g y V Z H k o T D g y E c o r Q G O m K D F 8 b g k m i t m s i E y x w s T Y o i q 2 B G / 1 y + u k e 1 X 3 3 L p 3 1 6 i 1 G k U d Z T i D c 7 g E D 6 6 h B b f Q h g 4 Q U P A M r / D m P D k v z r v z s R w t O c X O K f y B 8 / k D 8 M u S v Q = = &lt; / l a t e x i t &gt; p 9 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t v 1 F 3 Q 5 W j H 7 L r y S m n Y g X t k r Y W x Q = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k o O 4 K b l x W s A 9 o x 5 J J M 2 1 o J j M k G a U M 8 x 9 u X C j i 1 n 9 x 5 9 + Y a W e h r Q c C h 3 P u 5 Z 4 c P x Z c G 8 f 5 R q W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 V G i K G v T S E S q 5 x P N B J e s b b g R r B c r R k J f s K 4 / v c n 9 7 i N T m k f y 3 s x i 5 o V k L H n A K T F W e h i E x E z 8 I I 2 z Y X q d D a s 1 p + 7 M g V e J W 5 A a F G g N q 1 + D U U S T k E l D B d G 6 7 z q x 8 V K i D K e C Z Z V B o l l M 6 J S M W d 9 S S U K m v X S e O s N n V h n h I F L 2 S Y P n 6 u + N l I R a z 0 L f T u Y p 9 b K X i / 9 5 / c Q E V 1 7 K Z Z w Y J u n i U J A I b C K c V 4 B H X D F q x M w S Q h W 3 W T G d E E W o s U V V b A n u 8 p d X S e e i 7 j p 1 9 6 5 R a z a K O s p w A q d w D i 5 c Q h N u o Q V t o K D g G V 7 h D T 2 h F / S O P h a j J V T s H M M f o M 8 f 8 l C S v g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t v 1 F 3 Q 5 W j H 7 L r y S m n Y g X t k r Y W x Q = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k o O 4 K b l x W s A 9 o x 5 J J M 2 1 o J j M k G a U M 8 x 9 u X C j i 1 n 9 x 5 9 + Y a W e h r Q c C h 3 P u 5 Z 4 c P x Z c G 8 f 5 R q W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 V G i K G v T S E S q 5 x P N B J e s b b g R r B c r R k J f s K 4 / v c n 9 7 i N T m k f y 3 s x i 5 o V k L H n A K T F W e h i E x E z 8 I I 2 z Y X q d D a s 1 p + 7 M g V e J W 5 A a F G g N q 1 + D U U S T k E l D B d G 6 7 z q x 8 V K i D K e C Z Z V B o l l M 6 J S M W d 9 S S U K m v X S e O s N n V h n h I F L 2 S Y P n 6 u + N l I R a z 0 L f T u Y p 9 b K X i / 9 5 / c Q E V 1 7 K Z Z w Y J u n i U J A I b C K c V 4 B H X D F q x M w S Q h W 3 W T G d E E W o s U V V b A n u 8 p d X S e e i 7 j p 1 9 6 5 R a z a K O s p w A q d w D i 5 c Q h N u o Q V t o K D g G V 7 h D T 2 h F / S O P h a j J V T s H M M f o M 8 f 8 l C S v g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t v 1 F 3 Q 5 W j H 7 L r y S m n Y g X t k r Y W x Q = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k o O 4 K b l x W s A 9 o x 5 J J M 2 1 o J j M k G a U M 8 x 9 u X C j i 1 n 9 x 5 9 + Y a W e h r Q c C h 3 P u 5 Z 4 c P x Z c G 8 f 5 R q W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 V G i K G v T S E S q 5 x P N B J e s b b g R r B c r R k J f s K 4 / v c n 9 7 i N T m k f y 3 s x i 5 o V k L H n A K T F W e h i E x E z 8 I I 2 z Y X q d D a s 1 p + 7 M g V e J W 5 A a F G g N q 1 + D U U S T k E l D B d G 6 7 z q x 8 V K i D K e C Z Z V B o l l M 6 J S M W d 9 S S U K m v X S e O s N n V h n h I F L 2 S Y P n 6 u + N l I R a z 0 L f T u Y p 9 b K X i / 9 5 / c Q E V 1 7 K Z Z w Y J u n i U J A I b C K c V 4 B H X D F q x M w S Q h W 3 W T G d E E W o s U V V b A n u 8 p d X S e e i 7 j p 1 9 6 5 R a z a K O s p w A q d w D i 5 c Q h N u o Q V t o K D g G V 7 h D T 2 h F / S O P h a j J V T s H M M f o M 8 f 8 l C S v g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t v 1 F 3 Q 5 W j H 7 L r y S m n Y g X t k r Y W x Q = " &gt; A A A B 9 X i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k o O 4 K b l x W s A 9 o x 5 J J M 2 1 o J j M k G a U M 8 x 9 u X C j i 1 n 9 x 5 9 + Y a W e h r Q c C h 3 P u 5 Z 4 c P x Z c G 8 f 5 R q W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 V G i K G v T S E S q 5 x P N B J e s b b g R r B c r R k J f s K 4 / v c n 9 7 i N T m k f y 3 s x i 5 o V k L H n A K T F W e h i E x E z 8 I I 2 z Y X q d D a s 1 p + 7 M g V e J W 5 A a F G g N q 1 + D U U S T k E l D B d G 6 7 z q x 8 V K i D K e C Z Z V B o l l M 6 J S M W d 9 S S U K m v X S e O s N n V h n h I F L 2 S Y P n 6 u + N l I R a z 0 L f T u Y p 9 b K X i / 9 5 / c Q E V 1 7 K Z Z w Y J u n i U J A I b C K c V 4 B H X D F q x M w S Q h W 3 W T G d E E W o s U V V b A n u 8 p d X S e e i 7 j p 1 9 6 5 R a z a K O s p w A q d w D i 5 c Q h N u o Q V t o K D g G V 7 h D T 2 h F / S O P h a j J V T s H M M f o M 8 f 8 l C S v g = = &lt; / l a t e x i t &gt; p 10 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i X J l n 1 Z + z f D h r H n 7 V V N x + 5 5 r + F c = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R Q l 0 W 3 L i s Y B / Q h j C Z T t q h k 0 m Y m Q g 1 5 E v c u F D E r Z / i z r 9 x 0 m a h r Q c G D u f c y z 1 z g o Q z p R 3 n 2 6 p s b e / s 7 l X 3 a w e H R 8 d 1 + + S 0 r + J U E t o j M Y / l M M C K c i Z o T z P N 6 T C R F E c B p 4 N g f l v 4 g 0 c q F Y v F g 1 4 k 1 I v w V L C Q E a y N 5 N v 1 c Y T 1 L A i z J P c z 1 8 l 9 u + E 0 n S X Q J n F L 0 o A S X d / + G k 9 i k k Z U a M K x U i P X S b S X Y a k Z 4 T S v j V N F E 0 z m e E p H h g o c U e V l y + A 5 u j T K B I W x N E 9 o t F R / b 2 Q 4 U m o R B W a y i K n W v U L 8 z x u l O r z x M i a S V F N B V o f C l C M d o 6 I F N G G S E s 0 X h m A i m c m K y A x L T L T p q m Z K c N e / v E n 6 1 0 3 X a b r 3 r U a n V d Z R h X O 4 g C t w o Q 0 d u I M u 9 I B A C s / w C m / W k / V i v V s f q 9 G K V e 6 c w R 9 Y n z / R G 5 M h &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i X J l n 1 Z + z f D h r H n 7 V V N x + 5 5 r + F c = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R Q l 0 W 3 L i s Y B / Q h j C Z T t q h k 0 m Y m Q g 1 5 E v c u F D E r Z / i z r 9 x 0 m a h r Q c G D u f c y z 1 z g o Q z p R 3 n 2 6 p s b e / s 7 l X 3 a w e H R 8 d 1 + + S 0 r + J U E t o j M Y / l M M C K c i Z o T z P N 6 T C R F E c B p 4 N g f l v 4 g 0 c q F Y v F g 1 4 k 1 I v w V L C Q E a y N 5 N v 1 c Y T 1 L A i z J P c z 1 8 l 9 u + E 0 n S X Q J n F L 0 o A S X d / + G k 9 i k k Z U a M K x U i P X S b S X Y a k Z 4 T S v j V N F E 0 z m e E p H h g o c U e V l y + A 5 u j T K B I W x N E 9 o t F R / b 2 Q 4 U m o R B W a y i K n W v U L 8 z x u l O r z x M i a S V F N B V o f C l C M d o 6 I F N G G S E s 0 X h m A i m c m K y A x L T L T p q m Z K c N e / v E n 6 1 0 3 X a b r 3 r U a n V d Z R h X O 4 g C t w o Q 0 d u I M u 9 I B A C s / w C m / W k / V i v V s f q 9 G K V e 6 c w R 9 Y n z / R G 5 M h &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i X J l n 1 Z + z f D h r H n 7 V V N x + 5 5 r + F c = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R Q l 0 W 3 L i s Y B / Q h j C Z T t q h k 0 m Y m Q g 1 5 E v c u F D E r Z / i z r 9 x 0 m a h r Q c G D u f c y z 1 z g o Q z p R 3 n 2 6 p s b e / s 7 l X 3 a w e H R 8 d 1 + + S 0 r + J U E t o j M Y / l M M C K c i Z o T z P N 6 T C R F E c B p 4 N g f l v 4 g 0 c q F Y v F g 1 4 k 1 I v w V L C Q E a y N 5 N v 1 c Y T 1 L A i z J P c z 1 8 l 9 u + E 0 n S X Q J n F L 0 o A S X d / + G k 9 i k k Z U a M K x U i P X S b S X Y a k Z 4 T S v j V N F E 0 z m e E p H h g o c U e V l y + A 5 u j T K B I W x N E 9 o t F R / b 2 Q 4 U m o R B W a y i K n W v U L 8 z x u l O r z x M i a S V F N B V o f C l C M d o 6 I F N G G S E s 0 X h m A i m c m K y A x L T L T p q m Z K c N e / v E n 6 1 0 3 X a b r 3 r U a n V d Z R h X O 4 g C t w o Q 0 d u I M u 9 I B A C s / w C m / W k / V i v V s f q 9 G K V e 6 c w R 9 Y n z / R G 5 M h &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i X J l n 1 Z + z f D h r H n 7 V V N x + 5 5 r + F c = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y W R Q l 0 W 3 L i s Y B / Q h j C Z T t q h k 0 m Y m Q g 1 5 E v c u F D E r Z / i z r 9 x 0 m a h r Q c G D u f c y z 1 z g o Q z p R 3 n 2 6 p s b e / s 7 l X 3 a w e H R 8 d 1 + + S 0 r + J U E t o j M Y / l M M C K c i Z o T z P N 6 T C R F E c B p 4 N g f l v 4 g 0 c q F Y v F g 1 4 k 1 I v w V L C Q E a y N 5 N v 1 c Y T 1 L A i z J P c z 1 8 l 9 u + E 0 n S X Q J n F L 0 o A S X d / + G k 9 i k k Z U a M K x U i P X S b S X Y a k Z 4 T S v j V N F E 0 z m e E p H h g o c U e V l y + A 5 u j T K B I W x N E 9 o t F R / b 2 Q 4 U m o R B W a y i K n W v U L 8 z x u l O r z x M i a S V F N B V o f C l C M d o 6 I F N G G S E s 0 X h m A i m c m K y A x L T L T p q m Z K c N e / v E n 6 1 0 3 X a b r 3 r U a n V d Z R h X O 4 g C t w o Q 0 d u I M u 9 I B A C s / w C m / W k / V i v V s f q 9 G K V e 6 c w R 9 Y n z / R G 5 M h &lt; / l a t e x i t &gt; p 11 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 d z g v O Z k R + G C k b C y p E M P l F m h z r k = " &gt; A A A B + H i c b V D L S g M x F L 1 T X 7 U + O u r S T b A I r s p E B F 0 W 3 L i s Y B / Q D k M m z b S h m c y Q Z I Q 6 9 E v c u F D E r Z / i z r 8 x 0 8 5 C W w 8 E D u f c y z 0 5 Y S q 4 N p 7 3 7 V Q 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 e 3 T c 1 U m m K O v Q R C S q H x L N B J e s Y 7 g R r J 8 q R u J Q s F 4 4 v S 3 8 3 i N T m i f y w c x S 5 s d k L H n E K T F W C t z 6 M C Z m E k Z 5 O g 9 y j O e B 2 / C a 3 g J o n e C S N K B E O 3 C / h q O E Z j G T h g q i 9 Q B 7 q f F z o g y n g s 1 r w 0 y z l N A p G b O B p Z L E T P v 5 I v g c n V t l h K J E 2 S c N W q i / N 3 I S a z 2 L Q z t Z x N S r X i H + 5 w 0 y E 9 3 4 O Z d p Z p i k y 0 N R J p B J U N E C G n H F q B E z S w h V 3 G Z F d E I U o c Z 2 V b M l 4 N U v r 5 P u Z R N 7 T X x / 1 W h d l X V U 4 R T O 4 A I w X E M L 7 q A N H a C Q w T O 8 w p v z 5 L w 4 7 8 7 H c r T i l D s n 8 A f O 5 w / S o J M i &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 d z g v O Z k R + G C k b C y p E M P l F m h z r k = " &gt; A A A B + H i c b V D L S g M x F L 1 T X 7 U + O u r S T b A I r s p E B F 0 W 3 L i s Y B / Q D k M m z b S h m c y Q Z I Q 6 9 E v c u F D E r Z / i z r 8 x 0 8 5 C W w 8 E D u f c y z 0 5 Y S q 4 N p 7 3 7 V Q 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 e 3 T c 1 U m m K O v Q R C S q H x L N B J e s Y 7 g R r J 8 q R u J Q s F 4 4 v S 3 8 3 i N T m i f y w c x S 5 s d k L H n E K T F W C t z 6 M C Z m E k Z 5 O g 9 y j O e B 2 / C a 3 g J o n e C S N K B E O 3 C / h q O E Z j G T h g q i 9 Q B 7 q f F z o g y n g s 1 r w 0 y z l N A p G b O B p Z L E T P v 5 I v g c n V t l h K J E 2 S c N W q i / N 3 I S a z 2 L Q z t Z x N S r X i H + 5 w 0 y E 9 3 4 O Z d p Z p i k y 0 N R J p B J U N E C G n H F q B E z S w h V 3 G Z F d E I U o c Z 2 V b M l 4 N U v r 5 P u Z R N 7 T X x / 1 W h d l X V U 4 R T O 4 A I w X E M L 7 q A N H a C Q w T O 8 w p v z 5 L w 4 7 8 7 H c r T i l D s n 8 A f O 5 w / S o J M i &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 d z g v O Z k R + G C k b C y p E M P l F m h z r k = " &gt; A A A B + H i c b V D L S g M x F L 1 T X 7 U + O u r S T b A I r s p E B F 0 W 3 L i s Y B / Q D k M m z b S h m c y Q Z I Q 6 9 E v c u F D E r Z / i z r 8 x 0 8 5 C W w 8 E D u f c y z 0 5 Y S q 4 N p 7 3 7 V Q 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 e 3 T c 1 U m m K O v Q R C S q H x L N B J e s Y 7 g R r J 8 q R u J Q s F 4 4 v S 3 8 3 i N T m i f y w c x S 5 s d k L H n E K T F W C t z 6 M C Z m E k Z 5 O g 9 y j O e B 2 / C a 3 g J o n e C S N K B E O 3 C / h q O E Z j G T h g q i 9 Q B 7 q f F z o g y n g s 1 r w 0 y z l N A p G b O B p Z L E T P v 5 I v g c n V t l h K J E 2 S c N W q i / N 3 I S a z 2 L Q z t Z x N S r X i H + 5 w 0 y E 9 3 4 O Z d p Z p i k y 0 N R J p B J U N E C G n H F q B E z S w h V 3 G Z F d E I U o c Z 2 V b M l 4 N U v r 5 P u Z R N 7 T X x / 1 W h d l X V U 4 R T O 4 A I w X E M L 7 q A N H a C Q w T O 8 w p v z 5 L w 4 7 8 7 H c r T i l D s n 8 A f O 5 w / S o J M i &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 d z g v O Z k R + G C k b C y p E M P l F m h z r k = " &gt; A A A B + H i c b V D L S g M x F L 1 T X 7 U + O u r S T b A I r s p E B F 0 W 3 L i s Y B / Q D k M m z b S h m c y Q Z I Q 6 9 E v c u F D E r Z / i z r 8 x 0 8 5 C W w 8 E D u f c y z 0 5 Y S q 4 N p 7 3 7 V Q 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 e 3 T c 1 U m m K O v Q R C S q H x L N B J e s Y 7 g R r J 8 q R u J Q s F 4 4 v S 3 8 3 i N T m i f y w c x S 5 s d k L H n E K T F W C t z 6 M C Z m E k Z 5 O g 9 y j O e B 2 / C a 3 g J o n e C S N K B E O 3 C / h q O E Z j G T h g q i 9 Q B 7 q f F z o g y n g s 1 r w 0 y z l N A p G b O B p Z L E T P v 5 I v g c n V t l h K J E 2 S c N W q i / N 3 I S a z 2 L Q z t Z x N S r X i H + 5 w 0 y E 9 3 4 O Z d p Z p i k y 0 N R J p B J U N E C G n H F q B E z S w h V 3 G Z F d E I U o c Z 2 V b M l 4 N U v r 5 P u Z R N 7 T X x / 1 W h d l X V U 4 R T O 4 A I w X E M L 7 q A N H a C Q w T O 8 w p v z 5 L w 4 7 8 7 H c r T i l D s n 8 A f O 5 w / S o J M i &lt; / l a t e x i t &gt; p 12</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r 7 u H 1 Y a q N n I 9</p><formula xml:id="formula_22">0 c + 3 Y Y 8 T I Y l n J U E = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y U p B V 0 W 3 L i s Y B / Q h j C Z T t q h k 0 m Y m Q g 1 5 E v c u F D E r Z / i z r 9 x 0 m a h r Q c G D u f c y z 1 z g o Q z p R 3 n 2 6 p s b e / s 7 l X 3 a w e H R 8 d 1 + + S 0 r + J U E t o j M Y / l M M C K c i Z o T z P N 6 T C R F E c B p 4 N g f l v 4 g 0 c q F Y v F g 1 4 k 1 I v w V L C Q E a y N 5 N v 1 c Y T 1 L A i z J P c z t 5 X 7 d s N p O k u g T e K W p A E l u r 7 9 N Z 7 E J I 2 o 0 I R j p U a u k 2 g v w 1 I z w m l e G 6 e K J p j M 8 Z S O D B U 4 o s r L l s F z d G m U C Q p j a Z 7 Q a K n + 3 s h w p N Q i C s x k E V O t e 4 X 4 n z d K d X j j Z U w k q a a C r A 6 F K U c 6 R k U L a M I k J Z o v D M F E M p M V k R m W m G j T V c 2 U 4 K 5 / e Z P 0 W 0 3 X a b r 3 7 U a n X d Z R h X O 4 g C t w 4 R o 6 c A d d 6 A G B F J 7 h F d 6 s J + v F e r c + V q M V</formula><p>q 9 w 5 g z + w P n 8 A 1 C W T I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r 7 u H 1 Y a q N n I 9</p><formula xml:id="formula_23">0 c + 3 Y Y 8 T I Y l n J U E = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y U p B V 0 W 3 L i s Y B / Q h j C Z T t q h k 0 m Y m Q g 1 5 E v c u F D E r Z / i z r 9 x 0 m a h r Q c G D u f c y z 1 z g o Q z p R 3 n 2 6 p s b e / s 7 l X 3 a w e H R 8 d 1 + + S 0 r + J U E t o j M Y / l M M C K c i Z o T z P N 6 T C R F E c B p 4 N g f l v 4 g 0 c q F Y v F g 1 4 k 1 I v w V L C Q E a y N 5 N v 1 c Y T 1 L A i z J P c z t 5 X 7 d s N p O k u g T e K W p A E l u r 7 9 N Z 7 E J I 2 o 0 I R j p U a u k 2 g v w 1 I z w m l e G 6 e K J p j M 8 Z S O D B U 4 o s r L l s F z d G m U C Q p j a Z 7 Q a K n + 3 s h w p N Q i C s x k E V O t e 4 X 4 n z d K d X j j Z U w k q a a C r A 6 F K U c 6 R k U L a M I k J Z o v D M F E M p M V k R m W m G j T V c 2 U 4 K 5 / e Z P 0 W 0 3 X a b r 3 7 U a n X d Z R h X O 4 g C t w 4 R o 6 c A d d 6 A G B F J 7 h F d 6 s J + v F e r c + V q M V</formula><p>q 9 w 5 g z + w P n 8 A 1 C W T I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r 7 u H 1 Y a q N n I 9</p><formula xml:id="formula_24">0 c + 3 Y Y 8 T I Y l n J U E = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y U p B V 0 W 3 L i s Y B / Q h j C Z T t q h k 0 m Y m Q g 1 5 E v c u F D E r Z / i z r 9 x 0 m a h r Q c G D u f c y z 1 z g o Q z p R 3 n 2 6 p s b e / s 7 l X 3 a w e H R 8 d 1 + + S 0 r + J U E t o j M Y / l M M C K c i Z o T z P N 6 T C R F E c B p 4 N g f l v 4 g 0 c q F Y v F g 1 4 k 1 I v w V L C Q E a y N 5 N v 1 c Y T 1 L A i z J P c z t 5 X 7 d s N p O k u g T e K W p A E l u r 7 9 N Z 7 E J I 2 o 0 I R j p U a u k 2 g v w 1 I z w m l e G 6 e K J p j M 8 Z S O D B U 4 o s r L l s F z d G m U C Q p j a Z 7 Q a K n + 3 s h w p N Q i C s x k E V O t e 4 X 4 n z d K d X j j Z U w k q a a C r A 6 F K U c 6 R k U L a M I k J Z o v D M F E M p M V k R m W m G j T V c 2 U 4 K 5 / e Z P 0 W 0 3 X a b r 3 7 U a n X d Z R h X O 4 g C t w 4 R o 6 c A d d 6</formula><p>A G B F J 7 h F d 6 s J + v F e r c + V q M V q 9 w 5 g z + w P n 8 A 1 C W T I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r 7 u H 1 Y a q N n I 9 </p><formula xml:id="formula_25">0 c + 3 Y Y 8 T I Y l n J U E = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y U p B V 0 W 3 L i s Y B / Q h j C Z T t q h k 0 m Y m Q g 1 5 E v c u F D E r Z / i z r 9 x 0 m a h r Q c G D u f c y z 1 z g o Q z p R</formula><formula xml:id="formula_26">F z d G m U C Q p j a Z 7 Q a K n + 3 s h w p N Q i C s x k E V O t e 4 X 4 n z d K d X j j Z U w k q a a C r A 6 F K U c 6 R k U L a M I k J Z o v D M F E M p M V k R m W m G j T V c 2 U 4 K 5 / e Z P 0 W 0 3 X a b r 3 7 U a n X d Z R h X O 4 g C t w 4 R o 6 c A d d 6</formula><p>A G B F J 7 h F d 6 s J + v F e r c + V q M V q 9 w 5 g z + w P n 8 A 1 C W T I w = = &lt; / l a t e x i t &gt; L(football) = L MLM (x 7 ) + L SBO (x 4 , x 9 , p 7 )</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 + N H V i e v k u y l g n B k e T s u 5 R U 7 </p><formula xml:id="formula_27">K H M = " &gt; A A A C c X i c b V H L S g M x F M 2 M 7 / q q j 4 2 I E l o E p V J m R F A X Q t G N C 0 V F W 4 W 2 D J</formula><formula xml:id="formula_28">p I C T 4 R I t v D p 3 h M 9 t K h f n V 5 l R l v L v h B + p J 5 R 3 u V f 2 x 3 Z 9 c / b Y f 7 e K w 7 + e 7 i f A u v W H a q z q D w X + C O Q B m N 6 s Y r v r U 6 k i Y h i 4 A K o n X T d W J o p 0 Q B p 4 J l h V a i W U z o E + m y p o E R C Z l u p 4 P E M r x j m A 4 O p D I r A j x g x y d S E m r d D 3 3 j z C + p f 2 s 5 + Z / W T C A 4 b q c 8 i h N g E R 0 e F C Q C g 8 R 5 / L j D F a M g + g Y Q q r i 5 K 6 Y 9 o g g F 8 0 k F E 4 L 7 + 8 l / Q e O g 6 j p V 9 / a w X K u N 4 p h F m 6 i E d p G L j l A N X a A b V E c U f V j r 1 p a 1 b X 3 a G z a 2 S 0 O r b Y 1 m 1 t C P s i t f O v a + / g = = &lt;</formula><p>/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 + N H V i e v k u y l g n B k e T s u 5 R U 7 </p><formula xml:id="formula_29">K H M = " &gt; A A A C c X i c b V H L S g M x F M 2 M 7 / q q j 4 2 I E l o E p V J m R F A X Q t G N C 0 V F W 4 W 2 D J</formula><formula xml:id="formula_30">p I C T 4 R I t v D p 3 h M 9 t K h f n V 5 l R l v L v h B + p J 5 R 3 u V f 2 x 3 Z 9 c / b Y f 7 e K w 7 + e 7 i f A u v W H a q z q D w X + C O Q B m N 6 s Y r v r U 6 k i Y h i 4 A K o n X T d W J o p 0 Q B p 4 J l h V a i W U z o E + m y p o E R C Z l u p 4 P E M r x j m A 4 O p D I r A j x g x y d S E m r d D 3 3 j z C + p f 2 s 5 + Z / W T C A 4 b q c 8 i h N g E R 0 e F C Q C g 8 R 5 / L j D F a M g + g Y Q q r i 5 K 6 Y 9 o g g F 8 0 k F E 4 L 7 + 8 l / Q e O g 6 j p V 9 / a w X K u N 4 p h F m 6 i E d p G L j l A N X a A b V E c U f V j r 1 p a 1 b X 3 a G z a 2 S 0 O r b Y 1 m 1 t C P s i t f O v a + / g = = &lt;</formula><p>/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 + N H V i e v k u y l g n B k e T s u 5 R U 7 </p><formula xml:id="formula_31">K H M = " &gt; A A A C c X i c b V H L S g M x F M 2 M 7 / q q j 4 2 I E l o E p V J m R F A X Q t G N C 0 V F W 4 W 2 D J</formula><formula xml:id="formula_32">p I C T 4 R I t v D p 3 h M 9 t K h f n V 5 l R l v L v h B + p J 5 R 3 u V f 2 x 3 Z 9 c / b Y f 7 e K w 7 + e 7 i f A u v W H a q z q D w X + C O Q B m N 6 s Y r v r U 6 k i Y h i 4 A K o n X T d W J o p 0 Q B p 4 J l h V a i W U z o E + m y p o E R C Z l u p 4 P E M r x j m A 4 O p D I r A j x g x y d S E m r d D 3 3 j z C + p f 2 s 5 + Z / W T C A 4 b q c 8 i h N g E R 0 e F C Q C g 8 R 5 / L j D F a M g + g Y Q q r i 5 K 6 Y 9 o g g F 8 0 k F E 4 L 7 + 8 l / Q e O g 6 j p V 9 / a w X K u N 4 p h F m 6 i E d p G L j l A N X a A b V E c U f V j r 1 p a 1 b X 3 a G z a 2 S 0 O r b Y 1 m 1 t C P s i t f O v a + / g = = &lt;</formula><p>/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 + N H V i e v k u y l g n B k e T s u 5 R U 7  88.7% F1 on SQuAD 1.1 and 2.0 <ref type="bibr" target="#b36">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b35">(Rajpurkar et al., , 2018))</ref>, respectively. We also observe similar gains on five additional extractive question answering benchmarks (NewsQA, TriviaQA, SearchQA, HotpotQA, and Natural Questions). <ref type="foot" target="#foot_0">1</ref>SpanBERT also arrives at a new state of the art on the challenging CoNLL-2012 ("OntoNotes") shared task for document-level coreference resolution, where we reach 79.6% F1, exceeding the previous top model by 6.6% absolute. Finally, we demonstrate that SpanBERT also helps on tasks that do not explicitly involve span selection, and show that our approach even improves performance on TACRED <ref type="bibr" target="#b49">(Zhang et al., 2017)</ref> and GLUE <ref type="bibr" target="#b44">(Wang et al., 2019)</ref>.</p><formula xml:id="formula_33">K H M = " &gt; A A A C c X i c b V H L S g M x F M 2 M 7 / q q j 4 2 I E l o E p V J m R F A X Q t G N C 0 V F W 4 W 2 D J</formula><formula xml:id="formula_34">p I C T 4 R I t v D p 3 h M 9 t K h f n V 5 l R l v L v h B + p J 5 R 3 u V f 2 x 3 Z 9 c / b Y f 7 e K w 7 + e 7 i f A u v W H a q z q D w X + C O Q B m N 6 s Y r v r U 6 k i Y h i 4 A K o n X T d W J o p 0 Q B p 4 J l h V a i W U z o E + m y p o E R</formula><p>While others show the benefits of adding more data <ref type="bibr" target="#b47">(Yang et al., 2019)</ref> and increasing model size <ref type="bibr" target="#b22">(Lample and Conneau, 2019)</ref>, this work demonstrates the importance of designing good pretraining tasks and objectives, which can also have a significant impact.</p><p>2 Background: BERT BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> is a self-supervised approach for pre-training a deep transformer encoder <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref>, before fine-tuning it for a particular downstream task. BERT optimizes two training objectives -masked language modeling (MLM) and next sentence prediction (NSP)which only require a large collection of unlabeled text.</p><p>Notation Given a sequence of word or subword tokens X = (x 1 , . . . , x n ), BERT trains Masked Language Modeling (MLM) Also known as a cloze test, MLM is the task of predicting missing tokens in a sequence from their placeholders. Specifically, a subset of tokens Y  X is sampled and substituted with a different set of tokens. In BERT's implementation, Y accounts for 15% of the tokens in X; of those, 80% are replaced with [MASK], 10% are replaced with a random token (according to the unigram distribution), and 10% are kept unchanged. The task is to predict the original tokens in Y from the modified input.</p><p>BERT selects each token in Y independently by randomly selecting a subset. In SpanBERT, we define Y by randomly selecting contiguous spans (Section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Next Sentence Prediction (NSP)</head><p>The NSP task takes two sequences X A , X B as input, and predicts whether X B is the direct continuation of X A . This is implemented in BERT by first reading X A from the corpus, and then (1) either reading X B from the point where X A ended, or (2) randomly sampling X B from a different point in the corpus. The two sequences are separated by a special [SEP]token. Additionally, a special [CLS]token is added to X A , X B to form the input, where the target of [CLS]is whether X B indeed follows X A in the corpus.</p><p>In SpanBERT, we remove the NSP objective and sample a single full-length sequence (Sec-tion 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We present SpanBERT, a self-supervised pretraining method designed to better represent and predict spans of text. Our approach is inspired by BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, but deviates from its bi-text classification framework in three ways. First, we use a different random process to mask spans of tokens, rather than individual ones. We also introduce a novel auxiliary objective -the span boundary objective (SBO) -which tries to predict the entire masked span using only the representations of the tokens at the span's boundary. Finally, SpanBERT samples a single contiguous segment of text for each training example (instead of two), and thus has no use for BERT's next sentence prediction objective, which we omit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Span Masking</head><p>Given a sequence of tokens X = (x 1 , . . . , x n ), we select a subset of tokens Y  X by iteratively sampling spans of text until the masking budget (e.g. 15% of X) has been spent. At each iteration, we first sample the span's length from a geometric distribution  Geo(p), which is skewed towards shorter spans. We then randomly (uniformly) select the starting point for the span.</p><p>Following preliminary trials, we set p = 0.2, and also clip at max = 10. This yields a mean span length of  = 3.8. We also measure span length in complete words, not subword tokens, making the masked spans even longer. Figure <ref type="figure" target="#fig_7">2</ref> shows the distribution of span mask lengths.</p><p>As in BERT, we also mask 15% of the tokens in total: replacing 80% of the masked tokens with [MASK], 10% with random tokens and 10% with the original tokens. However, we perform this replacement at the span level and not for each token individually; i.e. all the tokens in a span are replaced with [MASK]or sampled tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Span Boundary Objective (SBO)</head><p>Span selection models <ref type="bibr" target="#b25">(Lee et al., 2016</ref><ref type="bibr" target="#b23">(Lee et al., , 2017;;</ref><ref type="bibr" target="#b13">He et al., 2018)</ref> typically create a fixed-length representation of a span using its boundary tokens (start and end). To support such models, we would ideally like the representations for the end of the span to summarize as much of the internal span content as possible. We do so by introducing a span boundary objective that involves predicting each token of a masked span using only the representations of the observed tokens at the boundaries (Figure <ref type="figure" target="#fig_6">1</ref>).</p><p>Formally, given a masked span (x s , . . . , x e )  Y , where (s, e) indicates its start and end positions, we represent each token x i in the span using the encodings of the external boundary tokens x s1 and x e+1 , as well as the positional embedding of the target token p i :</p><formula xml:id="formula_35">y i = f (x s1 , x e+1 , p i )</formula><p>In this work, we implement the representation function f () as a 2-layer feed-forward network with GeLU activations <ref type="bibr" target="#b14">(Hendrycks and Gimpel, 2016)</ref> and layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>:</p><formula xml:id="formula_36">h = LayerNorm(GeLU(W 1  [x s1 ; x e+1 ; p i ])) f () = LayerNorm(GeLU(W 2  h))</formula><p>We then use the vector representation y i to predict x i and compute the cross-entropy loss exactly like the MLM objective.</p><p>SpanBERT sums the loss from both the span boundary and the regular masked language modeling objectives for each token in the masked span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Single-Sequence Training</head><p>As described in Section 2, BERT's examples contain two sequences of text (X A , X B ), and an objective that trains the model to predict whether they are connected (NSP). We find that this setting is almost always worse than simply using a single sequence without the NSP objective (see Section 4.3 for further details). We conjecture that single-sequence training is superior to bi-sequence training with NSP because (a) the model benefits from longer full-length contexts, or (b) conditioning on context from another document adds noise to the masked language model. Therefore, in our approach, we remove both the NSP objective and the two-segment sampling procedure, and simply sample a single contiguous segment of up to n = 512 tokens, rather than two half-segments that sum up to n tokens together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks</head><p>We evaluate on a comprehensive suite of tasks, including seven question answering tasks, coreference resolution, nine tasks in the GLUE benchmark <ref type="bibr" target="#b44">(Wang et al., 2019)</ref>, and relation extraction. We expect that the span selection tasks, question answering and coreference resolution, will particularly benefit from our span-based pre-training.</p><p>Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer.</p><p>We first evaluate on SQuAD 1.1 and 2.0 <ref type="bibr" target="#b36">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b35">(Rajpurkar et al., , 2018))</ref>, which have served as major question answering benchmarks, particularly for pre-trained models <ref type="bibr" target="#b32">(Peters et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019;</ref><ref type="bibr" target="#b47">Yang et al., 2019)</ref>. We also evaluate on five more datasets from the MRQA shared task: 2 NewsQA <ref type="bibr" target="#b42">(Trischler et al., 2017)</ref>, SearchQA <ref type="bibr" target="#b11">(Dunn et al., 2017)</ref>, TriviaQA <ref type="bibr" target="#b18">(Joshi et al., 2017)</ref>, HotpotQA <ref type="bibr" target="#b48">(Yang et al., 2018)</ref> and Natural Questions (NaturalQA) <ref type="bibr" target="#b21">(Kwiatkowski et al., 2019)</ref>. Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good testbed for evaluating whether our pretrained models can generalize well across different data distributions.</p><p>Following BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, we use the same QA model for all the datasets. We first convert the passage P = (p 1 , . . . , p l ) and question Q = (q 1 , . . . , q l ) into a single sequence X = 2 https://github.com/mrqa/MRQA-Shared-Task-2019. MRQA changed the original datasets to unify them into the same format, e.g. all the contexts are truncated to a maximum of 800 tokens and only answerable questions are kept.</p><p>[CLS]p 1 . . . p l [SEP]q 1 . . . q l [SEP], pass it to the pre-trained transformer encoder, and train two linear classifiers independently on top of it for predicting the answer span boundary (start and end). For the unanswerable questions in SQuAD 2.0, we simply set the answer span to be the special token GLUE The General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b44">(Wang et al., 2019)</ref> consists of 9 sentence-level classification tasks: 2 single-sentence tasks: CoLA <ref type="bibr" target="#b45">(Warstadt et al., 2018)</ref>, SST-2 <ref type="bibr" target="#b38">(Socher et al., 2013)</ref>, 3 sentence similarity tasks: MRPC (Dolan and <ref type="bibr" target="#b9">Brockett, 2005)</ref>, STS-B <ref type="bibr" target="#b2">(Cer et al., 2017)</ref>, QQP,<ref type="foot" target="#foot_1">3</ref> and 4 natural language inference tasks: MNLI <ref type="bibr" target="#b46">(Williams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b36">(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b5">(Dagan et al., 2005;</ref><ref type="bibr" target="#b1">Bar-Haim et al., 2006;</ref><ref type="bibr" target="#b12">Giampiccolo et al., 2007)</ref> and WNLI <ref type="bibr" target="#b26">(Levesque et al., 2011)</ref>. While recent work <ref type="bibr" target="#b27">(Liu et al., 2019a)</ref> has applied several task-specific strategies to increase performance on the individual GLUE tasks, we follow BERT's single-task setting and add a linear classifier on top of the [CLS]token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>We reimplemented BERT's model and pretraining method in fairseq <ref type="bibr" target="#b31">(Ott et al., 2019)</ref>. We used the model configuration of BERT-large as in <ref type="bibr" target="#b8">Devlin et al. (2019)</ref> and also trained all our models on the same corpus: BooksCorpus and English Wikipedia using cased word-piece tokens.</p><p>The main difference in our implementation is that we use different masks at each epoch while BERT samples 10 different masks for each sequence during data processing. Additionally, the original BERT implementation samples shorter sequences with a small probability (0.1) while we always take sequences of up to 512 tokens until it reaches a document boundary. <ref type="foot" target="#foot_2">4</ref>We also deviate from the optimization by running for 2.4M steps and using an epsilon of 1e-8 for Adam <ref type="bibr" target="#b19">(Kingma and Ba, 2015)</ref>, which converges to a better set of model parameters. The pre-training was done on 32 Volta V100 GPUs, and took 15 days to complete.</p><p>Fine-tuning is implemented based on Hugging-Face's codebase. <ref type="foot" target="#foot_3">5</ref> Appendix A has more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare SpanBERT to three baselines:</p><p>Google BERT The pre-trained models released by Devlin et al. (2019). 6   Our BERT Our reimplementation of BERT with improved data preprocessing and optimization (Section 4.2).</p><p>Our BERT-1seq Our reimplementation of BERT trained on single full-length sequences without NSP (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We compare SpanBERT to the baselines per task, and draw conclusions based on the overall trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Per-Task Results</head><p>Extractive Question Answering Table <ref type="table" target="#tab_4">1</ref> shows the performance on both SQuAD 1.1 and 2.0. SpanBERT exceeds our BERT baseline by 2.0% (SQuAD 1.1) and 2.8% (SQuAD 2.0) F1. In SQuAD 1.1, this result accounts for over 27% error reduction, reaching 3.4% F1 above human performance.</p><p>Table <ref type="table">2</ref> demonstrates that this trend goes beyond SQuAD, and is consistent in every MRQA dataset. On average, we see a 2.9% F1 improvement from our reimplementation of BERT. Although some gains are coming from singlesequence training (+1.1%), most of the improvement stems from span masking and the span boundary objective (+1.8%), with particularly large gains on TriviaQA (+3.2%) and HotpotQA (+2.7%).</p><p>Coreference Resolution Table <ref type="table" target="#tab_5">3</ref> shows the performance on the OntoNotes coreference resolution benchmark. Our BERT reimplementation improves the Google BERT model by 1.2% on the average F1 metric and single-sequence training brings another 0.5% gain. Finally, SpanBERT significantly improves on top of that, achieving a new state of the art of 79.6% F1 (previous best result is 73.0%). <ref type="table" target="#tab_7">5</ref> shows the performance on TACRED. SpanBERT achieves close to the current state of the art <ref type="bibr" target="#b37">(Soares et al., 2019)</ref>, and exceeds our reimplementation of BERT by 3.3% F1. Most of this gain (+2.6%) stems from single-sequence training although the contribution of span masking and the span boundary objective is still significant (+0.7%), resulting largely from higher recall. In addition to a different input encoding scheme, the current state of the art <ref type="bibr" target="#b37">(Soares et al., 2019)</ref> pre-trains relation representations using distant supervision from entity-linked text. <ref type="table" target="#tab_6">4</ref> shows the performance on GLUE. For most tasks, the different models appear to perform similarly. Moving to singlesequence training without the NSP objective substantially improves CoLA, and yields smaller (but significant) improvements on MRPC and MNLI. The main gains from SpanBERT are in the SQuAD-based QNLI dataset (+1.3%) and in RTE (+6.9%), the latter accounting for most of the rise in SpanBERT's GLUE average. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Extraction Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLUE Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Trends</head><p>We compared our approach to three BERT baselines on 17 benchmarks, and found that Span-BERT outperforms BERT on almost every task. In 14 tasks, SpanBERT performed better than all baselines. In 2 tasks (MRPC and QQP), it performed on-par with single-sequence trained BERT, but still outperformed the other baselines.</p><p>In 1 task (SST-2), Google's BERT baseline performed better than SpanBERT by 0.4% accuracy.</p><p>When considering the magnitude of the gains, it appears that SpanBERT is especially better at extractive question answering. In SQuAD 1.1, for example, we observe a solid gain of 2.0% F1 even though the baseline is already well above human performance. On MRQA, Span-BERT improves between 2.0% (NaturalQA) and 4.6% (TriviaQA) F1 on top of our BERT baseline.</p><p>Finally, we observe that single-sequence training works considerably better than bi-sequence training with next sentence prediction (NSP) for a wide variety of tasks. This is surprising because BERT's ablations showed gains from the NSP objective <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. However, the ablation studies still involved bi-sequence data processing, i.e., the pre-training stage only controlled for the NSP objective while still sampling two half-length sequences. 7 We hypothesize that bi-sequence training, as it is implemented in BERT (see Section 2), impedes the model from learning longer-range features, and consequently hurts performance on many downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ablation Studies</head><p>We compare our random span masking scheme with linguistically-informed masking schemes, and find that masking random spans is a competitive and often better approach. We then study the impact of the span boundary objective (SBO), and contrast it with BERT's next sentence prediction (NSP) objective.<ref type="foot" target="#foot_5">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Masking Schemes</head><p>Previous work <ref type="bibr">(Sun et al., 2019)</ref> has shown improvements in downstream task performance by masking linguistically-informed spans during pre-training for Chinese. We compare our random span masking scheme with masking of linguistically-informed spans. Specifically, we train the following five baseline BERT models differing only in the way tokens are masked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subword Tokens</head><p>We sample random wordpiece tokens, as in the original BERT.</p><p>Whole Words We sample random words, and then mask all of the subword tokens in those words. The total number of masked subtokens is around 15%.</p><p>Named Entities At 50% of the time, we sample from named entities in the text, and sample random whole words for the other 50%. The total number of masked subtokens is 15%. Specifically, we run spaCy's named entity recognizer (Honnibal and Montani, 2017)<ref type="foot" target="#foot_6">9</ref> on the corpus and select all the non-numerical named entity mentions as candidates.</p><p>Noun Phrases Similar as Named Entities, we sample from noun phrases at 50% of the time. The noun phrases are extracted by running spaCy's constituency parser.</p><p>Random Spans We sample random spans from a geometric distribution, as in our SpanBERT (see Section 3.1).</p><p>Table <ref type="table" target="#tab_8">6</ref> shows how different pre-training masking schemes affect performance on a selection of tasks. With the exception of coreference resolution, masking random spans is preferable to other strategies. Although linguistic masking schemes (named entities and noun phrases) are often competitive with random spans, their performance is not consistent; for instance, masking noun phrases achieves parity with random spans on NewsQA, but underperforms on TriviaQA (-1.1% F1).</p><p>On coreference resolution, we see that masking random subword tokens is preferable to any form of span masking. Nevertheless, we shall see in the following experiment that combining random span masking with the span boundary objective can significantly improve upon this result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Auxiliary Objectives</head><p>In Section 4.3, we saw that bi-sequence training with the next sentence prediction (NSP) objective can hurt performance on downstream tasks, when compared to single-sequence training. We test whether this holds true for models pre-trained with span masking, and also evaluate the effect of replacing the NSP objective with the sentence boundary objective (SBO).</p><p>Table <ref type="table" target="#tab_9">7</ref> confirms that single-sequence training typically improves performance. Adding SBO further improves performance, with a substantial gain on coreference resolution (+2.7% F1) over span masking alone. Unlike the NSP objective, SBO does not appear to have any adverse effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Pre-trained contextualized word representations that can be trained from unlabeled text <ref type="bibr">(Dai and Le, 2015;</ref><ref type="bibr" target="#b30">Melamud et al., 2016;</ref><ref type="bibr" target="#b32">Peters et al., 2018)</ref> have had immense impact on NLP lately, particularly as methods for initializing a large model before fine-tuning it for a specific task <ref type="bibr" target="#b16">(Howard and Ruder, 2018;</ref><ref type="bibr" target="#b34">Radford et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019)</ref>. Beyond differences in model hyperparameters and corpora, these methods mainly differ in their pre-training tasks and loss functions, with a considerable amount of contemporary literature proposing augmentations of BERT's masked language modeling (MLM) objective.</p><p>While previous and concurrent work has looked at masking <ref type="bibr">(Sun et al., 2019)</ref> or dropping <ref type="bibr" target="#b39">(Song et al., 2019;</ref><ref type="bibr">Chan et al., 2019)</ref> multiple words from the input -particularly as pretraining for language generation tasks -SpanBERT pretrains span representations <ref type="bibr" target="#b25">(Lee et al., 2016)</ref> ( <ref type="bibr">Sun et al., 2019)</ref> shows improvements on Chinese NLP tasks using phrase and named entity masking. MASS <ref type="bibr" target="#b39">(Song et al., 2019)</ref> focuses on language generation tasks, and adopts the encoderdecoder framework to reconstruct a sentence fragment given the remaining part of the sentence. We attempt to more explicitly model spans using the SBO objective, and show that (geometrically distributed) random span masking works as well, and sometimes better than, masking linguisticallycoherent spans. We evaluate on English benchmarks for question answering, relation extraction, and coreference resolution in addition to GLUE. A different ERNIE <ref type="bibr" target="#b50">(Zhang et al., 2019)</ref> focuses on integrating structured knowledge bases with contextualized representations with an eye on knowledge-driven tasks like entity typing and relation classification. UNILM <ref type="bibr" target="#b10">(Dong et al., 2019)</ref> uses multiple language modeling objectives -unidirectional (both left-to-right and right-to-left), bidirectional, and sequence-to-sequence prediction -to aid generation tasks like summarization and question generation. XLM <ref type="bibr" target="#b22">(Lample and Conneau, 2019)</ref> explores cross-lingual pre-training for multilingual tasks such as translation and crosslingual classification. Kermit <ref type="bibr">(Chan et al., 2019)</ref>, an insertion based approach, fills in missing tokens (instead of predicting masked ones) during pretraining; they show improvements on machine translation and zero-shot question answering.</p><p>Concurrent with our work, RoBERTa <ref type="bibr" target="#b28">(Liu et al., 2019b)</ref> presents a replication study of BERT pre-training that measures the impact of many key hyperparameters and training data size. Also con-current, XLNet <ref type="bibr" target="#b47">(Yang et al., 2019)</ref> combines an autoregressive loss and the Transformer-XL <ref type="bibr">(Dai et al., 2019)</ref> architecture with a more than an eight-fold increase in data to achieve current stateof-the-art results on multiple benchmarks. XL-Net also masks spans (of 1-5 tokens) during pretraining, but predicts them autoregressively. Our model focuses on incorporating span-based pretraining, and as a side effect, we present a stronger BERT baseline while controlling for the corpus, architecture, and the number of parameters.</p><p>Related to our SBO objective, pair2vec <ref type="bibr" target="#b17">(Joshi et al., 2019)</ref> encodes word-pair relations using a negative sampling-based multivariate objective during pre-training. Later, the word-pair representations are injected into the attention-layer of downstream tasks, and thus encode limited downstream context. Unlike pair2vec, our SBO objective yields "pair" (start and end tokens of spans) representations which more fully encode the context during both pre-training and finetuning, and are thus more appropriately viewed as span representations. <ref type="bibr" target="#b40">Stern et al. (2018)</ref> focus on improving language generation speed using a block-wise parallel decoding scheme; they make predictions for multiple time steps in parallel and then back off to the longest prefix validated by a scoring model. Also related are sentence representation methods <ref type="bibr" target="#b20">(Kiros et al., 2015;</ref><ref type="bibr" target="#b29">Logeswaran and Lee, 2018)</ref> which focus on predicting surrounding contexts from sentence embeddings.</p><p>We presented a new method for span-based pretraining which extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. Together, our pre-training process yields models that outperform all BERT baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>K J P p p B 0 6 m Y S Z G 7 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e T a i F g 9 4 C z h f k T H S o S C U b T S Y B B R n A R h 9 j Q f e s N q z a 2 7 C 5 B 1 4h W k B g V a w + r X Y B S z N O I K m a T G 9 D 0 3 Q T + j G g W T f F 4 Z p I Y n l E 3 p m P c t V T T i x s 8 W m e f k w i o j E s b a P o V k o f 7 e y G h k z C w K 7 G S e 0 a x 6 u f i f 1 0 8 x v P E z o Z I U u W L L Q 2 E q C c Y k L 4 C M h O Y M 5 c w S y r S w W Q m b U E 0 Z 2 p o q t g R v 9 c v r p H N V 9 9 y 6 d 9 + o N R t F H W U 4 g 3 O 4 B A + u o Q l 3 0 I I 2 M E j g G V 7 h z U m d F + f d + V i O l p x i 5 x T + w P n 8 A S Y q k b I = &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / A i Q R o 6 P e m d E R O W 9 8 6 e B W M M c x 9 k = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q U d F l w 4 7 K C f U A b y m R 6 0 w 6 d T M L M R C y h v + H G h S J u / R l 3 / o 2 T N g t t P T B w O O d e 7 p k T J I J r 4 7 r f z s b m 1 v b O b m m v v H 9 w e H R c O T n t 6 D h V D N s s F r H q B V S j 4 B L b h h u B v U Q h j Q K B 3 W B 6 m / v d R 1 S a x / L B z B L 0 I z q W P O S M G i s N B h E 1 k y D M n u b D + r B S d W v u A m S d e A W p Q o H W s P I 1 G M U s j V A a J q j W f c 9 N j J 9 R Z T g T O C 8 P U o 0 J Z V M 6 x r 6 l k k a o / W y R e U 4 u r T I i Y a z s k 4 Y s 1 N 8 b G Y 2 0 n k W B n c w z 6 l U v F / / z + q k J b / y M y y Q 1 K N n y U J g K Y m K S F 0 B G X C E z Y m Y J Z Y r b r I R N q K L M 2 J r K t g R v 9 c v r p F O v e W 7 N u 2 9 U m 4 2 i j h K c w w V c g Q f X 0 I Q 7 a E E b G C T w D K / w 5 q T O i / P u f C x H N 5 x i 5 w z + w P n 8 A S e u k b M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / A i Q R o 6 P e m d E R O W 9 8 6 e B W M M c x 9 k = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q U d F l w 4 7 K C f U A b y m R 6 0 w 6 d T M L M R C y h v + H G h S J u / R l 3 / o 2 T N g t t P T B w O O d e 7 p k T J I J r 4 7 r f z s b m 1 v b O b m m v v H 9 w e H R c O T n t 6 D h V D N s s F r H q B V S j 4 B L b h h u B v U Q h j Q K B 3 W B 6 m / v d R 1 S a x / L B z B L 0 I z q W P O S M G i s N B h E 1 k y D M n u b D + r B S d W v u A m S d e A W p Q o H W s P I 1 G M U s j V A a J q j W f c 9 N j J 9 R Z T g T O C 8 P U o 0 J Z V M 6 x r 6 l k k a o / W y R e U 4 u r T I i Y a z s k 4 Y s 1 N 8 b G Y 2 0 n k W B n c w z 6 l U v F / / z + q k J b / y M y y Q 1 K N n y U J g K Y m K S F 0 B G X C E z Y m Y J Z Y r b r I R N q K L M 2 J r K t g R v 9 c v r p F O v e W 7 N u 2 9 U m 4 2 i j h K c w w V c g Q f X 0 I Q 7 a E E b G C T w D K / w 5 q T O i / P u f C x H N 5 x i 5 w z + w P n 8 A S e u k b M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / A i Q R o 6 P e m d E R O W 9 8 6 e B W M M c x 9 k = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q U d F l w 4 7 K C f U A b y m R 6 0 w 6 d T M L M R C y h v + H G h S J u / R l 3 / o 2 T N g t t P T B w O O d e 7 p k T J I J r 4 7 r f z s b m 1 v b O b m m v v H 9 w e H R c O T n t 6 D h V D N s s F r H q B V S j 4 B L b h h u B v U Q h j Q K B 3 W B 6 m / v d R 1 S a x / L B z B L 0 I z q W P O S M G i s N B h E 1 k y D M n u b D + r B S d W v u A m S d e A W p Q o H W s P I 1 G M U s j V A a J q j W f c 9 N j J 9 R Z T g T O C 8 P U o 0 J Z V M 6 x r 6 l k k a o / W y R e U 4 u r T I i Y a z s k 4 Y s 1 N 8 b G Y 2 0 n k W B n c w z 6 l U v F / / z + q k J b / y M y y Q 1 K N n y U J g K Y m K S F 0 B G X C E z Y m Y J Z Y r b r I R N q K L M 2 J r K t g R v 9 c v r p F O v e W 7 N u 2 9 U m 4 2 i j h K c w w V c g Q f X 0 I Q 7 a E E b G C T w D K / w 5 q T O i / P u f C xH N 5 x i 5 w z + w P n 8 A S e u k b M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / A i Q R o 6 P e m d E R O W 9 8 6 e B W M M c x 9 k = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q U d F l w 4 7 K C f U A b y m R 6 0 w 6 d T M L M R C y h v + H G h S J u / R l 3 / o 2 T N g t t P T B w O O d e 7 p k T J I J r 4 7 r f z s b m 1 v b O b m m v v H 9 w e H R c O T n t 6 D h V D N s s F r H q B V S j 4 B L b h h u B v U Q h j Q K B 3 W B 6 m / v d R 1 S a x / L B z B L 0 I z q W P O S M G i s N B h E 1 k y D M n u b D + r B S d W v u A m S d e A W p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>3 n 2 6 p s b e / s 7 l X 3 a w e H R 8 d 1+ + S 0 r + J U E t o j M Y / l M M C K c i Z o T z P N 6 T C R F E c B p 4 N g f l v 4 g 0 c q F Y v F g 1 4 k 1 I v w V L C Q E a y N 5 N v 1 c Y T 1 L A i z J P cz t 5 X 7 d s N p O k u g T e K W p A E l u r 7 9 N Z 7 E J I 2 o 0 I R j p U a u k 2 g v w 1 I z w m l e G 6 e K J p j M 8 Z S O D B U 4 o s r L l s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>k 0 0 w Y z k y G 5 I 5 Z h 9 n 6 f O 3 / C j T 9 g p i 1 Y H x c C 5 5 5 z b h 4 n f i y 4 B s d 5 t + y J y a n p m d m 5 w v z C 4 t J y c W W 1 o W W i K K t T K a R 6 9 I l m g k e s D h w E e 4 w V I 6 E v 2 I P / d J 7 r D 8 9 M a S 6 j e + j H r B 2 S b s Q D T g k Y y i u + t k I C P U p E e p n t t o C 9 Q B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>k 0 0 w Y z k y G 5 I 5 Z h 9 n 6 f O 3 / C j T 9 g p i 1 Y H x c C 5 5 5 z b h 4 n f i y 4 B s d 5 t + y J y a n p m d m 5 w v z C 4 t J y c W W 1 o W W i K K t T K a R 6 9 I l m g k e s D h w E e 4 w V I 6 E v 2 I P / d J 7 r D 8 9 M a S 6 j e + j H r B 2 S b s Q D T g k Y y i u + t k I C P U p E e p n t t o C 9 Q B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>k 0 0 w Y z k y G 5 I 5 Z h 9 n 6 f O 3 / C j T 9 g p i 1 Y H x c C 5 5 5 z b h 4 n f i y 4 B s d 5 t + y J y a n p m d m 5 w v z C 4 t J y c W W 1 o W W i K K t T K a R 6 9 I l m g k e s D h w E e 4 w V I 6 E v 2 I P / d J 7 r D 8 9 M a S 6 j e + j H r B 2 S b s Q D T g k Y y i u + t k I C P U p E e p n t t o C 9 Q B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>k 0 0 w Y z k y G 5 I 5 Z h 9 n 6 f O 3 / C j T 9 g p i 1 Y H x c C 5 5 5 z b h 4 n f i y 4 B s d 5 t + y J y a n p m d m 5 w v z C 4 t J y c W W 1 o W W i K K t T K a R 6 9 I l m g k e s D h w E e 4 w V I 6 E v 2 I P / d J 7 r D 8 9 M a S 6 j e + j H r B 2 S b s Q D T g k Y y i u + t k I C P U p E e p n t t o C 9 Q B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An illustration of SpanBERT training. In this example, the span an American football game is masked. The span boundary objective then uses the boundary tokens was and to to predict each token in the masked span.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: We sample random span lengths from a geometric distribution  Geo(p = 0.2) clipped at max = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>[CLS]for both training and testing.Coreference Resolution Coreference resolution is the task of clustering mentions in text which refer to the same real-world entities. We evaluate on the CoNLL-2012 shared task<ref type="bibr" target="#b33">(Pradhan et al., 2012)</ref> for document-level coreference resolution. The model is an augmentation ofLee  et al.'s (2018)  higher-order coreference model, which replaces the original LSTM-based encoders with BERT's pre-trained transformer encoders.Relation Extraction TACRED<ref type="bibr" target="#b49">(Zhang et al., 2017)</ref> is a challenging relation extraction dataset. Given one sentence and two spans within it -subject and object -the task is to predict the relation between the spans from 42 pre-defined relation types, including no_relation. We follow the entity masking schema from<ref type="bibr" target="#b49">Zhang et al. (2017)</ref> and replace the subject and object entities by their NER tags such as "[CLS][SUBJ-PER] was born in [OBJ-LOC] , Michigan, . . . ", and finally add a linear classifier on top of the [CLS]token to predict the relation type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Test results on SQuAD 1.1 and SQuAD 2.0.</figDesc><table><row><cell></cell><cell cols="2">SQuAD 1.1</cell><cell cols="2">SQuAD 2.0</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>Human Perf.</cell><cell cols="2">82.3 91.2</cell><cell cols="2">86.8 89.4</cell></row><row><cell>Google BERT</cell><cell cols="2">84.3 91.3</cell><cell cols="2">80.0 83.3</cell></row><row><cell>Our BERT</cell><cell cols="2">86.5 92.6</cell><cell cols="2">82.8 85.9</cell></row><row><cell>Our BERT-1seq</cell><cell cols="2">87.5 93.3</cell><cell cols="2">83.8 86.6</cell></row><row><cell>SpanBERT</cell><cell cols="2">88.8 94.6</cell><cell cols="2">85.7 88.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance on the OntoNotes coreference resolution benchmark. The main evaluation is the average F1 of three metrics -MUC, B 3 , and CEAF 4 on the test set.</figDesc><table><row><cell></cell><cell></cell><cell cols="9">NewsQA TriviaQA SearchQA HotpotQA NaturalQA (Avg)</cell></row><row><cell cols="2">Google BERT</cell><cell>68.8</cell><cell></cell><cell>77.5</cell><cell cols="2">81.7</cell><cell></cell><cell>78.3</cell><cell cols="2">79.9</cell><cell>77.3</cell></row><row><cell>Our BERT</cell><cell></cell><cell>71.0</cell><cell></cell><cell>79.0</cell><cell cols="2">81.8</cell><cell></cell><cell>80.5</cell><cell cols="2">80.5</cell><cell>78.6</cell></row><row><cell cols="2">Our BERT-1seq</cell><cell>71.9</cell><cell></cell><cell>80.4</cell><cell cols="2">84.0</cell><cell></cell><cell>80.3</cell><cell cols="2">81.8</cell><cell>79.7</cell></row><row><cell cols="2">SpanBERT</cell><cell>73.6</cell><cell></cell><cell>83.6</cell><cell cols="2">84.8</cell><cell></cell><cell>83.0</cell><cell cols="2">82.5</cell><cell>81.5</cell></row><row><cell cols="11">Table 2: Performance (F1) on the five MRQA extractive question answering tasks.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MUC</cell><cell></cell><cell></cell><cell>B 3</cell><cell></cell><cell cols="2">CEAF 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Avg. F1</cell></row><row><cell cols="11">Prev. SotA: (Lee et al., 2018) 81.4 79.5 80.4 72.2 69.5 70.8 68.2 67.1 67.6</cell><cell>73.0</cell></row><row><cell>Google BERT</cell><cell></cell><cell></cell><cell cols="8">84.9 82.5 83.7 76.7 74.2 75.4 74.6 70.1 72.3</cell><cell>77.1</cell></row><row><cell>Our BERT</cell><cell></cell><cell></cell><cell cols="8">85.1 83.5 84.3 77.3 75.5 76.4 75.0 71.9 73.9</cell><cell>78.3</cell></row><row><cell>Our BERT-1seq</cell><cell></cell><cell></cell><cell cols="8">85.5 84.1 84.8 77.8 76.7 77.2 75.3 73.5 74.4</cell><cell>78.8</cell></row><row><cell>SpanBERT</cell><cell></cell><cell></cell><cell cols="8">85.8 84.8 85.3 78.3 77.9 78.1 76.4 74.2 75.3</cell><cell>79.6</cell></row><row><cell cols="3">CoLA SST-2</cell><cell cols="2">MRPC</cell><cell>STS-B</cell><cell></cell><cell>QQP</cell><cell></cell><cell>MNLI</cell><cell>QNLI RTE (Avg)</cell></row><row><cell>Google BERT</cell><cell>59.3</cell><cell>95.2</cell><cell cols="7">88.5/84.3 86.4/88.0 71.2/89.0 86.1/85.7</cell><cell>93.0</cell><cell>71.1</cell><cell>80.4</cell></row><row><cell>Our BERT</cell><cell>58.6</cell><cell>93.9</cell><cell cols="7">90.1/86.6 88.4/89.1 71.8/89.3 87.2/86.6</cell><cell>93.0</cell><cell>74.7</cell><cell>81.1</cell></row><row><cell>Our BERT-1seq</cell><cell>63.5</cell><cell>94.8</cell><cell cols="7">91.2/87.8 89.0/88.4 72.1/89.5 88.0/87.4</cell><cell>93.0</cell><cell>72.1</cell><cell>81.7</cell></row><row><cell>SpanBERT</cell><cell>64.3</cell><cell>94.8</cell><cell cols="7">90.9/87.9 89.9/89.1 71.9/89.5 88.1/87.7</cell><cell>94.3</cell><cell>79.0</cell><cell>82.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Test set performance metrics on GLUE tasks. MRPC: F1/accuracy, STS-B: Pearson/Spearmanr correlation, QQP: F1/accuracy, MNLI: matched/mistached accuracies. WNLI (not shown) is always set to majority class (65.1% accuracy) and included in the average.</figDesc><table><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Curr. SotA: (Soares et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>71.5</cell></row><row><cell>Google BERT</cell><cell cols="3">69.1 63.9 66.4</cell></row><row><cell>Our BERT</cell><cell cols="3">67.8 67.2 67.5</cell></row><row><cell>Our BERT-1seq</cell><cell cols="3">72.4 67.9 70.1</cell></row><row><cell>SpanBERT</cell><cell cols="3">70.8 70.9 70.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Test set performance on the TACRED relation extraction benchmark.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>, which are widely used for question answering, coreference resolution and a variety of other tasks. ERNIE SQuAD 2.0 NewsQA TriviaQA Coreference MNLI-m QNLI The effect of replacing BERT's original masking scheme (Subword Tokens) with different masking schemes. Results are F1 scores for QA tasks and accuracy for MNLI and QNLI on the development sets. All the models are based on bi-sequence training with NSP.</figDesc><table><row><cell>Subword Tokens</cell><cell>83.8</cell><cell>72.0</cell><cell>76.3</cell><cell>77.7</cell><cell>86.7</cell><cell>92.5</cell></row><row><cell>Whole Words</cell><cell>84.3</cell><cell>72.8</cell><cell>77.1</cell><cell>76.6</cell><cell>86.3</cell><cell>92.8</cell></row><row><cell>Named Entities</cell><cell>84.8</cell><cell>72.7</cell><cell>78.7</cell><cell>75.6</cell><cell>86.0</cell><cell>93.1</cell></row><row><cell>Noun Phrases</cell><cell>85.0</cell><cell>73.0</cell><cell>77.7</cell><cell>76.7</cell><cell>86.5</cell><cell>93.2</cell></row><row><cell>Random Spans</cell><cell>85.4</cell><cell>73.0</cell><cell>78.8</cell><cell>76.4</cell><cell>87.0</cell><cell>93.3</cell></row><row><cell></cell><cell cols="7">SQuAD 2.0 NewsQA TriviaQA Coreference MNLI-m QNLI</cell></row><row><cell cols="2">Span Masking (2seq) + NSP</cell><cell>85.4</cell><cell>73.0</cell><cell>78.8</cell><cell>76.4</cell><cell>87.0</cell><cell>93.3</cell></row><row><cell>Span Masking (1seq)</cell><cell></cell><cell>86.7</cell><cell>73.4</cell><cell>80.0</cell><cell>76.3</cell><cell>87.3</cell><cell>93.8</cell></row><row><cell cols="2">Span Masking (1seq) + SBO</cell><cell>86.8</cell><cell>74.1</cell><cell>80.3</cell><cell>79.0</cell><cell>87.6</cell><cell>93.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>The effects of different auxiliary objectives, given MLM over random spans as the primary objective.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use the modified MRQA version of these datasets. See more details in Section 4.1.an encoder that produces a contextualized vector representation for each token: x 1 , . . . , x n = enc(x 1 , . . . , x n ). Since the encoder is implemented via a deep transformer, it uses positional embeddings p 1 , . . . , p n to mark the absolute position of each token in the sequence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We refer the reader to RoBERTa<ref type="bibr" target="#b28">(Liu et al., 2019b)</ref> for further discussion on these modifications and their effects.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">https://github.com/huggingface/pytorch-pretrained-BERT.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">6 https://github.com/google-research/bert.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">To save time and resources, we use the checkpoints at 1.2M steps for all the ablation experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6">https://spacy.io/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Pranav Rajpurkar and Robin Jia for patiently helping us evaluate Span-BERT on SQuAD. We thank our colleagues at Facebook AI Research and the University of Washington for their insightful feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A Fine-tuning Hyperparameters</p><p>We applied the following fine-tuning hyperparameters to all methods, including the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extractive</head><p>Question Answering For all the question tasks, we use max_seq_length = 512 and a sliding window of size 128 if the lengths are longer than 512. We choose learning rates from {5e-6, 1e-5, 2e-5, 3e-5, 5e-5} and batch sizes from {16, 32} and fine-tune 4 epochs for all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coreference Resolution</head><p>We divide the documents into multiple chunks of lengths up to and encode each chunk independently. We choose max_seq_length from {128, 256, 384, 512}, BERT learning rates from {1e-5, 2e-5}, task-specific learning rates from {1e-4, 2e-4, 3e-4} and fine-tune 20 epochs for all the datasets. We use batch size = 1 (one document) for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLUE &amp; Relation Extraction</head><p>We use max_seq_length = 128 and choose learning rates from {5e-6, 1e-5, 2e-5, 3e-5, 5e-5} and batch sizes from {16, 32} and fine-tuning 10 epochs for all the datasets. The only exception is CoLA, where we used 4 epochs (following Devlin et al. <ref type="bibr">(2019)</ref>), because 10 epochs lead to severe overfitting.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The second PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second PASCAL challenges workshop on recognising textual entailment</title>
				<meeting>the second PASCAL challenges workshop on recognising textual entailment</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Evaluation (SemEval)</title>
				<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">KERMIT: Generative insertionbased modeling for sequences</title>
		<idno type="arXiv">arXiv:1906.01604</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<editor>
			<persName><surname>Springer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><surname>Le</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2005">2005. 2015</date>
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
	<note>Machine Learning Challenges Workshop</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>American Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Paraphrasing</title>
				<meeting>the International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03197</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SearchQA: A new Q&amp;A dataset augmented with context from a search engine</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Ugur Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
				<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jointly predicting predicates and arguments in neural semantic role labeling</title>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="364" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-ofdistribution examples in neural networks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<title level="m">Universal language model fine-tuning for text classification</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">pair2vec: Compositional word-pair embeddings for crosssentence inference</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3597" to="3608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Transactions of the Association of Computational Linguistics (TACL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m">Cross-lingual language model pretraining</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarse-to-fine inference</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
	</analytic>
	<monogr>
		<title level="m">Dipanjan Das, and Jonathan Berant</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note>RoBERTa: A robustly optimized BERT pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>arxiv:1803.02893</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning generic context embedding with bidirectional LSTM</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Ope-nAI</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Time Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">Arthur</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Blockwise parallel decoding for deep autoregressive models</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danxiang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">ERNIE: Enhanced representation through knowledge integration</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">NewsQA: A machine comprehension dataset</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Workshop on Representation Learning for NLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amapreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Nikita Nangia, and Samuel Bowman</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
