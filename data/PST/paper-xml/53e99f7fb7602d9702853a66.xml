<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey of Monte Carlo Tree Search Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Cameron</forename><surname>Browne</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Edward</forename><surname>Powley</surname></persName>
							<email>e.powley@bradford.ac.uk</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Daniel</forename><surname>Whitehouse</surname></persName>
							<email>d.whitehouse1@bradford.ac.uk</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Simon</forename><surname>Lucas</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Peter</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
							<email>p.i.cowling@bradford.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Philipp</forename><surname>Rohlfshagen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Tavener</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Diego</forename><surname>Perez</surname></persName>
							<email>dperez@essex.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Spyridon</forename><surname>Samothrakis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Colton</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Com-puting</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computing, Informatics and Media</orgName>
								<orgName type="institution">University of Bradford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey of Monte Carlo Tree Search Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TCIAIG.2012.2186810</idno>
					<note type="submission">received October 22, 2011; revised January 12, 2012; accepted January 30, 2012. Digital Object Identifier 10.1109/TCIAIG.2012.2186810</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Monte Carlo Tree Search (MCTS)</term>
					<term>Upper Confidence Bounds (UCB)</term>
					<term>Upper Confidence Bounds for Trees (UCT)</term>
					<term>Bandit-based methods</term>
					<term>Artificial Intelligence (AI)</term>
					<term>Game search</term>
					<term>Computer Go</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monte Carlo Tree Search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarise the results from the key game and non-game domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>M ONTE Carlo Tree Search (MCTS) is a method for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results. It has already had a profound impact on Artificial Intelligence (AI) approaches for domains that can be represented as trees of sequential decisions, particularly games and planning problems.</p><p>In the five years since MCTS was first described, it has become the focus of much AI research. Spurred on by some prolific achievements in the challenging task of computer Go, researchers are now in the process of attaining a better understanding of when and why MCTS succeeds and fails, and of extending and refining the basic algorithm. These developments are greatly increasing the range of games and other decision applications for which MCTS is a tool of choice, and pushing its performance to ever higher levels. MCTS has many attractions: it is a statistical anytime algorithm for which more computing power generally leads to better performance. It can be used with little or no domain knowledge, and has succeeded on difficult problems where other techniques have failed. Here we survey the range of published work on MCTS, to provide the reader Fig. <ref type="figure">1</ref>. The basic MCTS process <ref type="bibr" target="#b16">[17]</ref>.</p><p>with the tools to solve new problems using MCTS and to investigate this powerful approach to searching trees and directed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Overview</head><p>The basic MCTS process is conceptually very simple, as shown in Figure <ref type="figure">1</ref> (from <ref type="bibr" target="#b16">[17]</ref>). A tree 1 is built in an incremental and asymmetric manner. For each iteration of the algorithm, a tree policy is used to find the most urgent node of the current tree. The tree policy attempts to balance considerations of exploration (look in areas that have not been well sampled yet) and exploitation (look in areas which appear to be promising). A simulation 2 is then run from the selected node and the search tree updated according to the result. This involves the addition of a child node corresponding to the action taken from the selected node, and an update of the statistics of its ancestors. Moves are made during this simulation according to some default policy, which in the simplest case is to make uniform random moves. A great benefit of MCTS is that the values of intermediate states do not have to be evaluated, as for depth-limited minimax search, which greatly reduces the amount of domain knowledge required. Only the value of the terminal state at the end of each simulation is required.</p><p>While the basic algorithm (3.1) has proved effective for a wide range of problems, the full benefit of MCTS is typically not realised until this basic algorithm is adapted to suit the domain at hand. The thrust of a good deal of MCTS research is to determine those variations and enhancements best suited to each given situation, and to understand how enhancements from one domain may be used more widely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Importance</head><p>Monte Carlo methods have a long history within numerical algorithms and have also had significant success in various AI game playing algorithms, particularly imperfect information games such as Scrabble and Bridge. However, it is really the success in computer Go, through the recursive application of Monte Carlo methods during the tree-building process, which has been responsible for much of the interest in MCTS. This is because Go is one of the few classic games for which human players are so far ahead of computer players. MCTS has had a dramatic effect on narrowing this gap, and is now competitive with the very best human players on small boards, though MCTS falls far short of their level on the standard 19×19 board. Go is a hard game for computers to play: it has a high branching factor, a deep tree, and lacks any known reliable heuristic value function for non-terminal board positions.</p><p>Over the last few years, MCTS has also achieved great success with many specific games, general games, and complex real-world planning, optimisation and control problems, and looks set to become an important part of the AI researcher's toolkit. It can provide an agent with some decision making capacity with very little domainspecific knowledge, and its selective sampling approach may provide insights into how other algorithms could be hybridised and potentially improved. Over the next decade we expect to see MCTS become a greater focus for increasing numbers of researchers, and to see it adopted as part of the solution to a great many problems in a variety of domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Aim</head><p>This paper is a comprehensive survey of known MCTS research at the time of writing (October 2011). This includes the underlying mathematics behind MCTS, the algorithm itself, its variations and enhancements, and its performance in a variety of domains. We attempt to convey the depth and breadth of MCTS research and its exciting potential for future development, and bring together common themes that have emerged.</p><p>This paper supplements the previous major survey in the field <ref type="bibr" target="#b171">[170]</ref> by looking beyond MCTS for computer Go to the full range of domains to which it has now been applied. Hence we aim to improve the reader's understanding of how MCTS can be applied to new research questions and problem domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Structure</head><p>The remainder of this paper is organised as follows.</p><p>In Section 2, we present central concepts of AI and games, introducing notation and terminology that set the stage for MCTS. In Section 3, the MCTS algorithm and its key components are described in detail. Section 4 summarises the main variations that have been proposed. Section 5 considers enhancements to the tree policy, used to navigate and construct the search tree. Section 6 considers other enhancements, particularly to simulation and backpropagation steps. Section 7 surveys the key applications to which MCTS has been applied, both in games and in other domains. In Section 8, we summarise the paper to give a snapshot of the state of the art in MCTS research, the strengths and weaknesses of the approach, and open questions for future research. The paper concludes with two tables that summarise the many variations and enhancements of MCTS and the domains to which they have been applied.</p><p>The References section contains a list of known MCTSrelated publications, including book chapters, journal papers, conference and workshop proceedings, technical reports and theses. We do not guarantee that all cited works have been peer-reviewed or professionally recognised, but have erred on the side of inclusion so that the coverage of material is as comprehensive as possible. We identify almost 250 publications from the last five years of MCTS research. 3  We present a brief </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>This section outlines the background theory that led to the development of MCTS techniques. This includes decision theory, game theory, and Monte Carlo and bandit-based methods. We emphasise the importance of game theory, as this is the domain to which MCTS is most applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Decision Theory</head><p>Decision theory combines probability theory with utility theory to provide a formal and complete framework for decisions made under uncertainty <ref type="bibr" target="#b179">[178,</ref><ref type="bibr">Ch.13</ref>]. <ref type="foot" target="#foot_0">4</ref> Problems whose utility is defined by sequences of decisions were pursued in operations research and the study of Markov decision processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Markov Decision Processes (MDPs)</head><p>A Markov decision process (MDP) models sequential decision problems in fully observable environments using four components <ref type="bibr" target="#b179">[178,</ref><ref type="bibr">Ch.17</ref>]:</p><p>• S: A set of states, with s 0 being the initial state.</p><p>• A: A set of actions.</p><p>• T (s, a, s ): A transition model that determines the probability of reaching state s if action a is applied to state s. • R(s): A reward function. Overall decisions are modelled as sequences of (state, action) pairs, in which each next state s is decided by a probability distribution which depends on the current state s and the chosen action a. A policy is a mapping from states to actions, specifying which action will be chosen from each state in S. The aim is to find the policy π that yields the highest expected reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Partially Observable Markov Decision Processes</head><p>If each state is not fully observable, then a Partially Observable Markov Decision Process (POMDP) model must be used instead. This is a more complex formulation and requires the addition of:</p><p>• O(s, o): An observation model that specifies the probability of perceiving observation o in state s. The many MDP and POMDP approaches are beyond the scope of this review, but in all cases the optimal policy π is deterministic, in that each state is mapped to a single action rather than a probability distribution over actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Game Theory</head><p>Game theory extends decision theory to situations in which multiple agents interact. A game can be defined as a set of established rules that allows the interaction of one <ref type="foot" target="#foot_1">5</ref> or more players to produce specified outcomes.</p><p>A game may be described by the following components:</p><p>• S: The set of states, where s 0 is the initial state.</p><p>• S T ⊆ S: The set of terminal states.</p><p>• n ∈ N: The number of players.</p><p>• A: The set of actions.</p><p>• f : S × A → S: The state transition function.</p><p>• R : S → R k : The utility function.</p><p>• ρ : S → (0, 1, . . . , n): Player about to act in each state. Each game starts in state s 0 and progresses over time t = 1, 2, . . . until some terminal state is reached. Each player k i takes an action (i.e. makes a move) that leads, via f , to the next state s t+1 . Each player receives a reward (defined by the utility function R) that assigns a value to their performance. These values may be arbitrary (e.g. positive values for numbers of points accumulated or monetary gains, negative values for costs incurred) but in many games it is typical to assign nonterminal states a reward of 0 and terminal states a value of +1, 0 or −1 (or +1, + 1  2 and 0) for a win, draw or loss, respectively. These are the game-theoretic values of a terminal state.</p><p>Each player's strategy (policy) determines the probability of selecting action a given state s. The combination of players' strategies forms a Nash equilibrium if no player can benefit by unilaterally switching strategies <ref type="bibr" target="#b179">[178,</ref><ref type="bibr">Ch.17]</ref>. Such an equilibrium always exists, but computing it for real games is generally intractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Combinatorial Games</head><p>Games are classified by the following properties:</p><p>• Zero-sum: Whether the reward to all players sums to zero (in the two-player case, whether players are in strict competition with each other). • Information: Whether the state of the game is fully or partially observable to the players. • Determinism: Whether chance factors play a part (also known as completeness, i.e. uncertainty over rewards). • Sequential: Whether actions are applied sequentially or simultaneously. • Discrete: Whether actions are discrete or applied in real-time. Games with two players that are zero-sum, perfect information, deterministic, discrete and sequential are described as combinatorial games. These include games such as Go, Chess and Tic Tac Toe, as well as many others. Solitaire puzzles may also be described as combinatorial games played between the puzzle designer and the puzzle solver, although games with more than two players are not considered combinatorial due to the social aspect of coalitions that may arise during play. Combinatorial games make excellent test beds for AI experiments as they are controlled environments defined by simple rules, but which typically exhibit deep and complex play that can present significant research challenges, as amply demonstrated by Go.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">AI in Real Games</head><p>Real-world games typically involve a delayed reward structure in which only those rewards achieved in the terminal states of the game accurately describe how well each player is performing. Games are therefore typically modelled as trees of decisions as follows:</p><p>• Minimax attempts to minimise the opponent's maximum reward at each state, and is the traditional search approach for two-player combinatorial games. The search is typically stopped prematurely and a value function used to estimate the outcome of the game, and the α-β heuristic is typically used to prune the tree. The max n algorithm is the analogue of minimax for non-zero-sum games and/or games with more than two players. • Expectimax generalises minimax to stochastic games in which the transitions from state to state are probabilistic. The value of a chance node is the sum of its children weighted by their probabilities, otherwise the search is identical to max n . Pruning strategies are harder due to the effect of chance nodes. • Miximax is similar to single-player expectimax and is used primarily in games of imperfect information. It uses a predefined opponent strategy to treat opponent decision nodes as chance nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Monte Carlo Methods</head><p>Monte Carlo methods have their roots in statistical physics where they have been used to obtain approximations to intractable integrals, and have since been used in a wide array of domains including games research.</p><p>Abramson <ref type="bibr" target="#b0">[1]</ref> demonstrated that this sampling might be useful to approximate the game-theoretic value of a move. Adopting the notation used by Gelly and Silver <ref type="bibr" target="#b93">[94]</ref>, the Q-value of an action is simply the expected reward of that action:</p><formula xml:id="formula_0">Q(s, a) = 1 N (s, a) N (s) i=1 I i (s, a)z i</formula><p>where N (s, a) is the number of times action a has been selected from state s, N (s) is the number of times a game has been played out through state s, z i is the result of the ith simulation played out from s, and I i (s, a) is 1 if action a was selected from state s on the ith play-out from state s or 0 otherwise.</p><p>Monte Carlo approaches in which the actions of a given state are uniformly sampled are described as flat Monte Carlo. The power of flat Monte Carlo is demonstrated by Ginsberg <ref type="bibr" target="#b96">[97]</ref> and Sheppard <ref type="bibr" target="#b200">[199]</ref>, who use such approaches to achieve world champion level play in Bridge and Scrabble respectively. However it is simple to construct degenerate cases in which flat Monte Carlo fails, as it does not allow for an opponent model <ref type="bibr" target="#b28">[29]</ref>.</p><p>Alth öfer describes the laziness of flat Monte Carlo in non-tight situations <ref type="bibr" target="#b4">[5]</ref>. He also describes unexpected basin behaviour that can occur <ref type="bibr" target="#b5">[6]</ref>, which might be used to help find the optimal UCT search parameters for a given problem.</p><p>It is possible to improve the reliability of gametheoretic estimates by biasing action selection based on past experience. Using the estimates gathered so far, it is sensible to bias move selection towards those moves that have a higher intermediate reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Bandit-Based Methods</head><p>Bandit problems are a well-known class of sequential decision problems, in which one needs to choose amongst K actions (e.g. the K arms of a multi-armed bandit slot machine) in order to maximise the cumulative reward by consistently taking the optimal action. The choice of action is difficult as the underlying reward distributions are unknown, and potential rewards must be estimated based on past observations. This leads to the exploitationexploration dilemma: one needs to balance the exploitation of the action currently believed to be optimal with the exploration of other actions that currently appear suboptimal but may turn out to be superior in the long run.</p><p>A K-armed bandit is defined by random variables X i,n for 1 ≤ i ≤ K and n ≥ 1, where i indicates the arm of the bandit <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b119">[119]</ref>, <ref type="bibr" target="#b120">[120]</ref>. Successive plays of bandit i yield X i,1 , X i,2 , . . . which are independently and identically distributed according to an unknown law with unknown expectation µ i . The K-armed bandit problem may be approached using a policy that determines which bandit to play, based on past rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Regret</head><p>The policy should aim to minimise the player's regret, which is defined after n plays as:</p><formula xml:id="formula_1">R N = µ n − µ j K j=1 E[T j (n)]</formula><p>where µ is the best possible expected reward and E[T j (n)] denotes the expected number of plays for arm j in the first n trials. In other words, the regret is the expected loss due to not playing the best bandit. It is important to highlight the necessity of attaching non-zero probabilities to all arms at all times, in order to ensure that the optimal arm is not missed due to temporarily promising rewards from a sub-optimal arm. It is thus important to place an upper confidence bound on the rewards observed so far that ensures this.</p><p>In a seminal paper, Lai and Robbins <ref type="bibr" target="#b124">[124]</ref> showed there exists no policy with a regret that grows slower than O(ln n) for a large class of reward distributions. A policy is subsequently deemed to resolve the exploration-exploitation problem if the growth of regret is within a constant factor of this rate. The policies proposed by Lai and Robbins made use of upper confidence indices, which allow the policy to estimate the expected reward of a specific bandit once its index is computed. However, these indices were difficult to compute and Agrawal <ref type="bibr" target="#b1">[2]</ref> introduced policies where the index could be expressed as a simple function of the total reward obtained so far by the bandit. Auer et al. <ref type="bibr" target="#b12">[13]</ref> subsequently proposed a variant of Agrawal's index-based policy that has a finite-time regret logarithmically bound for arbitrary reward distributions with bounded support. One of these variants, UCB1, is introduced next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Upper Confidence Bounds (UCB)</head><p>For bandit problems, it is useful to know the upper confidence bound (UCB) that any given arm will be optimal. The simplest UCB policy proposed by Auer et al. <ref type="bibr" target="#b12">[13]</ref> is called UCB1, which has an expected logarithmic growth of regret uniformly over n (not just asymptotically) without any prior knowledge regarding the reward distributions (which have to have their support in [0, 1]). The policy dictates to play arm j that maximises:</p><formula xml:id="formula_2">UCB1 = X j + 2 ln n n j</formula><p>where X j is the average reward from arm j, n j is the number of times arm j was played and n is the overall number of plays so far. The reward term X j encourages the exploitation of higher-reward choices, while the right hand term encourages the exploration of lessvisited choices. The exploration term is related to the size of the one-sided confidence interval for the average reward within which the true expected reward falls with overwhelming probability <ref type="bibr">[13, p 237]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MONTE CARLO TREE SEARCH</head><p>This section introduces the family of algorithms known as Monte Carlo Tree Search (MCTS). MCTS rests on two fundamental concepts: that the true value of an action may be approximated using random simulation; and that these values may be used efficiently to adjust the policy towards a best-first strategy. The algorithm progressively builds a partial game tree, guided by the results of previous exploration of that tree. The tree is used to estimate the values of moves, with these estimates (particularly those for the most promising moves) becoming more accurate as the tree is built.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithm</head><p>The basic algorithm involves iteratively building a search tree until some predefined computational budget -typically a time, memory or iteration constraint -is reached, at which point the search is halted and the bestperforming root action returned. Each node in the search tree represents a state of the domain, and directed links to child nodes represent actions leading to subsequent states.</p><p>Four steps are applied per search iteration <ref type="bibr" target="#b51">[52]</ref>: </p><formula xml:id="formula_3">v l ← TREEPOLICY(v 0 ) ∆ ← DEFAULTPOLICY(s(v l )) BACKUP(v l , ∆) return a(BESTCHILD(v 0 ))</formula><p>the tree until the most urgent expandable node is reached. A node is expandable if it represents a nonterminal state and has unvisited (i.e. unexpanded) children.</p><p>2) Expansion: One (or more) child nodes are added to expand the tree, according to the available actions. 3) Simulation: A simulation is run from the new node(s) according to the default policy to produce an outcome. 4) Backpropagation: The simulation result is "backed up" (i.e. backpropagated) through the selected nodes to update their statistics.</p><p>These may be grouped into two distinct policies:</p><p>1) Tree Policy: Select or create a leaf node from the nodes already contained within the search tree (selection and expansion). 2) Default Policy: Play out the domain from a given non-terminal state to produce a value estimate (simulation).</p><p>The backpropagation step does not use a policy itself, but updates node statistics that inform future tree policy decisions. These steps are summarised in pseudocode in Algo-rithm 1. 6 Here v 0 is the root node corresponding to state s 0 , v l is the last node reached during the tree policy stage and corresponds to state s l , and ∆ is the reward for the terminal state reached by running the default policy from state s l . The result of the overall search a(BESTCHILD(v 0 )) is the action a that leads to the best child of the root node v 0 , where the exact definition of "best" is defined by the implementation. Note that alternative interpretations of the term "simulation" exist in the literature. Some authors take it to mean the complete sequence of actions chosen per iteration during both the tree and default policies (see for example <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b205">[204]</ref>, <ref type="bibr" target="#b93">[94]</ref>) while most take it to mean the sequence of actions chosen using the default policy only. In this paper we shall understand the terms playout and simulation to mean "playing out the task to completion according to the default policy", i.e. the sequence of actions chosen after the tree policy steps of selection and expansion have been completed.</p><p>Figure <ref type="figure">2</ref> shows one iteration of the basic MCTS algorithm. Starting at the root node 7 t 0 , child nodes are recursively selected according to some utility function until a node t n is reached that either describes a terminal state or is not fully expanded (note that this is not necessarily a leaf node of the tree). An unvisited action a from this state s is selected and a new leaf node t l is added to the tree, which describes the state s reached from applying action a to state s. This completes the tree policy component for this iteration.</p><p>A simulation is then run from the newly expanded leaf node t l to produce a reward value ∆, which is then 6. The simulation and expansion steps are often described and/or implemented in the reverse order in practice <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b66">[67]</ref>.</p><p>7. Each node contains statistics describing at least a reward value and number of visits. backpropagated up the sequence of nodes selected for this iteration to update the node statistics; each node's visit count is incremented and its average reward or Q value updated according to ∆. The reward value ∆ may be a discrete (win/draw/loss) result or continuous reward value for simpler domains, or a vector of reward values relative to each agent p for more complex multiagent domains.</p><p>As soon as the search is interrupted or the computation budget is reached, the search terminates and an action a of the root node t 0 is selected by some mechanism. Schadd <ref type="bibr" target="#b189">[188]</ref> describes four criteria for selecting the winning action, based on the work of Chaslot et al <ref type="bibr" target="#b59">[60]</ref>:</p><p>1) Max child: Select the root child with the highest reward. 2) Robust child: Select the most visited root child.</p><p>3) Max-Robust child: Select the root child with both the highest visit count and the highest reward. If none exist, then continue searching until an acceptable visit count is achieved <ref type="bibr" target="#b69">[70]</ref>. 4) Secure child: Select the child which maximises a lower confidence bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Development</head><p>Monte Carlo methods have been used extensively in games with randomness and partial observability <ref type="bibr" target="#b69">[70]</ref> but they may be applied equally to deterministic games of perfect information. Following a large number of simulated games, starting at the current state and played until the end of the game, the initial move with the highest win-rate is selected to advance the game. In the majority of cases, actions were sampled uniformly at random (or with some game-specific heuristic bias) with no game-theoretic guarantees <ref type="bibr" target="#b119">[119]</ref>. In other words, even if the iterative process is executed for an extended period of time, the move selected in the end may not be optimal <ref type="bibr" target="#b120">[120]</ref>. Despite the lack of game-theoretic guarantees, the accuracy of the Monte Carlo simulations may often be improved by selecting actions according to the cumulative reward of the game episodes they were part of. This may be achieved by keeping track of the states visited in a tree. In 2006 Coulom <ref type="bibr" target="#b69">[70]</ref> proposed a novel approach that combined Monte Carlo evaluations with tree search. His proposed algorithm iteratively runs random simulations from the current state to the end of the game: nodes close to the root are added to an incrementally growing tree, revealing structural information from the random sampling episodes. In particular, nodes in the tree are selected according to the estimated probability that they are better than the current best move.</p><p>The breakthrough for MCTS also came in 2006 and is primarily due to the selectivity mechanism proposed by Kocsis and Szepesvári, whose aim was to design a Monte Carlo search algorithm that had a small error probability if stopped prematurely and that converged to the game-theoretic optimum given sufficient time <ref type="bibr" target="#b120">[120]</ref>.</p><p>This may be achieved by reducing the estimation error of the nodes' values as quickly as possible. In order to do so, the algorithm must balance exploitation of the currently most promising action with exploration of alternatives which may later turn out to be superior. This exploitation-exploration dilemma can be captured by multi-armed bandit problems (2.4), and UCB1 <ref type="bibr" target="#b12">[13]</ref> is an obvious choice for node selection. 8  Table <ref type="table">1</ref> summarises the milestones that led to the conception and popularisation of MCTS. It is interesting to note that the development of MCTS is the coming together of numerous different results in related fields of research in AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Upper Confidence Bounds for Trees (UCT)</head><p>This section describes the most popular algorithm in the MCTS family, the Upper Confidence Bound for Trees (UCT) algorithm. We provide a detailed description of the algorithm, and briefly outline the proof of convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">The UCT algorithm</head><p>The goal of MCTS is to approximate the (true) gametheoretic value of the actions that may be taken from the current state <ref type="bibr">(3.1)</ref>. This is achieved by iteratively building a partial search tree, as illustrated in Figure <ref type="figure">2</ref>. How the tree is built depends on how nodes in the tree are selected. The success of MCTS, especially in Go, is primarily due to this tree policy. In particular, Kocsis and Szepesvári <ref type="bibr" target="#b119">[119]</ref>, <ref type="bibr" target="#b120">[120]</ref> proposed the use of UCB1 (2.4.2) as tree policy. In treating the choice of child node as a multi-armed bandit problem, the value of a child node is the expected reward approximated by the Monte Carlo simulations, and hence these rewards correspond to random variables with unknown distributions.</p><p>UCB1 has some promising properties: it is very simple and efficient and guaranteed to be within a constant factor of the best possible bound on the growth of regret. It is thus a promising candidate to address the exploration-exploitation dilemma in MCTS: every time a node (action) is to be selected within the existing tree, the choice may be modelled as an independent multi-armed bandit problem. A child node j is selected to maximise:</p><formula xml:id="formula_4">U CT = X j + 2C p 2 ln n n j</formula><p>where n is the number of times the current (parent) node has been visited, n j the number of times child j has been visited and C p &gt; 0 is a constant. If more than one child node has the same maximal value, the tie is usually broken randomly <ref type="bibr" target="#b120">[120]</ref>. The values of X i,t and thus of X j are understood to be within [0, 1] (this holds true for both the UCB1 and the UCT proofs). It is generally understood that n j = 0 yields a UCT value of ∞, so that 8. Coulom <ref type="bibr" target="#b69">[70]</ref> points out that the Boltzmann distribution often used in n-armed bandit problems is not suitable as a selection mechanism, as the underlying reward distributions in the tree are non-stationary. <ref type="bibr">1990</ref> Abramson demonstrates that Monte Carlo simulations can be used to evaluate value of state <ref type="bibr" target="#b0">[1]</ref>. 1993 Br ügmann <ref type="bibr" target="#b30">[31]</ref> applies Monte Carlo methods to the field of computer Go. 1998 Ginsberg's GIB program competes with expert Bridge players. 1998 MAVEN defeats the world scrabble champion <ref type="bibr" target="#b200">[199]</ref>. 2002 Auer et al. <ref type="bibr" target="#b12">[13]</ref> propose UCB1 for multi-armed bandit, laying the theoretical foundation for UCT. 2006 Coulom <ref type="bibr" target="#b69">[70]</ref> describes Monte Carlo evaluations for tree-based search, coining the term Monte Carlo tree search. 2006 Kocsis and Szepesvari <ref type="bibr" target="#b119">[119]</ref> associate UCB with tree-based search to give the UCT algorithm. 2006 Gelly et al. <ref type="bibr" target="#b95">[96]</ref> apply UCT to computer Go with remarkable success, with their program MOGO. 2006 Chaslot et al. describe MCTS as a broader framework for game AI <ref type="bibr" target="#b51">[52]</ref> and general domains <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2007</head><p>CADIAPLAYER becomes world champion General Game Player <ref type="bibr" target="#b82">[83]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2008</head><p>MOGO achieves dan (master) level at 9 × 9 Go <ref type="bibr" target="#b128">[128]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2009</head><p>FUEGO beats top human professional at 9 × 9 Go <ref type="bibr" target="#b80">[81]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2009</head><p>MOHEX becomes world champion Hex player <ref type="bibr" target="#b6">[7]</ref>.</p><p>TABLE 1 Timeline of events leading to the widespread popularity of MCTS.</p><p>previously unvisited children are assigned the largest possible value, to ensure that all children of a node are considered at least once before any child is expanded further. This results in a powerful form of iterated local search.</p><p>There is an essential balance between the first (exploitation) and second (exploration) terms of the UCB equation. As each node is visited, the denominator of the exploration term increases, which decreases its contribution. On the other hand, if another child of the parent node is visited, the numerator increases and hence the exploration values of unvisited siblings increase. The exploration term ensures that each child has a nonzero probability of selection, which is essential given the random nature of the playouts. This also imparts an inherent restart property to the algorithm, as even lowreward children are guaranteed to be chosen eventually (given sufficient time), and hence different lines of play explored.</p><p>The constant in the exploration term C p can be adjusted to lower or increase the amount of exploration performed. The value C p = 1/ √ 2 was shown by Kocsis and Szepesvári <ref type="bibr" target="#b120">[120]</ref> to satisfy the Hoeffding ineqality with rewards in the range [0, 1]. With rewards outside this range, a different value of C p may be needed and also certain enhancements 9 work better with a different value for C p <ref type="bibr">(7.1.3)</ref>.</p><p>The rest of the algorithm proceeds as described in Section 3.1: if the node selected by UCB descent has children that are not yet part of the tree, one of those is chosen randomly and added to the tree. The default policy is then used until a terminal state has been reached. In the simplest case, this default policy is uniformly random. The value ∆ of the terminal state s T is then backpropagated to all nodes visited during this iteration, from the newly added node to the root.</p><p>Each node holds two values, the number N (v) of times it has been visited and a value Q(v) that corresponds to the total reward of all playouts that passed through this state (so that Q(v)/N (v) is an approximation of the node's game-theoretic value). Every time a node is 9. Such as RAVE (5.3.5).</p><p>part of a playout from the root, its values are updated. Once some computational budget has been reached, the algorithm terminates and returns the best move found, corresponding to the child of the root with the highest visit count.</p><p>Algorithm 2 shows the UCT algorithm in pseudocode. This code is a summary of UCT descriptions from several sources, notably <ref type="bibr" target="#b93">[94]</ref>, but adapted to remove the twoplayer, zero-sum and turn order constraints typically found in the existing literature.</p><p>Each node v has four pieces of data associated with it: the associated state s(v), the incoming action a(v), the total simulation reward Q(v) (a vector of real values), and the visit count N (v) (a nonnegative integer). Instead of storing s(v) for each node, it is often more efficient in terms of memory usage to recalculate it as TREEPOLICY descends the tree. The term ∆(v, p) denotes the component of the reward vector ∆ associated with the current player p at node v.</p><p>The return value of the overall search in this case is a(BESTCHILD(v 0 , 0)) which will give the action a that leads to the child with the highest reward, 10 since the exploration parameter c is set to 0 for this final call on the root node v 0 . The algorithm could instead return the action that leads to the most visited child; these two options will usually -but not always! -describe the same action. This potential discrepancy is addressed in the Go program ERICA by continuing the search if the most visited root action is not also the one with the highest reward. This improved ERICA's winning rate against GNU GO from 47% to 55% <ref type="bibr" target="#b107">[107]</ref>.</p><p>Algorithm 3 shows an alternative and more efficient backup method for two-player, zero-sum games with alternating moves, that is typically found in the literature. This is analogous to the negamax variant of minimax search, in which scalar reward values are negated at each level in the tree to obtain the other player's reward. Note that while ∆ is treated as a vector of rewards with an entry for each agent in Algorithm 2, 11 it is a single scalar value representing the reward to the agent running the 10. The max child in Schadd's <ref type="bibr" target="#b189">[188]</ref> terminology. 11. ∆(v, p) denotes the reward for p the player to move at node v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2</head><p>The UCT algorithm. function UCTSEARCH(s 0 ) create root node v 0 with state s 0 while within computational budget do</p><formula xml:id="formula_5">v l ← TREEPOLICY(v 0 ) ∆ ← DEFAULTPOLICY(s(v l )) BACKUP(v l , ∆) return a(BESTCHILD(v 0 , 0)) function TREEPOLICY(v) while v is nonterminal do if v not fully expanded then return EXPAND(v) else v ← BESTCHILD(v, Cp) return v function EXPAND(v) choose a ∈ untried actions from A(s(v)) add a new child v to v with s(v ) = f (s(v), a) and a(v ) = a return v function BESTCHILD(v, c) return arg max v ∈children of v Q(v ) N (v ) + c 2 ln N (v) N (v )</formula><p>function DEFAULTPOLICY(s) while s is non-terminal do choose a ∈ A(s) uniformly at random s ← f (s, a) return reward for state s</p><formula xml:id="formula_6">function BACKUP(v, ∆) while v is not null do N (v) ← N (v) + 1 Q(v) ← Q(v) + ∆(v, p) v ← parent of v Algorithm 3 UCT backup for two players. function BACKUPNEGAMAX(v, ∆) while v is not null do N (v) ← N (v) + 1 Q(v) ← Q(v) + ∆ ∆ ← −∆ v ← parent of v</formula><p>search in Algorithm 3. Similarly, the node reward value Q(v) may be treated as a vector of values for each player Q(v, p) should circumstances dictate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Convergence to Minimax</head><p>The key contributions of Kocsis and Szepesvári <ref type="bibr" target="#b119">[119]</ref>, <ref type="bibr" target="#b120">[120]</ref> were to show that the bound on the regret of UCB1 still holds in the case of non-stationary reward distributions, and to empirically demonstrate the workings of MCTS with UCT on a variety of domains. Kocsis and Szepesvári then show that the failure probability at the root of the tree (i.e. the probability of selecting a suboptimal action) converges to zero at a polynomial rate as the number of games simulated grows to infinity. This proof implies that, given enough time (and memory), UCT allows MCTS to converge to the minimax tree and is thus optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Characteristics</head><p>This section describes some of the characteristics that make MCTS a popular choice of algorithm for a variety of domains, often with notable success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Aheuristic</head><p>One of the most significant benefits of MCTS is the lack of need for domain-specific knowledge, making it readily applicable to any domain that may be modelled using a tree. Although full-depth minimax is optimal in the game-theoretic sense, the quality of play for depthlimited minimax depends significantly on the heuristic used to evaluate leaf nodes. In games such as Chess, where reliable heuristics have emerged after decades of research, minimax performs admirably well. In cases such as Go, however, where branching factors are orders of magnitude larger and useful heuristics are much more difficult to formulate, the performance of minimax degrades significantly.</p><p>Although MCTS can be applied in its absence, significant improvements in performance may often be achieved using domain-specific knowledge. All topperforming MCTS-based Go programs now use gamespecific information, often in the form of patterns (6.1.9). Such knowledge need not be complete as long as it is able to bias move selection in a favourable fashion.</p><p>There are trade-offs to consider when biasing move selection using domain-specific knowledge: one of the advantages of uniform random move selection is speed, allowing one to perform many simulations in a given time. Domain-specific knowledge usually drastically reduces the number of simulations possible, but may also reduce the variance of simulation results. The degree to which game-specific knowledge should be included, with respect to performance versus generality as well as speed trade-offs, is discussed in <ref type="bibr" target="#b76">[77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Anytime</head><p>MCTS backpropagates the outcome of each game immediately (the tree is built using playouts as opposed to stages <ref type="bibr" target="#b119">[119]</ref>) which ensures all values are always upto-date following every iteration of the algorithm. This allows the algorithm to return an action from the root at Fig. <ref type="figure">3</ref>. Asymmetric tree growth <ref type="bibr" target="#b67">[68]</ref>. any moment in time; allowing the algorithm to run for additional iterations often improves the result.</p><p>It is possible to approximate an anytime version of minimax using iterative deepening. However, the granularity of progress is much coarser as an entire ply is added to the tree on each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Asymmetric</head><p>The tree selection allows the algorithm to favour more promising nodes (without allowing the selection probability of the other nodes to converge to zero), leading to an asymmetric tree over time. In other words, the building of the partial tree is skewed towards more promising and thus more important regions. Figure <ref type="figure">3</ref> from <ref type="bibr" target="#b67">[68]</ref> shows asymmetric tree growth using the BAST variation of MCTS (4.2).</p><p>The tree shape that emerges can even be used to gain a better understanding about the game itself. For instance, Williams <ref type="bibr" target="#b232">[231]</ref> demonstrates that shape analysis applied to trees generated during UCT search can be used to distinguish between playable and unplayable games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison with Other Algorithms</head><p>When faced with a problem, the a priori choice between MCTS and minimax may be difficult. If the game tree is of nontrivial size and no reliable heuristic exists for the game of interest, minimax is unsuitable but MCTS is applicable (3.4.1). If domain-specific knowledge is readily available, on the other hand, both algorithms may be viable approaches.</p><p>However, as pointed out by Ramanujan et al. <ref type="bibr" target="#b165">[164]</ref>, MCTS approaches to games such as Chess are not as successful as for games such as Go. They consider a class of synthetic spaces in which UCT significantly outperforms minimax. In particular, the model produces bounded trees where there is exactly one optimal action per state; sub-optimal choices are penalised with a fixed additive cost. The systematic construction of the tree ensures that the true minimax values are known. 12 In this domain, UCT clearly outperforms minimax and the gap in performance increases with tree depth.</p><p>Ramanujan et al. <ref type="bibr" target="#b163">[162]</ref> argue that UCT performs poorly in domains with many trap states (states that lead to losses within a small number of moves), whereas iterative deepening minimax performs relatively well. Trap states are common in Chess but relatively uncommon in Go, which may go some way towards explaining the algorithms' relative performance in those games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Terminology</head><p>The terms MCTS and UCT are used in a variety of ways in the literature, sometimes inconsistently, potentially leading to confusion regarding the specifics of the algorithm referred to. For the remainder of this survey, we adhere to the following meanings:</p><p>• Flat Monte Carlo: A Monte Carlo method with uniform move selection and no tree growth. • Flat UCB: A Monte Carlo method with bandit-based move selection (2.4) but no tree growth. • MCTS: A Monte Carlo method that builds a tree to inform its policy online. • UCT: MCTS with any UCB tree selection policy.</p><p>• Plain UCT: MCTS with UCB1 as proposed by <ref type="bibr">Kocsis</ref> and Szepesvári <ref type="bibr" target="#b119">[119]</ref>, <ref type="bibr" target="#b120">[120]</ref>. In other words, "plain UCT" refers to the specific algorithm proposed by Kocsis and Szepesvári, whereas the other terms refer more broadly to families of algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VARIATIONS</head><p>Traditional game AI research focusses on zero-sum games with two players, alternating turns, discrete action spaces, deterministic state transitions and perfect information. While MCTS has been applied extensively to such games, it has also been applied to other domain types such as single-player games and planning problems, multi-player games, real-time games, and games with uncertainty or simultaneous moves. This section describes the ways in which MCTS has been adapted to these domains, in addition to algorithms that adopt ideas from MCTS without adhering strictly to its outline. <ref type="bibr" target="#b67">[68]</ref> propose flat UCB which effectively treats the leaves of the search tree as a single multiarmed bandit problem. This is distinct from flat Monte Carlo search (2.3) in which the actions for a given state are uniformly sampled and no tree is built. Coquelin and Munos <ref type="bibr" target="#b67">[68]</ref> demonstrate that flat UCB retains the adaptivity of standard UCT while improving its regret bounds in certain worst cases where UCT is overly optimistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Flat UCB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coquelin and Munos</head><p>12. This is related to P-game trees (7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bandit Algorithm for Smooth Trees (BAST)</head><p>Coquelin and Munos <ref type="bibr" target="#b67">[68]</ref> extend the flat UCB model to suggest a Bandit Algorithm for Smooth Trees (BAST), which uses assumptions on the smoothness of rewards to identify and ignore branches that are suboptimal with high confidence. They applied BAST to Lipschitz function approximation and showed that when allowed to run for infinite time, the only branches that are expanded indefinitely are the optimal branches. This is in contrast to plain UCT, which expands all branches indefinitely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning in MCTS</head><p>MCTS can be seen as a type of Reinforcement Learning (RL) algorithm, so it is interesting to consider its relationship with temporal difference learning (arguably the canonical RL algorithm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Temporal Difference Learning (TDL)</head><p>Both temporal difference learning (TDL) and MCTS learn to take actions based on the values of states, or of stateaction pairs. Under certain circumstances the algorithms may even be equivalent <ref type="bibr" target="#b202">[201]</ref>, but TDL algorithms do not usually build trees, and the equivalence only holds when all the state values can be stored directly in a table. MCTS estimates temporary state values in order to decide the next move, whereas TDL learns the long-term value of each state that then guides future behaviour. Silver et al. <ref type="bibr" target="#b203">[202]</ref> present an algorithm that combines MCTS with TDL using the notion of permanent and transient memories to distinguish the two types of state value estimation. TDL can learn heuristic value functions to inform the tree policy or the simulation (playout) policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Temporal Difference with Monte Carlo (TDMC(λ))</head><p>Osaki et al. describe the Temporal Difference with Monte Carlo (TDMC(λ)) algorithm as "a new method of reinforcement learning using winning probability as substitute rewards in non-terminal positions" <ref type="bibr" target="#b157">[157]</ref> and report superior performance over standard TD learning for the board game Othello (7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Bandit-Based Active Learner (BAAL)</head><p>Rolet et al. <ref type="bibr" target="#b176">[175]</ref>, <ref type="bibr" target="#b174">[173]</ref>, <ref type="bibr" target="#b175">[174]</ref> propose the Bandit-based Active Learner (BAAL) method to address the issue of small training sets in applications where data is sparse. The notion of active learning is formalised under bounded resources as a finite horizon reinforcement learning problem with the goal of minimising the generalisation error. Viewing active learning as a single-player game, the optimal policy is approximated by a combination of UCT and billiard algorithms <ref type="bibr" target="#b174">[173]</ref>. Progressive widening (5.5.1) is employed to limit the degree of exploration by UCB1 to give promising empirical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Single-Player MCTS (SP-MCTS)</head><p>Schadd et al. <ref type="bibr" target="#b192">[191]</ref>, <ref type="bibr" target="#b190">[189]</ref> introduce a variant of MCTS for single-player games, called Single-Player Monte Carlo Tree Search (SP-MCTS), which adds a third term to the standard UCB formula that represents the "possible deviation" of the node. This term can be written</p><formula xml:id="formula_7">σ 2 + D n i ,</formula><p>where σ 2 is the variance of the node's simulation results, n i is the number of visits to the node, and D is a constant. The D ni term can be seen as artificially inflating the standard deviation for infrequently visited nodes, so that the rewards for such nodes are considered to be less certain. The other main difference between SP-MCTS and plain UCT is the use of a heuristically guided default policy for simulations.</p><p>Schadd et al. <ref type="bibr" target="#b192">[191]</ref> point to the need for Meta-Search (a higher-level search method that uses other search processes to arrive at an answer) in some cases where SP-MCTS on its own gets caught in local maxima. They found that periodically restarting the search with a different random seed and storing the best solution over all runs considerably increased the performance of their SameGame player <ref type="bibr">(7.4)</ref>.</p><p>Bj örnsson and Finnsson <ref type="bibr" target="#b20">[21]</ref> discuss the application of standard UCT to single-player games. They point out that averaging simulation results can hide a strong line of play if its siblings are weak, instead favouring regions where all lines of play are of medium strength. To counter this, they suggest tracking maximum simulation results at each node in addition to average results; the averages are still used during search.</p><p>Another modification suggested by Bj örnsson and Finnsson <ref type="bibr" target="#b20">[21]</ref> is that when simulation finds a strong line of play, it is stored in the tree in its entirety. This would be detrimental in games of more than one player since such a strong line would probably rely on the unrealistic assumption that the opponent plays weak moves, but for single-player games this is not an issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Feature UCT Selection (FUSE)</head><p>Gaudel and Sebag introduce Feature UCT Selection (FUSE), an adaptation of UCT to the combinatorial optimisation problem of feature selection <ref type="bibr" target="#b88">[89]</ref>. Here, the problem of choosing a subset of the available features is cast as a single-player game whose states are all possible subsets of features and whose actions consist of choosing a feature and adding it to the subset.</p><p>To deal with the large branching factor of this game, FUSE uses UCB1-Tuned (5.1.1) and RAVE <ref type="bibr">(5.3.5)</ref>. FUSE also uses a game-specific approximation of the reward function, and adjusts the probability of choosing the stopping feature during simulation according to the depth in the tree. Gaudel and Sebag <ref type="bibr" target="#b88">[89]</ref> apply FUSE to three benchmark data sets from the NIPS 2003 FS Challenge competition (7.8.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Multi-player MCTS</head><p>The central assumption of minimax search (2.2.2) is that the searching player seeks to maximise their reward while the opponent seeks to minimise it. In a two-player zero-sum game, this is equivalent to saying that each player seeks to maximise their own reward; however, in games of more than two players, this equivalence does not necessarily hold.</p><p>The simplest way to apply MCTS to multi-player games is to adopt the max n idea: each node stores a vector of rewards, and the selection procedure seeks to maximise the UCB value calculated using the appropriate component of the reward vector. Sturtevant <ref type="bibr" target="#b208">[207]</ref> shows that this variant of UCT converges to an optimal equilibrium strategy, although this strategy is not precisely the max n strategy as it may be mixed.</p><p>Cazenave <ref type="bibr" target="#b39">[40]</ref> applies several variants of UCT to the game of Multi-player Go (7.1.5) and considers the possibility of players acting in coalitions. The search itself uses the max n approach described above, but a rule is added to the simulations to avoid playing moves that adversely affect fellow coalition members, and a different scoring system is used that counts coalition members' stones as if they were the player's own. There are several ways in which such coalitions can be handled. In Paranoid UCT, the player considers that all other players are in coalition against him. In UCT with Alliances, the coalitions are provided explicitly to the algorithm. In Confident UCT, independent searches are conducted for each possible coalition of the searching player with one other player, and the move chosen according to whichever of these coalitions appears most favourable. Cazenave <ref type="bibr" target="#b39">[40]</ref> finds that Confident UCT performs worse than Paranoid UCT in general, but the performance of the former is better when the algorithms of the other players (i.e. whether they themselves use Confident UCT) are taken into account. Nijssen and Winands <ref type="bibr" target="#b155">[155]</ref> describe the Multi-Player Monte-Carlo Tree Search Solver (MP-MCTS-Solver) version of their MCTS Solver enhancement (5.4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Coalition Reduction</head><p>Winands and Nijssen describe the coalition reduction method <ref type="bibr" target="#b156">[156]</ref> for games such as Scotland Yard (7.7) in which multiple cooperative opponents can be reduced to a single effective opponent. Note that rewards for those opponents who are not the root of the search must be biased to stop them getting lazy <ref type="bibr" target="#b156">[156]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Multi-agent MCTS</head><p>Marcolino and Matsubara <ref type="bibr" target="#b139">[139]</ref> describe the simulation phase of UCT as a single agent playing against itself, and instead consider the effect of having multiple agents (i.e. multiple simulation policies). Specifically, the different agents in this case are obtained by assigning different priorities to the heuristics used in Go program FUEGO's simulations <ref type="bibr" target="#b80">[81]</ref>. If the right subset of agents is chosen (or learned, as in <ref type="bibr" target="#b139">[139]</ref>), using multiple agents improves playing strength. Marcolino and Matsubara <ref type="bibr" target="#b139">[139]</ref> argue that the emergent properties of interactions between different agent types lead to increased exploration of the search space. However, finding the set of agents with the correct properties (i.e. those that increase playing strength) is computationally intensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Ensemble UCT</head><p>Fern and Lewis <ref type="bibr" target="#b81">[82]</ref> investigate an Ensemble UCT approach, in which multiple instances of UCT are run independently and their root statistics combined to yield the final result. This approach is closely related to root parallelisation (6.3.2) and also to determinization (4.8.1).</p><p>Chaslot et al. <ref type="bibr" target="#b58">[59]</ref> provide some evidence that, for Go, Ensemble UCT with n instances of m iterations each outperforms plain UCT with mn iterations, i.e. that Ensemble UCT outperforms plain UCT given the same total number of iterations. However, Fern and Lewis <ref type="bibr" target="#b81">[82]</ref> are not able to reproduce this result on other experimental domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Real-time MCTS</head><p>Traditional board games are turn-based, often allowing each player considerable time for each move (e.g. several minutes for Go). However, real-time games tend to progress constantly even if the player attempts no move, so it is vital for an agent to act quickly. The largest class of real-time games are video games, which -in addition to the real-time element -are usually also characterised by uncertainty (4.8), massive branching factors, simultaneous moves (4.8.10) and open-endedness. Developing strong artificial players for such games is thus particularly challenging and so far has been limited in success.</p><p>Simulation-based (anytime) algorithms such as MCTS are well suited to domains in which time per move is strictly limited. Furthermore, the asymmetry of the trees produced by MCTS may allow a better exploration of the state space in the time available. Indeed, MCTS has been applied to a diverse range of real-time games of increasing complexity, ranging from Tron and Ms. Pac-Man to a variety of real-time strategy games akin to Starcraft. In order to make the complexity of real-time video games tractable, approximations may be used to increase the efficiency of the forward model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Nondeterministic MCTS</head><p>Traditional game AI research also typically focusses on deterministic games with perfect information, i.e. games without chance events in which the state of the game is fully observable to all players (2.2). We now consider games with stochasticity (chance events) and/or imperfect information (partial observability of states).</p><p>Opponent modelling (i.e. determining the opponent's policy) is much more important in games of imperfect information than games of perfect information, as the opponent's policy generally depends on their hidden information, hence guessing the former allows the latter to be inferred. Section 4.8.9 discusses this in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.1">Determinization</head><p>A stochastic game with imperfect information can be transformed into a deterministic game with perfect information, by fixing the outcomes of all chance events and making states fully observable. For example, a card game can be played with all cards face up, and a game with dice can be played with a predetermined sequence of dice rolls known to all players. Determinization<ref type="foot" target="#foot_3">13</ref> is the process of sampling several such instances of the deterministic game with perfect information, analysing each with standard AI techniques, and combining those analyses to obtain a decision for the full game.</p><p>Cazenave <ref type="bibr" target="#b35">[36]</ref> applies depth-1 search with Monte Carlo evaluation to the game of Phantom Go (7.1.5). At the beginning of each iteration, the game is determinized by randomly placing the opponent's hidden stones. The evaluation and search then continues as normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.2">Hindsight optimisation (HOP)</head><p>Hindsight optimisation (HOP) provides a more formal basis to determinization for single-player stochastic games of perfect information. The idea is to obtain an upper bound on the expected reward for each move by assuming the ability to optimise one's subsequent strategy with "hindsight" knowledge of future chance outcomes. This upper bound can easily be approximated by determinization. The bound is not particularly tight, but is sufficient for comparing moves.</p><p>Bjarnason et al. <ref type="bibr" target="#b19">[20]</ref> apply a combination of HOP and UCT to the single-player stochastic game of Klondike solitaire (7.7). Specifically, UCT is used to solve the determinized games sampled independently by HOP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.3">Sparse UCT</head><p>Sparse UCT is a generalisation of this HOP-UCT procedure also described by Bjarnason et al. <ref type="bibr" target="#b19">[20]</ref>. In Sparse UCT, a node may have several children corresponding to the same move, each child corresponding to a different stochastic outcome of that move. Moves are selected as normal by UCB, but the traversal to child nodes is stochastic, as is the addition of child nodes during expansion. <ref type="bibr">Bjarnason et al. [20]</ref> also define an ensemble version of Sparse UCT, whereby several search trees are constructed independently and their results (the expected rewards of actions from the root) are averaged, which is similar to Ensemble UCT (4.6.1).</p><p>Borsboom et al. <ref type="bibr" target="#b22">[23]</ref> suggest ways of combining UCT with HOP-like ideas, namely early probabilistic guessing and late random guessing. These construct a single UCT tree, and determinize the game at different points in each iteration (at the beginning of the selection and simulation phases, respectively). Late random guessing significantly outperforms early probabilistic guessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.4">Information Set UCT (ISUCT)</head><p>Strategy fusion is a problem with determinization techniques, which involves the incorrect assumption that different moves can be chosen from different states in the same information set. Long et al. <ref type="bibr" target="#b130">[130]</ref> describe how this can be measured using synthetic game trees.</p><p>To address the problem of strategy fusion in determinized UCT, Whitehouse et al. <ref type="bibr" target="#b231">[230]</ref> propose information set UCT (ISUCT), a variant of MCTS that operates directly on trees of information sets. All information sets are from the point of view of the root player. Each iteration samples a determinization (a state from the root information set) and restricts selection, expansion and simulation to those parts of the tree compatible with the determinization. The UCB formula is modified to replace the "parent visit" count with the number of parent visits in which the child was compatible.</p><p>For the experimental domain in <ref type="bibr" target="#b231">[230]</ref>, ISUCT fails to outperform determinized UCT overall. However, ISUCT is shown to perform well in precisely those situations where access to hidden information would have the greatest effect on the outcome of the game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.5">Multiple MCTS</head><p>Auger <ref type="bibr" target="#b15">[16]</ref> proposes a variant of MCTS for games of imperfect information, called Multiple Monte Carlo Tree Search (MMCTS), in which multiple trees are searched simultaneously. Specifically, there is a tree for each player, and the search descends and updates all of these trees simultaneously, using statistics in the tree for the relevant player at each stage of selection. This more accurately models the differences in information available to each player than searching a single tree. MMCTS uses EXP3 (5.1.3) for selection.</p><p>Auger <ref type="bibr" target="#b15">[16]</ref> circumvents the difficulty of computing the correct belief distribution at non-initial points in the game by using MMCTS in an offline manner. MMCTS is run for a large number of simulations (e.g. 50 million) to construct a partial game tree rooted at the initial state of the game, and the player's policy is read directly from this pre-constructed tree during gameplay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.6">UCT+</head><p>Van den Broeck et al. <ref type="bibr" target="#b224">[223]</ref> describe a variant of MCTS for miximax trees (2.2.2) in which opponent decision nodes are treated as chance nodes with probabilities determined by an opponent model. The algorithm is called UCT+, although it does not use UCB: instead, actions are selected to maximise</p><formula xml:id="formula_8">X j + cσ Xj ,</formula><p>where X j is the average reward from action j, σ Xj is the standard error on X j , and c is a constant. During backpropagation, each visited node's X j and σ Xj values are updated according to their children; at opponent nodes and chance nodes, the calculations are weighted by the probabilities of the actions leading to each child.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.7">Monte Carlo α-β (MC αβ )</head><p>Monte Carlo α-β (MC αβ ) combines MCTS with traditional tree search by replacing the default policy with a shallow α-β search. For example, Winands and Bj örnnsson <ref type="bibr" target="#b233">[232]</ref> apply a selective two-ply α-β search in lieu of a default policy for their program MC αβ , which is currently the strongest known computer player for the game Lines of Action (7.2). An obvious consideration in choosing MC αβ for a domain is that a reliable heuristic function must be known in order to drive the α-β component of the search, which ties the implementation closely with the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.8">Monte Carlo Counterfactual Regret (MCCFR)</head><p>Counterfactual regret (CFR) is an algorithm for computing approximate Nash equilibria for games of imperfect information. Specifically, at time t + 1 the policy plays actions with probability proportional to their positive counterfactual regret at time t, or with uniform probability if no actions have positive counterfactual regret. A simple example of how CFR operates is given in <ref type="bibr" target="#b178">[177]</ref>.</p><p>CFR is impractical for large games, as it requires traversal of the entire game tree. Lanctot et al. <ref type="bibr" target="#b125">[125]</ref> propose a modification called Monte Carlo counterfactual regret (MCCFR). MCCFR works by sampling blocks of terminal histories (paths through the game tree from root to leaf), and computing immediate counterfactual regrets over those blocks. MCCFR can be used to minimise, with high probability, the overall regret in the same way as CFR. CFR has also been used to create agents capable of exploiting the non-Nash strategies used by UCT agents <ref type="bibr" target="#b197">[196]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.9">Inference and Opponent Modelling</head><p>In a game of imperfect information, it is often possible to infer hidden information from opponent actions, such as learning opponent policies directly using Bayesian inference and relational probability tree learning. The opponent model has two parts -a prior model of a general opponent, and a corrective function for the specific opponent -which are learnt from samples of previously played games. Ponsen et al. <ref type="bibr" target="#b159">[159]</ref> integrate this scheme with MCTS to infer probabilities for hidden cards, which in turn are used to determinize the cards for each MCTS iteration. When the MCTS selection phase reaches an opponent decision node, it uses the mixed policy induced by the opponent model instead of banditbased selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.10">Simultaneous Moves</head><p>Simultaneous moves can be considered a special case of hidden information: one player chooses a move but conceals it, then the other player chooses a move and both are revealed.</p><p>Shafiei et al. <ref type="bibr" target="#b197">[196]</ref> describe a simple variant of UCT for games with simultaneous moves. Shafiei et al. <ref type="bibr" target="#b197">[196]</ref> argue that this method will not converge to the Nash equilibrium in general, and show that a UCT player can thus be exploited.</p><p>Teytaud and Flory <ref type="bibr" target="#b217">[216]</ref> use a similar technique to Shafiei et al. <ref type="bibr" target="#b197">[196]</ref>, the main difference being that they use the EXP3 algorithm (5.1.3) for selection at simultaneous move nodes (UCB is still used at other nodes). EXP3 is explicitly probabilistic, so the tree policy at simultaneous move nodes is mixed. Teytaud and Flory <ref type="bibr" target="#b217">[216]</ref> find that coupling EXP3 with UCT in this way performs much better than simply using the UCB formula at simultaneous move nodes, although performance of the latter does improve if random exploration with fixed probability is introduced.</p><p>Samothrakis et al. <ref type="bibr" target="#b185">[184]</ref> apply UCT to the simultaneous move game Tron (7.6). However, they simply avoid the simultaneous aspect by transforming the game into one of alternating moves. The nature of the game is such that this simplification is not usually detrimental, although Den Teuling <ref type="bibr" target="#b73">[74]</ref> identifies such a degenerate situation and suggests a game-specific modification to UCT to handle this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Recursive Approaches</head><p>The following methods recursively apply a Monte Carlo technique to grow the search tree. These have typically had success with single-player puzzles and similar optimisation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.1">Reflexive Monte Carlo Search</head><p>Reflexive Monte Carlo search <ref type="bibr" target="#b38">[39]</ref> works by conducting several recursive layers of Monte Carlo simulations, each layer informed by the one below. At level 0, the simulations simply use random moves (so a level 0 reflexive Monte Carlo search is equivalent to a 1-ply search with Monte Carlo evaluation). At level n &gt; 0, the simulation uses level n − 1 searches to select each move.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.2">Nested Monte Carlo Search</head><p>A related algorithm to reflexive Monte Carlo search is nested Monte Carlo search (NMCS) <ref type="bibr" target="#b41">[42]</ref>. The key difference is that nested Monte Carlo search memorises the best sequence of moves found at each level of the search.</p><p>Memorising the best sequence so far and using this knowledge to inform future iterations can improve the performance of NMCS in many domains <ref type="bibr" target="#b44">[45]</ref>. Cazenaze et al. describe the application of NMCS to the bus regulation problem (7.8.3) and find that NMCS with memorisation clearly outperforms plain NMCS, which in turn outperforms flat Monte Carlo and rule-based approaches <ref type="bibr" target="#b44">[45]</ref>.</p><p>Cazenave and Jouandeau <ref type="bibr" target="#b48">[49]</ref> describe parallelised implementations of NMCS. Cazenave <ref type="bibr" target="#b42">[43]</ref> also demonstrates the successful application of NMCS methods for the generation of expression trees to solve certain mathematical problems (7.8.2). Rimmel et al. <ref type="bibr" target="#b169">[168]</ref> apply a version of nested Monte Carlo search to the Travelling Salesman Problem (TSP) with time windows (7.8.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.3">Nested Rollout Policy Adaptation (NRPA)</head><p>Nested Rollout Policy Adaptation (NRPA) is an extension of nested Monte Carlo search in which a domain-specific policy is associated with the action leading to each child <ref type="bibr" target="#b177">[176]</ref>. These are tuned adaptively starting from a uniform random policy. NRPA has achieved superior results in puzzle optimisation tasks, including beating the human world record for Morpion Solitaire (7.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.4">Meta-MCTS</head><p>Chaslot et al. <ref type="bibr" target="#b55">[56]</ref> replace the default policy with a nested MCTS program that plays a simulated sub-game in their Meta-MCTS algorithm. They describe two versions of Meta-MCTS: Quasi Best-First (which favours exploitation) and Beta Distribution Sampling (which favours exploration). Both variants improved the playing strength of the program MOGO for 9 × 9 Go when used for generating opening books.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.5">Heuristically Guided Swarm Tree Search</head><p>Edelkamp et al. <ref type="bibr" target="#b77">[78]</ref> introduce the Heuristically Guided Swarm Tree Search (HGSTS) algorithm. This algorithm conducts an exhaustive breadth-first search to a certain level in the game tree, adding a node to the UCT tree for each game tree node at that level. These nodes are inserted into a priority queue, prioritised in descending order of UCB value. The algorithm repeatedly takes the front k elements of the queue and executes an iteration of UCT starting from each of them. Heuristics are used to weight the move probabilities used during simulation. Edelkamp et al. <ref type="bibr" target="#b77">[78]</ref> describe a parallel implementation of this algorithm (using a technique they term set-based parallelisation), and also describe how the breadth-first search portion of the algorithm can be implemented on a GPU for a significant gain in speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">Sample-Based Planners</head><p>Planners for many complex structured domains can be learned with tractable sample complexity if near optimal policies are known. These are generally similar to Single-Player MCTS techniques, but tend to be applied to domains other than games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.1">Forward Search Sparse Sampling (FSSS)</head><p>Walsh et al. <ref type="bibr" target="#b228">[227]</ref> show how to replace known policies with sample-based planners in concert with sampleefficient learners in a method called Forward Search Sparse Sampling (FSSS). They describe a negative case for UCT's runtime that can require exponential computation to optimise, in support of their approach.</p><p>Asmuth and Littman <ref type="bibr" target="#b8">[9]</ref> extend the FSSS technique to Bayesian FSSS (BFS3), which approaches Bayesoptimality as the program's computational budget is increased. They observe that "learning is planning" <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.2">Threshold Ascent for Graphs (TAG)</head><p>Threshold Ascent for Graphs (TAG) is a method that extends the MCTS paradigm by maximizing an objective function over the sinks of directed acyclic graphs <ref type="bibr">[166] [73]</ref>. The algorithm evaluates nodes through random simulation and grows the subgraph in the most promising directions by considering local maximum k-armed bandits. TAG has demonstrated superior performance over standard optimisation methods for automatic performance tuning using DFT and FFT linear transforms in adaptive libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.3">RRTs</head><p>Rapidly-exploring Random Trees (RRTs), a special case of Rapidly-exploring Dense Trees (RTDs), were first introduced by Steven LaValle <ref type="bibr" target="#b126">[126]</ref>. The basic idea of RRTs is to drive the exploration towards unexplored portions of the search space, incrementally pulling the search tree towards them. The tree is built in a similar way to MCTS, by repeating this process multiple times to explore the search space. RRTs share many ideas with MCTS, such as the use of state-action pairs, the tree structure, and the exploration of the search space based on random actions, often guided by heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.4">UNLEO</head><p>Auger and Teytaud describe the UNLEO 14 algorithm as "a heuristic approximation of an optimal optimization algorithm using Upper Confidence Trees" <ref type="bibr" target="#b14">[15]</ref>. UNLEO is based on the No Free Lunch (NFL) and Continuous Free Lunch (CFL) theorems and was inspired by the known optimality of Bayesian inference for supervised learning when a prior distribution is available. Bayesian inference is often very expensive, so Auger and Teytaud use UCT to make the evaluation of complex objective functions achievable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.5">UCTSAT</head><p>Previti et al. <ref type="bibr" target="#b160">[160]</ref> introduce the UCTSAT class of algorithms to investigate the application of UCT approaches to the satisfiability of conjunctive normal form (CNF) problems (7.8.2). They describe the following variations:</p><p>• UCTSAT cp generates a random assignment of variables for each playout.  <ref type="bibr" target="#b158">[158]</ref> argue that random exploration of the search space for a planning problem is inefficient, since the probability of a given simulation actually finding a solution is low. Second, MHSP's selection process simply uses average rewards with no exploration term, but initialises the nodes with "optimistic" average values.</p><p>In contrast to many planning algorithms, MHSP can operate in a truly anytime fashion: even before a solution has been found, MHSP can yield a good partial plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TREE POLICY ENHANCEMENTS</head><p>This section describes modifications proposed for the tree policy of the core MCTS algorithm, in order to improve performance. Many approaches use ideas from traditional AI search such as α-β, while some have no existing context and were developed specifically for MCTS. These can generally be divided into two categories:</p><p>• Domain Independent: These are enhancements that could be applied to any domain without prior knowledge about it. These typically offer small improvements or are better suited to a particular type of domain. • Domain Dependent: These are enhancements specific to particular domains. Such enhancements might use prior knowledge about a domain or otherwise exploit some unique aspect of it.</p><p>This section covers those enhancements specific to the tree policy, i.e. the selection and expansion steps.</p><p>15. AIXI is a mathematical approach based on a Bayesian optimality notion for general reinforcement learning agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Bandit-Based Enhancements</head><p>The bandit-based method used for node selection in the tree policy is central to the MCTS method being used. A wealth of different upper confidence bounds have been proposed, often improving bounds or performance in particular circumstances such as dynamic environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">UCB1-Tuned</head><p>UCB1-Tuned is an enhancement suggested by Auer et al. <ref type="bibr" target="#b12">[13]</ref> to tune the bounds of UCB1 more finely. It replaces the upper confidence bound 2 ln n/n j with:</p><formula xml:id="formula_9">ln n n j min{ 1 4 , V j (n j )}</formula><p>where:</p><formula xml:id="formula_10">V j (s) = (1/2 s τ =1 X 2 j,τ ) − X 2 j,s + 2 ln t s</formula><p>which means that machine j, which has been played s times during the first t plays, has a variance that is at most the sample variance plus 2 ln t)/s <ref type="bibr" target="#b12">[13]</ref>. It should be noted that Auer et al. were unable to prove a regret bound for UCB1-Tuned, but found it performed better than UCB1 in their experiments. UCB1-Tuned has subsequently been used in a variety of MCTS implementations, including Go <ref type="bibr" target="#b94">[95]</ref>, Othello <ref type="bibr" target="#b103">[103]</ref> and the real-time game Tron <ref type="bibr" target="#b185">[184]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Bayesian UCT</head><p>Tesauro et al. <ref type="bibr" target="#b214">[213]</ref> propose that the Bayesian framework potentially allows much more accurate estimation of node values and node uncertainties from limited numbers of simulation trials. Their Bayesian MCTS formalism introduces two tree policies:</p><formula xml:id="formula_11">maximise B i = µ i + 2 ln N n i</formula><p>where µ i replaces the average reward of the node with the mean of an extremum (minimax) distribution P i (assuming independent random variables) and:</p><formula xml:id="formula_12">maximise B i = µ i + 2 ln N n i σ i</formula><p>where σ i is the square root of the variance of P i . Tesauro et al. suggest that the first equation is a strict improvement over UCT if the independence assumption and leaf node priors are correct, while the second equation is motivated by the central limit theorem. They provide convergence proofs for both equations and carry out an empirical analysis in an artificial scenario based on an "idealized bandit-tree simulator". The results indicate that the second equation outperforms the first, and that both outperform the standard UCT approach (although UCT is considerably quicker). McInerney et al. <ref type="bibr" target="#b141">[141]</ref> also describe a Bayesian approach to bandit selection, and argue that this, in principle, avoids the need to choose between exploration and exploitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">EXP3</head><p>The Exploration-Exploitation with Exponential weights (EXP3) algorithm, originally proposed by Auer et al. <ref type="bibr" target="#b13">[14]</ref> and further analysed by Audibert and Bubeck <ref type="bibr" target="#b10">[11]</ref>, applies in the stochastic case (and hence also in the adversarial case). The EXP3 policy operates as follows:</p><p>• Draw an arm I t from the probability distribution p t .</p><p>• Compute the estimated gain for each arm.</p><p>• Update the cumulative gain. Then one can compute the new probability distribution over the arms. EXP3 has been used in conjunction with UCT to address games with partial observability and simultaneous moves <ref type="bibr" target="#b217">[216]</ref>, <ref type="bibr" target="#b218">[217]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Hierarchical Optimistic Optimisation for Trees</head><p>Bubeck et al. describe the Hierarchical Optimistic Optimisation (HOO) algorithm, which is a a generalisation of stochastic bandits <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. HOO constitutes an arm selection policy with improved regret bounds compared to previous results for a large class of problems.</p><p>Mansley et al. <ref type="bibr" target="#b138">[138]</ref> extend HOO into the playout planning structure to give the Hierarchical Optimistic Optimisation applied to Trees (HOOT) algorithm. The approach is similar to UCT, except that using HOO for action selection allows the algorithm to overcome the discrete action limitation of UCT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Other Bandit Enhancements</head><p>There are a number of other enhancements to banditbased methods which have not necessarily been used in an MCTS setting. These include UCB-V, PAC-UCB, Gaussian UCB, Meta-Bandits, Hierarchical Bandits, UCB(α), and so on. See also the bandit-based active learner (4.3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Selection Enhancements</head><p>Many enhancements alter the tree policy to change the way MCTS explores the search tree. Generally, selection assigns some numeric score to each action in order to balance exploration with exploitation, for example the use of UCB for node selection in UCT. In many domains it has proved beneficial to influence the score for each action using domain knowledge, to bias the search towards/away from certain actions and make use of other forms of reward estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">First Play Urgency</head><p>The MCTS algorithm specifies no way of determining the order in which to visit unexplored nodes. In a typical implementation, UCT visits each unvisited action once in random order before revisiting any using the UCB1 formula. This means that exploitation will rarely occur deeper in the tree for problems with large branching factors.</p><p>First play urgency (FPU) is a modification to MCTS proposed by Gelly et al. <ref type="bibr" target="#b94">[95]</ref> to address this issue, by assigning a fixed value to score unvisited nodes and using the UCB1 formula to score visited nodes. By tuning this fixed value, early exploitations are encouraged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Decisive and Anti-Decisive Moves</head><p>Teytaud and Teytaud <ref type="bibr" target="#b216">[215]</ref> demonstrate the benefit of decisive and anti-decisive moves for the connection game Havannah. Here, a decisive move is one that leads immediately to a win, and an anti-decisive move is one that prevents the opponent from making a decisive move on their next turn. The selection and simulation policies are replaced with the following policy: if either player has a decisive move then play it; otherwise, revert to the standard policy.</p><p>Teytaud and Teytaud <ref type="bibr" target="#b216">[215]</ref> show that this modification significantly increases playing strength, even when the increased computational cost of checking for decisive moves is taken into account. This approach is reminiscent of the pre-search handling of winning and losing moves suggested earlier <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Move Groups</head><p>In some games, it may be the case that the branching factor is large but many moves are similar. In particular, MCTS may need a lot of simulation to differentiate between moves that have a highly correlated expected reward. One way of reducing the branching factor to allow exploitation of correlated actions is to use move groups. This creates an extra decision layer in which all possible actions are collected into groups and UCB1 is used to select which of these groups to pick a move from. This idea was proposed in <ref type="bibr" target="#b62">[63]</ref> and was shown to be beneficial for the game Go. In addition, the use of transpositions allows information to be shared between these extra nodes where the state is unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Transpositions</head><p>MCTS naturally builds a search tree, but in many cases the underlying games can be represented as directed acyclic graphs (DAGs), since similar states can be reached through different sequences of move. The search tree is typically much larger than the DAG and two completely different paths from the root of the tree to a terminal state may traverse the same edge in the game's DAG. Hence extra information can be extracted from each simulation by storing the statistics for each edge in the DAG and looking these up during action selection. Whenever an identical state/action pair appears in the MCTS tree, this is referred to as a transposition. The use of transposition statistics can be considered as an enhancement to both the selection and backpropagation steps. Methods for making use of transpositions with MCTS are explored in <ref type="bibr" target="#b62">[63]</ref> and further covered in Section 6.2.4.</p><p>Transposition tables will have greater benefit for some games than others. Transposition tables were used in conjunction with MCTS for the game Arimaa by Kozlek <ref type="bibr" target="#b122">[122]</ref>, which led to a measurable improvement in performance. Transpositions were also used in a General Game Playing (GGP) context by Méhat et al. <ref type="bibr" target="#b144">[144]</ref>, giving an equivalent or better playing strength in all domains tested. Saffidine further explores the benefits of transposition tables to GGP in his thesis <ref type="bibr" target="#b182">[181]</ref>. Saffidine et al. <ref type="bibr" target="#b183">[182]</ref> also demonstrate the successful extension of MCTS methods to DAGs for correctly handling transpositions for the simple LeftRight game (7.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Progressive Bias</head><p>Progressive bias describes a technique for adding domain specific heuristic knowledge to MCTS <ref type="bibr" target="#b59">[60]</ref>. When a node has been visited only a few times and its statistics are not reliable, then more accurate information can come from a heuristic value H i for a node with index i from the current position. A new term is added to the MCTS selection formula of the form:</p><formula xml:id="formula_13">f (n i ) = H i n i + 1</formula><p>where the node with index i has been visited n i times. As the number of visits to this node increases, the influence of this number decreases.</p><p>One advantage of this idea is that many games already have strong heuristic functions, which can be easily injected into MCTS. Another modification used in <ref type="bibr" target="#b59">[60]</ref> and <ref type="bibr" target="#b233">[232]</ref> was to wait until a node had been visited a fixed number of times before calculating H i . This is because some heuristic functions can be slow to compute, so storing the result and limiting the number of nodes that use the heuristic function leads to an increase in the speed of the modified MCTS algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6">Opening Books</head><p>Opening books 16 have been used to improve playing strength in artificial players for many games. It is possible to combine MCTS with an opening book, by employing the book until an unlisted position is reached. Alternatively, MCTS can be used for generating an opening book, as it is largely domain independent. Strategies for doing this were investigated by Chaslot et al. <ref type="bibr" target="#b55">[56]</ref> using their Meta-MCTS approach (4.9.4). Their self-generated opening books improved the playing strength of their program MOGO for 9 × 9 Go.</p><p>Audouard et al. <ref type="bibr" target="#b11">[12]</ref> also used MCTS to generate an opening book for Go, using MOGO to develop a revised opening book from an initial handcrafted book. This opening book improved the playing strength of the program and was reported to be consistent with expert Go knowledge in some cases. Kloetzer <ref type="bibr" target="#b115">[115]</ref> demonstrates the use of MCTS for generating opening books for the game of Amazons (7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.7">Monte Carlo Paraphrase Generation (MCPG)</head><p>Monte Carlo Paraphrase Generation (MCPG) is similar to plain UCT except that the maximum reachable score for each state is used for selection rather than the (average) score expectation for that state <ref type="bibr" target="#b61">[62]</ref>. This modification is so named by Chevelu et al. due to its application in generating paraphrases of natural language statements (7.8.5). <ref type="bibr" target="#b15">16</ref>. Databases of opening move sequences of known utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.8">Search Seeding</head><p>In plain UCT, every node is initialised with zero win and visits. Seeding or "warming up" the search tree involves initialising the statistics at each node according to some heuristic knowledge. This can potentially increase playing strength since the heuristically generated statistics may reduce the need for simulations through that node. The function for initialising nodes can be generated either automatically or manually. It could involve adding virtual win and visits to the counts stored in the tree, in which case the prior estimates would remain permanently. Alternatively, some transient estimate could be used which is blended into the regular value estimate as the node is visited more often, as is the case with RAVE (5.3.5) or Progressive Bias (5.2.5).</p><p>For example, Szita et al. seeded the search tree with "virtual wins", to significantly improve the playing strength but required hand-tuning to set the appropriate number of virtual wins for each action. Gelly and Silver <ref type="bibr" target="#b91">[92]</ref> investigated several different methods for generating prior data for Go and found that prior data generated by a function approximation improved play the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.9">Parameter Tuning</head><p>Many MCTS enhancements require the optimisation of some parameter, for example the UCT exploration constant C p or the RAVE constant V (5.3.5). These values may need adjustment depending on the domain and the enhancements used. They are typically adjusted manually, although some approaches to automated parameter tuning have been attempted.</p><p>The exploration constant C p from the UCT formula is one parameter that varies between domains. For high performance programs for both Go <ref type="bibr" target="#b54">[55]</ref> and Hex <ref type="bibr" target="#b7">[8]</ref> it has been observed that this constant should be zero (no exploration) when history heuristics such as AMAF and RAVE are used (5.3), while other authors use non-zero values of C p which vary between domains. There have been some attempts to automatically tune this value online such as those described by Kozelek <ref type="bibr" target="#b122">[122]</ref>.</p><p>Given a large set of enhancement parameters there are several approaches to finding optimal values, or improving hand-tuned values. Guillaume et al. used the Cross-Entropy Method to fine tune parameters for the Go playing program MANGO <ref type="bibr" target="#b57">[58]</ref>. Cross Entropy Methods were also used in combination with handtuning by Chaslot et al. for their Go program MOGO <ref type="bibr" target="#b54">[55]</ref>, and neural networks have been used to tune the parameters of MOGO <ref type="bibr" target="#b56">[57]</ref>, using information about the current search as input. Another approach called dynamic exploration, proposed by Bourki et al. <ref type="bibr" target="#b24">[25]</ref>, tunes parameters based on patterns in their Go program MOGO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.10">History Heuristic</head><p>There have been numerous attempts to improve MCTS using information about moves previously played. The idea is closely related to the history heuristic <ref type="bibr" target="#b194">[193]</ref>, and is described by Kozelek <ref type="bibr" target="#b122">[122]</ref> as being used on two levels:</p><p>• Tree-tree level: Using history information to improve action selection in the MCTS tree.</p><p>• Tree-playout level: Using history information to improve the simulation policy (6.1). One approach at the tree-tree level was a grandfather heuristic approach suggested by Gelly and Silver <ref type="bibr" target="#b91">[92]</ref>. History information was used to initialise the action value estimates for new nodes, but was not as effective as other initialisation methods. Kozelek <ref type="bibr" target="#b122">[122]</ref> also used a history-based approach at the tree-tree level for the game Arimaa <ref type="bibr">(7.3)</ref>. A history bonus was given to the bandit score calculated during action selection and the score for an action was updated whenever it was selected independent of depth, giving a significant improvement.</p><p>Finnsson <ref type="bibr" target="#b82">[83]</ref> describes the benefits of the history heuristic for seeding node values in his world champion general game player CADIAPLAYER <ref type="bibr">(7.5)</ref>. See also the use of the history heuristic for improving simulation estimates (6.1.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.11">Progressive History</head><p>Nijssen and Winands <ref type="bibr" target="#b155">[155]</ref> propose the Progressive History enhancement, which combines Progressive Bias (5.2.5) with the history heuristic by replacing the heuristic value H i in the progressive bias calculation for each node i with that node's history score, during node selection. Progressive History was shown to perform well for some multi-player board games (7.3), indicating that it may be useful for multi-player games in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">All Moves As First (AMAF)</head><p>All Moves As First (AMAF) is an enhancement closely related to the history heuristic, first proposed in the context of Monte Carlo Go. The basic idea is to update statistics for all actions selected during a simulation as if they were the first action applied. The first attempt to combine AMAF with UCT was by Gelly et al. in the context of Go <ref type="bibr" target="#b91">[92]</ref>, and AMAF heuristics have since proved very successful for Go <ref type="bibr" target="#b93">[94]</ref>.</p><p>Figure <ref type="figure">4</ref> shows the AMAF heuristic in action on a simple artificial 3 × 3 game (from <ref type="bibr" target="#b101">[101]</ref>). In this situation, UCT selects the actions C2, A1 for black and white respectively, then the simulation plays black B1, white A3 and black C3 leading to a win for black. When UCT selected C2 as a move for black, UCT could have also selected B1 and C3 as alternatives. Since these moves were used during the simulation, these nodes have their reward/visit count updated by the AMAF algorithm. Similarly, UCT selected the move A1 for white, but could have selected A3 which was used in the simulation, so the AMAF algorithm updates the reward/visit for this node too. Nodes that receive the extra AMAF update during backpropagation are marked *.</p><p>The AMAF algorithm treats all moves played during selection and simulation as if they were played on a previous selection step. This means that the reward estimate for an action a from a state s is updated whenever a is encountered during a playout, even if a was not the actual move chosen from s. Some implementations keep track of the reward estimate generated this way, as well as the usual reward estimate used in the UCT algorithm, in which case the reward estimate generated by the AMAF heuristic is referred to as the AMAF score. Several AMAF variants are listed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Permutation AMAF</head><p>This algorithm is the same as AMAF but also updates nodes that can be reached by permutations of moves in the simulation that preserve the eventual state reached <ref type="bibr" target="#b101">[101]</ref>. For example, it may be possible to permute the actions played by each player during a simulation and reach an identical terminal position. Therefore there may be other leaf nodes in the tree from which the same terminal position could have been reached by playing the same moves but in a different order. Permutation AMAF would also update these nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">α-AMAF</head><p>The α-AMAF algorithm blends the standard (UCT) score for each node with the AMAF score <ref type="bibr" target="#b101">[101]</ref>. This requires that a separate count of rewards and visits for each type of update be maintained. It is called α-AMAF since the total score for an action is:</p><formula xml:id="formula_14">αA + (1 − α)U</formula><p>where U is the UCT score and A is the AMAF score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Some-First AMAF</head><p>This approach is the same as the standard AMAF algorithm except that the history used to update nodes is truncated after the first m random moves in the simulation stage <ref type="bibr" target="#b101">[101]</ref>. If m = 0 then only actions selected in the tree are used to update nodes, similarly if m is larger than the number of moves in the simulation, this is equivalent to the AMAF algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Cutoff AMAF</head><p>In Cutoff AMAF, the AMAF algorithm is used to update statistics for the first k simulations, after which only the standard UCT algorithm is used <ref type="bibr" target="#b101">[101]</ref>. The purpose of Cutoff AMAF is to warm-up the tree with AMAF data, then use the more accurate UCT data later in the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">RAVE</head><p>Rapid Action Value Estimation (RAVE) is a popular AMAF enhancement in computer Go programs such as MOGO <ref type="bibr" target="#b91">[92]</ref>. It is similar to α-AMAF, except that the α value used at each node decreases with each visit. Instead of supplying a fixed α value, a fixed positive integer V &gt; 0 is supplied instead. Then the value of α is calculated after n visits as <ref type="bibr" target="#b101">[101]</ref>:</p><formula xml:id="formula_15">max 0, V − v(n) V</formula><p>Fig. <ref type="figure">4</ref>. The All Moves As First (AMAF) heuristic <ref type="bibr" target="#b101">[101]</ref>.</p><p>The parameter V represents the number of visits a node will have before the RAVE values are not being used at all. RAVE is a softer approach than Cutoff AMAF since exploited areas of the tree will use the accurate statistics more than unexploited areas of the tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.6">Killer RAVE</head><p>Lorentz <ref type="bibr" target="#b133">[133]</ref> describes the Killer RAVE 17 variant in which only the most important moves are used for the RAVE updates for each iteration. This was found to be more beneficial for the connection game Havannah (7.2) than plain RAVE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.7">RAVE-max</head><p>RAVE-max is an extension intended to make the RAVE heuristic more robust <ref type="bibr" target="#b219">[218]</ref>, <ref type="bibr" target="#b221">[220]</ref>. The RAVE-max update rule and its stochastic variant δ-RAVE-max were found to improve performance in degenerate cases for the Sum of Switches game (7.3) but were less successful for Go.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.8">PoolRAVE</head><p>Hoock et al. <ref type="bibr" target="#b104">[104]</ref> describe the poolRAVE enhancement, which modifies the MCTS simulation step as follows:</p><p>• Build a pool of the k best moves according to RAVE.</p><p>• Choose one move m from the pool.</p><p>• Play m with a probability p, else the default policy. PoolRAVE has the advantages of being independent of the domain and simple to implement if a RAVE mechanism is already in place. It was found to yield improvements for Havannah and Go programs by Hoock et al. <ref type="bibr" target="#b104">[104]</ref> -especially when expert knowledge is small or absent -but not to solve a problem particular to Go known as semeai.</p><p>Helmbold and Parker-Wood <ref type="bibr" target="#b101">[101]</ref> compare the main AMAF variants and conclude that:</p><p>17. So named due to similarities with the "Killer Move" heuristic in traditional game tree search.</p><p>• Random playouts provide more evidence about the goodness of moves made earlier in the playout than moves made later. • AMAF updates are not just a way to quickly initialise counts, they are useful after every playout. • Updates even more aggressive than AMAF can be even more beneficial. • Combined heuristics can be more powerful than individual heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Game-Theoretic Enhancements</head><p>If the game-theoretic value of a state is known, this value may be backed up the tree to improve reward estimates for other non-terminal nodes. This section describes enhancements based on this property. Figure <ref type="figure" target="#fig_0">5</ref>, from <ref type="bibr" target="#b236">[235]</ref>, shows the backup of proven game-theoretic values during backpropagation. Wins, draws and losses in simulations are assigned rewards of +1, 0 and −1 respectively (as usual), but proven wins and losses are assigned rewards of +∞ and −∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">MCTS-Solver</head><p>Proof-number search (PNS) is a standard AI technique for proving game-theoretic values, typically used for endgame solvers, in which terminal states are considered to be proven wins or losses and deductions chained backwards from these <ref type="bibr" target="#b3">[4]</ref>. A non-terminal state is a proven win if at least one of its children is a proven win, or a proven loss if all of its children are proven losses. When exploring the game tree, proof-number search prioritises those nodes whose values can be proven by evaluating the fewest children.</p><p>Winands et al. <ref type="bibr" target="#b236">[235]</ref>, <ref type="bibr" target="#b235">[234]</ref> propose a modification to MCTS based on PNS in which game-theoretic values 18  are proven and backpropagated up the tree. If the parent node has been visited more than some threshold T times, normal UCB selection applies and a forced loss node is 18. That is, known wins, draws or losses. never selected; otherwise, a child is selected according to the simulation policy and a forced loss node may be selected. Nijssen and Winands <ref type="bibr" target="#b155">[155]</ref> also describe a multi-player version of their MCTS-Solver (4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Monte Carlo Proof-Number Search (MC-PNS)</head><p>Saito et al. <ref type="bibr" target="#b184">[183]</ref> introduce Monte Carlo proof-number search (MC-PNS), a variant of proof-number search in which nodes that do not immediately prove a game-theoretic value are evaluated by Monte Carlo simulation. Thus MC-PNS uses Monte Carlo evaluations to guide the proof-number search and expand the nodes of the tree in a more efficient order. This allows game-theoretic values to be proven twice as quickly in computer Go experiments <ref type="bibr" target="#b184">[183]</ref>, with a quarter of the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Score Bounded MCTS</head><p>Cazenave and Saffidine <ref type="bibr" target="#b50">[51]</ref> propose an MCTS enhancement for the case of games with multiple outcomes, e.g. a win or a draw, which result in a different score. Each node has a pessimistic and optimistic bound on the score of the node from the point of view of the maximizing player. These bounds converge to the estimated score for a node with more iterations, and a node is considered solved if the two bounds become equal to the score of the node. The two bounds on the score of a node are backpropagated through the tree.</p><p>The optimistic and pessimistic bounds can be used to prove nodes from the tree, and also to bias action selection by adding the bounds to the score estimate for a node, multiplied by some constant. MCTS with these enhancements was demonstrated to reduce the number of simulations required to solve seki situations in Go (7.1) and was also shown to be beneficial for the game Connect Four (Section 7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Move Pruning</head><p>The pruning of suboptimal moves from the search tree is a powerful technique when used with minimax, for example the α-β algorithm yields significant benefits for two-player zero-sum games. Move pruning can be similarly beneficial for MCTS approaches, as eliminating obviously poor choices allows the search to focus more time on the better choices.</p><p>An advantage of pruning strategies is that many are domain-independent, making them general improvements for a range of problems. In the absence of a reliable evaluation function, two types of move pruning have been developed for use with MCTS:</p><p>• Soft pruning of moves that may later be searched and selected, and • Hard pruning of moves that will never be searched or selected. Soft pruning alleviates the risk that the best move may have been prematurely pruned and removed from consideration. However, some pruning techniques require a reliable evaluation function for states, which is not always available when using MCTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Progressive Unpruning/Widening</head><p>Progressive unpruning/widening is an example of a heuristic soft pruning technique. Progressive unpruning was proposed by Chaslot et al. <ref type="bibr" target="#b59">[60]</ref> and the related idea of progressive widening was proposed by Coulomb <ref type="bibr" target="#b70">[71]</ref>. The advantage of this idea over hard pruning is that it exploits heuristic knowledge to immediately reduce the size of the tree, but that all moves will eventually be considered (given enough time). This idea is similar to First Play Urgency (5.2.1) in that it forces earlier exploitation. Teytaud and Teytaud found that progressive widening without heuristic move ordering had little effect on playing strength for the game of Havannah <ref type="bibr" target="#b215">[214]</ref>. It was found to give a small improvement in playing strength for the Go program MOGO <ref type="bibr" target="#b128">[128]</ref>.</p><p>Couëtoux et al. describe the extension of UCT to continuous stochastic problems through the use of double progressive widening <ref type="bibr" target="#b68">[69]</ref>, in which child nodes are either revisited, added or sampled from previously seen children, depending on the number of visits. Double progressive widening worked well for toy problems for which standard UCT failed, but less so for complex realworld problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Absolute and Relative Pruning</head><p>Absolute pruning and relative pruning are two strategies proposed by Huang <ref type="bibr" target="#b106">[106]</ref> to preserve the correctness of the UCB algorithm.</p><p>• Absolute pruning prunes all actions from a position except the most visited one, once it becomes clear that no other action could become more visited. • Relative pruning uses an upper bound on the number of visits an action has received, to detect when the most visited choice will remain the most visited. Relative pruning was found to increase the win rate of the Go program LINGO against GNU GO 3.8 by approximately 3% <ref type="bibr" target="#b106">[106]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Pruning with Domain Knowledge</head><p>Given knowledge about a domain, it is possible to prune actions known to lead to weaker positions. For example, Huang <ref type="bibr" target="#b106">[106]</ref> used the concept of territory in Go to significantly increase the performance of the program LINGO against GNU GO 3.8. Domain knowledge related to predicting opponents' strategies was used by Suoju et al. for move pruning in the game Dead End for a 51.17% improvement over plain UCT <ref type="bibr" target="#b99">[99]</ref>.</p><p>Arneson et al. use domain knowledge to prune inferior cells from the search in their world champion Hex program MOHEX <ref type="bibr" target="#b7">[8]</ref>. This is computationally expensive to do, so only nodes that had been visited a certain number of times had such domain knowledge applied. An added benefit of this approach is that the analysis would sometimes solve the position to give its true game-theoretic value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Expansion Enhancements</head><p>No enhancements specific to the expansion step of the tree policy were found in the literature. The particular expansion algorithm used for a problem tends to be more of an implementation choice -typically between single node expansion and full node set expansion -depending on the domain and computational budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">OTHER ENHANCEMENTS</head><p>This section describes enhancements to aspects of the core MCTS algorithm other than its tree policy. This includes modifications to the default policy (which are typically domain dependent and involve heuristic knowledge of the problem being modelled) and other more general modifications related to the backpropagation step and parallelisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Simulation Enhancements</head><p>The default simulation policy for MCTS is to select randomly amongst the available actions. This has the advantage that it is simple, requires no domain knowledge and repeated trials will most likely cover different areas of the search space, but the games played are not likely to be realistic compared to games played by rational players. A popular class of enhancements makes the simulations more realistic by incorporating domain knowledge into the playouts. This knowledge may be gathered either offline (e.g. from databases of expert games) or online (e.g. through self-play and learning). Drake and Uurtamo describe such biased playouts as heavy playouts <ref type="bibr" target="#b76">[77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Rule-Based Simulation Policy</head><p>One approach to improving the simulation policy is to hand-code a domain specific policy. Such rule-based policies should be fast, so as not to unduly impede the simulation process; Silver discusses a number of factors which govern their effectiveness <ref type="bibr" target="#b204">[203]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Contextual Monte Carlo Search</head><p>Contextual Monte Carlo Search <ref type="bibr" target="#b104">[104]</ref>, <ref type="bibr" target="#b168">[167]</ref> is an approach to improving simulations that is independent of the domain. It works by combining simulations that reach the same areas of the tree into tiles and using statistics from previous simulations to guide the action selection in future simulations. This approach was used to good effect for the game Havannah (7.2), for which each tile described a particular pairing of consecutive moves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Fill the Board</head><p>Fill the Board is an enhancement described in <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b54">[55]</ref> designed to increase simulation diversity for the game of Go. At each step in the simulations, the Fill the Board algorithm picks N random intersections; if any of those intersections and their immediate neighbours are empty then it plays there, else it plays a random legal move. The simulation policy in this case can make use of patterns (6.1.9) and this enhancement fills up board space quickly, so these patterns can be applied earlier in the simulation.</p><p>A similar approach to board filling can be used to good effect in games with complementary goals in which exactly one player is guaranteed to win, however the board is filled. Such games include the connection games Hex and Y, as discussed in Sections 6.1.9 and 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Learning a Simulation Policy</head><p>Given a new domain, it is possible to learn a new simulation policy using generic techniques. The relationship between MCTS and TD learning was mentioned in Section 4.3.1; other techniques that learn to adjust the simulation policy by direct consideration of the simulation statistics are listed below.</p><p>Move-Average Sampling Technique (MAST) is an approach first described by Finnsson and Bj örnsson <ref type="bibr" target="#b83">[84]</ref> for the world champion general game playing program CADIAPLAYER <ref type="bibr" target="#b82">[83]</ref>. A table is maintained for each action independent of state, in which the average reward Q(a) for each action a is stored and updated during the backpropagation step. Then, during subsequent simulations, these values are used to bias action selection towards more promising moves using a Gibbs distribution. A related technique called Tree-Only MAST (TO-MAST), in which only the actions selected within the search are updated, was also proposed <ref type="bibr" target="#b85">[86]</ref>.</p><p>Predicate-Average Sampling Technique (PAST) is similar to MAST and was proposed in <ref type="bibr" target="#b85">[86]</ref>. Each state is represented as a list of predicates that hold true in that state. Then, instead of a table of average values for actions, PAST maintains a table of average values for predicate/action pairs Q p (p, a). During the backpropagation process, these values are updated for every action selected and every predicate that is true in the state in which that action was selected. As with MAST, simulations select moves according to a Gibbs distribution, here depending on the maximum value of Q p (p, a) over all predicates p for the current state. MAST biases the simulations towards moves which are good on average, whereas PAST biases the simulations towards moves which are good in a certain context. <ref type="bibr" target="#b85">[86]</ref>. This is designed for use with games specified with the Game Description Language (GDL) used for the AAAI General Game Playing competitions <ref type="bibr">(7.5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature-Average Sampling Technique (FAST) is technique related to MAST and PAST and also proposed in</head><p>First, features of the game are extracted from the game definition (in this case piece type and board format), then the TD(λ) method is used to learn the relative importance of features, and this is in turn used to calculate the Q(a) values used for a simulation policy. It was found that this technique leads to a big improvement over a random simulation policy, as long as suitable features can be recognised from the game description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.5">Using History Heuristics</head><p>The history heuristic (5.2.10) assumes that a move good in one position may be good in another, to inform action choices during the selection step. A similar approach may also be applied during the simulation step, where it is described as "using history information at the treeplayout level" <ref type="bibr" target="#b122">[122]</ref>. MAST (6.1.4) is an example of this approach.</p><p>Bouzy <ref type="bibr" target="#b25">[26]</ref> experimented with history heuristics for Go. Two versions were tested:</p><p>1) an internal heuristic that alters moves made during the playouts, and 2) an external heuristic that changes the moves selected before the playout. The external history heuristic led to a significant improvement in playing strength. Drake and Uurtamo <ref type="bibr" target="#b76">[77]</ref> investigated whether search time is better spent improving the tree policy or the simulation policy. Their scenario included using history heuristics for Go and they concluded that it was more efficient to improve the simulation policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.6">Evaluation Function</head><p>It is possible to use an evaluation function to improve the simulation policy. For example, Winands and B ¨jornsson <ref type="bibr" target="#b233">[232]</ref> test several strategies for designing a simulation policy using an evaluation function for the board game Lines of Action (7.2). They found the most successful strategy to be one that initially uses the evaluation function to avoid bad moves, but later in the simulation transitions to greedily selecting the best move.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.7">Simulation Balancing</head><p>Silver describes the technique of simulation balancing using gradient descent to bias the policy during simulations <ref type="bibr" target="#b204">[203]</ref>. While it has been observed that improving the simulation policy does not necessarily lead to strong play <ref type="bibr" target="#b91">[92]</ref>, Silver and Tesauro demonstrate techniques for learning a simulation policy that works well with MCTS to produce balanced 19 if not strong play <ref type="bibr" target="#b204">[203]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.8">Last Good Reply (LGR)</head><p>Another approach to improving simulations is the Last Good Reply (LGR) enhancement described by Drake <ref type="bibr" target="#b74">[75]</ref>. Each move in a game is considered a reply to the previous move, and deemed successful if the player who makes the reply goes on to win. For each move, the last successful reply is stored and used after subsequent occurrences of that move. Since only one reply is stored per move, later replies will overwrite previous ones.</p><p>During the simulations, each player will play the last good reply stored if it is legal and otherwise use the default policy. This is referred to as the LGR-1 policy; Drake also defines a variant LGR-2 in <ref type="bibr" target="#b74">[75]</ref> which stores replies for the last two moves and uses LGR-1 if there is no LGR-2 entry for the last two moves.</p><p>Baier and Drake <ref type="bibr" target="#b16">[17]</ref> propose an extension to LGR-1 and LGR-2 called Last Good Reply with Forgetting (LGRF). In this context, "forgetting" means removing a stored reply if that reply leads to a loss during the last simulation. Two corresponding enhancements, LGRF-1 and LGRF-2, include forgetting. LGR enhancements were shown to be an improvement over the default policy for 19 × 19 Go, and storing a reply to the last two moves provided more benefit when forgetting was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.9">Patterns</head><p>In terms of board games such as Go, a pattern is a small non-empty section of the board or a logical test upon it. Patterns may also encode additional information such as the player to move, and are typically incorporated into <ref type="bibr" target="#b18">19</ref>. Games in which errors by one player are on average cancelled out by errors by the opponent on their next move <ref type="bibr" target="#b204">[203]</ref>. Fig. <ref type="figure">6</ref>. Patterns for a cut move in Go <ref type="bibr" target="#b95">[96]</ref>.</p><p>simulations by detecting pattern matches with the actual board position and applying associated moves.</p><p>For example, Figure <ref type="figure">6</ref> from <ref type="bibr" target="#b95">[96]</ref> shows a set of 3 × 3 patterns for detecting cut moves in Go. The first pattern must be matched and the other two not matched for the move to be recognised. Drake and Uurtamo <ref type="bibr" target="#b76">[77]</ref> suggest there may be more to gain from applying heuristics such as patterns to the simulation policy rather than the tree policy for Go.</p><p>The 3 × 3 patterns described by Wang and Gelly <ref type="bibr" target="#b229">[228]</ref> vary in complexity, and can be used to improve the simulation policy to make simulated games more realistic. Wang and Gelly matched patterns around the last move played to improve the playing strength of their Go program MOGO <ref type="bibr" target="#b229">[228]</ref>. Gelly and Silver <ref type="bibr" target="#b91">[92]</ref> used a reinforcement learning approach to improve the simulation policy, specifically a function approximator Q RLGO (s, a), which applied linear weights to a collection of binary features. 20 Several policies using this information were tested and all offered an improvement over a random policy, although a weaker handcrafted policy was stronger when used with UCT.</p><p>Coulom <ref type="bibr" target="#b70">[71]</ref> searched for useful patterns in Go by computing Elo ratings for patterns, improving their Go program CRAZY STONE. Hoock and Teytaud investigate the use of Bandit-based Genetic Programming (BGP) to automatically find good patterns that should be more simulated and bad patterns that should be less simulated for their program MOGO, achieving success with 9 × 9 Go but less so with 19 × 19 Go <ref type="bibr" target="#b105">[105]</ref>.</p><p>Figure <ref type="figure">7</ref> shows a bridge pattern that occurs in connection games such as Hex, Y and Havannah (7.2). The two black pieces are virtually connected as an intrusion by white in either cell can be answered by black at the other cell to restore the connection. Such intrusions can be detected and completed during simulation to significantly improve playing strength, as this mimics moves that human players would typically perform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Backpropagation Enhancements</head><p>Modifications to the backpropagation step typically involve special node updates required by other enhancement methods for forward planning, but some constitute enhancements in their own right. We describe those not explicitly covered in previous sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Weighting Simulation Results</head><p>Xie and Liu <ref type="bibr" target="#b238">[237]</ref> observe that some simulations are more important than others. In particular, simulations performed later in the search tend to be more accurate than those performed earlier, and shorter simulations tend to be more accurate than longer ones. In light of this, Xie and Liu propose the introduction of a weighting factor when backpropagating simulation results <ref type="bibr" target="#b238">[237]</ref>.</p><p>Simulations are divided into segments and each assigned a positive integer weight. A simulation with weight w is backpropagated as if it were w simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Score Bonus</head><p>In a normal implementation of UCT, the values backpropagated are in the interval [0, 1], and if the scheme only uses 0 for a loss and 1 for a win, then there is no way to distinguish between strong wins and weak wins. One way of introducing this is to backpropagate a value in the interval [0, γ] for a loss and [γ, 1] for a win with the strongest win scoring 1 and the weakest win scoring γ. This scheme was tested for Sums Of Switches (7.3) but did not improve playing strength <ref type="bibr" target="#b220">[219]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Decaying Reward</head><p>Decaying reward is a modification to the backpropagation process in which the reward value is multiplied by some constant 0 &lt; γ ≤ 1 between each node in order to weight early wins more heavily than later wins. This was proposed alongside UCT in <ref type="bibr" target="#b119">[119]</ref>, <ref type="bibr" target="#b120">[120]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Transposition Table Updates</head><p>Childs et al. <ref type="bibr" target="#b62">[63]</ref> discuss a variety of strategies -labelled UCT1, UCT2 and UCT3 -for handling transpositions (5.2.4), so that information can be shared between different nodes corresponding to the same state. Each variation showed improvements over its predecessors, although the computational cost of UCT3 was large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Parallelisation</head><p>The independent nature of each simulation in MCTS means that the algorithm is a good target for parallelisation. Parallelisation has the advantage that more simulations can be performed in a given amount of time and the wide availability of multi-core processors can be exploited. However, parallelisation raises issues such as the combination of results from different sources in a single search tree, and the synchronisation of threads of Fig. <ref type="figure">8</ref>. Parallelisation approaches for MCTS <ref type="bibr" target="#b58">[59]</ref>.</p><p>different speeds over a network. This section describes methods of parallelising MCTS and addressing such issues. Figure <ref type="figure">8</ref> shows the main parallelisation approaches for MCTS, as described by Chaslot et al. <ref type="bibr" target="#b58">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Leaf Parallelisation</head><p>Leaf parallelisation as defined in <ref type="bibr" target="#b58">[59]</ref> involves performing multiple simultaneous simulations every time the MCTS tree policy reaches (or creates) a leaf node. The idea is to collect better statistics at each leaf by achieving a better initial estimate. Cazenave and Jouandeau call this scheme at-the-leaves parallelisation <ref type="bibr" target="#b46">[47]</ref>. One problem is that the simulations may take differing lengths of time, hence the algorithm is limited to waiting for the longest simulation to finish. Kato and Takeuchi <ref type="bibr" target="#b113">[113]</ref> describe how leaf parallelisation can be implemented in a client-server network architecture, with a single client executing the MCTS search and calling upon several servers to perform simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Root Parallelisation</head><p>Root parallelisation <ref type="bibr" target="#b58">[59]</ref> is sometimes called multi-tree MCTS because multiple MCTS search trees are built simultaneously (i.e. parallelised at the root). Usually the information from the first layer in each tree is used to inform the move chosen by the algorithm. One advantage of this approach is that each thread can run for a fixed length of time and stop at any moment. Note that UCT with root parallelisation is not algorithmically equivalent to plain UCT, but is equivalent to Ensemble UCT (4.6.1).</p><p>Soejima et al. analyse the performance of root parallelisation in detail <ref type="bibr" target="#b206">[205]</ref>. They provide evidence that a majority voting scheme gives better performance than the conventional approach of playing the move with the greatest total number of visits across all trees.</p><p>Cazenave and Jouandeau also describe root parallelisation under the name single-run parallelisation <ref type="bibr" target="#b46">[47]</ref> and a related scheme called multiple-runs parallelisation in which the statistics for moves from the root of each tree are periodically shared between processes. Multiple-runs parallelisation is similar to the slow root parallelisation of Bourki et al. <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Tree Parallelisation</head><p>Tree parallelisation is a parallelisation process which involves simultaneous MCTS simulation steps on the same tree <ref type="bibr" target="#b58">[59]</ref>. Care must be taken to protect the tree from simultaneous access by different threads; each thread must gain exclusive access to a subtree of the whole search tree and the other threads must explore other areas until the lock is released. One scheme proposed in <ref type="bibr" target="#b58">[59]</ref> makes use of a global lock (mutex) at the root node. This would be a reasonable approach if the simulations took much longer than traversing the tree, since one thread can traverse or update the tree while others perform simulations. Another scheme uses local locks (mutexes) on each internal node, which are locked and unlocked every time a thread visits a node.</p><p>One issue with tree parallelisation is that each thread is likely to traverse the tree in mostly the same way as the others. One suggested solution is to assign a temporary "virtual loss" to a node when it is first encountered during action selection <ref type="bibr" target="#b58">[59]</ref>. This encourages different threads to select different nodes whilst any nodes that are clearly better than the others will still be preferred. This virtual loss is then removed immediately prior to the backpropagation step to restore the tree statistics.</p><p>Bourki et al. suggest a variation called slow tree parallelisation, in which statistics are synchronised between trees periodically and only on parts of the tree 21 <ref type="bibr" target="#b23">[24]</ref>. This is better suited to implementation in a messagepassing setting, where communication between processes is limited, e.g. when parallelising across clusters of machines. Bourki et al. find that slow tree parallelisation slightly outperforms slow root parallelisation, despite the increased communication overheads of the former. The idea of periodically synchronising statistics between trees is also explored in <ref type="bibr" target="#b90">[91]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">UCT-Treesplit</head><p>Schaefers and Platzner <ref type="bibr" target="#b193">[192]</ref> describe an approach they call UCT-Treesplit for performing a single MCTS search efficiently across multiple computer nodes. This allows an equal distribution of both the work and memory load among all computational nodes within distributed memory. Graf et al. <ref type="bibr" target="#b97">[98]</ref> demonstrate the application of UCT-Treesplit in their Go program GOMORRA to achieve high-level play. GOMORRA scales up to 16 nodes before diminishing returns reduce the benefit of splitting further, which they attribute to the high number of simulations being computed in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.5">Threading and Synchronisation</head><p>Cazenave and Jouandeau <ref type="bibr" target="#b47">[48]</ref> describe a parallel Master-Slave algorithm for MCTS, and demonstrate consistent improvement with increasing parallelisation until 16 slaves are reached. 22 The performance of their 9 × 9 Go program increases from 40.5% with one slave to 70.5% with 16 slaves against GNU GO 3.6.</p><p>Enzenberger and M üller <ref type="bibr" target="#b79">[80]</ref> describe an approach to multi-threaded MCTS that requires no locks, despite each thread working on the same tree. The results showed that this approach has much better scaling on multiple threads than a locked approach.</p><p>Segal <ref type="bibr" target="#b196">[195]</ref> investigates why the parallelisation of MCTS across multiple machines has proven surprisingly difficult. He finds that there is an upper bound on the improvements from additional search in single-threaded scaling for FUEGO, that parallel speedup depends critically on how much time is given to each player, and that MCTS can scale nearly perfectly to at least 64 threads when combined with virtual loss, but without virtual loss scaling is limited to just eight threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Considerations for Using Enhancements</head><p>MCTS works well in some domains but not in others. The many enhancements described in this section and the previous one also have different levels of applicability to different domains. This section describes efforts to understand situations in which MCTS and its enhancements may or may not work, and what conditions might cause problems. <ref type="bibr" target="#b20">21</ref>. For example, only on nodes above a certain depth or with more than a certain number of visits.</p><p>22. At which point their algorithm is 14 times faster than its sequential counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Consistency</head><p>Heavily modified MCTS algorithms may lead to incorrect or undesirable behaviour as computational power increases. An example of this is a game played between the Go program MOGO and a human professional, in which MOGO incorrectly deduced that it was in a winning position despite its opponent having a winning killer move, because that move matched a number of very bad patterns so was not searched once <ref type="bibr" target="#b18">[19]</ref>. Modifying MCTS enhancements to be consistent can avoid such problems without requiring that the entire search tree eventually be visited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Parameterisation of Game Trees</head><p>It has been observed that MCTS is successful for tricktaking card games, but less so for poker-like card games. Long et al. <ref type="bibr" target="#b130">[130]</ref> define three measurable parameters of game trees and show that these parameters support this view. These parameters could also feasibly be used to predict the success of MCTS on new games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">Comparing Enhancements</head><p>One issue with MCTS enhancements is how to measure their performance consistently. Many enhancements lead to an increase in computational cost which in turn results in fewer simulations per second; there is often a tradeoff between using enhancements and performing more simulations.</p><p>Suitable metrics for comparing approaches include:</p><p>• Win rate against particular opponents.</p><p>• Elo 23 ratings against other opponents.</p><p>• Number of iterations per second.</p><p>• Amount of memory used by the algorithm. Note that the metric chosen may depend on the reason for using a particular enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">APPLICATIONS</head><p>Chess has traditionally been the focus of most AI games research and been described as the "drosophila of AI" as it had -until recently -been the standard yardstick for testing and comparing new algorithms <ref type="bibr" target="#b225">[224]</ref>. The success of IBM's DEEP BLUE against grandmaster Gary Kasparov has led to a paradigm shift away from computer Chess and towards computer Go. As a domain in which computers are not yet at the level of top human players, Go has become the new benchmark for AI in games <ref type="bibr" target="#b123">[123]</ref>.</p><p>The most popular application of MCTS methods is to games and of these the most popular application is to Go; however, MCTS methods have broader use beyond games. This section summarises the main applications of MCTS methods in the literature, including computer Go, other games, and non-game domains. <ref type="bibr" target="#b22">23</ref>. A method for calculating relative skill levels between players that is widely used for Chess and Go, named after Arpad Elo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Go</head><p>Go is a traditional board game played on the intersections of a square grid, usually 19×19. Players alternately place stones on the board; orthogonally adjacent stones form groups, which are captured if they have no liberties (orthogonally adjacent empty spaces). The game ends when both players pass, and is won by the player who controls the most board territory.</p><p>Compared with Chess, strong AI methods for Go are a hard problem; computer Go programs using α-β search reached the level of a strong beginner by around 1997, but stagnated after that point until 2006, when the first programs using MCTS were implemented. Since then, progress has been rapid, with the program MOGO beating a professional player on a 9 × 9 board in 2008 <ref type="bibr" target="#b128">[128]</ref> and on a large board (with a large handicap) also in 2008. This success is also summarised in <ref type="bibr" target="#b129">[129]</ref>. Today, the top computer Go programs all use MCTS and play at the strength of a good amateur player. Computer Go tournaments are also dominated by MCTS players <ref type="bibr" target="#b127">[127]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Evaluation</head><p>There are several obstacles to making strong AI players for Go; games are long (around 200 moves) and have a large branching factor (an average of 250 legal plays per move), which poses a challenge for traditional AI techniques that must expand every node. However, a bigger obstacle for traditional search techniques is the lack of a good static evaluation function for non-terminal nodes <ref type="bibr" target="#b60">[61]</ref>. Evaluation functions are problematic for several reasons:</p><p>• A piece placed early in the game may have a strong influence later in the game, even if it will eventually be captured <ref type="bibr" target="#b75">[76]</ref>. • It can be impossible to determine whether a group will be captured without considering the rest of the board. • Most positions are dynamic, i.e. there are always unsafe stones on the board <ref type="bibr" target="#b69">[70]</ref>. MCTS programs avoid these issues by using random simulations and naturally handling problems with delayed rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Agents</head><p>It is indicative of the power of MCTS that over three dozen of the leading Go programs now use the algorithm. Of particular note are:</p><p>• MOGO <ref type="bibr" target="#b89">[90]</ref> [55], the first Go player to use MCTS and still an innovation in the field. It was the first program to use RAVE (5.3) and sequence-like patterns (6.1.9) and is currently the only top Go program using the Fill the Board technique (6.1.3). • CRAZY STONE <ref type="bibr" target="#b71">[72]</ref> was the first Go program using MCTS to win a tournament, and the first to beat a professional player with less than a 9 stone handicap. CRAZY STONE uses AMAF with a learned pattern library and other features to improve the default policy and perform progressive widening. • LEELA was the first commercial Go program to embrace MCTS, though also one of the weaker ones.</p><p>• FUEGO <ref type="bibr" target="#b78">[79]</ref> was the first program to beat a professional Go player in an even 9 × 9 game as white, and uses RAVE. At the 15th Computer Olympiad, ERICA won the 19 × 19 category using RAVE with progressive bias (5.2.5), a learned 3 × 3 pattern library <ref type="bibr" target="#b107">[107]</ref> and sophisticated time management <ref type="bibr" target="#b108">[108]</ref>. Commercial programs MYGOFRIEND and MANY FACES OF GO won the 9 × 9 and 13 × 13 categories respectively; both use MCTS, but no other details are available. The Fourth UEC Cup was won by FUEGO, with MCTS players ZEN and ERICA in second and third places; ZEN uses RAVE and a fullboard probabilistic model to guide playouts. Table <ref type="table" target="#tab_3">2</ref> from <ref type="bibr" target="#b93">[94]</ref> shows the relative Elo rankings of the main 9 × 9 Go programs, both MCTS and non-MCTS. FUEGO GB PROTOTYPE 24 produced excellent results against human experts for 9 × 9 Go <ref type="bibr" target="#b148">[148]</ref>. While its performance was less successful for 13×13, M üller observes that it still performed at a level that would have been unthinkable a few years ago.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Approaches</head><p>Most of the current Go programs use AMAF or RAVE <ref type="bibr">(5.3)</ref>, allowing the reuse of simulation information. Additionally, it has been observed by several authors that when using AMAF or RAVE, the exploration constant for the UCB formula should be set to zero. CRAZYSTONE and ZEN go further in extracting information from playouts, using them to build up a probabilistic score for each cell on the board. Drake <ref type="bibr" target="#b74">[75]</ref> suggests using the Last Good Reply heuristic (6.1.8) to inform simulations, modified by Baier and Drake <ref type="bibr" target="#b16">[17]</ref> to include the forgetting of bad moves. Most programs use parallelisation, often with lock-free hashtables <ref type="bibr" target="#b79">[80]</ref> and message-passing parallelisation for efficient use of clusters <ref type="bibr">(6.3)</ref>. Silver <ref type="bibr" target="#b202">[201]</ref> uses temporal difference learning methods (4.3.1) to extend the MCTS algorithm for superior results in 9×9 Go with MOGO.</p><p>Cazenave advocates the use of abstract game knowledge as an alternative to pattern-based heuristics <ref type="bibr" target="#b37">[38]</ref>. For example, his playing atari 25 heuristic, which modifies move urgency depending on whether the move threatens atari on enemy groups or addresses atari for friendly groups, was found to significantly improve play in his program GOLOIS. Cazenave also encouraged his program to spend more time on earlier and more important moves by stopping the search when each game is clearly decided.</p><p>Genetic Programming methods were used by Cazenave to evolve heuristic functions to bias move 24. A variant of FUEGO that uses machine-learnt pattern knowledge and an extra additive term in the UCT formula <ref type="bibr" target="#b148">[148]</ref>.</p><p>25. A group of stones under imminent threat of capture is in atari.  <ref type="bibr" target="#b93">[94]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Year</head><p>choice in the default policy for Go <ref type="bibr" target="#b36">[37]</ref>. These heuristic functions were in the form of symbolic expressions, and outperformed UCT with RAVE.</p><p>Cazenave <ref type="bibr" target="#b43">[44]</ref> also demonstrates how to incorporate thermography calculations into UCT to improve playing strength for 9 × 9 Go. Thermography, in the context of combinatorial game theory, is the study of a game's "temperature" as indicated by the prevalence of either warm (advantageous) moves or cool (disadvantageous) moves. It appears more beneficial to approximate the temperature separately on each game rather than globally over all games.</p><p>Huang et al. <ref type="bibr" target="#b110">[110]</ref> demonstrate how the clever use of time management policies can lead to significant improvements in 19 × 19 Go for their program ERICA. Examples of time management policies include the selfexplanatory Think Longer When Behind approach and better use of the additional time that becomes available as the opponent ponders their move.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.4">Domain Knowledge</head><p>Patterns (6.1.9) are used extensively in Go programs in both search and simulation; Chaslot et al. <ref type="bibr" target="#b54">[55]</ref> provide an excellent description of common patterns, tactical and strategic rules. Chaslot et al. <ref type="bibr" target="#b59">[60]</ref>, Huang et al. <ref type="bibr" target="#b109">[109]</ref>, Coulom <ref type="bibr" target="#b70">[71]</ref> and others all describe methods of learning patterns; Lee et al. <ref type="bibr" target="#b128">[128]</ref> show that hand-tuning pattern values is worthwhile. Aduard et al. <ref type="bibr" target="#b11">[12]</ref> show that opening books make a big improvement in play level; progressive widening or progressive unpruning (5.5.1) is used to manage the large branching factor, with patterns, tactical, and strategic rules <ref type="bibr" target="#b54">[55]</ref> used to determine the move priorities.</p><p>Wang et al. <ref type="bibr" target="#b229">[228]</ref> and Gelly et al. <ref type="bibr" target="#b95">[96]</ref> note that balanced playouts (equal strength for both players) are important and that increasing simulation strength may lead to weaker performance overall, so rules are chosen empirically to improve performance and vary from implementation to implementation. Wang and Gelly <ref type="bibr" target="#b229">[228]</ref> describe sequence-like 3 × 3 patterns which are now used widely to direct playouts, low liberty rules used to ensure sensible play when a group is in danger of being captured, and approximate rules for handling nakade 26   26. A nakade is a dead group that looks alive due to an internal space. and semeai. 27   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.5">Variants</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCTS has been applied to the following Go variants.</head><p>Random Go Helmstetter et al. <ref type="bibr" target="#b102">[102]</ref> describe an experiment where a strong human player played against MOGO (MCTS player) from randomly generated, fair positions. They conclude that randomly generated positions are harder for the human to analyse; with 180 or more random stones on the board, the artificial player becomes competitive with the human.</p><p>Phantom Go has imperfect information: each player can see only his own stones. The standard rules of Go apply, but each player reports their move to a referee, who reports back: illegal (stone may not be placed), legal (placement accepted), or a list of captured stones if captures are made. Cazenave <ref type="bibr" target="#b35">[36]</ref> applied flat Monte Carlo with AMAF to Phantom Go. Cazenave's program GOLOIS was the strongest Phantom Go program at the 2007 Computer Olympiad <ref type="bibr" target="#b45">[46]</ref>. Borsboom et al. <ref type="bibr" target="#b22">[23]</ref> found that Cazenave's technique outperforms several techniques based on UCT with determinization (4.8.1).</p><p>Blind Go follows the normal rules of Go, except that the human player cannot see the Go board. In contrast to Phantom Go, players have complete knowledge of their opponent's moves, the only source of "imperfect information" being the human player's imperfect memory. Chou et al. <ref type="bibr" target="#b64">[65]</ref>, pitting blindfold humans against the MCTS-based player MOGOTW on small boards, found that performance drops greatly for beginners, who were not able to complete a blindfold game, but noted only a small drop in play strength by the top players.</p><p>NoGo is a variant of Go in which players lose if they capture a group or are forced to suicide, which is equivalent to forbidding all captures and ending the game when there are no legal moves. Chou et al. <ref type="bibr" target="#b63">[64]</ref> implemented an artificial player for NoGo and 27. A semeai is a capturing race.</p><p>tested several standard enhancements. They conclude that RAVE and anti-decisive moves (5.2.2) lead to improvements in playing strength, slow node creation 28 leads to benefits for situations in which time or memory are the limiting factors, and that adding domain knowledge to playouts was most beneficial.</p><p>Multi-player Go is simply Go with more than two players. Cazenave <ref type="bibr" target="#b39">[40]</ref> compares several versions of UCT ranging from paranoid, 29 to one that actively seeks alliances with the other players. He concludes that in a competition, there is no best algorithm independent of the other competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.6">Future Work on Go</head><p>Rimmel et al. <ref type="bibr" target="#b171">[170]</ref> identify four types of flaws in the current generation of Go programs:</p><p>1) flaws in the opening library, 2) unwillingness to play in corners, 3) over-agressive play, and 4) handling of semeais and sekis (two groups that cannot be captured, but are not absolutely alive). Option (1) at least is an easy avenue for improvement.</p><p>Takeuchi et al. <ref type="bibr" target="#b211">[210]</ref>, <ref type="bibr" target="#b212">[211]</ref> use the relationship between the win probability obtained from playouts with actual games to calculate evaluation curves, which allow the comparison of different search methods, search parameters, and search performance at different stages of the game. These measurements promise to improve performance in Go and other MCTS applications.</p><p>Silver et al. <ref type="bibr" target="#b203">[202]</ref> describe Dyna-2, a learning system with permanent and dynamic values with parallels to RAVE, which can beat standard UCT. Sylvester et al. <ref type="bibr" target="#b209">[208]</ref> built a neural network that is stronger than standard UCT and found that a simple linear classifier was stronger still. Marcolino and Matsubara suggest that the next step in computer Go might be emergent behaviour <ref type="bibr" target="#b139">[139]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Connection Games</head><p>Connection games are games in which players strive to complete a specified type of connection with their pieces, be it connecting two or more goal regions, forming a loop, or gathering pieces into connected sets. The strongest known connection game agents at competition board sizes are currently all MCTS implementations.</p><p>Hex is the quintessential connection game, in which players strive to connect the opposite sides of a hexagonally tessellated rhombus marked with their colour with a chain of their pieces. Hex has the feature 28. A technique in which a node is not created unless its parent has already been created and it has been simulated a certain number of times.</p><p>29. The paranoid player assumes that all other players will make the moves that are most harmful towards it.</p><p>that exactly one player must win (since one player winning explicitly blocks the other from doing so), hence simulations may be performed until the board is full and the win test applied only once, for efficiency. This is similar to the Fill the Board policy used to improve simulations in Go (6.1.3).</p><p>Raiko <ref type="bibr" target="#b162">[161]</ref> first demonstrated the use of UCT for Hex in 2008, using domain knowledge in the form of bridge completion (6.1.9) during playouts. The resulting player was unranked and performed best on smaller boards, but also performed equally well on other hexagonally based connection games without modification.</p><p>Arneson et al. <ref type="bibr" target="#b7">[8]</ref> developed MOHEX, which uses UCT in conjunction with RAVE and domain knowledge in the form of inferior cell analysis to prune the search tree, and bridge completion during simulations. MoHex has won the 14th and 15th Computer Olympiads to become the reigning Computer Hex world champion <ref type="bibr" target="#b6">[7]</ref>. Other MCTS Hex players that competed include MIMHEX and YOPT <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b181">[180]</ref>.</p><p>Y, *Star and Renkula! Y is the most fundamental of connection games, in which players share the same goal of connecting the three sides of a hexagonally tessellated triangle with a chain of their pieces. *Star is one of the more complex connection games, which is played on a hexagonally tiled hexagon and involves outer cell and group scores. Renkula! is a 3D connection game played on the sphere which only exists virtually. Raiko's UCT connection game agent <ref type="bibr" target="#b162">[161]</ref> plays all of these three games and is the strongest known computer player at all board sizes.</p><p>Havannah is a connection race game with more complex rules, played on a hexagonal board tessellated by hexagons. A player wins by completing with their pieces:</p><p>1) a bridge connecting any two corners, 2) a fork connecting any three sides, and/or 3) a closed loop around any cells. The complexity of these multiple winning conditions, in addition to the large standard board of side length 10 (271 cells), makes it difficult to program an effective agent and perhaps even more difficult than Go <ref type="bibr" target="#b215">[214]</ref>. In terms of number of MCTS enhancements tested upon it, Havannah is arguably second only to Go (see Table <ref type="table">3</ref>).</p><p>K önnecke and Waldmann implemented a UCT Havannah player with AMAF and a playout horizon <ref type="bibr" target="#b121">[121]</ref>, concentrating on efficient implementation but not finding any other reliable computer opponent to test the playing strength of their agent. Teytaud and Teytaud <ref type="bibr" target="#b215">[214]</ref> then implemented another UCT player for Havannah and demonstrated that some lessons learnt from UCT for computer Go also apply in this context while some do not. Specifically, the RAVE heuristic improved playing strength while progressive widening did not. Teytaud and Teytaud <ref type="bibr" target="#b216">[215]</ref> further demonstrate the benefit of decisive and anti-decisive moves (5.2.2) to improve playing strength.</p><p>Rimmel et al. <ref type="bibr" target="#b170">[169]</ref> describe a general method for biasing UCT search using RAVE values and demonstrate its success for both Havannah and Go. Rimmel and Teytaud <ref type="bibr" target="#b168">[167]</ref> and Hook et al. <ref type="bibr" target="#b104">[104]</ref> demonstrate the benefit of Contextual Monte Carlo Search (6.1.2) for Havannah.</p><p>Lorentz <ref type="bibr" target="#b133">[133]</ref> compared five MCTS techniques for his Havannah player WANDERER and reports near-perfect play on smaller boards (size 4) and good play on medium boards (up to size 7). A computer Havannah tournament was conducted in 2010 as part of the 15th Computer Olympiad <ref type="bibr" target="#b134">[134]</ref>. Four of the five entries were MCTS-based; the entry based on α-β search came last.</p><p>Stankiewicz <ref type="bibr" target="#b207">[206]</ref> improved the performance of his MCTS Havannah player to give a win rate of 77.5% over unenhanced versions of itself by biasing move selection towards key moves during the selection step, and combining the Last Good Reply heuristic (6.1.8) with N-grams 30 during the simulation step.</p><p>Lines of Action is a different kind of connection game, played on a square 8 × 8 grid, in which players strive to form their pieces into a single connected group (counting diagonals). Winands et al. <ref type="bibr" target="#b237">[236]</ref> have used Lines of Action as a test bed for various MCTS variants and enhancements, including:</p><p>• The MCTS-Solver approach (5.4.1) which is able to prove the game-theoretic values of positions given sufficient time <ref type="bibr" target="#b235">[234]</ref>. • The use of positional evaluation functions with Monte Carlo simulations <ref type="bibr" target="#b233">[232]</ref>. • Monte Carlo α-β (4.8.7), which uses a selective twoply α-β search at each playout step <ref type="bibr" target="#b234">[233]</ref>. They report significant improvements in performance over straight UCT, and their program MC-LOA αβ is the strongest known computer player for Lines of Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Other Combinatorial Games</head><p>Combinatorial games are zero-sum games with discrete, finite moves, perfect information and no chance element, typically involving two players (2.2.1). This section summarises applications of MCTS to combinatorial games other than Go and connection games. P-Game A P-game tree is a minimax tree intended to model games in which the winner is decided by a global evaluation of the final board position, using some counting method <ref type="bibr" target="#b119">[119]</ref>. Accordingly, rewards are only associated with transitions to terminal states. Examples of such games include Go, Othello, Amazons and Clobber.</p><p>Kocsis and Szepesvári experimentally tested the performance of UCT in random P-game trees and found 30. Markovian sequences of words (or in this case moves) that predict the next action.</p><p>empirically that the convergence rates of UCT is of order B D/2 , similar to that of α-β search for the trees investigated <ref type="bibr" target="#b119">[119]</ref>. Moreover, Kocsis et al. observed that the convergence is not impaired significantly when transposition tables with realistic sizes are used <ref type="bibr" target="#b120">[120]</ref>.</p><p>Childs et al. use P-game trees to explore two enhancements to the UCT algorithm: treating the search tree as a graph using transpositions and grouping moves to reduce the branching factor <ref type="bibr" target="#b62">[63]</ref>. Both enhancements yield promising results.</p><p>Clobber is played on an 8 × 8 square grid, on which players take turns moving one of their pieces to an adjacent cell to capture an enemy piece. The game is won by the last player to move. Kocsis et al. compared flat Monte Carlo and plain UCT Clobber players against the current world champion program MILA <ref type="bibr" target="#b120">[120]</ref>. While the flat Monte Carlo player was consistently beaten by MILA, their UCT player won 44.5% of games, averaging 80,000 playouts per second over 30 seconds per move.</p><p>Othello is played on an 8 × 8 square grid, on which players take turns placing a piece of their colour to flip one or more enemy pieces by capping lines at both ends. Othello, like Go, is a game of delayed rewards; the board state is quite dynamic and expert players can find it difficult to determine who will win a game until the last few moves. This potentially makes Othello less suited to traditional search and more amenable to Monte Carlo methods based on complete playouts, but it should be pointed out that the strongest Othello programs were already stronger than the best human players even before MCTS methods were applied.</p><p>Nijssen <ref type="bibr" target="#b152">[152]</ref> developed a UCT player for Othello called MONTHELLO and compared its performance against standard α-β players. MONTHELLO played a non-random but weak game using straight UCT and was significantly improved by preprocessed move ordering, both before and during playouts. MONTHELLO achieved a reasonable level of play but could not compete against human experts or other strong AI players.</p><p>Hingston and Masek <ref type="bibr" target="#b103">[103]</ref> describe an Othello player that uses straight UCT, but with playouts guided by a weighted distribution of rewards for board positions, determined using an evolutionary strategy. The resulting agent played a competent game but could only win occasionally against the stronger established agents using traditional hand-tuned search techniques.</p><p>Osaki et al. <ref type="bibr" target="#b157">[157]</ref> apply their TDMC(λ) algorithm (4.3.2) to Othello, and report superior performance over standard TD learning methods. Robles et al. <ref type="bibr" target="#b173">[172]</ref> also employed TD methods to automatically integrate domain-specific knowledge into MCTS, by learning a linear function approximator to bias move selection in the algorithm's default policy. The resulting program demonstrated improvements over a plain UCT player but was again weaker than established agents for Othello using α-β search.</p><p>Takeuchi et al. <ref type="bibr" target="#b211">[210]</ref>, <ref type="bibr" target="#b212">[211]</ref> compare the win probabilities obtained for various search methods, including UCT, to those observed in actual games, to evaluate the effectiveness of each search method for Othello. Othello remains an open challenge for future MCTS research.</p><p>Amazons is one of the more interesting combinatorial games to emerge in recent years, remarkable for its large move complexity, having on average over 1,000 move combinations to choose from each turn. It is played on a 10 × 10 square grid, on which players take turns moving one of their amazons as per a Chess queen, then shooting an arrow from that piece along any unobstructed line (orthogonal or diagonal) to block the furthest cell. The number of playable cells thus shrinks with each turn, and the last player to move wins. Amazons has an obvious similarity to Go due to the importance of territory and connectivity.</p><p>Kocsis et al. demonstrated the superiority of plain UCT over flat Monte Carlo for Amazons <ref type="bibr" target="#b120">[120]</ref>. Similarly, Lorentz found that flat Monte Carlo performed poorly against earlier α-β players in their Amazons players INVADER and INVADERMC <ref type="bibr" target="#b132">[132]</ref>. The inclusion of UCT into INVADERMC its playing strength to defeat all previous versions and all other known Amazon agents. Forward pruning and progressive widening (5.5.1) are used to focus the UCT search on key moves.</p><p>Kloetzer has studied MCTS approaches to Amazons <ref type="bibr" target="#b116">[116]</ref>, <ref type="bibr" target="#b114">[114]</ref> culminating in a PhD thesis on the topic <ref type="bibr" target="#b115">[115]</ref>. This includes MCTS approaches to endgame analysis <ref type="bibr" target="#b117">[117]</ref>, <ref type="bibr" target="#b118">[118]</ref> and more recently the generation of opening books <ref type="bibr" target="#b115">[115]</ref>.</p><p>Arimaa is a Chess-like game designed in 1997 to defeat traditional AI analysis through its huge move space complexity; its branching factor averages between 17,000 to 50,000 move combinations per turn.</p><p>Kozelek <ref type="bibr" target="#b122">[122]</ref> describes the implementation of a UCT player for Arimaa. The basic player using straight UCT played a weak game, which was improved significantly using a technique described as the tree-tree history heuristic (5.2.10), parallelisation, and information sharing across the tree through transpositions (6.2.4). Implementing heavy playouts that incorporate tactical information and positional information from move advisers was also beneficial, but standard MCTS enhancements such as UCB tuning and RAVE were not found to work for this game. This was probably due to Arimaa's explosive combinatorial complexity requiring an infeasible number of simulations before significant learning could occur.</p><p>Kozelek <ref type="bibr" target="#b122">[122]</ref> found it preferable to handle each component sub-move as an individual action in the UCT tree, rather than entire move combinations. This reduces the search space complexity of such games with compound moves to a reasonable level, at the expense of strategic coherence within and between moves.</p><p>Khet is played on an 8 × 10 square board, on which players place and move pieces with mirrors on some sides. At the end of each turn, the mover activates a laser and captures enemy pieces that the reflected beam encounters, and wins by capturing the enemy pharaoh. The average branching factor is 69 moves and the average game length is 68 moves, giving an average game tree complexity of around 10 25 (similar to Checkers).</p><p>Nijssen <ref type="bibr" target="#b153">[153]</ref>, <ref type="bibr" target="#b154">[154]</ref> developed an MCTS Khet player using straight UCT with transposition tables but no other enhancements. Random playouts were found to take too long on average (many taking over 1,000 turns), so playouts were capped at a certain length and the game declared a draw at that point. The straight UCT player did not win a single game against their earlier α-β player.</p><p>Shogi is a Chess-like game most popular in Japan, in which captured pieces may be dropped back into play under the capturer's control during a standard move. Sato et al. <ref type="bibr" target="#b187">[186]</ref> describe a UCT Shogi player with a number of enhancements: history heuristic, progressive widening, killer moves, checkmate testing and the use of heavy playouts based on Elo rankings of move features as proposed for Go by Coulom <ref type="bibr" target="#b70">[71]</ref>. Sato et al. found that UCT without enhancement performed poorly for Shogi, but that their enhanced UCT player competed at the level of a strong amateur. However, even their enhanced program fared poorly against state of the art Shogi agents using traditional search techniques. These have now reached a high level of play due to the popularity of Shogi and it is unlikely that MCTS approaches will supersede them without significant research effort.</p><p>Takeuchi et al. <ref type="bibr" target="#b211">[210]</ref>, <ref type="bibr" target="#b212">[211]</ref> compare the win probabilities obtained for various search methods, including UCT, to those observed in actual games, to investigate the effectiveness of each method for Shogi.</p><p>Mancala is one of the oldest families of traditional combinatorial games. It is typically played on two lines of six holes from which stones are picked up and sown around subsequent holes on each turn, according to the rules for the variant being played.</p><p>Ramanujan and Selman <ref type="bibr" target="#b166">[165]</ref> implemented a UCT player for Mancala and found it to be the first known game for which minimax search and UCT both perform at a high level with minimal enhancement. It was shown that in this context, if the computational budget is fixed, then it is far better to run more UCT iterations with fewer playouts per leaf than to run fewer iterations with more playouts. Ramanujan and Selman also demonstrate the benefit of a hybrid UCT/minimax approach if some heuristic knowledge of the domain is available. Their work on comparing the performance of UCT with minimax in various search spaces (3.5) is continued elsewhere <ref type="bibr" target="#b165">[164]</ref>.</p><p>Blokus Duo is played on a 14 × 14 square grid with 21 polyominoes of size 3, 4 and 5 belonging to each player. Players take turns adding a piece to the board to touch at least one existing friendly at the corners only, and the game is won by the player to place the largest total piece area.</p><p>Shibahara and Kotani <ref type="bibr" target="#b201">[200]</ref> describe an MCTS player for Blokus Duo using plain UCT without enhancement, as the game is relatively new, hence it is difficult to reliably evaluate non-terminal board positions given the lack heuristic knowledge about it. Their program uses a sigmoid function to combine the search score and winning percentage in its search results, which was found to make more moves that they describe as "human" and "amusing" when losing. The program placed seventh out of 16 entries in a Computer Blokus Duo contest held in Japan.</p><p>Focus (also called Domination) is played on an 8 × 8 square board with truncated corners by two to four players. Players start with a number of pieces on the board, which they may stack, move and split, in order to force their opponent(s) into a position with no legal moves 31 . Nijssen and Winands <ref type="bibr" target="#b155">[155]</ref> applied their Multi-Player Monte-Carlo Tree Search Solver (4.5) and Progressive History (5.2.11) techniques to Focus to significantly improve playing strength against a standard MCTS player.</p><p>Chinese Checkers is a traditional game played on a star-shaped board by two to six players. Players aim to move their pieces from their home area to a target area on the opposite side of the board through a series of steps and jumps over adjacent pieces.</p><p>Nijssen and Winands <ref type="bibr" target="#b155">[155]</ref> also applied their Multi-Player Monte-Carlo Tree Search Solver (MP-MCTS-Solver) and Progressive History techniques to Chinese Checkers, but found that only Progressive History significantly improved playing strength against a standard MCTS player. The failure of the MP-MCTS-Solver enhancement in this case may be due to the fact that Chinese Checkers is a sudden-death game while Focus is not. In any event, Progressive History appears to be a useful enhancement for multi-player games.</p><p>Yavalath is played on a hexagonally tessellated hexagon of size 5, on which players strive to make 4-in-a-row of their colour without making 3-in-a-row beforehand. It is the first computer-designed board game to be commercially released. A plain UCT player with no enhancements beyond pre-search handling of winning and losing moves (similar to decisive and anti-decisive moves <ref type="bibr" target="#b216">[215]</ref>) played a competent game <ref type="bibr" target="#b27">[28]</ref>. <ref type="bibr" target="#b30">31</ref>. A simplified winning condition was used in the experiments to speed up the self-play trials.</p><p>Connect Four is a well known children's game played on a 7x6 square grid, in which players drop pieces down to make four in a row of their colour. Cazenave and Saffidine demonstrated the benefit of α-β-style cuts in solving the game for smaller boards using a Score Bounded MCTS (5.4.3) approach <ref type="bibr" target="#b50">[51]</ref>. Tic Tac Toe is a convenient test bed for MCTS algorithms due to its simplicity and small search space, but is rarely used as a benchmark for this very reason. One exception is Veness et al. who describe the application of ρUCT (4.10.6) in their MC-AIXA agent for Tic Tac Toe and a number of other simple games <ref type="bibr" target="#b227">[226]</ref>. Auger describes the application of MCTS methods to the partially observable case of Phantom Tic Tac Toe <ref type="bibr" target="#b15">[16]</ref>.</p><p>Sum of Switches (SOS) is an artificial number picking game played by two players, designed to represent the best-case scenario for history heuristics such as RAVE (5.3.5) for experimental purposes <ref type="bibr" target="#b220">[219]</ref>, <ref type="bibr" target="#b219">[218]</ref>, <ref type="bibr" target="#b221">[220]</ref>. A problem with the RAVE heuristic is that it can accumulate strong bias against correct moves when some moves are very good if played early, but very bad if played later in a simulation. This is a problem that does not happen in SOS. Tom and M üller <ref type="bibr" target="#b220">[219]</ref> indicate that UCT performance can be improved through careful tuning of the RAVE parameters to suit the situation, rather than necessarily focussing on parallelisation and ever greater numbers of playouts. Their extension RAVE-max (5.3.7) was found to improve RAVE performance for degenerate cases in SOS <ref type="bibr" target="#b221">[220]</ref>.</p><p>Chess and Draughts Ramanujan et al. <ref type="bibr" target="#b164">[163]</ref> describe pathologies in behaviour that result from UCT Chess players carefully constructed to explore synthetic search spaces. Surprisingly, however, there are no humancompetitive MCTS implementations reported in the literature for either Chess or Draughts, probably the western world's two most well known and widely played board games. Existing agents for these games may simply be too strong to invite competition or allow meaningful comparisons.</p><p>The commercial Chess program RYBKA provides a Monte Carlo feature to help players analyse positions <ref type="bibr" target="#b131">[131]</ref>. It is unclear exactly what "Monte Carlo" entails in this instance, but this feature can provide an alternative interpretation of degenerate board positions that confuse even strong Chess programs.</p><p>The relatively poor performance of UCT for Chess compared to other games may also be due to the occurrence of trap states (3.5) <ref type="bibr" target="#b163">[162]</ref>. Takeuchi et al. <ref type="bibr" target="#b211">[210]</ref>, <ref type="bibr" target="#b212">[211]</ref> compare the win probabilities obtained for various search methods, including UCT, to those observed in actual games, to investigate the effectiveness of each search method for Chess.</p><p>Gomoku is typically played with Go pieces on a Go board, although 15 × 15 is also a common board size. Players take turns adding a piece of their colour and win by making 5-in-a-row orthogonally or diagonally.</p><p>Gomoku is popular (especially as a recreation among Go players), simple to program, and makes an excellent test case for UCT; it is a very good game for quickly checking that a UCT implementation is working, and its similarity to Go makes it an obvious stepping stone towards a full Go program. Gomoku was an early UCT test case for several of this paper's authors, and is likely to have been an early test case for others as well. However, there is little mention of Gomoku in the literature and no specific Gomoku programs are described, possibly because the game has been solved up to at least 15 × 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Single-Player Games</head><p>Single-player (solitaire or puzzle) games are a special case of combinatorial game in which the solver competes against the null player or puzzle setter. This section describes the use of MCTS methods to solve various types of logic puzzles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Leftmost Path and Left Move Problems</head><p>The Leftmost Path and Left Move problems <ref type="bibr" target="#b41">[42]</ref> are simple artificial games designed to test the nested Monte Carlo search algorithm (4.9.2). The Leftmost Path Problem involves constructing a binary tree and scoring the number of moves on the leftmost part of the tree, hence leaf scores are extremely correlated with the structure of the search tree. This game is called LeftRight in <ref type="bibr" target="#b183">[182]</ref>, where it is used to demonstrate the successful extension of MCTS methods to DAGs for correctly handling transpositions (5.2.4). In the Left Move Problem the score of a leaf is the number of moves to the left that have been made during a game, hence leaf scores are less correlated with tree structure and NMCS is less informed.</p><p>Morpion Solitaire is an NP-hard solitaire puzzle, in which the player successively colours a vertex of an undirected graph, such that a line containing five coloured vertices can be drawn. The aim is to make as many moves as possible. Figure <ref type="figure" target="#fig_3">9</ref> from <ref type="bibr" target="#b41">[42]</ref> shows the standard board configuration. There are touching and non-touching versions of the puzzle, in which two moves in the same direction that share a circle at the end of a line are either legal or non-legal respectively.</p><p>Cazenave applied a Reflexive Monte Carlo Search (4.9.2) to solve the non-touching puzzle in 78 moves, beating the existing human record of 68 moves and AI record of 74 moves using simulated annealing <ref type="bibr" target="#b38">[39]</ref>. Cazenave then applied nested Monte Carlo search (NMCS) (4.9.2) to find an improved solution of 80 moves <ref type="bibr" target="#b41">[42]</ref>. The parallelisation of this problem technique is discussed in further detail in <ref type="bibr" target="#b48">[49]</ref>.  after about 36 days of computation <ref type="bibr" target="#b2">[3]</ref>. This record was for computer-generated solutions, since a human generated solution of 170 is known. Edelkamp et al. achieved a score of 128 using UCT with a number of enhancements in their heuristically guided swarm tree search <ref type="bibr" target="#b77">[78]</ref> and reproduced the score of 170 when the search was seeded with 111 moves. Rosin <ref type="bibr" target="#b177">[176]</ref> applied a Nested Rollout Policy Adaptation approach (4.9.3) to achieve a new record of 177 for touching Morpion Solitaire. This is the first automated method to improve upon the human-generated record that had stood for over 30 years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Crossword Construction</head><p>The construction of crosswords is technically a single-player game, though played from the designer's view rather than that player's; the goal is to devise the most amusing and challenging puzzles. Rosin's Nested Rollout Policy Adaptation (NRPA) approach (4.9.3) was also applied to crossword construction, seeking to use as many words as possible per puzzle <ref type="bibr" target="#b177">[176]</ref>.</p><p>SameGame, also called Bubble Breaker, is a logic puzzle game played on a 15 × 15 square grid which is initially coloured at random in five shades. At each turn, the player selects a coloured group of at least two orthogonally adjacent cells of the same colour, these are removed and the remaining cells collapse down to fill the gap. The game ends if the player fails to clear all cells on a given level, i.e. if some singleton groups remain. The average game length is estimated to be 64.4 moves and the average branching factor 20.7 moves, resulting in a game-tree complexity of 10 85 and state-space complexity of 10 159 <ref type="bibr" target="#b192">[191]</ref>.</p><p>Schadd et al. describe the Single-Player MCTS (SP-MCTS) variant (4.4) featuring modified backpropagation, parameter tuning and meta-search extension, and apply it to SameGame <ref type="bibr" target="#b192">[191]</ref>  <ref type="bibr" target="#b191">[190]</ref>. Their player achieved a higher score than any previous AI player <ref type="bibr" target="#b72">(73,</ref><ref type="bibr">998)</ref>. Cazenave then applied Nested Monte Carlo Search (4.9.2) to achieve an even higher score of 77,934 <ref type="bibr" target="#b41">[42]</ref>.</p><p>Matsumoto et al. later applied SP-MCTS with domain knowledge to bias move choices during playouts, for superior performance with little impact on computational time <ref type="bibr" target="#b140">[140]</ref>. Edelkamp et al. <ref type="bibr" target="#b77">[78]</ref> achieved a score of 82,604 using enhanced UCT in their heuristically guided swarm tree search (4.9.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sudoku and Kakuro</head><p>Sudoku, the popular logic puzzle, needs no introduction except perhaps to point out that it is NP-complete for arbitrarily large boards. Cazenave <ref type="bibr" target="#b41">[42]</ref> applied nested Monte Carlo search (4.9.2) to 16 × 16 Sudoku as the standard 9 × 9 puzzle proved too easy for comparison purposes and reported solution rates over 300,000 times faster than existing Forward Checking methods and almost 50 times faster than existing Iterative Sampling approaches.</p><p>Kakuro, also known as Cross Sums, is a similar logic puzzle in the same class as Sudoku that is also NP-complete. Cazenave <ref type="bibr" target="#b40">[41]</ref> applied nested Monte Carlo search (4.9.2) to 8 × 8 Kakaru puzzles for solution rates over 5,000 times faster than existing Forward Checking and Iterative Sampling approaches.</p><p>Wumpus World Asmuth and Littman <ref type="bibr" target="#b8">[9]</ref> apply their Bayesian FSSS (BFS3) technique to the classic 4x4 video game Wumpus World <ref type="bibr" target="#b179">[178]</ref>. Their BFS3 player clearly outperformed a variance-based reward bonus strategy, approaching Bayes-optimality as the program's computational budget was increased.</p><p>Mazes, Tigers and Grids Veness et al. <ref type="bibr" target="#b227">[226]</ref> describe the application of ρUCT (4.10.6) in their MC-AIXA agent to a range of puzzle games including:</p><p>• maze games, • Tiger games in which the player must select the door that maximises some reward, and • a 4 × 4 grid world game in which the player moves and teleports to maximise their score. Veness et al. <ref type="bibr" target="#b227">[226]</ref> also describe the application of ρUCT to a number of nondeterministic games, which are summarised in a following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">General Game Playing</head><p>General Game Players (GGPs) are software agents intended to play a range of games well rather than any single game expertly. Such systems move more of the mental work from the human to the machine: while the programmer may fine-tune a dedicated single-game agent to a high level of performance based on their knowledge of the game, GGPs must find good solutions to a range of previously unseen problems. This is more in keeping with the original aims of AI research to produce truly intelligent automata that can perform well when faced with complex real-world problems.</p><p>GGP is another arena that MCTS-based agents have dominated since their introduction several years ago. The use of random simulations to estimate move values is well suited to this domain, where heuristic knowledge is not available for each given game.</p><p>CADIAPLAYER was the first MCTS-based GGP player, developed by Hilmar Finnsson for his Masters Thesis in 2007 <ref type="bibr" target="#b82">[83]</ref>. The original incarnation of CADIAPLAYER used a form of history heuristic and parallelisation to improve performance, but otherwise used no enhancements such as heavy playouts. Finnsson and Bj örnsson point out the suitability of UCT for GGP as random simulations implicitly capture, in real-time, game properties that would be difficult to explicitly learn and express in a heuristic evaluation function <ref type="bibr" target="#b83">[84]</ref>. They demonstrate the clear superiority of their UCT approach over flat MC. CADIAPLAYER went on to win the 2007 and 2008 AAAI GGP competitions <ref type="bibr" target="#b20">[21]</ref>.</p><p>Finnsson and Bj örnsson added a number of enhancements for CADIAPLAYER, including the Move-Average Sampling Technique (MAST; 6.1.4), Tree-Only MAST (TO-MAST; 6.1.4), Predicate-Average Sampling Technique (PAST; 6.1.4) and RAVE (5.3.5), and found that each improved performance for some games, but no combination proved generally superior <ref type="bibr" target="#b84">[85]</ref>. Shortly afterwards, they added the Features-to-Action Sampling Technique (FAST; 6. <ref type="bibr">1.4)</ref>, in an attempt to identify common board game features using template matching <ref type="bibr" target="#b85">[86]</ref>. CADIAPLAYER did not win the 2009 or 2010 AAAI GGP competitions, but a general increase in playing strength was noted as the program was developed over these years <ref type="bibr" target="#b86">[87]</ref>.</p><p>ARY is another MCTS-based GGP player, which uses nested Monte Carlo search (4.9.2) and transposition tables (5.2.4 and 6.2.4), in conjunction with UCT, to select moves <ref type="bibr" target="#b144">[144]</ref>. Early development of ARY is summarised in <ref type="bibr" target="#b142">[142]</ref> and <ref type="bibr" target="#b143">[143]</ref>. ARY came third in the 2007 AAAI GGP competition <ref type="bibr" target="#b143">[143]</ref> and won the 2009 <ref type="bibr" target="#b145">[145]</ref> and 2010 competitions to become world champion. Méhat and Cazenave demonstrate the benefits of tree parallelisation (6.3.3) for GPP, for which playouts can be slow as games must typically be interpreted <ref type="bibr" target="#b146">[146]</ref>.</p><p>Other GGPs M öller et al. <ref type="bibr" target="#b147">[147]</ref> describe their programme CENTURIO which combines MCTS with Answer Set Programming (ASP) to play general games. CENTURIO came fourth in the 2009 AAAI GGP competition.</p><p>Sharma et al. <ref type="bibr" target="#b198">[197]</ref> describe domain-independent methods for generating and evolving domain-specific knowledge using both state and move patterns, to improve convergence rates for UCT in general games and improve performance against a plain UCT player. They then extended this approach using Reinforcement Learning and Ant Colony Algorithms, resulting in huge improvements in AI player ability <ref type="bibr" target="#b199">[198]</ref>. Mahlmamn et al. <ref type="bibr" target="#b135">[135]</ref> use an MCTS agent for testing and evaluating games described in their Strategy Game Description Game Language (SGDL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Real-time Games</head><p>MCTS has been applied to a diverse range of real-time games of varying complexity, ranging from Tron and Ms. Pac-Man to a variety of real-time strategy games akin to Starcraft. The greatest challenge facing MCTS approaches is to achieve the same level of intelligence and realistic behaviour achieved by standard methods of scripting, triggers and animations.</p><p>Tron Samothrakis et al. <ref type="bibr" target="#b185">[184]</ref> present an initial investigation into the suitability of UCT for Tron. They apply a standard implementation of MCTS to Tron: the only two game-specific modifications include the prevention of self-entrapment during the random simulation phase (1-ply look-ahead) and the distinction of a "survival mode" (once the players are physically separated), where the game essentially turns into a single-player game (a simple game tree is used here instead). They compare different MCTS variants, using UCB1, UCB-Tuned (5.1.1) and UCB-E (a modification of UCB1 due to Coquelin and Munos <ref type="bibr" target="#b67">[68]</ref>). Samothrakis et al. find that MCTS works reasonably well but that a large proportion of the random playouts produce meaningless outcomes due to ineffective play.</p><p>Den Teuling <ref type="bibr" target="#b73">[74]</ref> applies several enhancements to plain UCT for Tron, including progressive bias (5.2.5), MCTS-Solver (5.4.1), a game-specific mechanism for handling simultaneous moves (4.8.10), and game-specific simulation policies and heuristics for predicting the outcome of the game without running a complete simulation. These enhancements in various combinations increase the playing strength in certain situations, but their effectiveness is highly dependent on the layout of the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ms. Pac-Man</head><p>Numerous tree-search and Monte Carlo sampling approaches have been proposed in the past to tackle the game of Ms. Pac-Man. For example, Robles and Lucas <ref type="bibr" target="#b172">[171]</ref> expand a route-tree based on possible moves that Ms. Pac-Man can take, 32 and a flat Monte Carlo approach for the endgame strategy was proposed by Tong and Sung <ref type="bibr" target="#b223">[222]</ref> and Tong et al. <ref type="bibr" target="#b222">[221]</ref>, based on path generation and path testing components. The latter is carried out by means of Monte Carlo simulations, making some basic assumptions regarding the movement of Ms. Pac-Man and the four ghosts. This strategy, which may be used in conjunction with 32. The best path was subsequently evaluated using hand-coded heuristics.</p><p>other algorithms such as minimax or MCTS, improved the agent's score by 20%.</p><p>Samothrakis et al. <ref type="bibr" target="#b186">[185]</ref> used MCTS with a 5-player max n game tree, in which each ghost is treated as an individual player. Unlike traditional tree searches, MCTS's anytime nature lends itself nicely to the realtime constraints of the game. Knowledge about the opponent is clearly beneficial in this case, as it allows not only for a smaller tree but also much more accurate simulations in the forward projection.</p><p>Another application of MCTS to Ms. Pac-Man is due to Ikehata and Ito <ref type="bibr" target="#b111">[111]</ref>, who use MCTS to avoid pincer moves (i.e. moves where Ms. Pac-Man is trapped by ghosts covering all exits). Nguyen et al. <ref type="bibr" target="#b151">[151]</ref> also describe the use of MCTS for move planning in Ms. Pac-Man. In a follow-up paper <ref type="bibr" target="#b112">[112]</ref>, they extend their MCTS agent to use heuristics learned from game-play, such as the most dangerous places in the maze. Their improved agent won the Ms. Pac-Man screen-capture competition at IEEE CIG 2011, beating the previous best winner of the competition by a significant margin.</p><p>Pocman and Battleship Silver and Veness <ref type="bibr" target="#b205">[204]</ref> apply a POMDP (2.1.2) approach to Pocman (partially observable Pac-Man) and the classic children's game Battleship. Their players perform on a par with full-width planning methods, but require orders of magnitude less computation time and are applicable to much larger problem instances; performance is far superior to that of flat Monte Carlo. Veness et al. <ref type="bibr" target="#b227">[226]</ref> describe the application of ρUCT (4.10.6) in their MC-AIXA agent for partially observable Pac-Man.</p><p>Dead-End is a real-time predator/prey game whose participants are a cat (the player) and two dogs. The aim of the cat is to reach the exit of the board, starting from the bottom of the stage. On the other hand, the aim of the dogs is to catch the cat or to prevent it from reaching the exit within a period of time.</p><p>He et al. <ref type="bibr" target="#b100">[100]</ref> use UCT for the behaviour of the dogs in their artificial player. Their results show how the performance is better when the simulation time is higher and that UCT outperforms the flat Monte Carlo approach. The same authors <ref type="bibr" target="#b99">[99]</ref> used a more complex approach based on a KNN classifier that predicts the strategy of the player, to prune the search space in a knowledge-based UCT (KB-UCT). Results show that the pruned UCT outperforms the UCT that has no access to player strategy information.</p><p>Yang et al. <ref type="bibr" target="#b240">[239]</ref> and Fu et al. <ref type="bibr" target="#b87">[88]</ref> used MCTS methods to improve the performance of their joint ANN-based Dead End player. Zhang et al. <ref type="bibr" target="#b241">[240]</ref> deal with the problem of Dynamic Difficulty Adjustment (DDA) using a time-constrained UCT. The results show the importance of the length of simulation time for UCT. The performance obtained is seriously affected by this parameter, and it is used to obtain different difficulty levels for the game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-time Strategy (RTS) Games</head><p>Numerous studies have been published that evaluate the performance of MCTS on different variants of real-time strategy games. These games are usually modelled on well-known and commercially successful games such as Warcraft, Starcraft or Command &amp; Conquer, but have been simplified to reduce the number of available actions at any moment in time (the branching factor of the decision trees in such games may be unlimited).</p><p>Initial work made use of Monte Carlo simulations as a replacement for evaluation functions; the simulations were embedded in other algorithms such as minimax, or were used with a 1-ply look-ahead and made use of numerous abstractions to make the search feasible given the time constraints.</p><p>Wargus Balla and Fern <ref type="bibr" target="#b17">[18]</ref> apply UCT to a RTS game called Wargus. Here the emphasis is on tactical assault planning and making use of numerous abstractions, most notably the grouping of individual units. The authors conclude that MCTS is a promising approach: despite the lack of domain-specific knowledge, the algorithm outperformed baseline and human players across 12 scenarios. ORTS Naveed et al. <ref type="bibr" target="#b150">[150]</ref> apply UCT and RRTs (4.10.3) to the game engine ORTS. Both algorithms are used to find paths in the game and the authors conclude that UCT finds solutions with less search effort than RRT, although the RRT player outperforms the UCT player in terms of overall playing strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Nondeterministic Games</head><p>Nondeterministic games have hidden information and/or a random element. Hidden information may arise through cards or tiles visible to the player, but not the opponent(s). Randomness may arise through the shuffling of a deck of cards or the rolling of dice. Hidden information and randomness generally make game trees much harder to search, greatly increasing both their branching factor and depth.</p><p>The most common approach to dealing with this increase in branching factor is to use determinization, which involves sampling over the perfect information game instances that arise when it is assumed that all hidden and random outcomes are known in advance (see Section 4.8.1).</p><p>Skat is a trick-taking card game with a bidding phase. Schafer describes the UCT player XSKAT which uses information sets to handle the nondeterministic aspect of the game, and various optimisations in the default policy for both bidding and playing <ref type="bibr" target="#b195">[194]</ref>. XSKAT outperformed flat Monte Carlo players and was competitive with the best artificial Skat players that use traditional search techniques. A discussion of the methods used for opponent modelling is given in <ref type="bibr" target="#b34">[35]</ref>.</p><p>Poker Monte Carlo approaches have also been used for the popular gambling card game Poker <ref type="bibr" target="#b178">[177]</ref>. The poker game tree is too large to compute Nash strategies precisely, so states must be collected in a small number of buckets. Monte Carlo methods such as Monte Carlo Counter Factual Regret (MCCFR) <ref type="bibr" target="#b125">[125]</ref> (4.8.8) are then able to find approximate Nash equilibria. These approaches represent the current state of the art in computer Poker.</p><p>Maîtrepierre et al. <ref type="bibr" target="#b137">[137]</ref> use UCB to select strategies, resulting in global play that takes the opponent's strategy into account and results in unpredictable behaviour. Van den Broeck et al. apply MCTS methods to multiplayer no-limit Texas Hold'em Poker <ref type="bibr" target="#b224">[223]</ref>, enabling strong exploitative behaviour against weaker rule-based opponents and competitive performance against experienced human opponents.</p><p>Ponsen et al. <ref type="bibr" target="#b159">[159]</ref> apply UCT to Poker, using a learned opponent model (Section 4.8.9) to bias the choice of determinizations. Modelling the specific opponent by examining games they have played previously results in a large increase in playing strength compared to UCT with no opponent model. Veness et al. <ref type="bibr" target="#b227">[226]</ref> describe the application of ρUCT (4.10.6) to Kuhn Poker using their MC-AIXA agent. Dou Di Zhu is a popular Chinese card game with hidden information. Whitehouse et al. <ref type="bibr" target="#b231">[230]</ref> use information sets of states to store rollout statistics, in order to collect simulation statistics for sets of game states that are indistinguishable from a player's point of view. One surprising conclusion is that overcoming the problems of strategy fusion (by using expectimax rather than a determinization approach) is more beneficial than having a perfect opponent model.</p><p>Other card games such as Hearts and Spades are also interesting to investigate in this area, although work to date has only applied MCTS to their perfect information versions <ref type="bibr" target="#b208">[207]</ref>.</p><p>Klondike Solitaire is a well known single-player card game, which can be thought of as a single-player stochastic game: instead of the values of the hidden cards being fixed at the start of the game, they are determined by chance events at the moment the cards are turned over. 33  Bjarnason et al. <ref type="bibr" target="#b19">[20]</ref> apply a combination of the determinization technique of hindsight optimisation (HOP) with UCT to Klondike solitaire (Section 4.8.1). This system achieves a win rate more than twice that estimated for a human player.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Magic:</head><p>The Gathering is a top-selling two-player 33. This idea can generally be used to transform a single-player game of imperfect information into one of perfect information with stochasticity. card game. Ward and Cowling <ref type="bibr" target="#b230">[229]</ref> show bandit-based approaches using random rollouts to be competitive with sophisticated rule-based players. The rules of Magic: The Gathering are to a great extent defined by the cards in play, so the creation of strong techniques for Magic: The Gathering can be seen as an exercise in, or at least a stepping stone towards, GGP <ref type="bibr">(7.5)</ref>. 34 is a Chess variant played on three chessboards -one for each player and one for the referee -that incorporates the notion of "fog of war" as players can only see their own pieces while the opponent's pieces are in the dark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phantom Chess</head><p>Ciancarini and Favini developed an MCTS-based Phantom Chess player <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref> based on previous studies of Phantom Go (7.1.5). They tried different models from the player's and referee's perspectives, based on the partial information available to them, and used probabilities based on experience to influence moves during the playouts to simulate realistic behaviour, for unexpectedly good results.</p><p>Urban Rivals is a free internet game played by more than 10,000,000 registered users. Teytaud and Flory <ref type="bibr" target="#b218">[217]</ref> observe links between hidden information and simultaneous moves (4.8.10), in order to extend MCTS methods to this class of games and implement a UCT player for Urban Rivals. They find that UCT with EXP3 (5.1.3) outperforms plain UCT and UCT with greedy enhancements for this game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backgammon</head><p>The best current Backgammon agents use reinforcement learning on millions of offline games to learn positional evaluations, and are stronger than the best human players. The UCT-based player MCGAMMON developed by Van Lishout et al. <ref type="bibr" target="#b226">[225]</ref> only implemented a simplification of the game, but was found to correctly choose expert moves in some cases, despite making unfortunate choices in others. MCGAMMON achieved around 6,500 playouts per second and based its initial move on 200,000 playouts.</p><p>Settlers of Catan is a nondeterministic multi-player game that has won several major game design awards, and was the first "eurogame" to become widely popular outside Germany. Szita et al. <ref type="bibr" target="#b210">[209]</ref> implemented a multiplayer MCTS player (4.5) for Settlers of Catan, using domain knowledge based on players' resources and current position to bias move selection. Their program performed well against an existing artificial player, JSETTLERS, achieving victory in 49% of games and still achieving good scores in games that it lost. While the agent made generally competent moves against human players, it was found that expert human players could confidently beat it. Scotland Yard is a turn-based video game with imperfect information and fixed coalitions. Nijssen and Winands describe the application of MCTS to Scotland Yard using a coalition reduction method (4.5.1) to outperform a commercial program for this game <ref type="bibr" target="#b156">[156]</ref>.</p><p>Roshambo is a child's game more commonly known as Rock, Paper, Scissors. Veness et al. <ref type="bibr" target="#b227">[226]</ref> describe the application of ρUCT (4.10.6) to biased Roshambo using their MC-AIXA agent.</p><p>Thurn and Taxis is a German board game in the "eurogame" style for two or more players, with imperfect information and nondeterministic elements, including cards and virtual assistants. Schadd <ref type="bibr" target="#b189">[188]</ref> implemented an MCTS player for Thurn and Taxis that incorporated domain knowledge into the playout policy to improve performance (slightly) over a flat UCB implementation.</p><p>OnTop is a non-deterministic board game for two to four players. Briesemeister <ref type="bibr" target="#b26">[27]</ref> compared an MCTS OnTop player against a number of Minimax, Expectimax and flat Monte Carlo variants, and found that the MCTS implementation won 80% of games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8">Non-Game Applications</head><p>This section lists known examples of the application of MCTS methods to domains other than games. These domains include combinatorial optimisation, scheduling tasks, sample based planning, and procedural content generation. Other non-game MCTS applications are known, 35 but have not yet been published in the literature, so are not listed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.1">Combinatorial Optimisation</head><p>This sections lists applications of MCTS to combinatorial optimisation problems found in the literature.</p><p>Security Tanabe <ref type="bibr" target="#b213">[212]</ref> propose MCTS methods to evaluate the vulnerability to attacks in an imagebased authentication system. The results obtained are promising and suggest a future development of an MCTS based algorithm to evaluate the security strength of the image-based authentication systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed Integer Programming</head><p>In the study performed by Sabharwal and Samulowitz <ref type="bibr" target="#b180">[179]</ref>, UCT is applied to guide Mixed Integer Programming (MIP), comparing the performance of the UCT based node selection with that of CPLEX, a traditional MIP solver, and best-first, breadth-first and depth-first strategies, showing very promising results.</p><p>35. Including, for example, financial forecasting for the stock market and power plant management.</p><p>Travelling Salesman Problem The Travelling Salesman Problem (TSP) is addressed in <ref type="bibr" target="#b169">[168]</ref> using a nested Monte Carlo search algorithm (4.9.2) with time windows. State of the art solutions were reached up to 29 nodes, although performance on larger problems is less impressive.</p><p>The Canadian Traveller Problem (CTP) is a variation of the TSP in which some of the edges may be blocked with given probability. Bnaya et al. <ref type="bibr" target="#b21">[22]</ref> propose several new policies and demonstrate the application of UCT to the CTP, achieving near-optimal results for some graphs. <ref type="bibr">Kocsis and Szepesvári [119]</ref> apply UCT to the sailing domain, which is a stochastic shortest path (SSP) problem that describes a sailboat searching for the shortest path between two points under fluctuating wind conditions. It was found that UCT scales better for increasing problem size than other techniques tried, including asynchronous realtime dynamic programming (ARTDP) and a Markov decision process called PG-ID based on online sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sailing Domain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physics Simulations Mansely et al. apply the</head><p>Hierarchical Optimistic Optimisation applied to Trees (HOOT) algorithm (5.1.4) to a number of physics problems <ref type="bibr" target="#b138">[138]</ref>. These include the Double Integrator, Inverted Pendulum and Bicycle problems, for which they demonstrate the general superiority of HOOT over plain UCT. <ref type="bibr" target="#b67">[68]</ref> compare their BAST approach (4.2) with flat UCB for the approximation of Lipschitz functions, and observe that BAST outperforms flat UCB and is less dependent on the size of the search tree. BAST returns a good value quickly, and improves towards the optimal value as the computational budget is increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function Approximation Coquelin and Munos</head><p>Rimmel et al. <ref type="bibr" target="#b167">[166]</ref> apply the MCTS-based Threshold Ascent for Graphs (TAG) method (4.10.2) to the problem of automatic performance tuning using DFT and FFT linear transforms in adaptive libraries. They demonstrate superior performance of TAG over standard optimisation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.2">Constraint Satisfaction</head><p>This sections lists applications of MCTS methods to constraint satisfaction problems.</p><p>Constraint Problems Satomi et al. <ref type="bibr" target="#b188">[187]</ref> proposed a real-time algorithm based on UCT to solve a quantified constraint satisfaction problems (QCSP). 36 Plain UCT did not solve their problems more efficiently than random selections, so Satomi et al. added a constraint propagation technique that allows the tree to focus in the most favourable parts of the search space. This combined algorithm outperforms the results obtained 36. A QCSP is a constraint satisfaction problem in which some variables are universally quantified. by state of the art α-β search algorithms for large-scale problems <ref type="bibr" target="#b188">[187]</ref>.</p><p>Previti et al. <ref type="bibr" target="#b160">[160]</ref> investigate UCT approaches to the satisfiability of conjunctive normal form (CNF) problems. They find that their UCTSAT class of algorithms do not perform well if the domain being modelled has no underlying structure, but can perform very well if the information gathered on one iteration can successfully be applied on successive visits to the same node. <ref type="bibr">Cazenave [43]</ref> applied his nested Monte Carlo search method (4.9.2) to the generation of expression trees for the solution of mathematical problems. He achieved better results than existing methods for the Prime generating polynomials problem 37 and a finite algebra problem called the A 2 primal algebra, for which a particular discriminator term must be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mathematical Expressions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.3">Scheduling Problems</head><p>Planning is also a domain in which Monte Carlo tree based techniques are often utilised, as described below.</p><p>Benchmarks Nakhost and M üller apply their Monte Carlo Random Walk (MRW) planner (4.10.7) to all of the supported domains from the 4th International Planning Competition (IPC-4) <ref type="bibr" target="#b149">[149]</ref>. MRW shows promising results compared to the other planners tested, including FF, Marvin, YASHP and SG-Plan.</p><p>Pellier et al. <ref type="bibr" target="#b158">[158]</ref> combined UCT with heuristic search in their Mean-based Heuristic Search for anytime Planning (MHSP) method (4.10.8) to produce an anytime planner that provides partial plans before building a solution. The algorithm was tested on different classical benchmarks (Blocks World, Towers of Hanoi, Ferry and Gripper problems) and compared to some major planning algorithms (A*, IPP, SatPlan, SG Plan-5 and FDP). MHSP performed almost as well as classical algorithms on the problems tried, with some pros and cons. For example, MHSP is better than A* on the Ferry and Gripper problems but worse on Blocks World and the Towers of Hanoi.</p><p>Printer Scheduling Matsumoto et al. <ref type="bibr" target="#b140">[140]</ref> applied Single Player Monte Carlo Tree Search (4.4) to the game Bubble Breaker (7.4). Based on the good results obtained in this study, where the heuristics employed improved the quality of the solutions, the application of this technique is proposed for a re-entrant scheduling problem, trying to manage the printing process of the auto-mobile parts supplier problem. <ref type="bibr">Silver et al. [204]</ref> apply MCTS and UCT to the rock-sample problem (which simulates a Mars explorer robot that has to analyse and collect 37. Finding a polynomial that generates as many different primes in a row as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rock-Sample Problem</head><p>rocks) and two games: Battleship and Pocman (a partially observable variation of Pac-Man), showing a high degree of performance in all cases.</p><p>Production Management Problems (PMPs) can be defined as planning problems that require a parameter optimisation process. Chaslot et al. propose the use of an MCTS algorithm to solve PMPs, getting results faster than Evolutionary Planning Heuristics (EPH), reaching at least the same score in small problems and outperforming EPH in large scenarios <ref type="bibr" target="#b53">[54]</ref>. Double progressive widening (5.5.1) has been shown to work well for energy stock management and other toy problems, outperforming plain UCT with progressive widening and Q-learning methods <ref type="bibr" target="#b68">[69]</ref>, but did not work so well for complex real-world problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bus Regulation</head><p>The bus regulation problem is the task of scheduling bus waiting times so as to minimise delays for passengers <ref type="bibr" target="#b44">[45]</ref>. Nested Monte Carlo search with memorisation (4.9.2) was found to clearly outperform the other methods tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.4">Sample-Based Planning</head><p>Planners for many complex structured domains can be learned with tractable sample complexity if near optimal policies are known.</p><p>Large State Spaces Walsh et al. <ref type="bibr" target="#b228">[227]</ref> apply Forward Search Sparse Sampling (FSSS) to domains with large state spaces (4.10.1), where neither its sample nor computational efficiency is made intractable by the exponential number of states. They describe a negative case for UCT's runtime that can require exponential computation to optimise, in support of their approach.</p><p>Feature Selection To test their Feature UCT Selection (FUSE) algorithm (4.4.1), Gaudel and Sebag <ref type="bibr" target="#b88">[89]</ref> use three benchmark data sets from the NIPS 2003 FS Challenge competition in feature selection. The Arcene data set contains 10,000 features, which Gaudel and Sebag reduce to 2000 for tractability; the Madelon and Colon sets contain 500 and 2,000 features respectively. FUSE is found to achieve state of the art performance on these data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.5">Procedural Content Generation (PCG)</head><p>Browne describes ways in which MCTS methods may be extended to procedural content generation (PCG) for creative domains, such as game design, linguistics, and generative art and music <ref type="bibr" target="#b29">[30]</ref>. An important difference from the standard approach is that each search attempts to produce not a single optimal decision but rather a range of good solutions according to the target domain, for which variety and originality can be as important as quality. The fact that MCTS has an inherent restart mechanism (3.3.1) and inherently performs a local iter-ated search at each decision step makes it a promising approach for PCG tasks.</p><p>Chevelu et al. propose the Monte Carlo Paraphrase Generation (MCPG) modification to UCT (5.2.7) intended for natural language processing (NLP) tasks such as the paraphrasing of natural language statements <ref type="bibr" target="#b61">[62]</ref>.</p><p>Mahlmann et al. describe the use of UCT for content creation in a strategy game <ref type="bibr" target="#b136">[136]</ref>. This algorithm performs battle simulations as the fitness function of an evolutionary strategy, in order to fine tune the game unit types and their parameters. Again, the aim is not to produce the strongest AI player but to generate a satisfactory range of digital in-game content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">SUMMARY</head><p>The previous sections have provided a snapshot of published work on MCTS to date. In this section, we briefly reflect on key trends and possible future directions for MCTS research. Tables <ref type="table">3 and 4</ref> summarise the many variations and enhancements of MCTS and the domains to which they have been applied, divided into combinatorial games (Table <ref type="table">3</ref>) and other domains (Table <ref type="table">4</ref>).</p><p>The tables show us that UCT is by far the most widely used MCTS technique, and that Go is by far the domain for which most enhancements have been tried, followed by Havannah and General Game Playing. MCTS enhancements are generally applied to combinatorial games, while MCTS variations are generally applied to other domain types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Impact</head><p>MCTS has had a remarkable impact in the five years since researchers first used Monte Carlo simulation as a method for heuristically growing an interesting part of the search tree for a game. Generally speaking, MCTS appears to work for games and decision problems when:</p><p>• We can characterise the problem of making a good decision as a search problem on a large directed graph or tree (e.g. a state-action graph). • We can sample decisions by conducting random simulations, much faster than real-time. These simulations are (weakly) correlated with the true (expected) value of a given decision state. A good deal of the MCTS research has focussed on computer Go, spurred on by the success of MCTS players against human professionals on small boards in recent years. This success is remarkable, since humancompetitive computer Go was perceived by the AI community as an intractable problem until just a few years ago -or at least a problem whose solution was some decades away.</p><p>In the past, there have been two primary techniques for decision-making in adversarial games: minimax αβ search and knowledge-based approaches. MCTS provides an effective third way, particularly for games in which it is difficult to evaluate intermediate game states or to capture rules in sufficient detail. Hybridisation of MCTS with traditional approaches provides a rich area for future research, which we will discuss further below.</p><p>This survey has demonstrated the power of MCTS across a wide range of game domains, in many cases providing the strongest computer players to date. While minimax search has proven to be an effective technique for games where it is possible to evaluate intermediate game states, e.g. Chess and Checkers, MCTS does not require such intermediate evaluation and has proven to be a more robust and general search approach. Its success in such a wide range of games, and particularly in General Game Playing, demonstrates its potential across a broad range of decision problems. Success in non-game applications further emphasises its potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Strengths</head><p>Using MCTS, effective game play can be obtained with no knowledge of a game beyond its rules. This survey demonstrates that this is true for a wide range of games, and particularly for General Game Playing, where rules are not known in advance. With further enhancement to the tree or simulation policy, very strong play is achievable. Thus enhanced, MCTS has proven effective in domains of high complexity that are otherwise opaque to traditional AI approaches.</p><p>Enhancements may result from incorporating human knowledge, machine learning or other heuristic approaches. One of the great advantages of MCTS is that even when the information given by an enhancement is noisy or occasionally misleading, the MCTS sampling approach is often robust enough to handle this noise and produce stronger play. This is in contrast with minimax search, where the search is brittle with respect to noise in the evaluation function for intermediate states, and this is especially true for games with delayed rewards.</p><p>Another advantage of MCTS is that the forward sampling approach is, in some ways, similar to the method employed by human game players, as the algorithm will focus on more promising lines of play while occasionally checking apparently weaker options. This is especially true for new games, such as those encountered in the AAAI General Game Playing competitions, for which no strategic or heuristic knowledge exists. This "humanistic" nature of MCTS makes it easier to explain to the general public than search paradigms that operate very differently to the way in which humans search.</p><p>MCTS is often effective for small numbers of simulations, for which mistakes often appear plausible to human observers. Hence the approach is genuinely an "anytime" approach, producing results of plausibility that grows with increasing CPU time, through growing the tree asymmetrically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Weaknesses</head><p>Combining the precision of tree search with the generality of random sampling in MCTS has provided stronger decision-making in a wide range of games. However, there are clear challenges for domains where the branching factor and depth of the graph to be searched makes naive application of MCTS, or indeed any other search algorithm, infeasible. This is particularly the case for video game and real-time control applications, where a systematic way to incorporate knowledge is required in order to restrict the subtree to be searched.</p><p>Another issue arises when simulations are very CPUintensive and MCTS must learn from relatively few samples. Work on Bridge and Scrabble shows the potential of very shallow searches in this case, but it remains an open question as to whether MCTS is the best way to direct simulations when relatively few can be carried out.</p><p>Although basic implementations of MCTS provide effective play for some domains, results can be weak if the basic algorithm is not enhanced. This survey presents the wide range of enhancements considered in the short time to date. There is currently no better way than a manual, empirical study of the effect of enhancements to obtain acceptable performance in a particular domain.</p><p>A primary weakness of MCTS, shared by most search heuristics, is that the dynamics of search are not yet fully understood, and the impact of decisions concerning parameter settings and enhancements to basic algorithms are hard to predict. Work to date shows promise, with basic MCTS algorithms proving tractable to "in the limit" analysis. The simplicity of the approach, and effectiveness of the tools of probability theory in analysis of MCTS, show promise that in the future we might have a better theoretical understanding of the performance of MCTS, given a realistic number of iterations.</p><p>A problem for any fast-growing research community is the need to unify definitions, methods and terminology across a wide research field. We hope that this paper may go some way towards such unification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Research Directions</head><p>Future research in MCTS will likely be directed towards:</p><p>• Improving MCTS performance in general.</p><p>• Improving MCTS performance in specific domains.</p><p>• Understanding the behaviour of MCTS. MCTS is the most promising research direction to date in achieving human-competitive play for Go and other games which have proved intractable for minimax and other search approaches. It seems likely that there will continue to be substantial effort on game-specific enhancements to MCTS for Go and other games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.1">General-Purpose Enhancements</head><p>Many of the enhancements that have emerged through the study of Go have proven applicable across a wide range of other games and decision problems. The empirical exploration of general-purpose enhancements to the MCTS algorithm will likely remain a major area of investigation. This is particularly important for MCTS, as the approach appears to be remarkably general-purpose and robust across a range of domains. Indeed, MCTS may be considered as a high-level "meta-" technique, which has the potential to be used in conjunction with other techniques to produce good decision agents. Many of the papers surveyed here use MCTS in conjunction with other algorithmic ideas to produce strong results. If we compare other powerful "meta-" approaches such as metaheuristics and evolutionary algorithms, we can see that there is the potential for MCTS to grow into a much larger field of research in the future, capable of solving a very wide range of problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.2">MCTS Search Dynamics</head><p>Alongside the application-led study of possible enhancements, there are many questions about the dynamics of MCTS search. The theoretical and empirical work here shows promise and is ripe for further study, for example in comparing MCTS, minimax, A* and other search approaches on an empirical and theoretical level, and for understanding the impact of parameters and effective ways for (adaptively) finding suitable values. A related area is the idea of automatic methods for pruning subtrees based on their probability of containing game states that will actually be reached. While UCB1 has made the biggest impact as a bandit algorithm to date, the investigation of other bandit algorithms is an interesting area, for example when branching factor and depth of the tree is very large, or when the goal of search is to find a mixed strategy, which yields a strategy giving a probability to each of several possible decisions at each decision point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.3">Hybridisation</head><p>The flexibility of MCTS allows it to be hybridised with a range of other techniques, particularly minimax search, heuristic evaluation of intermediate game states and knowledge-based approaches. Hybridisation may allow problems that were intractable for search-based approaches to be effectively handled, in areas such as video games and real-time control. Work to date on MCTS for video games and other complex environments has focussed on easily-modelled decisions or games with fairly simple representation of state, where performing simulation playouts is straightforward. Work is needed on encoding state and incorporating human knowledge or adaptive learning approaches to create a tractable (state, action) graph in more general, complex environments.</p><p>The integration of MCTS with knowledge capture, and with data mining and other machine learning methods for automatically capturing knowledge, provides a ripe area for investigation, with the potential to provide a way forward for difficult problems in video gaming and real-time control where large volumes of data are available, e.g. from network play between human players.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.4">Dealing with Uncertainty and Hidden Information</head><p>Games with hidden information and stochastic elements often have intractably wide game trees for standard tree search approaches. Here MCTS has shown that it can create approximate Nash players that represent best-possible play, or at least play that is impossible for an opponent to exploit. The integration of MCTS with game-theoretic tools and with opponent-modelling approaches is a promising research direction, due to the importance of hidden information in video games and in practical problems in systems and economic modelling. We may use the tools of data mining and opponent modelling to infer both the hidden information and the strategy of an opponent. The tools of game theory may also prove to be effective for analysis of MCTS in this case. The need for mixed strategies in this case requires a rethink of the basic exploration/exploitation paradigm of MCTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.5">Non-Game Applications</head><p>MCTS shows great promise in non-game applications, in areas such as procedural content generation (indeed a recent issue of this journal was devoted to this topic) as well as planning, scheduling, optimisation and a range of other decision domains. For example, the introduction of an adversarial opponent provides an ability to work with "worst-case" scenarios, which may open up a new range of problems which can be solved using MCTS in safety critical and security applications, and in applications where simulation rather than optimisation is the most effective decision support tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>MCTS has become the pre-eminent approach for many challenging games, and its application to a broader range of domains has also been demonstrated. In this paper we present by far the most comprehensive survey of MCTS methods to date, describing the basics of the algorithm, major variations and enhancements, and a representative set of problems to which it has been applied. We identify promising avenues for future research and cite almost 250 articles, the majority published within the last five years, at a rate of almost one paper per week.</p><p>Over the next five to ten years, MCTS is likely to become more widely used for all kinds of challenging AI problems. We expect it to be extensively hybridised with other search and optimisation algorithms and become a tool of choice for many researchers. In addition to providing more robust and scalable algorithms, this will provide further insights into the nature of search and optimisation in difficult domains, and into how intelligent behaviour can arise from simple statistical processes. EP/I001964/1, EP/H048588/1 and EP/H049061/1, as part of the collaborative research project UCT for Games and Beyond being undertaken by Imperial College, London, and the Universities of Essex and Bradford.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Backup of proven game-theoretic values [235].</figDesc><graphic url="image-4.png" coords="21,48.96,53.14,514.08,188.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>20 .Fig. 7 .</head><label>207</label><figDesc>Fig. 7. Bridge completion for connection games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Akiyama et al. incorporated the AMAF heuristic (5.3) into NMCS to find a new world record solution of 146 moves for the touching version of the puzzle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. 80 move Morpion Solitaire solution [42].</figDesc><graphic url="image-7.png" coords="33,311.97,53.14,251.06,253.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-3.png" coords="20,48.96,53.14,514.05,178.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-6.png" coords="25,48.96,53.14,514.08,237.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table of Contents due to the breadth of material covered:</figDesc><table><row><cell>4 Variations</cell><cell></cell></row><row><cell>4.1 Flat UCB</cell><cell></cell></row><row><cell>4.2 Bandit Algorithm for Smooth Trees</cell><cell></cell></row><row><cell>4.3 Learning in MCTS: TDL; TDMC(λ); BAAL</cell><cell></cell></row><row><cell>4.4 Single-Player MCTS: FUSE</cell><cell></cell></row><row><cell>4.5 Multi-player MCTS: Coalition Reduction</cell><cell></cell></row><row><cell>4.6 Multi-agent MCTS: Ensemble UCT</cell><cell></cell></row><row><cell>4.7 Real-time MCTS</cell><cell></cell></row><row><cell>4.8 Nondeterministic MCTS: Determinization; HOP;</cell><cell></cell></row><row><cell>Sparse UCT; ISUCT; Multiple MCTS; UCT+; MC αβ ;</cell><cell></cell></row><row><cell>MCCFR; Modelling; Simultaneous Moves</cell><cell></cell></row><row><cell>4.9 Recursive Approaches: Reflexive MC; Nested MC;</cell><cell></cell></row><row><cell>NRPA; Meta-MCTS; HGSTS</cell><cell></cell></row><row><cell>4.10 Sample-Based Planners: FSSS; TAG; RRTs;</cell><cell></cell></row><row><cell>UNLEO; UCTSAT; ρUCT; MRW; MHSP</cell><cell></cell></row><row><cell>5 Tree Policy Enhancements</cell><cell></cell></row><row><cell>5.1 Bandit-Based: UCB1-Tuned; Bayesian UCT; EXP3;</cell><cell></cell></row><row><cell>HOOT; Other</cell><cell></cell></row><row><cell>5.2 Selection: FPU; Decisive Moves; Move Groups;</cell><cell></cell></row><row><cell>Transpositions; Progressive Bias; Opening Books;</cell><cell></cell></row><row><cell>MCPG; Search Seeding; Parameter Tuning;</cell><cell></cell></row><row><cell>History Heuristic; Progressive History</cell><cell></cell></row><row><cell>5.3 AMAF: Permutation; α-AMAF Some-First; Cutoff;</cell><cell></cell></row><row><cell>RAVE; Killer RAVE; RAVE-max; PoolRAVE</cell><cell></cell></row><row><cell>5.4 Game-Theoretic: MCTS-Solver; MC-PNS;</cell><cell></cell></row><row><cell>Score Bounded MCTS</cell><cell></cell></row><row><cell>5.5 Pruning: Absolute; Relative; Domain Knowledge</cell><cell></cell></row><row><cell>5.6 Expansion</cell><cell></cell></row><row><cell>6 Other Enhancements</cell><cell></cell></row><row><cell>6.1 Simulation: Rule-Based; Contextual; Fill the Board;</cell><cell></cell></row><row><cell>Learning; MAST; PAST; FAST; History Heuristics;</cell><cell></cell></row><row><cell>Evaluation; Balancing; Last Good Reply; Patterns</cell><cell></cell></row><row><cell>6.2 Backpropagation: Weighting; Score Bonus; Decay;</cell><cell></cell></row><row><cell>Transposition Table Updates</cell><cell></cell></row><row><cell>6.3 Parallelisation: Leaf; Root; Tree; UCT-Treesplit;</cell><cell></cell></row><row><cell>Threading and Synchronisation</cell><cell></cell></row><row><cell>6.4 Considerations: Consistency; Parameterisation;</cell><cell>1 Introduction</cell></row><row><cell>Comparing Enhancements</cell><cell>Overview; Importance; Aim; Structure</cell></row><row><cell>7 Applications</cell><cell>2 Background</cell></row><row><cell>7.1 Go: Evaluation; Agents; Approaches; Domain</cell><cell>2.1 Decision Theory: MDPs; POMDPs</cell></row><row><cell>Knowledge; Variants; Future Work</cell><cell>2.2 Game Theory: Combinatorial Games; AI in Games</cell></row><row><cell>7.2 Connection Games</cell><cell>2.3 Monte Carlo Methods</cell></row><row><cell>7.3 Other Combinatorial Games</cell><cell>2.4 Bandit-Based Methods: Regret; UCB</cell></row><row><cell>7.4 Single-Player Games</cell><cell></cell></row><row><cell>7.5 General Game Playing</cell><cell>3 Monte Carlo Tree Search</cell></row><row><cell>7.6 Real-time Games 7.7 Nondeterministic Games 7.8 Non-Game: Optimisation; Satisfaction; Scheduling; Planning; PCG</cell><cell>3.1 Algorithm 3.2 Development 3.3 UCT: Algorithm; Convergence to Minimax 3.4 Characteristics: Aheuristic; Anytime; Asymmetric</cell></row><row><cell>8 Summary Impact; Strengths; Weaknesses; Research Directions</cell><cell>3.5 Comparison with Other Algorithms 3.6 Terminology</cell></row><row><cell>9 Conclusion</cell><cell></cell></row></table><note>3. One paper per week indicates the high level of research interest.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>• UCTSAT sbs assigns variables one-by-one with random legal (satisfying) values.• UCTSAT h replaces playouts with a simple heuristic based on the fraction of satisfied clauses. domains. They describe the application of ρUCT to create their MC-AIXA agent, which approximates the AIXA15 model. MC-AIXA was found to approach optimal performance for several problem domains(7.8).</figDesc><table><row><cell>4.10.7 Monte Carlo Random Walks (MRW)</cell></row><row><cell>Monte Carlo Random Walks (MRW) selectively build the</cell></row><row><cell>search tree using random walks [238]. Xie et al. describe</cell></row><row><cell>the Monte Carlo Random Walk-based Local Tree Search</cell></row><row><cell>(MRW-LTS) method which extends MCRW to concen-</cell></row><row><cell>trate on local search more than standard MCTS methods,</cell></row><row><cell>allowing good performance in some difficult planning</cell></row><row><cell>problems [238].</cell></row><row><cell>4.10.8 Mean-based Heuristic Search for Anytime Plan-</cell></row><row><cell>ning (MHSP)</cell></row></table><note>4.10.6 ρUCTVeness et al.<ref type="bibr" target="#b227">[226]</ref> introduce ρUCT, a generalisation of UCT that approximates a finite horizon expectimax operation given an environment model ρ. ρUCT builds a sparse search tree composed of interleaved decision and chance nodes to extend UCT to a wider class of 14. The derivation of this term is not given. problem Pellier et al.<ref type="bibr" target="#b158">[158]</ref> propose an algorithm for planning problems, called Mean-based Heuristic Search for Anytime Planning (MHSP), based on MCTS. There are two key differences between MHSP and a conventional MCTS algorithm. First, MHSP entirely replaces the random simulations of MCTS with a heuristic evaluation function: Pellier et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 Approximate</head><label>2</label><figDesc>Elo rankings of 9 × 9 Go programs</figDesc><table><row><cell></cell><cell>Program</cell><cell>Description</cell><cell>Elo</cell></row><row><cell>2006</cell><cell>INDIGO</cell><cell cols="2">Pattern database, Monte Carlo simulation 1400</cell></row><row><cell>2006</cell><cell>GNU GO</cell><cell>Pattern database, α-β search</cell><cell>1800</cell></row><row><cell>2006</cell><cell>MANY FACES</cell><cell>Pattern database, α-β search</cell><cell>1800</cell></row><row><cell>2006</cell><cell>NEUROGO</cell><cell>TDL, neural network</cell><cell>1850</cell></row><row><cell>2007</cell><cell>RLGO</cell><cell>TD search</cell><cell>2100</cell></row><row><cell>2007</cell><cell>MOGO</cell><cell>MCTS with RAVE</cell><cell>2500</cell></row><row><cell>2007</cell><cell>CRAZY STONE</cell><cell>MCTS with RAVE</cell><cell>2500</cell></row><row><cell>2008</cell><cell>FUEGO</cell><cell>MCTS with RAVE</cell><cell>2700</cell></row><row><cell>2010</cell><cell>MANY FACES</cell><cell>MCTS with RAVE</cell><cell>2700</cell></row><row><cell>2010</cell><cell>ZEN</cell><cell>MCTS with RAVE</cell><cell>2700</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>34. Phantom Chess is sometimes called Kriegspiel, but should not be confused with the board game Kriegsspiel to which it bears little resemblance.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">. We cite Russell and Norvig<ref type="bibr" target="#b179">[178]</ref> as a standard AI reference, to reduce the number of non-MCTS references.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1">. Single-player games constitute solitaire puzzles.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">ln n nj</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_3">. For consistency with the existing literature, we use the Americanised spelling "determinization".</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Thanks to the anonymous reviewers for their helpful suggestions. This work was funded by EPSRC grants</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Expected-Outcome: A General Model of Static Evaluation</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">B</forename><surname>Abramson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="182" to="193" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sample mean based index policies with zero (log n) regret for the multi-armed bandit problem</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Appl. Prob</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1054" to="1078" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nested Monte-Carlo Search with AMAF Heuristic</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Komiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kotani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Tech. Applicat</title>
				<meeting>Int. Conf. Tech. Applicat<address><addrLine>Hsinchu, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
			<biblScope unit="page" from="172" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Proof-Number Search</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Allis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Meulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Van Den Herik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="124" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the Laziness of Monte-Carlo Game Tree Search in Non-tight Situations</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">I</forename><surname>Alth Öfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Friedrich-Schiller Univ., Jena, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Game Self-Play with Pure Monte-Carlo: The Basin Structure</title>
		<author>
			<persName><forename type="first">* --</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Friedrich-Schiller Univ., Jena, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MoHex Wins Hex Tournament</title>
		<author>
			<persName><forename type="first">B</forename><surname>Arneson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Hayward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="114" to="116" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monte Carlo Tree Search in Hex</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="251" to="258" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approaching Bayes-optimalilty using Monte-Carlo tree search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Asmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Automat. Plan</title>
				<meeting>21st Int. Conf. Automat. Plan<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning is planning: near Bayes-optimal reinforcement learning via Monte-Carlo tree search</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Uncert</title>
				<meeting>Conf. Uncert<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Minimax policies for adversarial and stochastic bandits</title>
		<author>
			<persName><forename type="first">* J.-Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Annu. Conf. Learn. Theory</title>
				<meeting>22nd Annu. Conf. Learn. Theory<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="773" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grid coevolution for adaptive simulations; application to the building of opening books in the game of Go</title>
		<author>
			<persName><forename type="first">P</forename><surname>Audouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Evol. Games, T übingen</title>
				<meeting>Evol. Games, T übingen<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="323" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finite-time Analysis of the Multiarmed Bandit Problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gambling in a rigged casino: The adversarial multi-armed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Symp. Found</title>
				<meeting>Annu. Symp. Found<address><addrLine>Milwaukee, Wisconsin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Continuous Lunches are Free Plus the Design of Optimal Optimization Algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Auger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="146" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple Tree for Partially Observable Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Auger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Evol. Games</title>
				<meeting>Evol. Games<address><addrLine>Torino, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Power of Forgetting: Improving the Last-Good-Reply Policy in Monte Carlo Go</title>
		<author>
			<persName><forename type="first">H</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Drake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="309" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">UCT for Tactical Assault Planning in Real-Time Strategy Games</title>
		<author>
			<persName><forename type="first">R.-K</forename><surname>Balla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Joint Conf</title>
				<meeting>21st Int. Joint Conf<address><addrLine>Pasadena, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="40" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Consistency Modifications for Automatically Tuned Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">V</forename><surname>Berthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Doghmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Learn</title>
				<meeting>Learn<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lower Bounding Klondike Solitaire with Monte-Carlo Planning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bjarnason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Int. Conf. Automat. Plan. Sched</title>
				<meeting>19th Int. Conf. Automat. Plan. Sched<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CadiaPlayer: A Simulation-Based General Game Player</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bj Örnsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Finnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Repeated-task Canadian traveler problem</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bnaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Felner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Shimony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Maksin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symp. Combin. Search</title>
				<meeting>Symp. Combin. Search<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Comparison of Monte-Carlo Methods for Phantom Go</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W H M</forename><surname>Uiterwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BeNeLux Conf</title>
				<meeting>BeNeLux Conf<address><addrLine>Utrecht, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scalability and Parallelization of Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bourki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coulm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Danjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Doghmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hérault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vayssière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. and Games</title>
				<meeting>Int. Conf. Comput. and Games<address><addrLine>Kanazawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6515</biblScope>
			<biblScope unit="page" from="48" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parameter Tuning by Simple Regret Algorithms and Multiple Simultaneous Hypothesis Testing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bourki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coulm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vayssière</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Inform. Control, Autom. and Robot</title>
				<meeting>Int. Conf. Inform. Control, Autom. and Robot<address><addrLine>Funchal, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="169" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Move Pruning Techniques for Monte-Carlo Go</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">B</forename><surname>Bouzy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Comput. Games</title>
				<meeting>Adv. Comput. Games<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">4250</biblScope>
			<biblScope unit="page" from="104" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Analysis and Implementation of the Game OnTop</title>
		<author>
			<persName><forename type="first">R</forename><surname>Briesemeister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Maastricht Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic Generation and Evaluation of Recombination Games</title>
		<author>
			<persName><forename type="first">C</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, Qld. Univ. Tech. (QUT)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the Dangers of Random Playouts</title>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="26" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards MCTS for Creative Domains</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Creat</title>
				<meeting>Int. Conf. Comput. Creat<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="96" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monte Carlo Go</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">B</forename><surname>Br Ügmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Max-Planke-Inst. Phys., Munich, Tech. Rep</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pure Exploration in Finitely-Armed and Continuously-Armed Bandits</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stoltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">412</biblScope>
			<biblScope unit="page" from="1832" to="1852" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online Optimization in X-Armed Bandits</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stoltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">X-Armed Bandits</title>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1587" to="1627" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving State Evaluation, Inference, and Search in Trick-Based Card Games</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Furtak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Sturtevant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Joint Conf</title>
				<meeting>21st Int. Joint Conf<address><addrLine>Pasadena, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1407" to="1413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Phantom Go Program</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cazenave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="120" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evolving Monte-Carlo Tree Search Algorithms</title>
	</analytic>
	<monogr>
		<title level="j">Dept. Inform., Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Univ. Paris</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Playing the Right Atari</title>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="42" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reflexive Monte-Carlo Search</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Games Workshop</title>
				<meeting>Comput. Games Workshop<address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="165" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-player Go</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5131</biblScope>
			<biblScope unit="page" from="50" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<author>
			<persName><forename type="first">Monte-Carlo</forename><surname>Kakuro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Comput. Games</title>
				<meeting>Adv. Comput. Games<address><addrLine>Pamplona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">6048</biblScope>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nested Monte-Carlo Search</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Joint Conf</title>
				<meeting>21st Int. Joint Conf<address><addrLine>Pasadena, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="456" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nested Monte-Carlo Expression Discovery</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf</title>
				<meeting>Euro. Conf<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1057" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Monte-Carlo Approximation of Temperature</title>
	</analytic>
	<monogr>
		<title level="s">Games of No Chance</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Monte-Carlo Bus Regulation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cazenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Balbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. IEEE Conf</title>
				<meeting>Int. IEEE Conf<address><addrLine>St Louis, Missouri</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Golois Wins Phantom Go Tournament</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cazenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="166" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the Parallelization of UCT</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cazenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jouandeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Games Workshop</title>
				<meeting>Comput. Games Workshop<address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="93" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A parallel Monte-Carlo tree search algorithm</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5131</biblScope>
			<biblScope unit="page" from="72" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Parallel Nested Monte-Carlo search</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Parallel Distrib. Processes Symp</title>
				<meeting>IEEE Int. Parallel Distrib. esses Symp<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Monte-Carlo Hex</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cazenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffidine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Board Games Studies Colloq</title>
				<meeting>Board Games Studies Colloq<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Score Bounded Monte-Carlo Tree Search</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Kanazawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6515</biblScope>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search: A New Framework for Game AI</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Szita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spronck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null<address><addrLine>California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="216" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Combining expert, offline, transient and online knowledge in Monte-Carlo exploration</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chatriot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lab. Rech. Inform. (LRI)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search in Production Management Problems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W H M</forename><surname>Uiterwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BeNeLux Conf</title>
				<meeting>BeNeLux Conf<address><addrLine>Namur, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adding Expert Knowledge and Exploration in Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Comput. Games</title>
				<meeting>Adv. Comput. Games<address><addrLine>Pamplona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6048</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Meta Monte-Carlo Tree Search for Automatic Opening Book Generation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Joint Conf</title>
				<meeting>21st Int. Joint Conf<address><addrLine>Pasadena, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the huge benefit of quasi-random mutations for multimodal optimization with application to grid-based tuning of neurocontrollers</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Euro. Symp. Artif. Neur. Net</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cross-Entropy for Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Szita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Van Den Herik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Parallel Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Van Den Herik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5131</biblScope>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Progressive Strategies for Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Van Den Herik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W H M</forename><surname>Uiterwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bouzy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Math. Nat. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="357" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search and Computer Go</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Inform. Intell. Sys</title>
		<imprint>
			<biblScope unit="volume">251</biblScope>
			<biblScope unit="page" from="201" to="225" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Introduction of a new paraphrase generation tool based on Monte-Carlo sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chevelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lepage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moudenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Joint Conf. Natur. Lang. Process</title>
				<meeting>4th Int. Joint Conf. Natur. Lang. ess<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="249" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Transpositions and Move Groups in Monte Carlo Tree Search</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Childs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Brodeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp</title>
				<meeting>IEEE Symp<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="389" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Revisiting Monte-Carlo Tree Search on a Normal Form Game: NoGo</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Applicat. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">6624</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Computational and Human Intelligence in Blind Go</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Doghmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Seoul</addrLine></address></meeting>
		<imprint>
			<publisher>South Korea</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Monte Carlo Tree Search Techniques in the Game of Kriegspiel</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ciancarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Favini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Joint Conf</title>
				<meeting>21st Int. Joint Conf<address><addrLine>Pasadena, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="474" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Monte Carlo tree search in Kriegspiel</title>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="670" to="684" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Bandit Algorithms for Tree Search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Coquelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Arnaud</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Munos</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Uncert</title>
				<meeting>Conf. Uncert<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Continuous Upper Confidence Trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Couëtoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sokolovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bonnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Learn</title>
				<meeting>Learn<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="433" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Comput. and Games</title>
				<meeting>5th Int. Conf. Comput. and Games<address><addrLine>Turin, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Computing Elo Ratings of Move Patterns in the Game of Go</title>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="198" to="208" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search in Crazy Stone</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Game Prog. Workshop</title>
				<meeting>Game Prog. Workshop<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="74" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Bandit-Based Optimization on Graphs with Application to Library Performance Tuning</title>
		<author>
			<persName><forename type="first">F</forename><surname>De Mesmay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Voronenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Üschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu. Int. Conf. Mach. Learn</title>
				<meeting>26th Annu. Int. Conf. Mach. Learn<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="729" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search for the Simultaneous Move Game Tron</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G P</forename><surname>Den</surname></persName>
		</author>
		<author>
			<persName><surname>Teuling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Maastricht, Netherlands, Tech. Rep</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The Last-Good-Reply Policy for Monte-Carlo Go</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Drake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="221" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Heuristics in Monte Carlo Go</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Drake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uurtamo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
				<meeting>Int. Conf<address><addrLine>Las Vegas, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Move Ordering vs Heavy Playouts: Where Should Heuristics be Applied in Monte Carlo Go</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd North Amer. Game-On Conf</title>
				<meeting>3rd North Amer. Game-On Conf<address><addrLine>Gainesville, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Finding the Needle in the Haystack with Heuristically Guided Swarm Tree Search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Edelkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kissmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sulewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Messerschmidt</surname></persName>
		</author>
		<editor>Multikonf. Wirtschaftsinform.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2295" to="2308" />
			<pubPlace>Gottingen, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fuego -An Open-source Framework for Board Games and Go Engine Based on Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enzenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Üller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Alberta</title>
		<imprint>
			<date type="published" when="2009-04">April, 2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A Lock-free Multithreaded Monte-Carlo Tree Search Algorithm</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Comput. Games</title>
				<meeting>Adv. Comput. Games<address><addrLine>Pamplona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6048</biblScope>
			<biblScope unit="page" from="14" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Fuego -An Open-Source Framework for Board Games and Go Engine Based on Monte Carlo Tree Search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enzenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Üller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Arneson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="259" to="270" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Ensemble Monte-Carlo Planning: An Empirical Study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Automat. Plan. Sched</title>
				<meeting>21st Int. Conf. Automat. Plan. Sched<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">CADIA-Player: A General Game Playing Agent</title>
		<author>
			<persName><forename type="first">H</forename><surname>Finnsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
		<respStmt>
			<orgName>Reykjavik Univ., Iceland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Simulation-Based Approach to General Game Playing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Finnsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bj Örnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc</title>
				<meeting>Assoc<address><addrLine>Chicago, Illinois</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="259" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Simulation Control in General Game Playing Agents</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
				<meeting>Int. Joint Conf<address><addrLine>Pasadena, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning Simulation Control in General Game-Playing Agents</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th AAAI Conf</title>
				<meeting>24th AAAI Conf<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="954" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">CadiaPlayer: Search-Control Techniques</title>
	</analytic>
	<monogr>
		<title level="j">K ünstliche Intelligenz</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">To Create Intelligent Adaptive Neuro-Controller of Game Opponent from UCT-Created Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fuzzy Sys</title>
				<meeting>Fuzzy Sys<address><addrLine>Tianjin, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="445" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Feature Selection as a One-Player Game</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gaudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Conf. Mach. Learn</title>
				<meeting>27th Int. Conf. Mach. Learn<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">A Contribution to Reinforcement Learning; Application to Computer-Go</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Paris-Sud, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">The Parallelization of Monte-Carlo Planning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalemkarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Inform. Control, Automat. and Robot</title>
				<meeting>5th Int. Conf. Inform. Control, Automat. and Robot<address><addrLine>Funchal, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="244" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Combining Online and Offline Knowledge in UCT</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Annu. Int. Conf. Mach. Learn. Corvalis</title>
				<meeting>24th Annu. Int. Conf. Mach. Learn. Corvalis<address><addrLine>Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Achieving Master Level Play in 9 x 9 Computer Go</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc</title>
				<meeting>Assoc<address><addrLine>Chicago, Illinois</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1537" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Monte-Carlo tree search and rapid action value estimation in computer Go</title>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1856" to="1875" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Exploration exploitation in Go: UCT for Monte-Carlo Go</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Modification of UCT with Patterns in Monte-Carlo Go</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inst. Nat. Rech. Inform. Auto. (INRIA)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Tech. Rep</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">GIB: Imperfect Information in a Computationally Challenging Game</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Ginsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="303" to="358" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Parallel Monte-Carlo Tree Search for HPC Systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Platzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schaefers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th</title>
				<meeting>17th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<author>
			<persName><surname>Int</surname></persName>
		</author>
		<author>
			<persName><surname>Euro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Parallel Distrib. Comput</title>
				<meeting><address><addrLine>Bordeaux, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6853</biblScope>
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Game Player Strategy Pattern Recognition and How UCT Algorithms Apply Pre-knowledge of Player&apos;s Strategy to Improve Opponent AI</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">2008. Dec. 2008</date>
			<biblScope unit="page" from="1177" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">To Create Adaptive Game Opponent by Using UCT</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">2008. Dec. 2008</date>
			<biblScope unit="page" from="67" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">All-Moves-As-First Heuristics in Monte-Carlo Go</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Helmbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parker-Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
				<meeting>Int. Conf<address><addrLine>Las Vegas, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="605" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Random positions in Go</title>
		<author>
			<persName><forename type="first">B</forename><surname>Helmstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Experiments with Monte Carlo Othello</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hingston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Masek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Congr</title>
				<meeting>IEEE Congr<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="4059" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Intelligent Agents for the Game of Go</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Intell. Mag</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="28" to="42" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Bandit-Based Genetic Programming</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Gen. Prog</title>
				<meeting>Euro. Conf. Gen. Prog<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6021</biblScope>
			<biblScope unit="page" from="268" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Pruning in UCT Algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Tech. Applicat</title>
				<meeting>Int. Conf. Tech. Applicat<address><addrLine>Hsinchu, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">New Heuristics for Monte Carlo Tree Search Applied to the Game of Go</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Taipei</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Nat. Taiwan Normal Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Monte-Carlo Simulation Balancing Applied to 9x9 Go</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="191" to="201" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Monte-Carlo Simulation Balancing in Practice</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Kanazawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6515</biblScope>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Time Management for Monte-Carlo Tree Search Applied to the Game of Go</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Tech. Applicat</title>
				<meeting>Int. Conf. Tech. Applicat<address><addrLine>Hsinchu City, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="462" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Monte Carlo Tree Search in Ms. Pac-Man</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ikehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Game Progr. Workshop</title>
				<meeting>15th Game Progr. Workshop<address><addrLine>Kanagawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search in Ms. Pac-Man</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games, Seoul, South Korea</title>
				<meeting>IEEE Conf. Comput. Intell. Games, Seoul, South Korea</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Parallel Monte-Carlo Tree Search with Simulation Servers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Tech. Applicat</title>
				<meeting>Int. Conf. Tech. Applicat<address><addrLine>Hsinchu City, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="491" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Experiments in Monte-Carlo Amazons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kloetzer</surname></persName>
		</author>
		<idno>GI-24</idno>
	</analytic>
	<monogr>
		<title level="j">J. Inform. Process. Soc. Japan</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Monte-Carlo Opening Books for Amazons</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Kanazawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6515</biblScope>
			<biblScope unit="page" from="124" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">The Monte-Carlo Approach in Amazons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kloetzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bouzy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Games Workshop</title>
				<meeting>Comput. Games Workshop<address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="113" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">A Comparative Study of Solvers in Amazons Endgames</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="378" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Playing Amazons Endgames</title>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="140" to="148" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Bandit based Monte-Carlo Planning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro. Conf. Mach. Learn</title>
				<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Improved Monte-Carlo Search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willemson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Tartu, Estonia, Tech. Rep</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Efficient Playouts for the Havannah Abstract Board Game</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Waldmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hochschule Technik</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Methods of MCTS and the game Arimaa</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kozelek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Prague</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Charles Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A New Benchmark for Artificial Intelligence</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Kroeker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="13" to="15" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Asymptotically Efficient Adaptive Allocation Rules</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Monte Carlo Sampling for Regret Minimization in Extensive Games</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1078" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Rapidly-Exploring Random Trees: A New Tool for Path Planning</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">S M</forename><surname>Lavalle</surname></persName>
		</author>
		<idno>TR 98-11</idno>
	</analytic>
	<monogr>
		<title level="j">Iowa State Univ., Comp Sci. Dept</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Guest Editorial: Special Issue on Monte Carlo Techniques and Computer Go</title>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="225" to="228" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">The Computational Intelligence of MoGo Revealed in Taiwan&apos;s Computer Go Tournaments</title>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-R</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="89" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A Novel Ontology for Computer Go Knowledge Management</title>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hoock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Fuzzy Sys</title>
				<meeting>IEEE Int. Conf. Fuzzy Sys<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08">Aug. 2009</date>
			<biblScope unit="page" from="1056" to="1061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Understanding the Success of Perfect Information Monte Carlo Sampling in Game Tree Search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Sturtevant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Furtak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc</title>
				<meeting>Assoc<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Rybka&apos;s Monte Carlo analysis</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">S</forename><surname>Lopez</surname></persName>
		</author>
		<ptr target="http://www.chessbase.com/newsdetail.asp?newsid=5075" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Amazons Discover Monte-Carlo</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Lorentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5131</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Improving Monte-Carlo Tree Search in Havannah</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Kanazawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6515</biblScope>
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">232</biblScope>
			<date type="published" when="2011">2011</date>
			<publisher>Castro Wins Havannah Tournament</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Modelling and evaluation of complex scenarios with the Strategy Game Description Language</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mahlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Towards Procedural Strategy Game Generation: Evolving Complementary Unit Types</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Applicat</title>
				<meeting>Applicat<address><addrLine>Torino, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6624</biblScope>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Adaptive play in Texas Hold&apos;em Poker</title>
		<author>
			<persName><forename type="first">R</forename><surname>Maîtrepierre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf</title>
				<meeting>Euro. Conf<address><addrLine>Patras, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="458" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Sample-Based Planning for Continuous Action Markov Decision Processes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mansley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Automat. Plan</title>
				<meeting>21st Int. Conf. Automat. Plan<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="335" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Multi-Agent Monte Carlo Go</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Marcolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Matsubara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Auton. Agents Multi. Sys</title>
				<meeting>Int. Conf. Auton. Agents Multi. Sys<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Evaluation of Simulation Strategy on Single-Player Monte-Carlo Tree Search and its Discussion for a Practical Scheduling Problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hirosue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Itonaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yokoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Futahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Multi Conf</title>
				<meeting>Int. Multi Conf</meeting>
		<imprint>
			<publisher>Hong Kong</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2086" to="2091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Multi-Armed Bandit Bayesian Decision Making</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcinerney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Oxford, Oxford, Tech. Rep</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Ary: A Program for General Game Playing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Méhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cazenave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Inform., Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Univ. Paris</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search for General Game Playing</title>
	</analytic>
	<monogr>
		<title level="j">Dept. Info., Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Univ. Paris</note>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Combining UCT and Nested Monte Carlo Search for Single-Player General Game Playing</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="271" to="277" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">A Parallel General Game Player</title>
	</analytic>
	<monogr>
		<title level="j">K ünstliche Intelligenz</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="47" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Tree Parallelization of Ary on a Cluster</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
				<meeting>Int. Joint Conf<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Centurio, a General Game Player: Parallel, Java-and ASP-based</title>
		<author>
			<persName><forename type="first">M</forename><surname>Öller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">K ünstliche Intelligenz</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="24" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Üller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fuego-GB Prototype at the Human machine competition in Barcelona 2010: a Tournament Report and Analysis</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Monte-Carlo Exploration for Deterministic Planning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nakhost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Üller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Joint Conf</title>
				<meeting>21st Int. Joint Conf<address><addrLine>Pasadena, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1766" to="1771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Monte-Carlo Planning for Pathfinding in Real-Time Strategy Games</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naveed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Kitchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Crampton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Workshop UK Spec</title>
				<meeting>28th Workshop UK Spec<address><addrLine>Brescia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">ICE gUCT</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ashida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thawonmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Comput. Entertain. Lab., Ritsumeikan Univ., Tech. Rep</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">Playing Othello Using Monte Carlo</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A M</forename><surname>Nijssen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Strategies</note>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">Using Intelligent Search Techniques to Play the Game Khet</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Maastricht Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M. S. Thesis</note>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Using Intelligent Search Techniques to Play the Game Khet</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A M</forename><surname>Nijssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W H M</forename><surname>Uiterwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Maastricht Univ</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Enhancements for Multi-Player Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A M</forename><surname>Nijssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Kanazawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6515</biblScope>
			<biblScope unit="page" from="238" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search for the Game of Scotland Yard</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games, Seoul, South Korea</title>
				<meeting>IEEE Conf. Comput. Intell. Games, Seoul, South Korea</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">An Othello Evaluation Function Based on Temporal Difference Learning using Probability of Winning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Osaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shibahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kotani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="205" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">An UCT Approach for Anytime Agent-Based Planning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pellier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bouzy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Métivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
				<meeting>Int. Conf<address><addrLine>Salamanca, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Integrating Opponent Models with Monte-Carlo Tree Search in Poker</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ponsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gerritsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-B</forename><surname>Chaslot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Assoc. Adv</title>
				<meeting>Conf. Assoc. Adv<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Monte-Carlo Style UCT Search for Boolean Satisfiability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Previti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaerf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th</title>
				<meeting>12th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Ital. Assoc. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">6934</biblScope>
			<biblScope unit="page" from="177" to="188" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Application of UCT Search to the Connection Games of</title>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peltonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renkula</forename><surname>Star</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Finn</title>
				<meeting>Finn<address><addrLine>Espoo, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="89" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">On Adversarial Search Spaces and Sampling-Based Planning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Int. Conf. Automat. Plan</title>
				<meeting>20th Int. Conf. Automat. Plan<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="242" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Understanding Sampling Style Adversarial Search Methods</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Uncert</title>
				<meeting>Conf. Uncert<address><addrLine>Catalina Island, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">On the Behavior of UCT in Synthetic Search Spaces</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Automat. Plan</title>
				<meeting>21st Int. Conf. Automat. Plan<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Trade-Offs in Sampling-Based Adversarial Planning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Automat. Plan. Sched</title>
				<meeting>21st Int. Conf. Automat. Plan. Sched<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="202" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Improvements and Evaluation of the Monte-Carlo Tree Search Algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lab. Rech. Inform. (LRI)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Multiple Overlapping Tiles for Contextual Monte Carlo Tree Search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Applicat</title>
				<meeting>Applicat<address><addrLine>Torino. Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Optimization of the Nested Monte-Carlo Algorithm on the Traveling Salesman Problem with Time Windows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cazenave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Applicat</title>
				<meeting>Applicat<address><addrLine>Torino, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="501" to="510" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Biasing Monte-Carlo Simulations through RAVE Values</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Kanazawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6515</biblScope>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Current Frontiers in Computer Go</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-R</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">A Simple Tree Search Method for Playing Ms. Pac-Man</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">D</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Milan, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="249" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Learning Non-Random Moves for Playing Othello: Improving Monte Carlo Tree Search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rohlfshagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Boosting Active Learning to Optimality: a Tractable Monte-Carlo, Billiard-based Algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf</title>
				<meeting>Euro. Conf<address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="302" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Optimal Robust Expensive Optimization is Tractable</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Annu. Conf</title>
				<meeting>11th Annu. Conf<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1951" to="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Upper Confidence Trees and Billiards for Optimal Active Learning</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. l&apos;Apprentissage Autom</title>
				<meeting>Conf. l&apos;Apprentissage Autom<address><addrLine>Hammamet, Tunisia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Nested Rollout Policy Adaptation for Monte Carlo Tree Search</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Joint Conf</title>
				<meeting>22nd Int. Joint Conf<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="649" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Computer poker: A review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="958" to="987" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">S J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<title level="m">Artificial Intelligence: A Modern Approach</title>
				<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>3rd ed. Upper Saddle River</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Guiding Combinatorial Optimization with UCT</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samulowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Automat. Plan</title>
				<meeting>21st Int. Conf. Automat. Plan<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Utilisation dUCT au Hex</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saffidine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecole Normale Super</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<title level="m" type="main">Some Improvements for Monte-Carlo Tree Search, Game Description Language Compilation, Score Bounds and Transpositions</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Paris-Dauphine Lamsade, France</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">UCD: Upper Confidence bound for rooted Directed acyclic graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saffidine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cazenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Méhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Tech. Applicat</title>
				<meeting>Conf. Tech. Applicat<address><addrLine>Hsinchu City, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="467" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Monte-Carlo Proof-Number Search for Computer Go</title>
		<author>
			<persName><forename type="first">* J.-T</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W H M</forename><surname>Uiterwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Van Den Herik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Comput. and Games</title>
				<meeting>5th Int. Conf. Comput. and Games<address><addrLine>Turin, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">A UCT Agent for Tron: Initial Investigations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp</title>
				<meeting>IEEE Symp<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="365" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Fast Approximate Max-n Monte-Carlo Tree Search for Ms Pac-Man</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="154" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">A Shogi Program Based on Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grimbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="80" to="92" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Real-Time Solving of Quantified CSPs Based on Monte-Carlo Game Tree Search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Satomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Joe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iwasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yokoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Joint Conf</title>
				<meeting>22nd Int. Joint Conf<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="655" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">Monte-Carlo Search Techniques in the Modern Board Game Thurn and Taxis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Schadd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Maastricht Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Selective Search in Games of Different Complexity</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P D</forename><surname>Schadd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Maastricht Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Addressing NP-Complete Puzzles with Monte-Carlo Methods</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P D</forename><surname>Schadd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Van Den Herik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aldewereld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null<address><addrLine>Aberdeen, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="55" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Single-Player Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P D</forename><surname>Schadd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Van Den Herik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W H M</forename><surname>Uiterwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5131</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">UCT-Treesplit -Parallel MCTS on Distributed Memory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schaefers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Platzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Automat. Plan</title>
				<meeting>21st Int. Conf. Automat. Plan<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">The History Heuristic and Alpha-Beta Search Enhancements in Practice</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">J</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1203" to="1212" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">The UCT Algorithm Applied to Games with Imperfect Information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schäfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Otto-Von-Guericke Univ. Magdeburg</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Diploma thesis</note>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">On the Scalability of Parallel UCT</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Kanazawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6515</biblScope>
			<biblScope unit="page" from="36" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Comparing UCT versus CFR in Simultaneous Games</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Sturtevant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
				<meeting>Int. Joint Conf<address><addrLine>Pasadena, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Knowledge Generation for Improving Simulations in UCT for General Game Playing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kobti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv</title>
				<meeting>Adv<address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Learning and Knowledge Generation in General Games</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp. Comput. Intell. Games</title>
				<meeting>IEEE Symp. Comput. Intell. Games<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="329" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">World-championship-caliber Scrabble</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">B</forename><surname>Sheppard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="241" to="275" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Combining Final Score with Winning Percentage by Sigmoid Function in Monte-Carlo Simulations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shibahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kotani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">Reinforcement Learning and Simulation-Based Search in Computer Go</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Edmonton</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Alberta</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Sample-Based Learning and Search with Permanent and Transient Memories</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Üller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Annu. Int. Conf. Mach. Learn</title>
				<meeting>25th Annu. Int. Conf. Mach. Learn<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="968" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Monte-Carlo Simulation Balancing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu. Int. Conf. Mach. Learn</title>
				<meeting>26th Annu. Int. Conf. Mach. Learn<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="945" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Monte-Carlo Planning in Large POMDPs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neur</title>
				<meeting>Neur<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Evaluating Root Parallelization in Go</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Soejima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kishimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="278" to="287" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<monogr>
		<title level="m" type="main">Knowledge-Based Monte-Carlo Tree Search in Havannah</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Stankiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Maastricht Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">An Analysis of UCT in Multi-Player Games</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Sturtevant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5131</biblScope>
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">A Linear Classifier Outperforms UCT in 9x9 Go</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lohre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Drake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
				<meeting>Int. Conf<address><addrLine>Las Vegas, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="804" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search in Settlers of Catan</title>
		<author>
			<persName><forename type="first">I</forename><surname>Szita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spronck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null<address><addrLine>Pamplona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="21" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Evaluation of Monte Carlo Tree Search and the Application to Go</title>
		<author>
			<persName><forename type="first">S</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Evaluation of Game Tree Search Methods by Game Records</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="288" to="302" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">A Study on Security Evaluation Methodology for Image-Based Biometrics Authentication Systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshizoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Biom.: Theory, Applicat. Sys</title>
				<meeting>IEEE Conf. Biom.: Theory, Applicat. Sys<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Bayesian Inference in Monte-Carlo Tree Search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Uncert</title>
				<meeting>Conf. Uncert<address><addrLine>Catalina Island, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="580" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Creating an Upper-Confidence-Tree program for Havannah</title>
		<author>
			<persName><forename type="first">F</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Comput. Games</title>
				<meeting>Adv. Comput. Games<address><addrLine>Pamplona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6048</biblScope>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">On the Huge Benefit of Decisive Moves in Monte-Carlo Tree Search Algorithms</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp</title>
				<meeting>IEEE Symp<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="359" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Lemmas on Partial Observation, with Application to Phantom Games</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games, Seoul, South Korea</title>
				<meeting>IEEE Conf. Comput. Intell. Games, Seoul, South Korea</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="243" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Upper Confidence Trees with Short Term Partial Information</title>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Applicat</title>
				<meeting>Applicat<address><addrLine>Torino, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<title level="m" type="main">Investigating UCT and RAVE: Steps towards a more robust method</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Edmonton</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Alberta</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">A Study of UCT and its Enhancements in an Artificial Game</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Üller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Comput. Games</title>
				<meeting>Adv. Comput. Games<address><addrLine>Pamplona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6048</biblScope>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Computational Experiments with the RAVE Heuristic</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Kanazawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6515</biblScope>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">A Monte-Carlo Approach for the Endgame of Ms. Pac-Man</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-B</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">A Monte-Carlo Approach for Ghost Avoidance in the Ms. Pac-Man Game</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Consum</title>
				<meeting>IEEE Consum</meeting>
		<imprint>
			<publisher>Elect. Soc. Games Innov. Conf., Hong Kong</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search in Poker using Expected Reward Distributions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Broeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Driessens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">5828</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="367" to="381" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">The Drosophila Revisited</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Van Den Herik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Comp. Games Assoc. J</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="66" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search in Backgammon</title>
		<author>
			<persName><forename type="first">F</forename><surname>Van Lishout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>-B. Chaslot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W H M</forename><surname>Uiterwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Games Workshop</title>
				<meeting>Comput. Games Workshop<address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">A Monte-Carlo AIXI Approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Uther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="95" to="142" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Integrating Samplebased Planning and Model-based Reinforcement Learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goschin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc</title>
				<meeting>Assoc<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="612" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Modifications of UCT and sequence-like simulations for Monte-Carlo Go</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp</title>
				<meeting>IEEE Symp<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Monte Carlo Search Applied to Card Selection in Magic: The Gathering</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp</title>
				<meeting>IEEE Symp<address><addrLine>Milan, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Determinization and Information Set Monte Carlo Tree Search for the Card Game Dou Di Zhu</title>
		<author>
			<persName><forename type="first">D</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Determining Game Quality Through UCT Tree Shape Analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">M.S. thesis, Imperial Coll</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Evaluation Function Based Monte-Carlo LOA</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bj Örnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Comput. Games</title>
				<meeting>Adv. Comput. Games<address><addrLine>Pamplona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6048</biblScope>
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">αβ-based Play-outs in Monte-Carlo Tree Search</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Intell. Games, Seoul, South Korea</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search Solver</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bj Örnsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. and Games</title>
				<meeting>Comput. and Games<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5131</biblScope>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Monte-Carlo Tree Search in Lines of Action</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="239" to="250" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Monte Carlo Tree Search in Lines of Action</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bj Örnsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="239" to="250" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Backpropagation Modification in Monte-Carlo Game Tree Search</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp</title>
				<meeting>Int. Symp<address><addrLine>NanChang, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="125" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">A Local Monte Carlo Tree Search Approach in Deterministic Planning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakhost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Üller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc</title>
				<meeting>Assoc<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1832" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">To Create Intelligent Adaptive Game Opponent by Using Monte-Carlo for Tree Search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Natural Comput</title>
				<meeting>5th Int. Conf. Natural Comput<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Tianjian</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="603" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Optimizing Player&apos;s Satisfaction through DDA of Game AI by UCT for the Game Dead-End</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. Natural Comput</title>
				<meeting>6th Int. Conf. Natural Comput<address><addrLine>Yantai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4161" to="4165" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
