<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-16">16 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
							<email>pangliang@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
							<email>lanyanyan@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-16">16 Jan 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2101.06423v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic text matching models have been widely used in community question answering, information retrieval, and dialogue. However, these models cannot well address the long-form text matching problem. That is because there are usually many noises in the setting of long-form text matching, and it is difficult for existing semantic text matching to capture the key matching signals from this noisy information. Besides, these models are computationally expensive because they simply use all textual data indiscriminately in the matching process. To tackle the effectiveness and efficiency problem, we propose a novel hierarchical noise filtering model in this paper, namely Match-Ignition. The basic idea is to plug the wellknown PageRank algorithm into the Transformer, to identify and filter both sentence and word level noisy information in the matching process. Noisy sentences are usually easy to detect because the sentence is the basic unit of a long-form text, so we directly use PageRank to filter such information, based on a sentence similarity graph. While words need to rely on their contexts to express concrete meanings, so we propose to jointly learn the filtering process and the matching process, to reflect the contextual dependencies between words. Specifically, a word graph is first built based on the attention scores in each self-attention block of Transformer, and keywords are then selected by applying PageRank on this graph. In this way, noisy words will be filtered out layer by layer in the matching process. Experimental results show that Match-Ignition outperforms both traditional text matching models for short text and recent long-form text matching models. We also conduct detailed analysis to show that Match-Ignition can efficiently capture important sentences or words, which are helpful for long-form text matching.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b34">35</ref> <p>seconds, including a 26-footer with 1.7 seconds remaining, to send the Spurs to San Antonio with a second loss in as … Figure <ref type="figure">1</ref>: The example at the top is a short-form text matching for the community question answering task, and the lines indicate the alignments between words from two sentences. The example at the bottom is a long-form text matching for redundancy news identification task, and the highlight words indicate the identity event of two news, thus the words remained are treated as noise for the matching task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semantic text matching is an essential problem in many natural language applications, such as community question answering <ref type="bibr" target="#b32">[33]</ref>, information retrieval <ref type="bibr" target="#b11">[12]</ref>, and dialogue <ref type="bibr" target="#b17">[18]</ref>. Many deep text matching models have been proposed and gain some improvement, such as representation based models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>, interaction based models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>, and their combinations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>However, these models cannot be well applied to long-form text matching problems, which have attracted increasing attention in the field of news recommendation <ref type="bibr" target="#b15">[16]</ref> and attachment suggestion <ref type="bibr" target="#b13">[14]</ref>. This is mainly because long-form text matching is quite different from the short-form text matching problem. For short-form text matching, almost every term in the short texts is critical to the matching score, because short text matching tasks are just like finding a reasonable semantic alignment between two sentences <ref type="bibr" target="#b22">[23]</ref>. For example, in community question answering, the major problem is to find the most relevant question for the given question. In this case, the matching score is mainly determined by the alignment between each word in the questions, as shown in Figure <ref type="figure">1</ref>.</p><p>Long-form text matching has its own characteristics. Firstly, longform text matching cares more about the global semantic meanings rather than the bipartite alignment. The fine-grained matching signals between long-form texts are usually very sparse, which makes the existing short text matching models hard to figure them out from huge noisy signals. For example, in redundant news identification, it merely focuses on where/when the event happened and what the event is, instead of who posted this news and the detailed descriptions of the news. Secondly, long-form text matching contains a very long text by nature, which makes the existing short text matching models computational expensive because they have to treat every word indiscriminately and emphasize the sufficient interactions between words <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref>. Furthermore, the long-form text often have to be truncated in the computation. For example, BERT only accepts text length of less than 512. These operations may hurt the final matching performance. Thirdly, long-form text intrinsically consists a two-level structure, i.e. sentences and words. Most existing short text matching approaches can only process text word by word while missing the sentence-level structure. For example, one sentence should be ignored entirely if it is irrelevant with the current document, e.g. advertisement, even some of its internal words are relevant with another document. From these discussions, we can see that noise is the main challenge in long-form text matching, to affect both performance and efficiency.</p><p>In this paper, we propose a novel hierarchical noise filtering model, namely Match-Ignition, to distill the significant matching signals via the well-known link analysis algorithm PageRank <ref type="bibr" target="#b2">[3]</ref>. PageRank utilizes random walk on a graph to determine the importance of each node. In this way, the noises (i.e. less important nodes) can be eliminated and the inference will be accelerated. Considering the two-level structures in the long-form text matching problem, our model contains two hierarchies, i.e. sentences-level and word-level. In the sentence-level noise filtering process, the nodes are defined as sentences from a prir of long-form texts, and the link are defined as the similarities between each pair of sentences. That is to way, the similarities inside each long-form text and between the two long-form texts are both captured in our graph. Then the noisy sentences could be identified by PageRank score, and be directly removed. The word-level noise filtering process is jointly learned with the matching process, because each word relies on its context to express its concrete meanings, thus noisy words at the word-level are composite and dynamic, which need to be estimated dynamically during the matching process. So we first apply the state-of-the-art Transformer to the texts, because it can well capture the contextual information among words. Therefore, the attention matrix in the self-attention block, the key component of Transformer, could be treated as a fully connected word-level similarity graph <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref>. PageRank is then applied to filter out noise words at each layer. We can see this technique is different from previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref> which focus on eliminating links in the graph, because our model focuses on filtering noisy words, i.e., nodes in the graph.</p><p>We experiments on two public long-form text matching dataset, provided by Liu et al. <ref type="bibr" target="#b15">[16]</ref>. The experimental results show that Match-Ignition outperforms baseline methods, such as text matching models for short text and recent long-form text matching models. The further analysis illustrates that Match-Ignition efficiently models important matching signals in long-form text, which helps understand the matching process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we first introduce the text matching models designed for short-form text matching, then review the most recent works for long-form text matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Short-form Text Matching</head><p>Existing text matching models fall into representation-based approaches, interaction-based approaches, and their combinations <ref type="bibr" target="#b8">[9]</ref>.</p><p>Representation-based matching approaches are inspired by the Siamese architecture <ref type="bibr" target="#b3">[4]</ref>. This kind of approach aims at encoding each input text in a pair into the high-level representations respectively based on a specific neural network encoder, and then the matching score is obtained by calculating the similarity between the two corresponding representation vectors. DSSM <ref type="bibr" target="#b11">[12]</ref>, C-DSSM <ref type="bibr" target="#b28">[29]</ref>, ARC-I <ref type="bibr" target="#b10">[11]</ref>, RNN-LSTM <ref type="bibr" target="#b20">[21]</ref> and MV-LSTM <ref type="bibr" target="#b30">[31]</ref> belong to this category. Interaction-based matching approaches are closer to the nature of the matching task to some extent since they aim at directly capturing the local matching patterns between two input text, rather than focusing on the text representations. The pioneering work includes ARC-II <ref type="bibr" target="#b10">[11]</ref>, MatchPyramid <ref type="bibr" target="#b22">[23]</ref>, and Match-SRNN <ref type="bibr" target="#b31">[32]</ref>. Recently, there has been a trend that the two aforementioned branches of matching models should complement each other, rather than being viewed separately as two different approaches. DUET <ref type="bibr" target="#b19">[20]</ref> is composed of two separated modules, one in a representations-based way, and another in an interaction-based way, the final matching score is just their weighted-sum result. The attention mechanism is another way to combine the above two approaches, such as RE2 <ref type="bibr" target="#b34">[35]</ref> and BERT <ref type="bibr" target="#b5">[6]</ref>.</p><p>However, these existing approaches for short-form text matching have limited success in long-form text matching setting, due to their inability to capture and distill the main information from long documents. Besides, these models are computationally expensive because they simply use all textual data indiscriminately in the matching process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Long-form Text Matching</head><p>Few work directly focus on long-form text matching in the past years, mainly because the public datasets and the efficient algorithms are lacking, making its application scenarios have not been fully explored. In recent years, thanks to the pioneer work SMASH proposed by Jiang et al. <ref type="bibr" target="#b13">[14]</ref>, they are the first to point out that long-form text matching, e.g. source text and target text both are long-form text, has a wide range of application scenarios, such as attachment suggestion, article recommendation, and citation recommendation. They propose a hierarchical recurrent neural network under Siamese architecture which is a kind of representation-based matching approach. It synthesizes information from different document structure levels, including paragraphs, sentences, and words. SMITH model <ref type="bibr" target="#b33">[34]</ref> follows the SMASH's settings, then utilizes powerful pre-trained language model BERT <ref type="bibr" target="#b5">[6]</ref> as their key component and break the 512 tokens limitation to build a representation-based matching approach. Another work on long-form text matching is Concept Interaction Graph (CIG) <ref type="bibr" target="#b15">[16]</ref>, which concerns modeling the relation between two documents, e.g. same event or story. It can be treated as an interaction-based matching approach, which selects a pair of sentences based on their concepts and similarities.</p><p>Besides, they also construct two types of duplicate news detection datasets, which are labeled by professional editors.</p><p>All the previous works ignore the fact that long-form text provides overabundance information for matching, that is to say, there are usually many noises in the setting of long-form text matching. This phenomenon also be discussed in query-document matching tasks, where a query is a short-form text and a document is a long-form text. DeepRank <ref type="bibr" target="#b23">[24]</ref> is the first work to treat query and document differently, in their model, each query term is act as a filter that picks out text spans in the document which contain this query term. That is to say, query irrelevant text spans are the noise that can be ignored in the matching process. PACRR <ref type="bibr" target="#b12">[13]</ref> also has similar findings, they filter document words using two kinds of process, 1) keep first 𝑘 terms in the document or 2) retain only the text that is highly relevant to the given query. These previous works provide strong evidence that our noise filtering motivation can be effective for long-form text matching problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MATCH-IGNITION</head><p>As we have seen, two levels of noise are in the long-form text, thus the proposed Match-Ignition model aims to filter out these noises to achieve both effectiveness and efficiency. In this section, we first introduce the two components of Match-Ignition. They are sentence-level noise filter and word-level noise filter, shown in Figure <ref type="figure" target="#fig_1">2</ref>(a) and Figure <ref type="figure" target="#fig_1">2</ref>(c) respectively. After that, the model training details are described in the last subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence-level Noise Filtering</head><p>To enable the application of graph-based ranking algorithms PageRank to natural languages, such as documents, a graph is needed to build that represents the relation between sentences. TextRank <ref type="bibr" target="#b18">[19]</ref> makes it possible to form a sentence extraction algorithm, which can identify key sentences in a given document. It becomes a mature approach in automatic summarization. A direct way is to apply the TextRank algorithm on each long-form text independently, to reduce the length of the long-form text and get their summarizations. However, the goal of long-form text matching is to find the matching signals between a pair of text, which is different from automatic summarization that extracts key information from one text. Straightly applying the TextRank algorithm to each text independently leads to the problem of matching signals loss.</p><p>Inspired by the previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>, who tell us that two texts can help each other for noise detection, thus both of long-form texts should be represented in one graph to involve the matching information across two texts. Firstly, sentences in both long-form texts are collected together to form a united sentence collection. Formally, two long-form texts are first split into sentences, denoted as</p><formula xml:id="formula_0">𝑑 𝑠 = [𝑠 1 1 , 𝑠 1 2 , . . . , 𝑠 1 𝐿 1 ] and 𝑑 𝑡 = [𝑠 2 1 , 𝑠 2 2 , . . . , 𝑠 2 𝐿 2 ],</formula><p>where 𝐿 1 and 𝐿 2 are the number of sentences in 𝑑 𝑠 and 𝑑 𝑡 respectively. The united sentence collection</p><formula xml:id="formula_1">S = {𝑠 1 1 , 𝑠 1 2 , . . . , 𝑠 1 𝐿 1 , 𝑠 2 1 , 𝑠 2 2 , . . . , 𝑠 2 𝐿 2</formula><p>} then have 𝐿 1 + 𝐿 2 elements. Thus, the sentence similarity graph can be constructed by evaluating the sentence pair similarities in the united sentence collection S. The sentence similarity is defined as the same as in TextRank <ref type="bibr" target="#b18">[19]</ref>, to measures the overlapping word ratio between two sentences:</p><formula xml:id="formula_2">𝑆𝑖𝑚(𝑠 𝑖 , 𝑠 𝑗 ) = |{𝑤 𝑘 |𝑤 𝑘 ∈ 𝑠 𝑖 , 𝑤 𝑘 ∈ 𝑠 𝑗 }| log(|𝑠 𝑖 |) + log(|𝑠 𝑗 |) , 𝑠 𝑖 , 𝑠 𝑗 ∈ S,<label>(1)</label></formula><p>where 𝑤 𝑘 denotes the word in the sentence, | • | denotes the length of the sentence or word set, and 𝑠 𝑖 , 𝑠 𝑗 are two sentences in the united sentence collection S. To make sentence similarity sparsity e.g. returns 0 at the most of the time, we remove the stopwords in the sentences before we calculate the similarities. Thus, the final sentence similarity graph has sparse links. Finally, a PageRank algorithm is applied to this constructed sentence similarity graph, to get the important score of each sentence. To balance the information coming from different long-form texts for the following step, the top 𝜆 sentences are extracted for each long-form texts respectively. Thus, both texts contain 𝜆 sentences as their digestion, which we called a sentence-level filter.</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>(b), the selected sentences are concatenated as a text sequence, which starts with [CLS] token and separates by [SEP] token. It is then treated as the input of the model in the word-level filter. Note that the hyper-parameter 𝜆 should be neither too small to lose a lot of information, nor too large to make text extremely long. A suitable 𝜆 can yield a moderate text sequence, which length is just less than the BERT max input length.</p><p>PageRank algorithm can also be used at word-level if we can define a word-by-word relation graph. However, sentences are adjective from each other, noise in this level is discrete than an entire sentence can be removed in an unsupervised way, while a word relies on its context to express concrete meanings, noise in this level is continuous that should be estimated during the model training. Therefore, we need to construct a graph within the Transformer model structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word-level Noise Filtering</head><p>To filter the noise in the word-level, a word-level graph needs to be constructed first in the inherent transformer structure (Sec 3.2.1). After that, the traditional PageRank algorithm is required to implement as a tensor version, for better to embed into the transformer structure (Sec 3.2.2). Finally, we propose our plug PageRank to the Transformer model for word-level noise filtering (Sec 3.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Transformer as a Graph.</head><p>Transformer architecture <ref type="bibr" target="#b29">[30]</ref> boosts the natural language processing a lot, where most well-known models are a member of this family, such as BERT <ref type="bibr" target="#b6">[7]</ref>, RoBERTa <ref type="bibr" target="#b16">[17]</ref>, and GPT2 <ref type="bibr" target="#b25">[26]</ref>. They achieve state-of-the-art performance in almost all NLP tasks, e.g. named entity recognition, text classification, machine translation, and also text semantic matching. For long-form text matching, we also adopt this architecture.</p><p>The self-attention block is the main component in Transformer architecture, which figure out how important all the other words in the sentence are for the contextual word around it. Thus, selfattention block builds the relations between words, that can be viewed as a fully connected graph among words <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref>. Knowing that the updated word representations are simply the sum of linear transformations of representations across all the words, weighted by their importance. It makes full use of the attention mechanism in deep neural networks to update word representations. As have shown in <ref type="bibr" target="#b29">[30]</ref>, the attention function can be formalized as a scaled dot-product attention with inputs H 𝑙 :</p><formula xml:id="formula_3">H 𝑙+1 = Attention(Q 𝑙 , K 𝑙 , V 𝑙 ) = Softmax Q 𝑙 (K 𝑙 ) 𝑇 √ 𝐸 V 𝑙 = A 𝑙 V 𝑙 ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">Q 𝑙 = W Q 𝑙 H 𝑙 ∈ R 𝑁 ×𝐸 denote the attention query matrices, K 𝑙 = W K 𝑙 H 𝑙 ∈ R 𝑁 ×𝐸</formula><p>the key matrix, and V 𝑙 = W V 𝑙 H 𝑙 ∈ R 𝑁 ×𝐸 the value matrix. 𝑁 denotes the number of words in a text, and 𝐸 denotes the dimensions of the representation. The attention mechanism can be explained as: for each attention query vector in Q, it first computes the dot products of the attention query with all keys, aiming to evaluate the similarity between the attention query and each key. Then, it is divided each by √ 𝐸, and applies a softmax function to obtain the weights on the values, denotes as A 𝑙 . Finally, the new representation of the attention query vector is calculated as weighed sum of values. Getting this dot-product attention mechanism to work proves to be tricky bad random initializations can de-stabilize the learning process. It can be overcome by performing multiple 'heads' of attention and concatenating the result:</p><formula xml:id="formula_5">H 𝑙+1 = Concat(ℎ𝑒𝑎𝑑 1 , • • • , ℎ𝑒𝑎𝑑 𝐻 )O 𝑙 ℎ𝑒𝑎𝑑 𝑘 = Attention(Q 𝑘𝑙 , K 𝑘𝑙 , V 𝑘𝑙 ) = A 𝑘𝑙 V 𝑘𝑙 ,<label>(3)</label></formula><p>where Q 𝑘𝑙 , K 𝑘𝑙 and V 𝑘𝑙 are of the 𝑘-th attention head at layer 𝑙 with different learnable weights, O 𝑙 down-projection to match the dimensions across layers, 𝐻 is the number of the heads in each layer and 𝐿 is the number of the layers.</p><p>If we treat each word as a node in a graph, they update their representations by aggregating all other contextual word representations, just like messages passing from other neighbor nodes in graph neural network <ref type="bibr" target="#b27">[28]</ref>. Thus, for self-attention block, it can be treated as a fully-connected word graph, where its adjacency matrix is the transpose of word-by-word similarity matrix A 𝑘𝑙 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">PageRank in A Tensor</head><p>View. PageRank <ref type="bibr" target="#b2">[3]</ref>, is a graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph. Formally, given a graph 𝐺 (𝑉 , 𝐸), where 𝑉 = {𝑣 1 , 𝑣 2 , . . . , 𝑣 𝑁 } is a set of nodes and 𝐸 is the links between these nodes. The goal is to determine the order of these nodes that the more important node has a higher rank. The PageRank value on each node 𝑣 𝑖 , denotes as 𝑢 𝑖 , is used to indicate the importance of the node 𝑣 𝑖 . For convenience, we define A as the adjacency matrix, that A 𝑖 𝑗 denotes the 𝑣 𝑖 has a link from 𝑣 𝑗 with weight A 𝑖 𝑗 . A is also a stochastic matrix because each column sums up to 1. At the initial step all 𝑢 𝑖 have the same value 1/𝑁 , denotes that all nodes are equally important. At each following step , then PageRank value 𝑢 𝑖 is updated using other nodes and links pointed to it,</p><formula xml:id="formula_6">𝑢 𝑖 = ∑︁ 𝑣 𝑗 ∈𝑉 A 𝑖 𝑗 • 𝑢 𝑗 .<label>(4)</label></formula><p>After several iterations, the PageRank values 𝑢 𝑖 will converge to a set of stable values 𝑢 𝑖 , and that is the solution of PageRank.</p><p>To implement PageRank in a tensor-based computational framework, such as TensorFlow <ref type="bibr" target="#b0">[1]</ref> or PyTorch <ref type="bibr" target="#b24">[25]</ref>, we need a tensor version of PageRank algorithm. Let u 𝑡 = [𝑢 𝑡 1 , 𝑢 𝑡 2 , . . . , 𝑢 𝑡 𝑛 ] to be a vector of length 𝑁 , that obtains all nodes PageRank values at step 𝑡. Then, PageRank can be rewritten as,</p><formula xml:id="formula_7">u 𝑡 +1 = Au 𝑡 .<label>(5)</label></formula><p>To solve the problem of isolated nodes, a stable version of PageRank is proposed <ref type="bibr" target="#b2">[3]</ref> and adopted by our work,</p><formula xml:id="formula_8">u 𝑡 +1 = 𝑑Au 𝑡 + 1 − 𝑑 𝑁 I,<label>(6)</label></formula><p>where 𝑑 ∈ [0, 1] is a real value to determine the ratio of the two parts, and I is a vector of length 𝑁 with all its values are 1. The factor 𝑑 is usually set to 0.85, and this is the value we are also using in our implementation.</p><p>In practice, the iteration steps 𝑇 is set to a fixed value for computational efficiency. Thus, u 𝑡 is the final PageRank scores for each 𝑣 𝑖 ∈ 𝑉 , and the larger of PageRank denotes the more important of this node in the current graph, thus we can filter out the nodes with small PageRank values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Plug PageRank in Transformer.</head><p>In this section, we propose a novel approach that plugs PageRank in the Transformer model to filter the noise at the word-level. Notice that, word-level noise is composite and dynamic, thus need to be estimated dynamically during the matching process, so in each self-attention block, an inherent PageRank algorithm is utilized to dynamically filter the noisy words, which can reduce the sequence length layer by layer.</p><p>Standard Transformer structure, which has been selected as our base model structure, has 𝐿 layers of multi-head self-attention blocks, stacked one after another, and maintains the same sequence length 𝑁 at each layer. From the description in Section 3.2.1, we have known that self-attention block in Transformer can be treated as a word-by-word graph, which can be specified using an adjacency matrix (A 𝑘𝑙 ) ⊤ at 𝑘-th head and 𝑙-th layer in Eq 3. The word-level noise filtering process is once per layer, thus we need to average the effects of all adjacency matrices across different heads in the 𝑙-th layer,</p><formula xml:id="formula_9">A 𝑙 = 1 𝐻 ∑︁ 𝐻 𝑘=1 A 𝑘𝑙 . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>Because A 𝑙 is the output of row-wise Softmax function, each row of A 𝑙 sum to 1. Thus, (A 𝑙 ) ⊤ is a stochastic matrix, which can be treated as the adjacency matrix in a graph. With above observation, we substitute (A 𝑙 ) ⊤ into Eq 5 and yield:</p><formula xml:id="formula_11">u 𝑡 +1 = 𝑑 (A 𝑙 ) ⊤ u 𝑡 + 1 − 𝑑 𝑁 I.<label>(8)</label></formula><p>Iteratively solving the equation above, we then get the PageRank values for all words/nodes in the (𝑙 −1)-th layer, denote as u. Thus, u represents the importance of the words in the (𝑙 − 1)-th layer. After applying the attention mechanism to the words in the (𝑙 −1)-th layer, we get a list of new word representations as to the input of 𝑙-th layer. To filter noisy words, we have to estimate the importance of the words/nodes in 𝑙-th layer, which can be evaluated by redistributing the importance of the word in (𝑙 − 1)-th layer under the distribution A 𝑙 :</p><formula xml:id="formula_12">r = A 𝑙 u.<label>(9)</label></formula><p>Finally, we can reduce the sequence length at 𝑙-th layer by removing the nodes which have the small values in r.</p><p>In this work, we design a strategy that remove the percentage 𝛼 ∈ [0%, 100%] nodes per layer, so that the 𝑙-th layer has (𝛼) 𝑙−<ref type="foot" target="#foot_0">1</ref> • 𝑁 nodes. The hyper-parameter 𝛼 is called a word reduction ratio. For example, let 𝐿 = 12, 𝑁 = 400, if we set 𝛼 to 10%, the numbers of nodes at each layer are <ref type="bibr">400,</ref><ref type="bibr">360,</ref><ref type="bibr">324,</ref><ref type="bibr">291,</ref><ref type="bibr">262,</ref><ref type="bibr">236,</ref><ref type="bibr">212,</ref><ref type="bibr">191,</ref><ref type="bibr">172,</ref><ref type="bibr">154,</ref><ref type="bibr">139,</ref><ref type="bibr">125</ref>.</p><p>For the BERT model, some words are too special to be removed, such as [CLS] token and [SEP] token. If the model occasionally removes these tokens during the training, it will lead to an unstable training process. It also affects the overall performance. Therefore, a token mask is designed to keep these tokens across all the layers.</p><p>Discussions: Many previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref> have also noticed the relation between Transformer and graph. Star-Transformer <ref type="bibr" target="#b9">[10]</ref> adds a hub node to model the long-distance dependence and eliminates the links far from 3-term steps. TransformerXL <ref type="bibr" target="#b4">[5]</ref> uses a segment-level recurrence with a state reuse strategy to remove all the links between words in different segments, so that can break the fixed-length limitation. Sparse-Transformer <ref type="bibr" target="#b35">[36]</ref> explicitly eliminate links in which attention scores are lower than the threshold to make the attention matrix sparse. All of these previous works focus on eliminating links in the graph, while in this work, we focus on filtering noise words, as well as nodes, in the graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training</head><p>The Match-Ignition consists of two components, sentence-level filter, and word-level filter. The sentence-level filter is the heuristic approach that does not need a training process. Thus, in this section, we only consider model training for the word-level filter component.</p><p>For the model training of word-level filter, we adopt the "pretraining + fine-tuning" paradigm as in BERT. In this paradigm, the pre-trained Transformer is firstly obtained using a large unlabeled plain text in an unsupervised learning fashion. Then, the Transformer with plugging PageRank at each layer is fine-tuned using the supervised downstream task. Note that word-level filters do not change the parameters in the original Transformer, due to all the parameters in the Transformer are input sequence length independent. Therefore, change the sequence length layer by layer does not affect the structure of the Transformer. Benefit from the good property of PageRank-Transformer, we can directly adopt a publicly released Transformer model, such as BERT or RoBERTa trained on a large corpus, as our pre-trained model.</p><p>In the fine-tuning step, we add the PageRank module in each self-attention layer, without introducing any additional parameters. The objective function for long-form text matching task is a binary cross-entropy loss:</p><formula xml:id="formula_13">L = − ∑︁ 𝑖 𝑦 𝑖 log 𝑝 𝑖 + (1 − 𝑦 𝑖 ) log(1 − 𝑝 𝑖 ),<label>(10)</label></formula><p>where 𝑝 𝑖 is the probability represents the matching score, generated by the representation of [CLS], and 𝑦 𝑖 is the ground-truth label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct experiments and in-depth analysis on a publicly large dataset to demonstrate the effectiveness and efficiency of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Very few public datasets for long-form text matching tasks, except the Chinese News Same Event dataset (CNSE) and Chinese News Same Story dataset (CNSS) have been released in <ref type="bibr" target="#b15">[16]</ref>. They are constructed based on large Chinese news articles, collected from major Internet news providers in China, covering diverse topics in the open domain 1 . The task is to identify whether a pair of news articles report the same breaking news (or event) for the CNSE dataset or whether they belong to the same series of a news story for the CNSS dataset. All of this pair of news articles are labeled by professional editors. Note that the major event (or story) is labeled since, in the real world, each breaking news article on the Internet must be intended to report some specific breaking news that has just happened to attract clicks and views. The objective of this dataset is to determine whether two news articles intend to report the same breaking news. The CNSE dataset contains 29,063 pairs of news articles, and the CNSS dataset contains 33,503 pairs of articles. We follow the settings in <ref type="bibr" target="#b15">[16]</ref> and split it into training, development, and testing set with the portion of instances 6:2:2. We carefully ensure that different splits do not contain any overlaps to avoid data leakage. The average number of words (represents Chinese characters in this paper) for all documents in the datasets is about 990 and the maximum value is 21,791. The negative samples in the two datasets are not randomly generated. Document pairs that contain similar keywords are selected and exclude samples with TF-IDF similarity below a certain threshold. The detailed datasets statistics are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Evaluation Metrics: The metrics used for performance evaluation are the accuracy and F1 scores, as the same as that in <ref type="bibr" target="#b15">[16]</ref>, which are the typical evaluation metrics for binary classification tasks. For each evaluated method, we perform training for 10 epochs and then choose the epoch with the best validation performance to be evaluated on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and Experimental Settings</head><p>We adopt two types of baseline methods for comparison, including traditional term-based methods and recent deep learning methods.</p><p>For traditional term-based methods, the implementation details are listed as follows and the experimental results directly bring from <ref type="bibr" target="#b15">[16]</ref>:</p><p>• BM25 [27]: a highly effective retrieval model that represents the classical probabilistic retrieval model. • LDA <ref type="bibr" target="#b1">[2]</ref>: a topic model that constructs document vector based on words co-occurrence in the documents. • SimNet <ref type="bibr" target="#b15">[16]</ref>: it extracts five text-pair similarities, including the TF-IDF cosine similarity, TF cosine similarity, BM25 cosine similarity, Jaccard similarity of 1-gram, and Ochiai similarity. These similarity scores are concatenated into a vector, classifying by a multi-layer neural network.</p><p>For deep learning methods, we compare three types of text matching models, including three representation-based approaches, i.e. DSSM <ref type="bibr" target="#b11">[12]</ref>, C-DSSM <ref type="bibr" target="#b28">[29]</ref>, and ARC-I <ref type="bibr" target="#b10">[11]</ref>, four interaction-based approaches, i.e. ARC-II <ref type="bibr" target="#b10">[11]</ref>, MatchPyramid <ref type="bibr" target="#b22">[23]</ref>, and three combination approaches DUET <ref type="bibr" target="#b19">[20]</ref>, RE2 <ref type="bibr" target="#b34">[35]</ref>, and BERT-Finetuning <ref type="bibr" target="#b6">[7]</ref>. Some implementation details are listed as follows:</p><p>• • DUET <ref type="bibr" target="#b19">[20]</ref>: a combination model that directly weighted sum representation-based and interaction-based models' results. • RE2 <ref type="bibr" target="#b34">[35]</ref>: a combination model using attention mechanism, which is keeping three key features directly available for inter-sequence alignment and fusion. We use the default settings of the model in the original paper. • BERT-Finetuning <ref type="bibr" target="#b6">[7]</ref>: a combination model using an attention mechanism, especially the Transformer structure. It is fine-tuned on text matching tasks by a large-scale pretraining language model, for example, BERT for Chinese<ref type="foot" target="#foot_1">2</ref> .</p><p>The results of DSSM, C-DSSM, ARC-I, ARC-II, MatchPyramid, and DUET are from the previous work <ref type="bibr" target="#b15">[16]</ref>, which uses the implementations from MatchZoo <ref type="bibr" target="#b7">[8]</ref> for the evaluation of these models <ref type="foot" target="#foot_2">3</ref> . BERT-Finetuning uses a pre-trained BERT model on Chinese, e.g. "bert-base-chinese", from the HuggingFace website. It is fine-tuned 10 epochs on the training set. RE2 is implemented using released code by the author <ref type="foot" target="#foot_3">4</ref> .</p><p>Concept Interaction Graph (CIG) model <ref type="foot" target="#foot_4">5</ref> is the state-of-the-art approach on CNSE and CNSS dataset.</p><p>• CIG-Siam-GCN <ref type="bibr" target="#b15">[16]</ref>: it generates the matching vector using a Siamese encoder for each vertex, after that a GCN is applied to this constructed graph to obtain the matching score. • CIG-Sim&amp;Siam-GCN <ref type="bibr" target="#b15">[16]</ref>: it generates the matching vector using Siamese encoder and term-based similarity encoder for each vertex, after that, a GCN is applied to this constructed graph to obtain the matching score. • CIG-Sim&amp;Siam-GCN-Sim 𝑔 <ref type="bibr" target="#b15">[16]</ref>: use of additional global features given by the five term-based similarity metrics mentioned in SimNet.</p><p>The hyper-parameters of our Match-Ignition model are listed below. In sentence-level filter, the number of selected sentences per long-form text 𝜆 is set to 5, the 𝑑 in PageRank algorithm defined in Eq 5 is set to 0.85. In word-level filter, we adopt a pre-trained BERT model for Chinese, e.g. "bert-base-chinese", which contains 𝐻 = 12 heads and 𝐿 = 12 layers. The words reduce ratio 𝛼 is set to 10%, that is to say, we remove 10% words per layer by default. The fine-tuning optimizer is Adam <ref type="bibr" target="#b14">[15]</ref> with the learning rate 10 −5 , 𝛽 1 = 0.9, 𝛽 2 = 0.999, 𝜖 = 10 −8 and batch size is set to 8. The model is built based on the Transformers<ref type="foot" target="#foot_5">6</ref> library using PyTorch 1.2 <ref type="bibr" target="#b24">[25]</ref>. The source code will be released at https://github.com/xxx/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>The performance comparison results of Match-Ignition against baseline models are shown in Table <ref type="table">2</ref> on the CNSE dataset and CNSS dataset respectively. From these experimental results, we can summarize as follow:</p><p>1) The importance of exact matching signals. The first blocks of Table <ref type="table">2</ref> illustrates the performances of traditional term-based methods. Their good performances illustrate that global matching features for long-form text pairs are important. Term-based methods assume that long-form text is a bag of words, and the matching Table <ref type="table">2</ref>: Experimental results on CNSE and CNSS datasets. Significant performance degradation with respect to Match-Ignition is denoted as (-) with p-value ≤ 0.05. We only do significant test on the models reimplemented from the source code, while the results bring from <ref type="bibr" target="#b15">[16]</ref> do not test due to the lack of the detailed predictions. 2) Text verbosity affects the performance of short text matching models. For representation-based approaches, when the text is long, it is hard to get an appropriate context vector representation for matching. For interaction-based approaches, most of the interactions between words in two long articles will be meaningless. A similar analysis can be found in applying the MatchPyramid model to the information retrieval task <ref type="bibr" target="#b21">[22]</ref>. That is the reason why recent short text matching models fail in long-form text matching task, see the second blocks of Table <ref type="table">2</ref>.</p><p>3) Benefit from modeling diverse matching requirements. As we can see, in the third blocks of Table <ref type="table">2</ref>, a combination of representationbased and interaction-based approaches leads to great improvement, e.g. about 14% accuracy in CNSE and 23% accuracy in CNSS. That is because combination approaches can model the diverse matching requirements using the attention mechanism. For the verbosity hypothesis, self-attention can aggregate all the information in the long-range. For the scope hypothesis, attention scores help the model to focus on a part of the long-form text at a time. Note that the naive weighted-sum combination approach, like DUET, does not work well for long-form text matching.</p><p>4) Noise filtering is helpful. The proposed Match-Ignition model outperforms other baselines on both CNSE and CNSS datasets. It achieves a new state-of-the-art performance comparing to the CIG based models. Note that the performance of the CIG model is largely depended on the term-based features, e.g. five text-pair similarities extracted in SimNet. It drops significantly when these hand-crafted features are removed, comparing the performances between CIG-Siam-GCN and CIG-Sim&amp;Siam-GCN. However, the Match-Ignition model achieves a higher performance without involving any handcrafted features. It only depends on realizing that noise filtering is an important job for long-form text matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To demonstrate the effects of two levels of noise filtering strategy, in this section, we do an ablation study on our Match-Ignition model. The experimental results are shown in Table <ref type="table" target="#tab_2">3</ref>.</p><p>The model "− Sentence-level Filter" denotes that we skip sentencelevel filtering that directly selects the top 𝜆 sentences for each longform text, while the word-level filter still exists. In contrast, the model "− Word-level Filter" denotes that we remove the word-level filtering, instead of using the original Transformer directly, while the sentence-level filter still exists. If we remove all the noise filters, the model reduces to "BERT-Finetuning" at the bottom line in Table <ref type="table" target="#tab_2">3</ref>.</p><p>As we can see, the same conclusions on both CNSE and CNSS datasets including 1) sentence-level and word-level filters are useful for the final performance, 2) using an identical pre-trained BERT model, noise filtering strategy brings a significant improvement, e.g. about 5% accuracy on CNSE and 4% accuracy on CNSS comparing with Match-Ignition and BERT-Finetuning.</p><p>There exist some differences between CNSE and CNSS datasets. For CNSE, a sentence-level filter is more efficient than a word-level filter. And besides, if we use a word-level filter only, it will harm the overall performance. In this dataset, to make a word-level filter function well, we need the help of a sentence-level filter. In contrast, in the CNSS dataset, either sentence-level or word-level filter leads to a large improvement. Unfortunately, the chain on these two filters brings in a very limited improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of Words Reduction Ratio</head><p>The words reduction ratio of 𝛼 is a major hyper-parameter in the word-level filter, which use to determine how many words/nodes should be deleted in each layer. Frankly speaking, it is not an optimal  way to reduce the number of words across the layers, but it is an alternative way to demonstrate the effects of word-level filtering.</p><p>As shown in Table <ref type="table" target="#tab_3">4</ref>, we evaluate four types of words reduction ratio, where 𝛼 = 0% means the word-level filter is turned off. The results illustrate that too small or too large a value of 𝛼 will lead to bad performance. Let 𝛼 = 10% yields the best performances on both CNSE and CNSS datasets. It also got the same conclusion in Section 4.4 that the CNSE dataset is sensitive to the word-level filter, while the CNSS dataset is not if the sentence-level filter is active.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Time Complexity</head><p>In this section, we further evaluate the efficiency of the Match-Ignition models Note that the sentence-level noise filtering is very fast that can be ignored when considering the time cost in the wordlevel noise filtering. Moreover, calculating word-by-word similarity matrix is the main computational cost in the Transformer, therefore, we only consider the time cost of that. In theoretically, let 𝑁 denotes the length of the text, 𝐿 denotes the number of layers, 𝛼 denotes the words reduction ratio at each layer, the computation cost can be approximated as:</p><formula xml:id="formula_14">𝑇 𝑖𝑚𝑒𝐶𝑜𝑠𝑡 (𝛼) = ∑︁ 𝐿−1 𝑙=0 (1 − 𝛼) 2𝑙 .<label>(11)</label></formula><p>Using the above equation, under the settings of the model in experiments, we have 𝑇𝑖𝑚𝑒𝐶𝑜𝑠𝑡 (0%) = 12, 𝑇𝑖𝑚𝑒𝐶𝑜𝑠𝑡 (5%) = 7.26, 𝑇𝑖𝑚𝑒𝐶𝑜𝑠𝑡 (10%) = 4.84, and 𝑇𝑖𝑚𝑒𝐶𝑜𝑠𝑡 (20%) = 2.76. Thus, 𝛼 = 20% is 4 times faster than 𝛼 = 0% in theoretical. We conduct our experiments on a single 12GB Nvidia K80 GPU with batch size 8, and the results are shown in Table <ref type="table" target="#tab_3">4</ref>. As we can see, 𝛼 = 20% is 1.6 times faster than 𝛼 = 0% at the training stage and 2 times faster in the evaluation stage.   the constructed sentence-level graph built-in Match-Ignition. The difference indicates the rationality of our model. For example, sentence 2238-01 in Doc2 highly connected with Doc1 becomes more important, while other sentences without any link with Doc2 becomes less important. That is to say, two documents can help each other to determine the key sentences. For words, we show their importance scores in different colors. Specifically, the importance is evaluated based on the number of layers retaining the word. The results show that more important words for matching will be kept until the last layer of the network, some of which are highlighted with rectangles. Furthermore, special tokens like [CLS] and [SEP] are also important for long-form text matching, which is a different form of short-form text matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a novel hierarchical noise filtering approach for the long-form text matching problem, based on the fact that only some keywords and sentences are critical for the matching, rather than all text information. The designed Match-Ignition model utilizes the well-known PageRank algorithm to identify and filter both sentence and word-level noisy information. The sentence-level filter of Match-Ignition is proposed to obtain key sentences based on the constructed sentence graph. While the word-level filter of Match-Ignition combines the filtering process and the matching process and can be jointly learned, to reflect the contextual dependencies between words. Specifically, a word graph is first built based on the attention scores in each self-attention block of Transformer, and keywords are then selected by applying PageRank on this graph. In this way, noisy words will be filtered out layer by layer in the matching process. Experimental results on public datasets demonstrate the effectiveness and efficiency of the Match-Ignition. Besides, it outperforms both traditional text matching models for short text and recent long-form text matching models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Sentence1:</head><label></label><figDesc>What makes a pizza the best? Short-form Text Matching (Community Question Answering): Sentence2: What makes a good pizza? Long-form Text Matching (Redundancy News Identification): Doc2: … McGrady had one of the most memorable performances of his career, the final 35 seconds win 13 points when against the San Antonio Spurs to secure a comeback victory. The sequence included four consecutive three-pointers … Doc1: … it took unreal shooting by Tracy McGrady. McGrady made four 3-point shots and scored 13 points in the final</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall architecture of Match-Ignition. (a) represents the sentence-level filter, (b) represents the outputs of the sentence-level filter, and (c) represents the word-level filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>DSSM [12]: a representation-based deep matching model with three layers feedforward network for each document. • C-DSSM [29]: a representation-based deep matching model with three layers 1D convolutional neural network for each document. • ARC-I [11]: a representation-based deep matching model with three layers 1D convolutional neural network for each document. • ARC-II [11]: an interaction-based deep matching model with two layers 2D convolutional neural network. • MatchPyramid [23]: an interaction-based deep matching model with two layers 2D convolutional neural network and uses the dot-product function to construct the word-level interaction matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>To illustrate the Match-Ignition model more intuitively, we give an example from the CNSE dataset, and visualize the sentence-level graph (Fig 3 (a)(b)) and word-level words importance (Fig 3 (c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>[Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) sentence graph for each document using Tex-tRank, (b) sentence graph built in Match-Ignition, each sentence is a node in the graph, its color represents the document it belongs to and its size represents the importance (PageRank value). (c) illustrates the word importances, and the darker color means the more important word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 3 (</head><label>3</label><figDesc>Fig 3 (a) demonstrates the graph if directly applying the Tex-tRank algorithm on each document separately, and Fig 3 (b) showsthe constructed sentence-level graph built-in Match-Ignition. The difference indicates the rationality of our model. For example, sentence 2238-01 in Doc2 highly connected with Doc1 becomes more important, while other sentences without any link with Doc2 becomes less important. That is to say, two documents can help each other to determine the key sentences. For words, we show their importance scores in different colors. Specifically, the importance is evaluated based on the number of layers retaining the word. The results show that more important words for matching will be kept until the last layer of the network, some of which are highlighted with rectangles. Furthermore, special tokens like [CLS] and [SEP] are also important for long-form text matching, which is a different form of short-form text matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Description of evaluation datasets.</figDesc><table><row><cell cols="2">Dataset Doc Len.</cell><cell>Pos / Neg</cell><cell>Train Dev Test</cell></row><row><cell>CNSE</cell><cell>982.7</cell><cell cols="2">12865 / 16198 17438 5813 5812</cell></row><row><cell>CNSS</cell><cell>996.6</cell><cell cols="2">16887 / 16616 20102 6701 6700</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of Match-Ignition, symbol '−' means to remove a specific component.</figDesc><table><row><cell></cell><cell cols="2">CNSE</cell><cell>CNSS</cell><cell></cell></row><row><cell>Model</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>Match-Ignition</cell><cell cols="4">86.32 84.55 91.28 91.39</cell></row><row><cell cols="5">− Sentense-level Filter 80.31 76.39 91.10 91.18</cell></row><row><cell>− Word-level Filter</cell><cell cols="4">84.11 82.17 91.04 91.07</cell></row><row><cell>BERT-Finetune</cell><cell cols="4">81.30 79.20 86.64 87.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The impact of word reduction ratio 𝛼 and the execution time of these models.</figDesc><table><row><cell>Word</cell><cell>CNSE</cell><cell></cell><cell>CNSS</cell><cell></cell><cell cols="2">Time per batch</cell></row><row><cell>Denoise</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Train</cell><cell>Eval</cell></row><row><cell>0%</cell><cell cols="5">84.11 82.17 91.04 91.07 1.73s</cell><cell>0.42s</cell></row><row><cell>5%</cell><cell cols="5">85.68 83.65 90.70 90.73 1.58s</cell><cell>0.37s</cell></row><row><cell>10%</cell><cell cols="5">86.32 84.55 91.28 91.39 1.33s</cell><cell>0.31s</cell></row><row><cell>20%</cell><cell cols="5">82.55 79.66 90.25 90.21 1.07s</cell><cell>0.21s</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Datasets are available at https://github.com/BangLiu/ArticlePairMatching</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Download from https://huggingface.co/bert-base-chinese.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Code is available at https://github.com/NTMC-Community/MatchZoo.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Code is available at https://github.com/alibaba-edu/simple-effective-text-matching.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">Code is available at https://github.com/BangLiu/ArticlePairMatching</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">Code is available at https://github.com/huggingface/transformers.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">2003. Jan (2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<meeting><address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianpeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07270</idno>
		<title level="m">Matchzoo: A toolkit for deep text matching</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep look into neural ranking models for information retrieval</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="page">102067</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Star-Transformer</title>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1315" to="1325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
				<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PACRR: A Position-Aware Neural IR Model for Relevance Matching</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melo</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic Text Matching for Long-Form Documents</title>
		<author>
			<persName><forename type="first">Jyun-Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Golbandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313707</idno>
		<ptr target="https://doi.org/10.1145/3308558.3313707" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<meeting><address><addrLine>San Francisco, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="795" to="806" />
		</imprint>
	</monogr>
	<note>WWW &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matching Article Pairs with Graphical Decomposition and Convolutions</title>
		<author>
			<persName><forename type="first">Bang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yancheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunfeng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1632</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1632" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6284" to="6294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A deep architecture for matching short texts</title>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1367" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into text</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 conference on empirical methods in natural language processing</title>
				<meeting>the 2004 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to match using local and distributed representations of text for web search</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Bhaskar Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1291" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</title>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinying</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rabab</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="694" to="707" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>TASLP)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04648[cs.IR]</idno>
		<title level="m">A Study of MatchPyramid Models on Ad-hoc Retrieval</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text matching as image recognition</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeprank: A new deep architecture for relevance ranking in information retrieval</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM CIKM</title>
				<meeting>the 2017 ACM CIKM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Probabilistic Relevance Framework: BM25 and Beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on conference on information and knowledge management</title>
				<meeting>the 23rd ACM international conference on conference on information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title>
		<author>
			<persName><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04378</idno>
		<title level="m">Match-srnn: Modeling the recursive matching structure with spatial rnn</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12297[cs.IR]</idno>
		<title level="m">Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Document Matching</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Runqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00300</idno>
		<title level="m">Simple and Effective Text Matching with Richer Alignment Features</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Sparse Transformer: Concentrated Attention Through Explicit Selection</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
