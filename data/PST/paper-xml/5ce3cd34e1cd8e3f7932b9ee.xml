<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aging-aware Lifetime Enhancement for Memristor-based Neuromorphic Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuhang</forename><surname>Zhang</surname></persName>
							<email>shuhang.zhang@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Chair of Electronic Design Automation</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Advanced Study</orgName>
								<orgName type="institution">Technical University of Munich (TUM)</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Grace</forename><forename type="middle">Li</forename><surname>Zhang</surname></persName>
							<email>grace-li.zhang@tum.de</email>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
							<email>b.li@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Chair of Electronic Design Automation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Helen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair of Electronic Design Automation</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Advanced Study</orgName>
								<orgName type="institution">Technical University of Munich (TUM)</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ulf</forename><surname>Schlichtmann</surname></persName>
							<email>ulf.schlichtmann@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Chair of Electronic Design Automation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Aging-aware Lifetime Enhancement for Memristor-based Neuromorphic Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Memristor-based crossbars have been applied successfully to accelerate vector-matrix computations in deep neural networks. During the training process of neural networks, the conductances of the memristors in the crossbars must be updated repetitively. However, memristors can only be programmed reliably for a given number of times. Afterwards, the working ranges of the memristors deviate from the fresh state. As a result, the weights of the corresponding neural networks cannot be implemented correctly and the classification accuracy drops significantly. This phenomenon is called aging, and it limits the lifetime of memristor-based crossbars. In this paper, we propose a co-optimization framework combining software training and hardware mapping to reduce the aging effect. Experimental results demonstrate that the proposed framework can extend the lifetime of such crossbars up to 11 times, while the expected accuracy of classification is maintained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Neural networks have been applied in various fields and achieved remarkable performances. As the structure of neural networks is becoming more complex, a huge number of vector-matrix multiplications are required. However, traditional CMOS-based architectures are inefficient in implementing these computations. Although general-purpose and specific hardware platforms, e.g., GPU <ref type="bibr" target="#b0">[1]</ref> and FPGA <ref type="bibr" target="#b1">[2]</ref>, have been adopted to implement neural networks, power consumption is still a limiting factor for such platforms <ref type="bibr" target="#b2">[3]</ref>. To meet the increasing computing demand in complex neural networks, memristor-based crossbar has been introduced as a new circuit architecture to implement vector-matrix multiplications. By taking advantage of its analog nature, this architecture can provide a remarkable computing efficiency with a significantly lower power consumption.</p><p>One of the major properties of memristors is that their conductances can be programmed. With this feature, computingintensive vector-matrix multiplication operations can be implemented efficiently, with the crossbar architecture shown in Fig. <ref type="figure" target="#fig_0">1</ref>  <ref type="bibr" target="#b3">[4]</ref>. The voltage vector V I = [V I 1 , V I 2 , ...V I m ] applied to the memristors in the jth column generates a current</p><formula xml:id="formula_0">I O j = m i=1 V I i g ij</formula><p>, where g ij is the conductance of the memristor in the ith row and jth column. Furthermore, the current at the bottom of the jth column is converted to voltage. Considered as a whole, the relation between the input voltage vector V I and the output voltage vector V O can be expressed as</p><formula xml:id="formula_1">V O = V I ? G ? R,</formula><p>where G is the conductance matrix and R is the diagonal resistance matrix. Since the conductances G of the memristors can be programmed, this crossbar can thus be used to accelerate the vector-matrix computations in neural networks.</p><p>Memristor crossbars have successfully been implemented in hardware to accelerate neural networks <ref type="bibr" target="#b4">[5]</ref>. The methods to integrate them into computing systems can be categorized into two groups. The first approach is online training <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In this approach, the conductances of the memristors are firstly programmed and then tuned in further iterations according to the accuracy of classification. The second approach of using memristor-based crossbars combines software training and online tuning. At first, the training process is performed at software level and the trained weights are mapped to the conductances of memristors. Thereafter, the conductances are further fine tuned to improve the classification accuracy. This method can achieve an expected accuracy more rapidly because the initial mapped conductances are already close to their target values.</p><formula xml:id="formula_2">V I 2 V I 1 V I i V I m V o 1 R2 R1 Rj Rn g1j gij g2j gmj V o 2 V o j V o n I O j = m i=1 V I i g ij</formula><p>No matter which approach above is used to implement neural networks with memristor-based crossbars, repetitive tuning is involved. During tuning the conductance of a memristor, also called programming, a pulse of relatively high voltage is applied to the memristor. This high voltage may cause change of filament inside the memristor, leading to a degradation of the valid range of its conductance and thus the number of usable conductance levels, an effect called aging. If a trained weight is mapped by assuming the fresh state of the memristor after aging, the target conductance might fall outside of the valid range of the memristor and the programmed conductance can thus deviate significantly from the target conductance. Consequently, the memristor needs to be programmed more times in the following tuning process, leading to even more degradation of the conductance range.</p><p>The aging problem of memristors is different from the drifting effect as described in <ref type="bibr" target="#b7">[8]</ref>, where the conductance of the memristor drifts away from its programmed value after being read repetitively during execution of classification. This drifting can be recovered by reprogramming the conductance, while the aging effect described above is irreversible <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>To reduce the aging effect, several methods have been proposed. In <ref type="bibr" target="#b8">[9]</ref>, programming voltages in triangular and sinusoidal forms are used. Consequently, the applied voltage may cause less aging because the average of the applied voltage is lower than the constant DC voltage. In <ref type="bibr" target="#b10">[11]</ref>, a resistor is connected to a memristor in series to suppress irregular voltage drop on the memristor. Furthermore, the swapping method in <ref type="bibr" target="#b11">[12]</ref> uses rows of memristors that are slightly aged to replace the rows that are heavily aged to extend the lifetime of the whole crossbar.</p><p>The previous methods above, however, deal with the aging effect with a gross granularity. These techniques incur either extra cost or a higher complexity of the system. In this paper, we propose a new framework to counter aging in software training and hardware mapping without extra hardware cost. The contributions of this paper include:</p><p>? The mechanism of aging of memristors is analyzed and a corresponding skewed-weight training is proposed. By pushing the conductances of memristors towards small values, the current flowing through memristors can be reduced to alleviate the aging effect.</p><p>? When mapping the weights acquired from the training process to memristors, the current aging status of the memristors is taken into account. This status is traced using representative memristors and the conductances of memristors are adjusted further with respect to the aging status of these memristors. Consequently, the number of tuning iterations after mapping can be reduced, leading to less aging and thus a longer lifetime.</p><p>? The proposed software training and hardware mapping are integrated into an aging-aware framework and can improve the lifetime of memristor-based crossbars up to 11? compared with those without considering aging, while no extra hardware cost is incurred. The rest of this paper is organized as follows. In Section II, we explain the background of memristor-based neuromorphic computing. The motivation of the proposed method is presented in Section III. In Section IV, we describe the ideas to counter aging in software training and hardware mapping. Experimental results are shown in Section V and conclusions are drawn in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND OF MEMRISTOR-BASED</head><p>NEUROMORPHIC COMPUTING To implement neural networks using memristor-based crossbars, several steps are involved: software training, hardware mapping, online tuning. Afterwards, the crossbars can be used to perform tasks such as image classification and speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Neural Networks and Software Training</head><p>The basic structure of neural networks is shown Fig. <ref type="figure" target="#fig_1">2</ref>, which consists of an input layer, a hidden layer, and an output layer. The nodes represent neurons and connections represent the relations between neurons in different layers. In a neural network, the output of a neuron can be expressed as a function of previous neurons, e.g., z 1 = A(w  function such as ReLu, Tanh, and sigmoid functions, which are used in each layer to introduce non-linearity in the neural network.</p><p>Before a neural network can be used to process applications, its weights should be determined. For this purpose, training data are fed into the neural network. The output of the network is compared with the expected output in the training data and the difference between them is defined as the cost, which is used as the feedback to the neural network to adjust the weights. In practice, L2 regularization is also often appended to prevent overfitting. Consequently, the cost function can be expressed as follows</p><formula xml:id="formula_3">Cost = Noutput i=1 (-y (i) log(? (i) ) -(1 -y (i) )log(1 -?(i) )) + Nlayers i=1 ? ? W i 2 (1) = C(W) + R(W)</formula><p>(2) where ?(i) is the ith output of the neural network, y (i) is the ith expected output from the training data, W i represents the weights in the ith layer, and ? is the penalty used in L2 regularization. The first term of the cost function (1) denotes the cross entropy. The second term in (1) implements the L2 regularization.</p><p>According to the result of the cost function, a back propagation algorithm is then applied to update the weights in the neural network to reduce the cost for a higher classification accuracy. For example, the weights can be updated as</p><formula xml:id="formula_4">W i = W i -LR ? ?Cost ?W i (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where LR denotes the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hardware Mapping</head><p>After training, the neural network needs to be implemented. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the input of a neuron is determined by the linear combination of those neurons in the previous layer. When all the neurons in a layer are considered together, a vector-matrix multiplication needs to be performed for each layer. To accelerate this computation, the memristor-based crossbar shown in Fig. <ref type="figure" target="#fig_0">1</ref> can be used as described in Section I.</p><p>In a memristor-based crossbar, the weights of the matrix are implemented in the programmable conductances of the memristors. Assume that the maximum and the minimum weights are w max and w min , respectively. Also assume that the maximum and the minimum conductances of memristors are g max and g min , respectively. The mapping of the weight w ij from the ith neuron in a layer to the jth neuron in the next layer onto the conductance of the corresponding memristor can be expressed as</p><formula xml:id="formula_6">g ij = ( g max -g min w max -w min ? (w ij -w min ) + g min ).<label>(4)</label></formula><p>In ( <ref type="formula" target="#formula_6">4</ref>), a common conductance range g max -g min is used during the mapping process because the currents should be summed linearly in each column.</p><p>In practice, it is difficult to program the conductances of memristors to exact values and thus the resistances are usually programmed instead <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. After software training, the weights usually satisfy a quasi-normal distribution as shown in Fig. <ref type="figure" target="#fig_2">3</ref>(a). However, due to the inverse relationship between conductance and resistance, the distribution of resistances of memristors has a skewed shape, as shown in Fig. <ref type="figure" target="#fig_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b).</head><p>To simplify the programming circuitry, the ranges of the resistances are further discretized into a given number of levels, shown as the dashed lines in Fig. <ref type="figure" target="#fig_2">3</ref>(b). For example, 32 levels are used in <ref type="bibr" target="#b13">[14]</ref> and 64 levels in <ref type="bibr" target="#b14">[15]</ref>. During mapping, the target resistance of a memristor is then approximated by the closest level in the discrete range. This process is called quantization. The quantized levels in the resistance range also lead to quantized levels in the conductance range, which are, however, not evenly distributed anymore due to the inverse relation between resistance and conductance, shown as the dashed lines in Fig. <ref type="figure" target="#fig_2">3(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Online Tuning</head><p>Due to quantization, the accuracy of classification directly using a neural network after weight mapping is lower than that of software training. To improve this accuracy, the conductances of the memristors are tuned further according to the results of applying training data, a process similar to software training. However, in hardware level, it is difficult to calculate the derivatives accurately to update the conductances using (3). Therefore, a simplified tuning calculation is applied by considering only the signs of the derivatives. Based on these signs, the polarities of constant amplitudes used to program the conductances can be determined <ref type="bibr" target="#b15">[16]</ref>, as</p><formula xml:id="formula_7">V i ? sign(- ?Cost ?W i )<label>(5)</label></formula><p>where V i represents the voltage levels of the tuning pulses for the memristors in the ith layer. This online tuning process is executed repetitively in the neural network until the target classification accuracy is achieved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. AGING OF MEMRISTOR CROSSBARS AND MOTIVATION</head><p>OF PROPOSED COUNTER-AGING METHODS As described in section II, the memristor-based computing framework requires a large number of programming iterations during online tuning. To program a memristor, a high voltage needs to be applied. This voltage generates a current flowing through the memristor and causes the filament inside it to change irrecoverably due to repetitive electrical charging/discharging or changes in temperature. Consequently, the valid resistance range of the memristor degrades, an effect called aging in the memristor, as observed in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>Assume that the upper bound and the lower bound of the resistance of a memristor are written as R max and R min , respectively. Generally, the new bounds of the resistance range of a memristor after aging can be expressed as</p><formula xml:id="formula_8">R aged,max = R f resh,max -f (T, t) (6) R aged,min = R f resh,min -g(T, t)<label>(7)</label></formula><p>where T represents temperature and t the accumulated time in which the memristor is programmed. f (?) and g(?) are aging functions. Both aging functions are Arrhenius-based <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> and the parameters in the functions can be extracted from measurement data. Among various aging failure types in memristors with different materials and parameters, a common aging scenario is illustrated in Fig. <ref type="figure">4</ref>, where both upper and lower bounds of the resistance range decrease as the accumulated programming time t increases.</p><p>After a memristor is programmed many times, the target resistances may fall outside of its aged resistance range. For example, a programming attempt to set the resistance of a memristor to Level 7 at time t in Fig. <ref type="figure">4</ref> can only end up with the resistance set to Level 2. This mismatch of weight mapping and target resistance necessitates more programming iterations during online tuning to improve the classification accuracy, which unfortunately leads to an even larger aging effect due to more programming iterations. To solve these problems, counter-aging methods are proposed in the following section. memristors, while the latter maps the weights to quantized levels considering the aging status of the crossbars to reduce online tuning iterations further. The proposed work flow is illustrated in Fig. <ref type="figure" target="#fig_3">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED COUNTER-AGING FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Skewed Weights in Software Training</head><p>As described previously, memristors experience aging when being programmed due to the generated currents flowing through them. To reduce these currents, we propose to map the resistances of the memristors to large values, as shown in Fig. <ref type="figure" target="#fig_4">6</ref>. This can be achieved by concentrating the weights to small values during software training. After being mapped to the crossbar, these small weights lead to small conductances and thus large resistances programmed into the memristors to reduce the currents.</p><p>Another benefit of the skewed weight distribution is that the conductances can be approximated more accurately during quantization. Since the levels of resistances are spread evenly in the resistance range, the corresponding discrete levels of conductances are concentrated to the region of small values due to the inverse relation between resistance and conductance, as shown in Fig. <ref type="figure" target="#fig_2">3(c</ref>). With the original quasi-normal weight distribution, only a small portion of conductances are located in the lower end of the conductance range, although there are more levels available in this region to approximate them. On the contrary, if the proposed weight concentration is applied, more weights are pushed towards small values, as shown in Fig. <ref type="figure" target="#fig_4">6(a)</ref>, where the denser levels on the left side of the range keep more information of the weights after quantization. Consequently, the accuracy after weight mapping onto memristor crossbars tends to be higher than that achieved by original quasi-normal weight distribution, leading to fewer iterations during online tuning.</p><p>The technique of weight concentration to small values is feasible in software training, because neural networks have much flexibility to achieve the required classification accuracy with different sets of weights. To implement the desired skewed distribution, the cost function during training is modified to incorporate the concentration of weights into back propagation.</p><p>Comparing the expected weight distribution in Fig. <ref type="figure" target="#fig_4">6</ref>(a) and the original weight distribution in Fig. <ref type="figure" target="#fig_2">3</ref>(a), we can observe that we need to select a reference weight ? i in the original range of the weights. On the left side of this point, weights should be penalized strongly so that they tend not to fall into this range. On the right side of this point, weights should Regularizations in skewed weight training. W trained is the distribution of the original weights. R1(W) and R2(W) are the new terms for the cost function to generate skewed weights. With these regularizations, a skewed weight distribution as in Fig. <ref type="figure" target="#fig_4">6</ref>(a) can be generated. also be concentrated in a way that the farther they are from this reference weight, the stronger they are penalized. The concept of this cost function for skewed weight distribution is illustrated in Fig. <ref type="figure">7</ref>, where the solid curve is the quasinormal distribution of the originally trained weights, and the two dashed lines are the cost functions on the left side and on the right side of the selected reference weight.</p><p>To implement the two-segment concentration of the weights in software training, we expand the L2 regularization term R(W) in (2) into two terms R 1 (W) and R 2 (W), shown as follows</p><formula xml:id="formula_9">Cost = C(W) + R 1 (W) + R 2 (W) (8) R 1 (W) = Nlayers i=1 ? 1 ? W i -? i 2 , W i &lt; ? i (9) R 2 (W) = Nlayers i=1 ? 2 ? W i -? i 2 , W i ? ? i (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where ? i is the reference weight around which the skewed weights are concentrated. ? 1 sets the penalty of the first regularization for weights falling on the left side of ? i and ? 2 sets the penalty of the second regularization for the weights on the right of ? i . Since the weights on the left of the ? i should be penalized heavily, ? 1 is larger than ? 2 . W i are the weights in the ith layer in the neural network, and are updated in iterations during training to meet the specified classification accuracy.</p><p>The two terms R 1 (W) and R 2 (W) in the cost function impose more penalty when a weight is far from the reference weight ? i , although the accuracy of the classification is still improved gradually as in the normal training process. When training is finished, a quasi-normal weight distribution in Fig. <ref type="figure">7</ref> is transformed into a skewed weight distribution as in Fig. <ref type="figure" target="#fig_4">6(a)</ref>, which also indicates the new weight range [w min , w max ] by its largest and smallest values, respectively. The shape of the</p><formula xml:id="formula_11">R U aged,max iterative selection M2 M1 M3 R L aged,max</formula><p>Raged,min skewed weight distribution is highly affected by the constants ? i , ? 1 and ? 2 . The setting of these parameters is provided in section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Aging-aware Mapping</head><p>Aging of the memristors is a dynamic process and depends on the history of programming. Without considering aging, the weights after software training are usually mapped to the resistance ranges of memristors in the fresh state. With aging, the upper bounds and the lower bounds of their resistance ranges and the numbers of quantized levels in the ranges may change. For example, the number of resistance levels in Fig. <ref type="figure">4</ref> decreases from eight in the fresh state to three after being programmed for the accumulated time t. If the weight is supposed to be mapped to Level 7 after aging, the memristor can only be programmed to Level 2. After programming, if this aged memristor is used directly for classification, the accuracy may degrade significantly. If otherwise online tuning is applied, many programming iterations need to be applied because many weights need to be re-trained to achieve the expected accuracy. This re-training, unfortunately, requires more programming iterations and increases aging further.</p><p>To alleviate the mapping problem after aging, we take the aged resistance ranges of memristors into account during hardware mapping. In the proposed method, we trace the programming iterations of every one out nine memristors, namely, the memristor at the center of every 3?3 block, and estimate the aged upper and lower bounds of the resistance ranges using ( <ref type="formula">6</ref>) and <ref type="bibr" target="#b6">(7)</ref>. Afterwards, the mapping of the weights onto memristors can be performed using (4). Since the memristors in a crossbar experience different programming iterations, their aged resistance ranges also differ. However, the currents flowing through the memristors in a column in the crossbar should be summed linearly, so that the resistance ranges of these memristors should be the same. In the proposed method, this common range is determined by an iterative selection process.</p><p>Assume that the aged resistance ranges of three memristors M 1, M 2 and M 3 are shown as in Fig. <ref type="figure" target="#fig_5">8</ref>. After aging, the original lower bounds are usually still located in the aged range as shown in Fig. <ref type="figure">4</ref>. The upper bounds of resistance ranges, however, have degraded by one, two and three levels, respectively. To determine which common resistance range should be used for hardware mapping, we iterate through all the aged upper bounds of the traced memristors between R L aged,max and R U aged,max as shown in Fig. <ref type="figure" target="#fig_5">8</ref> and map the weights. Thereafter, the upper bound that produces the highest classification accuracy is selected as the new common resistance range. The selected range may still not cover the aged ranges of all the memristors. For example, if the range of M 2 is selected, a weight mapped to the upper bound of the common resistance range onto M 3 may still fall outside of the common range. This inaccuracy is compensated by online tuning following hardware mapping, whose iterations are still reduced owing to the estimated common range to achieve the required classification accuracy.</p><p>V. EXPERIMENTAL RESULTS To evaluate the effectiveness of the proposed counter-aging framework, two different neural networks, LeNet-5 <ref type="bibr" target="#b18">[19]</ref> and VGG-16 <ref type="bibr" target="#b19">[20]</ref> were applied onto two different image datasets, Cifar10 and Cifar100 <ref type="bibr" target="#b20">[21]</ref>, respectively. The information of the neural networks and the datasets are shown in the first two columns of Table <ref type="table">I</ref>. LeNet-5 has 5 layers, with 2 convolutional layers and 3 fully-connected layers. VGG-16 has 13 convolutional layers and 3 fully-connected layers. Cifar10 consists of 60000 images, which are grouped into 10 different classes. Cifar100 also has 60000 images, which are grouped into 100 classes instead. The neural networks and the proposed algorithm were implemented using Tensorflow <ref type="bibr" target="#b21">[22]</ref> and tested with an Intel 3.6 GHz CPU and an NVIDA GeForce GTX 1080Ti graphics card.</p><p>For skewed weight training, the constants of the reference weight ? i shown in Fig. <ref type="figure">7</ref> and the penalty values ? 1 and ? 2 in ( <ref type="formula">9</ref>) and <ref type="bibr" target="#b9">(10)</ref> are shown in Table <ref type="table">II</ref>. By setting various combinations during software training, the parameters are selected to maintain both the classification accuracy and the expected skewed weight distribution. In Fig. <ref type="figure">7</ref> the mean value of the quasi-normal distribution is close to zero so that the reference weights were set to the standard deviation ? i multiplied by a constant value. ? 1 was much larger than ? 2 to implement the skewed penalty in LeNet-5. However, VGG-16 has a large number of layers, and the accuracy tends to be more sensitive to the parameters, so that ? 1 was set to the same value as ? 2 to prevent accuracy degradation during software training. With this setting, a proper skewed weight distribution can still be achieved. Fig. <ref type="figure">9</ref> shows the skewed weight distribution of the third layer of VGG-16 as an example. The weight distributions of other layers have similar tendencies with the setting of ? 1 and ? 2 above .</p><p>In Table <ref type="table">I</ref> the two columns on accuracy comparison show the classification accuracy without and with skewed weight software training. The classification accuracy is slightly lower with LeNet-5, while VGG-16 has a higher accuracy even with the skewed weight distribution. This comparison demonstrates that neural networks actually have flexibility in weight selection to produce results of the same quality. The proposed framework essentially takes advantage of this feature to optimize the lifetime of the crossbars.</p><p>As shown in Fig. <ref type="figure">9</ref>, most weights of VGG-16 are con- centrated towards small values in software training. After these weights are determined, iterations of online tuning are applied to achieve the specified classification accuracy. If the number of online tuning iterations becomes too large, the tuning cannot converge and thus the crossbars fail. For demonstration, the trends of iterations of online tuning of and VGG-16 with respect to the number of applications are shown in Fig. <ref type="figure" target="#fig_0">10</ref>. As the number of applications reaches certain thresholds, the numbers of online tuning iterations increase suddenly, indicating that the memristor crossbars are failing.</p><p>From this comparison, it can be observed that the proposed ST+AT framework can significantly improve the number of applications that can be processed successfully.</p><p>The last three columns of Table <ref type="table">I</ref> show the lifetime comparison of the test cases. For this comparison, we simulated the online tuning process for a certain number of applications and demonstrate the lifetime results with three scenarios including, traditional weight training and online tuning (T+T), skewed weight training and online tuning (ST+T), skewed weight training with aging-aware mapping and online tuning (ST+AT) We set the target accuracy of the two neural networks to ensure a given number of applications, 4 ? 10 7 in the experiments, can be implemented with fewer than 150 tuning iterations. The proportional relation of the numbers of applications being processed successfully in the cases T+T, ST+T and ST+AT are shown in the last three columns of Table <ref type="table">II</ref>. With skewed weight distribution, these numbers and thus the lifetime of LeNet-5 and VGG-16 have been improved by 6? and 7?. Together with aging-aware mapping, the lifetime can be improved by 8? and 11?.</p><p>In reality, neural networks may contain layers of different types such as convolutional layers and fully-connected layers. To demonstrate the aging effects in these layers, the average upper bounds of resistances of all memristors in the convolutional layers and fully connected layers of LeNet-5 and VGG-16 are shown in Fig. <ref type="figure" target="#fig_7">11</ref>. According to this comparison, the aging effect in convolutional layers is stronger than fullyconnected layers, because convolutional layers are used to extract the features of input data and are thus programmed more often. Therefore, such layers have a higher priority to apply counter-aging measures. VI. CONCLUSIONS In this paper, we have proposed a framework combining software training and hardware tuning to counter the aging effect of memristor-based neural networks. By pushing the weights to the region of small values, the currents flowing through memristors can be reduced to alleviate aging. The proposed aging-aware hardware mapping sets the valid resistance ranges of memristors properly to reduce the number of online tuning iterations and thus the aging effect further. Experimental results confirm that the proposed framework can extend the lifetime of memristor-based crossbars by up to 11 times without extra hardware cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Memristor crossbar architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Basic structure of neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Hardware mapping and quantization. (a) Weights after software training. (b) Resistance distribution after mapping with quantization. (c) Conductance distribution after mapping with quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Proposed counter-aging work flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Skewed weight mapping and quantization. (a) Wights are pushed towards small values, resulting a skewed distribution in contrast to Fig. 3b. (b) Resistance distribution corresponding to the skewed weight distribution.R1(W)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Range selection in hardware mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 )Fig. 9 .Fig. 10 .</head><label>3910</label><figDesc>Fig. 9. Skewed weight distribution of the third layer of VGG-16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Aging of convolutional layers and fully-connected layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>11 ?x 1 +w 21 ?x 2 +w 31 ?x 3 ), where w 11 , w 21 and w 31 are the weights of the connections from x 1 , x 2 , x 3 to z 1 , respectively. A(?) is the activation</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-3-9819263-2-3/DATE19/ c 2019 EDAA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Design, Automation And Test in Europe (DATE 2019)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale FPGA-based convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Akselrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Talay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scaling up Machine Learning: Parallel and Distributed Approaches</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="399" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Resistive computation: avoiding the power wall with low-leakage, stt-mram based computing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Soyata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="371" to="382" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A spiking neuromorphic design with resistive crossbar</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Design Autom</title>
		<meeting>Design Autom</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Memristor-based analog computation and neural network classification with a dot product engine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Materials</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1705914</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training itself: Mixed-signal training acceleration for memristor-based neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia and South Pacific Des</title>
		<meeting>Asia and South Pacific Des</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="361" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memristor-based multilayer neural networks with online gradient descent training</title>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Di</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolodny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kvatinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2408" to="2421" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A closed-loop design to enhance weight stability of memristor based neural network chips</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput.-Aided Des</title>
		<meeting>Int. Conf. Comput.-Aided Des</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Physical mechanisms of endurance degradation in TMO-RRAM</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Electron Dev. Meeting</title>
		<meeting>Int. Electron Dev. Meeting</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="12" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Balancing SET/RESET Pulse for &gt; 10 10 Endurance in HfO 2 /Hf 1T1R Bipolar RRAM</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Govoreanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Goux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Degraeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fantini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Wouters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Groeseneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kittl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jurczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron Devices</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3243" to="3249" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Voltage divider effect for the improvement of variability and endurance of TaOx memristor</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Strachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Grafals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Melendez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">20085</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long live TIME: improving lifetime for training-in-memory engines by structured gradient sparsification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Design Autom</title>
		<meeting>Design Autom</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mapping weight matrix of a neural network&apos;s layer onto memristor crossbar</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tarkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Memory and Neural Networks</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="115" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dot-product engine for neuromorphic computing: programming 1T1M crossbar to accelerate matrix-vector multiplication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Strachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Grafals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Design Autom</title>
		<meeting>Design Autom</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TiOx-based RRAM synapse with 64-levels of conductance and symmetric conductance change by adopting a hybrid pulse scheme for neuromorphic computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Electron Device Lett</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1559" to="1562" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BSB training scheme implementation on memristor-based circuit</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence for Security and Defense Applications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantitative endurance failure model for filamentary RRAM</title>
		<author>
			<persName><forename type="first">R</forename><surname>Degraeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fantini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Goux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Costantino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Govoreanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Linten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symp. VLSI Technol</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="188" to="T189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voltage-controlled cycling endurance of HfOx-based resistive-switching memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ambrogio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Calderoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ielmini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron Devices</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3365" to="3372" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparison of learning algorithms for handwritten digit recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brunot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sackinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Int. Conf. Artif. Neural Netw.</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="53" to="60" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
