<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vision GNN: An Image is Worth Graph of Nodes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-01">1 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
							<email>kai.han@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="department">State Key Lab of Computer Science</orgName>
								<orgName type="institution">ISCAS &amp; UCAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
							<email>yunhe.wang@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="department">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enhua</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Lab of Computer Science</orgName>
								<orgName type="institution">ISCAS &amp; UCAS</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Macau</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Vision GNN: An Image is Worth Graph of Nodes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-01">1 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.00272v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network architecture plays a key role in the deep learning-based computer vision system. The widely-used convolutional neural network and transformer treat the image as a grid or sequence structure, which is not flexible to capture irregular and complex objects. In this paper, we propose to represent the image as a graph structure and introduce a new Vision GNN (ViG) architecture to extract graphlevel feature for visual tasks. We first split the image to a number of patches which are viewed as nodes, and construct a graph by connecting the nearest neighbors. Based on the graph representation of images, we build our ViG model to transform and exchange information among all the nodes. ViG consists of two basic modules: Grapher module with graph convolution for aggregating and updating graph information, and FFN module with two linear layers for node feature transformation. Both isotropic and pyramid architectures of ViG are built with different model sizes. Extensive experiments on image recognition and object detection tasks demonstrate the superiority of our ViG architecture. We hope this pioneering study of GNN on general visual tasks will provide useful inspiration and experience for future research. The PyTroch code will be available at https://github.com/huawei-noah/ CV-Backbones and the MindSpore code will be avaiable at https://gitee. com/mindspore/models. * Equal contribution.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the modern computer vision system, convolutional neural networks (CNNs) used to be the de-facto standard network architecture <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16]</ref>. Recently, transformer with attention mechanism was introduced for visual tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3]</ref> and attained competitive performance. MLP-based (multi-layer perceptron) vision models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> can also work well without using convolutions or self-attention. These progresses are pushing the vision models towards an unprecedented height.</p><p>Different networks treat the input image in different ways. As shown in Figure <ref type="figure">1</ref>, the image data is usually represented as a regular grid of pixels in the Euclidean space. CNNs <ref type="bibr" target="#b26">[27]</ref> apply sliding window on the image and introduce the shift-invariance and locality. The recent vision transformer <ref type="bibr" target="#b7">[8]</ref> or MLP <ref type="bibr" target="#b46">[47]</ref> treats the image as a sequence of patches. For example, ViT <ref type="bibr" target="#b7">[8]</ref> divides a 224 × 224 image into a number of 16 × 16 patches and forms a sequence with length of 196 as input.</p><p>Instead of the regular grid or sequence representation, we process the image in a more flexible way. One basic task of computer vision is to recognize the objects in an image. Since the objects are usually not quadrate whose shape is irregular, the commonly-used grid or sequence structures in previous networks like ResNet and ViT are redundant and inflexible to process them. An object can be viewed as a composition of parts, e.g., a human can be roughly divided into head, upper body, Figure <ref type="figure">1</ref>: Illustration of the grid, sequence and graph representation of the image. In the grid structure, the pixels or patches are ordered only by the spatial position. In the sequence structure, the 2D image is transformed to a sequence of patches. In the graph structure, the nodes are linked by its content and are not constrained by the local position.</p><p>arms and legs. These parts linked by joints naturally form a graph structure. By analyzing the graph, we are able to recognize the human. Moreover, graph is a generalized data structure that grid and sequence can be viewed as a special case of graph. Viewing an image as a graph is more flexible and effective for visual perception.</p><p>Based on the graph representation of images, we build the vision graph neural network (ViG for short) for visual tasks. Instead of treating each pixel as a node which will result in too many nodes (&gt;10K), we divide the input image to a number of patches and view each patch as a node.</p><p>After constructing the graph of image patches, we use our ViG model to transform and exchange information among all the nodes. The basic cells of ViG include two parts: GCN (graph convolutional network) module for graph information processing and FFN (feed-forward network) module for node feature transformation. With Grapher and FFN modules, we build our ViG models in both isotropic and pydamid manners. In the experiments, we demonstrate the effectiveness of ViG model on visual tasks like image classification and object detection. For instance, our Pyramid ViG-S achieves 82.1% top-1 accuracy on ImageNet calssfication task, which outperforms the representative CNN (ResNet <ref type="bibr" target="#b15">[16]</ref>), MLP (CycleMLP <ref type="bibr" target="#b3">[4]</ref>) and transformer (Swin-T <ref type="bibr" target="#b32">[33]</ref>) with similar FLOPs (about 4.5G). To the best of our knowledge, our work is the first to successfully apply graph neural network on large-scale visual tasks. We hope our work will inspire the community to further explore more powerful network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we first revisit the backbone networks in computer vision. Then we review the development of graph neural network, especially GCN and its applications on visual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CNN, Transformer and MLP for Vision</head><p>The mainstream network architecture in computer vision used to be convolutional network <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16]</ref>. Starting from LeNet <ref type="bibr" target="#b26">[27]</ref>, CNNs have been successfully used in various visual tasks, e.g., image classification <ref type="bibr" target="#b24">[25]</ref>, object detection <ref type="bibr" target="#b39">[40]</ref> and semantic segmentation <ref type="bibr" target="#b33">[34]</ref>. The CNN architecture is evolving rapidly in the last ten years. The representative works include ResNet <ref type="bibr" target="#b15">[16]</ref>, MobileNet <ref type="bibr" target="#b19">[20]</ref> and NAS <ref type="bibr" target="#b67">[68]</ref>. Vision transformer was introduced for visual tasks from 2020 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3]</ref>. From then on, a number of variants of ViT <ref type="bibr" target="#b7">[8]</ref> were proposed to improve the performance on visual tasks. The main improvements include pyramid architecture <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b32">33]</ref>, local attention <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref> and position encoding <ref type="bibr" target="#b57">[58]</ref>. Inspired by vision transformer, MLP is also explored in computer vision <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. With specially designed modules <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref>, MLP can achieve competitive performance and work on general visual tasks like object detection and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Network</head><p>The earliest graph neural network was initially outlined in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42]</ref>. Micheli <ref type="bibr" target="#b35">[36]</ref> proposed the early form of spatial-based graph convolutional network by architecturally composite nonrecursive layers.</p><p>In recent several years, the variants of spatial-based GCNs have been introduced, such as <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref>. Spectral-based GCN was first presented by Bruna et al. <ref type="bibr" target="#b1">[2]</ref> that introduced graph convolution based on the spectral graph theory. Since this time, a number of works to improve and extend spectral-based GCN have been proposed <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref>. The GCNs are usually applied on graph data, such as social networks <ref type="bibr" target="#b11">[12]</ref>, citation networks <ref type="bibr" target="#b42">[43]</ref> and biochemical graphs <ref type="bibr" target="#b52">[53]</ref>.</p><p>The applications of GCN in the field of computer vision mainly include point clouds classification, scene graph generation, and action recognition. A point cloud is a set of 3D points in space which is usually collected by LiDAR scans. GCN has been explored for classifying and segmenting points clouds <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b54">55]</ref>. Scene graph generation aims to parse the input image intro a graph with the objects and their relationship, which is usually solved by combining object detector and GCN <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b62">63]</ref>. By processing the naturally formed graph of linked human joints, GCN was utilized on human action recognition task <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b61">62]</ref>. GCN can only tackle specific visual tasks with naturally constructed graph.</p><p>For general applications in computer vision, we need a GCN-based backbone network that directly processes the image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we describe how to transform an image to a graph and introduce vision GCN architectures to learn visual representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head for recognition</head><note type="other">Image</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ViG Block</head><p>Graph Representation of Image. For an image with size of H × W × 3, we divided it into N patches. By transforming each patch into a feature vector</p><formula xml:id="formula_0">x i ∈ R D , we have X = [x 1 , x 2 , • • • , x N ]</formula><p>where D is the feature dimension and i = 1, 2, • • • , N . These features can be viewed as a set of unordered nodes which are denoted as</p><formula xml:id="formula_1">V = {v 1 , v 2 , • • • , v N }.</formula><p>For each node v i , we find its K nearest neighbors N (v i ) and add an edge e ji directed from v j to v i for all v j ∈ N (v i ). Then we obtain a graph G = (V, E) where E denote all the edges. We denote the graph construction process as G = G(X) in the following. By viewing the image as a graph data, we can utilize GCN to extract its representation.</p><p>Graph-level processing. To be general, we start from the features X ∈ R N ×D . We first construct a graph based on the features: G = G(X). A graph convolutional layer can exchange information between nodes by aggregating features from its neighbor nodes. Specifically, graph convolution operates as follows:</p><formula xml:id="formula_2">G = F (G, W) = U pdate(Aggregate(G, W agg ), W update ),<label>(1)</label></formula><p>where W agg and W update are the learnable weights of the aggregation and update operations, respectively. More concretely, the aggregation operation compute the representation of a node by aggregating features of neighbor nodes:</p><formula xml:id="formula_3">x i = h(x i , g(x i , N (x i ), W agg ), W update ),<label>(2)</label></formula><p>where N (x l i ) is the set of neighbor nodes of x l i . Here we adopt max-relative graph convolution <ref type="bibr" target="#b27">[28]</ref> for its simplicity and efficiency:</p><formula xml:id="formula_4">g(•) = x i = max({x i − x j |j ∈ N (x i )}, (3) h(•) = x i = x i W update ,<label>(4)</label></formula><p>where the bias term is omitted. The above graph-level processing can be denoted as X = GraphConv(X).</p><p>We further introduce multi-head update operation of graph convolution. The aggregated feature x i is first split into h heads, i.e., head 1 , head 2 , • • • , head h and then these heads are updated with different weights respectively. All the heads can be updated in parallel and are concatenated as the final values:</p><formula xml:id="formula_5">x i = [head 1 W 1 update , head 2 W 2 update , • • • , head h W h update ].<label>(5)</label></formula><p>Multi-head update operation allows the model to update information in multiple representation subspaces, which is beneficial to the feature diversity. ViG block. The previous GCNs usually repeatedly use several graph convolution layers to extract aggregated feature of the graph data. The over-smoothing phenomenon in deep GCNs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref> will decrease the distinctiveness of node features and lead to performance degradation for visual recognition, as shown in Figure <ref type="figure" target="#fig_2">3</ref> where diversity is measured as X − 1x T with x = arg min x X − 1x T <ref type="bibr" target="#b6">[7]</ref>. To alleviate this issue, we introduce more feature transformations and nonlinear activations in our ViG block.</p><p>We apply a linear layer before and after the graph convolution to project the node features into the same domain and increase the feature diversity. A nonlinear activation function is inserted after graph convolution to avoid layer collapse. We call the upgraded module as Grapher module.</p><p>In practice, given the input feature X ∈ R N ×D , the Grapher module can be expressed as</p><formula xml:id="formula_6">Y = σ(GraphConv(XW in ))W out + X,<label>(6)</label></formula><p>where Y ∈ R N ×D , W in and W out are the weights of fully-connected layers, σ is the activation function, e.g., ReLU and GeLU <ref type="bibr" target="#b17">[18]</ref>, and the bias term is omitted.</p><p>To further encourage the feature transformation capacity and relief the over-smoothing phenomenon, we utilize feed-forward network (FFN) on each node. The FFN module is a simple multi-layer perceptron with two fully-connected layers:</p><formula xml:id="formula_7">Z = σ(Y W 1 )W 2 + Y,<label>(7)</label></formula><p>where Z ∈ R N ×D , W 1 and W 2 are the weights of fully-connected layers, and the bias term is omitted. The hidden dimension of FFN is usually larger than D. In both Grapher and FFN modules, batch normalization is applied after every fully-connected layer or graph convolution layer, which is omitted in Eq. 6 and 7 for concision. A stack of Grapher module and FFN module constitutes the ViG block which serves as the basic building unit for constructing a network. Based on the graph representation of images and the proposed ViG block, we can build the ViG network for visual tasks as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Compared to vanilla ResGCN <ref type="bibr" target="#b27">[28]</ref>, our ViG can maintain the feature diversity (Figure <ref type="figure" target="#fig_2">3</ref>) as the layer goes deeper so as to learn discriminative representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>In the field of computer vision, the commonly-used transformer usually has an isotropic architecture (e.g., ViT <ref type="bibr" target="#b7">[8]</ref>), while CNNs prefer to use pyramid architecture (i.e., ResNet <ref type="bibr" target="#b15">[16]</ref>). To have a extensive comparison with other types of neural networks, we build two kinds of network architectures for ViG, i.e., isotropic architecture and pyramid architecture.</p><p>Isotropic architecture. Isotropic architecture means the main body has features with equal size and shape throughout the network, such as ViT <ref type="bibr" target="#b7">[8]</ref> and ResMLP <ref type="bibr" target="#b47">[48]</ref>. We build three versions of isotropic ViG architecture with different models sizes, i.e., ViG-Ti, S and B. The number of nodes is set as N = 196. To enlarge the receptive field gradually, the number of neighbor nodes K increases from 9 to 18 linearly as the layer goes deep in these three models. The number of heads is set as h = 4 by default. The details are listed in Table <ref type="table" target="#tab_0">1</ref>. Pyramid architecture. Pyramid architecture considers the multi-scale property of images by extracting features with gradually smaller spatial size as the layer goes deeper, such as ResNet <ref type="bibr" target="#b15">[16]</ref> and PVT <ref type="bibr" target="#b53">[54]</ref>. Empirical evidences show that pyramid architecture is effective for visual tasks <ref type="bibr" target="#b53">[54]</ref>. Thus, we utilize the advanced design and build four versions of pyramid ViG models. The details are shown in Table <ref type="table" target="#tab_1">2</ref>.  Positional encoding. In order to represent the position information of the nodes, we add a positional encoding vector to each node feature:</p><formula xml:id="formula_8">H 4 × W 4 Conv×3 Conv×3 Conv×3 Conv×3 Stage 1 H 4 × W 4   D = 48 E = 4 K = 9   ×2   D = 80 E = 4 K = 9   ×2   D = 96 E = 4 K = 9   ×2   D = 128 E = 4 K = 9   ×2 Downsample H 8 × W 8 Conv Conv Conv Conv Stage 2 H 8 × W 8   D = 96 E = 4 K = 9   ×2   D = 160 E = 4 K = 9   ×2   D = 192 E = 4 K = 9   ×2   D = 256 E = 4 K = 9   ×2 Downsample H 16 × W 16 Conv Conv Conv Conv Stage 3 H 16 × W 16   D = 240 E = 4 K = 9   ×6   D = 400 E = 4 K = 9   ×6   D = 384 E = 4 K = 9   ×16   D = 512 E = 4 K = 9   ×18 Downsample H 32 × W 32 Conv Conv Conv Conv Stage 4 H 32 × W 32   D = 384 E = 4 K = 9   ×2   D = 640 E = 4 K = 9   ×2   D = 768 E = 4 K = 9   ×2   D = 1024 E = 4 K = 9   ×2<label>Head</label></formula><formula xml:id="formula_9">x i ← x i + e i ,<label>(8</label></formula><p>) where e i ∈ R D . The absolute positional encoding as described in Eq. 8 is applied in both iostropic and pyramid architectures. For pyramid ViG, we further include relative positional encoding by following the advanced designs like Swin Transformer <ref type="bibr" target="#b32">[33]</ref>. For node i and j, the relative positional distance between them is e T i e j , which will be added into the feature distance for constructing the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct experiments to demonstrate the effectiveness of ViG models on visual tasks including image recognition and object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Settings</head><p>Datasets. In image classification task, the widely-used benchmark ImageNet ILSVRC 2012 <ref type="bibr" target="#b40">[41]</ref> is used in the following experiments. ImageNet has 120M training images and 50K validation images, which belong to 1000 categories. For the license of ImageNet dataset, please refer to http://www.image-net.org/download. For object detection, we use COCO 2017 <ref type="bibr" target="#b31">[32]</ref> dataset with 80 object categories. COCO 2017 contains 118K training images and 5K validation images. For the licenses of these datasets, please refer to https://cocodataset.org/#home.  <ref type="bibr" target="#b44">[45]</ref> 0.1 Stochastic path <ref type="bibr" target="#b20">[21]</ref> 0.1 0.1 0.1 0.3 Repeated augment <ref type="bibr" target="#b18">[19]</ref> RandAugment <ref type="bibr" target="#b4">[5]</ref> Mixup prob. <ref type="bibr" target="#b65">[66]</ref> 0.8 Cutmix prob. <ref type="bibr" target="#b64">[65]</ref> 1.0 Random erasing prob. <ref type="bibr" target="#b66">[67]</ref> 0.25 Exponential moving average 0.99996</p><p>Experimental Settings. For all the ViG models, we utilize dilated aggregation <ref type="bibr" target="#b27">[28]</ref> in Grapher module and set the dilated rate as l/4 for the l-th layer. GELU <ref type="bibr" target="#b17">[18]</ref> is used as the nonlinear activation function in Eq. 6 and 7. For ImageNet classification, we use the commonlyused training strategy proposed in DeiT <ref type="bibr" target="#b48">[49]</ref> for fair comparison. The data augmentation includes RandAugment <ref type="bibr" target="#b4">[5]</ref>, Mixup <ref type="bibr" target="#b65">[66]</ref>, Cutmix <ref type="bibr" target="#b64">[65]</ref>, random erasing <ref type="bibr" target="#b66">[67]</ref> and repeated augment <ref type="bibr" target="#b18">[19]</ref>. The details are shown in Table <ref type="table" target="#tab_3">3</ref>. For COCO detection task, we take RetinaNet <ref type="bibr" target="#b30">[31]</ref> and Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> as the detection frameworks and use our Pyramid ViG as backbone. All the models are trained on COCO 2017 training set in "1×" schedule and evaluated on validation set. We implement the networks using PyTroch <ref type="bibr" target="#b38">[39]</ref> and MindSpore <ref type="bibr" target="#b21">[22]</ref> and train all our models on 8 NVIDIA V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results on ImageNet</head><p>Isotropic ViG The neural network with iostropic architecture keeps the feature size unchanged in its main computational body, which is easy to scale and is friendly for hardware acceleration. This scheme is widely used in transformer models for natural language processing <ref type="bibr" target="#b50">[51]</ref>. The recent neural networks in vision also explore it such as ConvMixer <ref type="bibr" target="#b46">[47]</ref>, ViT <ref type="bibr" target="#b7">[8]</ref> and ResMLP <ref type="bibr" target="#b47">[48]</ref>. We compare our isotropic ViG with the existing iostropic CNNs <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47]</ref>, transformers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49]</ref> and MLPs <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47]</ref> in Table <ref type="table" target="#tab_4">4</ref>. From the results, ViG performs better than other types of networks. For example, our ViG-Ti achieves 73.9% top-1 accuracy which is 1.7% higher than DeiT-Ti model with similar computational cost. Pyramid ViG The pyramid architecture gradually shrinks the spatial size of feature maps as the network deepens, which can leverage the scale-invariant property of images and produce multi-scale features. The advanced networks usually adopt the pyramid architecture, such as ResNet <ref type="bibr" target="#b15">[16]</ref>, Swin Transformer <ref type="bibr" target="#b32">[33]</ref> and CycleMLP <ref type="bibr" target="#b3">[4]</ref>. We compare our Pyramid ViG with those representative pyramid networks in Table <ref type="table" target="#tab_5">5</ref>. Our Pyramid ViG series can outperform or be comparable to the state-of-the-art pyramid networks including CNN, MLP and transformer. This indicates that graph neural network can work well on visual tasks and has the potential to be a basic component in computer vision system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conduct ablation study of the proposed method on ImageNet classification task and use the isotropic ViG-Ti as the base architecture. Type of graph convolution. We test the representative variants of graph convolution, including EdgeConv <ref type="bibr" target="#b54">[55]</ref>, GIN <ref type="bibr" target="#b60">[61]</ref>, GraphSAGE <ref type="bibr" target="#b11">[12]</ref> and Max-Relative GraphConv <ref type="bibr" target="#b27">[28]</ref>. From table 6, we can see that the top-1 accuracies of different graph convolutions are better than that of DeiT-Ti, indicating the flexibility of ViG architecture. Among them, Max-Relative achieves the best trade-off between FLOPs and accuracy. In rest of the experiments, we use Max-Relative GraphConv by default unless specially stated. The effects of modules in ViG. To make graph neural network adaptive to visual task, we introduce FC layers in Grapher module and utilize FFN block for feature transformation. We evaluate the effects of these modules by ablation study. We change the feature dimension of the compared models to make their FLOPs similar, so as to have a fair comparison. From Table <ref type="table" target="#tab_7">7</ref>, we can see that directly utilizing graph convolution for image classification performs poorly. Adding more feature transformation by introducing FC and FFN consistently increase the accuracy.</p><p>The number of neighbors. In the process of constructing graph, the number of neighbor nodes K is a hyperparameter controlling the aggregated range. Too few neighbors will degrade information exchange, while too many neighbors will lead to over-smoothing. We tune K from 3 to 20 and show the results in Table <ref type="table" target="#tab_8">8</ref>. We can see that the number of neighbor nodes in the range from 9 to 15 can perform well on ImageNet classification task. The number of heads. Multi-head update operation allows Grapher module to process node features in different subspaces. The number of heads h in Eq. 5 controls the transformation diversity in subspaces and the FLOPs. We tune h from 1 to 8 and show the results in Table <ref type="table" target="#tab_9">9</ref>. The FLOPs and top-1 accuracy on ImageNet changes slightly for different h. We select h = 4 as default value for the optimal trade-off between FLOPs and accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Object Detection</head><p>We apply our ViG model on object detection task to evaluate its generalization. To have a fair comparison, we utilize the ImageNet pretrained Pyramid ViG-S as the backbone of RetinaNet <ref type="bibr" target="#b30">[31]</ref> and Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> detection frameworks. The models are trained in the commonly-used "1x" schedule and FLOPs is calculated with 1280×800 input size. From the results in Table <ref type="table" target="#tab_10">10</ref>, we can see that our Pyramid ViG-S performs better than the representative backbones of different types, including ResNet <ref type="bibr" target="#b15">[16]</ref>, CycleMLP <ref type="bibr" target="#b3">[4]</ref> and Swin Transformer <ref type="bibr" target="#b32">[33]</ref> on both RetinaNet and Mask R-CNN. The superior results demonstrate the generalization ability of ViG architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization</head><p>To better understand how our ViG model works, we visualize the constructed graph structure in ViG-S. In Figure <ref type="figure" target="#fig_4">4</ref>, we show the graphs of two samples in different depths (the 1st and the 12th blocks). The pentagram is the center node, and the nodes with the same color are its neighbors. Two center nodes are visualized as drawing all the edges will be messy. We can observe that our model can select the content-related nodes as the first order neighbors. In the shallow layer, the neighbor nodes tend to be selected based on low-level and local features, such as color and texture. In the deep layer, the neighbors of the center nodes are more semantic and belong to the same category. Our ViG   network can gradually link the nodes by its content and semantic representation and help to better recognize the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we pioneer to study representing the image as graph data and leverage graph neural network for visual tasks. We divide the image into a number of patches and view them as nodes.</p><p>Constructing graph based on these nodes can better represent the irregular and complex objects in the wild. Directly using graph convolution on the image graph structure has over-smoothing problem and performs poorly. We introduce more feature transformation inside each node to encourage the information diversity. Based on the graph representation of images and improved graph block, we build our vision GNN (ViG) networks with both isotropic and pyramid architectures. Extensive experiments on image recognition and object detection demonstrate the superiority of the proposed ViG architecture. We hope this pioneering work on vision GNN can serve as a basic architecture for general visual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Theoretical Analysis</p><p>In our ViG block, we propose to increase feature diversity in nodes by utilizing more feature transformation such as FFN module. We show the empirical comparison between vanilla ResGCN and our ViG model in our paper. Here we make a simple theoretical analysis of the benefit of FFN module in on increasing the feature diversity. Given the output features of graph convolution X ∈ R N ×D , the feature diversity <ref type="bibr" target="#b6">[7]</ref> is measured as</p><formula xml:id="formula_10">γ(X) = X − 1x T , where x = arg min x X − 1x T ,<label>(9)</label></formula><p>where • is the 1,∞ norm of a matrix. By applying FFN module on the features, we have the following theorem. Theorem 1. Given a FFN module, the diversity γ(FFN(X)) of its output features satisfies</p><formula xml:id="formula_11">γ(FFN(X)) ≤ λγ(X), (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where λ is the Lipschitz constant of FFN with respect to p-norm for p ∈ [1, ∞].</p><p>Proof. The FFN includes weight matrix multiplication, bias addition and elementwise nonlinear function, which all preserve the constancy-across-rows property of FFN(1x T ). Therefore, we have</p><formula xml:id="formula_13">γ(FFN(X)) = FFN(X) − 1x T p ≤ FFN(X) − FFN(1x T ) p FFN preserves constancy-across-rows. ≤ λ X − 1x T p</formula><p>Definition of Lipschitz constant. = λγ(X),</p><p>The Lipschitz constant of FFN is related to the norm of weight matrices and is usually much larger than 1 <ref type="bibr" target="#b51">[52]</ref>. Thus, the Theorem 1 shows that introducing γ(FFN(X)) in our ViG block tends to improve the feature diversity in graph neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Pseudocode</head><p>The proposed Vision GNN framework is easy to be implemented based on the commonly-used layers without introducing complex operations. The pseudocode of the core part, i.e., ViG block is shown in Algorithm 1. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Grid structure. (b) Sequence structure. (b) Graph structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework of the proposed ViG model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Feature diversity of nodes as layer changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Input image. (b) Graph connection in the 1st block. (b) Graph connection in the 12th block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of the constructed graph structure. The pentagram is the center node, and the nodes with the same color are its neighbors in the graph.</figDesc><graphic url="image-11.png" coords="9,209.41,415.15,130.68,120.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Variants of our isotropic ViG architecture. The FLOPs are calculated for the image with 224×224 resolution. 'Ti' denotes tiny, 'S' denotes small, and 'B' denotes base.</figDesc><table><row><cell>Model</cell><cell>Depth</cell><cell>Dimension D</cell><cell>Params (M)</cell><cell>FLOPs (B)</cell></row><row><cell>ViG-Ti</cell><cell>12</cell><cell>192</cell><cell>7.1</cell><cell>1.3</cell></row><row><cell>ViG-S</cell><cell>16</cell><cell>320</cell><cell>22.7</cell><cell>4.5</cell></row><row><cell>ViG-B</cell><cell>16</cell><cell>640</cell><cell>86.8</cell><cell>17.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detailed settings of Pyramid ViG series.</figDesc><table><row><cell>Stage</cell><cell>Output size PyramidViG-Ti</cell><cell>PyramidViG-S</cell><cell>PyramidViG-M</cell><cell>PyramidViG-B</cell></row><row><cell>Stem</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>D: feature dimension, E: hidden dimension ratio in FFN, K: number of neighbors in GCN, H × W : input image size. 'Ti' denotes tiny, 'S' denotes small, 'M' denotes medium, and 'B' denotes base.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Training hyper-parameters for ImageNet.</figDesc><table><row><cell>(Pyramid) ViG</cell><cell>Ti</cell><cell>S</cell><cell>M</cell><cell>B</cell></row><row><cell>Epochs</cell><cell></cell><cell>300</cell><cell></cell><cell></cell></row><row><cell>Optimizer</cell><cell></cell><cell cols="2">AdamW [35]</cell><cell></cell></row><row><cell>Batch size</cell><cell></cell><cell>1024</cell><cell></cell><cell></cell></row><row><cell>Start learning rate (LR)</cell><cell></cell><cell>1e-3</cell><cell></cell><cell></cell></row><row><cell>Learning rate schedule</cell><cell></cell><cell cols="2">Cosine</cell><cell></cell></row><row><cell>Warmup epochs</cell><cell></cell><cell>20</cell><cell></cell><cell></cell></row><row><cell>Weight decay</cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell></row><row><cell>Label smoothing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of ViG and other isotropic networks on ImageNet. ♠ CNN, MLP, Transformer, GNN.</figDesc><table><row><cell>Model</cell><cell>Resolution</cell><cell>Params (M)</cell><cell>FLOPs (B)</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>♠ ResMLP-S12 conv3x3 [48]</cell><cell>224×224</cell><cell>16.7</cell><cell>3.2</cell><cell>77.0</cell><cell>-</cell></row><row><cell>♠ ConvMixer-768/32 [50]</cell><cell>224×224</cell><cell>21.1</cell><cell>20.9</cell><cell>80.2</cell><cell>-</cell></row><row><cell>♠ ConvMixer-1536/20 [50]</cell><cell>224×224</cell><cell>51.6</cell><cell>51.4</cell><cell>81.4</cell><cell>-</cell></row><row><cell>ViT-B/16 [8]</cell><cell>384×384</cell><cell>86.4</cell><cell>55.5</cell><cell>77.9</cell><cell>-</cell></row><row><cell>DeiT-Ti [49]</cell><cell>224×224</cell><cell>5.7</cell><cell>1.3</cell><cell>72.2</cell><cell>91.1</cell></row><row><cell>DeiT-S [49]</cell><cell>224×224</cell><cell>22.1</cell><cell>4.6</cell><cell>79.8</cell><cell>95.0</cell></row><row><cell>DeiT-B [49]</cell><cell>224×224</cell><cell>86.4</cell><cell>17.6</cell><cell>81.8</cell><cell>95.7</cell></row><row><cell>ResMLP-S24 [48]</cell><cell>224×224</cell><cell>30</cell><cell>6.0</cell><cell>79.4</cell><cell>94.5</cell></row><row><cell>ResMLP-B24 [48]</cell><cell>224×224</cell><cell>116</cell><cell>23.0</cell><cell>81.0</cell><cell>95.0</cell></row><row><cell>Mixer-B/16 [47]</cell><cell>224×224</cell><cell>59</cell><cell>11.7</cell><cell>76.4</cell><cell>-</cell></row><row><cell>ViG-Ti (ours)</cell><cell>224×224</cell><cell>7.1</cell><cell>1.3</cell><cell>73.9</cell><cell>92.0</cell></row><row><cell>ViG-S (ours)</cell><cell>224×224</cell><cell>22.7</cell><cell>4.5</cell><cell>80.4</cell><cell>95.2</cell></row><row><cell>ViG-B (ours)</cell><cell>224×224</cell><cell>86.8</cell><cell>17.7</cell><cell>82.3</cell><cell>95.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of Pyramid ViG and other pyramid networks on ImageNet. ♠ CNN, MLP, Transformer, GNN.</figDesc><table><row><cell>Model</cell><cell>Resolution</cell><cell>Params (M)</cell><cell>FLOPs (B)</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>♠ ResNet-18 [16, 56]</cell><cell>224×224</cell><cell>12</cell><cell>1.8</cell><cell>70.6</cell><cell>89.7</cell></row><row><cell>♠ ResNet-50 [16, 56]</cell><cell>224×224</cell><cell>25.6</cell><cell>4.1</cell><cell>79.8</cell><cell>95.0</cell></row><row><cell>♠ ResNet-152 [16, 56]</cell><cell>224×224</cell><cell>60.2</cell><cell>11.5</cell><cell>81.8</cell><cell>95.9</cell></row><row><cell>♠ BoTNet-T3 [44]</cell><cell>224×224</cell><cell>33.5</cell><cell>7.3</cell><cell>81.7</cell><cell>-</cell></row><row><cell>♠ BoTNet-T3 [44]</cell><cell>224×224</cell><cell>54.7</cell><cell>10.9</cell><cell>82.8</cell><cell>-</cell></row><row><cell>♠ BoTNet-T3 [44]</cell><cell>256×256</cell><cell>75.1</cell><cell>19.3</cell><cell>83.5</cell><cell>-</cell></row><row><cell>PVT-Tiny [54]</cell><cell>224×224</cell><cell>13.2</cell><cell>1.9</cell><cell>75.1</cell><cell>-</cell></row><row><cell>PVT-Small [54]</cell><cell>224×224</cell><cell>24.5</cell><cell>3.8</cell><cell>79.8</cell><cell>-</cell></row><row><cell>PVT-Medium [54]</cell><cell>224×224</cell><cell>44.2</cell><cell>6.7</cell><cell>81.2</cell><cell>-</cell></row><row><cell>PVT-Large [54]</cell><cell>224×224</cell><cell>61.4</cell><cell>9.8</cell><cell>81.7</cell><cell>-</cell></row><row><cell>CvT-13 [57]</cell><cell>224×224</cell><cell>20</cell><cell>4.5</cell><cell>81.6</cell><cell>-</cell></row><row><cell>CvT-21 [57]</cell><cell>224×224</cell><cell>32</cell><cell>7.1</cell><cell>82.5</cell><cell>-</cell></row><row><cell>CvT-21 [57]</cell><cell>384×384</cell><cell>32</cell><cell>24.9</cell><cell>83.3</cell><cell>-</cell></row><row><cell>Swin-T [33]</cell><cell>224×224</cell><cell>29</cell><cell>4.5</cell><cell>81.3</cell><cell>95.5</cell></row><row><cell>Swin-S [33]</cell><cell>224×224</cell><cell>50</cell><cell>8.7</cell><cell>83.0</cell><cell>96.2</cell></row><row><cell>Swin-B [33]</cell><cell>224×224</cell><cell>88</cell><cell>15.4</cell><cell>83.5</cell><cell>96.5</cell></row><row><cell>CycleMLP-B2 [4]</cell><cell>224×224</cell><cell>27</cell><cell>3.9</cell><cell>81.6</cell><cell>-</cell></row><row><cell>CycleMLP-B3 [4]</cell><cell>224×224</cell><cell>38</cell><cell>6.9</cell><cell>82.4</cell><cell>-</cell></row><row><cell>CycleMLP-B4 [4]</cell><cell>224×224</cell><cell>52</cell><cell>10.1</cell><cell>83.0</cell><cell>-</cell></row><row><cell>Poolformer-S12 [64]</cell><cell>224×224</cell><cell>12</cell><cell>2.0</cell><cell>77.2</cell><cell>93.5</cell></row><row><cell>Poolformer-S36 [64]</cell><cell>224×224</cell><cell>31</cell><cell>5.2</cell><cell>81.4</cell><cell>95.5</cell></row><row><cell>Poolformer-M48 [64]</cell><cell>224×224</cell><cell>73</cell><cell>11.9</cell><cell>82.5</cell><cell>96.0</cell></row><row><cell>Pyramid ViG-Ti (ours)</cell><cell>224×224</cell><cell>10.7</cell><cell>1.7</cell><cell>78.2</cell><cell>94.2</cell></row><row><cell>Pyramid ViG-S (ours)</cell><cell>224×224</cell><cell>27.3</cell><cell>4.6</cell><cell>82.1</cell><cell>96.0</cell></row><row><cell>Pyramid ViG-M (ours)</cell><cell>224×224</cell><cell>51.7</cell><cell>8.9</cell><cell>83.1</cell><cell>96.4</cell></row><row><cell>Pyramid ViG-B (ours)</cell><cell>224×224</cell><cell>92.6</cell><cell>16.8</cell><cell>83.7</cell><cell>96.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>ImageNet results of different types of graph convolution. The basic architecture is ViG-Ti.</figDesc><table><row><cell>GraphConv</cell><cell>Params (M)</cell><cell>FLOPs (B)</cell><cell>Top-1</cell></row><row><cell>EdgeConv [55]</cell><cell>7.2</cell><cell>2.4</cell><cell>74.3</cell></row><row><cell>GIN [61]</cell><cell>7.0</cell><cell>1.3</cell><cell>72.8</cell></row><row><cell>GraphSAGE [12]</cell><cell>7.3</cell><cell>1.6</cell><cell>74.0</cell></row><row><cell>Max-Relative GraphConv [28]</cell><cell>7.1</cell><cell>1.3</cell><cell>73.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The effects of modules in ViG on ImageNet.</figDesc><table><row><cell>GraphConv</cell><cell>FC in Grapher module</cell><cell>FFN module</cell><cell>Params (M)</cell><cell>FLOPs (B)</cell><cell>Top-1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5.8</cell><cell>1.4</cell><cell>67.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4.4</cell><cell>1.4</cell><cell>73.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>7.7</cell><cell>1.3</cell><cell>73.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>7.1</cell><cell>1.3</cell><cell>73.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Top-1 accuracy vs. K on ImageNet.</figDesc><table><row><cell>K</cell><cell>3</cell><cell>6</cell><cell>9</cell><cell>12</cell><cell>15</cell><cell>20</cell><cell>9 to 18</cell></row><row><cell>Top-1</cell><cell>72.2</cell><cell>73.4</cell><cell>73.6</cell><cell>73.6</cell><cell>73.5</cell><cell>73.3</cell><cell>73.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Top-1 accuracy vs. h on ImageNet.</figDesc><table><row><cell>h</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell></row><row><cell>FLOPs / Top-1</cell><cell>1.6B / 74.2</cell><cell>1.4B / 74.0</cell><cell>1.3B / 73.9</cell><cell>1.2B / 73.7</cell><cell>1.2B / 73.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Object detection and instance segmentation results on COCO val2017. Our Pyramid ViG is compared with other backbones on RetinaNet and Mask R-CNN frameworks.</figDesc><table><row><cell>Backbone</cell><cell>Param</cell><cell>FLOPs</cell><cell>mAP</cell><cell cols="2">RetinaNet 1× AP50 AP75</cell><cell>APS</cell><cell>APM</cell><cell>APL</cell></row><row><cell>ResNet50 [16]</cell><cell>37.7M</cell><cell>239.3B</cell><cell>36.3</cell><cell>55.3</cell><cell>38.6</cell><cell>19.3</cell><cell>40.0</cell><cell>48.8</cell></row><row><cell>ResNeXt-101-32x4d [59]</cell><cell>56.4M</cell><cell>319B</cell><cell>39.9</cell><cell>59.6</cell><cell>42.7</cell><cell>22.3</cell><cell>44.2</cell><cell>52.5</cell></row><row><cell>PVT-Small [54]</cell><cell>34.2M</cell><cell>226.5B</cell><cell>40.4</cell><cell>61.3</cell><cell>44.2</cell><cell>25.0</cell><cell>42.9</cell><cell>55.7</cell></row><row><cell>CycleMLP-B2 [4]</cell><cell>36.5M</cell><cell>230.9B</cell><cell>40.6</cell><cell>61.4</cell><cell>43.2</cell><cell>22.9</cell><cell>44.4</cell><cell>54.5</cell></row><row><cell>Swin-T [33]</cell><cell>38.5M</cell><cell>244.8B</cell><cell>41.5</cell><cell>62.1</cell><cell>44.2</cell><cell>25.1</cell><cell>44.9</cell><cell>55.5</cell></row><row><cell>Pyramid ViG-S (ours)</cell><cell>36.2M</cell><cell>240.0B</cell><cell>41.8</cell><cell>63.1</cell><cell>44.7</cell><cell>28.5</cell><cell>45.4</cell><cell>53.4</cell></row><row><cell>Backbone</cell><cell>Param</cell><cell>FLOPs</cell><cell>AP b</cell><cell cols="2">Mask R-CNN 1× AP b 50 AP b 75</cell><cell>AP m</cell><cell>AP m 50</cell><cell>AP m 75</cell></row><row><cell>ResNet50 [16]</cell><cell>44.2M</cell><cell>260.1B</cell><cell>38.0</cell><cell>58.6</cell><cell>41.4</cell><cell>34.4</cell><cell>55.1</cell><cell>36.7</cell></row><row><cell>PVT-Small [54]</cell><cell>44.1M</cell><cell>245.1B</cell><cell>40.4</cell><cell>62.9</cell><cell>43.8</cell><cell>37.8</cell><cell>60.1</cell><cell>40.3</cell></row><row><cell>CycleMLP-B2 [4]</cell><cell>46.5M</cell><cell>249.5B</cell><cell>42.1</cell><cell>64.0</cell><cell>45.7</cell><cell>38.9</cell><cell>61.2</cell><cell>41.8</cell></row><row><cell>PoolFormer-S24 [64]</cell><cell>41.0M</cell><cell>-</cell><cell>40.1</cell><cell>62.2</cell><cell>43.4</cell><cell>37.0</cell><cell>59.1</cell><cell>39.6</cell></row><row><cell>Swin-T [33]</cell><cell>47.8M</cell><cell>264.0B</cell><cell>42.2</cell><cell>64.6</cell><cell>46.2</cell><cell>39.1</cell><cell>61.6</cell><cell>42.0</cell></row><row><cell>Pyramid ViG-S (ours)</cell><cell>45.8M</cell><cell>258.8B</cell><cell>42.6</cell><cell>65.2</cell><cell>46.0</cell><cell>39.4</cell><cell>62.4</cell><cell>41.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Algorithm 1 PyTorch-like Code of ViG Block import torch.nn as nn from gcn_lib.dense.torch_vertex import DynConv2d # gcn_lib is downloaded from https://github.com/lightaime/deep_gcns_torch</figDesc><table><row><cell>nn.GELU(),</cell></row><row><cell>)</cell></row><row><cell>self.fc2 = nn.Sequential(</cell></row><row><cell>nn.Conv2d(hidden_channels, in_channels, 1, stride=1, padding=0),</cell></row><row><cell>nn.BatchNorm2d(in_channels),</cell></row><row><cell>)</cell></row><row><cell>self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()</cell></row><row><cell>def forward(self, x):</cell></row><row><cell>B, C, H, W = x.shape</cell></row><row><cell>x = x.reshape(B, C, -1, 1).contiguous()</cell></row><row><cell>shortcut = x</cell></row><row><cell>x = self.fc1(x)</cell></row><row><cell>x = self.graph_conv(x)</cell></row><row><cell>x = self.fc2(x)</cell></row><row><cell>x = self.drop_path(x) + shortcut</cell></row><row><cell>return x.reshape(B, C, H, W)</cell></row><row><cell>class FFNModule(nn.Module):</cell></row><row><cell>"""Feed-forward Network</cell></row><row><cell>"""</cell></row><row><cell>def __init__(self, in_channels, hidden_channels, drop_path=0.0):</cell></row><row><cell>super(FFNModule, self).__init__()</cell></row><row><cell>self.fc1 = nn.Sequential(</cell></row><row><cell>nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),</cell></row><row><cell>nn.BatchNorm2d(in_channels),</cell></row><row><cell>nn.GELU()</cell></row><row><cell>)</cell></row><row><cell>self.fc2 = nn.Sequential(</cell></row><row><cell>nn.Conv2d(hidden_channels, in_channels, 1, stride=1, padding=0),</cell></row><row><cell>nn.BatchNorm2d(in_channels),</cell></row><row><cell>)</cell></row><row><cell>self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()</cell></row><row><cell>def forward(self, x):</cell></row><row><cell>shortcut = x</cell></row><row><cell>x = self.fc1(x)</cell></row><row><cell>x = self.fc2(x)</cell></row><row><cell>x = self.drop_path(x) + shortcut</cell></row><row><cell>return x</cell></row><row><cell>class ViGBlock(nn.Module):</cell></row><row><cell>"""ViG block with Grapher and FFN modules</cell></row><row><cell>"""</cell></row><row><cell>def __init__(self, channels, k, dilation, drop_path=0.0):</cell></row><row><cell>super(ViGBlock, self).__init__()</cell></row><row><cell>self.grapher = GrapherModule(channels, channels * 2, k, dilation, drop_path)</cell></row><row><cell>self.ffn = FFNModule(channels, channels * 4, drop_path)</cell></row><row><cell>def forward(self, x):</cell></row><row><cell>x = self.grapher(x)</cell></row><row><cell>x = self.ffn(x)</cell></row><row><cell>return x</cell></row><row><cell>class GrapherModule(nn.Module):</cell></row><row><cell>"""Grapher module with graph conv and FC layers</cell></row><row><cell>"""</cell></row><row><cell>def __init__(self, in_channels, hidden_channels, k=9, dilation=1, drop_path=0.0):</cell></row><row><cell>super(GrapherModule, self).__init__()</cell></row><row><cell>self.fc1 = nn.Sequential(</cell></row><row><cell>nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),</cell></row><row><cell>nn.BatchNorm2d(in_channels),</cell></row><row><cell>)</cell></row><row><cell>self.graph_conv = nn.Sequential(</cell></row><row><cell>DynConv2d(in_channels, hidden_channels, k, dilation, act=None),</cell></row><row><cell>nn.BatchNorm2d(hidden_channels),</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2001">2001-2009, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cyclemlp: A mlp-like architecture for dense prediction</title>
		<author>
			<persName><forename type="first">Shoufa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention is not all you need: Pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName><forename type="first">Yihe</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2793" to="2803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCNN</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hire-mlp: Vision mlp via hierarchical rearrangement</title>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on vision transformer</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niv</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><surname>Huawei</surname></persName>
		</author>
		<author>
			<persName><surname>Mindspore</surname></persName>
		</author>
		<ptr target="https://www.mindspore.cn/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">As-mlp: An axial shifted mlp architecture for vision</title>
		<author>
			<persName><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural network for graphs: A contextual constructive approach</title>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An image patch is a wave: Phase-aware vision mlp</title>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Ilya O Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><surname>Uszkoreit</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Feedforward networks for image classification with data-efficient training</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Asher</forename><surname>Trockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kolter</forename><surname>Zico</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09792</idno>
		<title level="m">Patches are all you need?</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lipschitz regularity of deep neural networks: analysis and efficient estimation</title>
		<author>
			<persName><forename type="first">Aladin</forename><surname>Virmaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Scaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3839" to="3848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rethinking and improving relative position encoding for vision transformer</title>
		<author>
			<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10033" to="10041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeletonbased action recognition</title>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
