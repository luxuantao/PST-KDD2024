<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Robust Directional Saliency-Based Method for Infrared Small-Target Detection Under Various Complex Backgrounds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shengxiang</forename><surname>Qi</surname></persName>
							<email>shengxiang.qi@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Pattern Recognition and Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
							<email>majie@mail.hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Pattern Recognition and Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Pattern Recognition and Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changcai</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Pattern Recognition and Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinwen</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Pattern Recognition and Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">Ma</forename><surname>Qi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Robust Directional Saliency-Based Method for Infrared Small-Target Detection Under Various Complex Backgrounds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A339C67A293094A44F4C8C22ACC4340C</idno>
					<idno type="DOI">10.1109/LGRS.2012.2211094</idno>
					<note type="submission">received March 27, 2012; accepted July 26, 2012. Date of publication September 10, 2012; date of current version November 24, 2012.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Infrared image</term>
					<term>saliency detection</term>
					<term>target detection</term>
					<term>visual attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Infrared small-target detection plays an important role in image processing for infrared remote sensing. In this letter, different from traditional algorithms, we formulate this problem as salient region detection, which is inspired by the fact that a small target can often attract attention of human eyes in infrared images. This visual effect arises from the discrepancy that a small target resembles isotropic Gaussian-like shape due to the optics point spread function of the thermal imaging system at a long distance, whereas background clutters are generally local orientational. Based on this observation, a new robust directional saliency-based method is proposed incorporating with visual attention theory for infrared small-target detection. Experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art methods for real infrared images with various typical complex backgrounds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I NFRARED small-target detection plays a critical role in large amounts of practical projects such as infrared warning and defense alertness, in which not only accuracy is needed but also robustness is required. Generally, since a small target just occupies very few pixels in infrared images <ref type="bibr" target="#b0">[1]</ref>, detection results are majorly affected by random noise and nonstationary clutters. Although numerous methods have been proposed aiming at specific situations, e.g., sea-sky background <ref type="bibr" target="#b1">[2]</ref>, many of them may fail in other different circumstances. Because of this, small-target detection in infrared images is still a rather difficult and challenging problem. In a general way, the task is often performed as seeking an effective way to enhance targets relative to backgrounds to amplify the signal-to-clutter ratio (SCR) of images. For this purpose, many existing algorithms are presented and can be broadly divided into two categories: the filtering method and the predicting method. In the filtering method, targets are extracted from processed images, which are obtained by filtering original images directly <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. Alternatively, the predicting method computes the residual map between the original image and its background predicted image and then extracts targets from this map <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Different from traditional algorithms, we formulate the problem of infrared small-target detection as salient region detection. The innovation is inspired by the fact that a small target can often attract attention of human eyes in images. This visual effect arises from the discrepancy that a small target resembles isotropic Gaussian-like shape due to the optics point spread function (PSF) of the thermal imaging system at a long distance <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, whereas background clutters are generally local orientational. In other words, the "isotropic" targets are salient relative to the "orientational" backgrounds. Based on this observation, a new robust directional saliencybased method (DSBM) is proposed incorporating with visual attention theory <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref> for infrared small-target detection. First, we decompose the original image into different directional channels using a designed second-order directional derivative (SODD) filter based on a facet model <ref type="bibr" target="#b11">[12]</ref>. Second, we employ a saliency detection method, i.e., phase spectrum of Fourier transform (PFT) <ref type="bibr" target="#b9">[10]</ref>, to calculate saliency maps for each channel to enhance targets relative to backgrounds. Third, we compute the high SCR "target-saliency" map by a designed saliency fusing method that can fuse all saliency maps from each channel. Finally, we use an empirical threshold to extract targets from the target-saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THEORY AND METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Computing the SODD Map Using the Facet Model</head><p>In the proposed algorithm, we first decompose the original image into different directional channels. Due to the capability of detecting ramps and isolated uplift of signals, the SODD filter is used as the decomposition tool to compute SODD maps for each channel. It can transform clutters and targets into fixed orientational striplike texture and Gaussianlike spots (although it is not rigorous Gaussian), respectively. A procedure of processing a 2-D signal using our designed SODD filter is displayed in Fig. <ref type="figure" target="#fig_0">1</ref>. The original map [see Fig. <ref type="figure" target="#fig_0">1(a)</ref>] contains a ramp and a Gaussian uplift representing clutters and a small target, respectively. Fig. <ref type="figure" target="#fig_0">1(b</ref>) is the primitive SODD map in Fig. <ref type="figure" target="#fig_0">1</ref>(a) in an assigned direction (channel), and Fig. <ref type="figure" target="#fig_0">1(c</ref>) is the amended SODD map in which the ramp and the Gaussian uplift are transformed into shape of striplike texture and Gaussian-like spot, respectively. Specifically, the designed filter is described below in details.</p><p>Because the Laplacian operator is susceptible to noise and lack of smoothness, we use a facet model <ref type="bibr" target="#b11">[12]</ref> to design the SODD filter. This model assumes that the underlying graylevel intensity surface for each pixel can be approximated by a bivariate cubic function in its neighborhood. Assume that R = {-2 -1 0 1 2} and C = {-2 -1 0 1 2}, the 5 × 5 neighborhood window for a pixel can be denoted by R × C, in which the pixel is at the center (0, 0). Then, the intensity value of the pixel expressed by the bivariate cubic function defined over R × C is calculated as follows:</p><formula xml:id="formula_0">f (r, c) = 10 i=1 K i • P i (r, c)<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">{P i (r, c)} = {1, r, c, r 2 -2, rc, c 2 -2, r 3 -(17/5)r, (r 2 -2)c, r(c 2 -2), c 3 -(17/5</formula><p>)c} is the set of discrete orthogonal polynomials in which (r, c) ∈ R × C; K i are coefficients for the bivariate cubic function. Given (1), the second-order partial derivatives evaluated along the row and column at the center pixel (0, 0) in R × C are</p><formula xml:id="formula_2">∂ 2 f (r, c) ∂r 2 (0,0) = 2K 4 ∂ 2 f (r, c) ∂r∂c (0,0) = K 5 ∂ 2 f (r, c) ∂c 2 (0,0) = 2K 6 . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>Note that the values of coefficients K i (i = 4, 5, 6) are related to different pixels (x, y) in the image; we denote these coefficients as K i (x, y) (i = 4, 5, 6) to avoid confusion. According to <ref type="bibr" target="#b11">[12]</ref>, these coefficients are determined by least squares surface fitting and orthogonal property of polynomials with the result</p><formula xml:id="formula_4">K i (x, y) = r c (I(x + r, y + c)P i (r, c)/ r c P 2 i (r, c)) (i = 4, 5, 6</formula><p>), in which I(x, y) is the image intensity of (x, y). For actual calculation, they can be individually computed by a linear combination of the intensity values in the symmetric R × C neighborhood of pixels, each of which has a weight w i (r, c) = (P i (r, c)/ r c P 2 i (r, c)) (i = 4, 5, 6). Therefore, given the set of discrete orthogonal polynomials, i.e., {P i (r, c)}, the three weight kernels are obtained as follows:</p><formula xml:id="formula_5">W 4 = 1 70 ⎡ ⎢ ⎢ ⎢ ⎣ 2 2 2 2 2 -1 -1 -1 -1 -1 -2 -2 -2 -2 -2 -1 -1 -1 -1 -1 2 2 2 2 2 ⎤ ⎥ ⎥ ⎥ ⎦ W 5 = 1 100 ⎡ ⎢ ⎢ ⎢ ⎣ 4 2 0 -2 -4 2 1 0 -1 -2 0 0 0 0 0 -2 -1 0 1 2 -4 -2 0 2 4 ⎤ ⎥ ⎥ ⎥ ⎦ W 6 = W T 4 . (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>Given ( <ref type="formula" target="#formula_2">2</ref>), we then deduce the formula of SODD along direction vector l at pixel (x 0 , y 0 ) as follows:</p><formula xml:id="formula_7">∂ 2 f (x, y) ∂l 2 (x 0 ,y 0 ) = f xx (x, y) cos 2 α+2f xy (x, y) ×cos α cos β + f yy (x, y) cos 2 β (x 0 ,y 0 ) = 2K 4 (x 0 , y 0 ) cos 2 α + 2K 5 (x 0 , y 0 ) × cos α cos β + 2K 6 (x 0 , y 0 ) cos 2 β (4)</formula><p>where α is the angle between l and the x-axis (row of image); β is the angle between l and the y-axis (column of image). However, note that the target values of the primitive SODD map [see Fig. <ref type="figure" target="#fig_0">1(b)</ref>] acquired by ( <ref type="formula">4</ref>) are less than zero, whereas the neighborhood values of the target along the assigned direction are larger than zero. This effect gives rise to deviation of the target from Gaussian-like shape, which will lead to disturbance for saliency detection. Accordingly, we need to amend the SODD map, which consists of: 1) set map values larger than zero to be zero; 2) inverse the whole map after all values are normalized to a fixed range [0, 1]; and 3) filter the map using a 3 × 3 smoothing mask to smooth edges of targets and strips. After this, the shape characteristics of the target and clutters are highlighted in the amended map [see Fig. <ref type="figure" target="#fig_0">1(c)]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Computing the Directional Saliency Map Using PFT</head><p>After computing SODD maps for each directional channel, characteristics of targets and clutters are separated into two types, i.e., the former are Gaussian-like and the latter are striplike. In general as a fact, the "conspicuous" small target with its particular shape and brightness often attracts attention of human eyes relative to the "ordinary" clutters in SODD maps. Based on this observation, we use the saliency detection method <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref> derived from visual attention theory to highlight the Gaussianlike targets. This method often simulates the brain and vision system to identify the relevant salient regions, which accords with the mechanism of the human visual system. Additionally, the PFT method <ref type="bibr" target="#b9">[10]</ref> has very low computational complexity among most saliency detection approaches; it is suitable to compute saliency maps for SODD maps. For explanation, two instances of PFT are displayed in Fig. <ref type="figure" target="#fig_1">2</ref>. Each shows an original map at the left containing a Gaussian spot (small target) and orientational strips (clutters) and its PFT saliency map at the right. It is legible that Gaussian spots are highlighted as salient regions while strips are inhibited, which accords with our visual effects.</p><p>PFT <ref type="bibr" target="#b9">[10]</ref> is an improved method from <ref type="bibr" target="#b8">[9]</ref>. It proves that the location information of salient regions can be acquired from the image's PFT. More specifically, detailed description on PFT can be referred to <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref>. In this letter, we briefly express the procedure of computing saliency maps using PFT as follows:</p><formula xml:id="formula_8">f (u, v) = F [I(x, y)] (5) p(u, v) = P [f (u, v)] (6) S(x, y) = g(x, y) * F -1 e i•p(u,v) 2 (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where I(x, y) denotes the image, which is namely each SODD map in DSBM; F [.] and F -1 [.] denote the Fourier transform and the inverse Fourier transform, respectively; P [.] represents the phase spectrum of the image; g(x, y) is a 2-D Gaussian filter (σ = 2.5); and S(x, y) is the saliency map. These expressions indicate that the saliency map can be simply obtained by computing the phase spectrum of the original image and then calculating the inverse Fourier transform using them alone, in which the amplitude spectrum is set to be 1. The square and convolution in <ref type="bibr" target="#b6">(7)</ref> are just applied to standout relevant salient regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Computing the Target-Saliency Map Using a Designed Saliency Fusing Method</head><p>Directional saliency maps of different channels have different magnitudes. It is infeasible to fuse these maps directly since they represent a priori not comparable modalities with different dynamic ranges. To solve this problem, we employ a normalization operator N(.) <ref type="bibr" target="#b10">[11]</ref>, which can globally promote maps in which a small number of strong peaks of activity (conspicuous locations) are present, while globally suppressing maps that contain numerous comparable peak responses. It consists of: 1) normalize the values in the map to a fixed range [0, . . . , M] in order to eliminate modality-dependent amplitude differences; 2) find the location of the map's global maximum M and computing the average m of all its other local maxima; and 3) globally multiply the map by (Mm) 2 .</p><p>Then, we design an effective saliency fusing method to fuse all the directional saliency maps into a final "target-saliency" map, which is expressed as follows:</p><formula xml:id="formula_10">= g N(S g ) • N(⊥ S g ) (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where g represents the number of orthogonal direction groups; S g is defined as the directional saliency map and ⊥ S g is the orthogonal directional saliency map of S g in group g; and is the fused map, i.e., the "target-saliency" map. By means of multiplication between orthogonal directions, this expression can inhibit the orientational striplike background clutters and amplify the isotropic Gaussian-like targets.  SODD filter, and then directional saliency maps are computed by PFT; finally, the target-saliency map is obtained using the saliency fusing method. Detailed process steps of DSBM for infrared small-target detection are described as follows.</p><p>Step 1) Select appropriate orthogonal direction groups according to actual requirements. Generally, two groups, one for channel 1 Step 3) Compute SODD maps for each directional channel using designed SODD filter (4), and then amend these maps.</p><formula xml:id="formula_12">(α = 0 • β = 90 •</formula><p>Step 4) Compute directional saliency maps for each channel using the PFT method ( <ref type="formula">5</ref>), ( <ref type="formula">6</ref>), (7).</p><p>Step 5) Fuse directional saliency maps of all channels using the designed saliency fusing method (8) to acquire the target-saliency map.</p><p>Step 6) Choose the segmentation threshold, which is 0.35 times of the maximal value (an empirical value from experiments), to extract a small target from the target-saliency map. Note that although approaches of threshold selection are various, such a simple pattern is adequate in most cases according to experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>In order to validate the performance and robustness of our proposed algorithm for infrared small-target detection, experiments are performed on eight typical real infrared images with sky-cloud [see Fig. <ref type="figure">4</ref>  <ref type="figure">4</ref>(g) and (h)] clutters representing five kinds of complex backgrounds. These images and their corresponding target-saliency maps processed by DSBM are shown in Fig. <ref type="figure">4</ref>. Meanwhile, we compare our algorithm with four state-of-the-art approaches, including morphological detection method (Top-hat) <ref type="bibr" target="#b4">[5]</ref>, adaptive Butterworth high-pass filter method (BHPT) <ref type="bibr" target="#b1">[2]</ref>, facet-based method (facet model) <ref type="bibr" target="#b2">[3]</ref>, and least squares support vector machine-based method (LS-SVM) <ref type="bibr" target="#b3">[4]</ref>. For fair comparison, we adopt two common evaluation indicators <ref type="bibr" target="#b12">[13]</ref>, i.e., SCR Gain and background suppression factor (BSF) defined as follows:</p><formula xml:id="formula_13">SCR Gain = (S/C) out (S/C) in BSF = C in C out<label>(9)</label></formula><p>where S and C represent the signal amplitude and clutter standard deviation, respectively. In experiments, S is calculated as the difference between the mean of target and background values, and C is calculated as the standard deviation of background. The subscripts in and out express the images before and after the detection. Performed on MATLAB R2010 software in a 3.10-GHz PC, experimental results, including SCR Gain, BSF, and run time, are listed in Table <ref type="table" target="#tab_0">I</ref>. Data in Table <ref type="table" target="#tab_0">I</ref> show that DSBM has good performance in SCR Gain and BSF, particularly the former. Specifically, SCR Gain reflects the amplification of target signals relative to backgrounds after and before processing, whereas BSF only expresses the suppression level of backgrounds without any target information. Thus, compared with BSF, SCR Gain is more significant in target detection. Considering this fact, DSBM actually outperforms other methods in most cases, including Fig. <ref type="figure">4</ref>(a) and (c)-(h). The reason for a little lower BSF of DSBM in some cases is that saliency detection does not totally suppress backgrounds in essence but only to amplify the salient targets relative to clutters to make signals' conspicuousness. In addition, in images such as those in Fig. <ref type="figure">4</ref>(b), LS-SVM has a little better performance than DSBM. This is because the sea-wave clutters close to the target result in deviation from Gaussian-like shape of the target, whereas LS-SVM is almost unaffected since the position of the target's extreme point almost does not change by this deformation. However, in situations such as Fig. <ref type="figure">4</ref>(e), (f), and (h), which have strong clutters and noise, DSBM still performs better than LS-SVM. This fact can also prove that the proposed method is more robust than other methods in such harsh circumstances. In spite that DSBM is more time consuming than other methods, it has an acceptable order of magnitude just as traditional method BHPT and can be improved by parallel computing or using more efficient saliency detection methods.</p><p>Furthermore, in Fig. <ref type="figure" target="#fig_5">5</ref>, we draw receiver operation characteristic (ROC) curves of the detection results for each method to provide a quantitative comparison of detection performance. As well as the description in <ref type="bibr" target="#b5">[6]</ref>, the ROC curves represent the varying relationship of detection probability P d and false alarm rate P f . P d is defined as the ratio of the number of detected pixels to the number of real target pixels, and P f is the ratio of the number of false alarms to the total number of pixels in the whole image. Compared with "general" evaluation terms such as SCR Gain and BSF, ROC curves are more "specific" in describing the performance of target extraction using a threshold processed high SCR maps. In this sense, they are more significant than SCR Gain and BSF. In Fig. <ref type="figure" target="#fig_5">5</ref>, the results show that DSBM has better ROC curves than other methods, which indicates that the detection probability of the proposed method is always higher than others' under the same false alarm rate. In other words, DSBM is more accurate than other methods for target detection. Note that the morphological detection algorithm can also get good ROC curves for several images such as those in Fig. <ref type="figure">4</ref>(b) and (h). However, this method largely depends on the suitable structural elements according to priori knowledge of targets. Once there is no suitable structural element to the image, the results may become poor such as Fig. <ref type="figure" target="#fig_5">5</ref>(g). In summary, DSBM actually shows its great performance and robustness for infrared small-target detection under various complex backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this letter, we have presented a robust DSBM for infrared small-target detection inspired by visual effect of human eyes between a small target and backgrounds. Saliency detection is used to enhance target signals in this method. For fair and comprehensive evaluation, real infrared images with different typical complex backgrounds and common evaluation indicators, including SCR Gain, BSF, run time, and ROC curves, are used in the experiments. The results indicate that the proposed algorithm has good performance and robustness and performs better than the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Procedure of processing a 2-D signal using our designed SODD filter.</figDesc><graphic coords="2,42.47,70.25,250.22,77.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Two instances of PFT, each of which shows an original map at the left containing a Gaussian spot (small target) and orientational strips (clutters) and its PFT saliency map at the right.</figDesc><graphic coords="3,300.23,70.49,250.22,275.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Flow diagram of acquiring the high SCR target-saliency map by DSBM.</figDesc><graphic coords="3,300.23,392.21,250.22,268.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>) and channel 2</head><label>2</label><figDesc>(α = 90 • β = 0 • ), another for channel 3 (α = 45 • β = 45 • ) and channel 4 (α = 135 • β = 45 • ), are sufficient for detection. Step 2) Do convolutions between weight kernels (3) and the original infrared image to compute fitting coefficients K i (x, y) (i = 4, 5, 6) for each pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a)], sea-wave [see Fig. 4(b)-(d)], treefork [see Fig. 4(e)], road-side [see Fig. 4(f)], and continentground [see Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison of ROC curves obtained by five methods for each test image in Fig. 4.</figDesc><graphic coords="5,39.73,70.42,246.12,428.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EVALUATION</head><label>I</label><figDesc>INDICATORS OF FIVE DETECTION METHODS</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61004111, Grant 61074156, and Grant 61104191 and in part by the National 973 Plan of China under Grant 2012CB719900.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting and tracking dim moving point target in IR image sequence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared. Phys. Technol</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="323" to="328" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive detection for infrared small target under sea-sky complex background</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="1083" to="1085" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Facet-based infrared small target detection method</title>
		<author>
			<persName><forename type="first">G.-D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-B</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="1244" to="1246" />
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Infrared small target detection using directional highpass filters based on LS-SVM</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="156" to="158" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Morphologybased algorithm for point target detection in infrared backgrounds</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Bondaryk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Signal Data Process</title>
		<meeting>Signal Data ess<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">1954</biblScope>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A kernel-based nonparametric regression method for clutter removal in infrared small-target detection applications</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="469" to="473" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Performance evaluation of 2-D adaptive prediction filters for detection of small objects in image data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Zeidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="1993-07">Jul. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial processing techniques for the detection of small targets in IR clutter</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Langan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Staver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1990-04">Apr. 1990</date>
			<biblScope unit="volume">1305</biblScope>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Digital step edges from zero crossing of second directional derivatives</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="68" />
			<date type="published" when="1984-01">Jan. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selection of a clutter rejection algorithm for real-time target detection from an airborne platform</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Hilliard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2000-04">Apr. 2000</date>
			<biblScope unit="volume">4048</biblScope>
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
