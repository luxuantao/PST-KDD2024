<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A comparison of methods for sketch-based 3D shape retrieval q</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-12-07">7 December 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Texas State University</orgName>
								<address>
									<settlement>San Marcos</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Information Technology Laboratory</orgName>
								<orgName type="institution">National Institute of Standards and Technology</orgName>
								<address>
									<settlement>Gaithersburg</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
							<email>lu@txstate.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Texas State University</orgName>
								<address>
									<settlement>San Marcos</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Afzal</forename><surname>Godil</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Information Technology Laboratory</orgName>
								<orgName type="institution">National Institute of Standards and Technology</orgName>
								<address>
									<settlement>Gaithersburg</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Schreck</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer and Information Science</orgName>
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Bustos</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chile</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alfredo</forename><surname>Ferreira</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Instituto Superior Técnico/ Technical University of Lisbon/ INESC-ID</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takahiko</forename><surname>Furuya</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of Yamanashi</orgName>
								<address>
									<settlement>Yamanashi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Fonseca</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Instituto Superior Técnico/ Technical University of Lisbon/ INESC-ID</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Henry</forename><surname>Johan</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">Visual Computing</orgName>
								<orgName type="institution" key="instit2">Fraunhofer IDM@NTU</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takahiro</forename><surname>Matsuda</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of Yamanashi</orgName>
								<address>
									<settlement>Yamanashi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryutarou</forename><surname>Ohbuchi</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of Yamanashi</orgName>
								<address>
									<settlement>Yamanashi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><forename type="middle">B</forename><surname>Pascoal</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Instituto Superior Técnico/ Technical University of Lisbon/ INESC-ID</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Saavedra</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chile</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="laboratory">Computer Vision Research Group</orgName>
								<orgName type="institution">ORAND S.A</orgName>
								<address>
									<settlement>Santiago</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Texas State University</orgName>
								<address>
									<addrLine>601 University Drive</addrLine>
									<postCode>78666</postCode>
									<settlement>San Marcos</settlement>
									<region>TX</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A comparison of methods for sketch-based 3D shape retrieval q</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-12-07">7 December 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">4ECD3E49CD06A9A871C94EFCABF427A5</idno>
					<idno type="DOI">10.1016/j.cviu.2013.11.008</idno>
					<note type="submission">Received 23 September 2013 Accepted 22 November 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Sketch-based 3D model retrieval Evaluation SHREC contest Large-scale Benchmark</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sketch-based 3D shape retrieval has become an important research topic in content-based 3D object retrieval. To foster this research area, two Shape Retrieval Contest (SHREC) tracks on this topic have been organized by us in 2012 and 2013 based on a small-scale and large-scale benchmarks, respectively. Six and five (nine in total) distinct sketch-based 3D shape retrieval methods have competed each other in these two contests, respectively. To measure and compare the performance of the top participating and other existing promising sketch-based 3D shape retrieval methods and solicit the state-of-the-art approaches, we perform a more comprehensive comparison of fifteen best (four top participating algorithms and eleven additional state-of-the-art methods) retrieval methods by completing the evaluation of each method on both benchmarks. The benchmarks, results, and evaluation tools for the two tracks are publicly available on our websites [1,2].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sketch-based 3D model retrieval is focusing on retrieving relevant 3D models using sketch(es) as input. This intuitive and convenient scheme is easy for users to learn and use to search for 3D models. It is also popular and important for related applications such as sketch-based modeling and recognition, as well as 3D animation production via 3D reconstruction of a scene of 2D storyboard <ref type="bibr" target="#b0">[3]</ref>.</p><p>However, most existing 3D model retrieval algorithms target the Query-by-Model framework, that is, using existing 3D models as queries. In the areas of content-based 2D image retrieval and image synthesis, sketch-based methods have been addressed for some time now. In 3D model retrieval, on the other hand, less work has to date considered the Query-by-Sketch framework. In fact, it is a non-trivial task to perform sketch-based 3D model retrieval and also more difficult compared with the Query-by-Model case. This is because there exists a semantic gap between the sketches humans draw and the 3D models in the database, implying that the structure of the query and target objects differ. Specifically, target objects are typically given as precisely modeled objects, while the query sketch may differ drastically in level of detail, abstraction, and precision. In addition, until now there is no comprehensive evaluation or comparison for the large number of available sketch-based retrieval algorithms. Considering this, we organized the Shape Retrieval Contest (SHREC) 2012 track on Sketch-Based 3D Shape Retrieval <ref type="bibr">[1,</ref><ref type="bibr" target="#b1">4]</ref>, held in conjunction with the fifth Eurographics Workshop on 3D Object Retrieval, to foster this challenging research area by providing a common small-scale sketch-based retrieval benchmark and soliciting retrieval results from current state-of-the-art retrieval methods for comparison. We also provided corresponding evaluation code for computing a set of performance metrics similar to those typically used to evaluate Query-by-Model techniques. The objective of this track was to evaluate the performance of different sketch-based 3D model retrieval algorithms using both hand-drawn and standard line drawings sketch queries on a watertight 3D model dataset. Every participant performed the queries and sent us their retrieval results. We then did the performance assessment.</p><p>A satisfactory success has been achieved in the SHREC'12 sketch track <ref type="bibr" target="#b1">[4]</ref>. However, the contest has limitations in terms of its evaluation of different sketch-based retrieval algorithms based on a rather small benchmark and a comparison of a limited number of methods. Eitz et al. <ref type="bibr" target="#b2">[5]</ref> provided us the largest sketch-based 3D shape retrieval benchmark until 2012, based on the Princeton Shape Benchmark (PSB) <ref type="bibr" target="#b3">[6]</ref> with one user sketch for each PSB model. However, until now no comparative evaluation has been done on a very large-scale sketch-based 3D shape retrieval benchmark. Considering this and encouraged by the successful sketch-based 3D model retrieval track in SHREC'12 <ref type="bibr" target="#b1">[4]</ref>, in 2013 we organized another track [2,7] with a similar topic in SHREC'13 to further foster this challenging research area by building a very large-scale benchmark and soliciting retrieval results from current state-of-the-art retrieval methods for comparison. Similarly, we also provided corresponding evaluation code for computing the same set of performance metrics as the SHREC'12 sketch track. For this track, the objective was evaluating the performance of different sketchbased 3D model retrieval algorithms using a large-scale handdrawn sketch query dataset for querying from a generic 3D model dataset.</p><p>After finishing the above two SHREC contests, we have found that the participating methods for the two contests are not completely the same, thus a conclusion of the current state-of-the-art algorithm is still unavailable. In addition, to provide a more complete reference for the researchers in this research direction, it is necessary to perform a more incisive analysis on different participating methods w.r.t their scalability and efficiency performance, as well as the two benchmarks used in the two contest tracks. Motivated by the above two findings, we decided to perform a follow-up study by completing a more comprehensive evaluation of currently available top sketch-based retrieval algorithms on the two benchmarks such as to perform a more comprehensive comparison on them and solicit the state-of-the-art approaches. Thus, we sent invitations to the participants as well as the authors of recently published related papers (according to our knowledge) to ask them to contribute to the new comprehensive evaluation. Totally, 6 groups accepted our invitations and agreed to submit their results on schedule. Finally, 15 best-performing methods (4 top participating algorithms and 11 additional state-of-the-art approaches; totally 17 runs) from 4 groups successfully submitted their results, including running results (e.g. retrieval lists and timing information) and method description, which are also available on the SHREC'12 and SHREC'13 sketch track website <ref type="bibr">[1,</ref><ref type="bibr">2]</ref>. After that, we performed a comparative evaluation on them.</p><p>In this paper, we first review the related work (w.r.t. techniques and benchmarks, respectively) in Section 2. Then, in Section 3 we introduce the two benchmarks (one small-scale and one largescale) used in the two contest tracks. Section 4 gives a brief introduction of the contributors of the paper. A short and concise description for each contributed method is presented in Section 5. Section 6 describes the evaluation results of the 15 sketch-based 3D retrieval algorithms on the SHREC'12 small-scale benchmark and SHREC'13 large-scale benchmark, respectively. Section 7 further comments on the benchmarks and analyzes the contributed algorithms w.r.t the performance they achieved. Section 8 concludes the paper and further lists several future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sketch-based 3D model retrieval techniques</head><p>Existing sketch-based 3D model retrieval techniques can be categorized differently according to dissimilar aspects: Local versus global 2D features; Bag-of-Words framework versus direct shape feature matching; Fixed views versus clustered views; With versus without view selection. In this section, we will review some typical recent work in this field.</p><p>In 2003, Funkhouser et al. <ref type="bibr" target="#b5">[8]</ref> developed a search engine which supports both 2D and 3D queries based on an extended version of 3D spherical harmonics <ref type="bibr" target="#b6">[9]</ref> from 3D to 2D. Yoon et al. <ref type="bibr" target="#b7">[10]</ref> and Saavedra et al. <ref type="bibr" target="#b9">[11]</ref> developed their sketch-based 3D model retrieval algorithms based on suggestive contours <ref type="bibr" target="#b10">[12]</ref> feature views sampling and diffusion tensor fields feature representation or structure-based local approach (STELA). Aono and Iwabuchi <ref type="bibr" target="#b11">[13]</ref> proposed an image-based 3D model retrieval algorithm based on the Zernike moments and Histogram of Oriented Gradient (HOG) features. Eitz et al. <ref type="bibr" target="#b12">[14]</ref><ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref><ref type="bibr" target="#b2">5,</ref><ref type="bibr" target="#b15">17]</ref> implemented their sketch-based 2D/3D object retrieval algorithms by utilizing the Bag-of-Words framework and local features including HOG and its modified versions, as well as a feature named Gabor local line-based feature (GALIF). Shao et al. <ref type="bibr" target="#b16">[18]</ref> developed an efficient and robust contourbased shape matching algorithm for sketch-based 3D model retrieval. Li and Johan <ref type="bibr" target="#b17">[19]</ref> performed ''View Context'' <ref type="bibr" target="#b18">[20]</ref> based 2D sketch-3D model alignment before 2D-3D matching based on relative shape context matching <ref type="bibr" target="#b19">[21]</ref>. Li et al. <ref type="bibr" target="#b20">[22]</ref> further developed a sketch-based 3D model retrieval algorithm based on the idea of performing sketch recognition before sketch-model matching.</p><p>Recently, 2D line drawings have also been utilized to reconstruct correspondent 3D models, which often involves sketchbased 3D shape retrieval techniques. Several line drawing-based reconstruction algorithms <ref type="bibr" target="#b21">[23]</ref><ref type="bibr" target="#b22">[24]</ref><ref type="bibr" target="#b23">[25]</ref> have been proposed based on the idea of 2D parts separation, 3D parts search and combination to create a 3D model based on its 2D line drawing. On the other hand, Xie et al. <ref type="bibr" target="#b24">[26]</ref> developed a sketching-based 3D modification and variation modeling interface based on the idea of parts assembly. Further, Sketch2Scene <ref type="bibr" target="#b25">[27]</ref> builds a 3D scene based on a 2D scene sketch by incorporating an analysis of structural context information among the objects in the 2D scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">2D feature views</head><p>Matching 3D models with a 2D sketch requires us to sample and render appropriate 2D feature views of a 3D model for an as accurate as possible feature correspondence between the 2D and 3D information. In this section, we review typical feature views (some examples are shown in Fig. <ref type="figure" target="#fig_0">1</ref>) that have often been used or are promising in sketch-based 3D retrieval algorithms.</p><p>2.1.1.1. Silhouette view. This is to simply render the black and white image to represent a view of a 3D model. It has been adopted in several latest sketch-based 3D model retrieval algorithms, such as Kanai <ref type="bibr" target="#b26">[28]</ref>, Ohbuchi et al. <ref type="bibr" target="#b1">[4]</ref>, Aono and Iwabuchi <ref type="bibr" target="#b11">[13]</ref>.</p><p>2.1.1.2. Contour or outline feature view. Contours are a serial of points where the surface blends sharply and becomes invisible to the viewer <ref type="bibr" target="#b10">[12]</ref>. They have been utilized in Tatsuma and Aono <ref type="bibr" target="#b27">[29]</ref>, Aono and Iwabuchi <ref type="bibr" target="#b11">[13]</ref>, and Li and Johan <ref type="bibr" target="#b17">[19]</ref>.</p><p>2.1.1.3. Suggestive contours feature view <ref type="bibr" target="#b10">[12]</ref>. Suggestive contours are contours in the nearby views, that is, they will become contours after rotating the model a little bit. They have been used in the following sketch-based 3D model retrieval algorithms: Yoon et al. <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b28">30]</ref>, Saavedra et al. <ref type="bibr" target="#b29">[31]</ref> and Eitz et al. <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b1">4]</ref> (using both occluding contours and suggestive contours).</p><p>2.1.1.4. Apparent ridges. Apparent ridges <ref type="bibr" target="#b30">[32]</ref> are defined as the loci of points that maximize a view-dependent curvature and they are extensions beyond ridge and valleys <ref type="bibr" target="#b31">[33]</ref>. They have been utilized by Eitz et al. <ref type="bibr" target="#b12">[14]</ref> in their retrieval algorithm.</p><p>2.1.1.5. Other more sophisticated 3D line drawings. Recently, quite a few new and more sophisticated 3D line drawings have been proposed. We regard them as promising in achieving even better results compared with those features mentioned above. They include photic extremum lines (PEL) <ref type="bibr" target="#b32">[34]</ref> and its GPU-accelerated version GPEL <ref type="bibr" target="#b33">[35]</ref>, demarcating curves <ref type="bibr" target="#b34">[36]</ref>, perceptual-saliency extremum lines <ref type="bibr" target="#b35">[37]</ref>, Laplacian lines <ref type="bibr" target="#b36">[38]</ref>, Difference-of-Gaussian (DoG)-based 3D line drawing <ref type="bibr" target="#b37">[39]</ref>, as well as the latest multi-scale curves on 3D surface <ref type="bibr" target="#b38">[40]</ref>. For the classification and characteristics of the above methods, please refer to the survey written by Doug DeCarlo <ref type="bibr" target="#b39">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">2D shape descriptors</head><p>For a sketch-based 3D model retrieval algorithm, developing or selecting an appropriate 2D shape descriptor is an important part to represent a 2D sketch as well as the 2D feature views of a 3D model, such as those mentioned in Section 2.1.1. In this section, we present several typical and promising 2D shape descriptors for sketch-based retrieval.</p><p>Fourier descriptor (FD) is an important shape descriptor and has been successfully applied in many pattern recognition related applications such as shape analysis, classification and retrieval as well as character recognition <ref type="bibr" target="#b40">[42]</ref>. However, it assumes that we can get the boundary information of a shape beforehand and it does not consider the internal information of the shapes. Considering the above limitations, Zhang and Lu <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b42">44]</ref> extended the Fourier descriptor and proposed a more robust and accurate shape descriptor called generic Fourier descriptor (GFD) which applies Fourier transform on a polar-raster sampled shape image.</p><p>Zernike moments feature <ref type="bibr" target="#b43">[45]</ref> is one typical moment descriptor that outperforms other moments in terms of performance in different applications. For example, 3D Zernike moments <ref type="bibr" target="#b44">[46]</ref> feature has been developed to deal with 3D model retrieval. Revaud et al. <ref type="bibr" target="#b45">[47]</ref> proposed an improved Zernike moments <ref type="bibr" target="#b43">[45]</ref> comparator which considers not only the magnitude of the moments (classic Zernike moments comparator) but also their phase information. They demonstrated its better performance than the classic one.</p><p>Local binary pattern <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b46">48]</ref> divides the surrounding regions of any pixel in a binary image into eight directions, computes the percentages of the pixels falling in each bin and regards this distribution information as a local binary pattern (LBP) encoded using an 8-bit binary number, and finally represents the whole image based on the statistical distribution of all the local binary patterns. It can be used to measure the similarity between the 2D sketch after a pre-processing and the rendered feature images of a 3D model.</p><p>Shape context <ref type="bibr" target="#b19">[21]</ref> is a log-polar histogram and defines the relative distribution of other sample points with respect to a sample point. It has been successfully applied in diverse tasks. The default shape context definition partitions the surrounding area of a sample point of a 2D shape into 5 distance bins and 12 orientation bins. Thus, the shape context is represented by a 5 Â 12 matrix. Different points have different shape context features in one shape and similar points in two similar shapes usually have similar shape context features. Shape context is scale and transformation-invariant but not rotation-invariant. To achieve the property of rotation invariance, in <ref type="bibr" target="#b19">[21]</ref> a relative frame is defined by adopting the local tangent vector at each point as the reference x axis for angle computation and we refer to it as relative shape context. In addition, Edge histogram <ref type="bibr" target="#b47">[49]</ref> can be regarded as an alternative of shape context for sketch representation.</p><p>Scale-invariant feature transform (SIFT) <ref type="bibr" target="#b48">[50]</ref> feature together with the Bag-of-Features (BoF) framework has many applications in various computer vision research fields. To optimize the search accuracy, efficiency and memory usage in a large scale image retrieval scenario which utilizes SIFT features and BoF framework, Jégou et al. <ref type="bibr" target="#b49">[51]</ref> proposed a new compact image representation to aggregate SIFT local descriptors. It achieves a significantly better performance than BoF on condition that the feature vectors used have the same size. In addition, Ohbuchi et al. <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b51">53]</ref> proposed several extended versions of SIFT feature for 3D retrieval, such as Dense SIFT (DSIFT), Grid SIFT (GSIFT) and One SIFT (1SIFT) which also have achieved good retrieval performance.</p><p>Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b52">[54]</ref> was first proposed for human detection based on the local and combinational orientation and magnitude distribution of the gradients in each grid of an image. According to the characteristics of a sketch, HOG has been modified and applied in sketch-based 2D and 3D object retrieval. For example, to perform a large-scale 3D model retrieval, Eitz et al. <ref type="bibr" target="#b15">[17]</ref> utilized a simplified HOG (SHOG) feature (first proposed in <ref type="bibr" target="#b14">[16]</ref>), which only concerns the orientation information. HOG was also successfully used in sketch-based image retrieval, for instance, like <ref type="bibr" target="#b47">[49]</ref>. Eitz et al. <ref type="bibr" target="#b14">[16]</ref> also performed a comparative evaluation on several 2D shape descriptors for sketch-based image retrieval, including HOG, SHOG, local shape context and a modified version of shape context named ''Spark'' feature.</p><p>Other recent 2D shape representations or transforms include tensor representation, which was used in Yoon et al. <ref type="bibr" target="#b7">[10]</ref> and Eitz et al. <ref type="bibr" target="#b47">[49]</ref>, as well as the latest feature Gabor local line-based feature (GALIF) <ref type="bibr" target="#b2">[5]</ref> which has demonstrated outperforming performance than SIFT <ref type="bibr" target="#b48">[50]</ref>, Spherical Harmonics <ref type="bibr" target="#b5">[8]</ref>, and Diffusion Tensor representation <ref type="bibr" target="#b7">[10]</ref>. Motivated by the idea of Curvelet transform <ref type="bibr" target="#b53">[55]</ref>, GALIF is a transformation type feature and it approximates Curvelet by utilizing Gabor filters which only respond to some special frequency and orientation.</p><p>2.2. Sketch-based 3D shape retrieval benchmarks 2.2.1. Snograss and Vanderwart's standard line drawings <ref type="bibr">(1980)</ref> Snograss and Vanderwart <ref type="bibr" target="#b54">[56]</ref> built a dataset of 260 standard line drawings. These sketches were originally designed for experiments in cognitive psychology. They were carefully designed to be comparable regarding four variables fundamental to memory and cognitive processing, including name agreement, image agreement, familiarity, and visual complexity. Their main target is to explore the correlation among those four cognition factors and this pioneer work was followed by several research work with respect to different languages such as French <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref>, Spanish <ref type="bibr" target="#b57">[59]</ref> and Portuguese <ref type="bibr" target="#b58">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Cole et al.'s line drawing benchmark (2008)</head><p>Cole et al. <ref type="bibr" target="#b59">[61]</ref> built a line drawing benchmark (together with corresponding 3D models) such as to study the relationship between human-drawn sketches and computer graphics feature lines. They created line drawings of 12 models including bones, mechanical parts, tablecloths and synthetic shapes. However, either the number of sketches or that of models is very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Saavedra and Bustos's sketch dataset (2010)</head><p>Saavedra and Bustos <ref type="bibr" target="#b60">[62]</ref> built a small sketch dataset (rotation variations of 53 sketches) to test the performance of their sketchbased image retrieval algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">Yoon et al.'s sketch-based 3D model retrieval benchmark (2010)</head><p>To perform sketch-based 3D model retrieval and evaluate their algorithm, Yoon et al. <ref type="bibr" target="#b7">[10]</ref> built a benchmark which contains 250 sketches for the 260 models of the Watertight Model Benchmark (WMB) dataset <ref type="bibr" target="#b61">[63]</ref> and the sketches and models are categorized into 13 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5.">Eitz et al.'s sketch-based shape retrieval benchmark</head><p>Eitz et al. <ref type="bibr" target="#b2">[5]</ref> built a sketch dataset containing one sketch for each of the 1814 models in the Princeton Shape Benchmark (PSB) <ref type="bibr" target="#b3">[6]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.6.">Eitz et al.'s sketch recognition benchmark (2012)</head><p>Eitz et al. <ref type="bibr" target="#b15">[17]</ref> also built a sketch recognition benchmark which contains 20000 sketches, divided into 250 classes, each with 80 sketches. Currently, it is the most comprehensive sketch dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.7.">Comparison with our two benchmarks</head><p>The first three datasets or benchmarks cannot be used directly for our purpose while the fourth also has its limitations, such as the bias of different number of sketches per class and lacking of comprehensiveness. Considering these, we have built the SHREC'13 Sketch Track Benchmark using 7200 sketches selected from the large sketch collection presented in <ref type="bibr" target="#b15">[17]</ref> as query objects, and the SHREC'12 Sketch Track Benchmark which was extended from Yoon et al.'s benchmark. These two benchmarks either eliminate certain bias or add new evaluation datasets, thus are more comprehensive and objective when used to evaluate existing or newly developed sketch-based 3D model retrieval algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Benchmarks</head><p>In the SHREC'12 and SHREC'13 sketch tracks, we have built two sketch-based 3D model retrieval benchmarks, featuring smallscale and large-scale benchmarks, and sketches without and with internal features, respectively. In this section, we also introduce several evaluation metrics that are generally used to measure the retrieval performance of a sketch-based 3D model retrieval algorithm.</p><p>3.1. Small-scale benchmark: SHREC'12 Sketch Track Benchmark</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">3D target dataset</head><p>The 3D benchmark dataset is built based on the Watertight Model Benchmark (WMB) dataset <ref type="bibr" target="#b61">[63]</ref> which has 400 watertight models, divided into 20 classes, with 20 models each. The 3D target dataset contains two versions: Basic and Extended. The Basic version comprises 13 selected classes from the WMB dataset with each 20 models (in summary, 260 models). In the basic version, all 13 classes are considered relevant for the retrieval challenge. Fig. <ref type="figure" target="#fig_1">2</ref>(a) shows one typical example for each class of the basic benchmark. The Extended version adds to the basic version all remaining 7 classes of the WMB dataset (each 20 models). These additional classes, however, are not considered relevant for the retrieval challenge but added to increase the retrieval difficulty of the basic version. Fig. <ref type="figure" target="#fig_1">2(b)</ref> illustrates typical examples for these remaining 7 irrelevant classes. The extended version is utilized to test the scalability of a sketch-based retrieval algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">2D query set</head><p>The 2D query set comprises two subsets, falling into two different types.</p><p>Hand-drawn sketches. We utilize the hand-drawn sketch data compiled by TU Darmstadt and Fraunhofer IGD <ref type="bibr" target="#b7">[10]</ref>. It contains 250 hand-drawn sketches, divided into 13 classes. The query sketches were produced by a number of students asked to draw objects from the given categories without any further instructions. The sketches represent a spectrum of different sketching styles and qualities and are used to simulate retrieval by nonexpert users. They feature sketches with few internal feature lines. One typical example for each class is shown in Fig. <ref type="figure" target="#fig_1">2(c)</ref>. Standard line drawings. We also select 12 relevant sketches from the Snograss and Vanderwart's standard line drawings dataset <ref type="bibr" target="#b54">[56]</ref>. Note that just one sketch per query class is available in these drawings. Note that these queries are meant as a preliminary first step in eventually building a benchmark which controls for sketch standardization. Owing to their professional design quality the sketches can be considered representing ''ideal'' queries. Some examples are shown in Fig. <ref type="figure" target="#fig_1">2(d</ref>).</p><p>In the SHREC'12 sketch track, the two subsets were needed to be tested separately. However, users can also form a query set by combining these two to form a query set which contains diverse types of sketches.</p><p>3.2. Large-scale benchmark: SHREC'13 Sketch Track Benchmark</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Overview</head><p>Our large-scale sketch-based 3D model retrieval benchmark [2] is built on the latest large collection of human sketches collected by Eitz et al. <ref type="bibr" target="#b15">[17]</ref> and the well-known Princeton Shape Benchmark (PSB) <ref type="bibr" target="#b3">[6]</ref>. To explore how humans draw sketches and for the purpose of human sketch recognition using a crowdsourcing approach, they collected 20000 human-drawn sketches, categorized into 250 classes, each with 80 sketches. This sketch dataset is regarded as exhaustive in terms of the number of object categories. Furthermore, it represents a basis for a benchmark which can provide an equal and sufficiently large number of query objects per class, avoiding query class bias. In addition, the sketch variation within each class is high. Thus, we believe a new sketch-based 3D model retrieval benchmark built on <ref type="bibr" target="#b15">[17]</ref> and the PSB benchmark <ref type="bibr" target="#b3">[6]</ref> can foster the research of sketch-based 3D object retrieval methods. This benchmark presents a natural extension of the benchmark proposed in <ref type="bibr" target="#b2">[5]</ref> for very large-scale 3D sketch-based retrieval.</p><p>PSB is the most well-known and frequently used 3D shape benchmark and it also covers many commonly occurring objects. It contains two datasets: ''test'' and ''train'', each has 907 models, categorized into 92 and 90 distinct classes, respectively. Most of the 92 and 90 classes share the same categories with each other. However, PSB has quite different numbers of models for different classes, which is a ''target class'' bias for retrieval performance evaluation. For example, in the ''test'' dataset, the ''fighter_jet'' class has 50 models while the ''ant'' class only has 5 models. In <ref type="bibr" target="#b2">[5]</ref> the query sketch dataset and the target model dataset share the same distribution in terms of number of models in each class.</p><p>Considering the above fact and analysis, we build the benchmark (available in [2]) by finding common classes in both the sketch <ref type="bibr" target="#b15">[17]</ref> and the 3D model <ref type="bibr" target="#b3">[6]</ref> datasets. We search for the relevant 3D models (or classes) in PSB and the acceptance criterion is as follows: for each class in the sketch dataset, if we can find the relevant models and classes in PSB, we keep both sketches and models, otherwise we ignore both of them. In total, 90 of 250 classes, that is 7200 sketches, in the sketch dataset have 1258 relevant models in PSB. The benchmark is therefore composed of 7200 sketches and 1258 models, divided into 90 classes. Fig. <ref type="figure" target="#fig_2">3</ref> shows example sketches and their relevant models of 18 classes in the benchmark. We randomly select 50 sketches from each class for training and use the remaining 30 sketches per class for testing, while the 1258 relevant models as a whole are remained as the target dataset. The SHREC'13 sketch track participants need to submit results on the training and testing datasets, respectively. To provide a complete reference for the future users of our benchmark, we evaluate the contributed algorithms on both the testing dataset (30 sketches per class, totally 2700 sketches) and the complete benchmark (80 sketches per class, 7200 sketches).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">2D sketch dataset</head><p>The 2D sketch query set comprises the selected 7200 sketches (90 classes, each with 80 sketches), which have relevant models in PSB <ref type="bibr" target="#b3">[6]</ref>, from Eitz et al.'s <ref type="bibr" target="#b15">[17]</ref> human sketch recognition dataset. These sketches often contain internal feature lines. One example indicating the variations within one class is demonstrated in Fig. <ref type="figure" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">3D model dataset</head><p>The 3D model dataset is built on the PSB dataset <ref type="bibr" target="#b3">[6]</ref>. The target 3D model dataset comprises 1258 selected models distributed on 90 classes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation metrics</head><p>To have a comprehensive evaluation of a sketch-based 3D model retrieval algorithm based on the above two benchmarks, we employ seven commonly adopted performance metrics in Query-by-Model retrieval techniques. They are Precision-Recall plot (PR), Nearest Neighbor (NN), First Tier (FT), Second Tier (ST), E-Measures (E), Discounted Cumulated Gain (DCG) <ref type="bibr" target="#b3">[6]</ref> and Average Precision (AP) <ref type="bibr" target="#b62">[64]</ref>. We also have developed the code [1,2] to compute them for the two benchmarks. Their meaning and definitions are listed below.</p><p>Precision-Recall plot (PR): Precision measures the percentage of the relevant models in the top K (16 K 6 n) retrieval list, where n is the total number of models in the dataset. Recall calculates how much percentage of the relevant class in the database has been retrieved in the top K retrieval list. Nearest Neighbor (NN): NN is the precision of top 1 retrieval list. First Tier (FT): Assume there are C relevant models in the database, FT is the recall of the top C-1 retrieval list. Second Tier (ST): Similarly, ST is the recall of the top 2(C-1) retrieval list. E-Measure (E): E-Measure is motivated by the fact that people are more interested in the retrieval results in the first page. Thus, it is defined <ref type="bibr" target="#b3">[6]</ref> to measure the retrieval performance of the top 32 models in a retrieval list,</p><formula xml:id="formula_0">E ¼ 2 1 P þ 1 R :<label>ð1Þ</label></formula><p>Discounted Cumulated Gain (DCG): Since relevant models appear in the front of the retrieval list are more important than those in the rear of the list, DCG is defined as the normalized summed weighted value related to the positions of the relevant models. A retrieval list R is first transformed into a list G, where</p><formula xml:id="formula_1">G i ¼ 1 if R i is a relevant model, otherwise G i ¼ 0. DCG is then defined as follows. DCG i ¼ G 1 i ¼ 1 DCG iÀ1 þ G i lg 2 i otherwise (<label>ð2Þ</label></formula><formula xml:id="formula_2">Finally, it is normalized by the optimal DCG, DCG ¼ DCG n 1 þ P C j¼2 1 lg 2 j<label>ð3Þ</label></formula><p>where n is the total number of models in the dataset and C is the total number of relevant models in the class.</p><p>Average Precision (AP): AP is to measure the overall performance and it combines precision, recall as well as ranking positions. It can be computed by counting the total area under the Precision-Recall plot curve. A good AP needs both high recall and precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Contributors</head><p>The first four authors of this paper built the above two benchmarks, and organized the SHREC'12 and SHREC'13 tracks on the topic of sketch-based 3D retrieval and this follow-up study. Totally, 4 groups successfully contributed the following 15 methods (17 runs), including 4 top algorithms in the SHREC'12 and SHREC'13 sketch tracks (performance of other participating methods can be found in [1,4,2,7]) which are SBR-2D-3D, SBR-VC, BF-fDSIFT (a modified version of DSIFT) and FDC, as well as 11 additional state-of-the-art methods.</p><p>BF-fDSIFT, BF-fGALIF, BF-fGALIF + BF-fDSIFT; CDMR-BF-fDSIFT, CDMR-BF-fGALIF, CDMR-BF-fGALIF + CDMR-BF-fDSIFT; UMR-BF-fDSIFT, UMR-BF-fGALIF and UMR-BF-fGALIF + UMR-BF-fDSIFT submitted by Takahiko Furuya, Takahiro Matsuda, and Ryutarou Ohbuchi from the University of Yamanashi, Japan (Section 5.1). SBR-2D-3D_NUM_100, SBR-2D-3D_NUM_50, SBR-VC_NUM_100 and SBR-VC_NUM_50 submitted by Bo Li and Yijuan Lu from Texas State University, USA; and Henry Johan from Fraunhofer IDM@NTU, Singapore (Sections 5.2 and 5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Distance metric learning on Bag-of-Densely sampled local features</head><p>for sketch-based 3D shape retrieval <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b1">4]</ref>, by T. Furuya, T. Matsuda and R. Ohbuchi</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Method overview</head><p>To compare a hand-drawn sketch to a 3D model, most of existing methods compare a human-drawn 2D sketch with a set of multi-view rendered images of a 3D model. However, there is a gap between sketches and rendered images of 3D models. As human-drawn sketches contain stylistic variation, abstraction, inaccuracy and instability, these sketches are often dissimilar to rendered images of 3D models. The entries of their methods employ unsupervised distance metric learning to overcome this gap.</p><p>First approach, called Uniform Manifold Ranking, or UMR, is of unsupervised kind. It treats a feature extracted from a sketch and a feature (e.g., BF-GALIF <ref type="bibr" target="#b2">[5]</ref>) extracted from a view of a 3D model on the same ground. From a set of features, which include features of both sketches and 3D models, the UMR learns a graph structure or a Uniform Manifold (UM) that reflects low-dimensional structure of features. (It is called ''uniform'' as same feature extraction algorithm is used for both sketches and rendered images of 3D models, and these features are meshed into a single manifold graph.) Assuming N m 3D models rendered from N v viewpoints, and N s sketches, total of N s þ ðN m Â N v Þ features are connected to form the UM. Then diffusion distance from the feature of the sketch query to the features of multi-view renderings of 3D models are computed by using Manifold Ranking (MR) algorithm proposed by Zhou et al. <ref type="bibr" target="#b64">[66]</ref>.</p><p>In the experiments, they use either the BF-fGALIF, which is a modified version of BF-GALIF by Eitz et al. <ref type="bibr" target="#b2">[5]</ref> (Fig. <ref type="figure" target="#fig_5">5(a)</ref>), or the BF-fDSIFT, which is a regressed version of BF-DSIFT <ref type="bibr" target="#b1">[4]</ref> (Fig. <ref type="figure" target="#fig_5">5(b)</ref>), to form the UM.</p><p>Second approach, called Cross-Domain Manifold Ranking, or CDMR, may be either unsupervised, semi-supervised, or supervised <ref type="bibr" target="#b63">[65]</ref>. For the experiment described in this paper, they used the CDMR in the unsupervised mode. It is called cross-domain since it tries to bridge the gap between features extracted from two heterogeneous domains, i.e., hand-drawn sketch images and multiview rendered images of 3D models. Unlike UMR, which forms a manifold of features by using single feature, CDMR allows for the use of multiple measures of similarities, both feature-based and semantic-label based, to form an integrated Cross Domain Manifold (CDM) that spans heterogeneous domains (See Fig. <ref type="figure" target="#fig_6">6</ref>). A set of sketch images are formed into manifold of sketch images by using a feature (e.g., BF-GALIF <ref type="bibr" target="#b2">[5]</ref>) optimal for sketch-to-sketch comparison. Another manifold that connects 3D models are formed by using a feature (e.g., BF-DSIFT <ref type="bibr" target="#b50">[52]</ref>) that is optimal for comparison among 3D models. These two manifolds are then cross-linked by using a feature that is adept at comparing a sketch image to a view of 3D model, e.g., <ref type="bibr" target="#b2">[5]</ref>. The CDM is a graph containing N s þ N m vertices. Additionally, if available, class labels may also be used for cross-linking the domains. Semantic labels help significantly if a sketch (e.g., a stick figure human) is dissimilar to multi-view renderings of 3D models (e.g., a realistic 3D model of human). Similarity from a sketch query to a 3D model is computed by using MR <ref type="bibr" target="#b64">[66]</ref> algorithm, as is the case with the UMR. The diffusion of relevance value originates from the query, and spreads via edges of the CDM to 3D models. The diffusion occurs among 2D sketches, from 2D sketches to 3D models across the domain, and among 3D models. Note that, if a corpus of sketches is available in the database, the CDMR automatically performs a form of query expansion. The relevance is first diffused from the query to its neighboring sketches. Then, these sketches (expanded query set) behave as multiple secondary sources of diffusion.</p><p>For CDMR, they use two different features fit for the purposes to form subparts of the CDM. To form a manifold of 2D sketches, they use BF-fGALIF. To form a manifold of 3D models, they use BF-DSIFT <ref type="bibr" target="#b50">[52]</ref>. To link sketches with 3D models using feature similarity, they use either BF-fGALIF or BF-fDSIFT (Fig. <ref type="figure" target="#fig_5">5</ref>).  Descriptions of the BF-fGALIF and BF-fDSIFT features will be presented in the next section. For the BF-DSIFT, please refer to the original paper by Furuya et al. <ref type="bibr" target="#b50">[52]</ref>. They then briefly describe the UMR and the CDMR <ref type="bibr" target="#b63">[65]</ref> algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Features used in the experiments</head><p>5.1.2.1. BF-fGALIF. BF-GALIF <ref type="bibr" target="#b2">[5]</ref> proposed by Eitz et al. is designed for sketch-based 3D model retrieval. Their variation called BF-fGALIF (Fig. <ref type="figure" target="#fig_5">5(a)</ref>) is similar but not identical to the original.</p><p>For each 3D model, the model is rendered into Suggestive Contour (SC) <ref type="bibr" target="#b10">[12]</ref> images from multiple viewpoints and a set of fGALIF features is computed for each view. They use 42 viewpoints spaced uniformly in solid angle and image resolution of 256 Â 256 pixels. Unlike original GALIF, their fGALIF uses black background for the SC images (see Fig. <ref type="figure" target="#fig_5">5(a)</ref>).</p><p>Each rendered image is then normalized for rotation. To do so, they exploit response images produced by Gabor filtering on the rendered image. Gabor filter captures orientation of lines and intensity gradient in the image. For each pixel in the response image, a response vector is calculated according to the direction of Gabor filter and response magnitude at the pixel. The response vectors calculated at all the pixels in the image are voted against a histogram. The histogram comprises 18 orientation bins and voting is done according to orientation and magnitude of the response vectors. After voting, the image is rotated to the direction of the most populated orientation bin.</p><p>After normalizing for the rotation, fGALIF features are extracted densely at regular grid points on the image. They extract 1024 fGA-LIF features per image. Bandwidth and other parameters for the Gabor filter are determined through preliminary experiments so that the retrieval accuracy is the highest among the combinations of parameters they tried.</p><p>For each sketch image, fGALIF features are computed after the image is resized to 256 Â 256 pixels. Computation of fGALIF is carried out in the same manner as for a sketch image and for a rotation-normalized SC image of a 3D model.</p><p>The set of 1024 fGALIF features extracted from an image is integrated into a BF-fGALIF feature vector per image by using a standard Bag-of-Features (BF) approach. This integration reduces cost of image-to-image matching significantly compared with directly comparing a set of features to another set of features. They used vocabulary size of 2500. They used k-means clustering to learn the vocabulary, and used kd-tree to accelerate vector quantization of fGALIF features into words of the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.2.">BF-fDSIFT.</head><p>Original Bag-of-Features Dense SIFT (BF-DSIFT) <ref type="bibr" target="#b50">[52]</ref> computes a feature per 3D model for comparison among 3D models. Here, they use a variant of it, called BF-fDSIFT <ref type="bibr" target="#b1">[4]</ref> (Fig. <ref type="figure" target="#fig_5">5(b)</ref>) to compare a sketch image with multiple images rendered from multiple views of a 3D model.</p><p>The BF-fDSIFT turns both sketch and 3D model into silhouette images for comparison. To turn a line-drawing sketch with possible gaps in its circumference into a silhouette, dilation operation to close the gaps is followed by area filling. Some of the sketches fail to become silhouettes, but they tolerate them. To turn a 3D model into a set of silhouette images, it is rendered from 42 viewpoints into silhouettes of 256 Â 256 pixels each.</p><p>On each silhouette image, SIFT <ref type="bibr" target="#b48">[50]</ref> features are densely and randomly sampled. They extract 1200 SIFT features per image. There is no need to normalize images for rotation as SIFT is inherently invariant (to some extent) to rotation, translation, and scaling. The set of 1200 SIFT features extracted from an image is integrated into a BF-fDSIFT feature vector per image by using the BF approach. They used vocabulary size of about 10000. They use ERC-Tree <ref type="bibr" target="#b65">[67]</ref> algorithm to accelerate both vocabulary learning (clustering) and vector quantization of SIFT features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Ranking retrieval results</head><p>For the experiments, similarity ranking of retrieval results are performed by using three different algorithms; fixed distance, the UMR, and the CDMR.</p><p>5.1.3.1. Fixed distance. Symmetric version of Kullback-Leibler Divergence (KLD) is used as fixed distance metric between a pair of BF features. KLD performs well when comparing a pair of probability distributions, i.e., histograms. Distance between a sketch and a 3D model is the minimum of the 42 distances computed from a BF feature of the sketch and a set of 42 BF features of the 3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.2.">Uniform Manifold Ranking (UMR).</head><p>Input of the UMR is the BF-fGALIF or the BF-fDSIFT features of the sketches and the rendered images of 3D models. A graph is represented as a sparse matrix</p><formula xml:id="formula_3">W of size ðN s þ N m Á N v Þ Â ðN s þ N m Á N v Þ</formula><p>where N s and N m are the number of sketches and 3D models in a database respectively, and N v is the number of views for rendering (i.e., N v ¼ 42). The similarity between vertices i and j is computed by Eq. (4) where d (x i ; x j ) is KLD between feature vectors x i and x j , and kNN</p><formula xml:id="formula_4">(x i ) is a set of k-nearest neighbors of x i . W ij ¼ exp À dðx i ;x j Þ 2 r if x j 2 kNNðx i Þ 0 otherwise 8 &lt; :<label>ð4Þ</label></formula><p>They normalize W for S,</p><formula xml:id="formula_5">S ¼ D À 1 2 WD À 1 2<label>ð5Þ</label></formula><p>where D is a diagonal matrix whose diagonal element is</p><formula xml:id="formula_6">D ij ¼ P j W ij .</formula><p>They use the following iterative form of the MR to find relevance values in F given initial value, or ''source'' matrix Y. A higher relevance means a smaller distance.</p><formula xml:id="formula_7">F tþ1 ¼ aSF t þ ð1 À aÞY ð6Þ Y is a diagonal matrix of size ðN s þ N m Á N v Þ Â ðN s þ N m Á N v Þ that de- fines source(s) of relevance value diffusion. If a vertex i is the source of diffusion Y ii ¼ 1 and, if not, Y ii ¼ 0.</formula><p>In their case, the vertex corresponding to the query sketch becomes the source of diffusion. F ij is the relevance score of the rendered image j given the sketch i. Hence -F ij is the adaptive distance derived from the MR. Final relevance score between the sketch and the 3D model is the maximum of the 42 scores computed between the sketch and a set of 42 rendered images of the 3D model.</p><p>They add prefix ''UMR-'' before the feature extraction method (e.g., UMR-BF-fGALIF or UMR-BF-fDSIFT) to indicate UMR processed algorithms. Parameters for the UMR (i.e., k; r; a) are determined through preliminary experiments. Table <ref type="table" target="#tab_0">1</ref> shows combination of the parameters for the UMR-BF-fGALIF and the UMR-BF-fDSIFT.</p><p>To further improve retrieval accuracy, they experimented with combining the BF-fGALIF and BF-fDSIFT features via a late-fusion approach, that is, simply adds distances due to BF-fGALIF and BF-fDSIFT. Here, each distance may be treated with UMR or not. They denote the combination of fixed distances by ''BF-fGALIF + BF-fDSIFT'' and the combination of adaptive distances by ''UMR-BF-fGALIF + UMR-BF-fDSIFT''.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.3.">Cross Domain Manifold Ranking (CDMR).</head><p>The CDM graph is represented as a matrix W CDM whose vertices are the features from the sketch domain and the 3D model domain.</p><formula xml:id="formula_8">W CDM is size of ðN s þ N m Þ Â ðN s þ N m Þ</formula><p>where N s and N m are the number of sketches and 3D models in a database respectively.</p><formula xml:id="formula_9">W CDM ¼ W SS W SM W MS W MM<label>ð7Þ</label></formula><p>The submatrix W SS having size N s Â N s is the manifold of sketch features. Similarity between a pair of sketches is computed by using the BF-fGALIF. For sketch-to-sketch feature comparison, they do not normalize for image rotation as most sketches drawn by human are already aligned to a canonical orientation. The submatrix W MM having size N M Â N M is a manifold of features of 3D models. Similarity between a pair of 3D models is computed by using their 3D model-to-3D model comparison method BF-DSIFT <ref type="bibr" target="#b50">[52]</ref>. The submatrix W SM of size N S Â N M couples two submanifolds W MM and W SS that lie in different domains, that are, sketch feature domain and 3D model feature domain. Similarity between a pair of a sketch and a 3D model is computed by using the BF-fGALIF or the BF-fDSIFT described in Section 5.1.2. The submatrix W MS of size N M Â N S is a zero matrix as they assume no diffusion of similarity occurs from 3D models to sketches.</p><p>For each submatrix, the similarity between vertices i and j is computed by Eq. ( <ref type="formula" target="#formula_10">8</ref>) where dðx i ; x j Þ is KLD between feature vectors x i and x j . The parameter r controls diffusion of relevance value across the CDM. They use different values r SS ; r MM , and r SM for each of the submatrices W SS ; W MM , and W SM .</p><formula xml:id="formula_10">W ij ¼ exp À dðx i ;x j Þ r if i -j 0 otherwise (<label>ð8Þ</label></formula><p>After generating the CDM graph W CDM , the MR is applied on W CDM to diffuse relevance value from the sketch query to the 3D models over the CDM across the domain boundary.</p><p>They normalize W CDM for S CDM by Eq. ( <ref type="formula" target="#formula_5">5</ref>). They use the following closed form of the MR to find relevance values in F given source matrix Y. F ij is the relevance value of the 3D model j given the sketch i. A higher relevance means a smaller distance.</p><formula xml:id="formula_11">F ¼ ðI À aS CDM Þ À1 Y<label>ð9Þ</label></formula><p>They add prefix ''CDMR-'' before the feature comparison method used for computing W SM (e.g., CDMR-BF-fGALIF or CDMR-BF-fDSIFT) to indicate CDMR processed algorithms. Parameters for the CDMR (i.e., r SS ; r MM ; r SM and a) are determined through preliminary experiments. Table <ref type="table" target="#tab_1">2</ref> summarizes combination of the parameters for the CDMR-BF-fGALIF and the CDMR-BF-fDSIFT.</p><p>They also experimented with combining the CDMR-BF-fGALIF and CDMR-BF-fDSIFT. They employ a simple late-fusion approach identical to the one used for the UMR. They denote the combination of relevance values derived from the two features as ''CDMR-BF-fGALIF + CDMR-BF-fDSIFT''. 5.2. Sketch-based 3D model retrieval based on 2D-3D alignment and shape context matching (SBR-2D-3D) <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b1">4,</ref><ref type="bibr" target="#b4">7]</ref>, by B. Li, Y. Lu and H. Johan</p><p>The main idea of the sketch-based retrieval algorithm proposed in <ref type="bibr" target="#b17">[19]</ref> is that they want to maximize the chances that they have selected the most similar or optimal corresponding views for computing the distances between a 2D sketch and a set of selected sample views of a 3D model, while not adding additional online computation and avoiding the brute-force comparison between the sketch and many sample views of the model. They implemented the idea by utilizing a 3D model feature named View Context <ref type="bibr" target="#b18">[20]</ref>, which has a capability of differentiating different sample views of a 3D model. The candidate views selection rule is as follows: a sample view is replaced with the sketch and if its new View Context is very similar to the original one, then it is regarded as a candidate view. During online retrieval, for each 3D model, a set of candidate views are efficiently shortlisted in the 2D-3D alignment according to their top View Context similarities as that of the sketch. Finally, a more accurate shape context matching <ref type="bibr" target="#b19">[21]</ref> algorithm is employed to compute the distances between the query sketch and the candidate sample views. The algorithm is composed of precomputation and online retrieval stages, which are illustrated in Fig. <ref type="figure" target="#fig_8">7</ref>. Some important details and modifications about the algorithm are first given below.</p><p>Silhouette and outline feature views are respectively selected for View Context feature extraction and shape context-based 2D-3D matching. Two sets of examples are shown in Fig. <ref type="figure" target="#fig_9">8</ref>. For a query sketch, a silhouette feature view is generated based on the following six steps: binarization, Canny edge detection, morphological closing (infinite times, which means repeating until the image does not change), and filling holes, inversion and resizing into a 256 Â 256 image. The corresponding outline feature view is very easy to obtain based on the silhouette feature view. An integrated image descriptor, which contains region, contour, and geometrical information of the silhouette and outline feature views, is utilized to compute View Context. Considering the large-scale retrieval scenario, to reduce computational cost, they set the number of sample points to represent a contour feature view to 50 and only keep the top 4 candidate views during 2D-3D alignment. On the other hand, to save the memory needed to load the shape context features during online retrieval, they use the short integers to code the locations of the 5 Â 12 bins and values during the loading of the precomputed shape context features.</p><p>For clarity, the main steps of the algorithm are further described as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Feature extraction</head><p>Silhouette and outline feature views are generated for both 2D sketches and 3D models to effectively and efficiently measure the differences among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Feature distance</head><p>A computationally efficient integrated image descriptor named ZFEC is adopted for View Context computation. It contains a region-based Zernike moments feature Z for the silhouette view and a contour-based Fourier descriptor feature F for the outline view. Additionally, eccentricity feature E and circularity feature C are also utilized to extract the geometric feature of the outline view. To more accurately measure the difference between the sketch and each candidate view, the relative shape context matching method <ref type="bibr" target="#b19">[21]</ref> is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Sketch's View Context feature extraction</head><p>The integrated image descriptor distances between the sketch and all the base views of the target model are computed and the resulting distance vector D k ¼ hd 1 ; d 2 ; . . . ; d m i is named sketch's View Context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">2D-3D alignment</head><p>To align the 2D sketch and a 3D model, some candidate views are short listed by keeping a certain percentage (e.g. 20% or 16 sample views for the track) of the sample views with top View Context similarities as the sketch, in terms of correlation similarity S i ,</p><formula xml:id="formula_12">S i ¼ D s i Á D k D s i D k ;<label>ð10Þ</label></formula><p>where D s i and D k are the View Contexts of the i th sample view V s i of the 3D model and the 2D sketch, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.">Sketch-Model distance computation</head><p>Comparing the sketch with every candidate outline view using the relative shape context matching and regarding the minimum relative shape context distance obtained as the sketch-model distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6.">Ranking and output</head><p>Sorting all the sketch-model distances between the sketch and the models in an ascending order and listing the retrieved models accordingly.</p><p>The two runs, SBR-2D-3D_NUM_100 (for small-scale benchmark, Section 6.1) and SBR-VC_NUM_50 (for large-scale benchmark, Section 6.2), are two variations of the original SBR-2D-3D by setting the number of sample points for the contour(s) of each sketch, referred to as NUM, to 100 and 50, respectively. Then, a Fuzzy C-Means view clustering is performed on the sample views based on their viewpoint entropy values and viewpoint locations. Finally, shape context matching <ref type="bibr" target="#b19">[21]</ref> is utilized during online retrieval for the matching between a query sketch and the representative views for each target model. The retrieval algorithm comprises precomputation and online retrieval stages. An overview of the algorithm is shown in Fig. <ref type="figure" target="#fig_10">9</ref>.</p><p>The key component of the retrieval algorithm is viewpoint entropy-based adaptive view clustering, which comprises the following three steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Viewpoint entropy distribution</head><p>For each model, they sample a set of viewpoints by setting the cameras on the vertices of a subdivided icosahedron L n obtained by n times Loop subdivision on a regular icosahedron L 0 . Viewpoint entropy distributions of three models utilizing L 3 for view sampling are demonstrated in Fig. <ref type="figure" target="#fig_11">10</ref>. It can be seen that for a 3D model, the complexity of its entropy distribution pattern is highly related to the complexity of its geometry. For instance, the two  complex models, horse and Lucy, have a more complicated pattern than the relatively simpler model fish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Viewpoint entropy-based 3D visual complexity</head><p>The visual complexity metric is defined based on a class-level entropy distribution analysis on a 3D dataset. Mean and standard deviation entropy values m and s among all the sample views of a 3D model are first computed, followed by an average over all the models for each class. 3D visual complexity C is defined as</p><formula xml:id="formula_13">C ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ŝ2 þ m2 p</formula><p>, where ŝ and m are the normalized s and m by their respective maximums over all the classes. The metric is capable of reasonably reflecting the semantic distances among different classes of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Viewpoint entropy-based adaptive views clustering</head><p>Utilizing the visual complexity value C of a model, the number of representative outline feature views N c is adaptively assigned:</p><formula xml:id="formula_14">N c ¼ C</formula><p>2 Á N 0 (for small-scale benchmark, Section 6.1) or N c ¼ C 6 Á N 0 (for large-scale benchmark, Section 6.2), where N 0 is the total number of sample views and it is set to 81 in the algorithm. To speed up the retrieval process on the large-scale benchmark in Section 6.2, they choose the parameter setting of 1 6 , compared with the selection of 1 2 in the originally proposed algorithm. Finally, a Fuzzy C-Means view clustering is performed to obtain the representative views.</p><p>The two runs, SBR-VC_NUM_50 and SBR-VC_NUM_100, are two variations of the original SBR-VC by setting the number of sample points for the contour(s) of each sketch, referred to as NUM, to 50 and 100, respectively. 5.4. Hierarchical Topology 3D Descriptor (HTD) for sketch-based 3D shape retrieval <ref type="bibr" target="#b67">[69,</ref><ref type="bibr" target="#b68">70]</ref>, by P.B. Pascoal, A. Ferreira and M.J. Fonseca</p><p>In order to compare a 3D object with a 2D sketch, it is required to extract 2D views of the object. In the first step, the 3D object is rendered using the Cel-shading technique so that the object is drawn with a black outline and interior contour lines.</p><p>The black outline is drawn slightly larger than the object itself, and then using backface culling, back-facing triangles are hidden due to rendering the object as solid-filled. Afterwards, they extract 20 views of the 3D model from different camera positions. This approach is based on the method presented by Chen et al. <ref type="bibr" target="#b69">[71]</ref>, but instead of using 10 silhouettes they use all the 20 positions of the dodecahedron to get all possible sketches. An overview of the Hierarchical Topology 3D Descriptor process is demonstrated in Fig. <ref type="figure" target="#fig_0">11</ref>.</p><p>For each image, they then use an algorithm proposed by Ferreira et al. <ref type="bibr" target="#b67">[69]</ref> that detects polygons defined by a set of line segments and saves them in a vector format image. The algorithm can be summarized in 4 major steps, as illustrated in Fig. <ref type="figure" target="#fig_13">12</ref>.  First, it detects the line segment intersections using the Bentley-Ottmann algorithm <ref type="bibr" target="#b70">[72]</ref>. Then, creates a graph induced by the drawing, where vertices represent endpoints or proper intersection points of line segments and edges represent maximal relatively open subsegments that contain no vertices. The third step finds the Minimum Cycle Basis (MCB) <ref type="bibr" target="#b71">[73]</ref> of the graph induced in the previous step, using the algorithm proposed by Horton <ref type="bibr" target="#b72">[74]</ref>. Last step constructs a set of polygons based on cycles in the previously found MCB. This is straight-forward if we transform each cycle into a polygon, where each vertex in the cycle represents a vertex in the polygon and each edge in the cycle represents an edge in the polygon.</p><p>Finally, for classification, they used a method proposed by Sousa and Fonseca <ref type="bibr" target="#b68">[70]</ref> which uses a graph-based technique to describe the spatial arrangement of drawing components, combined with geometric information.</p><p>Their process starts by applying a simplification step, to remove small visual details while retaining dominant shapes in a drawing. After simplification, they identify visual elements, namely polygons and lines, and extract geometric and topological information from drawings.</p><p>The topology is simplified into the eight topological relationships defined by Egenhofer and Al-Taha <ref type="bibr" target="#b73">[75]</ref> (Disjoint, Meet, Overlap, Contain, Inside, Cover, Covered-By and Equal), starting from their neighborhood graph for topological relationships. This graph has a well-defined structure, with a root node representing the whole drawing and each next level of the graph describing polygons contained in the blocks identified before, adding more drawing details. Therefore, by going down in the depth of the graph, we are ''zooming in'' in drawing details as illustrated in Fig. <ref type="figure" target="#fig_14">13</ref>.</p><p>The resulting descriptor is a multidimensional vector, whose size depends on graph complexity. Very complex drawings will yield descriptors with higher dimensions, while simple drawings will result in descriptors with lower sizes. To solve this issue, they use the graph spectra to convert graphs into feature vectors, solving the problem of isomorphism between topology graphs to the much simpler computation of distances between descriptors. To generate the graph spectrum, firstly it creates the adjacency matrix of the graph; secondly calculates its eigenvalues; and finally sorts the absolute values to obtain the topology descriptor as shown in Fig. <ref type="figure" target="#fig_4">14</ref>.</p><p>As for the geometric information, it uses a general shape recognition library called CALI <ref type="bibr" target="#b74">[76]</ref>. By applying this method to each geometric entity in the figure, it provides complete description of the geometry of a drawing. The geometry and topology descriptors thus computed are combined and used as the descriptor for the respective image.   on computing an external contour (Silhouette) of 3D models from a defined number of viewpoints. After this stage, they compute a set of low-level features for each contour. Finally, they apply a similarity search to get a ranking under an input sketch. These involved processes lead to a framework composed of three stages: (1) Preprocessing, (2) Feature extraction, and (3) Similarity search. They describe each one of these stages as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1.">Pre-processing</head><p>They divide this stage into two sub-stages aiming to pre-process a 3D model and pre-process an input sketch. The goal of both sub-stages is to obtain a simple representation that allows the next stage to compute a low-level feature in an easier way.</p><p>5.5.1.1. 3D model pre-processing. They compute 2D projections from each 3D model using six defined viewpoints (top, bottom, right, left, back, front). The external contour of each projection is then extracted discarding internal holes in the underlying image. An example of this representation is shown in Fig. <ref type="figure" target="#fig_5">15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1.2.">Sketch pre-processing.</head><p>Considering that a sketch is commonly drawn roughly, which produces disconnected strokes, their first stage is to connect all the strokes using a sequence of morphological dilation operations. After that, they extract the external contour of each sketch. Similar to the 3D model pre-processing, internal holes of sketches will be discarded. An example of their results in this stage is presented in Fig. <ref type="figure" target="#fig_16">16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2.">Feature extraction</head><p>They take the result produced by the previous stage to compute low-level features. In this stage, they propose to use three feature extraction methods: Histogram of Edge Local Orientations (HELO) proposed by Saavedra et al. <ref type="bibr" target="#b60">[62]</ref>, Histogram of Oriented Gradients (HOG) proposed by Dalal et al. <ref type="bibr" target="#b52">[54]</ref>, and a Fourier descriptor using the approach presented by Zhang et al. <ref type="bibr" target="#b75">[77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2.1.">Histogram of Edge Local Orientations based on Silhouettes (HELO-SIL)</head><p>. HELO, proposed by Saavedra et al. <ref type="bibr" target="#b60">[62]</ref>, is a method for computing a histogram of edge orientations in the context of sketch-based image retrieval. HELO computes a K-bin histogram based on local edge orientations. To get the HELO feature vector, the sketch is divided into a W Â W grid. Then, an edge orientation is estimated for each cell in the grid. Compared with other orientation-based approaches, the main difference is that HELO estimates a representative orientation by computing squared gradients inside a cell.</p><p>Let ½G x ; G y T be the corresponding gradient vector for a pixel ðx; yÞ in a sketch image. The squared gradient is computed for each pixel doubling its gradient angle and squaring its gradient length. To this end, the gradient vector is represented in its corresponding polar coordinates. The squared gradient ½G sx ; G sy T corresponding to the gradient ½G x ; G y T is computed as follows,</p><formula xml:id="formula_15">G sx G sy ¼ G 2 x À G 2 y 2G x Á G y " # :<label>ð11Þ</label></formula><p>The estimated orientation for a cell is the orientation of the average squared gradient ½G sx ; G sy computed for the cell. Finally, a K-bin histogram is computed using the angles of all average squared gradients for each cell.</p><p>In order to exploit local information of sketches, they divide the sketch image into 6 Â 6 blocks, and compute a 36-bin HELO descriptor for each block. Then, they concatenate all the local HELO descriptors to get the final feature vector. In addition, they set W ¼ 25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2.2.">Histogram of Oriented Gradients based on Silhouettes (HOG-SIL).</head><p>The HOG <ref type="bibr" target="#b52">[54]</ref> approach allows us to compute a histogram of gradient orientations by concatenating local orientation histograms computed in small regions. This approach divides the sketch image into C Â C-size cells. For each cell a K-bin histogram of orientation is computed. In order to increase the robustness of the descriptor, each local histogram is then normalized using information of neighbor histograms. To this end, blocks of B Â B cells are formed. In this point, it is possible to form blocks with or without overlapping.</p><p>In their experimental evaluation, they set C ¼ 16, B ¼ 3, K ¼ 9, and they use the non-overlapping block approach. In addition, since the size of the descriptor depends on the size of the input image, they resize all the silhouette images, obtained from the 2D projections as well as from input sketches, to 100 Â 100 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2.3.">Fourier Descriptors on 3D models Silhouettes (FDC).</head><p>Let X be the set of x-coordinates and Y be the set of y-coordinates of a boundary. The Fourier descriptor is computed over the centroid distance C which is obtained as follows,</p><formula xml:id="formula_16">C ¼ dist E ðX À x c ; Y À y c Þ;<label>ð12Þ</label></formula><p>where ðx c ; y c Þ is the centroid of the shape and dist E is the Euclidean distance. To deal with different boundary sizes, they sample 128  points using the equal arc-length sampling as suggested by Zhang et al. <ref type="bibr" target="#b40">[42]</ref>.</p><p>Next, they apply a Fourier Transform on the sample set. Let F be the computed Fourier descriptor with 256 entries. To deal with the rotation invariance issue, they use the magnitude of the Fourier descriptor. In addition, considering that the sample set is composed of real values, they take only half of the Fourier entries. Finally, to deal with different scales, the descriptor is normalized with respect to F 0 , so the final descriptor is given by, FD ¼ jF 1 j jF 0 j ; jF 2 j jF 0 j ; . . . ; jF 128 j jF 0 j : ð13Þ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3.">Similarity search</head><p>To improve the efficiency of the searching, they use a KD-Tree index together with the Manhattan distance to search all the database of 3D models. For indexing and retrieving, they use the FLANN (Fast Library for Approximate Nearest Neighbors) implementation provided by Muja et al. <ref type="bibr" target="#b76">[78]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Small-scale benchmark: SHREC'12 Sketch Track Benchmark</head><p>In this section, we perform a comparative evaluation of the results of the 14 runs submitted by 3 of the 4 groups on SHREC'12 Sketch Track Benchmark (Pascoal's results are not available on this benchmark; Li's SBR-VC and SBR-2D-3D select NUM = 100 only). We measure retrieval performance based on the 7 metrics mentioned in Section 3.3: PR; NN; FT; ST; E; DCG and AP. In addition, we also compare their scalability and efficiency.</p><p>As described in Section 3.1, there are two versions of target dataset (Basic and Extended) as well as two types of sketch datasets (hand-drawn sketches and standard line drawings). This results in four combinations:  <ref type="table" target="#tab_2">3</ref><ref type="table" target="#tab_3">4</ref><ref type="table" target="#tab_4">5</ref><ref type="table" target="#tab_5">6</ref>.</p><p>First, we start with the overall performance evaluation. As shown in the aforementioned figures and tables, Furuya's CDMR-BF-fGALIF + CDMR-BF-fDSIFT performs best, closely followed by CDMR-BF-fGALIF. Then, CDMR-BF-fDSIFT, UMR-BF-fGALIF + UMR-BF-fDSIFT, UMR-BF-fGALIF, SBR-VC_NUM_100 and SBR-2D-3D_NUM_100 succeed. Performance of the remaining three methods is comparable and the disparity among them is relatively small. In a word, these runs can be ranked in several groups according to the overall performance and we also provide this ranking information (''R'') in the following Tables <ref type="table" target="#tab_2">3</ref><ref type="table" target="#tab_3">4</ref><ref type="table" target="#tab_4">5</ref><ref type="table" target="#tab_5">6</ref><ref type="table" target="#tab_6">7</ref><ref type="table" target="#tab_7">8</ref><ref type="table" target="#tab_8">9</ref><ref type="table" target="#tab_9">10</ref><ref type="table" target="#tab_10">11</ref><ref type="table" target="#tab_11">12</ref>. If we consider non-machine learning approaches, Li's SBR-VC and SBR-2D-3D perform best and they outperform either the GALIF or the DSIFT feature-based methods and we can expect better performance if we apply ''CDMR'' on them.</p><p>Second, we look into different types of queries. Compared with hand-drawn sketch queries, standard line drawing queries usually achieve superior performance. One possible explanation for this is that this dataset only contains a single line drawing per class, which has been carefully created to convey shape as well as salient features of that class.</p><p>Third, we asked contributors to also provide timing information, together with their hardware and software configurations, to compare runtime requirements of their methods, based on the second case (hand-drawn sketch queries and extended target dataset). Table <ref type="table" target="#tab_7">8</ref> lists the timing information in seconds and comparison results. In this distributed evaluation, it was not possible to control for the hardware platform (roughly comparable for all groups, though) or implementation efficiency of the setups. However, we believe that the timing information is useful for an approximate comparison of the runtime requirements of the algorithms.</p><p>Last but not least, we evaluate the scalability of the different methods to irrelevant models (Section 3.1.1) in the dataset. Table <ref type="table" target="#tab_6">7</ref> lists the percentage of performance decreases when using the extended target dataset instead of the basic one (using hand-drawn sketch queries). Similarly, we also list the ranking information in terms of scalability. Furuya's CDMR-BF-fGALIF has best scalability, closely followed by CDMR-BF-fGALIF + CDMR-BF-fDSIFT. In the Rank 2, UMR-BF-fGALIF, SBR-2D-3D_NUM_100, UMR-BF-fGA-LIF + UMR-BF-fDSIFT and SBR-VC_NUM_100 share similar scalability. Compared with the above six approaches, the other three methods exhibit a stronger decrease in retrieval performance when adding 140 irrelevant models to the target dataset. Please also note that compared with fGALIF, fDSIFT often has more decreases in retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Large-scale benchmark: SHREC'13 Sketch Track Benchmark</head><p>Similarly, a comparative evaluation of 16 runs (Li's SBR-2D-3D selects NUM = 50 only) of all the 15 contributed methods has been performed on the latest large-scale benchmark of SHREC'13 Sketch Track Benchmark. As described in Section 3.2, the complete query sketch dataset is divided into ''Training'' and ''Testing'' datasets, which is to accustom to machine learning-based retrieval algorithms. To provide complete reference performance data for both non-learning based and learning-based approaches, such as the contributed ''CDMR-'' and ''UMR-'' methods, we evaluate the submitted results on both ''Training'' and ''Testing'' datasets, as well as the complete sketch dataset. Fig. <ref type="figure" target="#fig_9">18</ref> and Tables 9-11 compare the contributed methods in terms of the 7 performance metrics on the above three datasets, respectively.</p><p>Similar as the results on the small-scale SHREC'12 Sketch Track Benchmark, the aforementioned figure and table show that Furuya's CDMR-BF-fGALIF + CDMR-BF-fDSIFT performs best, closely followed by their CDMR-BF-fGALIF and CDMR-BF-fDSIFT. Then, UMR-BF-fGALIF + UMR-BF-fDSIFT, BF-fGALIF + BF-fDSIFT, UMR-BF-fDSIFT and UMR-BF-fGALIF succeed, followed by the four methods of BF-fDSIFT, BF-fGALIF, SBR-VC and SBR-2D-3D. HOG-SIL, HELO-SIL and FDC have comparatively inferior performance, but they outperform HTD. We also can find that ''CDMR-'' based approaches often consistently (either in small-scale or large-scale retrieval scenarios) achieve better performance than their ''UMR-'' based counterparts. Similarly, if we only compare non-machine learning approaches, Li's SBR-VC_NUM_100 is comparable to BF-fGALIF and BF-fDSIFT, which are outperformed by their combinational one. SBR-2D-3D_NUM_50 is also comparable to SBR-VC_NUM_50.</p><p>However, when compared with the performance obtained on the SHREC'12 Sketch Track Benchmark which employed a much smaller benchmark, the performance of all the methods is much less successful. For example, even for the best-performing approach of CDMR-BF-fGALIF + CDMR-BF-fDSIFT, the decreases (comparing the performance on the ''Complete'' benchmark of SHREC'13 Sketch Track Benchmark and the ''Hand-drawn and Basic'' benchmark of SHREC'12 Sketch Track Benchmark) are 61.2%, 68.2%, 62.9%, 71.3%, 44.9% and 63.7% in NN, FT, ST, E, DCG and AP, respectively. This finding is worth noting because it evidently raises the issue of the scalability in the case of large-scale sketch-based model retrieval.</p><p>remaining bias deserves our further improvement, such as making each class contain the same number of 3D models by adding more models from other 3D model benchmarks.</p><p>Finally, we have a similar approximate efficiency performance comparison on this benchmark, as shown in Table <ref type="table" target="#tab_11">12</ref>. Obviously, Saavedra's three methods are still the most efficient, followed by Furuya's BF-fGALIF and BF-fDSIFT. Li's two methods still have the same ranks. However, Furuya's two ''CDMR-'' approaches drop from rank 2 to rank 4 while their two ''UMR-'' methods also drop 1 or 2 places. We compare the changes in timing information between the large-scale SHREC'13 Sketch Track Benchmark (1258 models) and the small-scale SHREC'12 Sketch Track Benchmark (400 models) in Fig. <ref type="figure" target="#fig_1">20</ref>. From the numbers, we also can find that the response time of ''CDMR-'' and ''UMR-'' approaches increase a lot with the increase of number of target 3D models. With the scale of the target database is increased by 3.2 times from 400 to 1258 models, the response time of CDMR-BF-fDSIFT, CDMR-BF-fGALIF, UMR-BF-fDSIFT and UMR-BF-fGALIF is correspondingly increased by about 730.3, 1230.9, 173.2, and 247.0 times; while it is 1.0 and 0.8 for BF-fDSIFT, BF-fGALIF, and 3.5 times for HOG-SIL, HELO-SIL and FDC, respectively. While, for SBR-VC and SBR-2D-3D, the response time has been reduced 10.7% and 75.4% respectively by selecting fewer representative views or sample points. In a word, the above best-performing approaches CDMR-BF-fGALIF and CDMR-BF-fDSIFT have inferior performance on efficiency, thus have much room for further improvement in this regard. A brief analysis on the computational efficiency of the ''CDMR-'' and ''UMR-'' approaches is as follows. Both approaches are used to improve accuracy of inter-feature distance and employ so called ''manifold ranking'' <ref type="bibr" target="#b64">[66]</ref> as its inter-feature distance metric learning. Manifold ranking comprises two steps: manifold graph formation and diffusion, which respectively have a upper bound of computational complexity of O (N 2 ) and O (N 3 ), where N is the total number of feature vectors. For the UMR, the matrix representing the manifold graph is sparse, thus its actual cost is much less than O (N 3 ), but still much more than O (N). The UMR forms a graph connecting all the features from all the views (42 views per 3D model) of all the 3D models. Therefore, if there are 1000 models in a database, N = 42 Â 1000 = 42000. It is true that this big size, e.g. N = 42000, pushes the limit of computational cost, both in terms of space and time. For the CDMR, the matrix representing the manifold graph is not sparse, but a much smaller number of feature vectors (thus smaller graph) leads to less computational cost than the UMR. A CDMR graph is smaller because it uses only one feature vector for each 3D model. Both UMR and CDMR produce better accuracy than simple distance computation among a pair of features. However, they are computationally expensive, and thus are not scalable for large benchmarks and deserve further study. We classify all contributed 15 methods with respect to the different classification methods mentioned in the first paragraph of Section 2.1. Most methods employ local features, except that SBR-VC, SBR-2D-3D, FDC and HTD perform global feature matching. Only SBR-VC and SBR-2D-3D perform view selection while all the other methods adopt the approach of fixed view sampling. All the 9 methods of the Furuya's group adopt a Bag-of-Words framework and among them 6 ''CDMR-'' and ''UMR-'' based methods utilize a distance metric learning approach <ref type="bibr" target="#b64">[66]</ref> (one type of machine learning techniques) to significantly improve the performance of their methods; while the remaining 6 methods contributed by the other 3 groups directly compare global features and do not adopt any machine learning approach.</p><p>Based on the fact that all the top algorithms on the two benchmarks are machine learning technique-based ones and it contributes much to the best performance obtained, we regard it as a promising approach to advance the performance of existing retrieval algorithms. This can be also found in our performance evaluation results: though SBR-VC and SBR-2D-3D are at least comparable to BF-fGALIF and BF-fDSIFT, CDMR-BF-fGALIF evidently outperforms either of them after employing the ''CDMR-'' distance learning approach. However, though utilizing a machine learning approach can obviously improve the retrieval performance of a sketch-based retrieval algorithm, it may also significantly increase its retrieval time. Therefore, more research and improvements are deserved to solve this contradictory issue in order to apply machine learning techniques into sketch-based 3D model retrieval in a scalable manner.</p><p>In terms of 2D shape descriptors, shape context, GALIF, DSIFT and HOG (refer to <ref type="bibr" target="#b1">[4,</ref><ref type="bibr" target="#b12">[14]</ref><ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref><ref type="bibr" target="#b2">5,</ref><ref type="bibr" target="#b15">17]</ref>) features are relatively more promising in achieving top performance if compared with Fourier descriptors, and structure or topology-based ones. We also have found that compared with global shape descriptors based on direct feature matching, it is much easier for local shape descriptors to achieve realtime efficiency while combined with a Bag-of-Words framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Benchmarks</head><p>The SHREC'12 Sketch Track Benchmark could be extended by sketch data as currently being compiled by other researchers, making it more representative. Also, controlling the level of standardization with respect to sketch parameters such as sketching quality, style, and level of detail is deemed interesting. The standard query sketches <ref type="bibr" target="#b54">[56]</ref> included in this benchmark are a starting point to this direction. While, the SHREC'13 Sketch Track Benchmark could be extended by adding more models to the target 3D model dataset to make each class contain the same number of models, which will remove the remaining bias and make the benchmark more representative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions and future work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Overall performance evaluation</head><p>(1) On the small-scale benchmark, we performed a comprehensively comparative evaluation of 14 state-of-the-art sketch-based retrieval methods in terms of accuracy, scalability and efficiency. Overall, Furuya's CDMR-BF-fGALIF + CDMR-BF-fDSIFT and CDMR-BF-fGALIF methods perform best, followed by the five comparable methods of Furuya's CDMR-BF-fDSIFT, UMR-BF-fGALIF + UMR-BF-fDSIFT and UMR-BF-fGALIF, as well as Li's SBR-VC_NUM_100 and SBR-2D-3D_NUM_100; (2) On the large-scale benchmark, we can draw a similar conclusion: Furuya's three CDMR-based algorithms (CDMR-BF-fGALIF + CDMR-BF-fDSIFT, CDMR-BF-fGALIF and CDMR-BF-fGALIF) have the best performance, followed by their three UMR-based algorithms (UMR-BF-fGALIF + UMR-BF-fDSIFT, UMR-BF-fDSIFT and UMR-BF-fGALIF) and their hybrid approach BF-fGA-LIF + BF-fDSIFT. Furuya's BF-fDSIFT, BF-fGALIF and Li's SBR-VC and SBR-2D-3D succeed them and they outperform Saavedra's three approaches (HOG-SIL, HELO-SIL and FDC) which are again better than HTD. However, compared with the case of small-scale benchmark, all the performance drops drastically, which indicates both the challenge of the large-scale benchmark and the new issues of scalability of existing sketch-based retrieval algorithms. This will also be helpful to guide our research on developing new sketchbased 3D retrieval algorithms or extending current methods to accustom to the large-scale retrieval scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Efficiency comparison</head><p>(1) On the small-scale benchmark, in terms of retrieval speed, we observe large differences between all methods: from 0.02 s (Saavedra's three methods) to 314.82 s (Furuya's UMR-BF-fDSIFT). In this case, the best-performing ''CDMR-'' based methods, together with BF-fDSIFT and BF-fGALIF are interactive, while other remaining approaches need improvement in this regard. (2) On the large-scale benchmark, the difference in the retrieval speed is even larger: from 0.09 s (Saavedra's three methods) to 54853.77 s (Furuya's UMR-BF-fDSIFT). What is more, we observe that the speed of the top ''CDMR-'' based methods drops drastically (730$1230 times slower) when scaled to a less than 2.5 times bigger benchmark, thus they can be further improved in this aspect. Saavedra's three methods are still the fastest, followed by Pascoal's HTD; while Li's SBR-VC and SBR-2D-3D methods extend their retrieval time proportionally according to the scale of the benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Concluding remark</head><p>In conclusion, the small-scale sketch-based retrieval track is the first attempt to include this topic in SHREC in order to foster this challenging and interesting research direction. Even though it is the first time, we already have 5 groups who have successfully </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Future work</head><p>This evaluation work helps us to identify the state-of-the-art approaches in terms of accuracy, scalability and efficiency, existing problems and current challenges, as well as promising research topics and techniques. Therefore, we identify the future direction of this research area is developing efficient algorithms which are scalable to different sizes and types of sketch queries and models. It can be achieved in the following eight aspects.</p><p>Scalable and interactive retrieval algorithms. In our evaluation, compared with the case of the small-scale benchmark, the performance of all the methods drops drastically on the large-scale benchmark, which evidently raises the issue of scalability of existing sketch-based retrieval algorithms. On the other hand, for some best-performing approaches their retrieval time increases too fast on a larger dataset, which creates a hurdle for their applications. It is well known that highly interactive response times are a key requirement for interactive sketch-based retrieval applications such as shadow drawing for interactive retrieval <ref type="bibr" target="#b77">[79]</ref>, or modeling by sketched example. Scalability of query processing time with respect to large target database and descriptor sizes is an important issue yet to be addressed. One possibility is to take advantage of already developed high-dimensional or metric index structures <ref type="bibr" target="#b78">[80,</ref><ref type="bibr" target="#b79">81]</ref>, which can accelerate the typically many nearest-neighbor computations required. However, this implies restricting the descriptors to vector spaces and metric distance functions. In case that more complex descriptors or distance functions are required for the retrieval task, developing ad hoc index structures may be required to efficiently fulfill this task. Also, we see bag-of-words approaches in conjunction with inverted indices as promising to provide scalable answer times. Building large-scale benchmarks. Since scalability is so important an issue, we should create a large-scale sketch-based 3D retrieval benchmark, in terms of both 2D sketches and 3D models, to evaluate the scalability property of sketch-based retrieval methods. Interdisciplinary research directions. We notice that generally the retrieval performance, either in terms of accuracy or efficiency, is far from satisfactory and the performance of existing sketch-based retrieval algorithms drops apparently when scaled to a large collection. To improve the retrieval accuracy, we recommend utilizing knowledge and techniques from other related disciplines, such as pattern recognition, machine learning, and computer vision. For example, to increase the accuracy and efficiency in sketch-based retrieval, we can perform sketch recognition first and use the result to prioritize the comparison between a query sketch and 3D models in the database. During the evaluation, we have found that all the top algorithms on the two benchmarks utilize machine learning technique, which contributes much to the best performance obtained. Therefore, machine learning is another promising interdisciplinary approach to further improve the performance of existing retrieval algorithms. However, we need to pay more attention to the scalability properties of the machine learning techniques employed and make sure they or their variations can meet the real time requirements of the retrieval applications. Query adaptive sketch-based retrieval algorithms. On the effectiveness side, another promising direction for future work is to consider adaptive sketch-based retrieval algorithms. Given that different types (or modes) of user sketches can occur (such as perspective, orthogonal, and abstract versus realistic), different descriptor types may be best suited for the search, depending on the sketch type. Adaptive retrieval systems could improve the search by deciding an appropriate descriptor which best supports the type of sketch query. Sketch-type dependent evaluation benchmarking is needed to understand the possible relationships between sketch types and method performance. Semantics-driven sketch-based retrieval and search intention study. To bridge the semantic gap between 2D sketches and 3D models, a promising research direction is to develop algorithms and benchmarks that deal more directly with semantics (e.g., semantic categories) and search intention (e.g., per session specification user intention, specifically, for example, by relevance feedback). Developing new local shape descriptors. According to the evaluation, we have found that most efficient retrieval algorithms adopt local shape descriptors. On the other hand, the retrieval algorithms based on HOG, GALIF, and DSIFT local shape descriptors, or the shape context global shape descriptor, outperform those based on Fourier descriptors, or structure or topology-based descriptors, in terms of retrieval accuracy. Therefore, developing novel local shape descriptors is a relatively more promising research direction to meet the requirements of future applications, which require high efficiency and accuracy. Domain specific sketch based retrieval algorithms and their evaluation methods. We can extend sketch-based retrieval for retrieving objects in images such as photographs and paintings; or special 3D objects like clothes and protein molecules. As such, it is necessary to build an appropriate image and 3D model database for benchmarking. New evaluation metrics may be also needed for the specific domains. Scene sketch and partial sketch-based 3D retrieval. Generally, existing sketch-based 3D model retrieval algorithms assume there is only one object in the query sketch, and do not consider the case of a 2D scene sketch query containing several objects, which may overlap each other and also have their spatial context information. Therefore, sketch-based 3D model retrieval in the context of a 2D scene sketch deserves our further explo-ration. Finally, we note that our benchmarks address, in spirit, the global retrieval model. Future benchmarking for sketchbased retrieval may in particular address partial sketch-based 3D retrieval algorithms which target interactively establishing correspondence between a partial sketch of an object and 3D model parts. Fig. <ref type="figure" target="#fig_1">20</ref>. Timing information comparison of the contributed algorithms based on the small-scale SHREC'12 Sketch Track Benchmark (''Hand-drawn and Extended'' dataset, 400 models) and the large-scale SHREC'13 Sketch Track Benchmark (''Testing'' dataset, 1258 models). Logarithmic scaling is applied on the Time axis. Methods fDSIFT, fGALIF, C-fDSIFT, C-fGALIF, U-fDSIFT, and U-fGALIF denote BF-fDSIFT, BF-fGALIF, CDMR-BF-fDSIFT, CDMR-BF-fGALIF, UMR-BF-fDSIFT, and UMR-BF-fGALIF, respectively. Note that for SBR-VC, NUM is set to 100 for both SHREC'12 and SHREC'13 Tracks, while different number of representative views have been adopted (Nc ¼ C 2 Á N0 and Nc ¼ C 6 Á N0 for SHREC'12 Sketch Track and SHREC'13 Sketch Track, respectively, see Section 5.3). Also note that different NUM values are used for SBR-2D-3D (NUM = 100 for SHREC'12 Sketch Track and NUM = 50 for SHREC'13 Sketch Track).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example feature views: (a) A horse model (in curvature view); (b) Silhouette feature view; (c) Contours (green) and suggestive contours (blue) feature views; (d) Apparent ridges feature view. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="3,116.22,67.92,368.79,87.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Typical 3D model and 2D sketch for each class of the SHREC'12 Sketch Track Benchmark.</figDesc><graphic coords="5,190.46,334.07,225.23,63.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example 2D sketches and their relevant 3D models in the SHREC'13 Sketch Track Benchmark.</figDesc><graphic coords="5,296.90,632.33,189.14,74.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3). Hierarchical Topology 3D Descriptor submitted by Pedro B. Pascoal, Alfredo Ferreira and Manuel J. Fonseca from Instituto Superior Técnico/ Technical University of Lisbon/ INESC-ID, Portugal (Section 5.4). HELO-SIL, HOG-SIL, and FDC submitted by Jose M. Saavedra from University of Chile, Chile and ORAND S.A., Chile and Benjamin Bustos from University of Chile, Chile (Section 5.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An example of intra-class variations of hand-drawn sketches: 35 sketches of the ''cell phone'' class.</figDesc><graphic coords="6,143.26,67.92,297.98,214.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Image-based feature comparison processes (applicable to fixed distance, UMR, and CDMR).</figDesc><graphic coords="7,323.66,67.92,226.83,339.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Feature comparison using Unsupervised Cross-Domain Manifold Ranking (CDMR).</figDesc><graphic coords="7,330.75,457.23,212.64,169.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5. 3 .</head><label>3</label><figDesc>Sketch-based 3D model retrieval based on view clustering and shape context matching (SBR-VC) [68,7], by B. Li, Y. Lu and H. Johan 3D models often differ in their visual complexities, thus there is no need to sample the same number of views to represent each model. Motivated by this, a Sketch-Based Retrieval algorithm based on adaptive View Clustering and Shape Context matching, named SBR-VC, has been proposed. Based on the viewpoint entropy distribution of a set of sample views of a model, they propose a 3D model visual complexity metric, based on which the number of the representative views of the 3D model is adaptively assigned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Flow chart of the sketch-based 3D model retrieval algorithm based on 2D-3D alignment and shape context matching.</figDesc><graphic coords="10,143.24,67.91,298.35,156.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The feature views of a 3D teddy model and a 2D ant standard line drawing sketch. For each row, from left to right: model/sketch, silhouette view; outline view.</figDesc><graphic coords="10,58.96,263.23,198.54,88.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. An overview of the SBR-VC algorithm. The first row is for precomputation while the second row is for retrieval stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Viewpoint entropy distribution examples: First row shows the models (front views); Second row demonstrates the viewpoint entropy distribution of each model seen from the viewpoint with respect to its front view. Entropy values are mapped as colors on the surface of the spheres based on HSV color model and smooth shading. Red: small entropy; green: mid-size entropy; blue: large entropy. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="11,155.84,619.25,67.16,66.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>5. 5 .Fig. 11 .</head><label>511</label><figDesc>Fig. 11. Overview of the Hierarchical Topology 3D Descriptor process.</figDesc><graphic coords="12,58.22,478.72,465.54,259.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Polygon detection process [69].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Drawing and topology graph.</figDesc><graphic coords="12,327.97,154.66,198.54,82.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Fig. 14. Block diagram for topology descriptor computation.</figDesc><graphic coords="13,96.38,67.92,411.30,86.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. A sketch and its silhouette representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(1) Hand-drawn sketch queries and Basic version of target dataset; (2) Standard line drawing queries and Basic version of target dataset; (3) Hand-drawn sketch queries and Extended version of target dataset; and (4) Standard line drawing queries and Extended version of target dataset. Comparisons of the 14 contributed methods for the above four cases are shown in Fig. 17 and Tables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Precision-Recall diagram performance comparisons of the ''airplane'' and ''ant'' classes on the complete SHREC'13 Sketch Track Benchmark for the 15 contributed methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>C-fGALIF U-fDSIFT U-fGALIF SBR-VC SBR-2D-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Parameters for the UMR.</figDesc><table><row><cell>Benchmark</cell><cell>Method</cell><cell>k</cell><cell>r</cell><cell>a</cell></row><row><cell>SHREC'12</cell><cell>UMR-BF-fGALIF</cell><cell>30</cell><cell>0.0001</cell><cell>0.9</cell></row><row><cell></cell><cell>UMR-BF-fDSIFT</cell><cell>140</cell><cell>0.0050</cell><cell>0.9</cell></row><row><cell>SHREC'13</cell><cell>UMR-BF-fGALIF</cell><cell>100</cell><cell>0.0025</cell><cell>0.9</cell></row><row><cell></cell><cell>UMR-BF-fDSIFT</cell><cell>160</cell><cell>0.0050</cell><cell>0.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Parameters for the CDMR.</figDesc><table><row><cell>Benchmark</cell><cell>Method</cell><cell>r SS</cell><cell>r MM</cell><cell>r SM</cell><cell>a</cell></row><row><cell>SHREC'12</cell><cell>CDMR-BF-fGALIF</cell><cell>0.010</cell><cell>0.025</cell><cell>0.040</cell><cell>0.9</cell></row><row><cell></cell><cell>CDMR-BF-fDSIFT</cell><cell>0.040</cell><cell>0.015</cell><cell>0.030</cell><cell>0.8</cell></row><row><cell>SHREC'13</cell><cell>CDMR-BF-fGALIF</cell><cell>0.006</cell><cell>0.010</cell><cell>0.025</cell><cell>0.9</cell></row><row><cell></cell><cell>CDMR-BF-fDSIFT</cell><cell>0.005</cell><cell>0.010</cell><cell>0.025</cell><cell>0.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Other Performance metrics for the performance comparison on the Hand-drawn sketch queries and Basic version of target dataset. ''R'' denotes the ranking group number.</figDesc><table><row><cell>Contributor</cell><cell>Method</cell><cell>NN</cell><cell>FT</cell><cell>ST</cell><cell>E</cell><cell>DCG</cell><cell>AP</cell><cell>R</cell></row><row><cell>Furuya</cell><cell>BF-fDSIFT</cell><cell>0.364</cell><cell>0.295</cell><cell>0.454</cell><cell>0.312</cell><cell>0.610</cell><cell>0.321</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF</cell><cell>0.600</cell><cell>0.367</cell><cell>0.513</cell><cell>0.357</cell><cell>0.689</cell><cell>0.393</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF + BF-fDSIFT</cell><cell>0.596</cell><cell>0.369</cell><cell>0.529</cell><cell>0.365</cell><cell>0.689</cell><cell>0.399</cell><cell>3</cell></row><row><cell></cell><cell>CDMR-BF-fDSIFT</cell><cell>0.504</cell><cell>0.428</cell><cell>0.604</cell><cell>0.418</cell><cell>0.701</cell><cell>0.491</cell><cell>2</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF</cell><cell>0.688</cell><cell>0.617</cell><cell>0.767</cell><cell>0.556</cell><cell>0.813</cell><cell>0.661</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF + CDMR-BF-fDSIFT</cell><cell>0.708</cell><cell>0.641</cell><cell>0.806</cell><cell>0.582</cell><cell>0.833</cell><cell>0.681</cell><cell>1</cell></row><row><cell></cell><cell>UMR-BF-fDSIFT</cell><cell>0.396</cell><cell>0.303</cell><cell>0.493</cell><cell>0.333</cell><cell>0.617</cell><cell>0.345</cell><cell>3</cell></row><row><cell></cell><cell>UMR-BF-fGALIF</cell><cell>0.612</cell><cell>0.443</cell><cell>0.576</cell><cell>0.411</cell><cell>0.718</cell><cell>0.478</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF + UMR-BF-fDSIFT</cell><cell>0.572</cell><cell>0.458</cell><cell>0.619</cell><cell>0.434</cell><cell>0.728</cell><cell>0.486</cell><cell>2</cell></row><row><cell>Li</cell><cell>SBR-VC_NUM_100</cell><cell>0.664</cell><cell>0.427</cell><cell>0.587</cell><cell>0.413</cell><cell>0.730</cell><cell>0.461</cell><cell>2</cell></row><row><cell></cell><cell>SBR-2D-3D_NUM_100</cell><cell>0.688</cell><cell>0.415</cell><cell>0.581</cell><cell>0.411</cell><cell>0.731</cell><cell>0.459</cell><cell>2</cell></row><row><cell>Saavedra</cell><cell>HELO-SIL</cell><cell>0.228</cell><cell>0.168</cell><cell>0.307</cell><cell>0.196</cell><cell>0.522</cell><cell>0.201</cell><cell>4</cell></row><row><cell></cell><cell>HOG-SIL</cell><cell>0.256</cell><cell>0.183</cell><cell>0.312</cell><cell>0.203</cell><cell>0.529</cell><cell>0.210</cell><cell>4</cell></row><row><cell></cell><cell>FDC</cell><cell>0.164</cell><cell>0.147</cell><cell>0.250</cell><cell>0.164</cell><cell>0.490</cell><cell>0.168</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Other performance metrics for the performance comparison on the Hand-drawn sketch queries and Extended version of target dataset. ''R'' denotes the ranking group number.</figDesc><table><row><cell>Contributor</cell><cell>Method</cell><cell>NN</cell><cell>FT</cell><cell>ST</cell><cell>E</cell><cell>DCG</cell><cell>AP</cell><cell>R</cell></row><row><cell>Furuya</cell><cell>BF-fDSIFT</cell><cell>0.280</cell><cell>0.225</cell><cell>0.370</cell><cell>0.244</cell><cell>0.552</cell><cell>0.246</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF</cell><cell>0.516</cell><cell>0.317</cell><cell>0.446</cell><cell>0.307</cell><cell>0.646</cell><cell>0.332</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF + BF-fDSIFT</cell><cell>0.508</cell><cell>0.317</cell><cell>0.461</cell><cell>0.316</cell><cell>0.648</cell><cell>0.338</cell><cell>3</cell></row><row><cell></cell><cell>CDMR-BF-fDSIFT</cell><cell>0.340</cell><cell>0.349</cell><cell>0.531</cell><cell>0.362</cell><cell>0.631</cell><cell>0.399</cell><cell>2</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF</cell><cell>0.648</cell><cell>0.566</cell><cell>0.741</cell><cell>0.535</cell><cell>0.787</cell><cell>0.616</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF + DMR-BF-fDSIFT</cell><cell>0.668</cell><cell>0.585</cell><cell>0.770</cell><cell>0.554</cell><cell>0.801</cell><cell>0.625</cell><cell>1</cell></row><row><cell></cell><cell>UMR-BF-fDSIFT</cell><cell>0.296</cell><cell>0.235</cell><cell>0.406</cell><cell>0.269</cell><cell>0.560</cell><cell>0.274</cell><cell>3</cell></row><row><cell></cell><cell>UMR-BF-fGALIF</cell><cell>0.572</cell><cell>0.387</cell><cell>0.530</cell><cell>0.375</cell><cell>0.684</cell><cell>0.426</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF + UMR-BF-fDSIFT</cell><cell>0.492</cell><cell>0.392</cell><cell>0.554</cell><cell>0.387</cell><cell>0.684</cell><cell>0.418</cell><cell>2</cell></row><row><cell>Li</cell><cell>SBR-VC_NUM_100</cell><cell>0.576</cell><cell>0.372</cell><cell>0.519</cell><cell>0.360</cell><cell>0.682</cell><cell>0.392</cell><cell>2</cell></row><row><cell></cell><cell>SBR-2D-3D_NUM_100</cell><cell>0.628</cell><cell>0.371</cell><cell>0.520</cell><cell>0.364</cell><cell>0.692</cell><cell>0.400</cell><cell>2</cell></row><row><cell>Saavedra</cell><cell>HELO-SIL</cell><cell>0.116</cell><cell>0.097</cell><cell>0.183</cell><cell>0.115</cell><cell>0.443</cell><cell>0.122</cell><cell>4</cell></row><row><cell></cell><cell>HOG-SIL</cell><cell>0.188</cell><cell>0.123</cell><cell>0.223</cell><cell>0.139</cell><cell>0.466</cell><cell>0.143</cell><cell>4</cell></row><row><cell></cell><cell>FDC</cell><cell>0.108</cell><cell>0.087</cell><cell>0.170</cell><cell>0.108</cell><cell>0.426</cell><cell>0.104</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Other performance metrics for the performance comparison on the Standard line drawing queries and Basic version of target dataset. ''R'' denotes the ranking group number.</figDesc><table><row><cell>Contributor</cell><cell>Method</cell><cell>NN</cell><cell>FT</cell><cell>ST</cell><cell>E</cell><cell>DCG</cell><cell>AP</cell><cell>R</cell></row><row><cell>Furuya</cell><cell>BF-fDSIFT</cell><cell>0.667</cell><cell>0.442</cell><cell>0.658</cell><cell>0.436</cell><cell>0.747</cell><cell>0.477</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF</cell><cell>0.500</cell><cell>0.421</cell><cell>0.612</cell><cell>0.410</cell><cell>0.722</cell><cell>0.443</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF + BF-fDSIFT</cell><cell>0.667</cell><cell>0.471</cell><cell>0.717</cell><cell>0.510</cell><cell>0.765</cell><cell>0.498</cell><cell>3</cell></row><row><cell></cell><cell>CDMR-BF-fDSIFT</cell><cell>0.667</cell><cell>0.583</cell><cell>0.833</cell><cell>0.580</cell><cell>0.800</cell><cell>0.633</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF</cell><cell>0.750</cell><cell>0.629</cell><cell>0.792</cell><cell>0.567</cell><cell>0.842</cell><cell>0.694</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF + CDMR-BF-fDSIFT</cell><cell>0.750</cell><cell>0.679</cell><cell>0.871</cell><cell>0.619</cell><cell>0.866</cell><cell>0.727</cell><cell>1</cell></row><row><cell></cell><cell>UMR-BF-fDSIFT</cell><cell>0.750</cell><cell>0.525</cell><cell>0.733</cell><cell>0.529</cell><cell>0.779</cell><cell>0.563</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF</cell><cell>0.500</cell><cell>0.487</cell><cell>0.617</cell><cell>0.442</cell><cell>0.752</cell><cell>0.517</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF + UMR-BF-fDSIFT</cell><cell>0.750</cell><cell>0.546</cell><cell>0.692</cell><cell>0.494</cell><cell>0.796</cell><cell>0.568</cell><cell>2</cell></row><row><cell>Li</cell><cell>SBR-VC_NUM_100</cell><cell>0.667</cell><cell>0.521</cell><cell>0.713</cell><cell>0.503</cell><cell>0.799</cell><cell>0.570</cell><cell>2</cell></row><row><cell></cell><cell>SBR-2D-3D_NUM_100</cell><cell>0.750</cell><cell>0.542</cell><cell>0.700</cell><cell>0.516</cell><cell>0.807</cell><cell>0.580</cell><cell>2</cell></row><row><cell>Saavedra</cell><cell>HELO-SIL</cell><cell>0.167</cell><cell>0.221</cell><cell>0.379</cell><cell>0.256</cell><cell>0.550</cell><cell>0.254</cell><cell>4</cell></row><row><cell></cell><cell>HOG-SIL</cell><cell>0.583</cell><cell>0.233</cell><cell>0.413</cell><cell>0.263</cell><cell>0.606</cell><cell>0.289</cell><cell>4</cell></row><row><cell></cell><cell>FDC</cell><cell>0.250</cell><cell>0.208</cell><cell>0.329</cell><cell>0.237</cell><cell>0.549</cell><cell>0.226</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Other performance metrics for the performance comparison on the Standard line drawing queries and Extended version of target dataset. ''R'' denotes the ranking group number.</figDesc><table><row><cell>Contributor</cell><cell>Method</cell><cell>NN</cell><cell>FT</cell><cell>ST</cell><cell>E</cell><cell>DCG</cell><cell>AP</cell><cell>R</cell></row><row><cell>Furuya</cell><cell>BF-fDSIFT</cell><cell>0.583</cell><cell>0.337</cell><cell>0.512</cell><cell>0.356</cell><cell>0.684</cell><cell>0.376</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF</cell><cell>0.417</cell><cell>0.337</cell><cell>0.496</cell><cell>0.346</cell><cell>0.656</cell><cell>0.336</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF + BF-fDSIFT</cell><cell>0.417</cell><cell>0.388</cell><cell>0.613</cell><cell>0.394</cell><cell>0.697</cell><cell>0.394</cell><cell>3</cell></row><row><cell></cell><cell>CDMR-BF-fDSIFT</cell><cell>0.500</cell><cell>0.467</cell><cell>0.717</cell><cell>0.494</cell><cell>0.724</cell><cell>0.523</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF</cell><cell>0.583</cell><cell>0.542</cell><cell>0.717</cell><cell>0.519</cell><cell>0.781</cell><cell>0.590</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF + CDMR-BF-fDSIFT</cell><cell>0.500</cell><cell>0.533</cell><cell>0.783</cell><cell>0.551</cell><cell>0.778</cell><cell>0.589</cell><cell>1</cell></row><row><cell></cell><cell>UMR-BF-fDSIFT</cell><cell>0.667</cell><cell>0.442</cell><cell>0.671</cell><cell>0.455</cell><cell>0.733</cell><cell>0.490</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF</cell><cell>0.500</cell><cell>0.371</cell><cell>0.550</cell><cell>0.388</cell><cell>0.701</cell><cell>0.427</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF + UMR-BF-fDSIFT</cell><cell>0.667</cell><cell>0.433</cell><cell>0.642</cell><cell>0.455</cell><cell>0.739</cell><cell>0.479</cell><cell>2</cell></row><row><cell>Li</cell><cell>SBR-VC_NUM_100</cell><cell>0.750</cell><cell>0.442</cell><cell>0.629</cell><cell>0.439</cell><cell>0.747</cell><cell>0.477</cell><cell>2</cell></row><row><cell></cell><cell>SBR-2D-3D_NUM_100</cell><cell>0.750</cell><cell>0.454</cell><cell>0.625</cell><cell>0.442</cell><cell>0.750</cell><cell>0.476</cell><cell>2</cell></row><row><cell>Saavedra</cell><cell>HELO-SIL</cell><cell>0.083</cell><cell>0.121</cell><cell>0.246</cell><cell>0.151</cell><cell>0.454</cell><cell>0.157</cell><cell>4</cell></row><row><cell></cell><cell>HOG-SIL</cell><cell>0.417</cell><cell>0.163</cell><cell>0.271</cell><cell>0.167</cell><cell>0.541</cell><cell>0.214</cell><cell>4</cell></row><row><cell></cell><cell>FDC</cell><cell>0.167</cell><cell>0.100</cell><cell>0.221</cell><cell>0.138</cell><cell>0.455</cell><cell>0.127</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Scalability comparison in terms of performance decrease (%, in terms of percentage of the performance results on the Basic version of target dataset) on the Hand-drawn sketch queries. ''R'' denotes the ranking group number.</figDesc><table><row><cell>Contributor</cell><cell>Method</cell><cell>NN</cell><cell>FT</cell><cell>ST</cell><cell>E</cell><cell>DCG</cell><cell>AP</cell><cell>R</cell></row><row><cell>Furuya</cell><cell>BF-fDSIFT</cell><cell>À23.1</cell><cell>À23.7</cell><cell>À18.5</cell><cell>À21.8</cell><cell>À9.5</cell><cell>À23.4</cell><cell>4</cell></row><row><cell></cell><cell>BF-fGALIF</cell><cell>À14.0</cell><cell>À13.6</cell><cell>À13.1</cell><cell>À14.0</cell><cell>À6.2</cell><cell>À15.5</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF + BF-fDSIFT</cell><cell>À14.8</cell><cell>À14.1</cell><cell>À12.9</cell><cell>À13.4</cell><cell>À6.0</cell><cell>À15.3</cell><cell>3</cell></row><row><cell></cell><cell>CDMR-BF-fDSIFT</cell><cell>À32.5</cell><cell>À18.5</cell><cell>À12.1</cell><cell>À13.4</cell><cell>À10.0</cell><cell>À18.7</cell><cell>3</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF</cell><cell>À5.8</cell><cell>À8.3</cell><cell>À3.4</cell><cell>À3.8</cell><cell>À3.2</cell><cell>À6.8</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF + CDMR-BF-fDSIFT</cell><cell>À5.6</cell><cell>À8.7</cell><cell>À4.5</cell><cell>À4.8</cell><cell>À3.8</cell><cell>À8.2</cell><cell>1</cell></row><row><cell></cell><cell>UMR-BF-fDSIFT</cell><cell>À25.3</cell><cell>À22.4</cell><cell>À17.6</cell><cell>À19.2</cell><cell>À9.2</cell><cell>À20.6</cell><cell>4</cell></row><row><cell></cell><cell>UMR-BF-fGALIF</cell><cell>À6.5</cell><cell>À12.6</cell><cell>À8.0</cell><cell>À8.8</cell><cell>À4.7</cell><cell>À10.9</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF + UMR-BF-fDSIFT</cell><cell>À14.0</cell><cell>À14.4</cell><cell>À10.5</cell><cell>À10.8</cell><cell>À6.0</cell><cell>À14.0</cell><cell>2</cell></row><row><cell>Li</cell><cell>SBR-VC_NUM_100</cell><cell>À13.3</cell><cell>À12.9</cell><cell>À11.6</cell><cell>À12.8</cell><cell>À6.6</cell><cell>À15.0</cell><cell>2</cell></row><row><cell></cell><cell>SBR-2D-3D_NUM_100</cell><cell>À8.7</cell><cell>À10.6</cell><cell>À10.5</cell><cell>À11.4</cell><cell>À5.3</cell><cell>À12.9</cell><cell>2</cell></row><row><cell>Saavedra</cell><cell>HELO-SIL</cell><cell>À49.1</cell><cell>À42.3</cell><cell>À40.4</cell><cell>À41.3</cell><cell>À15.1</cell><cell>À39.3</cell><cell>5</cell></row><row><cell></cell><cell>HOG-SIL</cell><cell>À26.6</cell><cell>À32.8</cell><cell>À28.5</cell><cell>À31.5</cell><cell>À11.9</cell><cell>À31.9</cell><cell>5</cell></row><row><cell></cell><cell>FDC</cell><cell>À34.2</cell><cell>À40.8</cell><cell>À32.0</cell><cell>À34.2</cell><cell>À13.1</cell><cell>À38.1</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>Timing information comparison on the ''hand-drawn and extended'' dataset of SHREC'12 Sketch Track Benchmark: t is the average response time (in seconds) per query. ''R'' denotes the ranking group number.</figDesc><table><row><cell>Contributor (with computer configuration)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9</head><label>9</label><figDesc>Performance metrics for the performance comparison on the training dataset of the SHREC'13 Sketch Track Benchmark. ''R'' denotes the ranking group number.</figDesc><table><row><cell>Contributor</cell><cell>Method</cell><cell>NN</cell><cell>FT</cell><cell>ST</cell><cell>E</cell><cell>DCG</cell><cell>AP</cell><cell>R</cell></row><row><cell>Furuya</cell><cell>BF-fDSIFT</cell><cell>0.153</cell><cell>0.099</cell><cell>0.158</cell><cell>0.095</cell><cell>0.354</cell><cell>0.119</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF</cell><cell>0.183</cell><cell>0.101</cell><cell>0.154</cell><cell>0.089</cell><cell>0.355</cell><cell>0.118</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF + BF-fDSIFT</cell><cell>0.210</cell><cell>0.123</cell><cell>0.185</cell><cell>0.106</cell><cell>0.381</cell><cell>0.142</cell><cell>2</cell></row><row><cell></cell><cell>CDMR-BF-fDSIFT</cell><cell>0.217</cell><cell>0.158</cell><cell>0.231</cell><cell>0.136</cell><cell>0.413</cell><cell>0.196</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF</cell><cell>0.238</cell><cell>0.175</cell><cell>0.267</cell><cell>0.149</cell><cell>0.428</cell><cell>0.215</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF + CDMR-BF-fDSIFT</cell><cell>0.273</cell><cell>0.205</cell><cell>0.301</cell><cell>0.167</cell><cell>0.459</cell><cell>0.247</cell><cell>1</cell></row><row><cell></cell><cell>UMR-BF-fDSIFT</cell><cell>0.160</cell><cell>0.115</cell><cell>0.182</cell><cell>0.107</cell><cell>0.369</cell><cell>0.136</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF</cell><cell>0.170</cell><cell>0.120</cell><cell>0.185</cell><cell>0.104</cell><cell>0.371</cell><cell>0.135</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF + UMR-BF-fDSIFT</cell><cell>0.208</cell><cell>0.131</cell><cell>0.201</cell><cell>0.115</cell><cell>0.387</cell><cell>0.152</cell><cell>2</cell></row><row><cell>Li</cell><cell>SBR-VC_NUM_100</cell><cell>0.160</cell><cell>0.097</cell><cell>0.149</cell><cell>0.085</cell><cell>0.349</cell><cell>0.113</cell><cell>3</cell></row><row><cell></cell><cell>SBR-VC_NUM_50</cell><cell>0.131</cell><cell>0.082</cell><cell>0.130</cell><cell>0.076</cell><cell>0.333</cell><cell>0.098</cell><cell>3</cell></row><row><cell></cell><cell>SBR-2D-3D_NUM_50</cell><cell>0.133</cell><cell>0.080</cell><cell>0.126</cell><cell>0.075</cell><cell>0.330</cell><cell>0.097</cell><cell>3</cell></row><row><cell>Pascoal</cell><cell>HTD</cell><cell>0.019</cell><cell>0.017</cell><cell>0.033</cell><cell>0.018</cell><cell>0.241</cell><cell>0.025</cell><cell>5</cell></row><row><cell>Saavedra</cell><cell>HOG-SIL</cell><cell>0.104</cell><cell>0.066</cell><cell>0.104</cell><cell>0.059</cell><cell>0.304</cell><cell>0.082</cell><cell>4</cell></row><row><cell></cell><cell>HELO-SIL</cell><cell>0.104</cell><cell>0.061</cell><cell>0.092</cell><cell>0.052</cell><cell>0.300</cell><cell>0.076</cell><cell>4</cell></row><row><cell></cell><cell>FDC</cell><cell>0.062</cell><cell>0.049</cell><cell>0.085</cell><cell>0.051</cell><cell>0.295</cell><cell>0.062</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10</head><label>10</label><figDesc>Performance metrics for the performance comparison on the testing dataset of the SHREC'13 Sketch Track Benchmark. ''R'' denotes the ranking group number.</figDesc><table><row><cell>Contributor</cell><cell>Method</cell><cell>NN</cell><cell>FT</cell><cell>ST</cell><cell>E</cell><cell>DCG</cell><cell>AP</cell><cell>R</cell></row><row><cell>Furuya</cell><cell>BF-fDSIFT</cell><cell>0.145</cell><cell>0.099</cell><cell>0.154</cell><cell>0.093</cell><cell>0.351</cell><cell>0.115</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF</cell><cell>0.176</cell><cell>0.101</cell><cell>0.156</cell><cell>0.091</cell><cell>0.354</cell><cell>0.119</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF + BF-fDSIFT</cell><cell>0.213</cell><cell>0.123</cell><cell>0.186</cell><cell>0.107</cell><cell>0.379</cell><cell>0.143</cell><cell>2</cell></row><row><cell></cell><cell>CDMR-BF-fDSIFT</cell><cell>0.217</cell><cell>0.156</cell><cell>0.231</cell><cell>0.135</cell><cell>0.411</cell><cell>0.193</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF</cell><cell>0.242</cell><cell>0.174</cell><cell>0.263</cell><cell>0.146</cell><cell>0.427</cell><cell>0.215</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF + CDMR-BF-fDSIFT</cell><cell>0.279</cell><cell>0.203</cell><cell>0.296</cell><cell>0.166</cell><cell>0.458</cell><cell>0.246</cell><cell>1</cell></row><row><cell></cell><cell>UMR-BF-fDSIFT</cell><cell>0.154</cell><cell>0.113</cell><cell>0.178</cell><cell>0.104</cell><cell>0.366</cell><cell>0.133</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF</cell><cell>0.159</cell><cell>0.119</cell><cell>0.179</cell><cell>0.102</cell><cell>0.367</cell><cell>0.131</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF + UMR-BF-fDSIFT</cell><cell>0.209</cell><cell>0.131</cell><cell>0.195</cell><cell>0.113</cell><cell>0.386</cell><cell>0.152</cell><cell>2</cell></row><row><cell>Li</cell><cell>SBR-VC_NUM_100</cell><cell>0.164</cell><cell>0.097</cell><cell>0.149</cell><cell>0.085</cell><cell>0.348</cell><cell>0.114</cell><cell>3</cell></row><row><cell></cell><cell>SBR-VC_NUM_50</cell><cell>0.132</cell><cell>0.082</cell><cell>0.131</cell><cell>0.075</cell><cell>0.331</cell><cell>0.098</cell><cell>3</cell></row><row><cell></cell><cell>SBR-2D-3D_NUM_50</cell><cell>0.132</cell><cell>0.077</cell><cell>0.124</cell><cell>0.074</cell><cell>0.327</cell><cell>0.095</cell><cell>3</cell></row><row><cell>Pascoal</cell><cell>HTD</cell><cell>0.017</cell><cell>0.016</cell><cell>0.031</cell><cell>0.018</cell><cell>0.240</cell><cell>0.024</cell><cell>5</cell></row><row><cell>Saavedra</cell><cell>HOG-SIL</cell><cell>0.110</cell><cell>0.069</cell><cell>0.107</cell><cell>0.061</cell><cell>0.307</cell><cell>0.084</cell><cell>4</cell></row><row><cell></cell><cell>HELO-SIL</cell><cell>0.110</cell><cell>0.064</cell><cell>0.096</cell><cell>0.054</cell><cell>0.302</cell><cell>0.079</cell><cell>4</cell></row><row><cell></cell><cell>FDC</cell><cell>0.069</cell><cell>0.048</cell><cell>0.085</cell><cell>0.051</cell><cell>0.296</cell><cell>0.051</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11</head><label>11</label><figDesc>Performance metrics for the performance comparison on the complete SHREC'13 Sketch Track Benchmark. ''R'' denotes the ranking group number.</figDesc><table><row><cell>Contributor</cell><cell>Method</cell><cell>NN</cell><cell>FT</cell><cell>ST</cell><cell>E</cell><cell>DCG</cell><cell>AP</cell><cell>R</cell></row><row><cell>Furuya</cell><cell>BF-fDSIFT</cell><cell>0.150</cell><cell>0.099</cell><cell>0.157</cell><cell>0.094</cell><cell>0.353</cell><cell>0.118</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF</cell><cell>0.180</cell><cell>0.101</cell><cell>0.154</cell><cell>0.090</cell><cell>0.354</cell><cell>0.119</cell><cell>3</cell></row><row><cell></cell><cell>BF-fGALIF + BF-fDSIFT</cell><cell>0.211</cell><cell>0.123</cell><cell>0.186</cell><cell>0.106</cell><cell>0.380</cell><cell>0.142</cell><cell>2</cell></row><row><cell></cell><cell>CDMR-BF-fDSIFT</cell><cell>0.217</cell><cell>0.157</cell><cell>0.231</cell><cell>0.135</cell><cell>0.412</cell><cell>0.195</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF</cell><cell>0.239</cell><cell>0.175</cell><cell>0.265</cell><cell>0.148</cell><cell>0.428</cell><cell>0.215</cell><cell>1</cell></row><row><cell></cell><cell>CDMR-BF-fGALIF + CDMR-BF-fDSIFT</cell><cell>0.275</cell><cell>0.204</cell><cell>0.299</cell><cell>0.167</cell><cell>0.459</cell><cell>0.247</cell><cell>1</cell></row><row><cell></cell><cell>UMR-BF-fDSIFT</cell><cell>0.158</cell><cell>0.114</cell><cell>0.180</cell><cell>0.106</cell><cell>0.368</cell><cell>0.135</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF</cell><cell>0.166</cell><cell>0.119</cell><cell>0.182</cell><cell>0.103</cell><cell>0.370</cell><cell>0.134</cell><cell>2</cell></row><row><cell></cell><cell>UMR-BF-fGALIF + UMR-BF-fDSIFT</cell><cell>0.209</cell><cell>0.131</cell><cell>0.199</cell><cell>0.114</cell><cell>0.387</cell><cell>0.152</cell><cell>2</cell></row><row><cell>Li</cell><cell>SBR-VC_NUM_100</cell><cell>0.161</cell><cell>0.097</cell><cell>0.149</cell><cell>0.085</cell><cell>0.349</cell><cell>0.113</cell><cell>3</cell></row><row><cell></cell><cell>SBR-VC_NUM_50</cell><cell>0.131</cell><cell>0.082</cell><cell>0.130</cell><cell>0.076</cell><cell>0.332</cell><cell>0.098</cell><cell>3</cell></row><row><cell></cell><cell>SBR-2D-3D_NUM_50</cell><cell>0.133</cell><cell>0.079</cell><cell>0.125</cell><cell>0.074</cell><cell>0.329</cell><cell>0.096</cell><cell>3</cell></row><row><cell>Pascoal</cell><cell>HTD</cell><cell>0.018</cell><cell>0.017</cell><cell>0.032</cell><cell>0.018</cell><cell>0.241</cell><cell>0.024</cell><cell>5</cell></row><row><cell>Saavedra</cell><cell>HOG-SIL</cell><cell>0.106</cell><cell>0.067</cell><cell>0.105</cell><cell>0.060</cell><cell>0.305</cell><cell>0.083</cell><cell>4</cell></row><row><cell></cell><cell>HELO-SIL</cell><cell>0.106</cell><cell>0.062</cell><cell>0.094</cell><cell>0.053</cell><cell>0.301</cell><cell>0.077</cell><cell>4</cell></row><row><cell></cell><cell>FDC</cell><cell>0.065</cell><cell>0.048</cell><cell>0.085</cell><cell>0.051</cell><cell>0.296</cell><cell>0.062</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12</head><label>12</label><figDesc>Timing information comparison on the ''Testing'' dataset of SHREC'13 Sketch Track Benchmark: t is the average response time (in seconds) per query. ''R'' denotes the ranking group number. Note that for SBR-VC, the number of representative views Nc ¼ C 6 Á N0.</figDesc><table><row><cell>Contributor (with computer configuration)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>B. Li et al. / Computer Vision and Image Understanding 119 (2014) 57-80</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the Army Research Office grant W911NF-12-1-0057, Texas State University Research Enhancement Program (REP), and NSF CRI 1305302 to Yijuan Lu, as well as the Shape Metrology IMS to Afzal Godil. The work of Benjamin Bustos has been funded by Fondecyt (Chile) Project 1110111. The work of Pedro B. Pascoal, Alfredo Ferreira, Manuel J. Fonseca reported in this paper has been supported by national funds through FCT under contract Pest-OE/EEI/LA0021/2013. Henry Johan is supported by Fraunhofer IDM@NTU, which is funded by the National Research Foundation (NRF) and managed through the multi-agency Interactive &amp; Digital Media Programme Office (IDMPO) hosted by the Media Development Authority of Singapore (MDA).</p><p>We would like to thank Mathias Eitz (TU Berlin, Germany), James Hays (Brown University, USA) and Marc Alexa (TU Berlin, Germany) who built the large collection of sketches.</p><p>We would also like to thank Sang Min Yoon (Yonsei University, Korea), Maximilian Scherer (TU Darmstadt, Germany), Tobias Schreck (University of Konstanz) and Arjan Kuijper (Fraunhofer IGD) who collected the TU Darmstadt and Fraunhofer IGD sketch data.</p><p>We would also like to thank Daniela Giorgi, Silvia Biasotti, Laura Paraboschi (CNR-IMATI, Italy) who built the Watertight Shape Benchmark for SHREC 2007 and Snograss and Vanderwart who built the standard line drawings dataset, as well as Philip Shilane, Patrick Min, Michael M. Kazhdan, Thomas A. Funkhouser (Princeton University, USA) who built the Princeton Shape Benchmark.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We noticed that all the retrieval performance metrics values are not high, which is mainly due to the challenges of the benchmark. Firstly, the 80 sketches in a query class represent many variations of an object, which adds the difficulty for accurate retrieval and deserves a higher standard on the scalability of retrieval algorithms. Secondly, as mentioned in Section 3.2.1, the query class bias has already been solved by making each query class contain the same number of sketches, while the bias in the target class still exists. There is a large variation in the number of models in different classes. For example, the ''airplane'' class contains 184 target models while the ''ant'' class only has 5 models. Thus, to accurately retrieve these classes of models in the First Tier and Second Tier is difficult. Therefore, their performance metrics values, especially on NN, FT and ST, are relatively much lower and this happens to all the 15 running methods. One demonstrating example is shown in Fig. <ref type="bibr" target="#b17">19</ref>. More details about the variations in the performance with respect to different classes for each contributed method can be found in the SHREC <ref type="bibr">'</ref>  participated. On the other hand, the large-scale sketch-based retrieval track is an attempt to further foster this challenging and interesting research direction encouraged by the success of SHREC'12 Sketch-based 3D shape retrieval track. Though the benchmark is very challenging, we still have 3 groups who have successfully participated in the track and they have contributed 5 runs of 4 methods. Through these two tracks, we provided two common platforms (the two benchmarks) to solicit current sketch-based 3D model retrieval approaches in terms of both small-scale and large-scale retrieval scenarios. This helps us identify state-of-the-art methods as well as future research directions for this research area. We also hope that the small-scale and large-scale sketch retrieval benchmarks together with the evaluation code will become useful references for researchers in this community.</p><p>To comprehensively evaluate the participating methods in the two tracks and other sketch-based 3D model retrieval algorithms, solicit the state-of-the-art approaches, and provide a more complete reference for the researchers in this community, we invite related authors to run their methods on both benchmarks. Finally, a detailed comparative evaluation has been accomplished based on 17 contributed runs of 15 (4 top participating algorithms and 11 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D object detection and viewpoint selection in sketch images using local patch-based Zernike moments</title>
		<author>
			<persName><forename type="first">A.-P</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lavoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CBMI</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Kollias</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Avrithis</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="189" to="194" />
			<date type="published" when="2009">2009</date>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Johan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuijper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yanagimachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Yoon</surname></persName>
		</author>
		<title level="m">SHREC&apos;12 track: Sketch-based 3D shape retrieval</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
	<note>Eurographics Workshop on 3D Object Retrieval</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sketch-based shape retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Princeton shape benchmark, in: Shape Modeling International</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shilane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society</title>
		<imprint>
			<biblScope unit="page" from="167" to="178" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Johan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tashiro</surname></persName>
		</author>
		<title level="m">SHREC&apos;13 track: large scale sketch-based 3D shape retrieval, in: Eurographics Workshop on 3D Object Retrieval (3DOR)</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A search engine for 3D models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dobkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="83" to="105" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rotation invariant spherical harmonic representation of 3D shape descriptors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Geometry Processing</title>
		<title level="s">ACM International Conference Proceeding Series</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Schröder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</editor>
		<imprint>
			<publisher>Eurographics Association</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sketch-based 3D model retrieval using diffusion tensor fields of suggestive contours</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuijper</surname></persName>
		</author>
		<editor>A.D. Bimbo, S.-F</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m">ACM Multimedia, ACM</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Smeulders</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">STELA: sketch-based 3D model retrieval using a structure-based local approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR, ACM</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">G B D</forename><surname>Natale</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Suggestive contours for conveying shape</title>
		<author>
			<persName><forename type="first">D</forename><surname>Doug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szymon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anthony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="848" to="855" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D shape retrieval from a 2D image as query</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iwabuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal &amp; Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<date type="published" when="2012">2012, 2012</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
		<respStmt>
			<orgName>APSIPA ASC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<title level="m">Sketch-based 3D shape retrieval</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>SIGGRAPH Talks, ACM</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An evaluation of descriptors for large-scale image retrieval from sketched feature lines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="482" to="498" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sketch-based image retrieval: benchmark and bag-of-features descriptors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visual. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1624" to="1636" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative sketch-based 3D model retrieval via robust shape matching</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2011" to="2020" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Johan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sketch-based 3D model retrieval by incorporating 2D-3D alignment</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="363" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">View context: a 3D model feature for retrieval</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Johan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM 2010</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Boll</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5916</biblScope>
			<biblScope unit="page" from="185" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fares</surname></persName>
		</author>
		<title level="m">Semantic sketch-based 3D model retrieval</title>
		<editor>
			<persName><forename type="first">Ieee</forename><surname>Icme</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Example-based 3D object reconstruction from line drawings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="302" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object cut: complex 3D object reconstruction through line drawing separation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="1149" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decomposition of complex line drawings with hidden lines for 3D planar-faced manifold object reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="233" to="245" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Sketch-to-design: context-based part assembly</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sketch2scene: sketch-based coretrieval and co-placement of 3D models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Content-based 3D mesh model retrieval from hand-written sketch</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Interact. Des. Manuf</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-Fourier spectra descriptor and augmentation with spectral clustering for 3D shape retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tatsuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="785" to="804" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">View-based 3D model retrieval using compressive sensing based classification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuijper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 7th International Symposium on Image and Signal Processing and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sketch-based 3D model retrieval using keyshapes for global and local representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval (3DOR)</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="47" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Apparent ridges for line drawing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ridge-valley lines on meshes via implicit surface fitting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohtake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Belyaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="609" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An effective illustrative visualization framework based on photic extremum lines (PELs)</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1328" to="1335" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Real-time computation of photic extremum lines (PELs), The Visual Comput</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="399" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Demarcating curves for shape illustration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolomenkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">157</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Perceptual-saliency extremum lines for 3D shape illustration, The Visual Comput</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="433" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-time shape illustration using Laplacian lines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="993" to="1006" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient and robust 3D line drawings using difference-of-gaussian</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mueller-Wittig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph. Models</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-scale curve detection on surfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolomenkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<editor>CVPR, IEEE</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Depicting 3D shape using lines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Decarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8291</biblScope>
			<biblScope unit="page" from="829116" to="829116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A comparative study of Fourier descriptors for shape representation and retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Asian Conference on Computer Vision (ACCV02)</title>
		<meeting>the Fifth Asian Conference on Computer Vision (ACCV02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="646" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image retrieval using modified generic fourier descriptors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sajjanhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers and their Applications</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Gupta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="32" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shape-based image retrieval using generic Fourier descriptor, Signal Process</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="page" from="825" to="848" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Invariant image recognition by Zernike moments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khotanzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="489" to="497" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3D Zernike descriptors for content based shape retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Novotni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Symposium on Solid Modeling and Applications</title>
		<meeting>the 8th ACM Symposium on Solid Modeling and Applications</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving Zernike moments comparison for optimal similarity and rotation angle retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lavoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="627" to="636" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A comparative study of texture measures with classification based on featured distributions, Pattern Recog</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A descriptor for large scale image retrieval based on sketched feature lines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SBM, Eurographics Association</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Grimm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><genName>Jr</genName></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dense sampling and fast encoding for 3D model retrieval using bag-of-visual features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CIVR</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2009">2009</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Furuya</surname></persName>
		</author>
		<title level="m">Scale-weighted dense bag of visual features for 3D model retrieval from a partial view 3D model</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
	<note>IEEE ICCV 2009 Workshop on Search in 3D and Video (S3DV</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Curvelets: A surprisingly effective nonadaptive representation of objects with edges</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">L</forename></persName>
		</editor>
		<meeting><address><addrLine>Nashville, TN</addrLine></address></meeting>
		<imprint>
			<publisher>Vanderbilt University Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A standardized set of 260 pictures: norms for name agreement, image agreement, familiarity, and visual complexity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vanderwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Experimen. Pyschol.: Human Learn. Mem</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="174" to="215" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A set of 400 pictures standardized for French: Norms for name agreement, image agreement, familiarity, visual complexity, image variability, and age of acquisition</title>
		<author>
			<persName><forename type="first">F.-X</forename><surname>Alario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Meth. Instrum., Comput</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="531" to="552" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A new set of 299 pictures for psycholinguistic studies: French norms for name agreement, image agreement, conceptual familiarity, visual complexity, image variability, age of acquisition, and naming latencies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bonin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peereman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malardier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chalard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Meth. Instrum., Comput</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="158" to="167" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A set of 254 Snodgrass-vanderwart pictures standardized for spanish: norms for name agreement, image agreement, familiarity, and visual complexity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Meth. Instrum., Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="537" to="555" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A set of 400 pictures standardised for portuguese: norms for name agreement, familiarity and visual complexity for children and adults</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pompia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bueno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arquivos de Neuro-Psiquiatria</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="330" to="337" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Where do people draw lines?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Golovinskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Limpaecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An improved histogram of edge local orientations for sketch-based image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bustos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM-Symposium</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Kuijper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6376</biblScope>
			<biblScope unit="page" from="432" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Haar</surname></persName>
		</author>
		<idno>UU-CS-2007-015</idno>
		<title level="m">SHREC 2007 3D Retrieval Contest</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Department of Information and Computing Sciences, Utrecht University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">3D model retrieval using hybrid features and class information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Johan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="821" to="846" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Ranking on cross-domain manifold for sketch-based 3D model retrieval</title>
		<author>
			<persName><forename type="first">T</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
			<date type="published" when="2004">2004</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Extremely randomized trees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sketch-based 3D model retrieval by viewpoint entropybased adaptive view clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Johan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DOR, Eurographics Association</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Castellani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Biasotti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Godil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Sketch-based retrieval of vector drawings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jorge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sketch-based Interfaces and Modeling</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="181" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Sketch-based retrieval of drawings using spatial proximity</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M A</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Lang. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="69" to="80" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">3D model search engine based on Lightfield descriptors</title>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2003">2003, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Algorithms for reporting and counting geometric intersections</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ottmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="643" to="647" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">An efficient cycle vector space algorithm for listing all cycles of a planar graph</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Syslo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="797" to="808" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A polynomial-time algorithm to find the shortest cycle basis of a graph</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Horton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="358" to="366" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Reasoning about gradual changes of topological relationships</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Egenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Al-Taha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatio-Temporal Reasoning</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">U</forename><surname>Frank</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Campari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Formentini</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">639</biblScope>
			<biblScope unit="page" from="196" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">CALI: An online scribble recognizer for calligraphic interfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jorge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 AAAI Spring Symposium Sketch Understanding</title>
		<meeting>the 2002 AAAI Spring Symposium Sketch Understanding</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">An integrated approach to shape based image retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Asian Conference on Computer Vision (ACCV 2002)</title>
		<meeting>the 5th Asian Conference on Computer Vision (ACCV 2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="652" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Theory and Applications</title>
		<meeting>the International Conference on Computer Vision Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Shadowdraw: real-time user guidance for freehand drawing</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Foundations of Multidimensional and Metric Data Structures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan Kaufman Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Similarity search: the metric space approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dohnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Database Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
