<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Audio-Visual Speech Modeling for Continuous Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Stéphane</forename><surname>Dupont</surname></persName>
							<email>dupont@tcts.fpms.ac.be</email>
						</author>
						<author>
							<persName><forename type="first">Juergen</forename><surname>Luettin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">International Computer Science Institute</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Maegenwil</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Audio-Visual Speech Modeling for Continuous Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">received August 31, 1999; revised June 19, 2000. received the Ph.D. degree in electronic and engineering from the of Sheffield, U.K. He joined IDIAP, Martigny, Switzerland, in 1996 as a Research Assistant and became head of the computer vision group in 1997. He was a Visiting Researcher at the Center for Language and Speech</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Joint audio-video sensor integration</term>
					<term>multistream hidden Markov models</term>
					<term>speech recognition</term>
					<term>visual feature extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a speech recognition system that uses both acoustic and visual speech information to improve the recognition performance in noisy environments. The system consists of three components: 1) a visual module; 2) an acoustic module; and 3) a sensor fusion module. The visual module locates and tracks the lip movements of a given speaker and extracts relevant speech features. This task is performed with an appearance-based lip model that is learned from example images. Visual speech features are represented by contour information of the lips and grey-level information of the mouth area. The acoustic module extracts noise-robust features from the audio signal. Finally, the sensor fusion module is responsible for the joint temporal modeling of the acoustic and visual feature streams and is realized using multistream hidden Markov models (HMMs). The multistream method allows the definition of different temporal topologies and levels of stream integration and hence enables the modeling of temporal dependencies more accurately than traditional approaches. We present two different methods to learn the asynchrony between the two modalities and how to incorporate them in the multistream models. The superior performance for the proposed system is demonstrated on a large multispeaker database of continuously spoken digits. On a recognition task at 15 dB acoustic signal-to-noise ratio (SNR), acoustic perceptual linear prediction (PLP) features lead to 56% error rate, noise robust RASTA-PLP (Relative Spectra) acoustic features to 7.2% error rate and combined noise robust acoustic features and visual features to 2.5% error rate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and deaf persons make extensive use of visual speech cues and some few individuals perform lip-reading to such a degree that enables almost perfect speech perception <ref type="bibr" target="#b0">[1]</ref>. It is well known that seeing the talker's face in addition to hearing his voice can improve speech intelligibility, particularly in noisy environments <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. The main advantage of the visual signal is its complementarity to the acoustic signal <ref type="bibr" target="#b3">[4]</ref>. Phonemes that are most difficult to perceive in the presence of noise are easier to distinguish visually and vice versa. The visual signal contains that kind of information that is acoustically most sensitive to noise <ref type="bibr" target="#b0">[1]</ref>. Studies have also shown that visual information leads to more accurate speech perception even in noise-free environments <ref type="bibr" target="#b4">[5]</ref>. The strong influence of visual speech cues on human speech perception is demonstrated by the McGurk effect <ref type="bibr" target="#b5">[6]</ref> in which, for example, a person hearing an audio recording of /baba/ and seeing the synchronised video of a person saying /dada/ often resulted in perceiving /gaga/.</p><p>Automatic speech recognition (ASR) has been an active research area for several decades, but in spite of the enormous efforts, the performance of current ASR systems is far from the performance achieved by humans: error rates are often one order of magnitude apart <ref type="bibr" target="#b6">[7]</ref>. Most state-of-the-art ASR systems make use of the acoustic signal only and ignore visual speech cues. They are therefore susceptible to acoustic noise <ref type="bibr" target="#b7">[8]</ref>, and essentially all real-world applications are subject to some kind of noise. Much research effort in ASR has therefore been directed toward systems for noisy speech environments and the robustness of speech recognition systems has been identified as one of the biggest challenges in future research <ref type="bibr" target="#b8">[9]</ref>.</p><p>In this paper, we focus on audiovisual feature extraction, modeling, and sensor integration, for noise-robust ASR. Lip-tracking is performed using a model-based image search. Visual features are then extracted from the lip contours and from the mouth region intensity. This is discussed in Section II. Features from the audio signal are obtained using an acoustic front-end based on the perceptual linear prediction (PLP) or on the noise-robust J-RASTA-PLP (relative spectra) methods. In Section III, we tackle the problem of integrating the information obtained from the visual and acoustic front-ends. We are interested in the possibly decoupled dynamics of the two modalities. Both are modeled using hidden Markov models (HMMs) and the joint interaction of visual and acoustic HMMs is realized using multistream topologies. Section IV describes our acoustic, visual, and audio-visual speech recognition systems. Finally, results on a multispeaker digit strings recognition task are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. VISUAL SPEECH FEATURE EXTRACTION</head><p>Facial feature extraction is a difficult problem due to large appearance differences across persons and due to appearance variability during speech production. Different illumination conditions and different face positions cause further difficulties in image analysis. For a real-world application, whether it is in a car, an office or a factory, the system should be able to deal with these kinds of image variability.</p><p>The main approaches for extracting visual speech information from image sequences can be grouped into the following approaches:</p><p>1) image-based;</p><p>2) visual-motion-based;</p><p>3) geometric-feature-based; and 4) model-based. In the image-based approach [10]- <ref type="bibr" target="#b12">[13]</ref>, the grey-level image containing the mouth is either used directly or after some image transform as feature vector whereas the visual-motion-based method <ref type="bibr" target="#b13">[14]</ref> assumes that visual motion during speech production contains relevant speech information. Geometric-featurebased techniques <ref type="bibr" target="#b14">[15]</ref>, on the other hand, assume that certain measures such as the height or width of the mouth opening are important features. Finally, in the model-based approach [16]- <ref type="bibr" target="#b17">[18]</ref>, a model of the visible speech articulators, usually the lip contours, is built and its configuration is described by a small set of parameters. The advantage of the latter approach is that important features can be represented in a low-dimensional space and can often be made invariant to image transforms like translation, scaling, rotation and lighting. A disadvantage is that the particular model used may not consider all relevant speech information. The main difficulty in the model-based approach is the definition of the model and the development of image search procedures that accurately find the correspondence between the model and the image.</p><p>The system presented here falls into the category of model-based feature extraction. We have used an appearance-based model of the visual articulators <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b16">[17]</ref>: point distribution models <ref type="bibr" target="#b18">[19]</ref> are used to track the lips and to extract relevant speech features from each image. Psychological studies suggest that the inner and outer lip contours are important visual speech features. The shape parameters obtained from the tracking results are therefore used as features for the speech recognition system. Lip shape information provides only part of the visual speech information. Other information is contained in the visibility of teeth and tongue, protrusion, and finer details. We therefore also extract intensity information from the mouth area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shape Modeling</head><p>The lip shape is represented by the coordinates of a point distribution model, outlining the inner and outer lip contours:</p><p>where are the coordinates of the th point . A shape is approximated by a weighted sum of basis shapes which are obtained by a Karhunen-Loéve expansion (1) where denotes the mean shape vector, the matrix of the first column eigenvectors corresponding to the largest eigenvalues and a vector containing the weights for the eigenvectors, computed for the covariance matrix of a representative set of example images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Intensity Modeling</head><p>Intensity modeling serves two purposes: firstly, it is used as a mean for a robust image representation to be used for image search in locating and tracking lips; secondly, it provides visual linguistic features for speech recognition. We therefore need to define dominant image features of the lip contours that we try to match with a certain representation of our model, but which also carry important speech information. Our approach to this problem is as follows. One-dimensional grey-level profiles of length are sampled perpendicular to the contour and centered at point , as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The profiles of all model points are concatenated to construct a global profile vector of dimension . Similar to shape modeling, the intensity vector can be approximated by a weighted sum of basis intensities by the K-L expansion using <ref type="bibr" target="#b1">(2)</ref> where denotes the mean intensity vector, the matrix of the first column eigenvectors corresponding to the largest eigenvalues and a vector containing the weights for each eigenvector. This approach is related to the local grey-level models described in <ref type="bibr" target="#b19">[20]</ref> and to the eigen-lips reported in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Search</head><p>The task of image search is to localize and track the lips in the image and to extract shape and intensity features. We define image search as finding the shape weight vector of the model that maximizes the posterior probability (MAP) of the model given the observed image <ref type="bibr" target="#b2">(3)</ref> is independent of and can therefore be ignored in the calculation of . We assume equal prior shape probabilities within certain limits (e.g., 3 standard deviations)</p><p>and zero probability otherwise. This reduces the MAP to the likelihood function which is defined as <ref type="bibr" target="#b3">(4)</ref> where the intensity weight vector can be obtained using <ref type="bibr" target="#b4">(5)</ref> and where represents the intensity profile of the image corresponding to the model configuration . The intensity weight vector is constrained to stay within certain limits (e.g., 3 standard deviations), assuming equal prior intensity probabilities within these limits.</p><p>The Downhill Simplex Method <ref type="bibr" target="#b20">[21]</ref> is applied to find a minimum of the cost function. We assume that a coarse estimate of the mouth location is given for the first image of a sequence to initialize the search process, for example by a face detection algorithm. Subsequent frames are processed by using the previous search results to initialize the Downhill Simplex Method. The obtained shape weight vector and intensity weight vector obtained from image search are used as visual feature vectors.</p><p>The accuracy of the lip-tracking algorithm might be estimated by comparing the results with the correct coordinates of the lip countour. These coordinates are however not available and might only be obtained by hand-labeling which is a very labourious and subjective task. Instead of evaluating the tracking performance separately we only evaluate the combined performance of lip-tracking, visual feature extraction, and visual speech modeling using the visual speech recognition performance. Experiments where lip-tracking performance has been evaluated separately can be found in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Much visual speech information is contained in the dynamics of lip movements rather than the actual shape or intensity. Furthermore, dynamic information is likely to be more robust to extra-linguistic variability, i.e., intensity values of the lips and skin will remain fairly constant during speech, while intensity values of the mouth opening will vary during speech. On the other hand, intensity values of the lips and skin will vary between speakers, but temporal intensity changes might be similar for different speakers and robust to illumination. Similar comparisons can be made with shape parameters. Dynamic parameters parameters) of the shape and intensity vectors were therefore used as additional features.</p><p>The feature extraction method described here has been compared with several image-based approaches (low-pass filtering, principal components analysis, optical flow) by Gray et al. <ref type="bibr" target="#b21">[22]</ref> and was found to outperform all of these methods. It was also found that the performance of image-based approaches can be considerably improved by the use of lip tracking results to normalize the images prior to processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. AUDIO-VISUAL SENSOR INTEGRATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem of Audio-Visual Sensor Integration</head><p>The strong influence of visual stimuli on human speech perception has notably been demonstrated by the McGurk effect <ref type="bibr" target="#b5">[6]</ref>. How humans integrate visual and acoustic information is not well understood. Several models for human integration have been proposed in the literature. They can be divided into early integration (EI) and late integration (LI) models <ref type="bibr" target="#b0">[1]</ref>. In the EI model, integration is performed in the feature space to form a composite feature vector of acoustic and visual features. Classification is based on this composite feature vector. The model makes the assumption of conditional dependence between the modes and is therefore more general than the LI model. It can furthermore account for temporal dependencies between the modes, such as the voice-onset-time<ref type="foot" target="#foot_0">2</ref> (VOT), which are important for the discrimination of certain phonemes. In the LI model, each modality is first pre-classified independently of each other. The final classification is based on the fusion of the outputs of both modalities by estimating their joint occurrence. In comparison with the early integration scheme, this method assumes that both data streams are conditionally independent. Furthermore, temporal information between the channels is lost in this approach. Audio-visual speech recognition (AVSR) systems based on EI models have, for example, been described in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b22">[23]</ref> and systems based on LI models in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b14">[15]</ref>. Although it is still not well known how humans integrate different modalities, it is generally agreed that integration occurs before speech is categorized phonetically <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b23">[24]</ref>. This conclusion is supported by several studies regarding the VOT perception <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> and the McGurk effect. In acoustic speech perception, on the other hand, there is much evidence that humans perform partial recognition across different acoustic frequency bands <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, which assumes conditional independence across bands. The auditory system seems to perform partial recognition which is independent across channels, whereas audio-visual perception seems to be based on early integration, which assumes conditional dependence between both modalities. These two hypotheses are controversial since the audio-visual theory of early integration assumes that no partial categorization is made prior to the integration of both modalities.</p><p>The approach described here follows Fletcher's theory of conditional independence <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, but it also allows the modeling of different levels of synchrony/asynchrony between the streams and can therefore account for some temporal dependencies, which otherwise can only be modeled by an EI integration model. Tomlinson et al. <ref type="bibr" target="#b22">[23]</ref> have addressed the issue of asynchrony between the visual and acoustic streams. Under the independence assumption, composite models were defined from independently trained audio and visual models. Although our work is related with <ref type="bibr" target="#b22">[23]</ref>, we propose different strategies for modeling (learning) the asynchrony between the two streams.</p><p>The bimodal speech signal can be considered as an observation vector consisting of acoustic and visual features. According to Bayesian decision theory, the maximum a posteriori probability classifier (MAP) is denoted by ( <ref type="formula">6</ref>) where represents a particular word string, represents the sequence of acoustic feature vectors, and represents the sequence of visual feature vectors. If the two modalities are independent, the likelihood becomes . In this work, the modalities are assumed to be independent although certain temporal constraints and reliability weights are introduced.</p><p>Previous AVSR systems based on conditional independence have essentially addressed the problem of isolated word recognition. Most of these contributions were mainly focused on finding an appropriate automatic weighting scheme so as to guarantee good performance in a wide range of acoustic SNRs. Compared to isolated word recognition, the problem of continuous speech recognition is more tricky. Waiting until the end of the spoken utterance before combining the streams, as in the LI integration model, introduces an undesirable time delay. As the best hypothesis using the acoustic information is not necessarily the same as the best hypothesis using the visual information, it also requires to generate N-best hypothesis lists for the two streams. Identical hypotheses must indeed be matched to combine the scores from the two streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multistream Model</head><p>The multistream approach, proposed in this work, does not require the use of such an N-best scheme. As we will show, it is an interesting candidate for multimodal continuous speech recognition as it allows for the following:</p><p>1) synchronous multimodal continuous speech recognition;</p><p>2) asynchrony of the visual and acoustic streams with the possibility to define phonological resynchronization points; 3) specific audio and video word or sub-word models; and 4) asynchrony patterns modeling. 1) Model for Decoupled Dynamics: The multistream approach <ref type="bibr" target="#b28">[29]</ref> used in this work is a principled way for merging different sources of information using cooperative HMMs (see <ref type="bibr" target="#b29">[30]</ref>). If the streams are supposed to be entirely synchronous and represented by HMMs with the same topologies, they may be accommodated simply. However, it is often the case that the streams are not synchronous, that they do not even have the same frame rate and it might be necessary to define models that do not have the same topology. The multistream approach allows to deal with this. In this framework, the input streams are pro-cessed independently of each other (using HMMs) up to certain anchor-points where they have to synchronize and combine their partial segment-based likelihoods. While the phonological level of score combination has to be defined a priori, the optimal temporal anchor-points are obtained automatically during recognition.</p><p>This structure is meant for processes that evolve independently, i.e., streams that have somewhat decoupled dynamics. With the early integration approach (see Section III-A), several feature vectors are combined into a single feature vector. If the generating processes are only loosely coupled, as it could be assumed for articulatory movements, lip movements and vocal folds movements, this increases the variance of the statistical models, hence reducing the performance of a recognition system. With the LI strategy however, different statistical models are defined for the different feature vectors. Moreover, modality reliability can easily be introduced in this LI approach. Multistream uses the same assumptions but additionally introduces stream synchronization at relevant phonological transitions points, between phonemes, syllable or words for instance.</p><p>An observation sequence , representing the utterance to be recognized, is assumed to be composed of input streams (possibly with different frame rates). A hypothesized model associated with is built by concatenating sub-unit models associated with the phonological level at which we want to perform the synchronization of the input streams (e.g., phonemes, syllables, words . To allow the processing of each of the input streams independently of each other up to the pre-defined sub-unit boundaries, each sub-unit model is composed of parallel HMMs (possibly with different topologies). These HMMs are forced to combine their respective segmental scores<ref type="foot" target="#foot_1">3</ref> at the synchronization points. The resulting model is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. <ref type="foot" target="#foot_2">4</ref> In this model, we note that:</p><p>1) the parallel HMMs associated with each of the input streams do not necessarily have the same topology; and 2) the synchronization anchor-point in Fig. <ref type="figure" target="#fig_1">2</ref>) is not a regular HMM state but combines the scores accumulated over the same temporal segment for all the streams.  2) Composite Model Formulation: Recognition using the Viterbi approximation <ref type="bibr" target="#b29">[30]</ref> appears to be a continuous speech decoding problem where all of the concurrent word segmentations, as well as all of the HMM state segmentations, must be hypothesized. However, as combination of the scores concerns sub-unit paths that must begin at the same time due to the synchrony constraints, and as the best sub-unit state paths are not the same for all of the streams (even if the model topologies are the same), it is necessary to keep track of the dynamic programming paths for all of the sub-unit starting points. Hence, an approach such as the asynchronous two-level dynamic programming <ref type="bibr" target="#b32">[33]</ref>, or a synchronous formulation of it, is required.</p><p>Alternatively, we can define composite HMMs <ref type="bibr" target="#b33">[34]</ref> where each state is built by merging a -tuple of states from the stream HMMs. The topology of this composite model is defined so as to represent all the possible state paths given the initial HMM topologies. The local scores associated with the composite states are computed as a combination of the local stream scores (see Section III-B4). This model allows to implement independent search within sub-units as well as intra-units synchrony constraints. Fig. <ref type="figure" target="#fig_2">3</ref> shows the composite model obtained from the multistream topology in Fig. <ref type="figure" target="#fig_3">4</ref>. This strategy was used in this study.</p><p>3) Psychoacoustic Motivations for Multistream: The human integration mechanism seems to be robust to small temporal asynchronies between information streams. In <ref type="bibr" target="#b34">[35]</ref>, a speech signal is partitioned into 19 quarter-octave frequency bands. These frequency channels are then randomly shifted in time according to a uniform distribution ranging from 0 to a maximum delay . Speech intelligibility experiments show that the word accuracy declines progressively as increases. However, it is still above 75% for a strong 140 ms asynchrony condition, although the mean duration of the phonetic segments is 72 ms. It is expected that standard phone-based HMM systems would fail in such conditions.</p><p>In the audio-visual field, experiments in <ref type="bibr" target="#b35">[36]</ref> introduced systematic asynchronies between the audio and video information sources. These intelligibility experiments, based on /ba/, /da/,/i/ and /u/ stimuli, indicated that the integration process is relatively robust for asynchronies up to 200 ms. Results by Smeele <ref type="bibr" target="#b36">[37]</ref> showed that audio-visual intelligibility of CVC stimuli does not degrade for asynchronies of up to 80 ms. The multistream approach proposed here might also provide a robust framework with respect to such asynchronies.</p><p>4) Stream Combination: Similar to the LI scheme, the multistream approach requires a formulation to combine the information of the two streams. In our case, this is done at each anchor-point. Combination of the independent likelihoods is done by multiplying the segment likelihoods from the two streams, thus assuming conditional independence of the visual and acoustic streams. This was done according to <ref type="bibr" target="#b6">(7)</ref> The weighting factor represents the reliability of the two modalities. It generally depends on the performance obtained by each modality and on the presence of acoustic or visual noise. Here, we estimate the optimal weighting factor on the development set which is subject to the same noise as the test set. The method used for final experiments however was to automatically estimate the acoustic SNR from the test data and to adjust the weighting factor accordingly. It can be observed empirically that the optimal weight is related almost linearly to the SNR ratio. For our best system (see Section IV), the correlation coefficient between the SNR and the optimal weight is 0.99, and the weight can be estimated using the following empirical linear regression SNR(dB) + 0.512, that is valid from 5 dB to clean speech. With clean speech (SNR 30 dB), the weighting factor ( 0.78) strongly favors the acoustic information. As the acoustic SNR decreases however, the visual information can become almost as important as the acoustic information ( = 0.56 when SNR = 5 dB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Multistream</head><p>Training: Given a combination weight, the multistream system parameters can be estimated using traditional (Viterbi-based) maximum likelihood techniques <ref type="bibr" target="#b29">[30]</ref> applied to HMM systems, or to HMM systems using artificial neural networks (ANNs) as HMM state probability estimators <ref type="bibr" target="#b37">[38]</ref>, as was done in this work.</p><p>As was shown in <ref type="bibr" target="#b38">[39]</ref>, maximum-likelihood estimation of the combination weight fails. Indeed, maximizing the likelihood with respect to the weighting factor yields to the selection of the modality with the highest likelihood (hence or ). The authors also show that additional constraints on the weight can yield to a satisfactory solution. Alternatively, generalized probabilitic descent (GPD) training using a minimumclassification-error criterion can also be used. In this work, we estimate the stream combination weight using a true word-level classification-error criterion, based on development data. This estimation step is performed at each iteration of the Viterbibased maximum-likelihood estimation algorithm.</p><p>It is important to note that the multistream parameter optimization procedure used here is different than the optimization of two single-stream systems independently, as proposed in <ref type="bibr" target="#b38">[39]</ref>. The multistream model topologies, particularly the synchronization anchor-points, introduce additional constraints in the forced alignment of the training data. These constraints can be important to "correct" the alignments of the visual stream. We observed in this study that, without such constraints, the alignments fail for a significant amount of the training: HMM states of particular words were sometimes shown to be aligned on signal portions pertaining to other words. In some cases, single HMM states were also shown to overlap several adjacent words.</p><p>The transition probabilities of a multistream model can be different over the two modalities, and this is reflected on the transition probabilities of the composite model used during decoding. These transition probabilities estimated jointly with the parameters representing the emission probabilities, using the standard maximum likelihood approach. Transition probabilities are often omitted when modeling speech processes, the observation likelihood being dominated by the emission probabilities. The fact is that the standard geometric duration model is not accurate and also that the transition probabilities do not help in the case of matched train/test conditions. Using accurate duration modeling techniques, it has been shown that transition probabilities can improve performance in the case of noisy speech <ref type="bibr" target="#b39">[40]</ref>. As explained in the next section, transition probabilities were used here for the modeling of duration and asynchrony patterns.</p><p>6) Synchrony/Asynchrony Modeling: Whereas HMMs are mainly used to model a single or several dependent processes, multistream models can be used for processes that evolve independently within predefined anchor-points (transitions between lexical sub-units) where the processes are assumed to resynchronize. However, the definition of these anchor-points is not obvious as the multiple stream dynamics is not known a priori. Moreover, it is very likely that many problems will be characterized by vector streams coming from processes that are neither dependent, nor completely decoupled, leading to tight or loose synchrony, probably depending on the model states. Finally, some processes could also lead to synchrony/asynchrony patterns, one of the streams being in advance, or systematically delayed, with respect to the other streams. Such asynchrony phenomena could be learned to improve the modeling accuracy.</p><p>Two approaches have been investigated here. The first one consist of static state pruning. This was done by pruning the multidimensional models by removing the least frequently visited states (based on the prior probabilities of these states).</p><p>The second approach for stream synchrony/asynchrony learning consists of modeling the asynchrony patterns resulting from the multistream processes. This was done by explicit modeling of the state durations and transition probabilities of the multidimensional models. Indeed, in the composite HMM, the off-diagonal state (Fig. <ref type="figure" target="#fig_2">3</ref>) durations correspond to the stream asynchrony delay and the transition probabilities to these states represent their associated probabilities. Explicit modeling of the synchronous and asynchronous state durations was used here to take advantage of the particular structure of state transitions. Viterbi decoding allows to use a particular HMM topology for duration modeling. We implement duration modeling by a chain of identical HMM states and a set of transition probabilities as shown in Fig. <ref type="figure" target="#fig_4">5</ref>. The topology is characterized by a maximum length after which the duration model simply becomes exponentially decaying, due to the loop on the last state. Each of the states of the multidimensional model is replaced by a similar topology and the transition probabilities are updated using a counting procedure on a forced Viterbi alignment of the training data. Other duration modeling techniques could also be used <ref type="bibr" target="#b39">[40]</ref>.</p><p>In the presence of noise, the prior information represented by the duration models might be of significant importance. It should be noted however that this model can only be applied at the expense of an important increase of the computational requirements. For a model of duration , each transition of the one state model is replaced by transitions toward the following HMM models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SPEECH RECOGNITION EXPERIMENTS</head><p>The M2VTS audio-visual database <ref type="bibr" target="#b40">[41]</ref> was used for all experiments. It contains 185 recordings of 37 subjects (12 females and 25 males). Each recording contains the acoustic and the video signal of the continuously pronounced French digits from zero to nine. Five recordings have been taken of each speaker, at one week intervals to account for minor face changes like beards. The video sequences consist of 286 360 pixel color images with a 25 Hz frame rate and the audio track was recorded at a 48 kHz sampling frequency and 16 bit PCM coding. The database contains a total of over 27 000 color images which were converted to grey-level images for the experiments reported here.</p><p>Although the M2VTS database is one of the largest databases of its type, it is still relatively small compared to reference audio databases used in the field of speech recognition. To increase the significance level of our experiments, we used a jack-knife approach. Five different cuts of the database were used. Each cut consisted of:</p><p>1) three pronunciations from the 37 speakers as training set;</p><p>2) one pronunciation from the 37 speakers as development set; and 3) one pronunciation from the 37 speakers as test set. The development set was used to optimize the audio-visual weighting exponent. This procedure allowed to use the whole database as test set (185 utterances) by performing independent experiments for each of the five cuts. The task could be qualified as multispeaker continuous digits speech recognition.</p><p>We note here that the digit sequence to be recognized is always the same (digits from "0" to "9"). This somewhat simplifies the task of the speech recognition system which always "sees" the pronounced words in the same context. Moreover, during recognition, only the hypothesis with the correct number of digits (ten digits) were considered. This choice was made to avoid the need to optimize word entrance penalties.</p><p>Although highly constrained, this task remains a true continuous speech recognition task. Such constrained problems are often used to evaluate features and acoustic models whereas large vocabulary tasks are mainly used to evaluate language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Acoustic Speech Recognition</head><p>The audio stream was first downsampled to 8 kHz. We used PLP parameters <ref type="bibr" target="#b41">[42]</ref> computed every 10 ms on 30 ms sample frames. The complete feature vectors consisted of 25 parameters: 12 PLP coefficients, the first temporal derivatives <ref type="bibr" target="#b42">[43]</ref> of these coefficients (12 PLP) and the energy.</p><p>We used left-right digit HMM models with between three and nine independent states, depending on the digit mean duration. This yielded a total of 52 states, including a standard HMM state representing the silence. The digit sequences were first segmented into digits using standard Viterbi alignment with a recognizer trained on the SWISS-FRENCH POLYPHONE database <ref type="bibr" target="#b43">[44]</ref> of 5000 speakers. Each M2VTS digit was then linearly segmented according to the number of states of the corresponding HMM model. This initial segmentation was used to train the HMM-state statistical models using an artificial neural network as HMM state probability estimator <ref type="bibr" target="#b37">[38]</ref>. We used a feed-forward Multilayer Perceptron (MLP) trained with speech features at its input to generate HMM state posterior probabilities. Nine adjacent frames of acoustic features were used at the input of the MLP. This allows to model local time correlation and was shown to improve classification performance <ref type="bibr" target="#b37">[38]</ref>. Back-propagation was used to adapt the MLP weights using a gradient descent algorithm. A neural network with 150 hidden units was used. Increasing the number of hidden neurons did not yield any performance improvement.</p><p>System training and tests were then performed according to the database partitioning described earlier. Results are summarized in Tables I and II for speech corrupted by stationary Gaussian white noise with different SNRs. <ref type="foot" target="#foot_3">5</ref> Comparing Table <ref type="table" target="#tab_0">I</ref> with Table <ref type="table" target="#tab_1">II</ref>, we can observe that the recognition performance is severely affected by additive noise, even at such moderate noise levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual Speech Recognition</head><p>The most dominant 12 shape features and 12 intensity features, described earlier, were used for the recognizer. These features were complemented by 24 temporal derivatives. We used the same HMM topologies and the same initial segmentation as for the previously described acoustic-based recognition system. In this case, the MLP had 70 hidden units.</p><p>The mean error rate for the five database cuts defined earlier was 40.3%. Since the visual signal only provides partial infor-  mation, the error rate for the video-based system was considerably higher than for the audio-based system. is mainly due to the high visual similarity of certain digits like "quatre," "cinq," "six," and "sept."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Audio-Visual Speech Recognition</head><p>Audio-visual speech recognition was experimentally investigated. Fig. <ref type="figure" target="#fig_5">6</ref> illustrates the audio-visual system architecture. Three kinds of model topologies were compared. These were based on the HMM word topologies already used in the previous sections. The differences between the models laid notably in the possible asynchrony of the visual stream with respect to the acoustic stream.</p><p>The first model (MODEL 0) was based on early integration. Acoustic and visual features were used as input to a single MLP with 150 hidden units.</p><p>The second model (MODEL 1) corresponds to a multistream model with combination at the state level and allows to use fusion criteria that can weight differently the two streams according to their respective reliability. However, it did not allow for any asynchrony between the two streams.</p><p>The third model (MODEL 2) was a multistream model with combination of the streams at the word level (Fig. <ref type="figure" target="#fig_3">4</ref>). This model thus allows the dynamic programming paths to be independent from the beginning up to the end of the words. In this work, the asynchrony was constrained to a difference of one state between the two modalities. In the example of Fig. <ref type="figure" target="#fig_2">3</ref>, the following states are not allowed: , , , , , and . This model also allows the transition from silence to speech and from speech to silence to occur at different time instants for the two streams. <ref type="foot" target="#foot_4">6</ref> Lip movement can occur before and after sound production and conversely. Fig. <ref type="figure" target="#fig_6">7</ref> shows in parallel a  speech spectrogram as well as the evolution of the first visual shape parameter, mainly representing the changes in the position of the lower lip contour <ref type="bibr" target="#b16">[17]</ref>. From this figure, as well as from studying the asynchrony of the streams using asynchrony lag histograms (see next section and Fig. <ref type="figure" target="#fig_7">8</ref>), it can clearly be seen that the two signals are partially in synchrony and partially asynchronous. Ideally, we would like to have a model which forces the streams to be synchronous where synchrony occurs and asynchronous where the signals are typically in asynchrony. This will be studied in the next section.</p><p>We used the same parameterization schemes as in the two previous sections. However, as the visual frame rate (25 Hz) is a quarter of the acoustic frame rate, visual vectors were copied (by copying frames), so that modalities are synchronously available.</p><p>Results are summarized in Tables I and II (PLP column). The optimal weighting factor was estimated on the development set which is subject to the same noise as the test set. In the case of clean speech, using visual information, in addition to the acoustics, does not yield significant performance improvement at .05. In the case of speech corrupted with noise, significant performance improvement can be obtained by using the visual stream as an additional information source. The slight improvement of MODEL 2 (compared to MODEL 1) is not significant. <ref type="foot" target="#foot_5">7</ref>Finally, the early integration approach yields inferior results. The gain of combination weight adaptation seems to surpass the possible loss due to the independence assumption. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Learning the Asynchrony Between the Streams</head><p>The asynchrony learning approaches proposed in Section III-B6 were then applied. A forced Viterbi alignment of the training data was obtained using MODEL 2. Composite model state priors were computed using this alignment and the states with small prior probability were removed from the model (synchronized states were always kept however). We call this approach static pruning. The number of remaining states is optimized to get the best word recognition performance on the cross-validation set. Interestingly, the best models contained 25 off-diagonal states (asynchronized states) in addition to the 52 synchronized states. Speech recognition results on the test set are finally obtained using these simplified composite models. As can be seen in Table <ref type="table" target="#tab_1">II</ref> (PLP column), they performed significantly better 8 (16.1% word error rate) than both state-resynchronization models (18.8%) and word-resynchronization model (17.9%).</p><p>As proposed in Section III-B6, duration models based on the composite models have also been developed. In this case, the states of the multidimensional composite model (Fig. <ref type="figure" target="#fig_2">3</ref>) are replaced by particular HMM topologies (Fig. <ref type="figure" target="#fig_4">5</ref>) which aim is to model the composite state durations. The length of these HMMs was set to 20 states, hence allowing an accurate duration modeling up to 20 frames. A self-loop on the last state allows longer durations with an exponentially decaying probability.</p><p>Transition delays were measured for the M2VTS database on a forced Viterbi alignment of the training data. MODEL 2 was used to obtain the alignment. Then, histograms of the transition delays were drawn. Observation of these histograms shows us that some are relatively narrow while other are very wide. Moreover, the transition delay mean is not always close to zero. Further observation even shows that some histograms are significantly shifted toward the positive values, indicating a pat-8 according to a bilateral hypothesis test with p &lt; .05 and knowing that the test set contains 5 2 1850 words because five noise conditions have been used.</p><p>tern where the visual transition is generally delayed compared to the acoustic transition, some other are shifted toward the negative values (see Fig. <ref type="figure" target="#fig_7">8</ref>). These observations tend to indicate that the transition delays are not only the product of alignment noise (as was hypothesized in <ref type="bibr" target="#b44">[45]</ref> for streams based on different frequency bands) but also reflect some structure of the audio-visual asynchrony patterns that could be useful for speech recognition. The analysis of the transition delay distributions, which is however out of the scope of this paper, might also provide some insight into the speech production mechanism.</p><p>With this approach, the duration models, which were introduced to model the asynchrony patterns, are also modeling the state durations. Comparing it with a standard HMM model without duration modeling is unfair. The same kind of duration modeling was thus used in additional experiments with the purely synchronized (standard HMM) topologies. It was also applied to the best statically pruned composite models obtained with the previously described procedure. We observed that duration modeling significantly improves the noise robustness of all kinds of models (see Table <ref type="table" target="#tab_1">II</ref>, PLP column).</p><p>For the systems without duration modeling, MODEL 2 simplified using static pruning performs significantly better than the other models. This suggest that allowing stream asynchronies, with asynchrony patterns learned in the form of multidimensional topologies can yield improved noise robustness. For the systems using duration modeling however, the results are not significantly different across the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Noise Robust Features</head><p>To allow a fair comparison with noise robust acoustic methods, this set of experiments was repeated using noise robust J-RASTA-PLP <ref type="bibr" target="#b45">[46]</ref> features for the audio stream. Comparing the results in Table <ref type="table" target="#tab_1">II</ref> shows the improved robustness of this kind of features over the PLP parameters. Moreover, using both information sources also results in an important leap forward in terms of robustness. Using decoding schemes allowing for stream asynchronies, even when using asynchrony modeling techniques, did not yield any performance gain (the results are not significantly different across the models). Results concerning asynchrony modeling are thus mitigated. Let us, however, emphasize here a single striking results from these experiments. At 15 dB SNR, PLP features lead to 56.3% error rate, J-RASTA-PLP features lead to 7.2% error rate, and using lip features in addition lead to 2.5% error rate (see Fig. <ref type="figure">9</ref>).</p><p>Finally, speech was corrupted by a highly nonstationary noise from the Madras <ref type="bibr" target="#b46">[47]</ref> database (moving cars recorded along a motorway). This noise was used because it is more realistic and more difficult to estimate than stationary white noise. The noise level was estimated using the automatic method described in <ref type="bibr" target="#b47">[48]</ref>. The technique was shown to yield a 7.6 dB mean square error on this kind of noise. Linear regression was used to dynamically adjust the stream combination weight, according to the estimated SNR and to the optimal weights resulting from the previous experiments on stationary white noise. Results are presented in Table <ref type="table" target="#tab_1">III</ref>. Here again, the main conclusion of these results is the important gain resulting from audio-visual integration. Fig. <ref type="figure">9</ref>. Word error rate for digit string recognition with number of digits known a priori, under various acoustic SNRs. The noise was a stationary Gaussian white noise. The upper most line is for the PLP audio system. The dotted line is for the J-RASTA-PLP audio system. The dashed line is for the early integration system using J-RASTA-PLP and visual features and the last continuous line is for our best audio-visual system using J-RASTA-PLP and visual features. V. CONCLUSIONS</p><p>We have described a complete audio-visual speech recognition system that was tested on a multispeaker continuous digit recognition task for different acoustic noise levels and noise sources.</p><p>We have described an approach based on appearance-based models for robust lip tracking and feature extraction. This method allows robust lip tracking for a broad range of subjects and without the need of lipstick or other visual aids. Visual speech information is compactly represented in the form of shape and intensity parameters. Visual speech recognition experiments have demonstrated that this technique leads to robust multispeaker continuous speech recognition.</p><p>We have presented a framework for the fusion of acoustic and visual information in an audio-visual speech recognition system based on the multistream approach. This provides a way of merging different sources of information using cooperative HMMs. Several significant advances have been achieved using this approach. Firstly, the method enables synchronous audio-visual decoding of continuous speech. Additionally, modality reliability can easily be introduced in the form of adaptive stream weights. It was shown that the gain of weight adaptation for speech recognition in noise is important and surpasses the possible loss due to the independence assumption of our fusion formalism. Finally, the approach allows to model the asynchrony between the two streams. In the case of audio-visual modeling, observations of HMM state alignment for audio and video streams tend to indicate that the transition delays are not only the product of alignment noise but also reflect some structure of the audio-visual asynchrony patterns that could be useful for speech recognition. Experimental results are however mitigated. Stream asynchrony modeling yield significant improvement in terms of noise robustness for ASR system using standard acoustic features whereas no improvement was observed for a system using noise robust features.</p><p>Comparisons with acoustic-only recognition systems show that the audio-visual system significantly reduces the error rate in the presence of noise, even in the case where noise robust acoustic features are used. The benefit of asynchrony modeling remains less conclusive and will be subject to further investigations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Grey-level profile extraction. The grey-level vectors are sampled perpendicular to the lip contour and centred at the model points.</figDesc><graphic url="image-45.png" coords="2,324.60,62.26,207.16,91.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. General form of a K-stream model with anchor-points between speech units, forcing synchrony between the streams.</figDesc><graphic url="image-317.png" coords="4,91.86,62.28,409.68,138.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. HMM topology for composite model built from the multistream model presented in Fig. 4.</figDesc><graphic url="image-373.png" coords="5,51.84,62.28,223.68,148.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Multistream model for audio-visual speech recognition with optional begin and end silence states (MODEL 2). These silence states are standard HMM states.</figDesc><graphic url="image-374.png" coords="5,39.54,253.98,248.16,92.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example of HMM topology for duration modeling. The emission probability model is common across the different states. The duration is encoded within the transition probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Audio-visual ASR system architecture.</figDesc><graphic url="image-465.png" coords="8,75.90,62.24,441.49,117.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Auditory spectrogram (evolution of the critical band energies) and evolution of the first visual shape parameter for one portion ("0" to "8") of an M2VTS utterance.</figDesc><graphic url="image-466.png" coords="8,110.40,224.39,372.44,305.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Transition lags (lips on acoustics) probability distribution. The upper figure is for the first state of the word "trois" and the lower figure is for the third state (out of seven) of "quatre."</figDesc><graphic url="image-469.png" coords="9,39.90,62.28,247.44,202.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig.9. Word error rate for digit string recognition with number of digits known a priori, under various acoustic SNRs. The noise was a stationary Gaussian white noise. The upper most line is for the PLP audio system. The dotted line is for the J-RASTA-PLP audio system. The dashed line is for the early integration system using J-RASTA-PLP and visual features and the last continuous line is for our best audio-visual system using J-RASTA-PLP and visual features. Error bars indicate 61 standard deviation across the five database cuts.</figDesc><graphic url="image-471.png" coords="10,42.30,62.28,245.76,204.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I WORD</head><label>I</label><figDesc>ERROR RATE OF PLP-BASED ACOUSTIC-, VISUAL-, AND ACOUSTIC-VISUAL-BASED (MODEL 1) SPEECH RECOGNITION SYSTEMS ON CLEAN SPEECH. STANDARD DEVIATIONS ACROSS THE FIVE DATABASE CUTS ARE IN BRACKETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II WORD</head><label>II</label><figDesc>ERROR RATE FOR DIGIT STRING RECOGNITION WITH NUMBER OF DIGITS KNOWN A PRIORI, USING SEVERAL KINDS OF ACOUSTIC-VISUAL-BASED SPEECH RECOGNITION SYSTEMS. THESE RESULTS REPRESENT THE MEAN WORD ERROR RATE ACROSS FIVE NOISE CONDITIONS: CLEAN SIGNAL, 20 dB, 15 dB, 10 dB, AND 5 dB SNR. THE NOISE WAS A STATIONARY GAUSSIAN WHITE NOISE. FOR EACH CONDITION, THE COMBINATION WEIGHT WAS OPTIMIZED ON A DEVELOPMENT SET SUBJECT TO THE SAME NOISE AS THE TEST SET. STANDARD DEVIATIONS ACROSS THE FIVE DATABASE CUTS ARE IN BRACKETS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">The time delay between the burst sound, coming from the plosive part of a consonant, and the movement of the vocal folds for the voiced part of a voiced consonant or the subsequent vowel.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">From now on, we will simply refer to likelihoods or probabilities as "scores."</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Different frameworks for more general networks have also been proposed in<ref type="bibr" target="#b30">[31]</ref> and<ref type="bibr" target="#b31">[32]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">In these experiments, the SNR was computed at the sentence level without removing the silence portions of the utterances.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">The "visual silence" state is a standard HMM state.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">This is in contradiction with results in<ref type="bibr" target="#b22">[23]</ref> which were in favor of a resynchronization level allowing asynchrony although it was limited to the phoneme level (phone models being composed of three different states).</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Part of this work was performed in the framework of the ACTS-M2VTS European Project with support from the Swiss Federal Office for Education and Science. The associate editor coordinating the review of this paper and approving it for publication was Dr. Masahiro Iwadare.</p><p>S. Dupont was with the TCTS Laboratory, Mons Polytechnical Institute (FPM's), Mons, Belgium. He is now with the</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lipreading and audio-visual speech perception</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Summerfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. R. Soc. London B</title>
		<imprint>
			<biblScope unit="volume">335</biblScope>
			<biblScope unit="page" from="71" to="78" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual contributions to speech intelligibility in noise</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Sumby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pollak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="212" to="215" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating the articulation index for auditory-visual input</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Braida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2952" to="2960" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Visual Speech and Speaker Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Sheffield, U.K.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Sheffield</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Easy to hear but hard to understand: A lip-reading advantage with intact auditory stimuli</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reisberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hearing by Eye: The Psychology of Lip-Reading</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Dodd</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Campbell</surname></persName>
		</editor>
		<meeting><address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="97" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hearing lips and seeing voices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mcgurk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="746" to="748" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech recognition by machines and humans</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech recognition in noisy environments: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="261" to="291" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The challenge of spoken language processing: Research directions for the nineties</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hirschmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural network models of sensory integration for improved vowel recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Yuhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990-10">Oct. 1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1658" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nonlinear manifold learning for visual speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Omohundro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Computer Vision, Piscataway</title>
				<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="494" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computer lipreading for improved accuracy in automatic speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Silsbee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="337" to="351" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An image transform approach for HMM based automatic lipreading</title>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
				<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="173" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic lipreading by optical flow analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst. Comput. Jpn</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic lipreading to enhance speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Petajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">2D deformable models for visual speech analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Coianiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Capril</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speechreading by Humans and Machines: Models, Systems and Applications</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hennecke</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
	<note>Series F: Computer and Systems Sciences</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speechreading using probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Thacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="178" />
			<date type="published" when="1997-02">Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D modeling and tracking of human lip motion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision</title>
				<meeting>IEEE Int. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995-01">Jan. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic interpretation and coding of face images using flexible models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="743" to="756" />
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simplex method for function optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="308" to="313" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic features for visual speechreading: A systematic comparison</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Petsche</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integrating audio and visual information to provide highly robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Russel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Brooke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech and Signal Processing</title>
				<meeting>IEEE Int. Conf. Acoust. Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="821" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crossmodal integration in the identification of consonants</title>
		<author>
			<persName><forename type="first">L</forename><surname>Braida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q. J. Exp. Psych</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="647" to="677" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Voice-mouth synthesis of tactual/visual perception of /pa, ba, ma</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Erber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>De Filippo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1015" to="1019" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the role of visual rate information in phonetic perception</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="276" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Speech and Hearing in Communication</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fletcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953">1953</date>
			<publisher>Krieger</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How do humans process and recognize speech?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="567" to="577" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sub-band-based speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int.. Conf. Acoustic Speech and Signal Processing</title>
				<meeting>IEEE Int.. Conf. Acoustic Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="1997-04">Apr. 1997</date>
			<biblScope unit="page" from="1251" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
		<title level="m">Fundamentals of Speech Recognition</title>
				<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall Signal Processing Series</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hidden Markov decision trees</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Petsche</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Toward Markov random field modeling of speech</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sigelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Language Processing</title>
				<meeting>Int. Conf. Spoken Language essing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-12">Dec. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two level DP matching-a dynamic time warping based pattern matching algorithm for continuous speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IECE Jpn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hidden Markov model decomposition of speech and noise</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustic Speech and Signal Processing</title>
				<meeting>IEEE Int. Conf. Acoustic Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="845" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Speech intelligibility in the presence of crosschannel spectral asynchrony</title>
		<author>
			<persName><forename type="first">T</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="933" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Perceiving asynchronous bimodal speech in consonant vowel and vowel syllables</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="127" to="134" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Intelligibility of audio-visually desynchronized speech: Asymmetrical effect of phoneme position</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Smeele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Language Processing</title>
				<meeting>Int. Conf. Spoken Language essing<address><addrLine>Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<title level="m">Connectionist Speech Recognition-A Hybrid Approach</title>
				<meeting><address><addrLine>Norwell, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discriminative training of hmm stream exponents for audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustic Speech and Signal Processing</title>
				<meeting>IEEE Int. Conf. Acoustic Speech and Signal essing<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="3733" to="3736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weighted viterbi algorithm and state duration modeling for speech recognition in noise</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Yoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustic Speech and Signal Processing</title>
				<meeting>IEEE Int. Conf. Acoustic Speech and Signal essing<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="709" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The M2VTS multimodal face database (release 1.00)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pigeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandendorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First International Conference on Audioand Video-based Biometric Person Authentication</title>
				<meeting>of the First International Conference on Audioand Video-based Biometric Person Authentication<address><addrLine>Crans-Montana, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990-04">Apr. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Speaker independant isolated word recognizer using dynamic features of speech spectrum</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="59" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Swiss French polyphone and polyvar: Telephone speech databases to model inter and intra-speaker variability</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Cochard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jaboulet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langlais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IDIAP</title>
		<imprint>
			<date type="published" when="1996">Martigny, Switzerland, 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Multi-Band Approach to Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Mirghafori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, Int. Comput. Sci. Inst</title>
		<imprint>
			<date type="published" when="1999-01">Jan. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rasta processing of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">ULg-acoustics laboratory-The MADRAS project</title>
		<author>
			<persName><surname>Ulg</surname></persName>
		</author>
		<ptr target="http://www.montefiore.ulg.ac.be/ser-vices/acous/homelab.html" />
		<imprint>
			<date type="published" when="1998-08">1998. Aug</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Assessing local noise level estimation methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Robust Methods for Speech Recognition in Adverse Conditions</title>
				<meeting><address><addrLine>Nokia, COST249, IEEE) Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="115" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">His research interests include speech processing, neural networks, pattern recognition, and computer music</title>
	</analytic>
	<monogr>
		<title level="m">Stéphane Dupont received the Ph.D. degree in electrical engineering from the Mons Polytechnical Institute</title>
				<meeting><address><addrLine>Mons, Belgium; Martigny, Switzerland; Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">2000. 1996</date>
		</imprint>
	</monogr>
	<note>He is currently a Research Assistant at the International Computer Science Institute. He has authored/coauthored over 20 papers on these topics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
