<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Persuasion Factor of User Decision for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lingrui</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoyi</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinlei</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hengliang</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><surname>Modeling</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Meituan Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Persuasion Factor of User Decision for Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539114</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>User Persuasion Factor</term>
					<term>Recommender Systems</term>
					<term>Graph Neural Networks</term>
					<term>Counterfactual Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In online information systems, users make decisions based on factors of several specific aspects, such as brand, price, etc. Existing recommendation engines ignore the explicit modeling of these factors, leading to sub-optimal recommendation performance. In this paper, we focus on the real-world scenario where these factors can be explicitly captured (the users are exposed with decision factor-based persuasion texts i.e., persuasion factors). Although it allows us for explicit modeling of user-decision process, there are critical challenges including the persuasion factor's representation learning and effect estimation, along with the data-sparsity problem. To address them, in this work, we present our POEM (short for Persuasion factOr Effect Modeling) system. We first propose the persuasion-factor graph convolutional layers for encoding and learning representations from the persuasion-aware interaction data. Then we develop a prediction layer that fully considers the user sensitivity to the persuasion factors. Finally, to address the data-sparsity issue, we propose a counterfactual learning-based data augmentation method to enhance the supervision signal. Realworld experiments demonstrate the effectiveness of our proposed framework of modeling the effect of persuasion factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Information systems applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommendation engines learn from behavioral history (i.e., useritem interaction) and various features, including user profiles, item attributes, and context, which is defined as feature-based recommendation or click-through rate (CTR) prediction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42]</ref>. Typical solutions include shallow models such as FM <ref type="bibr" target="#b22">[23]</ref>, and deep models such as DeepFM <ref type="bibr" target="#b6">[7]</ref>, AutoInt <ref type="bibr" target="#b24">[25]</ref>, etc. For example, FM <ref type="bibr" target="#b22">[23]</ref> exploits pairwise inner product for extracting second-order feature interactions; AutoInt <ref type="bibr" target="#b24">[25]</ref> uses self-attention layers to capture higher-order feature interactions.</p><p>Despite their effectiveness, these models usually work in an implicit manner, ignoring the explicit modeling of how positive behavior occurs. One of the main reasons is the lack of real-world data which can be used to understand how users make decisions. In the real world, the decision of a user is driven by several factors <ref type="bibr" target="#b36">[37]</ref> and the consumed item also matches several important aspects of user preferences. For example, in restaurant recommendation scenario, the aspects may include the taste of food, the environment of the restaurant, the variety of dishes, the price, etc., which affect the click behavior of users. When it comes to the hotel recommendation, these factors may consist of location, environment, facility, service quality, etc.</p><p>In China's most popular local life service provider, Meituan, during the process of recommending item to users, besides showing their regular features like name, picture and rating, there is a text message about the item that is relevant to one of aspects listed above, and it is intended to urge consumers to click. As shown in Figure <ref type="figure">1</ref>, these texts are generated according to the above-mentioned aspects, which provide us the opportunity of explicit modeling the user decision process. For example, as for facility, there is a corresponding "Full-equipped Facilities" text. Therefore, we define this kind of text as persuasion factor. In this scenario, we can model the effect of the persuasion factor on user decision, but however, there are still three critical challenges as follows.</p><p>• First, the encoding and representation of persuasion factors are difficult. Different from users and items of which the representations can be easily learned by many effective ways, persuasion factors reflect more fine-grained user decisions. Besides, they exert effects in a different way compared with attributes of users and items and are not explicitly reflected by click behaviors. Therefore, how to encode persuasion factors and learn their representations are challenging. • Second, the effect of persuasion factor on users is complex and heterogeneous. In fact, the specific user or item determines whether the exposed persuasion factor could have an influence. For example, some users will hardly be influenced by the exposed texts, and they usually make decisions solely by themselves. Therefore, simply using a persuasion factor-based model may not be beneficial for them. In summary, the difference in sensitivity among different users to persuasion factors is an essential problem in modeling the effect of persuasion factors. • Third, the historical data is always sparse. Compared with normal user-item interaction data, the users' behavioral log containing exposed persuasion factors are usually more sparse, which makes it more difficult to learn users' preferences on them, thus limiting the capability of modeling their effect on user decision. Hence, it is essential to address the data sparsity issue.</p><p>Considering the above issues and challenges, in this work, we propose a novel solution POEM (short for Persuasion factOr Effect Modeling) to tackle these problems. Specifically, we first organize users' clicks on items under the influence of persuasion factors by employing the form of graph, where users and items are nodes and persuasion factors serve as edges. Then we utilize a graph convolutional network to learn representations of users, items, and persuasion factors from the structured data. Following that, we present a user-sensitivity-based prediction that can adaptively assess how much persuasion factors are influencing users' ultimate behavior. As for the third challenge, we propose a counterfactual learningbased data augmentation method that generates data according to assumptions we make in the counterfactual world, which can significantly address the data-sparsity issue. The working pipeline of POEM is shown in Figure <ref type="figure" target="#fig_0">2</ref>, using the features of users and items, and persuasion factors in the dataset to generate recommendations, while collecting new user behavioral data and adding it to the dataset for future training. The main contributions of our work can be summarized as follows.    The 𝑖-th layer embedding of graph nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e (𝑖 ) 𝑁 𝑓</head><p>The 𝑖-th layer information of the node 𝑓 's neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝜔/W</head><p>The attention weight/transition matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Θ</head><p>Trainable parameters.</p><formula xml:id="formula_0">𝜆 𝐿 2 regularization hyperparameter. O 𝑓 /O 𝑐 𝑓 Factual/Counterfactual dataset. O 𝑐 𝑓 1 /O 𝑐 𝑓 2</formula><p>Counterfactual dataset according to assumption 1/2.</p><p>• We collect two real-world datasets for evaluation and release them to benefit the community. The extensive experimental results show the superiority of our proposed approach. Compared with existing methods, our method makes significant improvement on several metrics. In addition, we conduct ablation studies to validate the efficacy of designs in our model. We also illustrate the advantages of our model in terms of explainable recommendations by modeling the effect of persuasion factors on user decisions in large-scale recommendation scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM FORMULATION</head><p>The feature-based recommendation (also known as click-through rate (CTR) prediction) is formulated as predicting a binary target 𝑦 from a feature vector X, which is composed of the user profile and item attributes. In this work, we innovatively propose to consider the effect of persuasion factors on user decisions. Therefore, we formulate our problem as follows:</p><p>• Input: The feature vector x 𝑖 𝑗 ∈ R 𝑛 of the 𝑖-th user and the 𝑗-th item, and persuasion factor 𝑡 of the 𝑗-th item shown to the 𝑖-th user. • Output: The probability of the 𝑖-th user click on the 𝑗-th item, i.e., ŷ𝑖 𝑗 , under the condition of x 𝑖 𝑗 and 𝑡.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We propose an effective method entitled POEM (short for Persuasion factOr Effect Modeling) to model the effect of persuasion factors on user decisions. Figure <ref type="figure" target="#fig_2">3</ref> illustrates the holistic design of POEM, which consists of three key components:</p><p>• Persuasion-factor Graph Convolutional Layer. We propose to encode persuasion factors that influence users' decisions, along with users and items, as embeddings. In order to better learn representations from the brand new form of data, which consists of persuasion factors, we first construct a unified graph then propose graph convolutional layers based on information propagation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Persuasion-factor Graph Convolutional Layer</head><p>3.1.1 Unified Embedding Layer. Suppose that we have 𝑊 kinds of disentangled and independent persuasion factors, for the 𝑖-th persuasion factor, we define its one-hot vector as 𝑡 𝑖 . We then project its one-hot vector to latent embeddings, and the embedding matrix of persuasion factors E 𝑡 can be denoted as follows,</p><formula xml:id="formula_1">E 𝑡 = [e 𝑡 1 ; e 𝑡 2 ; • • • ; e 𝑡 𝑊 ],<label>(1)</label></formula><p>where E 𝑡 ∈ R 𝑊 ×𝑑 and 𝑑 denotes the embedding size. Then the embedding of the 𝑖-th persuasion factor can be denoted as e 𝑡 𝑖 = 𝑡 𝑖 E 𝑡 .</p><p>Similarly, the one-hot vectors are 𝑢 𝑗 for the 𝑗-th user and 𝑖 𝑘 for the 𝑘-th item. Following the paradigm of existing recommendation models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, we also represent users and items by embeddings, which have the same embedding size 𝑑 as persuasion factors, formulated as follows,</p><formula xml:id="formula_2">E 𝑢 = [e 𝑢 1 ; e 𝑢 2 ; • • • ; e 𝑢 𝑀 ], E 𝑖 = [e 𝑖 1 ; e 𝑖 2 ; • • • ; e 𝑖 𝑁 ],<label>(2)</label></formula><p>where e 𝑢 𝑗 ∈ R 𝑑 and e 𝑖 𝑘 ∈ R 𝑑 denote the embedding of the 𝑗th user and the 𝑘-th item, respectively. Here 𝑀/𝑁 denotes the number of users/items. Following the similar rule, we compute embeddings of the 𝑗-th user and the 𝑘-th item as e 𝑢 𝑗 = 𝑢 𝑗 E 𝑢 , e 𝑖 𝑘 = 𝑖 𝑘 E 𝑖 , respectively.</p><p>3.1.2 Graph convolutional layer. Persuasion factors reflect more fine-grained user decisions. Besides, they exert effects in a different way compared with attributes of users and items and are not explicitly reflected by click behaviors. To address this problem, we leverage representation learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref> based on the graph to model psychological persuasion factors. However, learning effective embeddings of persuasion factors is not trivial. Here we propose a graph convolutional network (GCN) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>-based solution to facilitate the embedding learning. Graph Construction We construct a heterogeneous graph to represent the user-item interaction affected by persuasion factors. The graph contains two kinds of nodes (users and items) and multiple edges (persuasion factors). Specifically, an edge 𝑡 𝑘 is added between user 𝑢 𝑖 and item 𝑖 𝑗 if the 𝑖-th user 𝑢 𝑖 clicks on the 𝑗-th item 𝑖 𝑗 under the 𝑘-th persuasion factor 𝑡 𝑘 . In this way, we are able to utilize GCN to represent user-item interactions influenced by persuasion factors, which is more efficient and explainable than conventional feature interaction models. Information Propagation As mentioned before, we characterize the interaction between user 𝑢 𝑖 and item 𝑖 𝑗 under persuasion factor 𝑡 𝑘 as a triplet (𝑢 𝑖 , 𝑡 𝑘 , 𝑖 𝑗 ). In the real-world scenario, a user will click on multiple items and an item will also be clicked by many users. Therefore, a node in the graph can be contained in several triplets.</p><formula xml:id="formula_3">Taking 𝑖 1 𝑡 1 ←→ 𝑢 1 𝑡 2 ←→ 𝑖 2 and 𝑖 4 𝑡 5 ←→ 𝑢 1 𝑡 2</formula><p>←→ 𝑖 2 as an example, 𝑢 1 combines information from 𝑖 1 and 𝑖 4 via 𝑡 1 and 𝑡 5 , respectively, and passes them to 𝑖 2 as its neighbor, which can be simulated by propagating information from 𝑖 1 and 𝑖 4 to 𝑖 2 , enriching its representation. Inspired by existing works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31</ref>? ], we perform information propagation between a node and its neighbors, which is shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>From the perspective of graph convolutional network, the embeddings of users and items defined in Equation ( <ref type="formula" target="#formula_2">2</ref>) are also layer-0 node embeddings in the graph, which can be reformulated as:</p><formula xml:id="formula_4">E (0) 𝑢 = [e (0) 𝑢 1 ; e (0) 𝑢 2 ; • • • ; e (0) 𝑢 𝑀 ], E (0) 𝑖 = [e (0) 𝑖 1 ; e (0) 𝑖 2 ; • • • ; e (0) 𝑖 𝑁 ].<label>(3)</label></formula><p>Similarly, the embeddings of persuasion factors defined in Equation (1) are also edge embeddings in the graph. For a node 𝑓 in the graph, N 𝑓 = {(𝑡, 𝑏)|(𝑓 , 𝑡, 𝑏) ∈ G} denotes the set of triplets where 𝑓 is involved. To represent the first-order connectivity of the node 𝑓 , taking layer-0 as an example, the information from all its neighbors can be formulated as</p><formula xml:id="formula_5">e (0) N 𝑓 = 1 |N 𝑓 | ∑︁ (𝑡,𝑏) ∈N 𝑓 e 𝑡 ⊙ e (0) 𝑏 .<label>(4)</label></formula><p>For simplicity, we omit the subscripts of nodes in Equation <ref type="bibr" target="#b3">(4)</ref>, where e 𝑡 is the embedding of persuasion factor 𝑡, e</p><p>𝑏 is the layer-0 embedding of node 𝑏 (users or items), e (0) 𝑁 𝑓 is the layer-0 information of the neighbors of node 𝑓 . |N 𝑓 | denotes the cardinality of the neighbor triplet set N 𝑓 .</p><p>To get the next layer representation of a node in the graph, we aim to aggregate the information from its neighbor and its current embedding. Formally, for the layer-1 embedding of node 𝑓 , we have e (1)</p><formula xml:id="formula_7">𝑓 = 𝑓 (e (0) N 𝑓 , e (0) 𝑓 ),<label>(5)</label></formula><p>where 𝑓 (•) is the aggregation function.</p><p>For the implementation of function 𝑓 (•), we have multiple choices, inspired by the existing works <ref type="bibr">[9, 14?</ref> ] and considering the complex effect from neighbors, we have:</p><formula xml:id="formula_8">𝑓 (e (0) N 𝑓 , e (0) 𝑓 ) = LeakyReLU(W 1 (e (0) N 𝑓 + e (0) 𝑓 )) +LeakyReLU(W 2 (e (0) N 𝑓 ||e (0) 𝑓 )) (6)</formula><p>where W 1 and W 2 are trainable transformation matrices. Multi-layer Propagation Following the same rule, we can stack more layers to capture the higher-order connectivity on the graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-MLP FC FC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User-sensitivity Based Prediction Persuasion Factor Influence Modeling Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Interaction Module Attribute Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One-Hot Encoder</head><p>Task A : persuasive tactics identification One-hot Vector  ).</p><formula xml:id="formula_9">𝒊 𝟏 𝒊 𝟐 𝒊 𝟓 …… 𝒗 𝟏 𝒗 𝟐 𝒗 𝟑 𝒓 𝟏 𝒓 𝟏 𝒓 𝟐 𝒓 𝟐 𝒓 𝟑 𝒓 𝟐 𝒊 𝟏 𝒊 𝟐 𝒊 𝟓 …… 𝒗 𝟏 𝒗 𝟐 𝒗 𝟑 𝒓 𝟏 𝒓 𝟏 𝒓 𝟐 𝒓 𝟐 𝒓 𝟑 𝒓 𝟐 𝒖 𝟏 𝒑 𝟐 𝒑 𝟓 …… 𝒗 𝟏 𝒗 𝟐 𝒗 𝟑 𝒓 𝟏 𝒓 𝟏 𝒓 𝟐 𝒓 𝟐 𝒓 𝟑 𝒓 𝟐 𝒖 𝟏 𝒑 𝟐 𝒑 𝟓 …… 𝒗 𝟏 𝒗 𝟐 𝒗 𝟑 𝒓 𝟏 𝒓 𝟏 𝒓 𝟐 𝒓 𝟐 𝒓 𝟑 𝒓 𝟐 𝒖 𝟏 𝒖 𝟐 𝒖 𝟓 …… 𝒊 𝟏 𝒊 𝟐 𝒊𝟑 𝒕 𝟑 𝒕 𝟏 𝒕 𝟒 𝒕 𝟐 𝒊 𝟒 𝒊 𝟓 𝒊 𝟗 …… 𝒖𝟏 𝒖 𝟐 𝒖 𝟑 𝒕 𝟐 𝒕 𝟓 𝒕 𝟑 𝒕 𝟏 𝒕 𝟑 Graph Convolutional Layer 𝒍 = 𝟏 𝒍 = 𝟐 𝒍 = 𝟑 𝒍 = 𝟏 𝒍 = 𝟐 𝒍 = 𝟑 𝒕 𝟐 𝒕 𝟐</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7)</head><p>Utilizing multi-hop propagation to capture high-order connections also makes better use of the information from collaborative filtering, which benefits the representation learning of nodes (users and items) and edges (persuasion factors).</p><p>After propagating for 𝑙-layers, the representation of users, items and persuasion factors can be formulated as</p><formula xml:id="formula_10">E (𝑙) 𝑢 = [e (𝑙) 𝑢 1 , e (𝑙) 𝑢 2 , • • • , e (𝑙) 𝑢 𝑀 ], E (𝑙) 𝑖 = [e (𝑙) 𝑖 1 , e (𝑙) 𝑖 2 , • • • , e (𝑙) 𝑖 𝑁 ], E 𝑡 = [e 𝑡 1 , e 𝑡 2 , • • • , e 𝑡 𝑊 ].<label>(8)</label></formula><p>In conclusion, graph convolutional network serves as an encoder for users, items, and persuasion factors in our model, where information propagation significantly contributes to learning their representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">User-sensitivity Based Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Base Model.</head><p>We construct our base model with widely used feature-based recommendation models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25]</ref>. It takes input</p><formula xml:id="formula_11">feature vector x = [x 1 , x 2 , • • • , x 𝑄 ]</formula><p>and generates embedding for the 𝑖-th field via</p><formula xml:id="formula_12">g 𝑖 = x 𝑖 G 𝑖 ,<label>(9)</label></formula><p>where G 𝑖 ∈ R 𝐾 𝑖 ×𝑑 denotes the embedding matrix of the 𝑖-th field, 𝐾 𝑖 denotes the number of features in the 𝑖-th field and 𝑑 denotes the embedding size. Overall embedding matrix of base model can be formulated as</p><formula xml:id="formula_13">G = [G 1 ; G 2 ; • • • ; G 𝑄 ].<label>(10)</label></formula><p>Then we calculate the prediction score with 𝐺 and other parameters Θ of the model, formulated as</p><formula xml:id="formula_14">ŷ = 𝜓 (x|G, Θ), (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>where ŷ is the prediction score, 𝜓 refers to the union of prediction models such as FM <ref type="bibr" target="#b22">[23]</ref> and MLP. The base model consists of three modules: feature embedding layer, feature interaction layer and MLP layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Prediction Module.</head><p>We divide the prediction score into two parts, one of which comes from feature interaction module and the other from persuasion factor influence modeling module. Specifically, we treat user profile and item attributes as normal features and put them into the feature interaction module, where IDs of user and item are also involved as features. For prediction, the feature interaction module consists of a base model. However, the user and item ID embeddings are extracted from the graph convolutional layer's output rather than the feature embedding layer of base model. We denote the prediction score between the 𝑖-th user and the 𝑗-th item from the feature interaction module as ŷ𝑓 𝑖 𝑗 . The persuasion factor influence modeling module also consists of a base model without feature embedding layer independent of the feature interaction module. Its input is the embedding of user ID, item ID, and persuasion factor, extracted from the graph convolutional layer. As mentioned before, persuasion factors reflect more fine-grained user decisions and exert effects in a different way compared with attributes of users and items. Therefore, before using embeddings to perform high-order feature interactions, we first conduct embedding projection for user and item, formulated as: 𝑖 and e 𝑡 , respectively, and we input them to the feature interaction layer and MLP layer of the persuasion factor influence modeling module. Similarly, we denote the prediction score between the 𝑖-th user and the 𝑗-th item from the persuasion factor influence modeling module as ŷ𝑡 𝑖 𝑗 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Fusion Strategy.</head><p>The effect of persuasion factors on user is heterogeneous. Considering this situation, there is an exploiting balance between prediction scores from two parts defined in Section 3.2.2. Formally, the final prediction score between the 𝑖-th user and the 𝑗-the item, i.e., ŷ𝑖 𝑗 , can be formulated as follows:</p><formula xml:id="formula_16">ŷ𝑖 𝑗 = 𝜔 ŷ𝑡 𝑖 𝑗 + (1 − 𝜔) ŷ𝑓 𝑖 𝑗 , (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>where 𝜔 is a learnable weight, 𝜔 ∈ [0, 1] and 𝜔 ∈ R. 𝜔 can also be seen as the sensitivity to persuasion factor of user 𝑢 𝑖 when fixing the item and the persuasion factor.</p><p>To adaptively learn the weight 𝜔, we utilize a fully connected layer and the sigmoid function to limit its scale. Specifically, 𝜔 of the user 𝑢 𝑖 under the 𝑗-th item and the 𝑘-th persuasion factor can be formulated as:</p><formula xml:id="formula_18">𝜔 = 𝜎 (MLP(e (𝑙) 𝑢 𝑖 ||e (𝑙) 𝑖 𝑗 ||e 𝑡 𝑘 )),<label>(14)</label></formula><p>where we denote the MLP we use as S-MLP (short for Sensitivity-MLP). The structure of S-MLP is a hyperparameter, 𝜎 (•) is sigmoid function and e (𝑙)</p><formula xml:id="formula_19">𝑢 𝑖 , e (𝑙)</formula><p>𝑖 𝑗 , e 𝑡 𝑘 denote the embeddings of the 𝑖-th user, the 𝑗-th item, and the 𝑘-th persuasion factor extracted from graph convolutional layer, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Signal-enhanced Counterfactual Learning</head><p>In real-world scenarios, the sparsity of exposed persuasion factors hinders effective learning of users' corresponding preferences. To tackle this issue, we present a counterfactual learning-based data augmentation technique to address this problem.</p><p>We define the treatment 𝑟 as whether a persuasion factor exists and the outcome 𝑦 as whether a user will click on the item recommended to him/her. If a persuasion factor exists, the decision of the user to click may be influenced by it. Then we can denote the outcome under 𝑟 = 0 as 𝑦 𝑟 0 and the outcome under 𝑟 = 1 as 𝑦 𝑟 1 . One of the crucial causes of data sparsity is the absence of counterfactual data. In the factual scenario, the persuasion factor can either exist (𝑟 = 1) or not (𝑟 = 0), so we can only observe the outcome under the treatment that has taken place. Then we denote the outcome we observe in collected data as the factual outcome 𝑦 𝑓 and unobserved outcome in the opposite treatment as the counterfactual outcome 𝑦 𝑐 𝑓 .</p><p>Inspired by in-depth analysis of the effect of persuasion factor on user decision, we propose to solve the challenge from the root by counterfactual data augmentation based on causal knowledge. According to our analysis, we propose the following two reasonable assumptions:</p><p>ASSUMPTION 1: If a user clicks on an item (𝑦 𝑓 = 1) without the existence of persuasion factor (𝑟 = 0), we assume that the user will still be likely to click on the item (𝑦 𝑐 𝑓 = 1) when a persuasion factor matching the item exists (𝑟 = 1). ASSUMPTION 2: If a user does not click on an item (𝑦 𝑓 = 0) with the existence of persuasion factor (𝑟 = 1), we assume that the user will not click on the item (𝑦 𝑐 𝑓 = 0) when the persuasion factor does not exist (𝑟 = 0).</p><p>The click behavior matching the first assumption is more related to the intrinsic interests of users. If the persuasion factor were exposed, it would enhance the possibility of clicking (i.e., the counterfactual label is 1). The users matching the second assumption will not click even if there is a persuasion factor, which means they do not like the item. If the persuasion factor were removed, it would further reduce the possibility of click (i.e., the counterfactual label is 0). Then we can generate counterfactual data based on the observed dataset and assumptions, formulated as follows,</p><formula xml:id="formula_20">O 𝑐 𝑓 1 = {𝑦 (𝑢 𝑗 ,𝑖 𝑘 ) 𝑐 𝑓 = 𝑦 (𝑢 𝑗 ,𝑖 𝑘 ) 𝑟 1 = 1|𝑦 (𝑢 𝑗 ,𝑖 𝑘 ) 𝑓 = 𝑦 (𝑢 𝑗 ,𝑖 𝑘 ) 𝑟 0 = 1, (𝑢 𝑗 , 𝑖 𝑘 ) ∈ O 𝑓 }, O 𝑐 𝑓 2 = {𝑦 (𝑢 𝑗 ,𝑖 𝑘 ) 𝑐 𝑓 = 𝑦 (𝑢 𝑗 ,𝑖 𝑘 ) 𝑟 0 = 0|𝑦 (𝑢 𝑗 ,𝑖 𝑘 ) 𝑓 = 𝑦 (𝑢 𝑗 ,𝑖 𝑘 ) 𝑟 1 = 0, (𝑢 𝑗 , 𝑖 𝑘 ) ∈ O 𝑓 }, O 𝑐 𝑓 = O 𝑐 𝑓 1 ∪ O 𝑐 𝑓 2 ,<label>(15)</label></formula><p>where 𝑢 𝑗 and 𝑖 𝑘 represent the 𝑗-th user and the 𝑘-th item in the dataset. O 𝑐 𝑓 1 and O 𝑐 𝑓 2 represent the counterfactual datasets we construct according to the two assumptions, O 𝑐 𝑓 represents counterfactual data and O 𝑓 represents the factual data we observe. In this way, we can obtain counterfactual labeled data, which well addresses the data-sparsity problem and can be used for training to improve model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Optimization</head><p>To optimize the prediction of interactions between users and items, our loss function is Log loss with 𝐿 2 regularization to prevent overfitting, which is defined as follows:</p><formula xml:id="formula_21">L = − 1 𝑁 𝑁 ∑︁ 𝑗=1 (𝑦 𝑗 𝑙𝑜𝑔( ŷ𝑗 ) + (1 − 𝑦 𝑗 )𝑙𝑜𝑔(1 − ŷ𝑗 )) + 𝜆||Θ|| 2 ,<label>(16)</label></formula><p>where 𝑦 𝑗 and ŷ𝑗 are ground truth and estimated CTR, respectively.</p><p>Here to benefit the community 1 . The statistics of two utilized datasets are reported in Table <ref type="table" target="#tab_4">2</ref>.</p><p>• Dataset-A. The dataset is collected from Meituan, which consists of exposure data during the users' browsing for hotels. The labels in the dataset indicate whether the user clicks the provided hotel ("1" for click). Each instance in the dataset also includes userprofiles and item attributes, such as the age group, gender, hotel category and star ratings. There are also persuasion factors in some instances, which is significant for the modeling of their effects. • Dataset-B. Meituan also collects this dataset but in a different time period, consisting of exposure data during the users' browsing for hotels. The feature fields of this dataset are similar to Dataset-A, but it covers different users and items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines.</head><p>To illustrate the effectiveness of POEM, we compare the performance with other feature-based models which are feasible to be deployed for real-world application. In baseline models, persuasion factors are treated as common features.</p><p>• LR is a linear regression model to combine all input features for CTR prediction. • NFM <ref type="bibr" target="#b10">[11]</ref> introduces a Bi-Interaction Pooling layer to capture the second-order feature interaction and replace the FM module. It also contains a deep neural network to model high-order feature interactions to enhance prediction performance.</p><p>• PNN <ref type="bibr" target="#b21">[22]</ref> defines a product layer which exploits inner product and outer product to capture high-order feature interactions. • DeepFM <ref type="bibr" target="#b6">[7]</ref> ensembles factorization machines and deep neural networks to capture both low and high order feature interactions to better fit the relationship between input features and labels. • AutoInt <ref type="bibr" target="#b24">[25]</ref> extends multi-head self-attentive network with residual connections to capture the correlation between different fields.</p><p>• DCN <ref type="bibr" target="#b27">[28]</ref>, i.e., DeepCrossNet, improves the Wide part of Wide &amp; Deep <ref type="bibr" target="#b0">[1]</ref> by using Cross Network to perform feature interactions, which avoids manual feature selection. • xDeepFM <ref type="bibr" target="#b14">[15]</ref> includes a Compressed Interaction Network, which produce high-order cross features by using outer product of stacked feature matrix. The model combines the Compressed Interaction Network and Deep Neural Networs for CTR prediction.</p><p>• AFN <ref type="bibr" target="#b1">[2]</ref> is capable to automatically learn the specific order of high-order feature interactions, avoiding the huge computational effort and weak correlation that may arise when constructing high-order features. 4.1.3 Metrics. We adopt two widely-used and accepted metrics in CTR prediction tasks, AUC (Area Under the ROC curve) and 1 https://github.com/tsinghua-fib-lab/POEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logloss (cross entropy). These two metrics evaluate model performance through different perspectives.</head><p>• AUC measures the probability that a model will score a randomly chosen positive item higher than a randomly chosen negative item. It concerns more about the order of predicted instances, reflecting the model's capability to distinguish between positive and negative samples. • Logloss measures the distance between the predicted score and true label for each instance. Our proposed method also aims to minimize Logloss in Equation ( <ref type="formula" target="#formula_21">16</ref>), so we also use it as a straightforward metric.</p><p>It is worth mentioning that a slightly higher AUC or lower Logloss at 0.001-level is considered significant for recommendation tasks, which is emphasized by many previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. In our real-world application scenarios in Meituan, a relatively small improvement in offline datasets will lead to a much more significant increasing on online metrics, which has also been verified by Google and many existing works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation Details.</head><p>We implement our model in Py-Torch. We first shuffle each dataset and select the early 80% as the training set, middle 10% as the validation set, and the last 10% as the test set. For a fair comparison, the number of learnable parameters should be the same in different models, so we empirically fix the feature embedding size to 8 in all embedding-based methods. Considering the relatively large data volume, we set the batch size to 50000. For AutoInt <ref type="bibr" target="#b24">[25]</ref>, we use three interaction layers, and the number of hidden units is 64. For each interaction layer, there are two attention heads. For xDeepFM <ref type="bibr" target="#b14">[15]</ref>, we use two interaction layers following the default settings. For AFN <ref type="bibr" target="#b1">[2]</ref>, we set the number of hidden units of the logarithmic neuron to 128 for its better performance. After all network structures are fixed, we deploy a grid search to achieve optimal performance on our dataset. To prevent models from overfitting, we also apply a grid search for dropout rate in {0.0, 0.1, • • • , 0.8} for two datasets and find that it has a limited effect on final results. We also adopt a careful grid search of learning rate in {10 −4 , 10 −3 , 10 −2 } and the coefficient of 𝐿 2 normalization in {10 −5 , 10 −4 , • • • , 10 −1 }. We use the Adam optimizer and Xavier initialization for model parameters. Moreover, early stopping is used. We stop training when the validation performance does not increase for two successive valid epochs. We conduct several experiments on two dataset using different models and obtained average experimental results with different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance</head><p>We show the performance of all models in Table <ref type="table" target="#tab_6">3</ref>. From the experimental data, we can conclude as follows.</p><p>• Our proposed model steadily achieves the best performance.</p><p>Our   <ref type="bibr" target="#b10">[11]</ref> 0.6342 0.3138 0.6327 0.3322 PNN <ref type="bibr" target="#b21">[22]</ref> 0.6429 0.2816 0.6407 0.4052 DeepFM <ref type="bibr" target="#b6">[7]</ref> 0.6359 0.3389 0.6327 0.3813 AutoInt <ref type="bibr" target="#b24">[25]</ref> 0.6356 0.3617 0.6375 0.3583 DCN <ref type="bibr" target="#b27">[28]</ref> 0.6435 0.3128 0.6389 0.3176 xDeepFM <ref type="bibr" target="#b14">[15]</ref> 0.6414 0.3654 0.6444 0.2995 AFN <ref type="bibr" target="#b1">[2]</ref> 0 • Failure to properly model the effect of persuasion factors will have a relatively large impact on model performance.</p><p>On other widely-used benchmark datasets like Criteo, Avazu and Movielens, AutoInt <ref type="bibr" target="#b24">[25]</ref> and AFN <ref type="bibr" target="#b1">[2]</ref> are relatively the best baselines. However, although they have a complex feature interaction structure, they do not perform well on our dataset. On Dataset-A, AFN cannot beat DCN. On Dataset-B, AFN also cannot perform better than xDeepFM, w.r.t. AUC. The performance of AutoInt is also not so outstanding on both datasets. These results indicate that special designs are acquired to consider the effect of persuasion factors and utilizing only conventional feature interaction structure may lead to sub-optimal results. Our model adopts a graph convolutional network to explicitly represent the effect of persuasion factors along with user-sensitivity based prediction and counterfactual learning, which steadily achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To further measure the role of each designed component, we perform several ablation experiments. Our proposed method POEM contains four special designs: persuasion-factor graph convolutional network, embedding projection, user-sensitivity based prediction and signal-enhanced counterfactual learning. To study the effectiveness of these designs, we remove these modules in turn to verify their contribution to the model performance.</p><p>The experimental results on two datasets are reported in Figure <ref type="figure" target="#fig_5">5</ref>. We also present the specific results in Table <ref type="table" target="#tab_8">4</ref> for visual comparison, LogLoss by 0.0238 on Dataset-B, which is a significant performance drop according to existing works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. By utilizing GCN Encoder, we explicitly model the effect of persuasion factors in the form of graph, and the idea of collaborative filtering is also applied in the process of information propagation. In general, GCN Encoder is an essential part of our proposed model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Explainable Recommendation</head><p>Researchers and developers in the industry have been paying more attention to the explainability of deep learning-based recommender   systems in recent years <ref type="bibr" target="#b24">[25]</ref>, since more explainable models allow for more consistent performance. In this part, we use specific case studies to illustrate that our model is explainable with respect to prediction results. For the sake of simplicity, we will take the performance of model on Dataset-A as the example.</p><p>For each user, we filter out his click records from the dataset, while observing the persuasion factor conditions under which these clicks occur. For instance, if the maximum amount of the user's clicks is from the presence of service persuasion factor, we regard this user as a service-concern user. From this perspective, each user can be regarded as concerned a certain kind of persuasion factor. We want to investigate whether our model can learn the persuasion factor that each user concerns, and distinguish difference preferences on persuasion factors between users. To solve this problem, we conduct experiments by in-depth analysis for user embeddings.</p><p>We select four groups of users which concern about four types of persuasion factors, location, facility, roomtype and food, in hotel recommendation scenario, respectively. We use t-SNE to shrink the embeddings of these users into 2-D plane. From Figure <ref type="figure" target="#fig_6">6</ref>, we find that before information propagation (layer-0 user embeddings), the embeddings of four groups of users is randomly distributed throughout the 2-D plane. However, after using the graph convolutional layer, we can see that the embedding of users (layer-3 user embeddings) who concern about the same type of persuasion factor are almostly clustered together in 2-D spatial regions. We also find that for users who concern about the same persuasion factor, the embeddings are also not too close to each other in the 2-D plane. It is explainable because, in addition to preference about persuasion factors, users still vary in other features, leading to the difference of their embeddings. Overall, our model is capable to express users' preferences on persuasion factors, while also well reflecting the differences among users, which is critical for better recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Click-through rate prediction. Click-through rate prediction is of great importance for many Internet business companies that provide services to users and it is also a significant research field in recommender systems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref>. Deep neural networks (DNN)-based CTR prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref> has become a paradigm in this research area nowadays. A common approach is to embed features to learnable dense vectors then use deep neural networks for feature interactions. Qu et al. <ref type="bibr" target="#b21">[22]</ref> introduce product layer before DNN to perform complex and adequate feature interactions. He et al. <ref type="bibr" target="#b10">[11]</ref> replace product layer with bi-interaction pooling layer for feature interactions, and Yang et al. <ref type="bibr" target="#b34">[35]</ref> use field-aware and attention mechanism to conduct feature interactions. However, a significant disadvantages of using DNN for higher-order feature interactions is implicit, and even the elements within the same field embedding vector will influence each other. Then, other models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref> try to not rely on DNN but explicitly achieve finite-order feature interactions to improve the performance.</p><p>In recent years, graph neural network (GNN) has been widely noticed and utilized in the field of CTR prediction. Ying et al. <ref type="bibr" target="#b35">[36]</ref> combine random walks and graph convolutions to generate item embeddings. Ouyang et al. <ref type="bibr" target="#b19">[20]</ref> focus on cold-start ads and build a graph to connect old and new ads and adaptively distill useful information. Guo et al. <ref type="bibr" target="#b7">[8]</ref> utilize dual graph embedding to alleviate feature sparsity and user behavior sparsity problem. He et al. <ref type="bibr" target="#b9">[10]</ref> build hypergraphs to yield modal-specific representations of users and micro-videos to better capture user preferences. Besides, we model the effect of persuasion factors on user decision with GCN, which enables better model performance and much more explainability. Counterfactual learning-based data augmentation. Counterfactual learning is an effective method to implement data augmentation, which is a common way to address data sparsity problem. Wang et al. <ref type="bibr" target="#b31">[32]</ref> develop a counterfactual learning-based sampler model to generate new user behavior sequences based on the observed ones. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> model counterfactual data distribution to identify noisy and indispensable behaviors and replace dispensable and indispensable concepts within the original concept behaviors. Xiong et al <ref type="bibr" target="#b33">[34]</ref> investigate the influence of feature-level interests on user decisions as well as augment training samples by intervening the feature-level interests of users in a counterfactual manner. In our work, we make reasonable assumptions about user behaviors and utilize counterfactual learning to generate effective new training samples, which contributes a lot to overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we handle recommendation from a brand new perspective, modeling the effect of persuasion factors, which has not been well explored by existing works, and propose an effective method POEM. We first build a graph where users and items are nodes and persuasion factors are edges, then perform information propagation by graph convolutional networks to learn their representations. After that, considering the heterogeneity between users and items, we present user-sensitivity based prediction. We also propose a counterfactual-based data augmentation method to alleviate data-sparsity problem. Extensive experiments verify the effectiveness of our model and the effect of persuasion factors, which is a new field that is worthy of further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX FOR REPRODUCIBILITY A.1 Additional Results of Case Study</head><p>For items, we can also deploy a similar approach to analyze their embeddings. Assuming that the maximum amount of the clicks belong to the item is under the presence of the environment persuasion factor, we regard this item as an environment-matching item. We also select four groups of items which match four persuasion factors, facility, roomtype, soundproof and price. According to the same analysis as user embeddings, we present the results in Figure <ref type="figure" target="#fig_8">7</ref>. From the results, our model also achieves a balance between modeling the item persuasion factor matching relations and its individual attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyper-parameter Study</head><p>In this section, we explore the impact of different hyperparameters on the proposed model for two datasets. The hyperparameters include the number of propagation layers, aggregator and Sensitivity-MLP structure. Other hyperparameters such as node dropout rate and message dropout rate also affect the performance of model based on our further research, but we omit the results due to the space limit.</p><p>A.2.1 Effect of the Number of Propagation Layers. We test the performance of different numbers of propagation layers, also called hops. Based on the existing works [9, 31? ], we search for optimal hops in {1, 2, 3, 4, 5, 6} to find the most suitable number of propagation layers. The results are shown in Figure <ref type="figure" target="#fig_10">8</ref>.</p><p>From the results, we can find that our proposed model can achieve relatively higher performance with three propagation layers w.r.t. a higher AUC and a lower LogLoss on Dataset-A, w.r.t. a competitive AUC and much lower LogLoss on Dataset-B. Although the model with two propagation layers achieves the highest AUC on Dataset-B, but its performance of LogLoss is much worse. The phenomenon about the effect of number of propagation layers on model performance is understandable and explainable. If we only have few propagation layers, only low-order connectivity, i.e., nearest neighbors, can contribute to information propagation, limiting the amount of information. However, if we stack too many propagation layers, the performance of our model can also be adversely affected. Since the nodes in the graph obtain information from its neighbors, too many propagation layers could make all nodes aggregate information from almost all other nodes in the graph, making embeddings of nodes indistinguishable, and this is the over-smoothing of graph representations. Hence, considering the need for information propagation and prevent from overfitting, we empirically recommend to use three propagation layers.</p><p>A.2.2 Effect of Aggregator. We also analyze the influence of aggregator on the performance of our model, just using one part of our aggregation function (add or concatenate) and the results are shown in Table <ref type="table" target="#tab_11">5</ref>. We find that Mixed Aggregator outperforms other two aggregators significantly. Mixed Aggregator combines the advantages of the other two approaches for information aggregation, so it should be a relatively better aggregator choice.     depth of hidden layers and the shape of neural networks. They are all hyperparameters that we need to adjust.</p><p>• Effect of the number of neurons per layer. We search the number of neurons per layer in {64,128,256,512}. Increasing the number of neurons per layer introduces more learnable parameters. We present model performance with different number of neurons in Figure <ref type="figure" target="#fig_12">9</ref>. Experimental results show that 64-128 neurons are relatively better choices for two datasets. The reason is that an overly complex model is prone to overfitting the training data, which could explain the decrease of performance when the number of neurons becomes large. • Effect of the depth of hidden layers. We search the depth of hidden layers in {2,3,4,5}, and show the results in Figure <ref type="figure" target="#fig_14">10</ref>.</p><p>We find that our model can reach the best performance when the hidden layer is shallow. As the hidden layers get deeper, the model starts to behave worse. Specifically, two hidden layers are appropriate choice for both datasets. This kind of phenomenon is also due to overfitting. • Effect of the shape of neural networks. Existing works <ref type="bibr" target="#b6">[7]</ref> point out that the shape of neural network has an impact on the performance of the model. We test three different shapes of networks: increasing, constant and decreasing network. During     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Persuasion factor effect modeling application.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The persuasion-factor graph convolutional layer and user-sensitivity based prediction of our proposed model POEM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A detailed illustration of information propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>𝑢 + 𝜎 (e (𝑙) 𝑢 ⊙ e 𝑡 ) ⊙ e (𝑙) 𝑢 , e ′ 𝑖 = e (𝑙) 𝑖 + 𝜎 (e (𝑙) 𝑖 ⊙ e 𝑡 ) ⊙ e (𝑙) 𝑖 , (12) then the final representation of the user, item and persuasion factor change to e ′ 𝑢 , e ′</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation studies on two datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: In-depth analysis for user embeddings using t-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>A. 2 . 3</head><label>23</label><figDesc>Effect of Sensitivity-MLP Structure. The structure of Sensitivity-MLP contains three aspects: the number of neurons per layer, the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: In-depth analysis for item embeddings using t-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance of different propagation layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance of different neurons per layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance of different depth of hidden layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>: Notation Table. E 𝑢 , E 𝑖 , E 𝑡 Embedding matrix of users, items and persuasion factors. e 𝑢 , e 𝑖 , e 𝑡 Embedding of user, item and persuasion factor. e</figDesc><table><row><cell>Notations</cell><cell>Description</cell></row><row><cell>x</cell><cell>Feature vector.</cell></row><row><cell>(𝑖 )</cell><cell></cell></row><row><cell>𝑓</cell><cell></cell></row></table><note>𝑢, 𝑖, 𝑡User, item, persuasion factor.𝑓 , 𝑁 𝑓Graph node, the neighbor of node 𝑓 .𝑀, 𝑁 ,𝑊 , 𝑄The number of users, items, persuasion factors, fields.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>• User-sensitivity Based Prediction. To tackle the heterogeneity of users' sensitivity to external influence, we propose to model the sensitivity to persuasion factors for each user in the prediction process. • Signal-enhanced Counterfactual Learning. To address the challenge of data sparsity, we propose a counterfactual learningbased technique for data augmentation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>𝑗 is the index of training instance, and 𝑁 is the number of training samples. 𝜆 is the hyperparameter to control the 𝐿 2 regularization term. Θ represents all trainable parameters in our model, including embeddings, weights of MLPs, attention weight 𝜔, etc., and || • || 2 denotes the 𝐿 2 -regularization term. Statistics of two datasets.</figDesc><table><row><cell>4 EXPERIMENTS</cell></row><row><cell>4.1 Experimental Settings</cell></row><row><cell>4.1.1 Dataset. We conduct our experiments on two million-scale</cell></row><row><cell>datasets collected in two time periods, from the real-world local</cell></row></table><note>service-providing application Meituan, widely downloaded and used in China. Most of the records in these datasets contain persuasion factors, which is an advantage that existing public datasets do not have. To supply further research, we have released our datasets</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Overall performance comparison.</figDesc><table><row><cell>Model</cell><cell cols="2">Dataset-A</cell><cell cols="2">Dataset-B</cell></row><row><cell></cell><cell>AUC</cell><cell>LogLoss</cell><cell>AUC</cell><cell>LogLoss</cell></row><row><cell>LR</cell><cell>0.5377</cell><cell>0.3216</cell><cell>0.5238</cell><cell>0.3029</cell></row><row><cell>NFM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>LogLoss by 0.0394 on Dataset-A, where the best baseline is DCN. Our model improves over w.r.t. LogLoss by 0.0226 on Dataset-B, where the best baseline is xDeepFM. If we consider the model with the lowest LogLoss as the best baseline, under this perspective, our model can improve over w.r.t. AUC by 0.0093 on Dataset-A, where the best baseline is PNN. Our model improves over w.r.t. AUC by 0.0211 on Dataset-B, where the best baseline is AFN.</figDesc><table><row><cell></cell><cell>.6362</cell><cell>0.3001</cell><cell>0.6303</cell><cell>0.2931</cell></row><row><cell>POEM (Ours)</cell><cell>0.6522</cell><cell>0.2734</cell><cell>0.6514</cell><cell>0.2769</cell></row><row><cell>Imp.</cell><cell cols="4">+0.0087 -0.0082 +0.0060 -0.0162</cell></row><row><cell cols="2">will improve over w.r.t.</cell><cell></cell><cell></cell><cell></cell></row></table><note>• Necessity and significance of modeling the effect of persuasion factors on user decision. Compared with other recommendation models that treat persuasion factors as a common feature, our model considers the specific influence of persuasion factors on user-item interactions. POEM achieves the best performance w.r.t. AUC and LogLoss by fine-grained modeling the effect of persuasion factors from different perspectives.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablation Study for the designs of POEM. 𝑤/𝑜 , Projection 𝑤/𝑜 , Sensitivity 𝑤/𝑜 and CF 𝑤/𝑜 , denote the removal of the four modules, respectively. • Effectiveness of GCN Encoder. We compare the performance between the model with GCN Encoder for the embedding of users, items and persuasion factors and the model only with Xavier embedding initialization. The results show that the model with GCN Encoder outperforms the other model w.r.t. AUC by 0.0209, w.r.t. LogLoss by 0.0141 on Dataset-A, w.r.t. AUC by 0.0164, w.r.t.</figDesc><table><row><cell>Model</cell><cell cols="2">Dataset-A</cell><cell cols="2">Dataset-B</cell></row><row><cell></cell><cell>AUC</cell><cell>LogLoss</cell><cell>AUC</cell><cell>LogLoss</cell></row><row><cell cols="2">POEM (Ours) 0.6522</cell><cell>0.2734</cell><cell>0.6514</cell><cell>0.2769</cell></row><row><cell>GCN 𝑤/𝑜</cell><cell>0.6313</cell><cell>0.2875</cell><cell>0.6350</cell><cell>0.3007</cell></row><row><cell>Projection 𝑤/𝑜</cell><cell>0.6518</cell><cell>0.2775</cell><cell>0.6459</cell><cell>0.2925</cell></row><row><cell>CF 𝑤/𝑜</cell><cell>0.6494</cell><cell>0.3079</cell><cell>0.6448</cell><cell>0.2957</cell></row><row><cell>Sensitivity 𝑤/𝑜</cell><cell>0.6480</cell><cell>0.2992</cell><cell>0.6438</cell><cell>0.2995</cell></row><row><cell>where GCN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>• Effectiveness of Embedding Projection. Without embedding projection, the performance on all metrics declines to a certain degree. The results show that the model without embedding projection decreases w.r.t. AUC by 0.0004, w.r.t. LogLoss by 0.0041 on Dataset-A, w.r.t. AUC by 0.0055, w.r.t. LogLoss by 0.0156 on Dataset-B, which is relatively significant. It demonstrates the importance of embedding projection in fine-grained modeling the relations between user, item and persuasion factors. • Effectiveness of Sensitivity Modeling. After compare our proposed method with the model which has fixed attention weight, we find that the proposed method performs better w.r.t. AUC by 0.0042, w.r.t. LogLoss by 0.0258 on Dataset-A, w.r.t. AUC by 0.0076, w.r.t. LogLoss by 0.0226 on Dataset-B. The experimental results verify our theory that the effect of persuasion factor is complex and heterogeneous. Naive method will lead to significant decline on overall performance. It is essential to assign individual attention weight for each data instance. • Effectiveness of Counterfactual Learning. We compare the performance between the model training with extra counterfactual data and the model training with only original data. From the results, we find that the model with counterfactual data augmentation outperforms the model training only on original data w.r.t. AUC by 0.0028 , w.r.t. LogLoss by 0.0345 on Dataset-A, w.r.t.</figDesc><table><row><cell>AUC by 0.0066, w.r.t. LogLoss by 0.0188 on Dataset-B. The signifi-</cell></row><row><cell>cant improvement in performance illustrates that counterfactual</cell></row><row><cell>learning-based data augmentation is a reasonably effective way</cell></row><row><cell>to address the data sparsity problem, contributing to modeling</cell></row><row><cell>the effect of persuasion factors.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Performance of different aggregators.</figDesc><table><row><cell>Model</cell><cell cols="2">Dataset-A</cell><cell cols="2">Dataset-B</cell></row><row><cell></cell><cell cols="4">AUC LogLoss AUC LogLoss</cell></row><row><cell>Add</cell><cell>0.6491</cell><cell>0.2824</cell><cell>0.6471</cell><cell>0.2848</cell></row><row><cell cols="2">Concat 0.6495</cell><cell>0.2840</cell><cell>0.6476</cell><cell>0.2838</cell></row><row><cell cols="2">Mixed 0.6522</cell><cell>0.2734</cell><cell>0.6514</cell><cell>0.2769</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Performance of different neural network shapes. we fix the number of hidden layers as two and the number of total neurons as 128. Then the three kinds of network shape are: increasing<ref type="bibr" target="#b31">(32,</ref>96), constant (64,64) and decreasing (96,32). As we can see in Table6, the constant shape of neural network achieves relatively better performance. However, we can also know from the results that the decreasing shape of neural network gets a lower LogLoss on Dataset-A than the constant shape. Then these is a balance: if you are more concerned about AUC (like ranking tasks), you should choose the constant network shape. If a lower LogLoss is what you need (requires accuracy of CTR prediction), then the decreasing network shape will meet your demand without losing too much AUC.</figDesc><table><row><cell>Model</cell><cell cols="2">Dataset-A</cell><cell cols="2">Dataset-B</cell></row><row><cell></cell><cell cols="4">AUC LogLoss AUC LogLoss</cell></row><row><cell>Constant</cell><cell>0.6522</cell><cell>0.2734</cell><cell>0.6514</cell><cell>0.2769</cell></row><row><cell>Increasing</cell><cell>0.6483</cell><cell>0.2893</cell><cell>0.6515</cell><cell>0.2892</cell></row><row><cell cols="2">Decreasing 0.6505</cell><cell>0.2706</cell><cell>0.6497</cell><cell>0.2817</cell></row><row><cell>our test,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">• We take the pioneering step to approach the problem of modeling the effect of persuasion factors on user decision for large-scale recommendation in Meituan, which is an important problem in real-world applications but has not been well explored. • We propose a solution that first constructs a graph and deploys graph convolutional layers to learn representations from the graph. We further suggest adaptive user sensitivity-based prediction and counterfactual learning to alleviate the data sparsity</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work is supported in part by National Key Research and Development Program of China under 2020YFA0711403, and by National Natural Science Foundation of China under 61972223, 61971267 and U1936217. This work is also supported by Meituan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
				<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Weiyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linpeng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03276[cs.LG]</idno>
		<title level="m">Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep session interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuyu</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06482</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingrong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghua</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12843</idno>
		<title level="m">Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards a theory of vector embeddings of structured data</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2020">2020. word2vec, node2vec, graph2vec, x2vec</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ruiming Tang, and Xiuqiang He. 2021. Dual Graph enhanced Embedding Neural Network for CTR Prediction</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00314[cs.IR]</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Click-Through Rate Prediction with Multi-Modal Hypergraphs</title>
		<author>
			<persName><forename type="first">Li</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Jameel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="690" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05031[cs.IR]</idno>
		<title level="m">Neural Collaborative Filtering</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Tongwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
	<note>In Recsys</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Field-aware factorization machines for CTR prediction</title>
		<author>
			<persName><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
	<note>In Recsys</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Feature generation by convolutional neural network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingzhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhou</forename><surname>Zhang</surname></persName>
		</author>
		<idno>WWW. 1119-1129</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Interest-aware message-passing gcn for recommendation</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<idno>WWW. 1296-1305</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structural relationship representation learning with graph embedding for personalized product search</title>
		<author>
			<persName><forename type="first">Shang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="915" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>H Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel Golovin</surname></persName>
		</author>
		<idno>SIGKDD. 1222-1230</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Graph Meta Embeddings for Cold-Start Ads in Click-Through Rate Prediction</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuwu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shukui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinmei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pinnersage: Multi-modal user embedding framework for recommendations at pinterest</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chantat</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2311" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting clicks: estimating the click-through rate for new ads</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewa</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Autoint: Automatic feature interaction learning via selfattentive neural networks</title>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In CIKM. 1161-1170</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hoafm: A high-order attentive factorization machine for CTR prediction</title>
		<author>
			<persName><forename type="first">Zhulin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">102076</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online user representation learning across heterogeneous social networks</title>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Tem: Tree-enhanced embedding model for explainable recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 1543-1552</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning Intents behind Interactions with Knowledge Graph for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinglin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yancheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 878-887</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Counterfactual data-augmented sequential recommendation</title>
		<author>
			<persName><forename type="first">Zhenlei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent recommender networks</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">How</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Counterfactual Review-based Recommendation</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2021">Jun Zhou. 2021</date>
			<biblScope unit="page" from="2231" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Operationaware neural networks for user response prediction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baile</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaofeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="161" to="168" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Persuade to click: Context-aware persuasion model for online textual advertisement</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hancheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guozhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09096[cs.LG]</idno>
		<title level="m">Field-aware Neural Factorization Machine for Click-Through Rate Prediction</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Causerec: Counterfactual user sequence synthesis for sequential recommendation</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gcn-based user representation learning for unifying robust recommendation and fraudster detection</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
	<note>Quoc Viet Nguyen Hung, Zi Huang, and Lizhen Cui</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint deep modeling of users and items using reviews for recommendation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep interest evolution network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5941" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
