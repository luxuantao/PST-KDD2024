<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Performance of corporate bankruptcy prediction models on imbalanced dataset: The effect of sampling methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-01-03">3 January 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ligang</forename><surname>Zhou</surname></persName>
							<email>mrlgzhou@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Management and Administration</orgName>
								<orgName type="institution">Macau University of Science and Technology</orgName>
								<address>
									<settlement>Taipa, Macau</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Performance of corporate bankruptcy prediction models on imbalanced dataset: The effect of sampling methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-01-03">3 January 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">06CBAF5F7626AD42520292DECD5DC75C</idno>
					<idno type="DOI">10.1016/j.knosys.2012.12.007</idno>
					<note type="submission">Received 9 July 2012 Received in revised form 10 December 2012 Accepted 20 December 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Bankruptcy prediction Imbalanced dataset Undersampling Oversampling Classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Corporate bankruptcy prediction is very important for creditors and investors. Most literature improves performance of prediction models by developing and optimizing the quantitative methods. This paper investigates the effect of sampling methods on the performance of quantitative bankruptcy prediction models on real highly imbalanced dataset. Seven sampling methods and five quantitative models are tested on two real highly imbalanced datasets. A comparison of model performance tested on random paired sample set and real imbalanced sample set is also conducted. The experimental results suggest that the proper sampling method in developing prediction models is mainly dependent on the number of bankruptcies in the training sample set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When a company applies for a loan from a creditor, the creditor needs to answer a question, ''Is it possible that the borrower will go bankrupt and will not repay the loan?'' Before an investor make investment in stock of a company, the investor always worries about the bankruptcy of the company which may cause a loss of all investment. Therefore, it is important for creditors and investors to be able to predict corporate bankruptcy.</p><p>From a statistical point of view, the corporate bankruptcy prediction problem is a typical classification problem, in which a company is classified into a non-bankrupt class or a bankrupt class in terms of the company's features. The quantitative methods are usually used to the catch the relationship between a company's bankruptcy and its financial information in the most recent fiscal year before its bankruptcy or other information in the same period reflecting the company's operating environment, such as industry position or macroeconomic environment. This relationship is often described as a corporate bankruptcy prediction model (CBPM) which is constructed with a part of historical observations and is evaluated with another part of historical observations. With the assumption that the relationship holds in the future, the model can be used to predict a company's bankruptcy in the future with the currently available information of the company.</p><p>The development of these corporate bankruptcy prediction models is a data-fitting based empirical research and the typical processes of models development are shown as Fig. <ref type="figure" target="#fig_1">1</ref>. It shows that the performance of models is dependent on a series of processes, such as sampling, features selection, modeling and evaluation criteria <ref type="bibr" target="#b0">[1]</ref>.</p><p>For the features selection in the development of CBPMs, a lot of research has been conducted. Beaver <ref type="bibr" target="#b1">[2]</ref> identified 30 different ratios considered to be important factors for forecasting corporate bankruptcy and tested them by a univariate discriminant analysis model on 79 pairs of bankrupt/non-bankrupt firms; the empirical results showed that ''working capital funds flow/total assets'' and ''net income/total assets'' were the two most efficient ratios that could correctly classify 90% and 88% of the firms, respectively. Altman <ref type="bibr" target="#b2">[3]</ref> selected five ratios, employed a multivariate discriminant analysis model, and tested the model on 33 pairs of bankrupt/non-bankrupt firms. The model could correctly identify 90% of the firms one year prior to failure. The five selected ratios were: working capital/total assets, retained earnings/total assets, EBIT/total assets, market value equity/book value of total debt, and sales/total assets. Ravi Kumar and Ravi <ref type="bibr" target="#b3">[4]</ref> reviewed 128 papers and listed more than 500 different variables that have been used for bankruptcy prediction. To obtain models with better predictive performance, many quantitative techniques and methods from statistics and data mining have been employed, such as discriminant analysis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, linear regression <ref type="bibr" target="#b5">[6]</ref>, decision tree <ref type="bibr" target="#b6">[7]</ref>, neural networks <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, support vector machines <ref type="bibr" target="#b11">[12]</ref> and a wide variety of hybrid methods <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. In addition, some new hybrid methods based on fuzzy theory can be potential alternatives to predict corporate bankruptcy <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">1</ref> In the process of sample selection, one simple strategy is to use all available samples. For a large dataset, it will cause the failure of many quantitative approaches due to the unacceptable computational time and space. Another simple strategy is random sampling. If the sample size is not large, it has no problem of computational time and space. However, in practice, the bankrupt cases is very rare, while the number of non-bankrupt cases is very large, therefore, the proportion of bankrupt companies is very close to zero, which lead to a seriously imbalanced classification problem. Both of above two simple strategies without special treatment on the imbalanced samples may cause the quantitative models which always seek an accurate performance over training samples to classify all the test samples into the non-bankrupt class, which is not helpful for decision making.</p><p>The classification on imbalanced datasets has received great attention in recent research of data mining because of its wide real applications <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. Garcı ´a et al. investigated the influence of both the imbalance ratio and classifier on the performance of four resampling strategies to deal with imbalanced data sets and found that over-sampling the minority class consistently outperforms under-sampling the majority class when data sets are strongly imbalanced <ref type="bibr" target="#b23">[24]</ref>. However, the imbalance property in real datasets for bankruptcy prediction problem has been largely ignored by most literature about bankruptcy prediction. Most research uses dataset with paired samples for training and testing bankruptcy prediction models, in which the number of bankrupt companies is the same as that of the non-bankrupt companies. One important advantage of this strategy is that there is no class bias in the training samples and testing samples, the simple performance measure: classification accuracy on bankrupt and non-bankrupt instances can be used to evaluate the performance of models, which makes the objective of minimizing classification error consistent in the process of model construction (training process) and model evaluation (testing process). However, in real world bankruptcy prediction, the ratio of bankrupt firms to non-bankrupt firms, i.e. degree of imbalance, can be approximately as low as 1 to 100 or even 1 to 1000. There are only a few articles discussing the imbalance problem in bankruptcy prediction. Wilson and Sharda <ref type="bibr" target="#b9">[10]</ref> made a comparison of predictive capabilities between neural networks and multivariate discriminant analysis with different degree of imbalance: 50/50, 20/80, and 10/90. They concluded that neural networks outperformed discriminant analysis in classification accuracy and neural network was shown to perform well in predicting both bankrupt firms and non-bankrupt firms when presented with equal numbers of examples in the learning phase. Neves and Vieira <ref type="bibr" target="#b24">[25]</ref> tested the effect of different proportions of non-bankrupt firms in the sample to show the performance of an improved neural network model. They only tested three different degrees of imbalance: 50/50, 36/64, 28/72 and selected classification accuracy as the performance measure. Alfaro-Cid et al. <ref type="bibr" target="#b25">[26]</ref> tested genetic programming approach incorporating cost matrix for bankruptcy prediction using a highly imbalanced dataset in which about 5-6% of companies went bankrupt. They selected 10 bankrupt firms and 150 non-bankrupt firms as the training set from the total 484 Spanish companies in the database and selected other firms with various numbers of bankrupt and non-bankrupt cases as the training set for different years. As pointed out by the authors, the highly unbalance complicates the classification, but it is an accurate reflection of the real world. Mathiasi Horta et al. <ref type="bibr" target="#b26">[27]</ref> discussed some of the main problems in the preparation of models for bankruptcy prediction with the application of data mining techniques and pointed out that the first problem is the class imbalance which causes a poor classification performance and used ensemble strategy to deal with the imbalance problem.</p><p>Although Alfaro-Cid et al. <ref type="bibr" target="#b25">[26]</ref> and Mathiasi Horta et al. <ref type="bibr" target="#b26">[27]</ref> used cost matrix strategy and ensemble strategy to handle imbalance problem in bankruptcy prediction separately, for a real situation of CBPMs construction, with a large imbalanced dataset, the analyst need to know the answer to following questions: what strategies should be used to select the training sample; if the performance of CBPMs will be affected by the sampling strategy and how much will it be affected by the sampling strategy. Just like what Alfaro-Cid et al. <ref type="bibr" target="#b25">[26]</ref> did in the selection of training samples, most research in bankruptcy prediction randomly selected a fixed number of cases in the training samples, few of them discuss the effect of sampling for training set on performance on real imbalanced test set.</p><p>The main purpose of this paper is to explore the effect of different sampling methods on the performance of CBPMs on real highly imbalanced datasets and make a comparison among several commonly used CBPMs in a real situation. The outline of this paper is as follows. Section 2 describes some popular sampling strategies and brought forward two new sampling strategies that will be used in this research. Section 3 introduces performance measures for imbalanced datasets. Section 4 reports the results of empirical study on two datasets with different sampling methods and quantitative methods. Section 5 concludes the paper and gives some discussion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Raw data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sampling strategies</head><p>Some research in machine learning community has addressed the strategy of resampling the original dataset to deal with the issue of class imbalance <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. The commonly used resampling strategies include oversampling and undersampling. Oversampling is to sample the minority class over and over to achieve the balanced distribution of the two classes, while undersampling is to select a portion of the majority class to achieve the distribution balance of the two classes. In the original imbalanced training dataset, let the original sample set of minority class and majority class denoted by S mi and S ma separately, the size of minority class jS mi j is much less than the size of majority class jS ma j. The training dataset is denoted by S. A simple example of an original training dataset for bankruptcy corporate prediction is shown in Table <ref type="table" target="#tab_0">1</ref>. This example is just for the purpose to explain sampling strategy and it is imbalanced but not highly imbalanced. Each observation has M features and an observed financial status: Bankrupt or Non-bankrupt in the next year. The bankrupt observation is marked with value 1 in the ''Bankrupt'' column while the non-bankrupt observation is marked with value 0. Each observation has a unique ID and the bankrupt observation with an ID staring with a letter ''I'' and non-bankrupt observation starting with a letter ''M''. In this example, bankrupt observations belong to the minority class and non-bankrupt observations belong to the majority class. Therefore, the set of minority class S mi = {I 1 , I 2 , I 3 , I 4 } and jS mi j = 4, the set of majority class S ma = {M 1 , M 2 , . . . , M 10 } and jS ma j = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Oversampling</head><p>Two widely used oversampling methods: Random Oversampling with Replication (ROWR) and Synthetic Minority Over-sampling Technique (SMOTE) are employed in this study. ROWR is the method to balance class distribution through the random replication of minority class examples <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Random oversampling with replication (ROWR)</head><p>Algorithm 1. ROWR (S mi , S ma ) Input: The original sample set of minority class S mi and sample set of majority class S ma . Output: sample set with balanced class S of size 2jS ma j. 1. S = S ma ; 2. for i = 1 to jS ma j randomly selected an element a i from sample set S mi S = S [ {a i }. endfor</p><p>For the original training dataset shown in Table <ref type="table" target="#tab_0">1</ref>, an example of the final training set S may like the follows: {M 1 , M 2 , . . . , M 10 , I 3 ,</p><formula xml:id="formula_0">I 1 , I 4 , I 2 , I 2 , I 1 , I 3 , I 4 , I 3 , I 1 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Synthetic Minority Over-sampling Technique (SMOTE)</head><p>SMOTE, proposed by Chawla et al. <ref type="bibr" target="#b31">[32]</ref>, is an improved oversampling approach in which the minority class is oversampled by creating ''synthetic'' examples rather than by oversampling with replacement. The main idea of SMOTE is to oversample the minority class by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbor. The detail of SMOTE is as follows <ref type="bibr" target="#b31">[32]</ref>:   <ref type="bibr" target="#b32">[33]</ref>. RU employed in this study is to randomly select part of the majority class to achieve the balance which obtains the equivalent result. Algorithm 3. RU (S mi , S ma )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>The original sample set of minority class S mi and sample set of majority class S ma . Output: sample set with balanced class S of size 2jS mi j 1. S = S mi ; 2.</p><p>for i = 1 to jS mi j randomly selected an element a i from sample set S ma S = S [ {a i }. endfor For the original dataset in Table <ref type="table" target="#tab_0">1</ref>, one training set S generated by RU like follows: {I 1 , I 2 , I 3 , I 4 , M 8 , M 3 , M 7 , M 5 } and its size is 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Undersampling Based on Clustering from Nearest Neighbor (UBOCFNN)</head><p>The idea of UBOCFNN is inspired by clustering problem to partition the points in sample space into K clusters and select the point which is the nearest to the central point of each cluster to represent the whole cluster. The detail of UBOCFNN is as follows: 1. S = S ma 2. Partition the points in S ma into jS mi j clusters with following two phases <ref type="bibr" target="#b33">[34]</ref>: 2.1. Uses batch updates, where each iteration consists of reassigning points to their nearest cluster centroid, all at once, followed by recalculation of cluster centroids.</p><p>2.2. Uses online updates, where points are individually reassigned if doing so will reduce the sum of distances, and cluster centroids are recomputed after each reassignment. 3. for i = 1 to jS mi j Find the central point c i of cluster i Select the point a i which is the nearest point in cluster i to the point c i S = S [ {a i } endfor For the original dataset in Table <ref type="table" target="#tab_0">1</ref>, suppose the non-bankrupt observations are clustered into four clusters as follows {M 2 , M 3 }, {M 1 , M 10 , M 7 }, {M 4 , M 6 , M 8 , M 9 }, {M 5 }. It is easy to calculate the central point of each cluster, the coordinates of the central point is the average corresponding coordinates of all points in the cluster. Then the nearest point to the central point in each cluster can be found. One example of the final training set may like S = {I 1 , I 2 , I 3 , I 4 , M 2 , M 7 , M 6 , M 5 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Undersampling Based on Clustering from Gaussian Mixture Distribution (UBOCFGMD)</head><p>The idea of UBOCFGMD is similar to UBOCFNN, except that the clustering is based on Gaussian mixture distribution. The Expectation Maximization (EM) algorithm is adopted to estimate the parameters in a Gaussian mixture model with K components for data in S ma then the data in S ma will be partitioned into K clusters in terms of the K components of Gaussian mixture distribution <ref type="bibr" target="#b33">[34]</ref>. In this study, we set K in UBOCFGMD and UBOCFNN to be jS mi- j. For UBOCFNN will select the point which is nearest to the central point of each cluster and then we can obtain a training set with size of 2jS mi j. But for UBOCFGMD, it may happen that there is no points partitioned into one cluster. In this case, for each cluster with points, we randomly select one to represent that cluster and for the clusters without points, we randomly select one from the nearest cluster with points. Here the nearest cluster is the one whose cluster number is closest to the cluster number of the concerned cluster.</p><p>For the original dataset in Table <ref type="table" target="#tab_0">1</ref>, suppose the non-bankrupt observations are clustered into four clusters based on Gaussian mixture distribution as follows: {M 1 , M 3 }, {M 2 , M 10 , M 8 }, {M 4 , M 6 , -M 5 , M 7 , M 9 }, {}. The fourth cluster happens to be null. The points selected for each cluster is similar to the process in UBOCFNN and for the fourth cluster, a point from cluster 3 is randomly selected to represent the fourth cluster. One example of S may be {I 1 , I 2 , I 3 , I 4 , M 1 , M 10 , M 6 , M 5 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Performance measures</head><p>Five common performance measures as follows are selected to evaluate the performance of the models: (5) Area under ROC curve (AUC): ROC graphs are two-dimensional graph in which Sensitivity is plotted on the Y axis and 1-Specificity is plotted on X axis. An ROC graph depicts relative trade-off between benefits (true non-bankruptcy) and costs (false non-bankruptcy), which is useful for organizing classifiers and visualizing their performance especially in the domains with skewed class distribution and unequal classification error costs. The AUC of a classifier is equivalent to the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">USA Bankruptcy Dataset (USABD)</head><p>The initial USABD consists of all firms from non-financial industry with observed financial status (Non-bankrupt or Bankrupt) from 1981 to 2009 in Compustat North America dataset provided by Wharton Research Data Service. The bankrupt company is defined as one whose reason for deletion is marked as ''bankruptcy'' or ''liquidation'' in the original Compustat North America dataset.</p><p>There is a wide variety of financial information declared by the company, therefore, it is always challenging to identify which group of information is effective in predicting bankruptcy. <ref type="bibr">Zhou et al.</ref> explored what information should be selected for bankruptcy prediction with different features ranking strategies for bankruptcy prediction <ref type="bibr" target="#b0">[1]</ref>. Tsai <ref type="bibr" target="#b36">[37]</ref> explored some features selection methods based on statistical characteristics for bankruptcy prediction, while Pacheco <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> and Unler <ref type="bibr" target="#b39">[40]</ref> employed heuristic optimization methods to select features for models in financial applications. However, we only restrict this study to those variables which are well accepted as explanatory variable in   <ref type="bibr" target="#b3">[4]</ref>. All these variables are financial ratios which can be computed in terms of financial items declared in the financial statements. The 10 explanatory variables selected in this study include: net income/total assets (NI/TA), current ratio (CR), retained earnings/total assets (RE/TA), working capital/total assets (WC/TA), EBIT/total assets (EBIT/TA), sales/total assets (S/ TA), cash/total assets (C/TA), current assets/total assets (CA/TA), stock holder's equity/total debt (SHE/TD), cash/current liabilities (C/CL). This set of variables includes the five important and widely used variables proposed by Altman <ref type="bibr" target="#b2">[3]</ref>.</p><p>The empirical study of this research is consistent with most of the previous literature. All prediction is conducted on base of yearly financial reports. The available financial data from a firm in the most recent fiscal year is used to do the bankruptcy prediction at the time of estimation. For the selected 10 variables, in the case of missing value, the missing value will be imputed with the previous available value. If there is no available value in all the previous years, then this case will be removed.</p><p>Fig. <ref type="figure" target="#fig_2">2</ref> shows the number of bankruptcies and non-bankruptcies by year over the sample period. As seen, the number of bankruptcies has been increasing over time, with the largest number of bankruptcies occurring in the late 1980s and early 1990s and in 1997 and 1998. Although subprime crisis happened in USA in 2007, since most bankrupt firms are in financial industries and some firms were officially declared as bankruptcy after 2009, there are only a couple of bankrupt firms after 2007 in this data set. Finally, there are a total of 918 bankrupt observations and 85,211 non-bankrupt observations from observed year 1981 to 2009. It shows that the dataset is highly imbalanced and the degree of imbalance ranges from 0/4128 (0) to 88/2181 (0.0403) over years.</p><p>To test the performance of different sampling and quantitative models, the total sample space is split into training sample set and test sample set. The training set consists of all observations from observed year 1981 to 2001. All models are estimated with the same training set and are used in predicting bankruptcies for the period 2002-2009.</p><p>The employed quantitative methods include linear discrimiant analysis (LDA), logistic regression (LOGR), decision tree C4.5 (DT), neural network (NN) and support vector machines (SVM). The decision tree C4.5 is implemented with the J48 function provided by Weka <ref type="bibr" target="#b34">[35]</ref>. Other models are implemented with Matlab on PC with 2G RAM. The neural network model is a three layer back propagation network. The support vector machine model adopts the least square SVM proposed by Suykens, et al. <ref type="bibr" target="#b40">[41]</ref>. The parameter k is set to be 5 in the SMOTE sampling methods while N is tested with value of 100, 500, 1000, 2000. The SMOTE methods with different N are denoted by SMOTE100, SMOTE500, SMOTE1K and SMOTE2K respectively.</p><p>For ROWR method, it just randomly duplicates the instances in minority class until the balance of two classes in the final training sample set. SMOTE generates new instances of minority class in terms of the parameter N. Both underampling method UBOCFNN and UBOCFGMD select a proportion of majority class to keep balance of two classes in the final training sample set. The simple sampling strategy to select all observations directly as the training set is also tested. This strategy is denoted by AS (all sample). The observations in or before 2001 compose the training sample set and others make the test sample set. The number of sample in the training set and test set from each sampling method is shown as Table <ref type="table" target="#tab_1">2</ref>. Since most previous literature uses random undersampling strategy and tests models on paired samples, to make the comparison of performance of the models on paired test sample set and highly imbalanced test set in real situation, the RU sampling strategy is tested on test sample set with paired sample and highly imbalanced sample set.    The results of predictive performance of above five quantitative models for test observations in year 2002-2009 are listed in Table <ref type="table" target="#tab_2">3</ref>. For ROWR and RU sampling strategies, the results are the average value of 10 iterations. For SMOTE with different values of N, the results are the average value of performance with different N.</p><p>To make a comparison among these sampling methods, Wilcoxon Signed-Rank test is employed. Unlike student test, this nonparametric test method does not require assumption of normal distribution of the random variable. Moreover, some empirical results suggest that it is also stronger than student test especially in the comparison of a pair of classifiers <ref type="bibr" target="#b41">[42]</ref>. Although both F-measure and AUC are commonly used measures for imbalanced dataset, all groups of test in this experiment show that the correla-tion coefficient of these two measure is only 0.2960. F-measure, like Sen, Spe and Acc, is determined by a fixed threshold value which defines the boundary value to classify the instance into bankruptcy or non-bankruptcy. AUC shows the relationship of Sen and 1-Spe with the change of threshold, the point with coordination of (Sen, 1-Spe) is a just one point in the ROC graph with a default threshold in the quantitative models, such as 0 for logistic regression in this study. In data mining community, AUC is the most proper performance measure for the imbalanced dataset, therefore, statistical comparison of sampling methods is conducted in terms of AUC. Table <ref type="table" target="#tab_3">4</ref> shows the p values of Wilcoxon signed rank tests between pairs of all the sampling methods that are tested on imbalanced test set with 88 bankrupt observations and 35,630 non-bankrupt observations. As seen, only AUC by UBOCFNN sampling method is significant less than that by RU. Since oversampling method ROWR and SMOTE and directly sampling all method (AS) cannot significantly increase performance of models, with a view to computational time, the undersampling methods: Most previous literature trains and tests models with balanced sample by random undersampling method, but in real world, the test sample set is highly imbalanced. Therefore, to see if there is any difference on performance of models trained by the balanced sample but tested on balanced and highly imbalanced test set, Wilcoxon signed rank test is employ to make a comparison between two groups of results: Group 1 consists of all performance results from five models (LDA, LOGR, DT, NN, SVM) trained by 1660 instances (830 bankruptcies and 830 non-bankruptcies) and tested on 35,718 instances (88 bankruptcies and 35,630 non-bankruptcies); Group 2 consists of all performance results from the same five models trained by same training set as Group 1 but tested on 176 instances (88 bankruptcies and 88 non-bankruptcies). The p values of Wilcoxon signed rank test on different performance measures of these two groups are listed in Table <ref type="table" target="#tab_4">5</ref>. It shows that there is significant difference on the performance of Sensitivity, F-measure and AUC. It is interesting to observe in this experiment that above three performance measures of models tested on paired sample is underestimated when compared to those tested on highly imbalanced sample. Although the difference between them is slight, it is statistically significant.</p><p>Fig. <ref type="figure" target="#fig_6">3</ref> shows the AUC of four models trained with sample set obtained by SMOTE sampling method with different parameter N. There is no clear relationship between the parameter N and AUC performance, but NN achieves the biggest AUC (0.8142) with N = 2000.</p><p>The summary statistics of AUC performance on imbalanced test set of five methods trained by 10 different sample sets obtained with RU method is show in Table <ref type="table" target="#tab_5">6</ref>. It shows that SVM model gets the best mean AUC with the lowest standard deviation in the 10 iterations of test with different training sample sets. Fig. <ref type="figure" target="#fig_3">4</ref> shows the AUC of NN and SVM of the 10 iterations of tests. It indicates that both NN and SVM can perform stably even they are trained with different randomly paired samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Japanese Bankruptcy Dataset (JPNBD)</head><p>Japanese Bankruptcy Dataset is smaller than USABD. The samples in JPNBDS are retrieved from Compustat Global, Wharton Research Data Service. Only non-financial firms are included. JPNBDS include samples with observed financial status (Non-Bankrupt or Bankrupt) from 1989 to 2009. The bankrupt company is defined as ''bankruptcy'' or ''liquidation'' in the original database. Fig. <ref type="figure">5</ref> shows the number of bankruptcies and non-bankruptcies by year over the sample period. Finally, there are a total of 58 bankrupt observations and 36,578 non-bankrupt observations from observed year 1989 to 2009. It shows that the dataset is highly imbalanced and the degree of imbalance ranges from 0/2547 (0) to 10/ 2085 (0.0048) over years.</p><p>The variables selection and models setting on JPNBD is the same as that for USABD. The observations in or before 2001 compose the training sample space and others make the test sample set. The number of sample in the training set and test set from each sampling method is shown as Table <ref type="table" target="#tab_6">7</ref>. The number of final selected sample by the undersampling methods is only 46 in the training sample set including 23 bankruptcies and 23 non-bankruptcies; therefore, it is a classification problem with small-size sample.</p><p>The results of predictive performance of the five quantitative models for test observations in year 2002-2009 are listed in Table 8. For ROWR and RU sampling strategies, the results are the average value of 10 runs. For SMOTE with different values of N, the results are the average value of performance with different N.    <ref type="table" target="#tab_9">10</ref>, there is no significant difference between the two group test on all performance measure except F-measure, which indicates that it would be proper to use the paired training sample and test sample in the models development and models selection. Among all test with different sampling methods and quantitative models, the LOGR with SMOTE sampling strategy achieves the best AUC performance (0.9269) and Fig. <ref type="figure" target="#fig_8">6</ref> shows the AUC of four models trained with sample set obtained by SMOTE sampling method with different parameter N. There is no clear relationship between the parameter N and AUC performance, but LOGR and LDA perform stably with different N. Therefore, in the case that there is only dozens of minority class sample, oversampling method could be better choice for the model development. In this experiment, SVM failed in the oversampling method due to high computational space requirement, a combination of SMOTE and undersampling strategy may reduce the total sample size and therefore reduce the space requirement in SVM model construction. The summary statistics of AUC performance on imbalanced test set of five methods trained by 10 different sample sets obtained with RU method is show in Table <ref type="table" target="#tab_12">11</ref>. It shows that LDA model get the greatest mean AUC and followed by SVM in the 10 runs of test with different training sample sets. Fig. <ref type="figure">7</ref> shows the AUC of LDA and SVM of the 10 runs of tests. It indicates that both LDA and SVM can perform stably even they are trained with different paired samples obtained by RU method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper investigates the effect of six different sampling methods on the performance of five quantitative bankruptcy prediction models. Each sampling method and quantitative model is tested on two datasets. The experimental results shows that when there are hundreds of bankrupt observations in the dataset, undersampling method is better than oversampling method because there is no significant difference on performance but oversampling method consumes more computational time. When there are only dozens of bankrupt cases in the dataset, oversampling method SMOTE is a better choice and if the training sample size is too large to cause the failure of model construction, the combination of SMOTE and undersampling maybe an alternative. In the test on both datasets, it is interesting to observe that the difference of AUC performance of all models, trained by sample set obtained by random undersample method, tested on random paired sample and real highly imbalanced sample is very slight or not significant, therefore, in the bankruptcy prediction model selection, the models can be evaluated and measured on their performance on random balanced sample set instead of the real highly imbalanced test sample.</p><p>This paper mainly focuses on the sampling method for bankruptcy prediction model construction with highly imbalanced dataset. All tested quantitative models just adopt the fundamental form and have no parameters and model optimization. The performance of models varies with the sampling method, but SVM can achieve good performance in most scenarios. There are a lot of bankruptcy prediction models, in practice, model selection process should be conducted since no model can always perform well. How sample distribution affects the power of prediction models? Can we identify the difficult and easy observation for test in terms of characteristics of training sample and prediction models? These problems will be our future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, the performance of bankruptcy prediction model is not only dependent on what features are selected and what quantitative methods are employed, but also dependent on what samples are selected and used for fitting the models. The form of a model is determined by the type of the quantitative approach, linear or nonlinear, implicit or explicit, but the parameters in the models are determined by the data-fitting process and therefore are determined by the selection of samples. Zhou et al. [1] investigated the performance of more than 20 models constructed by different quantitative methods with different features sets selected by six different features ranking techniques. The study just tried to explore what features should be selected and what quantitative methods should be employed in the development of corporate bankruptcy prediction models. It used paired samples as what most research in bankruptcy prediction did. The numbers of bankrupt and non-bankrupt observations are the same in the data set by randomly undersampling the non-bankrupt observations in the original data set. This study is to investigate how the performance of widely used bankruptcy prediction models is affected by different training sample sets which are used to estimate the models and are selected by different sampling strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Development processes of empirical bankruptcy prediction models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 .</head><label>2</label><figDesc>SMOTE (jS mi j, N, k) Input: Number of minority class samples jS mi j; Amount of minority class being oversampled N%; Number of nearest neighbors k Output: Union of (N/100) Â jS mi j synthetic minority class samples and the majority set S ma 1. if N &lt; 100 then Randomize the jS mi j minority class samples T = (N/100) Â jS mi j N = 100 endif 2. N = (int)(N/100) 3. k = number of neighbors 4. numattrs = Number of attributes 5. Sample[][]: array for original minority class samples 6. newindex = 0//keeps a count of number of synthetic samples generated 7. Synthetic[][]: array for synthetic samples 8. for i = 1 to jS mi j Compute k nearest neighbors for i, and save the indices in the nnarray Populate (N, i, nnarray, newindex, Synthetic) endfor Populate (N, i, nnarray, newindex, Synthetic) 9. while N -0 nn = random number between 1 and k for attr = 1 to numattrs dif = Sample[nnarray[nn]][attr]ÀSample[nnarray[i]][attr] gap = random number between 0 and 1 Synthetic[newindex][attr] = Sample[i][attr] + gap Â dif endfor newindex++ N = N À 1 Endwhile</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 4 .</head><label>4</label><figDesc>UBOCFNN (S mi , S ma ) Input: The original sample set of minority class S mi and sample set of majority class S ma . Output: sample set with balanced class S of size 2jS mi j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )( 2 )( 3 )</head><label>123</label><figDesc>Sensitivity ðSenÞ ¼ NN NBþNN Specificity ðSpeÞ ¼ BB BNþBB Accuracy ðAccÞ ¼ NNþBB NBþNNþBBþBN where NN: non-bankruptcy classified as bankruptcy, NB: non-bankruptcy classified as bankruptcy, BB: bankruptcy classified as bankruptcy, BN: bankruptcy classified as nonbankruptcy. (4) F-measure ðFÞ ¼ 2ÂPrecisionÂRecall ðPrecisionþRecallÞ ½35 where Precision ¼ NN NNþBN , Recall = Sensitivity. Precision shows what proportion of observations classified as nonbankruptcy by a model is real non-bankrupt, while Recall/ Sensitivity shows what proportion of the real non-bankrupt observations has been correctly classified as non-bankruptcy by the models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Bankruptcy and Non-bankruptcy by in USABD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>NFig. 3 .</head><label>3</label><figDesc>Fig. 3. AUC performance of models trained by different training set obtained by SMOTE sampling method with different N on USABD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.</head><label></label><figDesc>Fig. Bankruptcy and Non-bankruptcy by year in JPNBD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>NFig. 6 .</head><label>6</label><figDesc>Fig. 6. AUC performance of models trained by different training set obtained by SMOTE sampling method with different N on JPNBD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>A simple example of original training dataset.For the original training dataset shown in Table1, suppose the parameters in SMOTE algorithm are set as follows:jS mi j = 4, N = 300, k = 2, then the output of SMOTE is a training set with a total of 22 observations including 12 bankrupt observations and 10 non-bankrupt observations {M 1 ,M 2 , . . . , M 10 }. The most import step to generate the 12 bankrupt observations is the step 8 in above SMOTE. Suppose the i th feature of observation with ID I j is denoted by F j_i . For each bankrupt observation, the step 8 generate another N/100 = 3 observations which are so called ''synthetic'' examples. For I 1 , suppose its two nearest neighbors are I 2 and I 4 and the random number nn in step 9 in SMOTE is 2, then one synthetic example b I 1 1 for I 1 is generated as follows: Random under-sampling method is to balance class distribution through the random elimination of majority class examples</figDesc><table><row><cell>ID.</cell><cell>F 1</cell><cell>F 2</cell><cell>Á Á Á</cell><cell>F M</cell><cell>Bankrupt</cell></row><row><cell>I 1</cell><cell>1.05</cell><cell>0.03</cell><cell>Á Á Á</cell><cell>0.01</cell><cell>1</cell></row><row><cell>I 2</cell><cell>1.20</cell><cell>0.03</cell><cell>Á Á Á</cell><cell>0.04</cell><cell>1</cell></row><row><cell>I 3</cell><cell>0.67</cell><cell>0.01</cell><cell>Á Á Á</cell><cell>À0.12</cell><cell>1</cell></row><row><cell>I 4</cell><cell>0.57</cell><cell>0.21</cell><cell>Á Á Á</cell><cell>À0.15</cell><cell>1</cell></row><row><cell>M 1</cell><cell>0.24</cell><cell>À0.57</cell><cell>Á Á Á</cell><cell>À0.64</cell><cell>0</cell></row><row><cell>M 2</cell><cell>8.50</cell><cell>À0.13</cell><cell>Á Á Á</cell><cell>0.88</cell><cell>0</cell></row><row><cell>M 3</cell><cell>12.47</cell><cell>À0.10</cell><cell>Á Á Á</cell><cell>0.89</cell><cell>0</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell>M 10</cell><cell>0.58</cell><cell>0.01</cell><cell>Á Á Á</cell><cell>À0.10</cell><cell>0</cell></row></table><note><p><p><p><p>F 1À1 ¼ 1:05; F 1À2 ¼ 0:03; . . . ; F 1ÀM ¼ 0:01</p>F 4À1 ¼ 0:57; F 4À2 ¼ 0:21; . . . ; F 4ÀM ¼ À0:15 DF 1 ¼ 0:57 À 1:05 ¼ À0:58; DF 2 ¼ 0:21 À 0:03 ¼ 0:18; . . . ; DF i ¼ F 4Ài À F 1Ài ; . . . ; DF M ¼ F 4ÀM À F 1ÀM</p>ðF</p>1 ; F 2 ; . . . ; F i ; . . . ; F M Þ ¼ ð1:05; 0:03; . . . ; F 1Ài ; . . . ; 0:01Þ þ RandðÞ Â ðDF 1 ; DF 2 ; . . . ; DF i ; . . . ; DF M Þ Rand() is a function to generate any real number between 0 and 1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Sample size of training set and test set by different sampling methods for USABD.</figDesc><table><row><cell>Sampling methods</cell><cell></cell><cell>ROWR</cell><cell>SMOTE N = 100</cell><cell>RU</cell><cell>UBOCFNN &amp; UBOCFGMD</cell><cell>AS</cell></row><row><cell>Training set size</cell><cell>Bankruptcy</cell><cell>49,581</cell><cell>1660</cell><cell>830</cell><cell>830</cell><cell>830</cell></row><row><cell></cell><cell>Non-bankruptcy</cell><cell>49,581</cell><cell>49,581</cell><cell>830</cell><cell>830</cell><cell>49,581</cell></row><row><cell>Test set size</cell><cell>Bankruptcy</cell><cell>88</cell><cell>88</cell><cell>88</cell><cell>88</cell><cell>88</cell></row><row><cell></cell><cell>Non-bankruptcy</cell><cell>35,630</cell><cell>35,630</cell><cell>35,630/88</cell><cell>35,630</cell><cell>35,630</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Performance of models on USABD by sampling methods.</figDesc><table><row><cell>Models</cell><cell>ROWR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 p</head><label>4</label><figDesc>Values of Wilcoxon signed rank test between pairs of sampling methods on USABD.</figDesc><table><row><cell></cell><cell cols="3">ROWR SMOTE RU</cell><cell cols="3">UBOCFNN UBOCFGMD AS</cell></row><row><cell>ROWR</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>0.500</cell><cell>1.000</cell><cell>0.250</cell></row><row><cell>SMOTE</cell><cell></cell><cell>1.000</cell><cell cols="2">0.625 0.125</cell><cell>0.875</cell><cell>0.625</cell></row><row><cell>RU</cell><cell></cell><cell></cell><cell>1.000</cell><cell>0.063 *</cell><cell>0.625</cell><cell>0.625</cell></row><row><cell>UBOCFNN</cell><cell></cell><cell></cell><cell></cell><cell>1.000</cell><cell>0.125</cell><cell>0.625</cell></row><row><cell>UBOCFGMD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.000</cell><cell>0.625</cell></row><row><cell>AS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 p</head><label>5</label><figDesc>Values of Wilcoxon signed rank test on performance measures of Groups 1 and 2 on USABD.</figDesc><table><row><cell></cell><cell>Sen</cell><cell>Spe</cell><cell>Acc</cell><cell>F</cell><cell>AUC</cell></row><row><cell>p Values</cell><cell>0.0049 *</cell><cell>0.6611</cell><cell>0.1909</cell><cell>0.0000 *</cell><cell>0.0043 *</cell></row></table><note><p>* Indicates significant different at 0.1 significance level.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Summary statistics of AUC performance on imbalanced test of five methods trained by 10 different sample sets obtained with RU method on USABD.</figDesc><table><row><cell></cell><cell>Min</cell><cell>Max</cell><cell>Mean</cell><cell>Median</cell><cell>Std</cell></row><row><cell>LDA</cell><cell>0.6619</cell><cell>0.7265</cell><cell>0.7047</cell><cell>0.7082</cell><cell>0.0197</cell></row><row><cell>LOGR</cell><cell>0.4955</cell><cell>0.7813</cell><cell>0.6428</cell><cell>0.6381</cell><cell>0.1162</cell></row><row><cell>DT</cell><cell>0.6931</cell><cell>0.7737</cell><cell>0.7460</cell><cell>0.7485</cell><cell>0.0234</cell></row><row><cell>NN</cell><cell>0.7546</cell><cell>0.8173</cell><cell>0.7881</cell><cell>0.7897</cell><cell>0.0188</cell></row><row><cell>SVM</cell><cell>0.7679</cell><cell>0.8080</cell><cell>0.7965</cell><cell>0.7999</cell><cell>0.0112</cell></row><row><cell cols="6">Fig. 4. AUC of NN and SVM trained by sample set obtained with RU sampling</cell></row><row><cell cols="2">method on USABD.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Sample size of training set and test set by different sampling method for JPNBD.</figDesc><table><row><cell>Sampling methods</cell><cell></cell><cell>ROWR</cell><cell>SMOTE N = 100</cell><cell>RU</cell><cell>UBOCFNN &amp; UBOCFGMD</cell><cell>AS</cell></row><row><cell>Training set size</cell><cell>Bankruptcy</cell><cell>17,280</cell><cell>46</cell><cell>23</cell><cell>23</cell><cell>23</cell></row><row><cell></cell><cell>Non-bankruptcy</cell><cell>17,280</cell><cell>17,280</cell><cell>23</cell><cell>23</cell><cell>17,280</cell></row><row><cell>Test set size</cell><cell>Bankruptcy</cell><cell>35</cell><cell>35</cell><cell>35</cell><cell>35</cell><cell>35</cell></row><row><cell></cell><cell>Non-bankruptcy</cell><cell>19,298</cell><cell>19,298</cell><cell>19,298/35</cell><cell>19,298</cell><cell>19,298</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>Performance of models on JPNBD by sampling methods.</figDesc><table><row><cell>Models</cell><cell>ROWR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 p</head><label>9</label><figDesc>Values of Wilcoxon signed rank test between pairs of sampling methods on USABD.</figDesc><table><row><cell></cell><cell cols="2">ROWR SMOTE RU</cell><cell cols="3">UBOCFNN UBOCFGMD AS</cell></row><row><cell>ROWR</cell><cell>1.0000 0.6250</cell><cell cols="2">0.8750 1.0000</cell><cell>1.0000</cell><cell>0.6250</cell></row><row><cell>SMOTE</cell><cell>1.0000</cell><cell cols="2">0.8750 0.6250</cell><cell>0.8750</cell><cell>0.1250</cell></row><row><cell>RU</cell><cell></cell><cell cols="2">1.0000 0.4375</cell><cell>0.8125</cell><cell>0.3750</cell></row><row><cell>UBOCFNN</cell><cell></cell><cell></cell><cell>1.0000</cell><cell>0.6250</cell><cell>0.2500</cell></row><row><cell>UBOCFGMD</cell><cell></cell><cell></cell><cell></cell><cell>1.0000</cell><cell>0.6250</cell></row><row><cell>AS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 p</head><label>10</label><figDesc>Values of Wilcoxon signed rank test on performance measures of Groups 1 and 2 on USABD.</figDesc><table><row><cell></cell><cell>Sen</cell><cell>Spe</cell><cell>Acc</cell><cell>F</cell><cell>AUC</cell></row><row><cell>p Values</cell><cell>0.8431</cell><cell>0.3235</cell><cell>0.2690</cell><cell>0.0000 *</cell><cell>0.8205</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc>shows the p values of Wilcoxon signed rank tests between two of all the sampling methods that tested on imbalanced test set with 35 bankrupt observations and 19,298 non-bankrupt observations. It indicates that there is no statistically significant difference among these sampling methods at 0.1 significance level.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11</head><label>11</label><figDesc>Summary statistics of AUC performance on imbalanced test of five methods trained by 10 different sample sets obtained with RU method on USABD. the difference of performance between two groups of test similar to that in USABD. Group 1 consists of all performance results from five models (LDA, LOGR, DT, NN, SVM) trained by 46 instances (23 bankruptcies and 23 non-bankruptcies) and tested on 19,333 instances (35 bankruptcies and 19,298 non-bankruptcies); Group 2 consists of all performance results from the same five models trained by the same training set as Group 1 but tested on 70 instances (35 bankruptcies and 35 non-bankruptcies). As seen in Table</figDesc><table><row><cell></cell><cell>Min</cell><cell>Max</cell><cell>Mean</cell><cell>Median</cell><cell>Std</cell></row><row><cell>LDA</cell><cell>0.7603</cell><cell>0.9342</cell><cell>0.8748</cell><cell>0.8868</cell><cell>0.0550</cell></row><row><cell>LOGR</cell><cell>0.6105</cell><cell>0.9216</cell><cell>0.8081</cell><cell>0.8207</cell><cell>0.0964</cell></row><row><cell>DT</cell><cell>0.6822</cell><cell>0.8738</cell><cell>0.8058</cell><cell>0.8167</cell><cell>0.0554</cell></row><row><cell>NN</cell><cell>0.6578</cell><cell>0.8554</cell><cell>0.7853</cell><cell>0.7868</cell><cell>0.0609</cell></row><row><cell>SVM</cell><cell>0.7925</cell><cell>0.8958</cell><cell>0.8467</cell><cell>0.8383</cell><cell>0.0339</cell></row></table><note><p><p>Fig. 7. AUC of NN and SVM trained by sample set obtained with RU sampling method on USABD.</p>shows</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>L. Zhou / Knowledge-Based Systems 41 (2013)<ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>* Indicates significant difference at 0.1 significance level.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>* Indicates significant different at 0.1 significance level.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Empirical models based on features ranking techniques for corporate financial distress prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Mathematics with Applications</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="2484" to="2496" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Financial ratios as predictors of failure</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Beaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Accounting Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="71" to="111" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Financial ratios, discriminant analysis and the prediction of corporate bankruptcy</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="589" to="609" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bankruptcy prediction in banks and firms via statistical and intelligent techniques -a review</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical methods for bankruptcy forecasting</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economics and Business</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="349" to="354" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical comparison of bankruptcy prediction models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Financial Management</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="52" to="57" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Business failure prediction using decision trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="536" to="555" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural network models and the prediction of bank bankruptcy</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Omega</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="429" to="445" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognizing financial distress patterns using a neural network tool</title>
		<author>
			<persName><forename type="first">P</forename><surname>Coats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Financial Management</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="142" to="155" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bankruptcy prediction using neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="545" to="557" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural network prediction analysis: the bankruptcy case</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leshno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Spector</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="125" to="147" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Support vector machines approach to pattern detection in bankruptcy prediction and its contingency</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bankruptcy prediction modeling with hybrid case-based reasoning and genetic algorithms approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="599" to="607" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bankruptcy forecasting: an empirical comparison of AdaBoost and neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alfaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garcı ´a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gámez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Elizondo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="110" to="122" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hybrid genetic algorithms and support vector machines for bankruptcy prediction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="652" to="660" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integration of case-based forecasting, neural network, and discriminant analysis for bankruptcy prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="415" to="422" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A novel bankruptcy prediction model based on an adaptive fuzzy k-nearest neighbor method</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1348" to="1359" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Determining membership functions and minimum fuzzy support in finding fuzzy association rules for classification problems</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="57" to="66" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An extended fuzzy measure on competitiveness correlation based on WCY 2011</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Tzeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="86" to="93" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analysis of an evolutionary RBFN design algorithm* CO2RBFN, for imbalanced data sets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Pérez-Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesus</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2375" to="2388" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge discovery from imbalanced and noisy data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="1513" to="1542" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cost-sensitive boosting for classification of imbalanced data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3358" to="3378" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evolutionary-based selection of generalized instances for imbalanced classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Garcı ´a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Derrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Carmona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3" to="12" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the effectiveness of preprocessing methods when dealing with different levels of class imbalance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Garcı ´a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Mollineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="13" to="21" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving bankruptcy prediction with Hidden Layer Learning Vector Quantization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vieira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Accounting Review</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="253" to="271" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A genetic programming approach for bankruptcy prediction using a highly unbalanced database</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alfaro-Cid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sharman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Esparcia-Alcázar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<biblScope unit="volume">4448</biblScope>
			<biblScope unit="page" from="169" to="178" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A semi-deterministic ensemble strategy for imbalanced datasets (SDEID) applied to bankruptcy prediction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Mathiasi Horta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S L</forename><surname>Pires De Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C H</forename><surname>Borges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WIT Transactions on Information and Communication Technologies</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="205" to="213" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cluster-based under-sampling approaches for imbalanced data distributions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="5718" to="5727" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Handling imbalanced datasets: a review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kotsiantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pintelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GESTS International Transactions on Computer Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A multiple resampling method for learning from imbalanced data sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Estabrooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="18" to="36" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SMOTE: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mixture of expert agents for handling imbalanced data sets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kotsiantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pintelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics, Computing &amp; TeleInformatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Statistics Toolbox User&apos;s Guide, The MathWorks</title>
		<author>
			<persName><surname>Mathworks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The WEKA data mining software: an update</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Roc Graphs: Notes and Practical Considerations for Data Mining Researchers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-01">January 2003</date>
			<pubPlace>Palo Alto, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>HP Laboratories</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature selection in bankruptcy prediction</title>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="120" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Use of VNS and TS in classification: variable selection and determination of the linear discrimination function coefficients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Núñez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA Journal of Management Mathematics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="191" to="206" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A variable selection method based in tabu search for logistic regression models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Núñez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="page" from="506" to="511" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A discrete particle swarm optimization method for feature selection in binary classification problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Unler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page" from="528" to="539" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Brabanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<title level="m">Least Squares Support Vector Machines</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data set</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
