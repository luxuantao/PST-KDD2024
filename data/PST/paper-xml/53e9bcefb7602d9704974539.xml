<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">P</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Cyber-Systems and Control</orgName>
								<orgName type="laboratory">National Laboratory of Industrial Control Technology</orgName>
								<orgName type="institution">Zhe-jiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering and Science</orgName>
								<orgName type="institution">Victoria University</orgName>
								<address>
									<postCode>8001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<postCode>5005</postCode>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BC00A96EABEBBFB135EA69B551D2817D</idno>
					<idno type="DOI">10.1109/TNNLS.2012.2232938</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dissipativity Analysis for Discrete-Time Stochastic</head><p>Neural Networks With Time-Varying Delays Zheng-Guang Wu, Peng Shi, Senior Member, IEEE, Hongye Su, and Jian Chu Abstract-In this paper, the problem of dissipativity analysis is discussed for discrete-time stochastic neural networks with timevarying discrete and finite-distributed delays. The discretized Jensen inequality and lower bounds lemma are adopted to deal with the involved finite sum quadratic terms, and a sufficient condition is derived to ensure the considered neural networks to be globally asymptotically stable in the mean square and strictly ( Q, S, R)-γ -dissipative, which is delay-dependent in the sense that it depends on not only the discrete delay but also the finite-distributed delay. Based on the dissipativity criterion, some special cases are also discussed. Compared with the existing ones, the merit of the proposed results in this paper lies in their reduced conservatism and less decision variables. Three examples are given to illustrate the effectiveness and benefits of our theoretical results.</p><p>Index Terms-Delay-dependent, dissipativity, neural networks, stochastic systems, time-delays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F OR more than a decade, many researchers have paid atten- tion to this paper on neural networks. This stems from the fact that neural networks have been successfully applied in a variety of areas, such as signal processing, pattern recognition, and combinatorial optimization <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. In recent years, neural networks with time-delay have also been studied extensively, because time delays do occur in electronic implementation of analog neural networks due to the transmission of signal and the finite switching speed of amplifiers, and may lead to the instability and poor performance of systems <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, and a great number of important and interesting results have been obtained on the analysis and synthesis of time-delay neural networks, including stability analysis, state estimation, passivity, etc. For example, the stability problem has been investigated for continuous-time neural networks with timedelay in <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>, and several stability conditions have been proposed based on linear matrix inequality (LMI) approach.</p><p>In the discrete-time setting, some sufficient criteria have been established in <ref type="bibr" target="#b11">[12]</ref> to ensure the delay-dependent stability of neural networks with time-varying delay. In <ref type="bibr" target="#b12">[13]</ref>, the state estimation problem has been investigated for continuous-time neural networks with time-delay, and some algorithms have been presented to compute the desired state estimators. The results on the state estimation problem of discrete-time neural networks with time-delay can be found in <ref type="bibr" target="#b13">[14]</ref>. The problem of passivity analysis for time-delay neural networks has been considered in <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, and some types of delay-dependent passivity conditions have been derived.</p><p>It should be pointed out that all the time-delays considered in the above-mentioned references are of the discrete nature. It is well known that neural networks usually have a spatial extent due to the presence of an amount of parallel pathways with a variety of axon sizes and lengths, and there will be a distribution of propagation delays. In this case, the signal propagation is no longer instantaneous and cannot be modeled with discrete delays <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Generally speaking, the distributed delays in the neural networks can be classified into two types, finite-distributed delays and infinite-distributed delays. Recent work has advanced the research for neural networks with distributed delays. Continuous-time neural networks with infinite-distributed delays have been investigated in <ref type="bibr" target="#b18">[19]</ref>. Based on the LMI method and other approaches, continuoustime neural networks with finite-distributed delays have also been discussed in <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b20">[21]</ref>. It seems that the corresponding results for discrete-time neural networks with distributed delays are relatively few. However, it has been shown that discrete-time neural networks are more important than continuous-time neural networks in our digital world. In <ref type="bibr" target="#b21">[22]</ref>, the authors made the first attempt to discuss the problem of stability analysis for discrete-time neural networks with infinitedistributed delays. The results obtained in <ref type="bibr" target="#b21">[22]</ref> include both deterministic and stochastic cases, and thus the results are very general and powerful. The problem of passivity analysis for a class of uncertain discrete-time stochastic neural networks with infinite-distributed delays has been investigated in <ref type="bibr" target="#b22">[23]</ref>, where some delay-dependent criteria have been established to ensure the passivity of the considered neural networks. In <ref type="bibr" target="#b23">[24]</ref>, the authors introduced the finite-distributed delay in the discrete-time setting for the first time, and have further investigated the state estimation problem for discrete-time neural networks with Markov jumping parameters as well as mode-dependent mixed time-delays, and sufficient conditions have been established that guarantee the existence of the state estimators.</p><p>In addition, the past decade has witnessed the rapid development of dissipative theory in system and control areas. The reason is mainly twofold: 1) the dissipative theory gives a framework for the design and analysis of control systems using an input-output description based on energy-related considerations <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and 2) the dissipative theory serves as a powerful or even indispensable tool in characterizing important system behaviors, such as stability and passivity, and has close connections with passivity theorem, bounded real lemma, Kalman-Yakubovich lemma, and the circle criterion <ref type="bibr" target="#b25">[26]</ref>. Very recently, in <ref type="bibr" target="#b26">[27]</ref>, the robust reliable dissipative filtering problem has been investigated for uncertain discrete-time singular system with interval time-varying delays and sensor failures. The problem of static output-feedback dissipative control has been studied for linear continuous-time system based on an augmented system approach in <ref type="bibr" target="#b27">[28]</ref>, where a necessary and sufficient condition for the existence of a desired controller has been given, and a corresponding iterative algorithm has also been developed to solve the condition. Note that the problem of dissipativity analysis has also been investigated for neural networks with time-delay in <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b29">[30]</ref>, and some delay-dependent sufficient conditions have been given to guarantee the dissipativity of the considered neural networks. However, the neural networks considered in <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b29">[30]</ref> are of the continuous-time nature. Unfortunately, up to now, there is little information in the published literature about the dissipativity analysis problem for discrete-time stochastic neural networks with discrete and finite-distributed delays. It is, therefore, the main purpose of the present research to linkage such a gap by making the first attempt to deal with the dissipativity analysis problem for discrete-time stochastic neural networks with both time-varying discrete and finitedistributed delays.</p><p>In this paper, we consider the problem of dissipativity analysis for discrete-time stochastic neural networks with both time-varying discrete and finite-distributed delays. Based on the discretized Jensen inequality and lower bounds lemma proposed in <ref type="bibr" target="#b30">[31]</ref>, a condition is established ensuring the stability and strict (Q, S, R)-γ -dissipativity of the considered neural networks, which depends not only on the discrete delay but also on the finite-distributed delay. Based on the derived condition, we also develop some results on several special cases. The obtained results have advantages over the existing ones because they not only have less conservatism but also require less decision variables. Three numerical examples are given to illustrate the effectiveness and superiority of the proposed methods.</p><p>Notation: The notations used throughout this paper are fairly standard. R n and R m×n denote the n-dimensional Euclidean space and the set of all m × n real matrices, respectively. The notation X &gt; Y (X ≥ Y ), where X and Y are symmetric matrices, means that X -Y is positive definite (positive semidefinite). I and 0 represent the identity matrix and a zero matrix, respectively. ( , F , P) is a probability space, is the sample space, F is the σ -algebra of subsets of the sample space, and P is the probability measure on F . E[•] denotes the expectation operator with respect to some probability measure P. For integers a and b with a &lt; b, </p><formula xml:id="formula_0">N[a, b] = {a, a + 1, . . . ,</formula><formula xml:id="formula_1">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ x(k + 1) = Dx(k) + Ag(x(k)) + Bg(x(k -d(k))) + u(k) + C τ (k) v=1 g(x(k -v)) + σ (k, x(k), x(k -d(k)))ω(k) y(k) = g(x(k)) x(k) = φ(k), k ∈ N[-max{d 2 , τ 2 }, 0]<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">x(k) = x 1 (k)x 2 (k) . . . x n (k) T , g(x(k)) = g 1 (x 1 (k)) g 2 (x 2 (k)) . . . g n (x n (k))</formula><p>T , and x i (k) is the state of the i -th neuron at time k, g i (x i (k)) denotes the activation function of the i -th neuron at time k, y(k) is the output of the neural network, u(k) ∈ l 2 [0, +∞) is the input, the function φ(k) is the initial condition; D = diag{ d1 , d2 , . . . , dn } describes the rate with which the each neuron will reset its potential to the resting state in isolation when disconnected from the networks and external inputs; A = (a i j ) n×n , B = (b i j ) n×n , and C = (c i j ) n×n are, respectively, the connection weight matrix, the discretely delayed connection weight matrix, and the distributively delayed connection weight matrix; ω(k) is a scalar Wiener process (Brownian Motion) on ( , F , P) with</p><formula xml:id="formula_3">E[ω(k)] = 0, E[ω(k) 2 ] = 1, E[ω(i )ω( j )] = 0 (i = j ) (2)</formula><p>d(k) and τ (k) denote the discrete delay and the finitedistributed delay, respectively, and satisfy</p><formula xml:id="formula_4">d 1 ≤ d(k) ≤ d 2 and τ 1 ≤ τ (k) ≤ τ 2 , where d 2 ≥ d 1 &gt; 0 and τ 2 ≥ τ 1 &gt; 0 are prescribed integers.</formula><p>The following assumptions are imposed on neural network (1), which will be needed to develop our main results in this paper.</p><p>Assumption 1 <ref type="bibr" target="#b19">[20]</ref>: Each activation function g i (•) in ( <ref type="formula" target="#formula_1">1</ref>) is continuous and bounded, and g i (0) = 0, and there exist constants δ i and ρ i such that</p><formula xml:id="formula_5">δ i ≤ g i (α 1 ) -g i (α 2 ) α 1 -α 2 ≤ ρ i , i = 1, 2, . . . , n<label>(3)</label></formula><p>where α 1 , α 2 ∈ R, and</p><formula xml:id="formula_6">α 1 = α 2 .</formula><p>Assumption 2 <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_7">σ (k) = σ (k, x(k), x(k -d(k))) : N × R n ×R n → R n</formula><p>is the diffusion coefficient vector, and assumed to satisfy</p><formula xml:id="formula_8">σ (k) T σ (k) ≤ x(k) x(k -d(k)) T G 1 G 2 * G 3 x(k) x(k -d(k)) (4)</formula><p>where</p><formula xml:id="formula_9">G 1 G 2 * G 3</formula><p>≥ 0 is known constant matrices.</p><p>We now introduce the following definitions for neural network <ref type="bibr" target="#b0">(1)</ref>.</p><p>Definition 1: The neural network (1) with u(k) = 0 is said to be globally asymptotically stable in the mean square, if for each solution x(k) of (1) with u(k) = 0, the following holds:</p><formula xml:id="formula_10">lim k→+∞ E[||x(k)|| 2 ] = 0. (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>Before giving the definition of dissipativity for neural network (1), we introduce a function r (u, y) : R m × R l → R satisfying r (0, 0) = 0, associated with neural network <ref type="bibr" target="#b0">(1)</ref>, which is called a supply rate if for all input-output pairs (u, y) satisfying (1), r (u, y)</p><formula xml:id="formula_12">satisfies k 2 α=k 1 |E[r (u(α), y(α))]| &lt; +∞, k 2 ≥ k 1 ≥ 0.</formula><p>The classical form of dissipation inequality <ref type="bibr" target="#b24">[25]</ref> can be extended to the neural network (1) as follows:</p><formula xml:id="formula_13">τ α=0 E[r (u(α), y(α))] ≥ 0 ∀τ ≥ 0.<label>(6)</label></formula><p>The neural network ( <ref type="formula" target="#formula_1">1</ref>) is dissipative with respect to the supply rate r (u, y) if the dissipation inequality ( <ref type="formula" target="#formula_13">6</ref>) holds under zero initial condition for any nonzero input u ∈ l 2 [0, +∞). It is easy to see that if there exists a nonnegative-definite function</p><formula xml:id="formula_14">V (x) : R n → R satisfying V (0) = 0 and E[V (x(τ + 1))] -E[V (x(0))] ≤ τ α=0 E[r (u(α), y(α))], τ ≥ 0 (7)</formula><p>then the neural network ( <ref type="formula" target="#formula_1">1</ref>) is dissipative with respect to the supply rate r (u, y). In this case, the function V (x) is called a storage function for the neural network (1).</p><p>Motivated by <ref type="bibr" target="#b31">[32]</ref>, in this paper, we consider a quadratic supply rate r (u, y). Specifically, the supply rate r (u, y) associated with neural network (1) is defined by</p><formula xml:id="formula_15">r (u, y) = y T Qy + 2y T Su + u T Ru (8)</formula><p>where Q, S, and R are real matrices with Q and R symmetric.</p><p>In this paper, it is assumed that Q ≤ 0. Definition 2: The neural network ( <ref type="formula" target="#formula_1">1</ref>) is said to be (Q, S, R)dissipative, the dissipation inequality (6) holds under zero initial condition for any nonzero input u ∈ l 2 [0, +∞). Furthermore, if for some scalar γ &gt; 0, the dissipation inequality</p><formula xml:id="formula_16">τ α=0 E[r (u(α), y(α))] ≥ γ τ α=0 E[u(α) T u(α)] ∀τ ≥ 0 (9)</formula><p>holds under zero initial condition for any nonzero input u ∈ l 2 [0, +∞), then the neural network ( <ref type="formula" target="#formula_1">1</ref>) is said to be strictly</p><formula xml:id="formula_17">(Q, S, R)-γ -dissipative.</formula><p>In this paper, we focus on the problem of dissipativity analysis for neural network (1). More specifically, by utilizing the LMI approach, we will derive a delay-dependent sufficient condition, under which the following two requirements are met simultaneously:</p><p>1) the neural network (1) with u(k) = 0 is globally asymptotically stable in the mean square;</p><p>2) the neural network (1) is strictly (Q, S, R)-γ -dissipative. In this case, the neural network ( <ref type="formula" target="#formula_1">1</ref>) is said to be globally asymptotically stable in the mean square and strictly (Q, S, R)-γ -dissipative;</p><p>In order to solve the above-mentioned problem, we present the following results.</p><p>Lemma 1 (Discretized Jensen Inequality) <ref type="bibr" target="#b32">[33]</ref>: For any matrix M &gt; 0, integers γ 1 and γ 2 satisfying γ 2 &gt; γ 1 , and vector function ω : N[γ 1 , γ 2 ] → R n , such that the sums concerned are well defined, then</p><formula xml:id="formula_18">(γ 2 -γ 1 + 1) γ 2 α=γ 1 ω(α) T Mω(α) ≥ γ 2 α=γ 1 ω(α) T M γ 2 α=γ 1 ω(α) (<label>10</label></formula><formula xml:id="formula_19">)</formula><p>Lemma 2 <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_20">For any matrix M S * M &gt; 0, integers d 1 , d 2 , d(k) satisfying d 1 ≤ d(k) ≤ d 2 , and vector function x(k + •) : N[-d 2 , -d 1 ] → R n</formula><p>, such that the sums concerned are well defined, then</p><formula xml:id="formula_21">-(d 2 -d 1 ) k-d 1 -1 α=k-d 2 ζ(α) T Mζ(α) ≤ (k) T Π (k) (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>where ζ(α) = x(α + 1)x(α) and</p><formula xml:id="formula_23">(k) = x(k -d 1 ) T x(k -d(k)) T x(k -d 2 ) T T Π = ⎡ ⎣ -M M-S S * -2M + S + S T -S + M * * -M ⎤ ⎦ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MAIN RESULTS</head><p>In this section, we will focus our study on dissipativity analysis of neural network <ref type="bibr" target="#b0">(1)</ref>. For simplicity, in the following, we denote</p><formula xml:id="formula_24">F 1 = diag{δ 1 ρ 1 , δ 2 ρ 2 , . . . , δ n ρ n } F 2 = diag δ 1 + ρ 1 2 , δ 2 + ρ 2 2 , . . . , δ n + ρ n 2 d 12 = d 2 -d 1 , ϑ = τ 2 (τ 2 + τ 1 )(τ 2 -τ 1 + 1) 2 .</formula><p>Theorem 1: Under Assumptions 1 and 2, neural network (1) is globally asymptotically stable in the mean square and strictly (Q, S, R)-γ -dissipative, if there exist</p><formula xml:id="formula_25">P &gt; 0, Q 1 &gt; 0, Q 2 &gt; 0, Q 3 &gt; 0, Z 1 &gt; 0, Z 2 &gt; 0, R &gt; 0, S, diagonal matrices Y &gt; 0, H &gt; 0,</formula><p>and scalars ρ &gt; 0 and γ &gt; 0 such that</p><formula xml:id="formula_26">P + d 2 1 Z 1 + d 2 12 Z 2 ≤ ρ I (12a) Ξ = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Ξ 11 Z 1 ρG 2 0 Ξ 15 Ξ 16 Ξ 17 Ξ 18 * Ξ 22 Ξ 23 S 0 0 0 0 * * Ξ 33 Ξ 34 0 F 2 H 0 0 * * * Ξ 44 0 0 0 0 * * * * Ξ 55 Ξ 56 Ξ 57 Ξ 58 * * * * * Ξ 66 Ξ 67 Ξ 68 * * * * * * Ξ 77 Ξ 78 * * * * * * * Ξ 88 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ &lt; 0 (12b) Z 2 S * Z 2 &gt; 0 (12c)</formula><p>where</p><formula xml:id="formula_27">Ξ 11 = D P D -P + Q 1 + (d 12 + 1)Q 2 + Q 3 +d 2 1 (D -I )Z 1 (D -I ) -Z 1 +d 2 12 (D -I )Z 2 (D -I ) + ρG 1 -F 1 Y Ξ 15 = D P A + d 2 1 (D -I )Z 1 A + d 2 12 (D -I )Z 2 A + F 2 Y Ξ 16 = D P B + d 2 1 (D -I )Z 1 B + d 2 12 (D -I )Z 2 B Ξ 17 = D PC + d 2 1 (D -I )Z 1 C + d 2 12 (D -I )Z 2 C Ξ 18 = D P + d 2 1 (D -I )Z 1 + d 2 12 (D -I )Z 2 Ξ 22 = -Q 1 -Z 1 -Z 2 Ξ 23 = Z 2 -S Ξ 33 = -Q 2 -2Z 2 + S + S T + ρG 3 -F 1 H Ξ 34 = -S + Z 2 Ξ 44 = -Q 3 -Z 2 Ξ 55 = A T P A + d 2 1 A T Z 1 A + d 2 12 A T Z 2 A + ϑ R -Y -Q Ξ 56 = A T P B + d 2 1 A T Z 1 B + d 2 12 A T Z 2 B Ξ 57 = A T PC + d 2 1 A T Z 1 C + d 2 12 A T Z 2 C Ξ 58 = -S + A T P + d 2 1 A T Z 1 + d 2 12 A T Z 2 Ξ 66 = B T P B + d 2 1 B T Z 1 B + d 2 12 B T Z 2 B -H Ξ 67 = B T PC + d 2 1 B T Z 1 C + d 2 12 B T Z 2 C Ξ 68 = B T P + d 2 1 B T Z 1 + d 2 12 B T Z 2 Ξ 77 = C T PC + d 2 1 C T Z 1 C + d 2 12 C T Z 2 C -R Ξ 78 = C T P + d 2 1 C T Z 1 + d 2 12 C T Z 2 Ξ 88 = P + d 2 1 Z 1 + d 2 12 Z 2 -R + γ I.</formula><p>Proof: First, we prove the stability of neural network <ref type="bibr" target="#b0">(1)</ref>. To this end, we define η(k) = x(k + 1)x(k) and consider the following Lyapunov functional for neural network (1) with u(k) = 0:</p><formula xml:id="formula_28">V (k, x(k)) = 7 s=1 V s (k, x(k)) (<label>13</label></formula><formula xml:id="formula_29">)</formula><p>where</p><formula xml:id="formula_30">V 1 (k, x(k)) = x(k) T Px(k) V 2 (k, x(k)) = k-1 α=k-d 1 x(α) T Q 1 x(α) V 3 (k, x(k)) = -d 1 +1 β=-d 2 +1 k-1 α=k-1+β x(α)Q 2 x(α) V 4 (k, x(k)) = k-1 α=k-d 2 x(α) T Q 3 x(α) V 5 (k, x(k)) = d 1 -1 β=-d 1 k-1 α=k+β η(α) T Z 1 η(α) V 6 (k, x(k)) = d 12 -d 1 -1 β=-d 2 k-1 α=k+β η(α) T Z 2 η(α) V 7 (k, x(k)) = τ 2 τ 2 β=τ 1 β v=1 k-1 α=k-v g(x(α)) T Rg(x(α)). Letting E[ΔV (k)] = E[V (k + 1, x(k + 1)) -V (k, x(k))],</formula><p>along the solution of neural network (1) with u(k) = 0, we have</p><formula xml:id="formula_31">E[ΔV 1 (k)] = E x(k +1) T Px(k +1)-x(k) T Px(k) = E x(k) T D P Dx(k) + 2x(k) T D PAg(x(k)) + 2x(k) T D P Bg(x(k -d(k))) + 2x(k) T D PC τ (k) v=1 g(x(k -v)) + g(x(k)) T A T P Ag(x(k)) + 2g(x(k)) T A T P Bg(x(k -d(k))) + 2g(x(k)) T A T PC τ (k) v=1 g(x(k -v)) + g(x(k -d(k))) T B T P Bg(x(k -d(k))) + 2g(x(k -d(k))) T B T PC τ (k) v=1 g(x(k -v)) + τ (k) v=1 g(x(k -v)) T C T PC τ (k) v=1 g(x(k -v)) + σ(k) T P σ (k) -x(k) T Px(k) (14) E[ΔV 2 (k)] = E x(k) T Q 1 x(k)-x(k -d 1 ) T Q 1 x(k -d 1 ) (15) E[ΔV 3 (k)] = E ⎡ ⎣ (d 2 -d 1 + 1)x(k) T Q 2 x(k) - k-d 1 α=k-d 2 x(α) T Q 2 x(α) ⎤ ⎦ ≤ E (d 12 + 1)x(k) T Q 2 x(k) -x(k -d(k)) T Q 2 x(k -d(k)) (16) E[ΔV 4 (k)] = E x(k) T Q 3 x(k) -x(k -d 2 ) T Q 3 x(k -d 2 ) (17) E[ΔV 5 (k)] = E ⎡ ⎣ d 2 1 η(k) T Z 1 η(k)-d 1 k-1 α=k-d 1 η(α) T Z 1 η(α) ⎤ ⎦ (18) E[ΔV 6 (k)] = E ⎡ ⎣ d 2 12 η(k) T Z 2 η(k) -d 12 k-d 1 -1 α=k-d 2 η(α) T Z 2 η(α) ⎤ ⎦ (19) E[ΔV 7 (k)] = E ϑg(x(k)) T Rg(x(k)) -E ⎡ ⎣ τ 2 τ 2 β=τ 1 β v=1 g(x(k -v)) T Rg(x(k -v)) ⎤ ⎦ ≤ E ϑg(x(k)) T Rg(x(k)) -E ⎡ ⎣ τ 2 τ (k) v=1 g(x(k -v)) T Rg(x(k -v)) ⎤ ⎦ . (<label>20</label></formula><formula xml:id="formula_32">)</formula><p>On the other hand, applying Lemma 1, we have</p><formula xml:id="formula_33">-d 1 k-1 α=k-d 1 η(α) T Z 1 η(α) ≤ - k-1 α=k-d 1 η(α) T Z 1 k-1 α=k-d 1 η(α) = -x(k) T Z 1 x(k) + 2x(k) T Z 1 x(k -d 1 ) -x(k -d 1 ) T Z 1 x(k -d 1 ). (<label>21</label></formula><formula xml:id="formula_34">)</formula><p>Note that η(k) = x(k + 1)x(k) and thus in the case of</p><formula xml:id="formula_35">u(k) = 0 η(k) = (D -I )x(k) + Ag(x(k)) + Bg(x(k -d(k))) +C τ (k) v=1 g(x(k -v)) + σ (k, x(k), x(k -d(k)))ω(k). (<label>22</label></formula><formula xml:id="formula_36">)</formula><p>Based on <ref type="bibr" target="#b17">(18)</ref>, <ref type="bibr" target="#b20">(21)</ref>, and ( <ref type="formula" target="#formula_35">22</ref>), we have that</p><formula xml:id="formula_37">E[ΔV 5 (k)] ≤ E d 2 1 x(k) T (D -I )Z 1 (D -I )x(k) + 2d 2 1 x(k) T (D -I )Z 1 Ag(x(k)) + 2d 2 1 x(k) T (D -I )Z 1 Bg(x(k -d(k))) + 2d 2 1 x(k) T (D -I )Z 1 C τ (k) v=1 g(x(k -v)) + d 2 1 g(x(k)) T A T Z 1 Ag(x(k)) + 2d 2 1 g(x(k)) T A T Z 1 Bg(x(k -d(k))) + 2d 2 1 g(x(k)) T A T Z 1 C τ (k) v=1 g(x(k -v)) + d 2 1 g(x(k -d(k))) T B T Z 1 Bg(x(k -d(k))) + 2d 2 1 g(x(k -d(k))) T B T Z 1 C τ (k) v=1 g(x(k -v)) + d 2 1 τ (k) v=1 g(x(k -v)) T C T Z 1 C τ (k) v=1 g(x(k -v)) + d 2 1 σ (k) T Z 1 σ (k) -x(k) T Z 1 x(k) + 2x(k) T Z 1 x(k -d 1 ) -x(k -d 1 ) T Z 1 x(k -d 1 ) . (<label>23</label></formula><formula xml:id="formula_38">)</formula><p>Furthermore, according to Lemma 2, we have</p><formula xml:id="formula_39">E[ΔV 6 (k)] = E d 2 12 x(k) T (D -I )Z 2 (D -I )x(k) + 2d 2 12 x(k) T (D -I )Z 2 Ag(x(k)) + 2d 2 12 x(k) T (D -I )Z 2 Bg(x(k -d(k))) + 2d 2 12 x(k) T (D -I )Z 2 C τ (k) v=1 g(x(k -v)) + d 2 12 g(x(k)) T A T Z 2 Ag(x(k)) + 2d 2 12 g(x(k)) T A T Z 2 Bg(x(k -d(k))) + 2d 2 12 g(x(k)) T A T Z 2 C τ (k) v=1 g(x(k -v)) + d 2 12 g(x(k -d(k))) T B T Z 2 Bg(x(k -d(k))) + 2d 2 12 g(x(k -d(k))) T B T Z 2 C τ (k) v=1 g(x(k -v)) + d 2 12 τ (k) v=1 g(x(k -v)) T C T Z 2 C τ (k) v=1 g(x(k -v)) + d 2 12 σ (k) T Z 2 σ (k) -x(k -d 1 ) T Z 2 x(k -d 1 ) + 2x(k -d 1 ) T (Z 2 -S)x(k -d(k)) + 2x(k -d 1 ) T Sx(k -d 2 ) + x(k -d(k)) T (-2Z 2 + S + S T )x(k -d(k)) + 2x(k -d(k)) T (-S + Z 2 )x(k -d 2 ) -x(k -d 2 ) T Z 2 x(k -d 2 ) . (<label>24</label></formula><formula xml:id="formula_40">)</formula><p>Using Lemma 1 again, we have that</p><formula xml:id="formula_41">-τ 2 τ (k) v=1 g(x(k -v)) T Rg(x(k -v)) ≤ - τ (k) v=1 g(x(k -v)) T R τ (k) v=1 g(x(k -v)). (<label>25</label></formula><formula xml:id="formula_42">)</formula><p>Thus</p><formula xml:id="formula_43">E[ΔV 7 (k)] ≤ E ϑg(x(k)) T Rg(x(k)) -E ⎡ ⎣ τ (k) v=1 g(x(k -v)) T R τ (k) v=1 g(x(k -v)) ⎤ ⎦ . (<label>26</label></formula><formula xml:id="formula_44">)</formula><p>On the other hand, from Assumption 2 and (12a), we have</p><formula xml:id="formula_45">σ (k) T P + d 2 1 Z 1 + d 2 12 Z 2 σ (k) ≤ ρx(k) T G 1 x(k) + 2ρx(k) T G 2 x(k -d(k)) +ρx(k -d(k)) T G 3 x(k -d(k)) (<label>27</label></formula><formula xml:id="formula_46">)</formula><p>From Assumption 1 and <ref type="bibr" target="#b19">[20]</ref>, we have that for any i = 1, 2, . . ., n</p><formula xml:id="formula_47">(g i (x i (k)) -ρ i x i (k))(g i (x i (k)) -δ i x i (k)) ≤ 0 (<label>28</label></formula><formula xml:id="formula_48">)</formula><p>which is equivalent to</p><formula xml:id="formula_49">x(k) g(x(k)) T δ i ρ i e i e T i -δ i +ρ i 2 e i e T i -δ i +ρ i 2 e i e T i e i e T i x(k)) g(x(k)) ≤ 0 (29)</formula><p>where e i denotes the unit column vector having 1 element on its i -th row and zeros elsewhere. Thus, for any appropriately dimensioned diagonal matrix Y &gt; 0, the following inequality holds:</p><formula xml:id="formula_50">x(k) g(x(k)) T F 1 Y -F 2 Y * Y x(k)) g(x(k)) ≤ 0 (30) that is -x(k) T F 1 Y x(k) + 2x(k) T F 2 Y g(x(k)) -g(x(k)) T Y g(x(k)) ≥ 0.<label>(31)</label></formula><p>Similarly, for any appropriately dimensioned diagonal matrix H &gt; 0, the following inequality also holds:</p><formula xml:id="formula_51">-x(k -d(k)) T F 1 H x(k -d(k)) + 2x(k -d(k)) T F 2 H g(x(k -d(k))) -g(x(k -d(k))) T H g(x(k -d(k))) ≥ 0. (<label>32</label></formula><formula xml:id="formula_52">)</formula><p>Hence, adding the left-hand sides of ( <ref type="formula" target="#formula_50">31</ref>) and ( <ref type="formula" target="#formula_51">32</ref>), we can obtain from ( <ref type="formula">14</ref>)-( <ref type="formula">17</ref>), and ( <ref type="formula" target="#formula_37">23</ref>)-( <ref type="formula" target="#formula_45">27</ref>) that</p><formula xml:id="formula_53">E[ΔV (k)] = E 7 s=1 ΔV s (k, x(k)) ≤ E θ(k) T Ξ θ(k) (<label>33</label></formula><formula xml:id="formula_54">)</formula><p>where</p><formula xml:id="formula_55">θ 1 (k) = x(k) T x(k -d 1 ) T x(k -d(k)) T x(k -d 2 ) T T θ 2 (k) = ⎡ ⎣ g(x(k)) T g(x(k -d(k))) T τ (k) v=1 g(x(k -v)) T ⎤ ⎦ T θ(k) = θ 1 (k) T θ 2 (k) T T Ξ = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Ξ 11 Z 1 ρG 2 0 Ξ 15 Ξ 16 Ξ 17 * Ξ 22 Ξ 23 S 0 0 0 * * Ξ 33 Ξ 34 0 F 2 H 0 * * * Ξ 44 Ξ 45 0 0 * * * * Ξ 55 Ξ 56 Ξ 57 * * * * * Ξ 66 Ξ 67 * * * * * * Ξ 77 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ .</formula><p>It is clear that we can conclude from (12b) that there exists a scalar ρ &gt; 0 such that</p><formula xml:id="formula_56">E[ΔV (k)] &lt; -ρE[||x(k)|| 2 ]<label>(34)</label></formula><p>which implies that for any k ≥ 0</p><formula xml:id="formula_57">-E[V (0, x(0))] ≤ E[V (k + 1, x(k + 1))] -E[V (0, x(0))] = k α=0 E[ΔV (α)] ≤ - ρ k α=0 E[||x(α)|| 2 ].<label>(35)</label></formula><p>So the following inequality holds:</p><formula xml:id="formula_58">E k α=0 ||x(α)|| 2 ≤ 1 ρ E[V (0, x(0))] &lt; ∞<label>(36)</label></formula><p>which in turn implies lim k→+∞ E[||x(k)|| 2 ] = 0. According to Definition 1, neural network (1) is globally asymptotically stable in the mean square.</p><p>Next, we study the dissipativity of neural network (1). To this end, we consider Lyapunov functional <ref type="bibr" target="#b12">(13)</ref> and the following index for neural network (1):</p><formula xml:id="formula_59">J τ,u = τ k=0 E[y(k) T Qy(k) + 2y(k) T Su(k) + u(k) T (R -γ I )u(k)].</formula><p>(37) Then, we can find that</p><formula xml:id="formula_60">τ k=0 E[ΔV (k)] -J τ,u ≤ τ k=0 E θ(k) T Ξ θ(k)<label>(38)</label></formula><p>where</p><formula xml:id="formula_61">θ(k) = θ(k) u(k) .</formula><p>We can get from (12b) and (38) that</p><formula xml:id="formula_62">τ k=0 E[ΔV (k)] ≤ J τ,u<label>(39)</label></formula><p>which implies</p><formula xml:id="formula_63">E[V (x(τ + 1))] -E[V (x(0))] ≤ J τ,u .<label>(40)</label></formula><p>Thus, (9) holds under zero initial condition. Therefore, according to Definition 2, neural network (1) is strictly (Q, S, R)γ -dissipative. This completes the proof. Remark 1: It is noted a sufficient condition is established in Theorem 1 to ensure the considered neural networks to be globally asymptotically stable in the mean square and strictly (Q, S, R)-γ -dissipative. It is clear that the given condition is delay-dependent in the sense that it depends on not only the discrete delay but also the finite-distributed delay. Thanks to the usage of inequalities <ref type="bibr" target="#b9">(10)</ref> and <ref type="bibr" target="#b10">(11)</ref>, only one matrix, including four decision variables are introduced in the theoretical derivation. Thus, the expression of the LMIs in Theorem 1 is much simpler than the corresponding results based on the free-weighting matrix method <ref type="bibr" target="#b9">[10]</ref>.</p><p>Remark 2: It is worth pointing out that the LMIs in ( <ref type="formula">12</ref>) are not only over the matrix variables, but also over the scalar γ . This implies that by setting δ = -γ and minimizing δ subject to <ref type="bibr" target="#b11">(12)</ref>, we can obtain the optimal dissipativity performance γ (by γ = -δ). Also, it is worth mentioning that given different d 1 , d 2 , τ 1 , and τ 2 , the optimal dissipativity performance γ achieved should be different, which will be illustrated via a numerical example in the next section.</p><p>We now give the characterization of dissipativity, namely, passivity of neural network (1) by choosing Q = 0, S = I , and R = 2γ I as follows.</p><p>Corollary 1: Under Assumptions 1 and 2, neural network (1) is passive, if there exist</p><formula xml:id="formula_64">P &gt; 0, Q 1 &gt; 0, Q 2 &gt; 0, Q 3 &gt; 0, Z 1 &gt; 0, Z 2 &gt; 0, R &gt; 0, S, diagonal matrices Y &gt; 0, H &gt; 0,</formula><p>and scalars ρ &gt; 0 and γ &gt; 0 such that (12a), (12c), and (41) hold </p><formula xml:id="formula_65">⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Ξ 11 Z 1 ρG 2 0 Ξ 15 Ξ</formula><formula xml:id="formula_66">Ξ55 = A T P A + d 2 1 A T Z 1 A + d 2 12 A T Z 2 A + ϑ R -Y Ξ58 = -I + A T P + d 2 1 A T Z 1 + d 2 12 A T Z 2 Ξ88 = P + d 2 1 Z 1 + d 2 12</formula><p>Z 2 -γ I. Remark 3: A delay-dependent sufficient condition is given in Corollary 1 to guarantee the passivity of neural network <ref type="bibr" target="#b0">(1)</ref>. It is clear that by minimizing γ subject to (12a), (12c), and (41), we can obtain the optimal passivity performance γ .</p><p>Remark 4: It is noted that although only finite-distributed delay is investigated in Theorem 1 and Corollary 1, the obtained results can be easily extended to the case of infinitedistributed delay by using similar method. Moreover, we can also get an H ∞ performance condition of neural network (1) from Theorem 1 by choosing Q = -I , S = 0, and R = (γ 2 + γ )I .</p><p>In the following, we consider the case of finite-distributed delay free. In this case, neural network (1) reduces to:</p><formula xml:id="formula_67">⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ x(k + 1) = Dx(k) + Ag(x(k)) + Bg(x(k -d(k))) + u(k) + σ (k, x(k), x(k -d(k)))ω(k) y(k) = g(x(k)) x(k) = φ(k), k ∈ N[-d 2 , 0]<label>(42)</label></formula><p>and the corresponding Lyapunov functional is given as follows:</p><formula xml:id="formula_68">V (k, x(k)) = 6 s=1 V s (k, x(k)) (<label>43</label></formula><formula xml:id="formula_69">)</formula><p>where V s (k, x(k)) follows the same definitions as those in <ref type="bibr" target="#b12">(13)</ref>. Then, by employing similar method used in Theorem 1, we can get the following result on dissipativity analysis for neural network (42). Theorem 2: Under Assumptions 1 and 2, neural network (42) is globally asymptotically stable in the mean square and strictly (Q, S, R)-γ -dissipative, if there exist</p><formula xml:id="formula_70">P &gt; 0, Q 1 &gt; 0, Q 2 &gt; 0, Q 3 &gt; 0, Z 1 &gt; 0, Z 2 &gt; 0, S, diagonal matrices Y &gt; 0, H &gt; 0,</formula><p>and scalars ρ &gt; 0 and γ &gt; 0 such that (12a), (12c), and (44) hold</p><formula xml:id="formula_71">⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Ξ 11 Z 1 ρG 2 0 Ξ 15 Ξ 16 Ξ 18 * Ξ 22 Z 2 -S S 0 0 0 * * Ξ 33 -S + Z 2 0 F 2 H 0 * * * -Q 3 -Z 2 0 0 0 * * * * Ξ55 Ξ 56 Ξ 58 * * * * * Ξ 66 Ξ 68 * * * * * * Ξ 88 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ &lt; 0 (44)</formula><p>where Ξ 11 , Ξ 15 , Ξ 16 , Ξ 18 , Ξ 22 , Ξ 33 , Ξ 56 , Ξ 58 , Ξ 66 , Ξ 68 , and Ξ 88 follow the same definitions as those in Theorem 1, and</p><formula xml:id="formula_72">Ξ55 = A T P A + d 2 1 A T Z 1 A + d 2 12 A T Z 2 A -Y -Q.</formula><p>Similarly, we can also specialize Theorem 2 to the passivity analysis of neural network (42) by choosing Q = 0, S = I , and R = 2γ I .</p><p>Corollary 2: Under Assumptions 1 and 2, neural network (42) is passive, if there exist</p><formula xml:id="formula_73">P &gt; 0, Q 1 &gt; 0, Q 2 &gt; 0, Q 3 &gt; 0, Z 1 &gt; 0, Z 2 &gt; 0, S, diagonal matrices Y &gt; 0, H &gt; 0,</formula><p>and scalars ρ &gt; 0 and γ &gt; 0 such that (12a), (12c), and (45) hold</p><formula xml:id="formula_74">⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Ξ 11 Z 1 ρG 2 0 Ξ 15 Ξ 16 Ξ 18 * Ξ 22 Z 2 -S S 0 0 0 * * Ξ 33 -S + Z 2 0 F 2 H 0 * * * -Q 3 -Z 2 0 0 0 * * * * Ξ55 Ξ 56 Ξ58 * * * * * Ξ 66 Ξ 68 * * * * * * Ξ88 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ &lt; 0 (45)</formula><p>where Ξ 11 , Ξ 15 , Ξ 16 , Ξ 18 , Ξ 22 , Ξ 33 , Ξ 56 , Ξ 66 , and Ξ 68 follow the same definitions as those in Theorem 1, Ξ58 and Ξ88 follow the same definitions as those in Corollary 1, and</p><formula xml:id="formula_75">Ξ55 = A T P A + d 2 1 A T Z 1 A + d 2 12 A T Z 2 A -Y. Remark 5:</formula><p>In <ref type="bibr" target="#b15">[16]</ref>, a delay-dependent criterion has also been established for the passivity analysis of neural network (42). It is noted that when</p><formula xml:id="formula_76">Z 1 = d -1 1 Z and Z 2 = d -1</formula><p>12 Z , the Lyapunov functional (43) reduces to the Lyapunov functional applied in <ref type="bibr" target="#b15">[16]</ref>. Thus, our Lyapunov functional is much more general than that of <ref type="bibr" target="#b15">[16]</ref>. On the other hand, the delay term</p><formula xml:id="formula_77">d 2 -d(k) is enlarged as d 2 -d 1 and another term d(k) -d 1 is also enlarged as d 2 -d 1 in [16], that is, d 2 -d 1 = d 2 -d(k) + d(k) -d 1 is enlarged as 2(d 2 -d 1 )</formula><p>. It is clear that the aforementioned treatment may neglect some information of the time-varying delay d(k) and lead to conservatism to some extent. Owing to the inequality <ref type="bibr" target="#b10">(11)</ref>, the condition in Corollary 2 takes full advantage of the information of the time-varying delay d(k). Considering the above, our result derived here has less conservativeness than that of <ref type="bibr" target="#b15">[16]</ref>.</p><p>Remark 6: It should be pointed out that, in order to get a less conservative condition, the free-weighting matrix method is used in <ref type="bibr" target="#b15">[16]</ref>, and fifteen additional matrices, including sixty decision variables are introduced. However, only one additional matrix, including four decision variables are introduced in Corollary 2 obtained in this paper. Therefore, the criterion derived here has much less decision variables than that of <ref type="bibr" target="#b15">[16]</ref>. More specifically, the number of decision variables involved in <ref type="bibr" target="#b15">[16]</ref> is 17.5n 2 + 4.5n + 2, whereas the number of decision variables involved in (12a), (12c), and (45) is only 4n 2 +5n+2.</p><p>It is well known that more decision variables imply an increase in the computational burden, and thus our result has clear computational advantages over <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NUMERICAL EXAMPLES</head><p>In this section, three numerical examples will be adopted to demonstrate the effectiveness and advantage of the results proposed in the above section. The first example is utilized to demonstrate the validity of the given dissipativity condition. The second example shows the reduced conservatism of the proposed passivity criterion. In the third example, the genetic regulatory network is applied to validate the proposed stability criterion.</p><p>Example </p><formula xml:id="formula_78">f 1 (α) = 1 20 (|α + 1| + |α -1|) f 2 (α) = 1 10 (|α + 1| + |α -1|).</formula><p>It can be verified that Assumption 1 is satisfied with In this example, we choose that</p><formula xml:id="formula_79">δ 1 = -0.1, ρ 1 = 0.1, δ 2 = -0.</formula><formula xml:id="formula_80">Q = -1 0 0 -1 , S = 1 0 1 1 , R = 3 0 0 3<label>.</label></formula><p>1) It is assumed that the distributed delay τ (k) satisfies 5 ≤ τ (k) ≤ 9, that is, τ 1 = 5 and τ 2 = 9. Our purpose here is to discuss the relationship between the optimal dissipativity performance γ and the discrete delay d(k).</p><p>We first assume that the lower bound of the discrete delay d(k) is fixed to be 2. When the upper bound of the discrete delay d(k) is fixed to be 8, by using Theorem 1, the optimal dissipativity performance obtained is γ = 1.7915. However, if we assume d 2 = 9, the optimal dissipativity performance obtained is γ =   When d 1 = 12, the optimal dissipativity performance obtained is γ = 2.0017. This shows that when the upper bound of the discrete delay d(k) is fixed, for a smaller d 1 , the obtained optimal dissipativity performance γ is usually smaller. A more detailed comparison for different values of d 1 is provided in Table <ref type="table" target="#tab_4">II</ref>.</p><p>2) The second task in this example is to show the relationship between the optimal dissipativity performance γ and the distributed delay τ (k). To this end, we assume d 1 = 3 and d 2 = 8, that is, the discrete delay d(k) satisfies 3 ≤ d(k) ≤ 8. Now, we choose τ 1 = 6. When τ 2 = 10 (corresponding to 6 ≤ τ (k) ≤ 10), by Theorem 1, the optimal dissipativity performance obtained is γ = 1.7337. When τ 2 = 12 (corresponding to 6 ≤ τ (k) ≤ 12), by Theorem 1, the optimal dissipativity performance obtained is γ = 0.6501. This shows that for the same τ 1 , a larger τ 2 corresponds to a smaller optimal dissipativity performance γ . A more detailed comparison for different values of τ 2 is provided in Table <ref type="table" target="#tab_4">III</ref>.</p><p>Next, we choose τ 2 = 8. According to Theorem 1, Table <ref type="table" target="#tab_6">IV</ref> gives the optimal dissipativity performance γ with different τ 1 . We can find from Table IV that for the same τ 2 , a larger τ 1 corresponds to a larger optimal dissipativity performance γ .  The noise diffusion coefficient vector satisfies Assumption 2 with</p><formula xml:id="formula_81">G 1 G 2 * G 3 = ⎡ ⎢ ⎢ ⎣ 0.003 0 0 0 0 0.003 0 0 0 0 0.002 0 0 0 0 0.002 ⎤ ⎥ ⎥ ⎦ .</formula><p>In this example, we are interested in comparing our result with the one of <ref type="bibr" target="#b15">[16]</ref>. To this end, we first assume that the time-varying delay d(k) has lower bound d 1 = 3. When the upper bound of time-varying delay d 2 = 8, the optimal passivity performance obtained is γ = 3.9708 by the method in <ref type="bibr" target="#b15">[16]</ref>. While by Corollary 2 proposed in this paper, the optimal passivity performance obtained is γ = 2.9660, which is 25.3% smaller than that in <ref type="bibr" target="#b15">[16]</ref>. Table V also lists the comparisons of γ , when the upper bound d 2 = 9, 10, 11, and 12. Next, we assume that the time-varying delay d(k) has upper bound d 2 = 13. The optimal passivity performance γ obtained by Corollary 2 and <ref type="bibr" target="#b15">[16]</ref> for different d 1 can be found in Table <ref type="table" target="#tab_6">VI</ref>. It can be seen from Remark 6 and Tables V and VI that our result uses fewer decision variables, but this is not at the expense of degrading the passivity performance, that is, compared with the condition in <ref type="bibr" target="#b15">[16]</ref>, Corollary 2 derived here not only has less decision variables, but also gives less conservative results.</p><p>It can also be found from Tables V and VI that for the same d 1 , a larger d 2 corresponds to a larger optimal passivity performance γ , and when the upper bound d 2 is fixed, a larger d 1 corresponds to a smaller optimal passivity performance γ .</p><p>Example 3: In this example, we will show the application of the proposed result to a biological network, which has been presented as the mathematical model of the repressilator and experimentally studied in Escherichia coli <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Here, we take the time-varying delay and noise perturbation into account and consider the following discrete-time genetic regulatory network:</p><formula xml:id="formula_82">⎧ ⎪ ⎨ ⎪ ⎩ m(k + 1) = Âm(k) + B f ( p(k -d(k))) + σ (k, p(k), p(k -d(k)))ω(k) p(k + 1) = Ĉ p(k) + Dm(k -d(k)) (<label>46</label></formula><formula xml:id="formula_83">)</formula><p>where m(k) = m 1 (k)m 2 (k) T , and f i (•) represents the feedback regulation of the protein on the transcription, which is generally a nonlinear function but has a form of monotonicity with each variable. B = ( bij ) ∈ R n×n is the coupling matrix of the genetic network, which is defined as follows: if there is no link from node j to node i , bij = 0; if transcription factor j is an activator of gene i , bij = α i j ; if transcription factor j is a repressor of gene i , bij = -α i j , α i j is the dimensionless transcriptional rate and is the bounded constant.</p><p>In this example, we consider a three-gene network, that is, n = 3. Choose f i (s) = s 2 /(1 + s 2 ) for any i <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, which implies δ i = 0 and ρ i = 0.65 for any i , and A straightforward calculation gives d 1 = 1 and d 2 = 5. Applying Theorem 2, it can be checked that neural network (42) is globally asymptotically stable, which implies the genetic regulatory network (46) is stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, the problem of dissipativity analysis for discrete-time stochastic neural networks with time-varying discrete and finite-distributed delays was studied. The discretized Jensen inequality and lower bounds lemma were used to deal with the involved finite sum quadratic terms, and a delay-dependent condition was given to ensure the considered neural network to be globally asymptotically stable in the mean square and strictly (Q, S, R)-γ -dissipative. The obtained condition depends on not only the discrete delay but also the finite-distributed delay. Some special cases were also investigated. Thanks to the usage of the discretized Jensen inequality and lower bounds lemma, the results derived here not only have less decision variables, but also have less conservatism compared with the existing ones, so can be easily used in practice. Numerical examples were given to show the effectiveness and advantage of the proposed methods.</p><p>It is worth mentioning that the results proposed in this paper can be extended to more complex neural networks, such as nonhomogeneous Markov jump neural networks, switched neural networks, and Markov jump neural networks with incomplete transition descriptions, which deserves further investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 , and ρ 2 =</head><label>22</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Example 2 :</head><label>2</label><figDesc>Consider neural network (42) with 2 -0.1 and suppose the activation functions satisfy Assumption 2 with δ 1 = δ 2 = 0 and ρ 1 = ρ 2 = 0.5. A straightforward calculation gives</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) T σ (k) ≤ p(k) p(kd(k)) T H 1 H 2 * H 3 p(k) p(kd(k))whereH 2 = 0 and , g(x(kd(k))) = m(kd(k)) f ( p(kd(k))) A = 0,we can find that genetic regulatory network (46) can be transformed in neural network (42). It is assumed that d(k) = 3 + 2 sin kπ 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>b -1, b}. The superscript "T" represents the transpose, and diag{. . .} stands for a blockdiagonal matrix. || •|| denotes the Euclidean norm of a vector and its induced norm of a matrix, and l 2 [0, +∞) is the space of square summable infinite sequence. For an arbitrary matrix B and two symmetric matrices A and C</figDesc><table><row><cell>A B</cell></row><row><cell>*  C</cell></row><row><cell>denotes a symmetric matrix, where " * " denotes the term that</cell></row><row><cell>is induced by symmetry. Matrices, if they are not explicitly</cell></row><row><cell>specified, are assumed to have compatible dimensions.</cell></row><row><cell>II. PRELIMINARIES</cell></row><row><cell>Consider the following discrete-time stochastic neural net-</cell></row><row><cell>work with mixed time-delays:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>16 Ξ 17 Ξ 18 * Ξ 22 Ξ Ξ 15 , Ξ 16 , Ξ 17 , Ξ 18 , Ξ 22 , Ξ 23 , Ξ 33 , Ξ 34 , Ξ 44 , Ξ 56 , Ξ 57 , Ξ 66 , Ξ 67 , Ξ 68 , Ξ 77 , and Ξ 78 follow the same definitions as those in Theorem 1, and</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>⎤</cell></row><row><cell cols="5">23 S 0  *   *  Ξ 33 Ξ 34 0 F 2 H 0 0 0 0 0  *   *   *  Ξ 44 0 0 0 0  *   *   *   *  Ξ55 Ξ 56 Ξ 57 Ξ58  *   *   *   *   *  Ξ 66 Ξ 67 Ξ 68  *   *   *   *   *   *  Ξ 77 Ξ 78</cell><cell>⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥</cell><cell>&lt; 0</cell><cell>(41)</cell></row><row><cell>*   *</cell><cell>*</cell><cell>*   *</cell><cell>*</cell><cell>*  Ξ88</cell></row><row><cell>where Ξ 11 ,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1.7284. A more detailed comparison for different values of d 2 is provided in TableI, which shows that when d 1 is fixed, a larger d 2 corresponds to a smaller optimal dissipativity performance γ . Next, the upper bound of the discrete delay d(k) is assumed to be 14. By Theorem 1, the optimal dissipativity performance obtained is γ = 2.0478, when the lower bound of the discrete delay d(k) is 13.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I OPTIMAL</head><label>I</label><figDesc>DISSIPATIVITY PERFORMANCE γ FOR DIFFERENT d 2</figDesc><table><row><cell>d 2</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell></row><row><cell>Theorem 1</cell><cell>1.7915</cell><cell>1.7284</cell><cell>1.6591</cell><cell>1.5819</cell><cell>1.4951</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V OPTIMAL</head><label>V</label><figDesc>PASSIVITY PERFORMANCE γ FOR DIFFERENT d 2</figDesc><table><row><cell>d 2</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell></row><row><cell>[16]</cell><cell>3.9708</cell><cell>4.8961</cell><cell>6.2501</cell><cell>8.3991</cell><cell>12.2439</cell></row><row><cell>Corollary 2</cell><cell>2.9660</cell><cell>3.3483</cell><cell>3.8428</cell><cell>4.4968</cell><cell>5.4007</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="6">OPTIMAL PASSIVITY PERFORMANCE γ FOR DIFFERENT d 1</cell></row><row><cell>d 1</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>[16]</cell><cell>16.1853</cell><cell>13.2125</cell><cell>11.0130</cell><cell>9.2978</cell><cell>7.9134</cell></row><row><cell>Corollary 2</cell><cell>6.1649</cell><cell>5.6743</cell><cell>5.2451</cell><cell>4.8664</cell><cell>4.5300</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>with | ĉi | &lt; 1, and âi and ĉi are the decay rates of mRNA and protein.D = diag{d 1 , d 2 , . . . , d n }. f ( p(t)) = f 1 ( p 1 (t)) f 2 ( p 2 (t)) • • • f n ( p n (t))</figDesc><table /><note><p><p>• • • m n (k) T , p(k) = p 1 (k), p 2 (k), • • • , p n (k)</p>T , and m i (k) and p i (k) can be viewed as the concentrations of mRNA and protein of the i -th node. Â = diag{ â1 , â2 , . . . , ân } with | âi | &lt; 1, Ĉ = diag{ ĉ1 , ĉ2 , . . . , ĉn }</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China, under Grant 61174029 and Grant 61174058, the National Key Basic Research Program, China, under Grant 2012CB215202, and the 111 Project, under Grant B12018. Z.-G. Wu, H. Su, and J. Chu are with the</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global asymptotic stability of a class of dynamical neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Fundam. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="568" to="571" />
			<date type="published" when="2000-04">Apr. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nonlinear adaptive decoupling control based on neural networks and multiple models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Innov. Comp. Inf. Control</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1867" to="1878" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural network based stochastic optimal control for nonlinear markov jump systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Innov. Comp. Inf. Control</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3715" to="3723" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust stability criterion for discrete-time uncertain markovian jumping neural networks with defective statistics of modes transitions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="170" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stability analysis for systems with large delay period: A switching method</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Innov. Comp. Inf. Control</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4235" to="4247" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global stabilization for a class of highorder time-delay nonlinear systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Innov. Comp. Inf. Control</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7119" to="7130" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust sliding mode control for uncertain linear discrete systems independent of time-delay</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Innov. Comp. Inf. Control</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="869" to="880" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On global asymptotic for a class of delayed neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Circuit Theory Appl</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1165" to="1174" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Novel delay-dependent robust stability analysis for switched neutral-type neural networks with time-varying delays via SC technique</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Part B, Cybern</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1480" to="1491" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">New delay-dependent stability criteria for neural networks with time-varying delay</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="314" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delay-dependent stability analysis for switched neural networks with time-varying delay</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Part B, Cybern</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1522" to="1530" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stability analysis of discrete-time recurrent neural networks with stochastic delay</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1330" to="1339" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">State estimation for delayed neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="279" to="284" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">State estimation for discrete-time neural networks with time-varying delay</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="647" to="655" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Passivity analysis of neural networks with time delay</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. II, Exp. Briefs</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="471" to="475" />
			<date type="published" when="2005-08">Aug. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Passivity analysis of discrete-time stochastic neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1782" to="1788" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Passivity analysis for discretetime stochastic Markovian jump neural networks with mixed time delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1566" to="1575" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A note on control of a class of discrete-time stochastic systems with distributed delays and nonlinear disturbances</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="543" to="548" />
			<date type="published" when="2010-03">Mar. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Global asymptotic stability of reactiondiffusion Cohen-Grossberg neural networks with continuously distributed delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="49" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Global exponential stability of generalized recurrent neural networks with discrete and distributed delays</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="667" to="675" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stability analysis for stochastic Cohen-Grossberg neural networks with mixed time delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="814" to="820" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asymptotic stability for neural networks with mixed time-delays: The discrete-time case</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="74" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">New passivity results for uncertain discrete-time stochastic neural networks with mixed time delays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="3291" to="3299" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">State estimation for discrete-time Markovian jumping neural networks with mixed mode-dependent delays</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Lett. A</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="7147" to="7155" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Nonlinear Dynamical Systems and Control: A Lyapunov-Based Approach</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chellaboina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Princeton Univ. Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The stability of nonlinear dissipative systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="708" to="711" />
			<date type="published" when="1976-10">Oct. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust reliable dissipative filtering for discrete delay singular systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Process</title>
		<imprint>
			<date type="published" when="2012-12">Dec. 2012</date>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="3010" to="3025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dissipative control for linear systems by static output feedback</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Syst. Sci</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stability and dissipativity analysis of distributed delay cellular neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="981" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stability and dissipativity analysis of static neural networks with time delay</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reciprocally convex approach to stability of systems with time-varying delays</title>
		<author>
			<persName><forename type="first">P</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="238" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dissipative dynamical systems, part I: General theory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Willems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Ration. Mech. Anal</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="321" to="351" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">New delay-dependent stability results for discrete-time recurrent neural networks with time-varying delay</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="3376" to="3383" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Admissibility and dissipativity analysis for discrete-time singular systems with mixed time-varying delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ju</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the transient and steady-state estimates of interval genetic regulatory networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Part B: Cybern</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="336" to="349" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On delayed genetic regulatory networks with polytopic uncertainties: Robust stability analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Nanobiosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="163" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
