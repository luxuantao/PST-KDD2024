<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intrusion Detection using Sequences of System Calls</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Hofmeyr</surname></persName>
							<email>steveah@cs.unm.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of New Mexico Albuquerque</orgName>
								<address>
									<postCode>87131-1386</postCode>
									<region>NM</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephanie</forename><surname>Forrest</surname></persName>
							<email>forrest@cs.unm.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of New Mexico Albuquerque</orgName>
								<address>
									<postCode>87131-1386</postCode>
									<region>NM</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anil</forename><surname>Somayaji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of New Mexico Albuquerque</orgName>
								<address>
									<postCode>87131-1386</postCode>
									<region>NM</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intrusion Detection using Sequences of System Calls</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A42E1271448AA3EB3F6B3A50EBB7C29E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Revision: 12/17/97</head><p>A method is introduced for detecting intrusions at the level of privileged processes. Evidence is given that short sequences of system calls executed by running programs are a good discriminator between normal and abnormal operating characteristics of several common UNIX programs. Normal behavior is collected in two ways: Synthetically, by exercising as many normal modes of usage of a program as possible, and in a live user environment by tracing the actual execution of the program. In the former case several types of intrusive behavior were studied; in the latter case, we analyze results were analyzed for false positives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern computer systems are plagued by security vulnerabilities. Whether it is the latest UNIX buffer overflow or bug in Microsoft Internet Explorer, our applications and operating systems are full of security flaws on many levels. From the viewpoint of the traditional security paradigm, it should be possible to eliminate such problems through more extensive use of formal methods and better software engineering. This view rests on several assumptions: That security policy can be explicitly and correctly specified, that programs can be correctly implemented, and that systems can be correctly configured. Although these assumptions may be theoretically reasonable, in practice none of them holds. Computers systems are not static: They are continually changed by vendors, system administrators, and users. Programs are added and removed, and configurations are changed. Formal verification of a statically defined system is time-consuming and hard to do correctly; formal verification of a dynamic system is impractical. Without formal verifications, tools such as encryption, access controls, firewalls, and audit trails all become fallible, making perfect implementation of a security policy impossible, even if a correct policy could be devised in the first place. If we accept that our security policies, our implementations, and our configurations are flawed in practice, then we must also accept that we will have imperfect security. We can incrementally improve security through the use of tools such as Intrusion Detection Systems (IDS). The IDS approach to security is based on the assumption that a system will not be secure, but that violations of security policy (intrusions) can be detected by monitoring and analyzing system behavior.</p><p>There are many different levels on which an IDS can monitor system behavior. It is critical to profile normal behavior at a level that is both robust to variations in normal and perturbed by intrusions. In the work reported here, we chose to monitor behavior at the level of privileged processes. Privileged processes are running programs that perform services (such as sending or receiving mail), which require access to system resources that are inaccessible to the ordinary user. To enable these processes to perform their jobs, they are given privileges over and above those of an ordinary user (even though they can be invoked by ordinary users). In UNIX, processes usually run with the privileges of the user that invoked them. However, privileged processes can change their privileges to that of the superuser by means of the setuid mechanism. One of the security problems with privileged processes in UNIX is that the granularity of permissions is too coarse: Privileged processes need superuser status to access system resources, but granting them such status gives them more permission than necessary to perform their specific tasks <ref type="bibr" target="#b27">[30]</ref>. Consequently, they have permission to access all system resources, not just those that are relevant to their operation. Privileged processes are trusted to access only relevant system resources, but in cases where there is some programming error in the code that the privileged process is running, or if the privileged process is incorrectly configured, an ordinary user may be able to gain superuser privileges by exploiting the problem in the program. For the sake of brevity, we usually refer to privileged processes (or programs) simply as "processes" (or "programs"), and use the qualifier only to resolve ambiguities.</p><p>It is clear that privileged processes are a good level to focus on because exploitation of vulnerabilities in privileged processes can give an intruder super-user status. Furthermore, privileged processes constitute a natural boundary for a computer, especially processes that listen to a particular port. In UNIX, privileged programs, such as telnetd and logind, function as servers that control access into the system. Corruption of these servers can allow an intruder to access the system remotely. Monitoring privileged processes also offers some advantages over monitoring user behavior, which has been the most common method to date (for example, see <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr" target="#b9">[12]</ref>, <ref type="bibr" target="#b23">[26]</ref>, <ref type="bibr">[35]</ref>, <ref type="bibr">[39]</ref>). The range of behaviors of privileged processes is limited compared to the range of behaviors of users; privileged processes usually perform a specific, limited function, whereas users can carry out a wide variety of actions. Finally, the behavior of privileged processes is relatively stable over time, especially compared to user behavior. Not only do users perform a wider variety of actions, but the actions performed may change considerably over time, whereas the actions (or at least the functions) of privileged processes usually do not vary much with time.</p><p>Our approach to detecting irregularities in the behavior of privileged programs is to regard the program as a black box, which, when run, emits some observable. We believe that this observable should be a dynamic characteristic of that program; although code stored on disk may have the potential to do harm, it has to be actually running to realize that potential. If we regard the program as a black-box, we do not need specialized knowledge of the internal functioning or the intended role of the program; we can infer these indirectly by observing its normal behavior 1 . A natural observable for processes in UNIX would be based on system calls, because UNIX processes accesses system resources through the use of system calls. We have chosen short sequences of system calls as our observable.</p><p>In an earlier study we reported preliminary evidence that short sequences of system calls are a good simple discriminator for several types of intrusions <ref type="bibr" target="#b15">[18]</ref>. The results reported here extend the earlier study, with several important differences. First, we have slightly changed how we record sequences of system calls: Previously, we used look-ahead pairs, with a look-ahead value of 6; here we use exact sequences of length 10. Consequently, the database sizes reported here are smaller than in the earlier study. Next, we have used a measure of anomalous behavior that is independent of trace length (based on Hamming matches between sequences). Finally, we have collected normal behavior in a real, live 2  environment, and analyzed it for false positives.</p><p>We want an IDS that is stable and lightweight (efficient), all of which depends on the discriminator (observable) that we use to distinguish between acceptable and unacceptable behavior. By stable we mean that the discriminator reliably distinguishes between acceptable and unacceptable behavior. Our approach is experimental because we believe that current theories do not adequately describe how implemented systems really 1 There are other approaches that require knowledge of the internals and intended role of a program, most notably the program specification method <ref type="bibr" target="#b27">[30]</ref>, which attempts to constrain the process in such a way that it can perform only those operations that the program is designed to do, and no more, i.e. the method refines the permissions structure to accommodate specific privileged programs. The differences between our method and this are discussed more fully in section 6. 2 We use the words "real" and "live" to refer to a production environment, i.e. an environment which is currently in normal, everyday use. We contrast this to our "synthetic" environment, which is an isolated test environment.</p><p>run. In this paper we are primarily concerned with determining empirically if the discriminator is stable. Efficiency is a secondary consideration, and is addressed in this paper to the extent that we analyze the complexity of our algorithm; however, we do not report actual running times for the method on a production system.</p><p>Our work is inspired by the defenses of natural immune systems. There are compelling similarities between the problems faced by immune systems and by computer security <ref type="bibr" target="#b14">[17]</ref>. Both systems must protect a highly complex system from penetration by inimical agents; to do this, they must be able to discriminate between broad ranges of normal and abnormal behavior. In the immune system, this discrimination task is known as the problem of distinguishing "self" (the harmless molecules normally within the body) from "nonself" (dangerous pathogens and other foreign materials). Discrimination in the immune system is based on a characteristic structure called a peptide (a short protein fragment) that is both compact and universal in the body. This limits the effectiveness of the immune system; for example, the immune system cannot protect the body against radiation. However, proteins are a component of all living matter, and generally differ between self and nonself, so they provide a good distinguishing characteristic. We view our chosen discriminator (short sequences of system calls) as analogous to a peptide. The structure of this paper is as follows. In section 2 we review related work in intrusion detection. Section 3 describes our method of anomaly intrusion detection: First we describe how to build up profiles of normal program behavior, and then we define three ways of detecting anomalies. We then use the method to build a synthetic normal profile in section 4, demonstrating its effectiveness at detecting intrusions and other anomalies. In section 5 we consider the consequences of collecting our normal data in online, functioning environments, discuss false positives, and present experimental results on false positive rates. The limitations and implications of our approach are discussed in section 6. A brief appendix is included which details the various intrusions that we used in our experiments, the methods we used to generate synthetic normal, and a brief overview of UNIX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>An Intrusion Detection System (IDS) continuously monitors some dynamic behavioral characteristics of a computer system to determine if an intrusion has occurred. This definition excludes many useful computer security methods. Security analysis tools, such as SATAN <ref type="bibr" target="#b13">[16]</ref> and COPS <ref type="bibr" target="#b12">[15]</ref> are used to scan a system for weaknesses and possible security holes. They are not IDS because they do not monitor some dynamic characteristic of the system for intrusions or evidence of intrusions, rather they scan the system for weaknesses such as configuration errors or poor password choices that could lead to intrusions. Other important non-IDS solutions to computer security problems are provided by cryptography <ref type="bibr" target="#b10">[13]</ref>, which is especially useful for authentication and secure communications <ref type="bibr">[36]</ref>. Virus protection schemes such as that described in <ref type="bibr" target="#b25">[28]</ref> are also not IDS under our definition, because they scan static code, not dynamic behavioral characteristics. Some approaches are not easily classified, for example, integrity checking systems such as TRIPWIRE <ref type="bibr" target="#b26">[29]</ref> monitor important files for changes that could indicate intrusions. Although such files are static code, they become a dynamic characteristic indicative of intrusions when modified by intrusive activities, and so TRIPWIRE could be classified as an IDS.</p><p>There are many different architectures for IDS. IDS can be centralized (i.e. processing is performed on a single machine) or distributed across many machines. Almost all IDS are centralized; the autonomous agents approach <ref type="bibr" target="#b7">[10]</ref> is one of the few proposed IDS that is truly distributed. Furthermore, an IDS can be host-based or network-based; the former type monitors activity on a single computer, whereas the latter type monitors activity over a network. Network-based IDS can monitor information collated from audit trails from many different hosts (multi-host monitoring) or they can monitor network traffic. NADIR <ref type="bibr" target="#b23">[26]</ref> and DIDs <ref type="bibr" target="#b22">[25]</ref> are examples of IDS that do both multi-host and network traffic monitoring; NSM <ref type="bibr" target="#b21">[24]</ref> is an IDS that monitors only network traffic. Regardless of other architectural considerations, any IDS must have three components: Data collection (and reduction), data classification and data reporting. Data reporting is usually very simple, with system administrators being informed of anomalous or intrusive behavior; few IDS take it upon themselves to act rapidly to deal with irregularities. Various methods for data collection and classification are discussed below.</p><p>An IDS that monitors for intrusive behavior needs to collect data on the dynamic state of the system. Selecting a set of dynamic behavioral characteristics to monitor is a key design decision for an IDS, one which will influence the types of analyses that can be performed and the amount of data that will be collected. Most systems (for example, IDES/NIDES [34], [35], <ref type="bibr" target="#b3">[5]</ref>, Wisdom&amp;Sense <ref type="bibr">[33]</ref> and TIM [39]) collect profiles of user behavior, generated by audit logs. Other systems look at network traffic, for example, NSM and the system presented in <ref type="bibr" target="#b20">[23]</ref>. Other approaches attempt to characterize the behavior of privileged processes, as in the program specification method <ref type="bibr" target="#b27">[30]</ref>. Different behavioral characteristics will generate different amounts of data; as an extreme example, systems monitoring user profiles process large volumes of raw data (an average user will generate from 3 to 35MB of audit data per day <ref type="bibr" target="#b19">[22]</ref>). In the latter case the data may need to be reduced to a manageable size.</p><p>Once a behavioral characteristic is selected, it is used to classify data. In the simplest case, this is a binary decision problem: The data is classified as either normal (acceptable) or anomalous (and possibly intrusive). Data classification can be more complex, for instance, trying to identify the particular type of intrusion associated with anomalous behavior. A plethora of methods have been used for data classification, the majority of them using artificial intelligence techniques (see <ref type="bibr" target="#b19">[22]</ref> for a detailed overview). Classification techniques can be divided into two categories, depending on whether they look for known intrusion signatures (misuse intrusion detection), or for anomalous behavior (anomaly intrusion detection). Misuse-IDS encode intrusion signatures or scenarios and scan for occurrences of these, which requires prior knowledge of the nature of the intrusion. By contrast, in anomaly-IDS, it is assumed that the nature of the intrusion is unknown, but that the intrusion will result in behavior different from that normally seen in the system. Anomaly IDS use models of normal or expected behavior to monitor systems; deviations from the normal model indicate possible intrusions. Some systems incorporate both categories, a good example being NIDES, or Denning's generic model of an IDS <ref type="bibr" target="#b9">[12]</ref>.</p><p>Relatively few IDS deal with misuse intrusion detection. One type of implementation uses an expert system to fit data to known intrusion signatures, for example, in IDES/NIDES, or Stalker [37], knowledge of past intrusions is encoded by human experts in expert system rules. Other approaches attempt to generate intrusion signatures automatically, for example, one approach uses a pattern matching model based on colored Petri nets [31] and [32], while USTAT <ref type="bibr" target="#b24">[27]</ref> represents potential intrusions as sequences of system states in the form of state transition diagrams.</p><p>Because of the difficulty of encoding known intrusions, and the continual occurrence of new intrusions, many systems focus on anomaly intrusion detection. A wide variety of methods have been used. TRIPWIRE monitors the state of special files (such as the /etc./hosts.equiv file on a UNIX system, or UNIX daemon binaries) for change; normal is simply the static MD5 checksum of a file. A program specification language is used in <ref type="bibr" target="#b27">[30]</ref> to define normal for privileged programs in terms of the allowed operations for that program. Rule-based induction systems such as TIM have been used to generate temporal models of normal user behavior. Wisdom&amp;Sense incorporates an unsupervised tree-learning algorithm to build models of patterns in user transactions. Other systems, such as NIDES, have employed statistical methods to generate models of normal user behavior in terms of frequency distributions. NSM uses a hierarchical model in combination with a statistical approach to determine network traffic usage profiles. On the biologically inspired side, connectionist or neural nets have been used to classify data <ref type="bibr" target="#b18">[21]</ref>, and genetic programming has been proposed as a means of developing classifications <ref type="bibr" target="#b7">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Anomaly Intrusion Detection</head><p>The method we present here performs anomaly intrusion detection (although it could also be used for misuse detection---see section 6). We build up a profile of normal behavior for a program of interest, treating deviations from this profile as anomalies. There are two stages to the anomaly detection: In the first stage we build up profiles or databases of normal behavior (this is analogous to the training phase for a learning system); in the second stage we use these databases to monitor process behavior for significant deviations from normal (analogous to the test phase).</p><p>Recall that we have chosen to define normal in terms of short sequences of system calls. In the interests of simplicity, we ignore the parameters passed to the system calls, and look only at their temporal orderings. This definition of normal behavior ignores many other important aspects of process behavior, such as timing information, instruction sequences between system calls, and interactions with other processes. Certain intrusions may only be detectable by examining these other aspects of process behavior, and so we may need to consider them later. Our philosophy is to see how far we can go with the simplest possible assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Profiling Normal Behavior</head><p>The algorithm used to build the normal databases is extremely simple. We scan traces of system calls generated by a particular program, and build up a database of all unique sequences of a given length, k, that occurred during the trace. Each program of interest has a different database, which is specific to a particular architecture, software version and configuration, local administrative policies, and usage patterns. Once a stable database is constructed for a given program, the database can be used to monitor the ongoing behavior of the processes invoked by that program. This method is complicated by the fact that in UNIX a program can invoke more than one process. Processes are created via the fork system call or its virtual variant vfork. The essential difference between the two is that a fork creates a new process which is an instance of the same program (i.e. a copy), whereas a vfork replaces the existing process with a new one, without changing the process ID. We trace forks individually and include their traces as part of normal, but we do not yet trace virtual forks because a virtual fork executes a new program. In the future, we will switch databases dynamically to follow the virtual fork.</p><p>Given the large variability in how individual systems are currently configured, patched, and used, we conjecture that individual databases will provide a unique definition of normal for most systems. We believe that such uniqueness, and the resulting diversity of systems, is an important feature of the immune system, increasing the robustness of populations to infectious diseases <ref type="bibr" target="#b17">[20]</ref>. The immune system of each individual is vulnerable to different pathogens, greatly limiting the spread of a disease across a population. Traditionally, computer systems have been biased towards increased uniformity because of the advantages offered, such as portability and maintainability. However, all the advantages of uniformity become potential weaknesses when errors can be exploited by an attacker. Once a method is discovered for penetrating the security of one computer, all computers with the same configuration become similarly vulnerable.</p><p>The construction of the normal database is best illustrated with an example. Suppose we observe the following trace of system calls <ref type="bibr">(</ref> For efficiency, these sequences are currently stored as trees, with each tree rooted at a particular system call. The set of trees corresponding to our example is given in Figure <ref type="figure" target="#fig_0">1</ref>. We record the size of the database in terms of the number of unique sequences N, (in the example just given, N = = 4 ) so an upper bound on the storage requirements for the normal database is ( ( ) )</p><p>O Nk . In practice, the storage requirements are much lower because the sequences are stored as trees. For example, the sendmail database, which contains 1318 unique sequences of length 10, has 7578 nodes in the forest, where each node corresponds to a system call. If we had a node for every single system call in all 1318 sequences, we would have 13180 nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Measuring Anomalous Behavior</head><p>Once we have a database of normal behavior, we use the same method that we used to generate the database to check new traces of behavior. We look at all overlapping sequences of length k in the new trace and determine if they are represented in the normal database. Sequences that do not occur in the normal database are considered to be mismatches. By recording the number of mismatches, we can determine the strength of an anomalous signal. Thus the number of mismatches occurring in a new trace is the simplest determinant of anomalous behavior. We report these counts both as a raw number and as a percentage of the total number of matches performed in the trace, which reflects the length of the trace. Ideally, we would like these numbers to be zero for new examples of normal behavior, and for them to jump significantly when abnormalities occur.</p><p>We make a clear distinction here between normal and legal behavior. In the ideal case we want the normal database to contain all variations in normal behavior, but we do not want it to contain every single possible path of legal behavior, because our approach is based upon the assumption that normal behavior forms only a subset of the possible legal execution paths through a program, and unusual behavior that deviates from those normal paths signifies an intrusion or some other undesirable condition. We want to be able to detect not only intrusions, but also unusual conditions that are indicative of system problems. For example, when a process runs out of disk space, it may execute some error code that results in an unusual execution sequence (path through the program). Clearly such a path is legal, but certainly it should not be regarded as normal.</p><p>If the normal database does contain all variations in normal behavior, then when we encounter a sequence that is not present in the normal database, we can regard it as anomalous, i.e. we can consider a single mismatch to be significant. In reality, it is likely to be impossible to collect all normal variations in behavior (these issues are discussed more fully in sections 4 and 5), so we must face the possibility that a normal database provides incomplete coverage of normal behavior. One solution is to count the number of mismatches occurring in a trace, and only regard as anomalous those traces that produce more than a certain number of mismatches. This is problematic however, because the count is dependent on trace length, which might be indefinite for continuously running processes.</p><p>An alternative is to constrain the measure locally. The anomalies we have studied are temporally clumped: Anomalous sequences due to intrusions seem to occur in local bursts. However, defining a local measure is difficult because we have an unordered state space, i.e. we have no true notion of locality---how "close" one system call is to another, or how "close" one system call sequence is to another. We have chosen "Hamming distance"<ref type="foot" target="#foot_0">3</ref> between sequences as the measure. Although this choice is somewhat arbitrary, it is related to how closely anomalies are clumped. We cannot theoretically justify this measure, so we determine its worth empirically.</p><p>We use the "Hamming distance" between two sequences to compute how much a new sequence actually differs from existing normal sequences. The similarity between two sequences can be computed using a matching rule that determines how the two sequences are compared. The matching rule used here is based on Hamming distance, i.e. the difference between two sequences i and j is indicated by the Hamming distance ( ( ) )</p><formula xml:id="formula_0">d i j</formula><p>, between them. For each new sequence i, we determine the minimal Hamming distance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( ( ) )</head><p>d i min between it and the set of normal sequences:</p><formula xml:id="formula_1">( ( ) ) ( ( ) ) { { } } d i d i j j min min , = =</formula><p>for all normal sequences .</p><p>The d min value represents the strength of the anomalous signal, i.e. how much it deviates from a known pattern. Note that this measure is not dependent on trace length and is still amenable to the use of thresholds for binary decision making.</p><p>The various measures can be illustrated with a small example. These three different measures have different time-complexities. To determine that a new sequence is a mismatch requires at most k --1 comparisons, because the normal sequences are stored in a forest of trees, where the root of each tree corresponds to a different system call. Similarly, it will take k --1 comparisons to confirm that a sequence is actually in the normal database. If the sequence is not in the normal database, then computing d min for that sequence is much more expensive. Because ( ( ) )</p><formula xml:id="formula_2">d i min</formula><p>is the smallest Hamming distance between i and all normal sequences, we have to check every single sequence in normal before we can determine ( ( ) ) d i min , which will require a total of ( (</p><p>) )</p><p>N k --1 comparisons (recall that N is the number of sequences in the database). However, we expect anomalies to be rare, so most of the time, the algorithm will be confirming normal sequences, which is much cheaper to do. If our rate of anomalous to normal sequences is R A , then the average complexity of computing</p><formula xml:id="formula_3">( ( ) ) d i min per sequence is ( ) ( ) ( ) N k R k R A A - + - - 1<label>1 1 , which is ( ) ( )</label></formula><formula xml:id="formula_4">O k R N A + 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification Errors</head><p>An IDS using these measures will be making decisions based on the observed values of the measures. In the simplest case, these are binary decisions: Either a sequence is anomalous, or it is normal. With binary decision making, there are two types of classification errors: False positives and false negatives. We define these errors asymmetrically: A false positive occurs when a single sequence generated by legitimate behavior is classified as anomalous; and a false negative occurs when none of the sequences generated by an intrusion are classified as anomalous, i.e. when all of the sequences generated by an intrusion appear in the normal database. In statistical decision theory, false negatives and false positives are called type I and type II errors, respectively.</p><p>To detect an intrusion, at least one of the sequences generated by the intrusion must be classified as anomalous. In terms of our measures, what we require is that at least one of the sequences generated by the intrusion has d min &gt; 0 . We measure the strength of the anomaly by d min , and because we want intrusions to generate strong anomalies, we assume that the higher the d min the more likely it is that the sequence was actually generated by an intrusion. In practice, we report the maximum d min value that was encountered during a trace, because that represents the strongest anomalous signal found in the trace, i.e. we compute the signal of the anomaly, S A , as:</p><formula xml:id="formula_5">( ( ) ) { { } } S d i i A = = max min</formula><p>for all new sequences . In our example above, S A = = 1. Generally, we do not report the actual S A value, but rather the S A value normalized over the sequence length k, to enable us to compare S A values for different values of k, i.e.:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S S k</head><p>A A = = .</p><p>Although we would like to minimize both kinds of errors, we are more willing to tolerate false negatives than false positives. False negatives can be reduced by adding layers of defense, whereas layering will not reduce overall false positive rates. A simple example illustrates this. Consider a system with L layers of defense that an intruder must penetrate, where at each layer there is a probability p n that the intruder will escape detection (i.e. p n is the false negative rate). If the probability of detection is independent for each layer, then the probability that the intruder will penetrate all layers undetected is p n L . So, in this example, the overall false negative rate is exponentially reduced by adding layers of protection (provided we have independence). By contrast, if we assume that at each layer we have an (independent) probability p f of generating a false positive, then the expected number of false positives is p L f . In this case layering compounds false positives.</p><p>False positives can be measured when we collect normal behavior in live environments (see section 5). If we are collecting normal empirically, the occurrence of rare but acceptable events could result in an incomplete normal database. If the normal were incomplete, false positives could be the result, as we encounter acceptable sequences that are not yet included in our normal database. To limit false positives, we set thresholds on the ( ) d i min values, i.e. we regard as anomalous any sequence i for which ( )</p><formula xml:id="formula_6">d i C min ≥ .</formula><p>where 1 ≤ ≤ C k is the threshold value. To summarize, if a sequence i of length k is sufficiently different from all normal sequences it is flagged as anomalous. The validity of the assumption that intrusive behavior is characterized by increased Hamming distance from normal sequences is tested empirically in the sections that follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Behavior in a Synthetic Environment</head><p>There are two methods for choosing the normal behavior that is used to define the normal database: <ref type="bibr" target="#b0">(1)</ref> We can generate a "synthetic" normal by exercising a program in as many normal modes as possible and tracing its behavior; (2) we can generate a "real" normal by tracing the normal behavior of a program in a live user environment. A synthetic normal is useful for replicating results, comparing performance in different settings, and other kinds of controlled experiments. Real normal is more problematic to collect and evaluate (these issues are discussed in section 5); however, we need real normal to determine how our system is likely to perform in realistic settings. For example, if we generate normal synthetically we have no idea what false positive rates we will get in realistic settings because our synthetic, by definition, includes all variations on normal behavior (although not all variations on legal behavior). We could exclude some synthetically generated traces from normal and see what false positives resulted, but it is not clear which traces to exclude---the choice is arbitrary and the resulting false positives would be equally arbitrary. In this section we present results using a synthetic normal; in section 5 we present results using a real normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Building a Synthetic Normal Database</head><p>We studied normal behavior for three different programs in UNIX: sendmail, lpr and wu.ftpd (the first two were running under SunOS 4.1.x, and the last one was running under Linux). Sendmail is a program that sends and receives mail, lpr is a program that enables users to print documents on a printer, and ftpd is a program for the transfer of files between local and remote hosts. Because sendmail is the most complex of these programs, we will briefly describe how we exercised sendmail to produce a profile of normal behavior (the methods for constructing synthetic normal for the other two programs are described in Appendix 2). We considered variations in message length, number of messages, message content (text, binary, encoded, encrypted), message subject line, who sent the mail, who received the mail, and mailers. In addition, we looked at the effects of forwarding, bounced mail and queuing. Lastly, we considered the effects of the origin of all these variations in the cases of remote and local delivery.</p><p>A suite of 112 artificially constructed messages was used to exercise sendmail (version 5), producing a trace of a combined length of over 1.5 million system calls. Table <ref type="table" target="#tab_2">1</ref> shows how many messages of each type were used to generate the normal databases. We began with message length, testing 12 different message lengths, ranging from 1 line to 300,000 bytes. From this, we selected the shortest length that produced the most varied pattern of system calls (50,000 bytes), and then used that as the standard message length for the remaining test messages. Similarly, with the number of messages in a sendmail run, we first sent 1 message and traced sendmail, then we sent 5 messages, tracing sendmail, and so forth, up to 20 messages. This was intended to test the response of sendmail to bursts of messages. We tested message content by sending messages containing ASCII text, uuencoded data, gzipped data, and a pgp encrypted file. In each case, a number of variations was tested and the one that generated the most variations in system call patterns was selected as a single default before moving on to the next stage.</p><p>These messages constituted our corpus of normal behavior. We reran this set of standard messages on each different operating system and sendmail.cf (the sendmail configuration file) variant that we tried, thus generating a normal database that was tailored to the exact operating conditions under which sendmail was running.</p><p>Of the features considered, the following seemed to have little or no effect: Number of messages, message content, subject line, who sent the mail, who received the mail, mail programs and queuing. Message length has a considerably different effect on the sequence of system calls, depending on the message origin: Remote mail produces traces of system calls that are proportional to the length of the message, with little sequence variation in these traces; local mail produces traces that are roughly the same length, regardless of the size of message, but the sequence of system calls used changes considerably as the message size increases. In both cases, once a large enough message size (50K) is used to generate normal, message size makes no difference. The effect of forwarding mail on remote traces is negligible, whereas it has a small but noticeable affect on local traces. Bounced mail had more of an effect remotely, but the effects are still evident in the local case. For each test, we generated databases for different values of k for each of the three programs tested, i.e. sendmail, lpr and ftpd. The results for k = = 10 are shown in Table <ref type="table" target="#tab_3">2</ref>. Our choice of sequence length was determined by two conflicting criteria. On the one hand we want a sequence length as short as possible to minimize the size of the database and the computation involved in detection (recall that the time complexity of detection is proportional to k). On the other hand, if the sequence length is too small we will not be able to discriminate between normal and anomalous behavior. These databases are remarkably compact, for example, the sendmail database contains only 1318 unique sequences of length 10, which requires 9085 bytes to store in our current implementation. sendmail is one of the most complex of the privileged programs currently used in UNIX systems, and if its behavior can be described so compactly, then we can expect that other privileged programs will have normals at least as compact. The data are encouraging because they indicate that the range of normal behavior of these programs is limited. Too much variability in normal would preclude detecting anomalies; in the worst case, if all possible sequences of length k show up as legal normal behavior, then no anomalies could ever be detected.</p><p>How many possible sequences of length k are there? If we have an alphabet ∑ ∑ of system calls, with size ∑ ∑ , then there are ∑ ∑ k possible sequences of length k. Choosing the alphabet size can be problematic without knowing exactly which system calls are used by sendmail, considering that there are a total of 182 system calls in the SunOS 4.1.x operating system. As a conservative estimate, we assume that sendmail uses no more than 53 calls (the number in the synthetic normal database), so, for k = = 10 there are 53 10 , or approximately 10 17 possible sequences. Thus our sendmail normal database only contains about 10 13 -percent of the total possible number of patterns. Of course, this is not completely accurate, because the number of possible sequences that sendmail can actually use is limited by the structure of the code. To determine this would require a detailed analysis of the source code, which is precisely what we wish to avoid because one of the strengths of our approach is that it does not require specialized knowledge of any particular program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detecting Anomalous Behavior</head><p>Is intrusive behavior anomalous under this definition of normal? Ideally, we want most, if not all, intrusive behavior to be anomalous. To test this, we compared the normal databases against a range of known abnormal behavior.</p><p>In these experiments, we report the number of mismatches, the percentage of mismatches, and the normalized anomaly signal S A . Because S A is not dependent on the length of the trace, it is our preferred measure. However, S A values are meaningful only in the context of detection thresholds, and thresholds are dependent on the acceptable level of false positives. Because of the way we constructed normal, we have zero false positives for synthetic data; thus, in principle, any S A &gt; &gt; 0 indicates an anomaly (although our goal is clear separation between the anomaly and normal, i.e. we want the S A values to be large). The issue of false positives in a real environment is explored in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Distinguishing Between Programs</head><p>The first experiments we performed compared sendmail with other UNIX programs. If we could not distinguish between sendmail and other programs, then we would be unlikely to detect small deviations in the behavior of a single program. We have done this comparison for varying sequence lengths. When the sequence length is very low, ( k = 1), there are very few mismatches, in the range of 0 to 7%. When the sequence length reaches k = 30 there are 100% mismatches against all programs. Results of comparisons for k = = 10 are presented in Table <ref type="table" target="#tab_4">3</ref>.</p><p>Each program showed a significant number of anomalous sequences (at least 57%), and at least one anomalous sequence is quite different from the normal sendmail sequences, as evinced by S A , which is at least 0.6, indicating that the most anomalous sequence differs from the normal sequences in over half of its positions. The programs shown are distinct from sendmail because the actions they perform are considerably different from those of sendmail. We also tested the normal database for lpr and achieved similar results (data not shown). lpr exhibits even more separation than that shown in Table <ref type="table" target="#tab_4">3</ref>, presumably because it is a smaller program with more limited behavior. These results suggest that the behavior of different programs is easily distinguishable using sequence information alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program</head><p>Number </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Detecting Intrusions</head><p>The second set of experiments was to detect intrusions that exploit flaws in three programs: sendmail, lpr and wu.ftpd. Some of the intrusions were successful, and others unsuccessful because of updates and patches in software. We report results for both. We would like to be able to detect most (if not all) of these attempted intrusions, even if they fail. Detection of failed intrusions would be a useful warning sign that an attacker is attempting to break into a system. A third behavioral category that we would like to be able to detect is the occurrence of error states, such as sendmail forwarding loops. Although these error states are technically legal behavior, they are properly regarded as abnormal because they indicate the existence of problems.</p><p>We compared system call traces for each of the three categories (successful exploits, unsuccessful exploits and error conditions) with the normal database for the relevant program and recorded the number of mismatches, percentage of mismatches over the trace, and S A values. Table <ref type="table" target="#tab_5">4</ref> shows results for successful intrusions. Each row in the table reports data for one typical trace. In most cases, we have conducted multiple runs of the intrusion with identical or nearly identical results; where runs differed significantly, we report a range of values. To date, we have collected data on five successful intrusions, three of them for sendmail, one for lpr <ref type="bibr" target="#b2">[4]</ref> and one for ftpd <ref type="bibr" target="#b6">[8]</ref> CERT. swinstall vulnerability. Ftp://info.cert.org/pub/ cert_advisories/ CA-96:27, December 1996.</p><p>[9]. The three sendmail intrusions were: sunsendmailcp <ref type="bibr" target="#b1">[2]</ref>, syslogd [3], <ref type="bibr" target="#b5">[7]</ref>, and a decode alias intrusion. These intrusions are described in the appendix. Most of the successful intrusions are clearly detected, with S A values of 0.5 to 0.7. The exception to this is the decode intrusion, which, on the low end of the range, generates only 7 mismatches and a S A value of 0.2. These results suggest approximate detection thresholds that we would need in an online system to detect intrusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anomaly</head><p>Number In summary, we are able to detect all the abnormal behaviors we tested against, including successful intrusions, failed intrusion attempts, and unusual error conditions.</p><p>We have only reported results for k = 10 because experiments show that varying sequence length has little effect on detection, in terms of the S A measure. We analyzed sequences of length k = 2 to k = = 30 . The minimum sequence length used was 2, because k = 1 will just give S A = 0 or S A = 1, which is not sufficiently informative. The maximum sequence length used was 30 because the cost of computation scales with sequence length. The results are reported in Figure <ref type="figure" target="#fig_2">2</ref>. The decode intrusion is not detectable for k &lt; &lt; 6 , but beyond this value of k, sequence length seems to make little difference for S A . Sometimes an increased sequence length results in a decreased anomaly signal. This could happen if the anomalies consisted of short clumps of system calls separated by large gaps: As sequence length increases, longer sequences would be more similar to normal sequences. For example, say we had a normal sequence open, read, mmap, mmap, open, read, which an intrusion disrupted in the first three positions to give close, close, close, mmap, open, read. Then k = 3 would give . S A = = 3 3 10 (from the first three system calls), and k = 6 would give . S A = = 3 6 0 5. Figure <ref type="figure" target="#fig_2">2</ref> implies that the best sequence length to use would be 6 or slightly larger than 6, because that will allow detection of anomalies while minimizing computation, which is directly proportional to k. We have chosen k = 10 because that gives a margin for error.  S A plotted against sequence length k. From this plot we infer that sequence length makes little difference once we have a length of at least 6.</p><p>Considering only the three anomaly measures gives a limited picture of the sorts of perturbations caused by intrusions and other unacceptable behaviors. For example, the S A values indicate only the most anomalous sequence without giving any clear idea of how anomalous sequences are temporally distributed. The anomaly profile in Figure <ref type="figure" target="#fig_3">3</ref> shows the temporal distribution of anomalous sequences for a successful sendmail intrusion, one of the syslogd intrusion runs. From this figure we can see how noticeable intrusions are, and how anomalies are clumped. It also indicates that if we were doing real-time monitoring, we might be able to detect some intrusions before an intruder gains access, right at the start of the intrusive behavior. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Behavior in a Real Environment</head><p>The results reported in section 4 were based on normal databases generated synthetically, i.e. we attempted to exercise all normal modes of behavior of a given program and used the resulting traces to build our normal databases. For an IDS that is deployed to protect a functioning system, this may not be the best way to generate normal. The real normal behavior of a given program on a particular machine could be quite different from the synthetic normal. Some synthetic normal behaviors may be absent in an actual system; on the other hand, the real normal might include behavior that we had not thought of, or were unable to incorporate into the synthetic. In this section we attempt to build normal in a real environment.</p><p>Several questions arise when we consider collecting real normal on a running system:</p><p>1. How do we ensure that we have not included abnormal sequences? That is, how do we ensure that the system is not being exploited as we collect the normal traces? Including abnormal sequences could result in false negatives. 2. How do we ensure that our normal is sufficiently comprehensive? How long do we collect normal for? How much normal is enough? An incomplete normal could result in false positives. 3. Are intrusions still detectable as we increase the size of the normal? As the size of normal increases, we include rare normal sequences that could overlap more with abnormal sequences, thus reducing detection rates, i.e. increasing false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Collecting Real Normal</head><p>We have collected normal for lpr in two different real environments, at the Massachusetts Institute of Technology's Artificial Intelligence laboratory (MIT), and at the University of New Mexico's Computer Science Department (UNM). In both cases, we used a very simple solution to question 1 posed above: How do we ensure that intrusive behavior is not included in these normals? For the lpr we have studied, we are aware of only one intrusion (reported in section 4.2.2 above) which requires that lpr generate a 1000 print jobs in close succession, which is something we as observers could easily detect on a system that never generates more than 200 jobs in a day. This does not guarantee that our normal is free of intrusion traces, but at least we have excluded the intrusion against which we perform our analysis. In general, however, the problem will not be so trivial, particularly if we do not know the nature of the intrusion beforehand, i.e. if we are concerned with true anomaly detection. Possible ways of excluding intrusive behavior from the normal trace include:</p><p>• Collect normal in the real, open environment, whilst monitoring the environment very carefully to ensure that no intrusions have happened during our collection of normal.</p><p>This is what we did for lpr. • Collect normal in an isolated environment where we are sure no intrusions can happen. The disadvantage of this solution is that the normal will possibly be incomplete, because the environment is of necessity limited, particularly in the case of programs, such as sendmail, that communicate with the outside world.</p><p>In the MIT environment, we traced lpr running on 77 different hosts, each running SunOS, for two weeks, to obtain traces of a total of 2766 print jobs. The growth of the size N of the normal database is shown in Figure <ref type="figure" target="#fig_5">4</ref>. As more print jobs are traced and the traces added into normal, so the number of unique sequences N in the normal database grows. Initially, the growth is very rapid, but then tapers off, in particular, for k = = 6 and k = = 10 , there is minimal database growth past 1000 print jobs. This reinforces the idea of choosing as short a sequence length as possible, because we can accumulate the full range of normal sequences much more rapidly for short sequences. We regard Figure <ref type="figure" target="#fig_5">4</ref> as promising, because it indicates that normal behavior is limited and can be collected in a short period of time (depending on how much the system is used).  How much does normal vary between different environments? We have some answers in the case of lpr because we have two normals collected independently at MIT and UNM, for the identical program and operating system. These represent considerably different environments, as can be seen from the differences listed in Table <ref type="table" target="#tab_7">6</ref>, for example, we traced lpr on only one host at UNM, whereas we traced it on 77 hosts at MIT. Despite the differences in environment, the patterns of database growth in the UNM environment are similar to those at MIT (data not shown), although the resulting database sizes are different: 569 unique sequences for UNM and 876 for MIT. These databases not only differ in size, but also in content: For example, a comparison of the unique sequences in both databases for k = = 6 indicates that only 141 of the sequences are the same between the databases, which represents 40% of the UNM database and 29% of the MIT database. Although these databases are very different, they both detect the lprcp intrusion almost identically. When we analyze the anomalous sequences generated by the intrusion, we find that there are 16 unique anomalous sequences detected by the UNM database, which are identical to 16 of the 17 unique anomalous sequences detected by the MIT database, i.e. the anomaly is almost identical for both databases. This suggests that intrusion signatures could be encoded in sequences of system calls, i.e. the system call signature could be the basis of a misuse-IDS, or an IDS that does both anomaly and misuse detection (for a further exploration of these ideas see [38]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">How much Normal is enough?</head><p>This section addresses questions 2 and 3 posed above: How much normal is enough? And, are intrusions still detectable as the size of normal increases? In our experiments we used the lpr data we collected in the real environments at MIT and UNM. In both cases, we divided the set of data into two, the first set is used as the training set, and the second set as the test set. The training data are used to build up a normal database, and the test data are scanned using this normal database (we explain below how we choose the test and training sets). A false positive is then any sequence i in the test set for which ( )</p><formula xml:id="formula_7">d i C min ≥ .</formula><p>We determine the lowest false positive rate ε ε fp by setting the threshold C to be the maximum value needed for the normal database to detect the lprcp intrusion. Because we only have one intrusion to test against, and we set the threshold so that we always detect it, we have zero false negatives. The false positive rate is simply the number of false positives per print job.</p><p>The expected false positive rate was calculated using the bootstrap technique, which is a procedure for estimating (approximating) the distribution of a statistic from a random sample <ref type="bibr" target="#b11">[14]</ref>. We divided the jobs into test and training sets as follows: up to 700 jobs were chosen randomly with replacement for the training set, and the remaining jobs were used for the test set (thus we had a test set of 2066 jobs for MIT and one of 534 jobs for UNM). This process was repeated 100 times to get the bootstrap estimate. The bootstrap is applicable here because the data appear to be stationary. We checked for stationarity by sampling the jobs both randomly, and in small chronologically consecutive groups, and comparing the means produced by the two sampling methods. A two-tailed, two sample ttest between these two samples gives a P-value of 0.19. Thus the probability that these means are different is insignificant.</p><p>The expected false positive rates and standard deviations are shown in Figure <ref type="figure" target="#fig_6">5</ref>  . This is about 1 false positive in every 100 jobs, or, on the MIT system, an average of 2 false positives per day. This rate was computed for a normal database of 700 jobs, with 2066 jobs in the test set. From Figure <ref type="figure" target="#fig_6">5</ref> the false positive rate appears to be leveling off. However, when we increase the size of the normal database to 1400 jobs (not shown in the figure), with a test set of 1366 jobs, the rate drops to 0 005 0 002 . . ± , which is one false positive per day. We are hesitant to draw too many conclusions from these data because they are derived from a single program for which we have only one true positive (an intrusion), and so we cannot get an accurate measure of false negatives, or the false positive rate we could expect if we had to detect several different intrusions. Furthermore, although we have done tests to check for stationarity, we cannot be absolutely sure that there are no time-dependent effects in the data.</p><p>If we build the normal database chronologically from the first 700 jobs and compare that to the remaining 2066 jobs, we get a false positive rate of 0.004 for a sequence length of 10. Although this is within the bootstrap distribution, there is a probability of only 0.05 of getting a false positive rate that low when the jobs are randomly selected. So it may be that there are temporal dependencies not detected by our tests for stationarity. In an online system, normal would be constructed from the first jobs encountered, and so in this case we could expect lower false positive rates.</p><p>It is worth noting that these false positive rates are computed for a system in which we have only spent 3 or 4 days collecting normal behavior. Provided the size of the normal database does not grow indefinitely, we could expect our false positive rates to reduce as we spend more days on normal collection. This is illustrated by the fact that when we increase the size of the normal database to include 1400 jobs (7 days), our false positive rate halves. Furthermore, even if we use all of the normal behavior traced over two weeks to build the normal database, the threshold for detection of the lprcp intrusion does not drop (see Table <ref type="table" target="#tab_7">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of False Positives</head><p>We looked at the sequences which were responsible for the false positives to get an idea of what could be causing rare but acceptable behavior. We investigated several false positives and found unusual circumstances behind all of them, including:</p><p>1. Trying to print on a machine where the file /dev/printer did not exist. This file is a named local socket that connects to lpd running on the machine. Apparently lpr would place a job in the queue, but could not communicate with lpd. It is unclear whether lpd indicated an error. It is likely that the job did not print. 2. Printing from symbolic links. lpr was told to print a file in the current directory using the -s flag. It seems that the file to be printed was actually a symbolic link to another file, so lpr followed the symbolic link to the original file, and then placed a symbolic link to the real file in the spool directory. 3. Printing from a separately administered machine with a very different configuration. 4. Trying to print a job so large that lpr ran out of disk space for the log file.</p><p>When the normal database is built chronologically, there are only 6 false positives, 3 of which are caused by the first case (1) above, and 3 of which are caused by the second case <ref type="bibr" target="#b1">(2)</ref>. Are these really false positives? A false positive is some sort of acceptable behavior that is classified as anomalous. If the behavior is unacceptable, even if it is not caused by an intrusion, we would want to know about it, because it indicates that the system is not functioning properly or efficiently. Points 1 and 4 above are both instances of irregular behavior symptomatic of a problem with the system; both indicate conditions that need to be rectified. In this sense, neither 1 nor 4 are false positives. This kind of analysis indicates that our actual false positive rate is lower than the reported values, for example, in the case of a chronological normal, the number of false positives would be reduced from 6 to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>The previous two sections have presented evidence that short sequences of system calls are good discriminators between normal and abnormal operating characteristics of several common UNIX programs. In essence, we have found a regularity in executing programs that is highly likely to be perturbed by intrusive activities. These results are interesting for several reasons: They suggest a possible implementation path for a lightweight intrusiondetection system; the techniques might be applicable to security problems in other computational settings; they illustrate the value of studying the empirical behavior of actual systems; and they suggest a strategy for approaching other on-line problems in computing that are not well solved by conventional methods.</p><p>Although the results presented in Sections 4 and 5 are suggestive, much more testing needs to be completed to validate the approach. In particular, extensive testing on a wider variety of UNIX programs being subjected to large numbers of different kinds of intrusions is desirable. For each of these programs, we would ideally like to have results both in controlled environments (in which we could run large numbers of intrusions) and in live user environments. Overall, we expect that discrimination will be more difficult in highly stressed environments (high user loads, overloaded networks, etc.) in which many exceptional conditions are raised. Furthermore, we would like to test these ideas in different operating systems, such as Windows NT. Recently, we have successfully detected intrusions in two other programs: a buffer overflow in the xlock program running in Linux, and a symbolic link vulnerability in the swinstall program running under HP-UX <ref type="bibr" target="#b6">[8]</ref>  <ref type="foot" target="#foot_1">4</ref> .</p><p>However, there are some logistical problems associated with collecting data in live user environments. Most operating systems are not shipped with robust tracing facilities, and as much as possible, we would like to collect data in standardized environments. It is difficult to justify installing code with known vulnerabilities (needed to run large numbers of different intrusions) in a production environment, thus putting the user community at risk of real intrusions. Finally, there are no obvious stopping criteria. Every system is slightly different---when can we say that we have collected enough data on enough different programs in enough different environments?</p><p>Assuming that more detailed experiments confirm our results, there are a host of systemsengineering questions that need to be addressed before an IDS based on these principles could be implemented and deployed. First, what combination of synthetic and actual behavior should be collected to define a normal database? In many user environments, certain (legitimate) features of programs might be seldom used, and so a database generated from live user traces might generate false positives, whereas constructing a synthetic database appropriately could prevent these false positives. It would also be much easier to distribute an IDS that did not require a lot of customization at the time it is installed---an IDS should make systems administration easier not harder. Thus, the collection of real usage data at install-time would have to be highly automated. A related complication is how to guarantee that no intrusions take place during the collection of normal behavior. Second, which UNIX programs should be monitored, and how (and when) should databases be switched when different processes are started? We could use a completely different database for each program---earlier we emphasized that normal behavior for different programs is significantly different (ranging from 40% to 80%). However, these percentages also imply that there is much behavior in common between different programs, and so in a running implementation we might be able to reduce resource requirements by exploiting this commonality. Finally, we envision our IDS as a real-time, on-line system that could potentially discover and interrupt some intrusions before they were successful. The feasibility of this is highly dependent on efficient design and implementation of both the tracing facility and the algorithms that detect mismatches.</p><p>Our emphasis has been on determining if our approach can be successful at all. We were not too concerned with efficiency issues in this paper. However, for the system to be able to detect intrusions in real-time---as they are happening---will require careful attention to efficiency issues. As a first step towards this we have analyzed the complexity of our algorithm, although we have not been able to measure its efficiency in a production environment. Should the implementation prove too inefficient, there are numerous simplifications we could experiment with, such as looking only at specific kinds of calls, or only at every tenth call, etc.</p><p>An important question in the context of an IDS is what response is most appropriate once a possible intrusion has been detected. This is a deep topic and largely beyond the scope of our paper. Most IDS respond by sending an alarm to a human operator. In the long run, however, we believe that the response side will have to be largely automated if IDS technology is going to be widely deployed. We have some evidence that intrusions generate highly regular signatures, so it might be possible to store these signatures for known intrusions and respond more aggressively when those signatures are detected. Then for new anomalies more cautious actions could be taken. One advantage of monitoring at the process level is that a wide range of responses is possible, ranging from shutting down the computer completely (most radical) to simply running the process at lower priority.</p><p>The method we propose is not a panacea---it will certainly miss some forms of intrusions. One example is race condition attacks, which typically involve stealing a resource (such as a file) created by a process running as root, before the process has had a chance to restrict access to the resource. If the root process does not detect an unusual error state, a normal set of system calls will be made, defeating our method. Other examples of intrusions that would be missed are password hijacking (when one user masquerades as another), and cases in which a user violates policy without using privileged processes.</p><p>The idea of looking at short sequences of behavior is quite general and might be applicable to several other security problems. For example, people have suggested applying the technique to several other computational systems, including: The Java virtual machine, the CORBA distributed object system, security for ATM switches, and network security. For each of these potential applications, it would be necessary first to determine empirically whether simple definitions (analogous to sequences of system calls) give a clear and compact signature of normal behavior, and then to determine if the signature is perturbed by intrusive behavior.</p><p>Our approach is similar to several other approaches, although the differences are critical. Ko et al <ref type="bibr" target="#b27">[30]</ref> have also chosen the level of privileged processes, but they characterize the behavior of a privileged process by a program specification or policy, which is a description of what the program should be able to do. This policy is derived from the program code and so requires specialized knowledge of program function. Writing a policy can be prone to the same sorts of errors as writing the program, i.e. it is hard to guarantee correctness. Most importantly, from our perspective, such a policy could easily include behavior that is legal but not normal because it is hard to determine beforehand what behavior should be normal. We avoid these issues by treating the program as a black box, and relying purely on empirical observation to ascertain program behavior. Another key difference is that we rely exclusively on sequencing information, unlike the specification approach, which monitors individual operations. However, there are other approaches, such as TIM <ref type="bibr">[39]</ref>, that consider sequencing information. These differ from our approach in that they look at the domain of user behavior, and use a probabilitistic approach for detecting anomalies. Because our results are sufficiently promising the added complexity of using probabilities seems unnecessary. It is possible that our simple deterministic approach is successful because our data is well-structured. If this is the case, it may well be that probabilities are necessary in less structured domains, such as user behavior.</p><p>In earlier papers, we have advocated a comprehensive approach to computer security based on a collection of organizing principles derived from our study of the immune system <ref type="bibr">[38]</ref>. The immune-system perspective has certainly influenced many of our design decisions, but in this paper we are emphasizing concrete computational mechanisms and largely ignoring the immune system connection. Details of how our approach to IDS fits into the overall immune-system vision are given in <ref type="bibr" target="#b14">[17]</ref>. Extensions are suggested by analogy.</p><p>An important bias underlying our approach is that modern computers are "complex systems" in the sense that they are comprised of a large number of components, many of which interact nonlinearly. These components are continually evolving, as well as the environments in which they are embedded, their users, and the programmers who implement them. This complexity threatens to overwhelm design strategies based on functional decomposition. Furthermore, it implies that although we design and build computers, we do not necessarily understand how they behave. An example of this is the fact that the normal behavior of a highly complex program such as sendmail can be captured by such a small number of system call sequences---it would have been hard to predict this. Rather than making assumptions about how we believe that programs or users will behave, or trying to prespecify their behavior (and being surprised), this paper asks the question: What behavior do we observe? That is, we take existing artifacts and study their behavior rigorously. Although such an approach might be dismissed as "merely empirical" rather than theoretical, our point is that we need to spend more time asking to what extent our existing theories describe our existing artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We presented a method for anomaly intrusion detection at the process level. Normal was defined in terms of short sequences of system calls executed by running privileged processes. Our profiles of normal behavior, which consisted of unique sequences of length 10, were remarkably compact, for example, the sendmail database contained only 1318 such sequences. Three measures were used to detect abnormal behavior as deviations from profiles of normal. These measures allowed us to successfully detect several classes of abnormal behavior, including: Intrusions in the UNIX programs sendmail, lpr and ftpd; failed intrusion attempts against sendmail; and error conditions in sendmail. We studied two different methods of accumulating normal profiles: Generating normal synthetically by attempting to exercise the program in as many modes of normal operation as possible, and tracing a process in a live user environment. In the latter case we have analyzed the data for false positives. Our false positive rates for lpr were about 1 in every 100 print jobs (and explainable in terms of system problems), but these results are tentative because we did not have sufficient data for a comprehensive analysis. In future we intend to expand our base of intrusions and gather more data for more programs running in real environments, so we can get more realistic estimates of false positive and false negative rates. "decode," which resolves to uudecode, a UNIX program that converts a binary file encoded in plain text into its original form and name. uudecode respects absolute filenames, so if a file "bar.uu" says that the original file is "/home/foo/.rhosts" then when uudecode is given "bar.uu", it will attempt to create foo's .rhosts file. sendmail will generally run uudecode as the semi-privileged user daemon, so email sent to decode cannot overwrite any file on the system; however, if the target file happens to be world-writable, the decode alias entry allows these files to be modified by a remote user.</p><p>• lprcp: The lprcp attack script uses lpr to replace the contents of an arbitrary file with those of another. This attack exploits the fact that older versions of lpr use only 1000 different names for printer queue files, and they do not remove the old queue files before reusing them. The attack consists of getting lpr to place a symbolic link to the victim file in the queue, incrementing lpr's counter 1000 times, and then printing the new file, overwriting the victim file's contents.</p><p>• ftpd: This is a configuration problem. Wu.ftpd is misconfigured at compile time, allowing users SITE EXEC access to /bin. Users can then run executables such as bash with root privilege.</p><p>• unsuccessful intrusions: sm5x, sm565a.</p><p>• forwarding loops: A local forwarding loops occurs in sendmail when a set of $HOME/.forward files form a logical circle. We considered the simplest case, with the following setup: Email address .forward file foo@host1 bar@host2 bar@host2 foo@host1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Revisions made to the Paper</head><p>We list all of the major changes here, indexed by section and, where appropriate, the number of the paragraph within the section. We also indicate the page numbers of the changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Title.</head><p>Changed the title on page 1 to: "Intrusion Detection using Sequences of System Calls" Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modified paragraph 2 on page 2:</head><p>"There are many different levels on which an IDS can monitor system behavior. It is critical to profile normal behavior at a level that is both robust to variations in normal and perturbed by intrusions. In the work reported here, we chose to monitor behavior at the level of privileged processes. Privileged processes are running programs that perform services (such as sending or receiving mail), which require access to system resources that are inaccessible to the ordinary user. To enable these processes to perform their jobs, they are given privileges over and above those of an ordinary user (even though they can be invoked by ordinary users). In UNIX, processes usually run with the privileges of the user that invoked them. However, privileged processes can change their privileges to that of the superuser by means of the setuid mechanism. One of the security problems with privileged processes in UNIX is that the granularity of permissions is too coarse: Privileged processes need superuser status to access system resources, but granting them such status gives them more permission than necessary to perform their specific tasks <ref type="bibr" target="#b27">[30]</ref>. Consequently, they have permission to access all system resources, not just those that are relevant to their operation. Privileged processes are trusted to access only relevant system resources, but in cases where there is some programming error in the code that the privileged process is running, or if the privileged process is incorrectly configured, an ordinary user may be able to gain superuser privileges by exploiting the problem in the program. For the sake of brevity, we usually refer to privileged processes (or programs) simply as "processes" (or "programs"), and use the qualifier only to resolve ambiguities."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modified paragraph 3 on pages 2-3:</head><p>"It is clear that privileged processes are a good level to focus on because exploitation of vulnerabilities in privileged processes can give an intruder super-user status. Furthermore, privileged processes constitute a natural boundary for a computer, especially processes that listen to a particular port. In UNIX, privileged programs, such as telnetd and logind, function as servers that control access into the system. Corruption of these servers can allow an intruder to access the system remotely. Monitoring privileged processes also offers some advantages over monitoring user behavior, which has been the most common method to date (for example, see <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr" target="#b9">[12]</ref>, <ref type="bibr" target="#b23">[26]</ref>,[35],[39]). The range of behaviors of privileged processes is limited compared to the range of behaviors of users; privileged processes usually perform a specific, limited function, whereas users can carry out a wide variety of actions. Finally, the behavior of privileged processes is relatively stable over time, especially compared to user behavior. Not only do users perform a wider variety of actions, but the actions performed may change considerably over time, whereas the actions (or at least the functions) of privileged processes usually do not vary much with time."</p><p>Modified paragraph 4 on page 3: "Our approach to detecting irregularities in the behavior of privileged programs is to regard the program as a black box, which, when run, emits some observable. We believe that this observable should be a dynamic characteristic of that program; although code stored on disk may have the potential to do harm, it has to be actually running to realize that potential. If we regard the program as a black-box, we do not need specialized knowledge of the internal functioning or the intended role of the program; we can infer these indirectly by observing its normal behavior. A natural observable for processes in UNIX would be based on system calls, because UNIX processes accesses system resources through the use of system calls. We have chosen short sequences of system calls as our observable."</p><p>Added paragraph 6 on pages 3-4: "We want an IDS that is stable and lightweight (efficient), all of which depends on the discriminator (observable) that we use to distinguish between acceptable and unacceptable behavior. By stable we mean that the discriminator reliably distinguishes between acceptable and unacceptable behavior. Our approach is experimental because we believe that current theories do not adequately describe how implemented systems really run. In this paper we are primarily concerned with determining empirically if the discriminator is stable. Efficiency is a secondary consideration, and is addressed in this paper to the extent that we analyze the complexity of our algorithm; however, we do not report actual running times for the method on a production system. "</p><p>Removed the paragraph: "One of our goals is lightweight intrusion detection. By lightweight we mean methods that could be implemented in an IDS to run on-line, and that would use minimal computing resources. Because an IDS is a preventative measure that does not enhance system performance (quite the reverse), it is hard to justify the use of a computationally expensive IDS. In many cases, if an IDS is not sufficiently lightweight, it simply will not be used. Another advantage of having an IDS be sufficiently lightweight to run on-line is the potential to detect intrusions in progress, perhaps to prevent the intrusion attempt from succeeding."</p><p>Section 3.1.</p><p>Added paragraph 2 on page 7: "This method is complicated by the fact that in UNIX a program can invoke more than one process. Processes are created via the fork system call or its virtual variant vfork.</p><p>The essential difference between the two is that a fork creates a new process which is an instance of the same program (i.e. a copy), whereas a vfork replaces the existing process with a new one, without changing the process ID. We trace forks individually and include their traces as part of normal, but we do not yet trace virtual forks because a virtual fork executes a new program. In the future, we will switch databases dynamically to follow the virtual fork."</p><p>Section 3.2. Added paragraph 2 on page 9: "We make a clear distinction here between normal and legal behavior. In the ideal case we want the normal database to contain all variations in normal behavior, but we do not want it to contain every single possible path of legal behavior, because our approach is based upon the assumption that normal behavior forms only a subset of the possible legal execution paths through a program, and unusual behavior that deviates from those normal paths signifies an intrusion or some other undesirable condition. We want to be able to detect not only intrusions, but also unusual conditions that are indicative of system problems. For example, when a process runs out of disk space, it may execute some error code that results in an unusual execution sequence (path through the program). Clearly such a path is legal, but certainly it should not be regarded as normal."</p><p>Added paragraph 4 on pages 9-10: "An alternative is to constrain the measure locally. The anomalies we have studied are temporally clumped: Anomalous sequences due to intrusions seem to occur in local bursts. However, defining a local measure is difficult because we have an unordered state space, i.e. we have no true notion of locality---how "close" one system call is to another, or how "close" one system call sequence is to another. We have chosen "Hamming distance" between sequences as the measure. Although this choice is somewhat arbitrary, it is related to how closely anomalies are clumped. We cannot theoretically justify this measure, so we determine its worth empirically."</p><p>Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modifed paragraph 2 on page 11:</head><p>"To detect an intrusion, at least one of the sequences generated by the intrusion must be classified as anomalous. In terms of our measures, what we require is that at least one of the sequences generated by the intrusion has d min &gt; 0 . We measure the strength of the anomaly by d min , and because we want intrusions to generate strong anomalies, we assume that the higher the d min the more likely it is that the sequence was actually generated by an intrusion. In practice, we report the maximum d min value that was encountered during a trace, because that represents the strongest anomalous signal found in the trace, i.e. we compute the signal of the anomaly, S A , as:"</p><p>Added the last sentence to paragraph 5 on page 12: "The validity of the assumption that intrusive behavior is characterized by increased Hamming distance from normal sequences is tested empirically in the sections that follow."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Section 4.</head><p>Modified the fourth sentence of paragraph 1 on page 12-13: "For example, if we generate normal synthetically we have no idea what false positive rates we will get in realistic settings because our synthetic, by definition, includes all variations on normal behavior (although not all variations on legal behavior)."</p><p>Section 4.1.</p><p>Modified paragraph 1 on page 13: "We studied normal behavior for three different programs in UNIX: sendmail, lpr and wu.ftpd (the first two were running under SunOS 4.1.x, and the last one was running under Linux). Sendmail is a program that sends and receives mail, lpr is a program that enables users to print documents on a printer, and ftpd is a program for the transfer of files between local and remote hosts. Because sendmail is the most complex of these programs, we will briefly describe how we exercised sendmail to produce a profile of normal behavior (the methods for constructing synthetic normal for the other two programs are described in Appendix 2). We considered variations in message length, number of messages, message content (text, binary, encoded, encrypted), message subject line, who sent the mail, who received the mail, and mailers. In addition, we looked at the effects of forwarding, bounced mail and queuing. Lastly, we considered the effects of the origin of all these variations in the cases of remote and local delivery." Section 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modified paragraph 3 on pages 17-18:</head><p>We compared system call traces for each of the three categories (successful exploits, unsuccessful exploits and error conditions) with the normal database for the relevant program and recorded the number of mismatches, percentage of mismatches over the trace, and S A values. Table <ref type="table" target="#tab_5">4</ref> shows results for successful intrusions. Each row in the table reports data for one typical trace. In most cases, we have conducted multiple runs of the intrusion with identical or nearly identical results; where runs differed significantly, we report a range of values. To date, we have collected data on five successful intrusions, three of them for sendmail, one for lpr <ref type="bibr" target="#b2">[4]</ref> and one for ftpd <ref type="bibr" target="#b6">[8]</ref> CERT. swinstall vulnerability. Ftp://info.cert.org/pub/ cert_advisories/ CA-96:27, December 1996.</p><p>[9]. The three sendmail intrusions were: sunsendmailcp <ref type="bibr" target="#b1">[2]</ref>, syslogd [3], <ref type="bibr" target="#b5">[7]</ref>, and a decode alias intrusion. These intrusions are described in the appendix. Most of the successful intrusions are clearly detected, with S A values of 0.5 to 0.7. The exception to this is the decode intrusion, which, on the low end of the range, generates only 7 mismatches and a S A value of 0.2. These results suggest approximate detection thresholds that we would need in an online system to detect intrusions. "How much does normal vary between different environments? We have some answers in the case of lpr because we have two normals collected independently at MIT and UNM, for the identical program and operating system. These represent considerably different environments, as can be seen from the differences listed in Table <ref type="table" target="#tab_7">6</ref>, for example, we traced lpr on only one host at UNM, whereas we traced it on 77 hosts at MIT. Despite the differences in environment, the patterns of database growth in the UNM environment are similar to those at MIT (data not shown), although the resulting database sizes are different: 569 unique sequences for UNM and 876 for MIT. These databases not only differ in size, but also in content: For example, a comparison of the unique sequences in both databases for k = = 6 indicates that only 141 of the sequences are the same between the databases, which represents 40% of the UNM database and 29% of the MIT database."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modified the first and third rows of</head><p>Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Added paragraph 3 on page 26:</head><p>"Are these really false positives? A false positive is some sort of acceptable behavior that is classified as anomalous. If the behavior is unacceptable, even if it is not caused by an intrusion, we would want to know about it, because it indicates that the system is not functioning properly or efficiently. Points 1 and 4 above are both instances of irregular behavior symptomatic of a problem with the system; both indicate conditions that need to be rectified. In this sense, neither 1 nor 4 are false positives. This kind of analysis indicates that our actual false positive rate is lower than the reported values, for example, in the case of a chronological normal, the number of false positives would be reduced from 6 to 3."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Section 6.</head><p>Added the last sentence to paragraph 2 on page 26: "Recently, we have successfully detected intrusions in two other programs: a buffer overflow in the xlock program running in Linux, and a symbolic link vulnerability in the swinstall program running under HP-UX."</p><p>Added paragraph 5 on page 27: "Our emphasis has been on determining if our approach can be successful at all. We were not too concerned with efficiency issues in this paper. However, for the system to be able to detect intrusions in real-time---as they are happening---will require careful attention to efficiency issues. As a first step towards this we have analyzed the complexity of our algorithm, although we have not been able to measure its efficiency in a production environment. Should the implementation prove too inefficient, there are numerous simplifications we could experiment with, such as looking only at specific kinds of calls, or only at every tenth call, etc."</p><p>Added paragraph 9 on pages 28-29: "Our approach is similar to several other approaches, although the differences are critical. Ko et al <ref type="bibr" target="#b27">[30]</ref> have also chosen the level of privileged processes, but they characterize the behavior of a privileged process by a program specification or policy, which is a description of what the program should be able to do. This policy is derived from the program code and so requires specialized knowledge of program function. Writing a policy can be prone to the same sorts of errors as writing the program, i.e. it is hard to guarantee correctness. Most importantly, from our perspective, such a policy could easily include behavior that is legal but not normal because it is hard to determine beforehand what behavior should be normal. We avoid these issues by treating the program as a black box, and relying purely on empirical observation to ascertain program behavior. Another key difference is that we rely exclusively on sequencing information, unlike the specification approach, which monitors individual operations. However, there are other approaches, such as TIM <ref type="bibr">[39]</ref>, that consider sequencing information. These differ from our approach in that they look at the domain of user behavior, and use a probabilitistic approach for detecting anomalies. Because our results are sufficiently promising the added complexity of using probabilities seems unnecessary. It is possible that our simple deterministic approach is successful because our data is well-structured. If this is the case, it may well be that probabilities are necessary in less structured domains, such as user behavior."</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An example of a forest of system call sequence trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.S A plotted against sequence length k. From this plot we infer that sequence length makes little difference once we have a length of at least 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Anomaly profile for a run of the syslogd intrusion. The data represents a trace of system calls that is a concatenation of 5 forked sendmail processes. The S A value for this intrusion is 0.7, i.e. the highest point reached on the y axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Growth of database size for lpr real normal collected at MIT. The x axis indicates the number of print jobs traced, and the y axis indicates the number of unique sequences N in the normal database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Bootstrap estimate of change in expected false positive rate as normal database size increases. To summarize, the lowest expected false positive rate in Figure 5 is 0 01 0 004 . . ±. This is about 1 false positive in every 100 jobs, or, on the MIT system, an average of 2 false positives per day. This rate was computed for a normal database of 700 jobs, with 2066 jobs in the test set. From Figure5the false positive rate appears to be leveling off. However, when we increase the size of the normal database to 1400 jobs (not shown in the figure), with a test set of 1366 jobs, the rate drops to 0 005 0 002 . . ± , which is one false positive per day. We are hesitant to draw too many conclusions from these data because they are derived from a single program for which we have only one true positive (an intrusion), and so we cannot get an accurate measure of false negatives, or the false</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 . Number of messages of each type used to generate synthetic sendmail normal. Each number in the table indicates the number of variants used, for example, we used 12 different message lengths.</head><label>1</label><figDesc></figDesc><table><row><cell>Type of Behavior</cell><cell># of Mail Messages</cell></row><row><cell>message length</cell><cell>12</cell></row><row><cell>number of messages</cell><cell>70</cell></row><row><cell>message content</cell><cell>6</cell></row><row><cell>subject</cell><cell>2</cell></row><row><cell>sender/receiver</cell><cell>4</cell></row><row><cell>different mailers</cell><cell>4</cell></row><row><cell>forwarding</cell><cell>4</cell></row><row><cell>bounced mail</cell><cell>4</cell></row><row><cell>queuing</cell><cell>4</cell></row><row><cell>vacation</cell><cell>2</cell></row><row><cell>Total</cell><cell>112</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . Synthetic normal database size N for sequence length of 10, for sendmail, lpr and ftpd.</head><label>2</label><figDesc>Our choice of 10 is based on empirical observations (see section 4.2.2).</figDesc><table><row><cell>Program</cell><cell>Database Size N</cell></row><row><cell>sendmail</cell><cell>1318</cell></row><row><cell>lpr</cell><cell>198</cell></row><row><cell>ftpd</cell><cell>1017</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 . Distinguishing sendmail from other programs. Each column reports results for a single anomalous measures: Mismatches (column 2), percentage of mismatches over a trace (column 3), and S A (column 4). The results shown are for a sequence length of k = = 10 . There are no mismatches against sendmail itself because the database includes all variations.</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>Mismatches</cell><cell>% Mismatches</cell><cell>S A</cell></row><row><cell>ls</cell><cell>42</cell><cell>75</cell><cell>0.6</cell></row><row><cell>ls -l</cell><cell>134</cell><cell>91</cell><cell>1.0</cell></row><row><cell>ls -a</cell><cell>44</cell><cell>76</cell><cell>0.6</cell></row><row><cell>ps</cell><cell>539</cell><cell>97</cell><cell>0.6</cell></row><row><cell>ps -ux</cell><cell>1123</cell><cell>99</cell><cell>0.6</cell></row><row><cell>finger</cell><cell>67</cell><cell>83</cell><cell>0.6</cell></row><row><cell>ping</cell><cell>41</cell><cell>57</cell><cell>0.6</cell></row><row><cell>ftp</cell><cell>271</cell><cell>90</cell><cell>0.7</cell></row><row><cell>pine</cell><cell>430</cell><cell>77</cell><cell>1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 . Detection of successful intrusions for sendmail, lpr and ftpd. The data for the syslogd intrusion show the results of tracing sendmail (rather than tracing syslogd itself). The three columns list the results for various anomalous measures, from mismatches, percentage of mismatches over a trace, to S A . In some cases the columns list a range of values, from minimum to maximum. The results are for</head><label>4</label><figDesc>The results for unsuccessful intrusions and error conditions are shown in Table5. The unsuccessful intrusions are based on attack scripts called sm565a and sm5x. SunOS 4.1.4 has patches that prevent these particular intrusions. Overall, these unsuccessful intrusions are as clearly detectable as the successful intrusions. Error conditions are also detectable within a similar range of S A values. As a clear case of undesirable errors, we have studied local forwarding loops in sendmail (see appendix for a description).</figDesc><table><row><cell></cell><cell>Mismatches</cell><cell>% Mismatches</cell><cell>S A</cell></row><row><cell>syslogd</cell><cell>248 -529</cell><cell>17 -30</cell><cell>0.7</cell></row><row><cell>sunsendmailcp</cell><cell>92</cell><cell>25</cell><cell>0.6</cell></row><row><cell>decode</cell><cell>7 -22</cell><cell>1 -2</cell><cell>0.2 -0.5</cell></row><row><cell>lprcp</cell><cell>242</cell><cell>9</cell><cell>0.5</cell></row><row><cell>ftpd</cell><cell>496</cell><cell>38</cell><cell>0.7</cell></row></table><note><p>k = = 10 .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 . Detection of unsuccessful intrusions and error conditions for sendmail. The three columns list the results for various anomalous measures, from mismatches, percentage of mismatches over a trace, to S A . The results are for k</head><label>5</label><figDesc></figDesc><table /><note><p>= = 10 .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 . Comparison of lpr normals collected at MIT and at UNM. These results are for k = 10 .</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell>UNM</cell><cell>MIT</cell></row><row><cell>Number of hosts</cell><cell>1</cell><cell>77</cell></row><row><cell>Number of print jobs</cell><cell>1234</cell><cell>2766</cell></row><row><cell>Time period (weeks)</cell><cell>13</cell><cell>2</cell></row><row><cell>DB Size N</cell><cell>569</cell><cell>876</cell></row><row><cell>Detection of lprcp:</cell><cell></cell><cell></cell></row><row><cell># mismatches</cell><cell>11009</cell><cell>11006</cell></row><row><cell>% mismatches</cell><cell>7</cell><cell>7</cell></row><row><cell>S A</cell><cell>0.4</cell><cell>0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>table 4</head><label>4</label><figDesc>Added the fourth sentence to the caption of table 4 on page 18:"In some cases the columns list a range of values, from minimum to maximum."</figDesc><table><row><cell>on page 18 :</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Although we are not using a binary alphabet, the measure we use is analogous to a binary Hamming distance, i.e. it is the number of positions in which the two sequences differ.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>This data was collected by Mark Crosbie, at Hewlett Packard.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors gratefully acknowledge support from the Defense Advanced Research Projects Agency (grant N00014-96-1-0680), the Office of Naval Research (grant N00014-95-1-0364), and the National Science Foundation (grant IR-9157644). Some of this work was performed whilst the authors were at the MIT Artificial Intelligence Laboratory, where a very helpful support staff provided a good environment for experimentation. Many people have contributed important ideas and suggestions for this paper, including D. Ackley, A. Koseresow, B. Sanchez, B. LeBaron, P. D'haeseleer, A. B. Maccabe, K. McCurly, N. Minar, G. Hunsicker and M. Crosbie. Some of the data presented in Table <ref type="table">4</ref> and in Table <ref type="table">5</ref> were generated with the help of L. Rogers and T. Longstaff of the Computer Emergency Response Team (CERT).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 1: Description of Intrusions</head><p>This appendix gives more detailed descriptions of intrusions and error conditions that we tested against.</p><p>• sunsendmailcp: The sunsendmailcp script uses a special command line option to cause sendmail to append an email message to a file. By using this script on a file such as /.rhosts, a local user may obtain root access.</p><p>• syslogd: The syslogd attack uses the syslog interface to overflow a buffer in sendmail. A message is sent to the sendmail on the victim machine, causing it to log a very long, specially created error message. The log entry overflows a buffer in sendmail, replacing part of the sendmail's running image with the attacker's machine code. The new code is then executed, causing the standard I/O of a rootowned shell to be attached to a port. The attacker may then attach to this port at his or her leisure. This attack can be run either locally or remotely; we have tested both modes. We also varied the number of commands issued as root after a successful attack.</p><p>• decode: In older sendmail installations, the alias database contains an entry called</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 2: Synthetic Normal Generation</head><p>This appendix describes briefly how synthetic normals were generated for ftpd and lpr.</p><p>The synthetic ftpd was generated by tracing the execution of ftpd, using every option on the ftpd man page at least once.</p><p>The synthetic lpr was generated by tracing the following types of lpr jobs: Printing a text file, printing a postscript file, attempting to print a nonexistent file, printing to several different printers, printing with and without burst pages, printing with symbolic links (the -s option).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.8lgm.org/advisories.html" />
		<title level="m">LGM Advisories</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lgm</forename></persName>
		</author>
		<ptr target="Http://www.8lgm.org/advisories.html" />
		<imprint>
			<date type="published" when="1994-12">8lgm]-advisory-16.unix.sendmail-6-dec-1994. December 94</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lgm</forename></persName>
		</author>
		<ptr target="Http://www.8lgm.org/advisories.html" />
		<imprint>
			<date type="published" when="1991-08-19">8lgm]-advisory-3.unix.lpr.19-aug-1991. August 91</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Next Generation Intrusion Detection Expert Systems (NIDES): A Summary</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Frivold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valdes</surname></persName>
		</author>
		<idno>SRI-CSL-95-07</idno>
		<imprint>
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Laboratory, SRI International</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cert Advisories</surname></persName>
		</author>
		<ptr target="ftp://info.cert.org/pub/cert-advisories/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Syslog vulnerability -a workaround for sendmail</title>
		<author>
			<persName><surname>Cert</surname></persName>
		</author>
		<ptr target="Ftp://info.cert.org/pub/cert_advisories/CA-95:13.syslog.vul" />
		<imprint>
			<date>October 95</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cert</surname></persName>
		</author>
		<ptr target="Ftp://info.cert.org/pub/cert_advisories/CA-96:27" />
		<imprint>
			<date type="published" when="1996-12">December 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Defending a Computer System using Autonomous Agents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Crosbie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Spafford</surname></persName>
		</author>
		<idno>No. 95-022</idno>
	</analytic>
	<monogr>
		<title level="j">Sciences</title>
		<imprint>
			<date type="published" when="1996-03">March 1996</date>
			<publisher>Dept. of Comp</publisher>
		</imprint>
		<respStmt>
			<orgName>Purdue University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Immunological Approach to Change Detection: Algorithms, Analysis and Implications</title>
		<author>
			<persName><forename type="first">P</forename><surname>D'haeseleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Helman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 IEEE Symposium on Computer Security and Privacy</title>
		<meeting>the 1996 IEEE Symposium on Computer Security and Privacy</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Intrusion Detection Model</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Denning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Software Engineering</title>
		<meeting><address><addrLine>Los Alamos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Cryptography and Data Security</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Denning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Addison-Wesley Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The COPS Security Checker System</title>
		<author>
			<persName><forename type="first">D</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Spafford</surname></persName>
		</author>
		<idno>CSD-TR-993</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Purdue University, Comp. Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving the Security of your Site by Breaking into It</title>
		<author>
			<persName><forename type="first">D</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Venema</surname></persName>
		</author>
		<ptr target="ftp://ftp.win.tue.nl/pub/security/admin-guide-to-cracking.101.Z.1995" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S A</forename><surname>Hofmeyr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Somajayi</surname></persName>
		</author>
		<author>
			<persName><surname>Computer</surname></persName>
		</author>
		<author>
			<persName><surname>Immunology</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="88" to="96" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Sense of Self for UNIX Processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Hofmeyr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Somajayi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 IEEE Symposium on Computer Security and Privacy</title>
		<meeting>the 1996 IEEE Symposium on Computer Security and Privacy</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-Nonself discrimination in a Computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Perelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cherukuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1994 IEEE Symposium on Computer Security and Privacy</title>
		<meeting>1994 IEEE Symposium on Computer Security and Privacy</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building diverse computer systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Somayaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ackley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Hot Topics in Operating Systems</title>
		<meeting>the Sixth Workshop on Hot Topics in Operating Systems<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Society Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Neural Network Approach Towards Intrusion Detection</title>
		<author>
			<persName><forename type="first">K L</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><surname>Henning R R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J H</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Simonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th National Computer Security Conference</title>
		<meeting>the 13th National Computer Security Conference<address><addrLine>Washington DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-10">Oct 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Artificial Intelligence and Intrusion Detection: Current and Future Directions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17 th National Computer Security Conference</title>
		<meeting>the 17 th National Computer Security Conference</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Architecture of a Network Level Intrusion Detection System</title>
		<author>
			<persName><forename type="first">R</forename><surname>Heady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maccabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Servilla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990-08">August 1990</date>
		</imprint>
		<respStmt>
			<orgName>Dept. Comp. Science, University of New Mexico</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Network Security Monitor</title>
		<author>
			<persName><forename type="first">L</forename><surname>Heberlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Levitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wolber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Research in Computer Security and Privacy</title>
		<meeting>the IEEE Symposium on Research in Computer Security and Privacy</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Internet Security Monitor: An Intrusion Detection System for Large Scale Networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Heberlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Levitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 15th National Computer Security Conference</title>
		<meeting>15th National Computer Security Conference</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NADIR: An Automated System for Detecting Network Intrusion and Misuse</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stallings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J F</forename><surname>Mcclary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="248" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Porras Ph A. State Transition Analysis: A Rule-Based Intrusion Detection Approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Illgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kemmerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Biologically Inspired Immune System for Computers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kephart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life IV</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Brooks</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Design and Implementation of Tripwire: A File System Integrity Checker</title>
		<author>
			<persName><forename type="first">G H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Spafford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2cnd ACM Conference on Computer and Communications Security</title>
		<meeting>the 2cnd ACM Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated Detection of Vulnerabilities in Privileged Programs by Execution Monitoring</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Levitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 Computer Security</title>
		<meeting>the 1994 Computer Security</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
