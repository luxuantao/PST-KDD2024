<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Effective Statistical Approach to Blog Post Opinion Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ben</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craigm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiyin</forename><surname>He</surname></persName>
							<email>jiyinhe@science.uva.nl</email>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<email>ounis@dcs.gla.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Effective Statistical Approach to Blog Post Opinion Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1107B821E830A65139327A9B3D42C03B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms: Experimentation</term>
					<term>Performance Opinion</term>
					<term>Subjectivity</term>
					<term>Sentiment</term>
					<term>Blog</term>
					<term>Statistics</term>
					<term>Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finding opinionated blog posts is still an open problem in information retrieval, as exemplified by the recent TREC blog tracks. Most of the current solutions involve the use of external resources and manual efforts in identifying subjective features. In this paper, we propose a novel and effective dictionary-based statistical approach, which automatically derives evidence for subjectivity from the blog collection itself, without requiring any manual effort. Our experiments show that the proposed approach is capable of achieving remarkable and statistically significant improvements over robust baselines, including the best TREC baseline run. In addition, with relatively little computational costs, our proposed approach provides an effective performance in retrieving opinionated blog posts, which is as good as a computationally expensive approach using Natural Language Processing techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The rise on the Internet of blogging, the creation of journallike web page logs, has created a highly dynamic subset of the World Wide Web, which evolves and responds to real-world events. Indeed, blogs (or weblogs) have recently emerged as a new grassroots publishing medium. The socalled blogosphere (the collection of blogs on the Internet) opens up several new interesting research areas.</p><p>A key feature that distinguishes blog content from other Web content is their subjective nature. Bloggers tend to express opinions and comments towards some given targets, such as persons, organisations or products. A study of a query log from a commercial blog search engine found that many blog queries seem to be related to uncovering public opinions about a given target <ref type="bibr" target="#b14">[15]</ref>. For example, a user who is planning to buy a given laptop brand might wish to gauge the opinions of other users in the blogosphere about how they rate its features.</p><p>There have been several studies on how to find opinions in the Natural Language Processing (NLP) community. For example, Pang et al. proposed to find opinions from movie reviews using machine learning and NLP techniques <ref type="bibr" target="#b20">[21]</ref>. However, their approach is based on the assumption that the analysed documents are already known to be relevant. Building a retrieval system to uncover documents that are both opinionated and relevant remains a difficult challenge in information retrieval. Since 2006, the Text REtrieval Conference (TREC) has been running a Blog track and a corresponding opinion finding task for addressing this challenge, namely finding opinionated and relevant blog posts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>. The opinion finding task is an articulation of a user search task, where a user is trying to uncover what the public opinions are on the blogosphere, towards a given named-entity target <ref type="bibr" target="#b19">[20]</ref>.</p><p>Under the TREC opinion finding task, an important issue in evaluating a blog post opinion finding system is to look at how the system performs over a baseline for which no opinion feature is applied. The baseline retrieves as many relevant documents as possible, regardless of their opinionated nature. Various approaches have been proposed for the TREC Blog track opinion finding task <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>. However, the experimental results in this task have demonstrated considerable difficulty in improving strong retrieval baselines <ref type="bibr" target="#b19">[20]</ref>. Indeed, only a handful of groups achieved an improvement over their baseline, using techniques such as NLP (for example integrating OpinionFinder <ref type="bibr" target="#b7">[8]</ref>), or SVM classifiers <ref type="bibr" target="#b29">[30]</ref>. In general, most of the proposed approaches utilise different external sources of evidence, mostly heuristically, such as a long list of pre-compiled subjective terms, or rare terms, for detecting opinionated documents. However, evidence that can be learnt from the collection itself, by applying appropriate statistical methods, is not adequately utilised. As a consequence, these proposed approaches either involve considerable manual efforts in collecting evidence for opinions, or lead to little improvement over a baseline that does not include any opinion finding feature <ref type="bibr" target="#b17">[18]</ref>.</p><p>In this paper, we propose a statistical and light-weight automatic dictionary-based approach. We show that de-spite its apparent simplicity, it provides statistically significant improvements over robust baselines, including the best TREC baseline run, without any manual effort. In addition, we show that our proposed approach provides comparable opinion retrieval performances with a sophisticated approach adapting the NLP-based OpinionFinder toolkit <ref type="bibr" target="#b24">[25]</ref>, while being much less computationally expensive.</p><p>The remainder of this paper is organised in three parts. First, we survey previous work on retrieving opinionated blog posts. Next, we present our proposed method using an automatically built dictionary for opinion retrieval. Finally, we provide a thorough evaluation of the proposed method and its variants compared to strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>We introduce the TREC paradigm for experimenting with opinion retrieval in Section 2.1, and previous approaches to opinion retrieval in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The TREC Paradigm for Experimentation of Opinion Retrieval</head><p>The TREC Blog opinion finding task has been running since 2006. This task uses the Blog06 collection, representing a large sample crawled from the blogosphere over an eleven week period from December 6, 2005 until February 21, 2006 <ref type="bibr" target="#b11">[12]</ref>. The collection is 148GB in size, with three main components consisting of 38.6GB of XML feeds (i.e. the blog), 88.8GB of permalink documents (i.e. a single blog post and all its associated comments) and 28.8GB of HTML homepages (i.e. the main entry to the blog). The permalink documents are used as a retrieval unit for the opinion finding task <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>. There are over 3.2 million permalink documents in the Blog06 collection. In this paper, we follow the TREC setting and experiment on the permalink documents.</p><p>Each participating system is evaluated using a set of topics and their associated relevance assessment. For example, a Blog opinion finding topic is included in Figure <ref type="figure" target="#fig_0">1</ref>. The relevance assessment procedure for the documents retrieved for the topics had two levels. The first level assesses whether a given blog post, i.e. a permalink, contains information about the target and is therefore relevant. The second level assesses the opinionated nature of the blog post, if it was deemed relevant in the first assessment level <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>. A system's performance in retrieving opinionated blog post is evaluated by how the system performs over a baseline, which retrieves as many relevant documents as possible, independent of whether they contain an opinion or not.</p><p>For example, in the TREC opinion finding task, submission of baselines was encouraged in TREC 2006, and has been mandatory since TREC 2007 <ref type="foot" target="#foot_0">1</ref> .</p><p>Our experiments in this paper follow this paradigm. We examine if our proposed method brings an improvement when running on top of strong and robust baselines. Moreover, we experiment at the second relevance assessment level, which takes into account only blog posts that are both relevant and opinionated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Previous Work on Opinion Retrieval</head><p>In this section, we briefly survey previous studies on detecting opinion/sentiment for blog post retrieval. In the literature, the blog opinion retrieval system is usually built on top of a baseline, which retrieves as many relevant documents as possible for a given topic, independent of whether they contain an opinion or not. One or several sources of evidence for opinion/sentiment is (are) used for assigning an opinion score to each retrieved document. The opinion score is then combined with the initial relevance score given by the baseline to produce a final document ranking.</p><p>The approach proposed by Yang et al. is a typical example of the above described architecture of the current opinion retrieval approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Different sources of evidence were used in their approach, including pre-compiled lists of terms and indicators created by extracting opinionated terms from training data and manual editing. The subjectivity of blog posts is then determined by scoring the density of the potentially subjective indicators found in the posts. Documents are re-ranked by combining the opinion score of each post with the relevance score given by the baseline.</p><p>Java et al. applied a meta-learning approach using Support Vector Machine (SVM) classifiers based on a manual opinion term dictionary <ref type="bibr" target="#b8">[9]</ref>. Proximity between the opinionated terms in the dictionary and the query terms is considered in the classification. Approaches based on similar ideas were also proposed in the context of the TREC blog track <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>. Zhang et al. performed sentiment analysis on a per-sentence basis <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. For each query topic, they collected a set of subjective and objective sentences that are used to train a sentence classifier. Each query topic has its specific sentence classifier. They used the Wikipedia as an external source of objective sentences, and RateitAll.com and other Web sources as a source for subjective sentences. They then used SVMs with a default kernel to build the sentence classifier. This sentence classifier then gives a score that is combined with the relevance score produced by their baseline, which retrieves topic-relevant blog posts regardless of the notion of opinion. Mishne proposed a dictionarybased approach based on the use of the General Inquirer, specifically the Osgoods semantic dimensions and emotional categories <ref type="foot" target="#foot_1">2</ref> to produce an opinion score that is linearly combined with the baseline relevance score <ref type="bibr" target="#b15">[16]</ref>.</p><p>Many other approaches were also proposed. For instance, Amati et al. proposed a semi-automatic method for learning an opinion dictionary from the Blog06 collection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Yang et al. used logistic regression to classify the opinion or non-opinion statements at the sentence level. The model is trained on external corpora and was applied for crossdomain learning <ref type="bibr" target="#b25">[26]</ref>. Godbole et al. developed an algorithm to construct a sentiment lexicon by expanding small dimension sets of seed sentiment words. By marking up all sentiment words and the associate entities in the corpus, they assigned a subjectivity score to the text <ref type="bibr" target="#b5">[6]</ref>. Ernsting et al. expand the queries using the collection enrichment technique, based on the idea that an external resource could bring useful additional query terms <ref type="bibr" target="#b4">[5]</ref>. They also showed that applying Kullback-Leibler (KL) divergence-based language modelling with Jelinek-Mercer smoothing for opinion term weighting markedly hurts the retrieval performance. In this paper, we will explain the reason for the detrimental effect of applying KL-based language modelling for opinion term weighting. Our explanation will also be confirmed by experiments.</p><p>In essence, from the first two years of the TREC Blog track opinion finding task, it has proved to be difficult to improve over a reasonably strong topic-relevance baseline (i.e. a system where all opinion finding features are turned off). Indeed, only a few participating groups were able to do so <ref type="bibr" target="#b19">[20]</ref>. In the next section, we propose a purely statistical approach for opinion retrieval. Unlike the aforementioned approaches, the proposed technique does not require manual efforts, as the opinion dictionary is automatically derived from the collection itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE STATISTICAL DICTIONARY-BASED APPROACH TO OPINION RETRIEVAL</head><p>In this section, we propose a statistical approach to retrieving opinionated blog posts. Our proposed approach has four steps. First, it automatically generates a dictionary from the collection without requiring manual effort. Second, it assigns a weight to each term in the dictionary, which represents how opinionated the term is. Third, it assigns an opinion score to each document in the collection using the top weighted terms from the dictionary as a query. Finally, it appropriately combines the opinion score with the initial relevance score produced by the retrieval baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dictionary Generation</head><p>Our dictionary is automatically derived from the document collection used. To derive the dictionary, we apply the skewed query model to filter out too frequent or too rare terms in the collection <ref type="bibr" target="#b3">[4]</ref>. We remove those terms because if a term appears too many or too few times in the collection, then it probably contains too little (e.g. "and") or too specific (e.g. "aanandha") information so that it can not be generalised to different queries in indicating opinion. Using the skewed model, we firstly rank all terms in the collection by their within-collection frequencies in descending order. The terms, whose rankings are in the range (s•#terms, u•#terms), are selected in the dictionary. #terms is the number of unique terms in the collection. s and u are parameters of the skewed model. In this paper, we apply s = 0.00007 and u = 0.001 as suggested in <ref type="bibr" target="#b3">[4]</ref>.</p><p>A snippet of the automatically generated dictionary de- rived from the Blog06 collection is shown in Table <ref type="table" target="#tab_0">1</ref>. From this table, we can see that many terms in the dictionary are not necessarily opinionated, since the dictionary generation process is independent of the notion of opinion. However, as we show later in our experiments, they can be good indicators of opinion when they are put into the context of the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Term Weighting</head><p>This section presents how we assign weights to terms in the opinion dictionary. Our approach is inspired by the Divergence From Randomness (DFR) query expansion mechanism, which measures the divergence of a term's distribution in a pseudo-relevance set from its distribution in the whole collection <ref type="bibr" target="#b0">[1]</ref>. Our approach assumes a training step. For a set of training queries, we assume that D(Rel) is the document set containing all relevant documents, and D(opRel) is the document set containing all opinionated relevant documents. D(opRel) is a subset of D(Rel). For each term t in the opinion term dictionary, we measure wopn(t), the divergence of the term's distribution in D(opRel) from that in D(Rel). This divergence value measures how a term stands out from the opinionated documents, compared with all relevant, yet not necessarily opinionated, documents. The higher the divergence is, the more opinionated the term is.</p><p>In information retrieval, a commonly used measure for term weighting is the Kullback-Leibler (KL) divergence from a term's distribution in a document set to its distribution in the whole collection. For instance, Ernsting et al. applied the KL divergence-based language modelling with Jelinek-Mercer smoothing for weighting opinionated terms <ref type="bibr" target="#b4">[5]</ref>. However, their experimental results showed that this method has detrimental effect on the retrieval performance. Regarding this problem, we argue that the KL divergence measure considers only the divergence from one distribution to the other, while ignoring how frequent a term occurs in the opinionated documents. As a consequence, the weights of the terms in the opinion dictionary might be biased towards the terms with high KL divergence values, but containing low information in the opinionated document set D(opRel). For example, if a term appears only 3 times in the collection, and twice in the opinionated documents, this term is likely to have a high KL divergence. However, we don't consider this term to show a strong evidence of opinion because it appears only in at most two opinionated documents in the entire collection. Therefore, we rather apply the Bo1 term weighting model based on the Bose-Einstein statistics given by the geometric distribution, which measures how informative a term is in the set D(opRel) against D(Rel) <ref type="bibr" target="#b0">[1]</ref>. Using the Bo1 model, the weight of a term t in the opinionated document set D(opRel) is given by:</p><formula xml:id="formula_0">wopn(t) = tfx • log 2 1 + λ λ + log 2 (1 + λ) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where λ is the mean of the assumed Poisson distribution of the term t in the relevant documents. It is given by tf rel /N rel . tf rel is the frequency of the term t in the relevant documents, and N rel is the number of relevant documents. tfx is the frequency of the term t in the opinionated documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating the Opinion Score</head><p>We take the X top weighted terms from the opinion dictionary, and submit them to the retrieval system as a query Qopn. By doing this, the retrieval system assigns a relevance score to each document in the collection using a document weighting model, e.g. the BM25 model <ref type="bibr" target="#b21">[22]</ref>, or the PL2 Divergence From Randomness (DFR) model <ref type="bibr" target="#b0">[1]</ref>. Such a relevance score reflects the extent to which the top weighted opinionated terms are informative in the document, capturing the overall opinionated nature of the document.</p><p>We denote the relevance score, assigned for query Qopn for document d, as the opinion score Score(d, Qopn). In the next step, this opinion score is combined with the relevance score Score(d, Q), given by the initial document ranking, to produce the final document ranking. Note that the proposed opinion scoring method is light-weight because it is performed during indexing, independently of the retrieval stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Score Combination</head><p>We apply two different methods for combining the initial relevance score with the opinion score, namely an intuitive linear combination, and a combination method that maps document opinion scores to probabilities. The initial relevance score is given by a retrieval baseline, which is independent of any expressed opinion in the document. The first method applies a linear combination:</p><p>Linear combination:</p><formula xml:id="formula_2">Scorecom(d, Q) = (1-a)Score(d, Qopn)+a•Score(d, Q) (2)</formula><p>where each score Score(d, Qopn) (resp. Score(d, Q)) is scaled by dividing the score by the maximum Score(d, Qopn) (resp. Score(d, Q)). a is the free parameter of the linear combination.</p><p>Our second combination method maps each opinion score to the maximum likelihood of the probability P (opn|d, Qopn) of being opinionated as follows:</p><formula xml:id="formula_3">P (opn|d, Qopn) = Score(d, Qopn) P d∈Coll Score(d, Qopn) (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where Coll is the entire document collection. Since a high P (opn|d, Qopn) is supposed to indicate a high degree of opinion expressed in the document, we would like to have a combined score that is an increasing function of P (opn|d, Qopn). Therefore, such a probability P (opn|d, Qopn) is combined with the initial relevance score using a logarithmic function as follows:</p><p>Log. combination:</p><formula xml:id="formula_5">Scorecom(d, Q) = -k log2P (opn|d, Qt) + Score(d, Q) (4)</formula><p>where k is a free parameter. Both score combination methods use the stored opinion scores of all documents, computed during indexing. Therefore, there is only a negligible additional overhead during retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL ENVIRONMENT AND SETTINGS</head><p>We use the Terrier Information Retrieval platform for both indexing and retrieval <ref type="bibr" target="#b18">[19]</ref>. In the rest of this section, we describe our experimental environment and settings for evaluating our proposed dictionary-based approach to the blog opinion retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Blog06 Test Collection and Topics</head><p>We base our experiments on the Blog06 collection created for the TREC Blog track <ref type="bibr" target="#b11">[12]</ref>, which is currently the only available Blog test collection with relevance assessments. Following the official TREC setting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>, we index only the permalinks, which are the blog posts and their associated comments. The permalinks are used as the retrieval units in the TREC Blog track opinion finding task. Each term is stemmed using Porter's English stemmer, and standard English stopwords are removed.</p><p>We use the 100 topics from the TREC 2006 &amp; 2007 opinion finding tasks, numbered from 851 to 950. We use the 50 topics from the opinion finding task in 2006 for training, and the 50 topics from TREC 2007 for testing. Each topic contains three topic fields, namely title, description and narrative. We only use the title topic field that contains very few keywords related to the topic. The title-only queries are usually short<ref type="foot" target="#foot_2">3</ref> , which is a realistic snapshot of real user queries in practise and the official TREC setting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Retrieval Baselines</head><p>Following the aforementioned TREC opinion finding task paradigm, our baseline retrieves as many relevant documents as possible independently of whether they are opinionated or not.</p><p>Firstly, we apply the InLB document weighting model, which is generated from the Divergence from Randomness (DFR) modular framework <ref type="bibr" target="#b18">[19]</ref>. The InLB model applies the Inverse Document Frequency and Laplace succession for document weighting <ref type="bibr" target="#b0">[1]</ref>, and BM25's normalisation function to normalise the term frequency <ref type="bibr" target="#b21">[22]</ref>. We use InLB because it provides effective retrieval performance on the Blog06 collection, and because it is a hybrid model combining BM25 and the DFR document weighting paradigm. In InLB, for a given document d and query Q, the relevance score is given by:</p><formula xml:id="formula_6">Score(d, Q) = X t∈Q w(d, t) = X t∈Q qtw • tf n tf n + 1 log 2 N + 1 df + 0.5</formula><p>(5) where the query term weight qtw is given by qtf/qtfmax; qtf is the query term frequency. qtfmax is the maximum query term frequency among the query terms. N is the number of documents in the collection. df is the number of documents containing the query term t. The normalised term frequency tf n is given by BM25's normalisation function <ref type="bibr" target="#b21">[22]</ref> as follows:</p><formula xml:id="formula_7">tf n = tf (1 -b) + b • l avg l (6)</formula><p>where tf is the within-document term frequency, l is the document length and avg l is the average document length in the whole collection. b is a free parameter. In this paper, we set b to 0.2337 based on optimisation on the 50 training topics.</p><p>On top of the InLB model, our second baseline applies the pBiL2 randomness model <ref type="bibr" target="#b10">[11]</ref>, which utilises the query term proximity evidence for retrieval, to favour documents where the query terms appear in close proximity. The model we apply is based on the binomial randomness model. It computes the score of a pair of query terms in a document as follows:</p><formula xml:id="formula_8">score(d, Q2) = X p∈Q 2 1 pf n + 1 • " -log 2 (avg w -1)! + log 2 pf n! + log 2 (avg w -1 -pf n)! -pf n log 2 (pp)<label>( 7</label></formula><p>)</p><formula xml:id="formula_9">-(avg w -1 -pf n) log 2 (p p ) "</formula><p>where Q2 is the set of all query term pairs in query Q. avg w = T -N(ws-1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N</head><p>is the average number of windows of size ws tokens in each document in the collection, N is the number of documents in the collection, and T is the total number of tokens in the collection. pp = 1 avg w-1 , p p = 1pp, and pf n is the normalised frequency of the tuple p, as obtained using Normalisation 2 4 <ref type="bibr" target="#b10">[11]</ref>. In this paper, Normalisation 2's free parameter cp is set to 90 based on experiments on the 50 training topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">External Opinion Dictionary and Term Weighting</head><p>To compare with the dictionary derived from the collection itself, we also manually generate a dictionary compiled from various external linguistic resources such as Opinion-Finder <ref type="bibr" target="#b24">[25]</ref> and those used in the approaches mentioned in Section 2. The dictionary contains approximately 12,000 English words, mostly adjectives, adverbs and nouns, which are supposed to be subjective. A snippet of this dictionary is shown in Table <ref type="table" target="#tab_1">2</ref>. In this paper, we denote the manually edited dictionary by the external dictionary, and we denote the automatically derived one by the internal dictionary.</p><p>As suggested in Section 3.2, the KL divergence measure does not consider how informative a term is in the opinionated documents. Therefore, the term weights, assigned by the KL divergence measure on one topic set, cannot be generalised to other topics because KL ignores how informative the term is in the opinionated documents. To confirm this argument, in addition to Bo1, we also apply the KL term weighting model based on the KL divergence measure. Using the KL model, the weight of a term t in the opinionated 4 Normalisation 2 is a term frequency normalisation method that assumes a decreasing density of term frequency with document length <ref type="bibr" target="#b0">[1]</ref> document set D(opRel) is given by <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_10">wopn(t) = p(t|D(opRel)) • log 2 p(t|D(opRel)) p(t|D(Rel)) (8)</formula><p>where p(t|D(opRel)) = tfx/c(D(opRel)) is the probability of observing term t in the opinionated document set. tfx is the frequency of the term t in the opinionated document set, and c(D(opRel)) is the number of tokens in the opinionated document set. p(t|D(Rel)) = tf rel /c(D(Rel)) is the probability of observing term t in the relevant document set D(Rel). tf rel is the frequency of t in D(Rel), and c(D(Rel)) is the number of tokens in D(Rel). In the next section, we compare the opinion term weighting using Bo1 with that using KL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS: OPINION TERM WEIGHTING</head><p>An underlying hypothesis of our proposed approach is that the most opinionated terms, derived from the relevant and opinionated documents for one query set, are also good indicators of opinion for other queries. In this section, we conduct experiments to examine this hypothesis with the use of two different term weighting models, namely KL (see Equation ( <ref type="formula">8</ref>)) and Bo1 (see Equation ( <ref type="formula" target="#formula_0">1</ref>)).</p><p>We randomly sample from the 50 training topics for 10 times, with each sample having 25 topics. During the sampling process, we ensure that each two samples have a reasonably small overlap (i.e. 65% maximum). For each sample of 25 topics, we rank the terms in the dictionary by their term weights using the corresponding relevance assessments information. Using the relevance assessments in each sample, the weight of each term is measured by the divergence of the term's distribution in the opinionated documents from its distribution in all relevant documents.</p><p>We compute the cosine similarity between the weights of the top 100 weighted terms from each two samples from the training topics. Figure <ref type="figure">2</ref> plots the distribution of the resulting cosine similarity scores using Bo1 and KL for external and internal opinion dictionaries, respectively. From this figure, we can see that the use of the Bo1 model for term weighting leads to high similarities (with a mean of 0.8487 for the external dictionary, and 0.7531 for the internal dictionary) between the term weights derived from different random samples of the training topics. On the contrary, the use of the KL model leads to a situation where different random samples agree little with each other in terms of the top weighted terms. When using the KL model, the cosine similarity between the top weighted terms from different samples is very low (with a mean of 0.04184 for the external dictionary and 0.07329 for the internal dictionary) as shown in Figure <ref type="figure">2</ref>. This confirms our argument in Section 3.2 that the term weighting by the KL divergence measure cannot be generalised to different topics because the KL divergence measure ignores how informative a term is in the opinionated document set. This also explains why the KL divergencebased language modelling for opinion term weighting did not work in a previous study <ref type="bibr" target="#b4">[5]</ref>. We have also conducted experiments with applying Jelinek-Mercer smoothing for the KL divergence and obtained similar findings. The related results are not included in this paper for brevity.</p><p>As an example, Tables <ref type="table" target="#tab_3">3</ref>     internal and external dictionary, respectively. From these two tables, we find that terms in both internal and external dictionaries, e.g. "Bush", "war", "movie" and "Iraq", are often related to controversial topics for which bloggers tend to express opinions. In the next sections, we show that both dictionaries actually result in comparable opinion retrieval performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS: VALIDATION</head><p>In this section, we describe our experiments for training the parameter X (i.e. the number of top-ranked terms in the dictionary used for assigning opinion scores to documents)  and the free parameters a and k in Equations ( <ref type="formula">2</ref>) &amp; (4).</p><p>For training X, we reuse the 10 samples of topics created in the previous section. For each sample, the 25 chosen topics are used for assigning term weights to the terms in the dictionary. The other <ref type="bibr" target="#b24">25</ref>   trieval performance according to our experimental results.</p><p>For each set of opinion score Scorei(d, Qopn), assigned by using the Xi top weighted terms in the dictionary, we tune the parameter of each score combination method (a and k in Equations ( <ref type="formula">2</ref>) &amp; ( <ref type="formula">4</ref>)) by maximising Mean Average Precision (MAP) on the validation topic set. The resulting maximised MAP, using the opinion scores assigned by the Xi top weighted terms in the dictionary on the jth validation set, is denoted as MAPmax(Xi,j ). The optimised X value is then the Xi that gives the highest MAPmax(Xi,j ), the mean MAPmax(Xi,j ) over the 10 validation sets. Figures <ref type="figure">3</ref> and<ref type="figure">4</ref> plot MAPmax(Xi,j ) against different X values used in the validation process, using Bo1 and KL for term weighting, respectively. From Figure <ref type="figure">3</ref>, we can see that the use of Bo1 for term weighting results in a consistent improvement over the baseline using InLB, with and without the use of term proximity. On the contrary, the use of KL for term weighting leads to a marked degradation of the retrieval performance (see Figure <ref type="figure">4</ref>) compared to the baseline. Such a degradation is statistically significant ac-cording to the Wilcoxon signed-rank matched-pairs test<ref type="foot" target="#foot_3">5</ref> at 0.01 level. This observation is expected since when KL is used for term weighting, different samples have little agreement on the most opinionated terms according to Figure <ref type="figure">2</ref>. Moreover, Figure <ref type="figure">3</ref> shows that using Bo1 for term weighting, the resulting retrieval performance of our approach is stable over a wide range of X values. In particular, X = 100 provides the best retrieval performance across the 10 different random samples of topics from the training topic set, for both the external and internal dictionaries. Therefore, we use X = 100 in our experiments on the test topics .</p><p>After X is fixed, on the 50 training topics, a parameter sweeping is applied to optimise the free parameters a and k in Equations ( <ref type="formula">2</ref>) &amp; (4). The sweeping is applied within [0, 1] with an interval of 0.05 for a, and within (0, 1000] with an interval of 50 for k. From the training, we obtain a = 0.25 and k = 250, which will be applied on the 50 test topics from the TREC 2007 Blog track opinion finding task.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EXPERIMENTS: EVALUATION</head><p>This section evaluates our proposed method on the test topics. Our experiments on the test topics are summarised in Figures <ref type="figure">5</ref> and<ref type="figure" target="#fig_4">6</ref>, without and with the use of term proximity in the baseline, respectively. From both figures, we can see that Bo1 results in a much better retrieval performance than KL in all cases. Indeed, KL's resulting MAP values are always statistically significantly lower than the those of Bo1, according to the Wilcoxon test at 0.01 level. This is expected because, as mentioned as Section 5, when KL is used for term weighting, different random samples from the training topic set agree little on the top weighted opinionated terms.</p><p>From Figures <ref type="figure">5</ref> and<ref type="figure" target="#fig_4">6</ref>, we also find that the effectiveness of the Log. combination method (see sub-Figures <ref type="figure">5(b</ref>) &amp; 6(b)) seems to be less sensitive to the change of its parame-ter value than the linear combination (see sub-Figures <ref type="figure">5(a</ref>) &amp; 6(a)). To test this observation, we compute the Entropy and the Thread measures proposed by Metzler for measuring the parameter sensitivity <ref type="bibr" target="#b13">[14]</ref>. Entropy measures how much variation of retrieval effectiveness is there over a working range of parameter values, and Spread measures the distance between the best and the worst retrieval effectiveness within this working range of parameter values <ref type="bibr" target="#b13">[14]</ref>. In our computation, this working range of values of parameters a or k are the same as those used for parameter sweeping introduced in Section 6. Table <ref type="table" target="#tab_8">5</ref> contains the obtained Entropy and Spread values for using Bo1. We can see that both combination methods lead to relatively similar Entropy values. However, the Log. combination method provides a smaller Spread value than the linear combination in all cases. As stated in <ref type="bibr" target="#b13">[14]</ref>, it is preferred to have a low Spread over a low Entropy. Therefore, we conclude that the Log. combination method (Equation ( <ref type="formula">4</ref>)) has a lower parameter sensitivity than the linear one (Equation ( <ref type="formula">2</ref>)).</p><p>Table <ref type="table" target="#tab_9">6</ref> compares the retrieval performance of our approach with the baselines. The setting of parameters a and k is obtained on the training topics, which is a = 0.25 and k = 250. We only report the results obtained using Bo1 in this Table since KL's performance is already shown to be less effective in Figures <ref type="figure">5</ref> and<ref type="figure" target="#fig_4">6</ref>. Table <ref type="table" target="#tab_9">6</ref> shows remarkable improvement over the baselines brought by our proposed dictionary-based approach. All improvements are statistically significant according to the Wilcoxon test at 0.01 level. Moreover, although the use of the external dictionary leads to a better performance than the internal one in all cases, the difference is minor and statistically insignificant according to the Wilcoxon test at 0.05 level. This demonstrates that our proposed approach is capable of achieving effective performance while being efficient and practical without the need for any manual effort.</p><p>In addition, we also examine if our proposed approach is able to improve the best TREC baseline run, namely uams07topic proposed in <ref type="bibr" target="#b4">[5]</ref> <ref type="foot" target="#foot_4">6</ref> . This run applies the collection enrichment technique, which expands the queries on the AQUAINT2 collection, and retrieves from the Blog06 collection using the expanded query. Run uams07topics achieved the best topic-relevance baseline run, and also the second best opinion finding run in the TREC 2007 Blog track opinion finding task, despite having no opinion finding features enabled <ref type="bibr" target="#b12">[13]</ref>. Table <ref type="table" target="#tab_10">7</ref> provides the result of applying our proposed approach on top of the best TREC baseline run.</p><p>From Table <ref type="table" target="#tab_10">7</ref>, we find that our proposed approach significantly improves uams07topics with the use of either external or internal dictionary. When the internal dictionary is used with Log. combination, our proposed approach provides an MAP of 0.3671, which would make it the second best run in the TREC 2007 Blog track opinion finding task. Note that the best run in this task was submitted by the University of Illinois at Chicago, which applies Support Vector Machines for sentiment analysis. Despite the effectiveness of their approach, its application requires extensive training on large amount of data collected from Wikipedia, RateitAll.com, etc. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. On the other hand, using our proposed approach, the time spent on training our model, including the dictionary generation, is relatively trivial (approximately 15 minutes with a Pentium III 1GHz processor). Note that in a dynamic environment where the collection grows from time to time, using our proposed method, it is not necessary to repeat the training in responce to the collection growth, unless the growth has a significant impact on the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">COMPARISON WITH BLOG OPINION RETRIEVAL USING OPINIONFINDER</head><p>We also compare our proposed approach with the one proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, which uses OpinionFinder, a freely available and sophisticated Natural Language Processing (NLP) toolkit <ref type="bibr" target="#b24">[25]</ref>, to identify subjectivity in text. Applying Opin-ionFinder was shown to be one of the most effective opinion identification features <ref type="bibr" target="#b19">[20]</ref>.</p><p>For a given document, OpinionFinder is adapted to produce an opinion score for each document, based on the identified opinionated sentences. The opinion score Score(d, OF ) of a document d produced by OpinionFinder is defined as follows:</p><formula xml:id="formula_11">Score(d, OF ) = sumdif f • #subj #sent<label>(9)</label></formula><p>where #subj and #sent are the number of subjective sentences and the number of sentences in the document, respectively. sumdiff is the sum of the diff value of each subjective sentence in the document, showing the confidence level of subjectivity estimated by OpinionFinder. For a given new query, such an opinion score is then combined with the relevance score Score(d, Q) to produce the final relevance score in the same way as described above  for the dictionary-based approach. The only difference is to replace Score(d, Qopn) with Score(d, OF ) in Equations ( <ref type="formula">2</ref>) &amp; (4). Moreover, the free parameters of the combination methods are set to a = 0.25 and k = 100 based on training using the topics of the TREC 2006 opinion finding task. Table <ref type="table" target="#tab_12">8</ref> compares the retrieval performance of our proposed approach with the one using OpinionFinder, with two different combination methods, and three different baselines. Bo1 is used for weighting the terms in the internal dictionary. From Table <ref type="table" target="#tab_12">8</ref>, we find that both the OpinionFinderbased approach and our dictionary-based approach provide comparable retrieval performance. According to the Wilcoxon test, there is no statistically significant difference between their resulting MAP values at the 0.05 level.</p><p>Our dictionary-based approach is light-weight because the opinion scoring of the documents are performed during indexing, and the overall process has negligible computational overheads. In contrast, using OpinionFinder to process the blog posts, it took approximately 19,370 CPU hours of a Pentium III 1GHz processor to process the 3.2 million documents in the Blog06 collection, and the processing time is expected to increase if the collection grows, even if the growth is insignificant. Such figures make the OpinionFinder-based approach difficult to use in an operational setting, despite its effectiveness in mining subjectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we have proposed an effective and practical approach to retrieving opinionated blog posts without the need for manual effort. The proposed approach is practical in the sense that the opinion scores are computed during indexing, and the involved computational cost is neglegible compared to other state-of-the-art approaches. Through extensive experiments on the large-scale Blog06 test collection, our proposed approach has shown marked and statistically significant improvements over strong and robust baselines, including the best TREC baseline run. Despite the simplicity of our proposed approach, it is effective and is capable of achieving the second best TREC run. The use of the automatically generated internal dictionary provides a retrieval performance that is as good as the use of an external dictionary manually compiled from various linguistic resources. In addition, our proposed approach provides a comparable retrieval performance to the approach using OpinionFinder, a toolkit for mining subjectivity based on NLP techniques, while being relatively less computationally expensive.</p><p>Moreover, in this paper, we have shown that the detection of opinionated blog documents can be effectively done in a statistical way, if appropriate statistics are applied. We have shown that different random samples from the collection reach a high concensus on the opinionated terms if the Bose-Einstein statistics given by the geometric distribution are applied. We have also explained the reason why the commonly used Kullback-Leibler divergence measure sometimes fails in selecting opinionated terms. Such an explanation was confirmed by our experiments.</p><p>In the future, we plan to investigate further applications of our proposed approach. For example, we plan to extend the work to detecting the polarity or the orientation of the retrieved opinionated documents <ref type="bibr" target="#b12">[13]</ref>. We also plan to study the connection of the opinion finding task to question answering, for example, by extracting the opinionated sentences within a blog post about a given target.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Blog 2007 opinion finding task, topic 930.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>&amp; 4 contain the top 20 weighted terms derived from one of the 10 random samples, from the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Cosine similarity distribution between the top 100 weighted terms from different samples of topics using Bo1 and KL with external and internal opinion dictionaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Parameter X (i.e. the number of top weighted opinion terms) against the mean best MAP obtained on the validation sets with probability (prob.) or linear combination. KL is used for term weighting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The combination parameter (a in Equation (2) or k in Equation (4)) against MAP obtained on the test topics using linear or Log. combination. Term proximity is applied in the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : A snippet of the dictionary derived from the Blog06 collection.</head><label>1</label><figDesc></figDesc><table><row><cell>girl</cell><cell>director</cell><cell cols="2">simply consider</cell></row><row><cell>fall</cell><cell>researcher</cell><cell>build</cell><cell>radio</cell></row><row><cell>class</cell><cell>large</cell><cell cols="2">respect version</cell></row><row><cell cols="2">subscriber education</cell><cell>flame</cell><cell>October</cell></row><row><cell>result</cell><cell>optional</cell><cell>leader</cell><cell>yeah</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : A snippet of the external opinion dictionary.</head><label>2</label><figDesc></figDesc><table><row><cell>dwelling</cell><cell>distinguished</cell><cell>Bush</cell><cell>taken</cell></row><row><cell>implementation</cell><cell>London</cell><cell>load</cell><cell>bonkers</cell></row><row><cell>Colorado</cell><cell>tantalizing</cell><cell>keenly</cell><cell>feisty</cell></row><row><cell>defiantly</cell><cell>agitation</cell><cell cols="2">torturous joyfully</cell></row><row><cell>trump</cell><cell>gray</cell><cell>Iraq</cell><cell>inventor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 : An example of the top 20 weighted terms from the internal dictionary on one of the sampled topic sets.</head><label>3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 : An example of the top 20 weighted terms from the external dictionary on one of the sampled topic sets.</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>remaining topics in the training set are used for validation. We call this set of 25 remaining topics the validation set. For each set of 25 sampled topics,</figDesc><table><row><cell>0.21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">external Log.</cell><cell></cell></row><row><cell>0.19</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">internal Log. external linear</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">internal linear</cell><cell></cell></row><row><cell>0.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">basline</cell><cell></cell></row><row><cell>MAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>400</cell><cell>450</cell><cell>500</cell></row><row><cell></cell><cell></cell><cell></cell><cell>X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>we use the corresponding relevance assessment to compute the opinion score Score(d, Qopn). Different values of X are used in our experiments. In this paper, we report only results with X ranging from 50 to 500 with an interval of 50, since larger or smaller X values do not result in better re-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 : The Entropy and Spread values obtained using the linear combination or the Log. combination with External or Internal dictionary.</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Baseline</cell><cell cols="2">Linear Comb.</cell><cell cols="2">Log Comb.</cell></row><row><cell>Baseline</cell><cell cols="2">MAP bl MAP l</cell><cell>diff.</cell><cell>MAP log</cell><cell>diff.</cell></row><row><cell></cell><cell></cell><cell cols="2">External dictionary</cell><cell></cell><cell></cell></row><row><cell>InLB</cell><cell>0.2727</cell><cell>0.2991</cell><cell>+9.68*</cell><cell>0.3049</cell><cell>+11.81*</cell></row><row><cell>InLB+Prox.</cell><cell>0.3027</cell><cell cols="2">0.3362 +12.28*</cell><cell>0.3402</cell><cell>+13.75*</cell></row><row><cell></cell><cell></cell><cell cols="2">Internal dictionary</cell><cell></cell><cell></cell></row><row><cell>InLB</cell><cell>0.2727</cell><cell>0.2924</cell><cell>+7.22*</cell><cell>0.2981</cell><cell>+9.31*</cell></row><row><cell>InLB+Prox.</cell><cell>0.3027</cell><cell cols="2">0.3304 +10.08*</cell><cell>0.3377</cell><cell>+12.83*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 : The MAP of the baselines (MAP bl ), opinion finding with linear combination (MAP l ), and that with Log. combination (MAP log ). diff. is the improvement over the baselines in percentage. Bo1 is used for term weighting. All improvements are statistically significant at 0.01 level as indicated by the stars.</head><label>6</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 : The MAP of the best TREC baseline run (MAPuams), opinion finding with linear combination (MAP l ), and that with Log. combination (MAP log ). diff. is the improvement over the baselines in percentage. Bo1 is used for term weighting. All improvements are statis- tically significant at 0.01 level as indicated by the stars.</head><label>7</label><figDesc></figDesc><table><row><cell cols="2">uams07topic baseline</cell><cell cols="2">Linear Comb.</cell><cell cols="2">Log Comb.</cell></row><row><cell cols="3">Dictionary MAPuams MAP l</cell><cell>diff.</cell><cell>MAP log</cell><cell>diff.</cell></row><row><cell>External</cell><cell>0.3453</cell><cell cols="2">0.3737 +8.22*</cell><cell>0.3749</cell><cell>+8.57*</cell></row><row><cell>Internal</cell><cell>0.3453</cell><cell cols="2">0.3655 +5.85*</cell><cell>0.3671</cell><cell>+6.31*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 : The MAP obtained by using OpinionFinder (MAP OF ) and by our opinion finding method using inter- nal dictionary (MAP int ) when applied to three different baselines. No statistically significant difference is found.</head><label>8</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://ir.dcs.gla.ac.uk/wiki/TREC-BLOG</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.wjh.harvard.edu/ ~inquirer/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p><ref type="bibr" target="#b0">1</ref>.74 words on average in the 100 topics used.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We call the Wilcoxon signed-rank matched-pairs test as the Wilcoxon test in the rest of this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We would like thank the TREC organisers for making run uams07topic available for our research.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Probabilistic models for information retrieval based on Divergence from Randomness</title>
		<author>
			<persName><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">FUB, IASI-CNR and University of Tor Vergata at TREC 2007 Blog Track</title>
		<author>
			<persName><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ambrosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gaibisso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gambosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic Construction of an Opinion-Term Vocabulary for Ad Hoc Retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ambrosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gaibisso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gambosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECIR</title>
		<meeting>ECIR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Case Study of Distributed Information Retrieval Architectures to Index One Terabyte of Text</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cacheda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language Modeling Approaches to Blog Post and Feed Finding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ernsting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-Scale Sentiment Analysis for News and Blogs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Srinivasaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Experiments in Blog and Enterprise Tracks with Terrier</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ranking Opinionated Blog Posts using OpinionFinder</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The BlogVox Opinion Retrieval System</title>
		<author>
			<persName><forename type="first">A</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bloggers : a portrait of the Internet&apos;s new storytellers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lenhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pew Internet &amp; American Life Project</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">University of Glasgow at TREC 2006: Experiments in Terabyte and Enterprise Tracks with Terrier</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
		<title level="m">The TREC Blog06 Collection : Creating and Analysing a Blog Test Collection DCS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2007 Blog Track</title>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation, sensitivity, and generalization in parameterized retrieval models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Study of Blog Search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECIR</title>
		<meeting>ECIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple Ranking Strategies for Opinion Retrieval in Blogs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using Blog Properties to Improve Retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2006 Blog Track</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSIR 2006 Workshop</title>
		<meeting>OSIR 2006 Workshop</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the TREC Blog track</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISWSM 2008</title>
		<meeting>ISWSM 2008</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Okapi at TREC-4</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 4</title>
		<meeting>TREC 4</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using Subjective Adjectives in Opinion Retrieval from Blogs</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">TREC: Experiment and Evaluation in Information Retrieval</title>
		<author>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">OpinionFinder: a system for subjectivity analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/EMNLP on Interactive Demonstrations</title>
		<meeting>HLT/EMNLP on Interactive Demonstrations</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge Transfer and Opinion Detection in the TREC 2006 Blog Track</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fusion Approach to Finding opinions in Blogosphere</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">WIDIT in TREC 2007 Blog Track: Combining Lexicon-Based Methods to Detect Opinionated Blogs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Opinion retrieval from blogs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2007</title>
		<meeting>CIKM 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">UIC at TREC 2007 Blog Track</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Topic Categorization for Relevance and Opinion Detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bayrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
