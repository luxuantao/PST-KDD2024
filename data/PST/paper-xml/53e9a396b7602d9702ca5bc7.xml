<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory Bypassing: Not Worth the Effort</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
							<email>gabriel.loh@yale.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Sami</surname></persName>
							<email>rahul.sami@yale.edu</email>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">H</forename><surname>Friendly</surname></persName>
							<email>daniel.friendly@yale.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science Dept. of Computer Science Dept. of Electrical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Memory Bypassing: Not Worth the Effort</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Memory dependence prediction establishes a read after write dependence between a store and a load instruction. If the processor accurately predicts the data dependence between a store and a subsequent load, we can completely bypass memory and forward the data directly from the store's producer to the load's consumer. Our simulation studies show that even in the case of processors with oracle dependence predictors, memory bypassing only provides a 2.3% IPC improvement over dependence prediction alone. Given the small potential gains in the ideal case and the hardware complexity required to implement memory bypassing, we argue that computer microarchitects should focus on memory dependence prediction and ignore memory bypassing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A memory dependence typically presents an obstacle to greater performance. A load instruction must wait for the most recent, earlier store to the same address before issuing. At the same time, we want unrelated loads to issue as soon as their arguments are ready. Memory dependence prediction attempts to determine whether a load instruction should issue immediately after its arguments are ready, or whether the load instruction should wait due to an earlier store to the same address. Issuing the load too early leads to memory ordering violations, and forcing the load to wait may cause false memory dependences.</p><p>Researchers have studied how to accurately predict memory dependences to enable more aggressive out-oforder issue of memory instructions. Chrysos and Emer proposed using Store Sets to predict memory dependences <ref type="bibr" target="#b2">[3]</ref>. They show that an out-of-order processor using store sets achieves performance levels that are very close to an ideal processor that is capable of perfect dependence prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>This research was partially supported by NSF Grant CCR-9702281, NSF Grant CCR-9702980 and by the DoD University Research Initiative (URI) program administered by the Office of Naval Research under Grant N00014-01-1-0795.</p><p>Data that flow through memory typically go through four basic steps. First, an operate instruction (such as an ADD) produces the data value. Next, a store instruction places the data value into a memory location. The reason for storing the value may be for passing arguments to a subroutine, or it may be a spill due to high register pressure. Later, before the data value is needed again, a load instruction retrieves the value from memory and places it in a register. Finally, some consumer instruction uses the value (perhaps as an operand to another arithmetic instruction).</p><p>The question we asked is if the processor can accurately predict a memory dependence between a load and a store, why bother with sending the data values to the memory subsystem and back? By playing some tricks with register renamings, we could potentially make all consumers of the data read the value directly from the original producer instruction. To the extent that the memory dependence predictor can accurately determine the dependences from stores to loads, this bypassing of memory can potentially save many cycles.</p><p>In this paper, we explore the idea of bypassing memory dependences in load-store (RISC) architectures. We evaluate a store sets based version of this bypassing technique, and discover that the performance gains are unimpressive. We also explore the performance limits of memory bypassing by simulating different types of ideal processors. Our overall conclusion is that this kind of memory bypassing results in relatively small performance gains (and would probably be quite complex to implement in hardware as well).</p><p>The rest of this paper is organized as follows. In Section 2, we briefly review how store sets work and explain how to extend this to enable memory bypassing. In Section 3, we present the results of our performance studies to quantify the benefit of memory bypassing. We explain in Section 4 why memory bypassing does not provide any substantial performance gain. In Section 5, we attempt to improve performance by increasing the opportunities for bypassing memory dependences. We briefly review some related work in Section 6, and we conclude the paper with some final remarks in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Idea</head><p>Store sets are an effective technique for memory dependence prediction. Using store sets, a processor dynamically identifies memory dependences between store and load instructions. In this section, we explain how to use this information to completely shortcut or bypass memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Store Sets</head><p>A static load instruction's store set is the set of all store instructions that the load has ever been dependent on. Knowing a load's store set enables accurate memory ordering by preventing the load from issuing when any earlier stores belonging to the load's store set are pending. A processor can dynamically learn the store sets for each load instruction by initially allowing all loads to speculatively issue as soon as the load's arguments are ready. When the processor detects a memory ordering violation, the corresponding store is added to the load's store set.</p><p>The store set implementation presented in <ref type="bibr" target="#b2">[3]</ref> uses a pair of tables to predict memory dependences. The first table learns the actual store sets. The second table, the Last Fetched Store Table (LFST) tracks the most recent store instructions currently in the instruction window that also belong to a store set. Each load instruction checks the LFST to determine if a store instruction currently in the instruction window is in its store set. If such a store exists, then the processor creates a dependence between the two instructions, forcing the load to wait for the store. For the purposes of this paper, it is sufficient to think of the store sets memory dependence predictor as a black box that takes a load instruction as an input, and returns a hardware identifier of an earlier store that the load depends on, or returns a null value if the load may issue as soon as its arguments are ready.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Memory Bypassing</head><p>Data values that are stored to and loaded from memory typically follow a produce-store-load-consume dependence chain. Memory bypassing converts this four instruction chain into a shorter producer-consumer relation.</p><p>An example of the produce-store-load-consume dependence chain is illustrated in Figure <ref type="figure" target="#fig_0">1a</ref>. An ADD instruction ? creates a new data value and stores the result in regis- ter R7, which has been renamed to physical register P18. At some later point in the instruction stream, the compiler decides that register R7 should be used for some other purpose, and uses a store instruction ? to place the result of the ADD into memory. Some number of cycles later, the program needs the result of the ADD again, and uses a load instruction ? to place the data from memory into register R3, which is mapped to physical register P23. Finally, another instruction ? uses the value as an input operand.</p><p>The pair of store and load instructions to temporarily store a data value to memory may take several cycles. Furthermore, either the load or the store may be delayed for some additional cycles due to the fact that their addresses may not be immediately known. On the other hand, if we have an accurate prediction that the load ? depends on the store ? , we know that the value loaded into register P23 is identical to the value produced by the original ADD instruction ? . Whether instruction ? reads its operand from phys- ical register P23 or P18, the final result will be the same. Reading the result directly from P18 avoids the long trip to and from memory.</p><p>The bypassed produce-consume dependence chain is really a dependence graph with a side branch to verify the dependence prediction, as shown in Figure <ref type="figure" target="#fig_0">1b</ref>. As soon as the corresponding data dependences are ready, instructions ? and ? may issue. Some time later, the load and store must both issue and verify that they were indeed referencing the same address and that no other store to the same address occurred between ? and ? . If the memory references were to different locations in memory, then the bypassing would have forwarded the wrong value to instruction ? . At this point, some sort of recovery is necessary to squash or reexecute instruction ? , and its dependent instructions. Further- more, some repair mechanism is needed since instruction ? really should be reading its argument from P23, not P18.</p><p>Intuitively, it would seem that bypassing memory should provide large performance gains. Bypassing removes the latency of going to the store forwarding buffer and back. Furthermore, memory bypassing removes the computation of addresses from the critical path. To the extent that dependence predictions are accurate, address calculations are relegated to non-critical address verifications. Although we think that a hardware mechanism for efficient recovery is possible, we will omit details of an implementation since bypassing ultimately does not gain much in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We experimentally quantify the performance benefits of memory bypassing. In this section, we explain our simulation methodology, our simulated processor model, and present the results of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Methodology</head><p>Our processor simulator is based on the SimpleScalar toolset, version 3.0 for the Alpha instruction set <ref type="bibr" target="#b0">[1]</ref>. Specifically, our simulator is derived from sim-outorder, a cycleaccurate out-of-order processor simulator based on the register update unit (RUU) <ref type="bibr" target="#b14">[15]</ref> which uses a unified reorder  buffer and issue queue. We added support for simulating load speculation, store sets memory dependence prediction, and memory bypassing.</p><formula xml:id="formula_0">? : ADD R7 = R8 + R10 ? : ADD R11 = R3 + R2 ? : ADD R11 = R3 + R2 ? : ST R7 ? 0[R12] ? : LD R3 = 16[R9] ? : LD R3 = 16[R9] ? : ST R7 ? 0[R12] ? : ADD R7 = R8 + R10 (verify dependence) (a) (b) (P18) (P18) (P18) (P23) (P23) (P18) (P18) (P23)</formula><p>We used an aggressive processor model that is comparable to the configuration used in the original store sets study <ref type="bibr" target="#b2">[3]</ref> and the 8-wide configuration from the Stack Value File study <ref type="bibr" target="#b6">[7]</ref>. The processor parameters are listed in Table 1. We use a McFarling style hybrid branch predictor (gshare/PAs) to predict conditional branches <ref type="bibr" target="#b8">[9]</ref>.</p><p>The simulated benchmarks come from the SPEC2000 in-teger codes <ref type="bibr">[16]</ref>. We use a mix of input sets from the test data set and the reduced run-length inputs from the University of Minnesota <ref type="bibr" target="#b5">[6]</ref>. We skipped the initial start-up sections for each benchmark, and then simulated 200 million instructions. The data sets and number of skipped instructions are included in Table <ref type="table" target="#tab_1">2</ref>. The mean IPCs reported are always the harmonic mean across all benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bypassing With Store Sets</head><p>In general, a memory dependence predictor need only predict when a load may safely issue. The store sets predictor also predicts the actual dependence; that is, a store in the window is explicitly predicted as the parent of the load instruction. By tracking this dependence information, we can update the register alias table so all instructions that are data dependent on the load now read their argument directly from the store's parent.</p><p>We illustrate an example in Figure <ref type="figure">2</ref> using the same instructions as shown in Figure <ref type="figure" target="#fig_0">1</ref>. In step (a), the processor renames the original data producing instruction ? to write its result into physical register P18. Step (b) shows the store instruction ? that writes ? 's result to memory. In step (c), the store sets mechanism predicts a memory dependence from ? to the load instruction ? . The register alias table (RAT) is then updated so all consumers of the load's result (logical register R3) will read the value directly from the destination register of the original producer ? . In step (d), the pro- cessor renames a consumer instruction ? 's arguments using the mappings in the RAT. Instruction ? can now receive its argument directly from instruction ? , thereby completely bypassing the memory references of ? and ? . Most of this process is standard in out-of-order superscalar processors. To perform bypassing, the processor only needs to perform the additional work of updating the RAT when the dependence is predicted.</p><p>In a processor that performs memory bypassing, the predicted dependent store and load instructions still must execute. The store instruction must write its value to memory to enforce the proper program semantics. The load instruction must at least perform its address computation to verify that the dependence prediction was actually correct. In the case of a misspeculation, the recovery procedure can be quite involved. The simplest approach is to flush the pipeline, rollback the register alias table, and restart fetching from the mispredicted load instruction. This approach squashes many unrelated instructions that are not data-dependent on the faulting load. Selective reissue only squashes and reissues those instructions that are actually data-dependent on the misspeculated load <ref type="bibr" target="#b11">[12]</ref>. This results in higher amounts of instruction-level parallelism, but may be complex to implement in hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experimental Results</head><p>We conducted simulation-based experiments to assess the performance gains derivable from memory bypassing. The baseline experiment is a processor that performs no memory speculation; loads may only issue when all earlier store addresses have been resolved. We then simulated a processor that speculates on load instructions using the store sets memory dependence predictor and another processor that uses store sets based memory bypassing. In both cases, we also use the set merging optimization described in <ref type="bibr" target="#b2">[3]</ref>, which we found to improve performance slightly. We simulated configurations using both squash recovery and selective reissue. With squash recovery, all instructions after a misspeculated load must reexecute, although they are kept in the window. With selective reissue, only the dependent instructions are forced to reissue.</p><p>The per-benchmark and harmonic mean IPCs are shown in Figure <ref type="figure">3</ref>. Regardless of the misspeculation recovery model, allowing loads to speculatively issue when the store sets mechanism does not predict a memory dependence shows significant performance gains. Under the squash recovery model, the processor augmented with store sets shows a 31.0% increase in the mean IPC. Adding memory bypassing to this processor yields a total mean IPC increase of 31.7% over the non-speculating configuration. Adding the hardware complexity of memory bypassing hardly seems to be worth the effort. Even considering a selective reissue policy, the gains of bypassing with store sets (39.4%) compared to store sets alone (36.6%) are not very impressive.</p><p>One possible explanation for the poor performance of our store sets memory bypassing is that it is possible that the dependence prediction does a good job at preventing memory order violations, but doesn't necessarily predict the true dependences between stores and loads. A load may be dependent on the first of two earlier store instructions. If the store sets mechanism predicted the load to be dependent on the second store, then the load is forced to wait for the second store to issue before it can proceed. If the second store always issues after the first, then the load will never cause a memory ordering violation. On the other hand, bypassing data from the wrong store will always result in a misspeculation.</p><p>To bound how well memory bypassing could possibly perform, we simulated a processor with an oracle memory dependence predictor. These results are also shown in Figure <ref type="figure">3</ref>. Even with perfect information about memory dependences, memory bypassing only improves the mean IPC by 2.3% over perfect dependence prediction alone. It is interesting to note that the store sets based memory bypassing (with selective reissue) achieves an IPC that is within 1.4% of perfect bypassing. This provides evidence that the store sets are accurately predicting dependent stores and loads. This is highly discouraging since the 2.3% improvement is a best case scenario. So why does memory bypassing provide such meager performance gains?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Explanation of Small Gains</head><p>In this section, we analyze the behavior of our benchmarks to explain why memory bypassing does not perform well. Based on the small performance increase in the ideal case of perfect memory bypassing, the natural hypotheses are (1) that there are relatively few opportunities for bypassing, and (2) the bypassed memory references are not on the critical path.</p><p>Subroutine calls present one possible situation that may provide opportunities for bypassing. Passing arguments to a function may require the caller to store the function arguments to the stack, and then subsequently the callee must load these values back into the registers. Typical register usage conventions allocate a few registers for passing function arguments, which avoids the back and forth trip to memory (under Digital Unix conventions, up to six arguments may be passed in R16 through R21 <ref type="bibr" target="#b13">[14]</ref>). Most functions only take a few arguments.</p><p>Due to the fixed number of architecturally visible registers, the compiler may be forced to spill values to memory when there are no more available registers. These spills may represent more opportunities for memory bypassing. The spill is implemented by writing the contents of the spilled register to memory with a store instruction. At a later point in the program when the original value is once again needed, the value is loaded back into a (possibly different) register. If both instructions of this store-load pair are inflight in the processor at the same time, then this presents a memory bypassing opportunity. However, assuming a reasonable register allocator, the spilled register is not likely to be used again very soon.</p><p>We would like to quantify how many possible opportunities exist for bypassing memory. To measure this, we simulated a processor configuration that performs naive speculation. That is, a load issues as soon as its address has been computed, subject to the availability of issue slots. Every time the processor detects a memory ordering violation, a store and a load to the same memory address exist in the instruction window which presents an opportunity for memory bypassing. The results are summarized in Table <ref type="table" target="#tab_1">2</ref>. We report the total number of load instructions, the total number of loads that resulted in a misspeculation, and the misspeculations as a percentage of the total number of loads. These misspeculations represent situations where the load is ready to issue, but it is being held up because the conflicting store is waiting for its data operand or the store has not yet computed its address. If the store is waiting for its data operand, then bypassing acts as a store value forwarding mechanism. If the store has not computed its address, then bypassing may save several cycles since the consumer of the load instruction does not have to wait for the address computation. The benchmark gap showed some of the largest IPC improvements, and our misspeculation counting shows that gap also has the largest number of bypassing opportunities. On the other hand, these statistics suggest that most of the benchmarks do not have very many opportunities for memory bypassing.</p><p>Part of the reason that there are so few bypassing op-  portunities may be due to the fact that the effective window size is too small. Store sets only consider stores and loads that are both in the instruction window at the same time. Although our instruction window has a capacity of 128 instructions, fetch stalls or bursts of high parallelism may cause the actual number of instructions in the window to be less than the maximum. To get a better idea of the characteristics of dependent stores and loads, we measured the distances between stores and loads. For every load, we count the number of instructions to the most recent store to the same address. Figure <ref type="figure" target="#fig_2">4</ref> shows the results of this experiment. The x-axis is the number of instructions that separate a load from the most recent store to the same address. The y-axis shows the cumulative frequency. For example, approximately 20% of all loads in gcc occur within 80 instructions of the previous store to the same address. The vertical line marks the average instruction window size for our processor configuration when no memory speculation is performed (not including instructions following a mispredicted branch).</p><p>The curves in our store-to-load frequency plots vary by benchmark. For some benchmarks, such as mcf, there are very few same address load-store pairs within the effective instruction window. Even extending to hundreds of instructions beyond the instruction window does not greatly increase the number of same address load-store pairs. The benchmark crafty does not exhibit very many same address load-store pairs within the effective instruction window, but the number of such pairs continues to increase as we consider more instructions. Some other benchmarks such as parser and vortex have more load-store pairs within the instruction window than the other benchmarks, but even for these benchmarks the bypassable loads comprise less than 25% of all load instructions. The benefit of bypassing these loads also depends on whether these loads are on the critical path. Effective hardware or software prefetching can potentially remove some of the loads from the critical path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Enlarging the Effective Window</head><p>In the previous section, we observed that there are relatively few dependent store-load pairs within the instruction window. For some of the benchmarks, the number of dependent pairs increases if we consider a larger window of instructions. Perhaps if we enlarge the effective window for detecting dependent stores and loads, we may be able to increase the number of bypassing opportunities and boost overall performance.</p><p>One problem with using store sets is that the dependence between a store and load is broken when the store retires from the instruction window. Normally, the LFST entry corresponding to the store is replaced with a null value to indicate that no store instruction in that store set is currently in the window. An alternative approach would be to store the value written by that store into the LFST entry. A subsequent load instruction that accesses this LFST entry can then directly use this saved valued. This can be thought of as a simple form of load value prediction.</p><p>We used another idealized processor model to determine the potential benefit of such an approach. We simulated a processor with perfect memory bypassing as before. Furthermore, we augment this processor with a FIFO buffer that tracks the last stores to retire. If a dependent store- load pair exist in the window, then the processor uses memory bypassing as described in Section 3.2. If the store had already retired before a load to the same address was fetched, then the processor forwards the value of the store from the FIFO store buffer to the load. Again, the processor has oracle knowledge of the load's address.</p><p>We simulated this processor model for various sizes of the FIFO store buffer. The harmonic mean IPCs are shown in Figure <ref type="figure" target="#fig_3">5</ref>. All IPC values are normalized with respect to the case of perfect speculation with no memory bypassing. Even in this idealized situation, the processor must track the last two thousand store instructions for the bypassing gains to achieve an additional 5% performance increase. Given that a 2K entry fully-associative buffer is probably too large to implement, and that these results rely on perfect memory dependence prediction, we conclude that any practical implementation of memory bypassing is not worth the hardware, the complexity, and the effort.</p><p>Another possibility that we considered was that the processor front-end was the bottleneck. If the fetch bandwidth was increased, perhaps there would be more opportunities for memory bypassing. We do not believe this is the case because the results in Figure <ref type="figure" target="#fig_3">5</ref> effectively model a larger instruction window with respect to increasing the number of bypassing opportunities. We also simulated an unrealistic processor with a 1024-entry instruction window and very large branch predictors. This greatly increased the average number of instructions in the instruction window, but the overall impact of memory bypassing over perfect speculation was only a 3.1% increase in the harmonic mean IPC.</p><p>Out of all of the benchmarks for the 1024-entry window simulations, vpr.route (we will refer to this benchmark as just vpr for the rest of this section) exhibited the highest rate of bypassing loads: approximately one quarter of all loads were bypassed. Still, the IPC increase for vpr was only 3.2%. We profiled vpr to find out what functions comprised most of the dynamic instructions, and then we plotted the store-load dependence graphs with VCG, a graph visualization tool <ref type="bibr" target="#b12">[13]</ref>. The graph for an invocation of the node to heap function is shown in Figure <ref type="figure">6</ref>. We use rectangles for load instructions, diamond shapes for stores, and bypassed loads are indicated by a dependence arc from the parent store.</p><p>The section of the graph annotated with code is actually from the function add to heap, which the compiler chose to inline into node to heap. Calls to node to heap comprise 28.6% of the dynamic instructions in our sample window. The main loop of the function is from the add to heap function, which inserts a new item into a heap data structure. There are very few bypassing opportunities in the main loop because of how a heap insert works.</p><p>A new node is inserted at the bottom of a binary tree, and then "floats" upward so long as the parent node's key is greater than the new node's key. When nodes are swapped, is written into the position of the parent node. On the following cycle, is read out of the heap to compare to the new parent node. This read is the dependent load, shown by the one cycle bypass arcs in Figure <ref type="figure">6</ref> in the section of the graph for add to heap. Even if the processor can bypass all of these loads, the processor is still held up by the loads of the various parent nodes.</p><p>Instructions from the function get heap head comprise 39.9% of all dynamic instructions in vpr for our sample window. Figure <ref type="figure" target="#fig_4">7</ref> shows a section of the program where the function try route (lightly shaded nodes) repeatedly calls get heap head (dark nodes) 1 . From the graph, we can see that some of the bypass paths span ten or more cycles. Unfortunately, there are also a large number of loads that we can not bypass. This suggests that even if we do bypass a load, then the critical path shifts to one of the many 1 The reason get heap head requires so many instructions is that nodes in the heap may be invalid, and so get heap head actually has to find the node with the smallest key that is also valid, rather than simply returning the root node. other loads. In such a situation, bypassing may gain one or two cycles, but not the ten or more that correspond to the length of the bypass path. Another important characteristic of the bypass dependence arcs is that there are many stores that have a high fan-out. That is, a single store is the parent of multiple bypassed loads. For vpr, we found that on average, a store fans-out to 4.55 bypassed loads. To gain more than just a few cycles from bypassing, the processor must correctly bypass all these loads, and then the overall gain may still only be a small number of cycles due to all of the unbypassable loads previously discussed. We also considered processor configurations where store-forwarding (not bypassing) requires multiple cycles.</p><p>Our default configuration has a store-forwarding latency of a single cycle, but in a processor with a longer forwarding latency, the number of cycles saved by bypassing may increase. We simulated one processor with perfect speculation and one with perfect bypassing, and increased the storeforwarding latency to two cycles. The relative performance increase of perfect bypassing over perfect dependence prediction is only 2.4%. We further increased the forwarding latency to four cycles and increased the L1 data cache latency to five cycles, and the relative IPC improvement is still only a meager 3.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>There are two main bodies of research related to our study. The first is in the prediction of memory dependences, and the second is in various forms of memory bypassing. Moshovos et al identified the importance of memory dependence prediction and proposed and evaluated techniques to implement dependence prediction in hardware <ref type="bibr" target="#b9">[10]</ref>. Chrysos and Emer proposed store sets for memory dependence prediction which we used in this study <ref type="bibr" target="#b2">[3]</ref>. The Alpha 21264 speculatively issues loads, but uses a simple table to track loads that cause misspeculations to prevent them from speculatively issuing in the future <ref type="bibr" target="#b4">[5]</ref>.</p><p>Several studies have used memory dependence to speed up loads from memory. Tyson and Austin introduced memory renaming <ref type="bibr" target="#b15">[17]</ref>, which has similarities to memory bypassing. Memory renaming forwards data to the dependents of a load by combining memory dependence prediction with load value prediction <ref type="bibr" target="#b7">[8]</ref>. Jourdan et al explore several renaming-based techniques for move elimination, memory disambiguation and memory bypassing <ref type="bibr" target="#b3">[4]</ref>. Moshovos and Sohi propose memory cloaking and bypassing to reduce the latency of memory instructions <ref type="bibr" target="#b10">[11]</ref>. Calder and Reinmann compare a variety of aggressive load speculation techniques and examine the effects of combining different approaches <ref type="bibr" target="#b1">[2]</ref>. They show that memory dependence prediction and value prediction together provide the greatest performance benefit, but further augmenting their processor with memory renaming does not provide any significant additional benefit. Lee et al's Stack Value File (SVF) <ref type="bibr" target="#b6">[7]</ref> computes the addresses of stack references early in the pipeline and redirects the stack traffic to a separate storage structure. Using register renaming, the SVF achieves effects similar to memory bypassing for stack references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>At face value, the idea of memory bypassing sounds like an excellent hardware optimization to remove long storeto-load dependence chains from the critical path. The main result of this study is that accurate memory dependence prediction can dramatically improve performance, but incorporating memory bypassing provides little additional benefit. Our analysis of memory dependence statistics suggests that this is because there are few opportunities for bypassing, and many of these opportunities are overlapping.</p><p>Our results are based on the Alpha instruction set architecture. However, an architecture with fewer logical registers, such as the x86 ISA, may yield different results. The limited number of registers leads to more frequent spilling and therefore potentially more bypassing opportunities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) A typical dependence chain from a producer instruction, through memory, to the consumer. (b) The dependence chain when memory is bypassed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. Using store sets to bypass memory: (a) the original value producing instruction, (b) the ? stores ? 's result to memory, (c) store sets predicts a dependence from ? to ? and we update the RAT so the load's children read their operand directly from the store's source, (d) the instruction ? is a dependent of the bypassed load ? , but has been renamed to receive its value directly from ? 's destina- tion register P18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The cumulative frequency of the distance between a dependent store and load for each benchmark. The average instruction window occupancy is included below the benchmark name.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Relative performance of perfect bypassing augmented with a buffer of the last retired store instructions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The bypass dependence graph of store and loads for the get heap head function (dark nodes), which is called by try route (light nodes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Parameters for the simulated proces- sor.</head><label>1</label><figDesc></figDesc><table><row><cell>Decode Width</cell><cell>8</cell></row><row><cell>Issue Width</cell><cell>8</cell></row><row><cell>Commit Width</cell><cell>8</cell></row><row><cell>IFQ Size</cell><cell>32</cell></row><row><cell>RUU Size</cell><cell>128</cell></row><row><cell>LSQ Size</cell><cell>64</cell></row><row><cell>IL1 cache</cell><cell>8-way 256KB</cell></row><row><cell>DL1 cache</cell><cell>4-way 64KB</cell></row><row><cell>IL1 hit</cell><cell>1 cycle</cell></row><row><cell>DL1 hit</cell><cell>3 cycles</cell></row><row><cell cols="2">Store Forwarding 1 cycle</cell></row><row><cell>Unified L2</cell><cell>4-way 256KB</cell></row><row><cell>L2 hit</cell><cell>16 cycles</cell></row><row><cell cols="2">Memory Latency 60 cycles</cell></row><row><cell>Int/FP ALU</cell><cell>8</cell></row><row><cell>Int Mult</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Each load-store memory ordering violation is an opportunity for memory bypassing.</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Benchmark</cell><cell cols="2">Input</cell><cell></cell><cell>Instructions</cell><cell cols="2">Number</cell><cell>Number of</cell><cell>Misspeculations</cell></row><row><cell></cell><cell></cell><cell cols="2">Set</cell><cell></cell><cell>Skipped</cell><cell cols="2">of Loads</cell><cell>Misspeculations (as % of all loads)</cell></row><row><cell></cell><cell>bzip2</cell><cell cols="2">test</cell><cell></cell><cell>200M</cell><cell cols="2">45,119,328</cell><cell>812,444</cell><cell>1.80</cell></row><row><cell></cell><cell>crafty</cell><cell cols="2">test</cell><cell></cell><cell>10M</cell><cell cols="2">62,433,982</cell><cell>454,349</cell><cell>0.73</cell></row><row><cell></cell><cell>eon</cell><cell cols="3">test (kajiya)</cell><cell>10M</cell><cell cols="2">60,081,228</cell><cell>3,588,252</cell><cell>5.97</cell></row><row><cell></cell><cell>gap</cell><cell cols="2">test</cell><cell></cell><cell>70M</cell><cell cols="2">62,961,510</cell><cell>7,292,950</cell><cell>11.58</cell></row><row><cell></cell><cell>gcc</cell><cell cols="2">test</cell><cell></cell><cell>200M</cell><cell cols="2">67,887,442</cell><cell>1,141,085</cell><cell>1.68</cell></row><row><cell></cell><cell>gzip.graphic</cell><cell cols="2">UMN</cell><cell></cell><cell>10M</cell><cell cols="2">48,086,571</cell><cell>2,529,345</cell><cell>5.26</cell></row><row><cell></cell><cell>gzip.source</cell><cell cols="2">UMN</cell><cell></cell><cell>10M</cell><cell cols="2">49,299,863</cell><cell>2,490,490</cell><cell>5.06</cell></row><row><cell></cell><cell>mcf</cell><cell cols="2">UMN</cell><cell></cell><cell>100M</cell><cell cols="2">58,855,822</cell><cell>275,950</cell><cell>0.47</cell></row><row><cell></cell><cell>parser</cell><cell cols="2">UMN</cell><cell></cell><cell>270M</cell><cell cols="2">57,772,158</cell><cell>652,342</cell><cell>1.13</cell></row><row><cell></cell><cell>vortex</cell><cell cols="2">test</cell><cell></cell><cell>10M</cell><cell cols="2">55,654,141</cell><cell>2,208,333</cell><cell>3.97</cell></row><row><cell></cell><cell>vpr.place</cell><cell cols="2">test</cell><cell></cell><cell>10M</cell><cell cols="2">56,082,583</cell><cell>2,136,052</cell><cell>3.81</cell></row><row><cell></cell><cell>vpr.route</cell><cell cols="2">test</cell><cell></cell><cell>54M</cell><cell cols="2">66,425,671</cell><cell>1,970,147</cell><cell>2.97</cell></row><row><cell>0.8 0.9</cell><cell>bzip2 85 insts</cell><cell>0.8 0.9</cell><cell></cell><cell cols="2">crafty 58 insts</cell><cell>0.8 0.9</cell><cell></cell><cell>eon 55 insts</cell><cell>0.8 0.9</cell><cell>gap 66 insts</cell></row><row><cell>0.7</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell>0.7</cell></row><row><cell>0.6</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.6</cell></row><row><cell>0.5</cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.5</cell></row><row><cell>0.4</cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.4</cell></row><row><cell>0.3</cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell>0.3</cell></row><row><cell>0.2</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.2</cell></row><row><cell>0.1</cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell>0.1</cell></row><row><cell>0</cell><cell cols="2">128 256 384 512</cell><cell>0</cell><cell cols="3">128 256 384 512</cell><cell>0</cell><cell>128 256 384 512</cell><cell>0</cell><cell>128 256 384 512</cell></row><row><cell>0.8 0.9</cell><cell>gcc 44 insts</cell><cell>0.8 0.9</cell><cell></cell><cell></cell><cell>gzip.graphic 74 insts</cell><cell>0.8 0.9</cell><cell></cell><cell>gzip.source 73 insts</cell><cell>0.8 0.9</cell><cell>mcf 48 insts</cell></row><row><cell>0.7</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell>0.7</cell></row><row><cell>0.6</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.6</cell></row><row><cell>0.5</cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.5</cell></row><row><cell>0.4</cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.4</cell></row><row><cell>0.3</cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell>0.3</cell></row><row><cell>0.2</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.2</cell></row><row><cell>0.1</cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell>0.1</cell></row><row><cell>0</cell><cell cols="2">128 256 384 512</cell><cell>0</cell><cell cols="3">128 256 384 512</cell><cell>0</cell><cell>128 256 384 512</cell><cell>0</cell><cell>128 256 384 512</cell></row><row><cell>0.8 0.9</cell><cell>parser 74 insts</cell><cell>0.8 0.9</cell><cell></cell><cell></cell><cell>vortex 96 insts</cell><cell>0.8 0.9</cell><cell></cell><cell>vpr.place 49 insts</cell><cell>0.8 0.9</cell><cell>vpr.route 87 insts</cell></row><row><cell>0.7</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell>0.7</cell></row><row><cell>0.6</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.6</cell></row><row><cell>0.5</cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.5</cell></row><row><cell>0.4</cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.4</cell></row><row><cell>0.3</cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell>0.3</cell></row><row><cell>0.2</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.2</cell></row><row><cell>0.1</cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell>0.1</cell></row><row><cell>0</cell><cell cols="2">128 256 384 512</cell><cell>0</cell><cell cols="3">128 256 384 512</cell><cell>0</cell><cell>128 256 384 512</cell><cell>0</cell><cell>128 256 384 512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>The bypass dependence graph of node to heap. Rectangles correspond to loads, diamonds correspond to stores, and the edges go from a store to a bypassed load.</head><label></label><figDesc></figDesc><table><row><cell>/* node to heap */</cell></row><row><cell>/* add to heap main loop */</cell></row><row><cell>while ((ito 1)</cell></row><row><cell>&amp;&amp; (heap[ifrom]? cost heap[ito]? cost))</cell></row><row><cell>temp ptr = head[ito];</cell></row><row><cell>heap[ito] = heap[ifrom];</cell></row><row><cell>heap[ifrom] = temp ptr;</cell></row><row><cell>ifrom = ito;</cell></row><row><cell>ito = ifrom/2;</cell></row><row><cell>Figure 6.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The SimpleScalar Tool Set, Version 2.0</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
		<idno>1342</idno>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Comparative Survey of Load Speculation Architectures</title>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Reinmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memory Dependence Prediction Using Store Sets</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Z</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Symposium on Computer Architecture</title>
		<meeting>the 25th International Symposium on Computer Architecture<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="142" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Novel Renaming Scheme to Exploit Value Temporal Locality through Physical Register Reuse and Unification</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bekerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bishara</forename><surname>Shomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Yoaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Symposium on Microarchitecture</title>
		<meeting>the 31st International Symposium on Microarchitecture<address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Alpha 21264 Microprocessor</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro Magazine</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="1999-04">March-April 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adapting the SPEC 2000 Benchmark Suite for Simulation-Based Computer Architecture Research</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kleinosowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Meares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Design, Workshop on Workload Characterization</title>
		<meeting>the International Conference on Computer Design, Workshop on Workload Characterization<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10">October 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stack Value File: Custom Microarchitecture for the Stack</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hsien-Hsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">S</forename><surname>Newburn</surname></persName>
		</author>
		<author>
			<persName><surname>Tyson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Symposium on High Performance Computer Architecture</title>
		<meeting>the 7th International Symposium on High Performance Computer Architecture<address><addrLine>Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Monterrey</publisher>
			<date type="published" when="2001-01">January 2001</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Value Locality and Load Value Prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mikko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Lipasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Paul</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Symposium on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10">October 1996</date>
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining Branch Predictors</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mcfarling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TN</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
		<respStmt>
			<orgName>Compaq Computer Corporation Western Research Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic Speculation and Synchronization of Data Dependences</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Breach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Symposium on Computer Architecture</title>
		<meeting>the 24th International Symposium on Computer Architecture<address><addrLine>Boulder, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="181" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speculative Memory Cloaking and Bypassing</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<date type="published" when="1999-10">October 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Yiannakis Sazeides, and Jim Smith. Trace Processors</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quinn</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Symposium on Microarchitecture</title>
		<meeting>the 30th International Symposium on Microarchitecture<address><addrLine>Research Triangle Park, NC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-12">December 1997</date>
			<biblScope unit="page" from="138" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph Layout Throught the VCG Tool</title>
		<author>
			<persName><forename type="first">Georg</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of theDIMACS International Workshop on Graph Drawing</title>
		<meeting>theDIMACS International Workshop on Graph Drawing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="194" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Alpha AXP Architecture Reference Manual</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sites</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995-10">October 1995</date>
			<pubPlace>Butterworth-Heinemann</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Instruction Issue Logic for High-Performance, Interruptable, Multiple Functional Unit, Pipelined Computers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gurindar</surname></persName>
		</author>
		<author>
			<persName><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Computers</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="359" />
			<date type="published" when="1990-03">March 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving the Accuracy and Performance of Memory Communication Through Renaming</title>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Symposium on Microarchitecture</title>
		<meeting>the 30th International Symposium on Microarchitecture<address><addrLine>Research Triangle Park, NC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="218" to="227" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
