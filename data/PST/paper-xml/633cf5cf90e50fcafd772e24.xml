<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-16">16 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-16">16 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.01296v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrievalaugmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b39">Rae et al., 2021;</ref><ref type="bibr" target="#b15">Hoffmann et al., 2022;</ref><ref type="bibr" target="#b10">Chowdhery et al., 2022)</ref>. For example, in open-domain question answering <ref type="bibr" target="#b7">(Chen et al., 2017)</ref>, demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions <ref type="bibr" target="#b20">(Joshi et al., 2017;</ref><ref type="bibr" target="#b58">Yang et al., 2018;</ref><ref type="bibr" target="#b23">Kwiatkowski et al., 2019)</ref>. Recent research <ref type="bibr" target="#b14">(Guu et al., 2020;</ref><ref type="bibr" target="#b26">Lewis et al., 2020;</ref><ref type="bibr" target="#b18">Izacard et al., 2022)</ref> shows that retrieval-augmentation can further improve LLMs' performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus.</p><p>Figure <ref type="figure">1</ref>: Illustration of evaluating (few-shot) open-domain question answering with (closed-book) direct generation <ref type="bibr" target="#b10">(Chowdhery et al., 2022)</ref>, (open-book) retrieval-augmented generation <ref type="bibr" target="#b18">(Izacard et al., 2022)</ref>, and (closed-book) recitation-augmented generation (ours). This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs.</p><p>The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory <ref type="bibr" target="#b5">(Carlini et al., 2021)</ref>. Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, "what is the tenth decimal of ??". They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read "the tenth decimal of ? is 5". However, there can be some sentences like "the first N digits of ? are 3.14159 26535..." existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: "The first 10 digits of ? are 3.14159 26535. So the answer is 5". Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.</p><p>We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure <ref type="figure">1</ref>. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models <ref type="bibr" target="#b41">(Roberts et al., 2020)</ref>. We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy.</p><p>Experiments on four large language models (PaLM <ref type="bibr" target="#b10">(Chowdhery et al., 2022)</ref>, UL2 <ref type="bibr">(Tay et al., 2022a)</ref>, OPT <ref type="bibr" target="#b61">(Zhang et al., 2022)</ref>), and Codex <ref type="bibr" target="#b8">(Chen et al., 2021)</ref> show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-based single-hop QA (Natural Questions, <ref type="bibr" target="#b23">Kwiatkowski et al. 2019)</ref>, trivia questions <ref type="bibr">(TriviaQA, Joshi et al. 2017)</ref>, and Wikipedia-based multi-hop QA (HotpotQA, <ref type="bibr" target="#b58">Yang et al. 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">OPEN-DOMAIN QUESTION ANSWERING</head><p>Open-domain question answering <ref type="bibr" target="#b36">(Prager et al., 2007)</ref> refers to the task of generating answers for arbitrary context-free questions. In the open-book setting, it is typically assumed that the QA model can find the answer in an external corpus, e.g., Wikipedia <ref type="bibr" target="#b7">(Chen et al., 2017;</ref><ref type="bibr" target="#b17">Izacard &amp; Grave, 2021)</ref> or web pages <ref type="bibr" target="#b25">(Lazaridou et al., 2022)</ref>. This is in analogy as taking an open-book exam where students can search over an external knowledge corpus. The standard pipeline <ref type="bibr" target="#b7">(Chen et al., 2017;</ref><ref type="bibr" target="#b17">Izacard &amp; Grave, 2021;</ref><ref type="bibr">2020)</ref> usually consists of a learnable or non-learnable document retriever module and a learnable neural network-based reader module.</p><p>In the closed-book setting, the QA model is not allowed to access any external knowledge, and needs to store all the knowledge in its parameters. It has been recently observed that large-scale pre-trained language models <ref type="bibr" target="#b13">(Devlin et al., 2019;</ref><ref type="bibr">Radford et al., a;</ref><ref type="bibr">Yang et al., 2019b)</ref> can internalize a sort of implicit "knowledge base" after pre-training <ref type="bibr" target="#b35">(Petroni et al., 2019;</ref><ref type="bibr" target="#b19">Jiang et al., 2020;</ref><ref type="bibr" target="#b46">Talmor et al., 2020)</ref>. <ref type="bibr" target="#b41">Roberts et al. (2020)</ref> show that after fine-tuning on open-book question-answer pairs, T5 <ref type="bibr" target="#b40">(Raffel et al., 2020)</ref> can answer a large portion of knowledge-intensive questions. This is similar as taking a closed-book exam. However, <ref type="bibr" target="#b27">Lewis et al. (2021)</ref> found that the high performance is mainly due to training set question memorization. <ref type="bibr" target="#b51">Wang et al. (2021)</ref> also found that it is still challenging for relatively small-scale pre-trained language models like RoBERTa <ref type="bibr" target="#b31">(Liu et al., 2019)</ref> or GPT-2 <ref type="bibr">(Radford et al., b)</ref> to answer closed-book questions.</p><p>In this work, we focus on evaluating the CBQA performance of large language models (LLMs) in the few-shot setting, which ideally minimizes the bias of train-test overlapping <ref type="bibr" target="#b30">(Liu et al., 2021)</ref>. We propose a recite-and-answer scheme, which is similar to a student first recite the factoid knowledge about the question, and then answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">IN-CONTEXT FEW-SHOT LEARNING</head><p>Large language models (LLMs) such as <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> have the surprising ability to do in-context learning, where the model learns to do new tasks simply by being prompted a few exemplars. The LLMs learn from these exemplars without being explicitly pre-trained for in-context learning and without any gradient updates or fine-tuning. Recent study showed that such ability improves with the scaling of both model size <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b39">Rae et al., 2021;</ref><ref type="bibr" target="#b10">Chowdhery et al., 2022)</ref> and number of tokens for training <ref type="bibr" target="#b15">(Hoffmann et al., 2022)</ref>. When evaluated on knowledgeintensive question answering tasks, these models are usually evaluated in the closed-book setting, where the factoid knowledge are completely stored in the model parameters of dense LLMs.</p><p>Recently, Atlas <ref type="bibr" target="#b18">(Izacard et al., 2022)</ref> shows that for knowledge-intensive NLP tasks, a relatively lite model with retrieval augmentations can achieve similar or even better performance through fewshot fine-tuning, which proves that memorization can be decoupled from generalization in LLMs. In contrast, we show that still a large underestimated amount of knowledge can be retrieved from LLMs' model weights through better-designed prompting. <ref type="bibr" target="#b28">Ling et al. (2017)</ref> pioneer the work of solving math word problems by generating step-by-step human-readable solutions described by natural language and math equations before the final answer. That is fundamentally different from other works which directly generate the final answers or use formal languages. e.g. equations only, to illustrate the intermediate solving steps <ref type="bibr" target="#b43">(Roy et al., 2016;</ref><ref type="bibr" target="#b0">Amini et al., 2019;</ref><ref type="bibr" target="#b9">Chen et al., 2019)</ref>. <ref type="bibr" target="#b12">Cobbe et al. (2021)</ref> extend <ref type="bibr" target="#b28">(Ling et al., 2017)</ref> by constructing a much larger dataset to finetune a pre-trained large language model to solve math word problems and a parameterized ranker is trained to rank candidate solutions to improve the solving rate. <ref type="bibr" target="#b56">Wei et al. (2022)</ref> propose chain-of-thought prompting which combines the idea of natural language rationales <ref type="bibr" target="#b28">(Ling et al., 2017;</ref><ref type="bibr" target="#b12">Cobbe et al., 2021)</ref> with few-shot prompting <ref type="bibr" target="#b3">(Brown et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">RATIONALE-AUGMENTED REASONING</head><p>In this work, instead of generating a chain of thought for multi-step reasoning questions, we decompose the process of answering a knowledge-intensive question into two steps: recite the relevant knowledge stored in the model parameters, and then answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">MEMORIZATION IN LARGE LANGUAGE MODELS</head><p>Recent study shows that large language models can memorize its training data, and generate texts from training data given certain prompts <ref type="bibr" target="#b5">(Carlini et al., 2021;</ref><ref type="bibr">2022;</ref><ref type="bibr" target="#b60">Zhang et al., 2021;</ref><ref type="bibr" target="#b21">Kharitonov et al., 2021;</ref><ref type="bibr" target="#b49">Thakkar et al., 2020;</ref><ref type="bibr" target="#b4">Carlini et al., 2019;</ref><ref type="bibr" target="#b50">Tirumala et al., 2022)</ref>. Most related to our work, <ref type="bibr" target="#b6">Carlini et al. (2022)</ref> found that the memorization ability of LLMs significantly grows as the model capacity increases, the number of times an example has been duplicated, and the number of tokens of context used to prompt the model. While these works mainly analyze the fundamental properties of memorization in the exact setting, where exactly N tokens are used as the prompt to reproduce the suffix of the prompt, our work relies on "fuzzy memorizaiton", where the prompts tend to not be exactly the same as the training data, but still improve the memorization accuracy.</p><p>The proposed recitation-augmented generation idea is also related to the line of work on utilizing Transformer memory as an information retrieval model <ref type="bibr">(Tay et al., 2022b)</ref> and self-talk models for commonsense reasoning <ref type="bibr" target="#b45">(Shwartz et al., 2020;</ref><ref type="bibr" target="#b29">Liu et al., 2022)</ref>. <ref type="bibr" target="#b64">Zhuang et al. (2022)</ref>; <ref type="bibr">Wang et al. (2022c)</ref>; <ref type="bibr" target="#b63">Zhou et al. (2022)</ref> proposed to augment documents at indexing time with a number of generated queries. <ref type="bibr" target="#b2">Bevilacqua et al. (2022)</ref> proposed to directly generate n-grams grounded in one or multiple documents with constrained decoding. The Oberoi family is part of a hotel company that has a head office in what city?</p><p>Recitation 1: The two major holding companies of The Oberoi Group are EIH Ltd and EIH Associated Hotels (formerly East India Hotels). The Oberoi family is the majority shareholder in EIH Ltd with a 32.11% stake.</p><p>Recitation 2: The Oberoi Group is an award-winning luxury hotel group with its head office in New Delhi, India.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;End of Recitation&gt;</head><p>The answer is Delhi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) multiple-recite-and-answer</head><p>Figure <ref type="figure" target="#fig_2">2</ref>: Illustration of prompt-based in-context learning for recitation generation, recitationaugmented question answering, self-consistency ensembling, and multiple-recite-and-answer for multi-hop questions (Sec. 3.1). In multiple-recite-and-answer scheme, the latter recitaiton can utilize the information from the previous ones, such as "Oberoi Group" in this case. The prompts for self-consistency and multi-hop recite-and-answer are dropped for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING TO RECITE FOR CLOSED-BOOK QUESTION ANSWERING</head><p>The goal of this paper is to mimic a human's ability to recite relevant factoid knowledge <ref type="bibr">(Mc-Daniel et al., 2009)</ref> before answering knowledge-intensive questions, such that these questions can be answered more accurately. In the following we describe our recite-and-answer scheme for fewshot closed-book question answering (CBQA), which consists of two components: (1) a evidencerecitation module for reciting relevant passages, and (2) a question-answering module for generating answers given the recited evidence. Notice that in this paper, we focus on few-shot setting, in which we assume only a few question-answer demonstrations are provided. In Natural Questions <ref type="bibr" target="#b23">(Kwiatkowski et al., 2019)</ref> benchmark, since the questions are from queries issued to the Google search engine by multiple users, and thus can be regarded as unannotated data, we further assume that we have top-retrieved Wikipedia pages for these questions. The paragraphs in these top-retrieved Wikipedia pages will be used to generate synthetic paired question-recitation data for fine-tuning the LM (described in Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PROMPT-BASED RECITE-AND-ANSWER FOR QUESTION-ANSWERING</head><p>Recitation-augmented question answering We start with single-hop question answering <ref type="bibr" target="#b23">(Kwiatkowski et al., 2019;</ref><ref type="bibr" target="#b20">Joshi et al., 2017)</ref>, where the answers are usually supported by a specific document in the corpus, which is sometimes referred as evidence <ref type="bibr" target="#b20">(Joshi et al., 2017)</ref>. Different from chain-of-thought prompting <ref type="bibr" target="#b56">(Wei et al., 2022)</ref> where a rationale is directly generated to explain the generated answer <ref type="bibr" target="#b20">(Joshi et al., 2017;</ref><ref type="bibr" target="#b33">Narang et al., 2020;</ref><ref type="bibr" target="#b24">Lampinen et al., 2022)</ref>, we propose to first recite a passage about the question, and then answer the question based on the recitation.</p><p>We propose a prompt-based learning-to-recite scheme by leveraging the LLM's in-context learning ability <ref type="bibr" target="#b3">(Brown et al., 2020)</ref>. We prompt the LLM with paired exemplars of questions and recited evidences, and the LLM can learn in an in-context manner to generate a recitation for an arbitrary question. To perform recitation-conditioned few-shot question answering, we append the recited passages at the beginning of the original question-answer exemplars as a single prompt, and then generate the final answer (Step 1 &amp; 2 in Figure <ref type="figure" target="#fig_2">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-consistency ensemble</head><p>The factual knowledge about a question can appear in several places in the language model's training corpora. For example, the fact of "Queen Elizabeth II opened the London Bridge on 17 March 1973" can appear in both Wikipedia page "London Bridge" and page "March 1973", so it is highly likely that there exists knowledge from different articles that could lead to the same, correct answer. With this motivation, we argue that similar to multi-step reasoning in chain-of-thought, recitation-augmented question answering can also benefit from the self-consistency technique with multiple-path decoding <ref type="bibr">(Wang et al., 2022b)</ref>. Specifically, given an arbitrary question, we first use top-k sampling to independently generate a few recitations, and then greedy decode the answer of the question based on the sampled recitations. Finally, we determine the optimal answer by taking a plurality/majority vote (Step 3 in Figure <ref type="figure" target="#fig_2">2</ref>).</p><p>Multiple-recite-and-answer for multi-hop question-answering Multi-hop question answering requires the QA system to find and reason over multiple supporting documents. However, the nature of recitation restricts us to recite passages from one article at a time. In order to apply the recite-andanswer scheme to solve multi-hop questions, we introduce multiple-recite-and-answer scheme (Step 4 in Figure <ref type="figure" target="#fig_2">2</ref>), that is, given the multiple-hop question, we use the prompt words such as "Recitation 1" and "Recitation 2" to elicit the LLM to generate recitation passages on different topics. Since the multiple recited passages are generated in one-pass from the LLM decoding sequentially, the generation of later passages can effectively utilize the information both in the original question and the previous recited ones. Our multiple-recite-and-answer scheme for multi-hop question-answering is also compatible with the self-consistency technique, by applying top-k sampling when generating multiple recitations and performing majority voting for the final answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PASSAGE HINT-BASED DIVERSIFIED RECITATION WITH FINE-TUNING</head><p>Passage hint-based diversified recitation While the sampling-based recitation and selfconsistency improves the robustness of recite-and-answer method, one argument for its inefficiency is that if the evidence-recitation module samples the wrong facts about the question, the questionanswering module will not be able to figure it out and tend to generate the wrong answer. Therefore, on the one hand, we need to use a low sampling temperature to avoid generating recitations with wrong facts, on the other hand, we want to make sure the sampled recitations have enough diversity.</p><p>To tackle such a dilemma, we propose passage hint-based diversified recitation. We observe that in well-formed text knowledge bases, such as Wikipedia, we can usually find a unique passage hint for each passage, by concatenating the section titles and the in-section order of each passage. For example, the passage hint of the second passage in Section 5.2 "Enforcement" of Wikipedia page "Child support" would be "Child support -Compliance and enforcement issues -Enforcement -Paragraph #2". In passage hint-based diversified recitation, we first use sampling to generate a diverse set of passage hints, and then use greedy decoding to ensure the factual accuracy of the contents in each passage.</p><p>Since each passage hint corresponds to a unique passage, we can first de-duplicate the passage hints and then generate the full passages to get more diverse recitation passages. Furthermore, as the recited passages are less likely to be similar due to unique passage hints, inspired by recent progress on question-answering with multiple retrieved passages <ref type="bibr" target="#b17">(Izacard &amp; Grave, 2021)</ref>, we use aggregated diverse recitations as a single context, and generate the answer with a few more question-answer pair demonstrations. Figure <ref type="figure">3</ref> (lower) illustrates the recite-and-answer scheme with passage hint-based diversified recitation.</p><p>Fine-tuning on few-shot generated questions We found that although the training data of many existing LLMs <ref type="bibr" target="#b13">(Devlin et al., 2019;</ref><ref type="bibr" target="#b10">Chowdhery et al., 2022)</ref> contains the Wikipedia corpora, which are usually regarded as the factoid documents for knowledge-intensive question answering tasks <ref type="bibr" target="#b20">(Joshi et al., 2017;</ref><ref type="bibr" target="#b23">Kwiatkowski et al., 2019)</ref>, the section titles are usually not explicitly included in training. This makes the pre-trained LLM hard to discover the mapping from the question to the passage hint, and to the full passage merely by few-shot prompting.</p><p>To address this issue, we propose an additional fine-tuning stage to adapt LLMs to learn such mappings. Assuming we have access to not only a few question-answer pairs, but also the top-retrieved Wikipedia pages for queries issued to the Google search engine by multiple users <ref type="bibr" target="#b23">(Kwiatkowski et al., 2019)</ref>, we can use few-shot prompting to generated synthetic question-hint-passage pairs and then finetune the LLMs on the generated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we report empirical evaluations of our proposed RECITE with recite-and-answer schemes on a diverse set of few-shot closed-book question answering tasks and different language models with varying scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">EVALUATION DATASETS</head><p>Natural Questions Natural Questions <ref type="bibr" target="#b23">(Kwiatkowski et al., 2019)</ref> consists of questions aggregated from the Google search engine and the answers from the Wikipedia page in the top 5 search results. We treat it as a single-hop question answering task. Since Natural Questions contains the so-called "long answer" annotations, which is a whole HTML bounding box containing enough information to infer the answer, we directly utilize the "long answer" as the ground-truth recitation exemplars in our prompt (Sec. 3.1). In order to make a direct comparison with recent LLMs <ref type="bibr" target="#b10">(Chowdhery et al., 2022;</ref><ref type="bibr" target="#b18">Izacard et al., 2022)</ref>, we evaluate our methods in 5-shot and 64-shot settings.</p><p>TriviaQA TriviaQA dataset <ref type="bibr" target="#b20">(Joshi et al., 2017)</ref> is constructed by collecting Trivia enthusiast authored question-answer pairs and their retrospectively collected evidence. Since there is no obvious way to collect a "long answer" in the retrospective evidence documents (the exact appearances of the answer may contain enough information to infer the answer), we evaluate TriviaQA in the singlehop 5-shot setting, and manually compose the recitation passage from Wikipedia for 5 randomly sampled training questions. The concrete prompt can be found in the appendix.</p><p>HotpotQA HotpotQA <ref type="bibr" target="#b58">(Yang et al., 2018)</ref> is designed to explicitly test QA systems' ability to perform multi-hop reasoning. It is collected by explicitly composing questions requiring reasoning about multiple supporting context documents. Following <ref type="bibr">Wang et al. (2022a)</ref>, we evaluate Hot-potQA as a multi-hop question answering task in the 4-shot setting. But instead of chain-of-thought prompting as in <ref type="bibr">(Wang et al., 2022a)</ref>, we use multiple-recite-and-answer (Sec. 3.1) to achieve multistep reasoning. We also provide the concrete prompt in the appendix.</p><p>Metrics We calculate the Exact Matching (EM) and F1 scores for the normalized answers, while the specific text normalization applied on each dataset can be slightly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">PRE-TRAINED LANGUAGE MODELS</head><p>We evaluate the effectiveness of RECITE on four langauge models: PaLM, UL2 <ref type="bibr">(Tay et al., 2022a)</ref>, OPT <ref type="bibr" target="#b61">(Zhang et al., 2022)</ref>, and Codex <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b34">Ouyang et al., 2022;</ref><ref type="bibr" target="#b8">Chen et al., 2021)</ref>. Due to the space limit, the detailed descriptions of them are provided in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTS</head><p>We use the test split for all tasks if the test split is available and has labels for evaluation, otherwise we use the dev split. In addition, TriviaQA and HotpotQA are too large to run large language models on, so we used the first 1,024 data points for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">PROMPT-BASED RESULTS</head><p>We report the single-hop closed-book question answering (CBQA) evaluation results on Natural Questions (NQ) and TriviaQA and the multi-hop CBQA evaluation results on HotpotQA. In Tab. 1, we report the results with prompt-based in-context learning and self-consistency.</p><p>From the tables, we can see that the proposed recite-and-answer scheme can significantly improve the CBQA performance on both datasets with various pre-trained language models. While the performance improvements on NQ is more consistent across different language models, we find that the improvements from recite-and-answer is more significant on smaller language models on TriviaQA.</p><p>Our hypothesis is that the Trivia-style question usually contains more contextual information in the question, thus weakened the effectiveness of recitation for strong LLMs like PaLM.</p><p>Besides, we can see that the recite-and-answer scheme can outperform the chain-of-thought prompting performance on the multi-hop reasoning task. Interestingly, we also find that for LLMs that have large gains from chain-of-thought (i.e., PaLM), they also have large improvements from recite-andanswer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">RESULTS OF PASSAGE HINT-BASED DIVERSIFIED RECITATION</head><p>For Natural Questions dataset, since it has the collection of top-retrieved Wikipeida pages corresponding to the unannotated queries issued to the Google search engine, we additionally report the diversified recitation results of fine-tuned PaLM model in Tab. 2. From the table, we find that diversified recitation can further improve the performance of PaLM on the NQ dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">ON THE NUMBER OF SELF-CONSISTENCY PATHS</head><p>We analyze how the number of passages recited would affect the performance of recite-and-answer under the self-consistency setting. Due to the costly inference of LLMs, we first sample up to k = 20 recitation passages, and then apply self-consistency to a randomly selected subset of recitations to simulate less paths. For each number of self-consistency paths, we evaluate the randomly selected subsets five times and report the mean and standard deviation. We conduct an analysis on OPT-30B and UL2-20B on the TriviaQA dataset and report the results in Fig. <ref type="figure">4</ref>. We can see that sampling more recitation passages tends to improve the recite-and-answer performance, while less randomness is observed with more self-consistency paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">ON THE ROBUSTNESS OF FEW-SHOT EXEMPLARS</head><p>A well-known problem of in-context few-shot learning is its instability to the choices of exemplars and their orders <ref type="bibr" target="#b62">(Zhao et al., 2021)</ref>. We evaluate the robustness of standard prompting and our reciteand-answer method with 5 random seeds and report the mean and standard deviation of UL2 model running on the TriviaQA dataset in Tab. 6. The 5-shot exemplars are randomly sampled and shuffled for each seed. From the table, we can see that with recitation sampling, recite-and-answer exhibits similar robustness (in terms of small performance deviation) as standard prompting under different random seeds and numbers of self-consistency paths. The overall gains by recite-and-answer are significant compared to standard prompting regardless of the choice of few-shot exemplars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">RECITATION V.S. RETRIEVAL V.S. GROUND-TRUTH</head><p>One may ask without the external corpus, whether the quality of recited passages with LLMs is better than simple retrieval models, e.g., BM25 <ref type="bibr" target="#b42">(Robertson et al., 2009)</ref> <ref type="foot" target="#foot_1">1</ref> . To answer this question, we evaluate the few-shot question-answering performance of UL2 and Codex on three kinds of context passages: retrieval, recitation, and ground-truth. We report the results on first 1024 validation examples in Natural Questions (NQ) dataset, since it is the only dataset that contains the "long answer" annotation that can be regarded as ground-truth context passage. From Tab. 3, we can see that the classic retrieval model, i.e., BM25, is still a very strong baseline for collecting information from the corpus. Nonetheless, compared to BM25, our recite-and-answer still achieves a quite competitive performance via generation only and without using any external corpus. Besides, we find that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">ERROR ANALYSIS</head><p>We perform an error analysis on the 1024 evaluation examples in the TriviaQA dataset. We classify the errors into three categories: 1) Not Recit., i.e., the correct answer is not recited in any of the 20 recited passages in self-consistency. 2) Hits@20-Recit., i.e., the correct answer can be found in one of the recited passage, but does not appear in the QA module's outputs. 3) Hits@20-Path, i.e., the correct answer is one of the final outputs of the 20 self-consistency paths, but it does not have the majority votes. The correct final answer is marked as Hits@Majority (i.e., Exact Matching). An algorithmic description is given in Algo. 1. We report the results of UL2-20B and OPT-30B in Tab. 4. We can see that "No Recit" and "Hits@20-Path" account for the majority of the errors, meaning that the QA module performs quite well (if the correct answer appears in one of the recitation passages, it will be extracted by the QA module in most of the cases), and the main bottleneck still lies in the recitation quality and answer aggregation strategies.</p><p>We also perform a per-path error analysis, i.e., how many questions can be answered correctly (or not) when the recitation exactly contains (or not) the answer tokens. The results are shown in Tab. 5. We can see that around 7% ? 10% questions have the correct recitation but cannot produce the correct answer, while around 12% questions do not have the correction recitation but can be answered correctly anyway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION &amp; DISCUSSION</head><p>In this paper, we propose a novel recitation-augmented generation framework to improve language models' performance in the closed-book question-answering setting. We hypothesize that for knowledge-intensive NLP tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory. In addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question. We show promising results over three large language models and across three different closed-book QA datasets, demonstrating the effectiveness of our proposed recite-and-answer approach.</p><p>One limitation of our method is that updating time-sensitive knowledge for a pure LLM-based method requires training or fine-tuning the LLMs on the new corpus, which can be costly. For future work, we plan to further validate the effectiveness of recitation-augmented generation for other knowledge-intensive NLP tasks in the closed-book setting, such as fact checking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank the support and feedback of many people from Google Brain team and the constructive suggestions from the anonymous reviewers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>The goal of this paper is to use recitation as an intermediate step to generate more accurate factual knowledge in the model's outputs. Therefore, our method should in principle improve the faithfulness of the LLM systems. However, unlike retrieval-augmented generation (RAG) models that can use external trustworthy corpus, all the intermediate steps in RECITE is generated by the LLM itself, RECITE may further enhance the existing biases in the LLMs' model weights compared to RAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>Model weights The model weights of two LLMs used in our experiments, i.e., UL2-20B <ref type="bibr">(Tay et al., 2022a)</ref> and OPT-30B <ref type="bibr" target="#b61">(Zhang et al., 2022)</ref>, are publicly released through GCP bucket (gs: //scenic-bucket/ul2) and Github (https://github.com/facebookresearch/ metaseq), respectively. The Codex (code-davinci-002) model is publicly available through API calls (https://beta.openai.com/examples/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation datasets</head><p>The three evaluation datasets used in our experiments (Natural Questions 2 , TriviaQA 3 , and HotpotQA 4 ) are all publicly accessible.</p><p>Prompts We provide all the used prompts in the appendix.</p><p>Source code Though the prompt examples in the appendix should be enough to reproduce all the results in our paper, we open-source all the evaluation code at https://github.com/ Edward-Sun/RECITE.</p><p>A ILLUSTRATIONS OF PROMPTS AND LANGUAGE MODEL OUTPUTS Fig. <ref type="figure">7</ref>-15 illustrate the evidence-recitation, question-answering prompt that we used for Natural Questions, TriviaQA, and HotpotQA dataset. We also provide the example sampled recitations for these datasets. Notice that for Natural Questions, we use the "long answer" annotation as the recitation in the prompt, while for the other two datasets, we manually compose a few recitation passages based on web search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PRINCIPLES OF PROMPT DESIGNS</head><p>We mainly follow <ref type="bibr" target="#b10">Chowdhery et al. (2022)</ref> and use two new line symbols "\n\n" as the separator between different components within exemplars, and use three new line symbol "\n\n\n" as the separator between different exemplars.</p><p>For the UL2 <ref type="bibr">(Tay et al., 2022a)</ref> model, since its original SentencePiece <ref type="bibr" target="#b22">(Kudo &amp; Richardson, 2018)</ref> vocabulary does not encode the new line symbol "\n", we instead use " ; " to replace "\n" as the separator in all the prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DETAILS OF PASSAGE HINT-BASED FINE-TUNING</head><p>For Natural Questions <ref type="bibr" target="#b23">(Kwiatkowski et al., 2019)</ref> dataset, we assume that we have top-retrieved Wikipedia pages for the unannotated queries issued to the Google search engine by multiple user.</p><p>We collect the passages in these pages as a corpus, and use the rule to annotate the hints of these passages.</p><p>To make a fair comparison with prompting-based models in both 5-shot and 64-shot, we only use 5 paired "long answer"-question exemplars as the prompt to generate the synthetic question for the sampled passages from the Wikipedia hint-passage corpus, and thus construct the synthetic questionhint-passage paired fine-tuning data.</p><p>We train PaLM in the constructed corpus for 10,000 steps with a batch size of 64, which takes approximately 1 day in 64 TPUv4 chips<ref type="foot" target="#foot_3">5</ref> . The fine-tuned model can be used for passage hintdiversified recitation without any further prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PRE-TRAINED LANGUAGE MODELS</head><p>PaLM PaLM is a family of densely activated decoder trained on the language modeling objective. It has strong capabilities in in-context few-shot learning, multilingual, as well as reasoning tasks. In this paper, we use the PaLM model with 62B parameters.</p><p>UL2 UL2 (Unifying Language Learning, <ref type="bibr">Tay et al. 2022a</ref>) is an encoder-decoder model trained on a mixture of denoising tasks in a unified framework. In this paper, since we mainly focus on the in-context learning ability of language models, we use UL2-20B in the S-Denoiser mode (i.e., pre-trained with the prefix language modeling)<ref type="foot" target="#foot_4">6</ref> .</p><p>OPT OPT (Open Pre-trained Transformer language model, <ref type="bibr" target="#b61">Zhang et al. 2022</ref>) is a family of recently released open-source densely activated language model that aims to re-reproduce comparable results as GPT-3 <ref type="bibr" target="#b3">(Brown et al., 2020)</ref>. We use the 30B one<ref type="foot" target="#foot_5">7</ref> in this paper.</p><p>Codex Codex <ref type="bibr" target="#b34">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b8">Chen et al., 2021)</ref> is a variant of GPT-3 model <ref type="bibr" target="#b3">(Brown et al., 2020)</ref> that can understand code. We use the public OpenAI API<ref type="foot" target="#foot_6">8</ref> to access the conditional generation outputs of the code-davinvi-002 model in this paper.  Algorithm 1 Per-question Error Analysis Require: N ground-truth answer labels {A i } N i=1 for a single question Require: recitations {R i } K i=1 and answer predictions</p><formula xml:id="formula_0">{P i } K i=1 from K paths normalized answers ? {normalize(A i )} N i=1 if normalize(majority vote({P i } K i=1</formula><p>)) ? normalized answers then return "Hits@Majority" else if Any({normalize(P i ) ? normalized answers} K i=1 ) then return "Hits@20 -Path." else if Any({Any({normalized answers j ? normalize(R i )} K i=1 )} N j=1 ) then return "Hits@20 -Recit." else return "NoRecit." end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ANALYSIS OF THE NUMBER OF EXAMPLES IN FEW-SHOT LEARNING</head><p>We analyze the influence of the number of shots on Natural Questions dataset for standard-prompting model and our recite-and-answer model in Fig. <ref type="figure" target="#fig_3">5</ref>. We can see that recite-and-answer prompting achieves consistent improvement over standard prompting, while the largest improvement is achieved in the 1-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ANALYSIS OF MODEL SIZES ON INSTRUCTION-FINETUNED LANGUAGE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODELS</head><p>Instruction-finetuned language models <ref type="bibr" target="#b44">(Sanh et al., 2021;</ref><ref type="bibr" target="#b1">Bach et al., 2022;</ref><ref type="bibr" target="#b55">Wei et al., 2021;</ref><ref type="bibr">Chung et al., 2022)</ref> trained on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. They tend to show better zero-shot or fewshot performance under the same model sizes compared to their vanilla LM pre-trained counterparts. We analyze the performance on one representative instruction-finetuned language model, FLAN-T5 <ref type="bibr">(Chung et al., 2022)</ref>, on TriviaQA dataset for standard-prompting model and our recite-and-answer model in Fig. <ref type="figure" target="#fig_4">6</ref>. We can see that recite-and-answer prompting achieves consistent improvement over standard prompting, while the largest improvement is achieved in the largest "xxl (11B)" setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G LANGUAGE MODEL HYPERPARAMETERS</head><p>We report the hyperparameters of the LLMs we used in Tab. 7.</p><p>Table <ref type="table">7</ref>: The model hyper-parameters of the large language models used in our experiments. Note that for Codex (i.e., code-davinci-002), the details (including size) are unknown, so we report the hyperparameters of GPT-3 as a common educational guess. Question: where did united states drop the atomic bomb</p><p>The answer to the above question can be found in the following Wikipedia page, section, and paragraph or table : 

Answer: During the final stage of World War II, the United States detonated two nuclear weapons over the Japanese cities of Hiroshima and Nagasaki on August 6 and 9, 1945, respectively. The United States dropped the bombs after obtaining the consent of the United Kingdom, as required by the Quebec Agreement. The two bombings killed at least 129,000 people, most of whom were civilians. They remain the only use of nuclear weapons in the history of warfare.</p><p>Question: who sells the most oil in the world</p><p>The answer to the above question can be found in the following Wikipedia page, section, and paragraph or table : 

Answer: The top three producers have in recent history been (alphabetically) Russia, Saudi Arabia, and the United States. Each of these countries experienced major production declines at different times in the past, but since 2014 all three have been producing near their peak rates of 9 to 11 million barrels per day. Saudi Arabia and Russia also top the list of oil exporting countries, while Russia is not a member of OPEC. The monthly U.S. oil production reached 10.07 million b/d in November 2017, the highest monthly level of crude oil production in U.S. history. In early 2018, the U.S. government forecast the country will become a net oil and gas exporter by 2022, the first time since 1953.</p><p>Figure <ref type="figure">7</ref>: Two sampled evidence-recitation exemplars in Natural Questions (NQ) dataset. Notice that in NQ, we can directly use the "long answer" annotation as the recitation demonstrations.  Question: Triggered by Rosa Parks' refusal to give up her seat, the public transportation system in what US city was devastated by a year long boycott of their busses?</p><p>The answer to the above question can be found in the following Wikipedia page, section, and paragraph:</p><p>Answer: On December 1, 1955, in Montgomery, Alabama, Parks rejected bus driver James F. Blake's order to vacate a row of four seats in the "colored" section in favor of a White passenger, once the "White" section was filled.</p><p>Question: What was Beethoven's last symphony?</p><p>The answer to the above question can be found in the following Wikipedia page, section, and paragraph:</p><p>Answer: The year 1823 saw the completion of three notable works, all of which had occupied Beethoven for some years, namely the Missa solemnis, the Ninth Symphony and the Diabelli Variations.</p><p>Question: In which 1972 John Boorman film is a leading character, played by Ned Beatty, raped by a 'Hillbilly'?</p><p>The answer to the above question can be found in the following Wikipedia page, section, and paragraph:</p><p>Answer: In 1972, Beatty made his film debut as Bobby Trippe in Deliverance, starring Jon Voight and Burt Reynolds, and set in northern Georgia. Beatty's character is forced to strip at gunpoint by two mountain men who humiliate and rape him, a scene so shocking that it is still referred to as a screen milestone.</p><p>Question: Which bridge crossing The River Thames did Queen Elizabeth II open on 17th March 1973?</p><p>The answer to the above question can be found in the following Wikipedia page, section, and paragraph:</p><p>Answer: Queen Elizabeth II of the United Kingdom opens the new London Bridge.</p><p>Question: "The song ""My Kind Of Town"", written by Sammy Cahn and Jimmy Van Heusen in 1964, was about which city?"</p><p>The answer to the above question can be found in the following Wikipedia page, section, and paragraph:</p><p>Answer: "My Kind of Town" made a minor appearance on the U.S. pop charts, reaching #110 in 1964. It was the second of two charting songs about Chicago recorded by Sinatra. The other was "Chicago (That Toddlin' Town)" from 1957, which reached U.S. #84.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 9:</head><p>The 5-shot prompt we used for performing evidence-recitation on TriviaQA dataset.</p><p>Question: Which physicist's principle asserts that the momentum &amp; position of a particle cannot both be precisely determined at the same time?</p><p>Recitation: In 1935, Heisenberg (along with his assistants Walther Gerlach and Walther von Braun) published a paper in which they described a "principle of uncertainty" that would later be dubbed the Heisenberg uncertainty principle. The principle asserts that the momentum and position of a particle cannot both be precisely determined.</p><p>Recitation: One of the consequences of the principle of special relativity is that the position and momentum of a moving object cannot both be measured simultaneously with high precision.</p><p>Recitation: The uncertainty principle states that it is impossible to know both the exact momentum and the exact position of an object (particle) at the same time, that is, the momentum and position are incompatible and, therefore, complementary. The uncertainty principle is one of the fundamental results of quantum mechanics, and is often regarded as one of its most surprising consequences.</p><p>Recitation: It states that the following two statements cannot both be true: the position of a particle cannot be both precisely determined and known exactly, and the momentum of a particle cannot be both precisely measured and known exactly. Question: Which magazine was started first Arthur's Magazine or First for Women?</p><p>The answer to the above question can be found in the following two Wikipedia page, section, and paragraphs:</p><p>Answer 1: Arthur magazine was a bi-monthly periodical that was founded in October 2002, by publisher Laris Kreslins and editor Jay Babcock.</p><p>Answer 2: First for Women is a woman's magazine published by Bauer Media Group in the USA. The magazine was started in 1989.</p><p>Question: The Oberoi family is part of a hotel company that has a head office in what city?</p><p>The answer to the above question can be found in the following two Wikipedia page, section, and paragraphs:</p><p>Answer 1: P.R.S. Oberoi is the current chairman of The Oberoi Group.</p><p>Answer 2: The Oberoi Group is an award-winning luxury hotel group with its head office in New Delhi, India.</p><p>Question: What nationality was James Henry Miller's wife?</p><p>The answer to the above question can be found in the following two Wikipedia page, section, and paragraphs:</p><p>Answer 1: In 1967, Miller married his fifth wife, Japanese born singer Hoki Tokuda.</p><p>Answer 2: Hoki Tokuda is an Japanese actress, known for Blind Woman's Curse (1970), The Abalone Girls (1965) and Nippon Paradise <ref type="bibr">(1964)</ref>.</p><p>Question: The Dutch-Belgian television series that "House of Anubis" was based on first aired in what year?</p><p>The answer to the above question can be found in the following two Wikipedia page, section, and paragraphs:</p><p>Answer 1: House of Anubis is a mystery television series developed for Nickelodeon based on the Dutch-Belgian television series Het Huis Anubis.  Q: Which magazine was started first Arthur's Magazine or First for Women?</p><p>A: The answer is Arthur's Magazine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q:</head><p>The Oberoi family is part of a hotel company that has a head office in what city?</p><p>A: The answer is Delhi.</p><p>Q: What nationality was James Henry Miller's wife?</p><p>A: The answer is American.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q:</head><p>The Dutch-Belgian television series that "House of Anubis" was based on first aired in what year?</p><p>A: The answer is 2006.   A: The Oberoi family is part of the hotel company called The Oberoi Group. The Oberoi Group has its head office in Delhi. The answer is Delhi.</p><p>Q: What nationality was James Henry Miller's wife?</p><p>A: James Henry Miller's wife is June Miller. June Miller is an American. The answer is American.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q:</head><p>The Dutch-Belgian television series that "House of Anubis" was based on first aired in what year?</p><p>A: "House of Anubis" is based on the Dutch-Belgian television series Het Huis Anubis. Het Huis Anubis is firstaired in September 2006. The answer is 2006.</p><p>Figure <ref type="figure" target="#fig_3">15</ref>: The 4-shot prompt we used for the Chain-of-Thought <ref type="bibr" target="#b56">(Wei et al., 2022)</ref> baseline on HotpotQA dataset. The prompt is taken from <ref type="bibr">(Wang et al., 2022a)</ref>.</p><p>Question: Were Scott Derrickson and Ed Wood of the same nationality?</p><p>Ground-truth Answer: Yes ------------------------------------------------------------------Answer (standard prompting): American ------------------------------------------------------------------Answer (chain-of-thought path-1): Scott Derrickson is an American. Ed Wood is also an American. The answer is both are American.</p><p>Answer (chain-of-thought path-2): Scott Derrickson is an American. Ed Wood is an American. So Scott Derrickson and Ed Wood are of the same nationality. The answer is yes.</p><p>Answer (chain-of-thought path-3): Scott Derrickson and Ed Wood were of the same nationality. Both Scott Derrickson and Ed Wood were American. The answer is Yes.</p><p>------------------------------------------------------------------Answer (recite-and-answer path-1): Answer 1: Scott Derrickson is an American screenwriter, director and producer. Answer 2: Edward D. Wood Jr. <ref type="bibr">(October 10, 1924</ref><ref type="bibr">-December 10, 1978)</ref> was an American filmmaker, actor, and author.</p><p>The answer is American.</p><p>Answer (recite-and-answer path-2): Answer 1: Scott Derrickson is an American film director, screenwriter and producer. Answer 2: Edward Davis Wood, Jr. <ref type="bibr">(October 10, 1924</ref><ref type="bibr">-December 10, 1978)</ref> was an American filmmaker, actor, writer, producer, and director.</p><p>The answer is yes.</p><p>Answer (recite-and-answer path-3): Answer 1: Scott Derrickson (born July 16, 1966) is an American film director, screenwriter and producer. Answer 2: Edward Davis Wood Jr. <ref type="bibr">(October 10, 1924</ref><ref type="bibr">-December 10, 1978)</ref> was an American filmmaker, actor, writer and producer.</p><p>The answer is yes.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3: Illustration of question answering with diverse recitation and the corresponding few-shot question generation and fine-tuning processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 2 :</head><label>2</label><figDesc>Figure 4: TriviaQA EM/F1 on OPT-30B and UL2-20B with different # of self-consistency paths.</figDesc><graphic url="image-1.png" coords="8,137.70,213.50,138.60,96.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Nature Questions EM/F1 on UL2-20B with different # of shots.</figDesc><graphic url="image-3.png" coords="16,117.90,81.86,178.20,122.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: TriviaQA EM/F1 on 5-shot FLAN-T5 with different model sizes.</figDesc><graphic url="image-5.png" coords="16,117.90,249.11,178.20,120.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Four recitation passages sampled from UL2 for the same example question from Natural Questions dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Four recitation passages sampled from UL2 for the same example question from Trivi-aQA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Answer 2 :</head><label>2</label><figDesc>Het Huis Anubis (English: The House of Anubis) is a Dutch-Belgian children's television mystery drama. It first aired in September 2006.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The 4-shot prompt we used for performing multiple-evidence recitation on HotpotQA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: The 4-shot prompt we used for performing question-answering on HotpotQA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Four recitation sampled from UL2 for the same example question from HotpotQA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :</head><label>16</label><figDesc>Figure16: Qualitative comparisons between standard prompting, chain-of-thought, and recite-andanswer on HotpotQA evaluation example (I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Qualitative comparisons between standard prompting, chain-of-thought, and recite-andanswer on HotpotQA evaluation example (II).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on Natural Questions (NQ), TriviaQA, and HotpotQA. The number of shots for each task are mentioned in parenthesis.</figDesc><table><row><cell></cell><cell></cell><cell>PaLM-62B</cell><cell>UL2-20B</cell><cell>OPT-30B</cell><cell>Codex-002</cell></row><row><cell></cell><cell></cell><cell>EM / F1</cell><cell>EM / F1</cell><cell>EM / F1</cell><cell>EM / F1</cell></row><row><cell>NQ</cell><cell>Standard-prompting (direct) Recite-and-answer (20-path)</cell><cell cols="3">25.76 / 36.47(5) 10.16 / 20.17(5) 14.97 / 22.93(5) 31.45 / 44.75(5) 28.98 / 40.13(64) 12.70 / 21.97(16) 28.70 / 39.76(5) 14.16 / 23.13(5) 17.84 / 26.74 (5) 35.84 / 49.12(5) 31.34 / 42.48(64) 14.94 / 24.29(16)</cell></row><row><cell>TriviaQA</cell><cell cols="4">Standard-prompting (direct) 65.38 / 71.85(5) 48.73 / 54.32(5) 45.90 / 50.68(5) 81.84 / 86.09(5) Recite-and-answer (20-path) 65.84 / 72.10(5) 53.42 / 58.69(5) 49.02 / 54.22(5) 83.50 / 88.03(5)</cell></row><row><cell></cell><cell cols="4">Standard-prompting (direct) 20.51 / 28.90(4) 16.99 / 24.99(4) 16.70 / 25.21(4) 28.32 / 39.03(4)</cell></row><row><cell>HotpotQA</cell><cell cols="4">Chain-of-thought (20-path) 23.73 / 32.80(4) 17.68 / 24.87(4) 16.89 / 24.03(4) 34.38 / 45.50(4)</cell></row><row><cell></cell><cell cols="4">Recite-and-answer (20-path) 26.46 / 35.67(4) 19.04 / 27.32(4) 17.77 / 26.58(4) 37.11 / 48.37(4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Natural Questions (NQ) results with different context passages.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>UL2-20B (5)</cell><cell>Codex-002 (5)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EM / F1</cell><cell>EM / F1</cell></row><row><cell></cell><cell>No passage</cell><cell></cell><cell cols="2">10.16 / 20.17 31.45 / 44.75</cell></row><row><cell></cell><cell cols="2">Ground-truth passage</cell><cell cols="2">41.02 / 55.73 49.32 / 64.32</cell></row><row><cell></cell><cell cols="2">BM25-Retrieval (Top-1)</cell><cell cols="2">16.31 / 27.66 33.20 / 47.45</cell></row><row><cell></cell><cell cols="4">LM-Recitation (5) (20-path) 14.16 / 23.13 35.84 / 49.12</cell></row><row><cell cols="3">Table 4: Per-question error analysis on TriviaQA.</cell><cell cols="3">Table 5: Per-path error analysis on TriviaQA.</cell></row><row><cell></cell><cell cols="2">UL2-20B (5) OPT-30B (5)</cell><cell cols="3">Recit. Ans. UL2-20B (5) OPT-30B (5)</cell></row><row><cell>Hits@Majority</cell><cell>53.42%</cell><cell>49.02%</cell><cell></cell><cell>33.60%</cell><cell>30.06%</cell></row><row><cell>Not Recit.</cell><cell>21.09%</cell><cell>22.27%</cell><cell></cell><cell>7.87%</cell><cell>9.79%</cell></row><row><cell>Hits@20-Recit.</cell><cell>5.66%</cell><cell>8.01%</cell><cell></cell><cell>12.10%</cell><cell>12.57%</cell></row><row><cell>Hits@20-Path</cell><cell>19.82%</cell><cell>20.07%</cell><cell></cell><cell>46.44%</cell><cell>47.58%</cell></row><row><cell cols="6">stronger models (i.e., Codex) tend to benefit more from the the model's own recitation than BM25</cell></row><row><cell>retrieved context.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6 :</head><label>6</label><figDesc>Robustness evaluation of UL2 on TriviaQA with different few-shot exemplars over 5 random seeds.</figDesc><table><row><cell></cell><cell>EM(5)</cell><cell>F1(5)</cell></row><row><cell>Standard (direct)</cell><cell cols="2">48.42 (? 0.71) 53.85 (? 0.57)</cell></row><row><cell>RECITE (5-path)</cell><cell cols="2">49.75 (? 0.50) 54.78 (? 0.46)</cell></row><row><cell cols="3">RECITE (20-path) 52.68 (? 0.62) 58.05 (? 0.58)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Question: when did day light savings start in the us Recitation: In the UK, the first Daylight Saving Time was first proposed by William Willett, an English builder and Liberal Party politician, in 1907, and after a public consultation his proposal was accepted in 1908. The first Daylight Saving in the UK was introduced on 25 March 1916, and the second on 27 March the following year. It was repealed in May 1919, but reintroduced in May 1919. Recitation: Daylight saving time, also known as daylight savings, and several other variants, is the practice of advancing clocks during summer months by one hour so that evening daylight lasts one more hour. Usually, clocks are adjusted forward one hour near the start of spring and are adjusted backward in autumn. Historically, the time change was intended to conserve energy by making better use of the available daylight at the equinox.Recitation: Daylight savings time in the United States is the practice of advancing clocks during summer months so that evening daylight lasts longer, while sacrificing normal sunrise times. The practice allows people to have more evening daylight and less morning daylight, which allows for more productivity and leisure.Recitation: The United States began Daylight Saving Time in the year 1918, putting in effect a nationwide summer time arrangement, which had been put forward by the National Defense Act of 1916. Originally, Daylight Saving Time was started in the United States to save energy and reduce the cost of living. Currently, many countries have discontinued the practice, but some, such as the United States, observe it on a regional basis..</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Question: Are Local H and For Against both from the United States? For Against is an indie rock band from New York City, New York.</figDesc><table><row><cell>Answer 1: Local H is an American band from San Diego, California.</cell></row><row><cell>Answer 2: For Against is an American punk band from Los Angeles, California.</cell></row><row><cell>Answer 1: Local H is a rock band from Los Angeles, California.</cell></row><row><cell>Answer 2: For Against is an American rock band from Phoenix, Arizona.</cell></row><row><cell>Answer 1: Local H is an American rock band from Austin, Texas.</cell></row><row><cell>Answer 2: For Against is a rock band from Nashville, Tennessee.</cell></row><row><cell>Answer 1: Local H is an American rock band from Chicago, Illinois.</cell></row><row><cell>Answer 2:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Specifically, we use the ground-truth evidence and question pairs as the prompt, and generate new questions with in-context learning for randomly sampled passages from Wikipedia pages. Next, based on the few-shot generated questions, we train the LLM to predict the original passage hint, as well as the passage content. Figure3(upper) illustrates the whole process of passage hint finetuning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>We use the pre-indexed "enwiki-paragraphs" corpus in the pyserini package (https://github.com/ castorini/pyserini), which is originally designed for BERTserini(Yang et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>2019a).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://cloud.google.com/tpu/docs/v4-users-guide</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>This can be achieved by append the "[NLG]" and "[extra id 0]" token to the beginning and the end of the prompt.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/facebookresearch/metaseq</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>https://openai.com/api/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards interpretable math word problem solving with operation-based formalisms</title>
		<author>
			<persName><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><surname>Mathqa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2357" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Promptsource: An integrated development environment and repository for natural language prompts</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><surname>F?vry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Autoregressive search engines: Generating substrings as document identifiers</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Ottaviano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10628</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<ptr target="https://github.com/google-research-datasets/natural-questions3https://nlp.cs.washington.edu/triviaqa/4https://github.com/hotpotqa/hotpot" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The secret sharer: Evaluating and testing unintended memorization in neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?lfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jernej</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th USENIX Security Symposium (USENIX Security 19)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="267" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting training data from large language models</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulfar</forename><surname>Erlingsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th USENIX Security Symposium (USENIX Security 21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2633" to="2650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Quantifying memorization across neural language models</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07646</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension</title>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingwei</forename><surname>Chang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling knowledge from reader to retriever for question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?douard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Main Volume</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Few-shot learning with retrieval augmented language models</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03299</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How can we know what language models know</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02782</idno>
		<title level="m">How bpe affects memorization in transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Andrew K Lampinen</surname></persName>
		</author>
		<author>
			<persName><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kory</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Henry</forename><surname>Matthewson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02329</idno>
		<title level="m">Can language models learn from explanations in context? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Internetaugmented language models through few-shot prompting for open-domain question answering</title>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Grigorev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05115</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Question and answer test-train overlap in open-domain question answering datasets</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Main Volume</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1000" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generated knowledge prompting for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3154" to="3169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Challenges in generalization in open domain question answering</title>
		<author>
			<persName><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01156</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The read-recite-review study strategy: Effective and portable</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><forename type="middle">O</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><surname>Einstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="516" to="522" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karishma</forename><surname>Malkan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14546</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Wt5?! training text-to-text models to explain their predictions</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Open-domain question-answering</title>
		<author>
			<persName><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="231" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Equation parsing: Mapping sentences to grounded equations</title>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1088" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised commonsense question answering with self-talk</title>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4615" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">olmpics-on what language model pre-training captures</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="743" to="758" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unifying language learning paradigms</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Vinh Q Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.05131</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Vinh Q Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06991</idno>
		<title level="m">Transformer memory as a differentiable search index</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Om</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Ramaswamy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07490</idno>
		<title level="m">Rajiv Mathews, and Franc ?oise Beaufays. Understanding unintended memorization in federated learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><forename type="middle">H</forename><surname>Markosyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10770</idno>
		<title level="m">Memorization without overfitting: Analyzing the training dynamics of large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Can generative pre-trained language models serve as knowledge bases for closed-book qa?</title>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3241" to="3251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.00747</idno>
		<title level="m">Rationaleaugmented ensembles in language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengmin</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoshuai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02743</idno>
		<title level="m">A neural corpus indexer for document retrieval</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">End-to-end open-domain question answering with bertserini</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aileen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
	<note>Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12938</idno>
		<title level="m">Counterfactual memorization in neural language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peitian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.09257</idno>
		<title level="m">Ultron: An ultimate retriever on corpus with a model-based indexer</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Bridging the gap between indexing and retrieval for differentiable search index with query generation</title>
		<author>
			<persName><forename type="first">Shengyao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houxing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10128</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
