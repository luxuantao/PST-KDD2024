<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEA: Graph Shell Attention in Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christian</forename><forename type="middle">M M</forename><surname>Frey</surname></persName>
							<email>christian.frey@lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Informatics Oettingenstr</orgName>
								<address>
									<postCode>67 80538</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunpu</forename><surname>Ma</surname></persName>
							<email>ma@dbs.ifi.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Informatics Oettingenstr</orgName>
								<address>
									<postCode>67 80538</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Schubert</surname></persName>
							<email>schubert@dbs.ifi.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Informatics Oettingenstr</orgName>
								<address>
									<postCode>67 80538</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SEA: Graph Shell Attention in Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A common issue in Graph Neural Networks (GNNs) is known as over-smoothing. By increasing the number of iterations within the message-passing of GNNs, the nodes' representations of the input graph align with each other and become indiscernible. Recently, it has been shown that increasing a model's complexity by integrating an attention mechanism yields more expressive architectures. This is majorly contributed to steering the nodes' representations only towards nodes that are more informative than others. Transformer models in combination with GNNs result in architectures including Graph Transformer Layers (GTL), where layers are entirely based on the attention operation. However, the calculation of a node's representation is still restricted to the computational working flow of a GNN. In our work, we relax the GNN architecture by means of implementing a routing heuristic. Specifically, the nodes' representations are routed to dedicated experts. Each expert calculates the representations according to their respective GNN workflow. The definitions of distinguishable GNNs result from k-localized views starting from the central node. We call this procedure Graph Shell Attention (SEA), where experts process different subgraphs in a transformer-motivated fashion. Intuitively, by increasing the number of experts, the models gain in expressiveness such that a node's representation is solely based on nodes that are located within the receptive field of an expert. We evaluate our architecture on various benchmark datasets showing competitive results compared to state-ofthe-art models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The modeling flow of Graph Neural Networks (GNNs) has been proven to be a convenient tool in a variety of realworld applications building on top of graph data <ref type="bibr">(Wu et al. 2021</ref>). These range from predictions in social networks over property predictions in molecular graph structures to content recommendations in online platforms. From a machine learning perspective, we can categorize them into various theoretical problems that are known as node classification, graph classification/regression -encompassing binary decisions or modeling a continuous-valued function -, and relation prediction. In our work, we propose a novel framework and show its applicability on graph-level classification and regression, as well as on node-level classification tasks.</p><p>The high-level intuition behind GNNs is that by increasing the number of iterations k = 1, . . . , K, a node's repre-sentation processes, contains, and therefore relies more and more on its k-hop neighborhood. However, a well-known issue with the vanilla GNN architecture refers to a problem called over-smoothing <ref type="bibr" target="#b5">(Hoory, Linial, and Wigderson 2006;</ref><ref type="bibr" target="#b20">Xu et al. 2018a</ref>). Suppose we are given a GNNmotivated model, the information flow between two nodes u, v ∈ V, where V denotes a set of nodes, is proportional to the reachability of node v on a k-step random walk starting from u. Hence, by increasing the layers within the GNN architecture, the information flow of every node approaches the stationary distribution of random walks over the graph. As a consequence, the localized information flow is getting lost. On graph data that follow strong connectivity, it takes k = O(log|V|) steps for a random walk starting from an arbitrary node to converge to an (almost) uniform distribution. Consequently, increasing the number of iterations within the GNN message-passing results in representations for all the nodes in the input graph that align and become indiscernible.</p><p>One strategy for increasing a GNN's expressiveness is by adding an attention mechanism into the architecture. Recently, it has been shown how the Transformer model <ref type="bibr" target="#b18">(Vaswani et al. 2017</ref>) can be applied on graph data <ref type="bibr">(Dwivedi and Bresson 2021)</ref> yielding competitive results to state-ofthe-art models. Generally, multi-headed attention shows vying results whenever we have prior knowledge to indicate that some neighbors might be more informative than others.</p><p>Our framework further improves the representational capacity by adding an expert motivated heuristic into the GNN architecture. More specifically, to compute a node's representation, a routing module first decides upon an expert that is responsible for a node's computation. The experts differ in how their k-hop localized neighborhood is processed and capture various depths of GNNs. We refer to the different substructures that individual experts process by Graph Shells. As each expert attends to a specific subgraph of the input graph, we introduce the concept of Graph Shell Attention (SEA). Hence, whereas a vanilla GNN lacks in over-smoothing the representations for all nodes, we introduce additional degrees of freedom in our architecture to simultaneously capture short-and long-term dependencies being processed by respective experts.</p><p>In summary, our contributions are as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In recent years, the AI community proposed various forms of (self-)attention in numerous domains. Attention itself refers to a mechanism in neural networks where a model learns to make predictions by selectively attending to a given set of data. The success of these models dates back to Vaswani et al. <ref type="bibr" target="#b18">(Vaswani et al. 2017</ref>) by introducing the Transformer model. It relies on scaled dot-product attention, i.e., given a query matrix Q, a key matrix K, and a value matrix V , the output is a weighted sum of the value vectors, where the dotproduct of the query with corresponding keys determines the weight that is assigned to each value. Transformer architectures have also been successfully applied to graph data. A thorough work by <ref type="bibr">Dwivedi et al. (Dwivedi and Bresson 2021)</ref> evaluates transformer-based GNNs. They conclude that the attention mechanism in Transformers applied on graph data should only aggregate the information from the local neighborhood, ensuring graph sparsity. As in Natural Language Processing (NLP), where a positional encoding is applied, they propose to use Laplacian eigenvectors as the positional encodings for further improvements. In their results, they outperform baseline GNNs on the graph representation task. A similar work <ref type="bibr" target="#b11">(Kreuzer et al. 2021)</ref> proposes a full Laplacian spectrum to learn the position of each node within a graph. <ref type="bibr" target="#b23">Yun et al. (Yun et al. 2019)</ref> proposed Graph Transformer Networks (GTN) that is capable of learning on heterogeneous graphs. The target is to transform a given heterogeneous input graph into a metapath-based graph and apply a convolution operation afterward. Hence, the focus of their attention framework is on interpreting generated meta-paths. Another transformer-based architecture that has been introduced by Hu et al. <ref type="bibr" target="#b7">(Hu et al. 2020b</ref>) is Heterogeneous Graph Transformer (HGT). Notably, their architecture can capture graph dynamics w.r.t the information flow in heterogeneous graphs. Specifically, they take the relative temporal positional encoding into account based on differences of temporal information given for the central node and the message-passing nodes. By including the temporal information, Zhou et al. <ref type="bibr" target="#b24">(Zhou et al. 2020</ref>) built a transformer-based generative model for generating temporal graphs by directly learning from the dynamic information in networks. The work of Ngyuen et al. <ref type="bibr" target="#b12">(Nguyen, Nguyen, and Phung 2019)</ref> proposes another idea for positional encoding. The authors of this work introduced a graph transformer for arbitrary homogeneous graphs with a coordinate embedding-based positional encoding scheme.</p><p>In <ref type="bibr" target="#b22">(Ying et al. 2021)</ref>, the authors introduced a transformer motivated architecture where various encodings are aggregated to compute the hidden representations. They propose graph structural encodings subsuming a spatial encoding, an edge encoding, and a centrality encoding.</p><p>Furthermore, a work exploring the effectiveness of largescale pre-trained GNN models is proposed by the GROVER model <ref type="bibr" target="#b14">(Rong et al. 2020)</ref>. The authors include an additional GNN applied in the attention sublayer to produce vectors for Q, K, and V . Moreover, they apply single long-range residual connections and two branches of feedforward networks to produce node and edge representations separately. In a self-supervised fashion, they first pre-train their model on 10 million unlabeled molecules before using the resulting node representations in downstream tasks.</p><p>Typically, all the models are built in a way such that the same parameters are used for all inputs. To gain more expressiveness, the motivation of the mixture of experts (MoE) heuristic <ref type="bibr" target="#b17">(Shazeer et al. 2017</ref>) is to apply different parameters w.r.t to the input data. Recently, Google proposed Switch Transformer (Fedus, Zoph, and Shazeer 2021), enabling training above a trillion parameter networks but keeping the computational cost in the inference step constant. In our work, we will show how we can apply MoE motivated heuristics in the scope of GNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>In this section, we provide definitions and recap on the general message-passing paradigm combined with Graph Transformer Layers (GTLs) <ref type="bibr">(Dwivedi and Bresson 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><p>Let G = (V, E) be an undirected graph where V denotes a set of nodes and E denotes a set of edges connecting nodes. We define N k (u) to be the k-hop neighborhood of a node u ∈ V, i.e., N k (u) = {v ∈ V : d G (u, v) ≤ k}, where d G (u, v) denotes the hop-distance between u and v on G. For N 1 (u) we will simply write N (u) and omit the index k. The induced subgraph by including the k-hop neighbors starting from node u is denoted by G k u . Moreover, in the following we will use a real-valued representation vector h u ∈ R d for a node u, where d denotes the embedding dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Neural Networks</head><p>Given a graph G = (V, E) with node attributes X V = {X u |u ∈ V} and edge attributes X E = {X uv |(u, v) ∈ E}, a GNN aims to learn an embedding vector h u for each node u ∈ V, and a vector h G for the entire graph G. For an L-layer GNN, a neighborhood aggregation scheme is performed to capture the L-hop information surrounding each node. The l-th layer of a GNN is formalized as follows:</p><formula xml:id="formula_0">h l+1 u = UPDATE l (h l u , m l N (u) )<label>(1)</label></formula><formula xml:id="formula_1">m l N (u) = AGGREGATE l ({(h l v ) : v ∈ N (u)}),<label>(2)</label></formula><p>where N (u) is the 1-hop neighborhood set of u, h</p><p>u denotes the representation of node u at the l-th layer, and h (0) u is initialized as the node attribute X u . Since h u summarizes the information of central node u, it is also referred as patch embedding in the literature. A graph's embedding h G is derived by a permutation-invariant readout function:</p><formula xml:id="formula_3">h G = READOUT({h u |u ∈ V})<label>(3)</label></formula><p>A common heuristic for the readout function is to choose a function READOUT(•) ∈ {mean(•), sum(•), max(•)}. The input H is projected by three matrices</p><formula xml:id="formula_4">W Q ∈ R d×d Q , W K ∈ R d×d K , and W V ∈ R d×d V to get the corresponding representation Q, K, V . The self-attention is calculated as: Q = HW Q K = HW K V = HW V , A = QK T √ d k , Attn(H) = softmax(A)V<label>(4)</label></formula><p>Notably, in NLP as well as in computer vision tasks, the usage of transformer models were boosters behind a large number of state-of-the-art systems. Recently, the Transformer architecture has also been modified to be applicable to graph data <ref type="bibr">(Dwivedi and Bresson 2021)</ref>, which we will briefly recap in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Transformer Layer</head><p>Generally, in NLP, the words in an input sequence can be represented as a fully connected graph that includes all connections between the input words. However, such an architecture does not leverage graph connectivity. Therefore, it can perform poorly whenever the graph topology is important and has not been encoded in the input data in any other way. These thoughts lead to the work <ref type="bibr">(Dwivedi and Bresson 2021)</ref>, where a general Graph Transformer Layer (GTL) has been introduced.</p><p>For the sake of completeness, we will recap on the most important equations. A layer update for layer l within a GTL including edge features is defined as:</p><formula xml:id="formula_5">ĥl+1 u = O l h H k=1 ( v∈Nu w k,l uv V k,l h l v ),<label>(5)</label></formula><formula xml:id="formula_6">êl+1 uv = O l e H k=1</formula><p>( ŵk,l uv ), where, (6)</p><formula xml:id="formula_7">w k,l uv = softmax v ( ŵk,l uv ),<label>(7)</label></formula><p>ŵk,l uv = (</p><formula xml:id="formula_8">Q k,l h l u • K k,l h l v √ d k ) • E k,l e l uv ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_9">Q k,l , K k,l , V k,l , E k,l ∈ R d k ×d , and O l h , O l e ∈ R d×d . The operator</formula><p>denotes the concatenation of attention heads k = 1, . . . , H. Subsequently, the outputs ĥl+1 u and êl+1</p><p>uv are passed to feedforward networks and succeeded by residual connections and normalization layers as in the vanilla Transformer architecture <ref type="bibr" target="#b18">(Vaswani et al. 2017)</ref>. The full architecture of GTLs is illustrated in figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Positional Encoding. Using the Laplacian eigenvectors as the positional encoding for nodes within a graph is a common heuristic <ref type="bibr">(Dwivedi and Bresson 2021)</ref>. From spectral analysis, the eigenvectors are computed by the factorization of the Laplacian matrix:</p><formula xml:id="formula_10">∆ = I − D −1/2 AD −1/2 = U T ΛU,<label>(9)</label></formula><p>where A denotes the n × n adjacency matrix, D denotes the degree matrix. The decomposition yields Λ, U corresponding to the eigenvalues and eigenvectors. For the positional encoding, we use the k smallest eigenvectors of a node. In our models, we also apply the Laplacian Positional Encoding (LPE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we introduce our Graph Shell Attention (SEA) architecture for graph data. SEA builds on top of the message-passing paradigm of Graph Neural Networks (GNNs) and integrates an expert heuristic.</p><p>Over-smoothing in GNNs is a well-known issue <ref type="bibr" target="#b20">(Xu et al. 2018a</ref>) and exacerbates the problem when we build deeper GNN models which generate similar representations for all nodes in the graph. Applying the same number of iterations for each node inhibits the expressiveness of short-and longterm dependencies simultaneously. Therefore, our architecture loosens up the strict workflow that is applied for each node equally. We gain expressiveness by routing each node representation towards dedicated experts, which only process nodes in their k-localized receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Shells</head><p>In our approach, we exploit the graph transformer architecture, including Graph Transformer Layers (GTLs) <ref type="bibr">(Dwivedi and Bresson 2021)</ref> and extend it by a set of experts. A routing layer decides upon which expert is most relevant for the computation of a node's representation. Intuitively, the expert's computation for a node representation differs in how k-hop neighbors are stored and processed within GTLs.</p><p>Generally, starting from a central node, Graph Shells refer to subgraphs that include only nodes that have at maximum a k-hop distance (k-neighborhood). More formally, the i-th expert comprises the information given in the i-th neighborhood</p><formula xml:id="formula_11">N i (u) = {v ∈ V : d G (u, v) ≤ i)}</formula><p>, where u ∈ V denotes the central node. We refer to the subgraph G i u as the expert's receptive field. Notably, increasing the number of iterations within GNNs correlate with the number of experts being used.</p><p>In the following, we introduce three variants on how experts process graph shells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEA-GNN</head><p>For the definition of the first graph shell model, we exploit the GNN architecture. Hence, the shells described by each expert are given by construction. From the formal definition expressed in eqs. 1 and 2, the information for the l-th expert is defined by the l-th iteration of the GNN. For N experts, we set the maximal number of hops to be L = N . Figure <ref type="figure" target="#fig_1">2a</ref> illustrates this graph shell model. From left to right, the information of nodes being reachable by more hops is processed. Experts storing information after few hops refer to short-term dependencies, whereas experts processing more hops yields information of long-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEA-AGGREGATED</head><p>For the computation of the hidden representation h l+1 u for node u on layer l + 1, the second model employs an aggregated value from the previous iteration. According to the message-passing paradigm of a vanilla GNN, the aggregation function of eq. 2 considers the 1-hop neighbors N 1 (u). For SEA-AGGREGATED, we send the aggregated value m N1 (u) l to all its 1-hop neighbors. For a node v ∈ N 1 (u), the values received by v are processed according to another aggregation function which can be chosen among AGGREGATE µ ∈ {mean(•), sum(•), max(•)}. Formally:</p><formula xml:id="formula_12">h l+1 u = AGGREGATE l µ ((m l N (v) ) : v ∈ N (u)})<label>(10)</label></formula><p>Figure <ref type="figure" target="#fig_1">2b</ref> illustrates this graph shell model. In the first iteration, there are no proceeding layers. Hence, the first expert processes in the same way as the first model. The representations for the second expert are computed by taking the aggregated values of the first shell into account. Sending values to neighboring nodes is illustrated by a full-colored shell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEA-K-HOP</head><p>For this model we relax the AGGREGATE function defined in eq. 2. Given a graph G, we also consider k-hop linkages in the graph connecting a node u with all entities having a maximum distance of d G (u, v) = k. The relaxations of eqs. 1 and 2 can then be written as:</p><formula xml:id="formula_13">h l+1 u = UPDATE l (h l u , m l N k (u) ) (11) m l N k (u) = AGGREGATE l ({(h l v ) : v ∈ N k (u)}), ,<label>(12)</label></formula><p>where N k (u) denotes the k-hop neighborhood set. This approach allows for processing each N 1 (u), . . . , N k (u) with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEA: Shell Attention</head><p>By endowing our models with experts referring to various graph shells, we gain expressiveness. A routing module decides to which graph shell the attention is steered. We apply a single expert strategy (Fedus, Zoph, and Shazeer 2021).</p><p>Originally introduced for language modeling and machine translation, Shazeer et al. <ref type="bibr" target="#b17">(Shazeer et al. 2017)</ref> proposed a Mixture-of-Experts (MoE) layer. The general idea relies on a routing mechanism for token representations x to determine the best expert from a set {E i (x)} N i=1 of N experts. The router module consists of a single linear transformation whose output is normalized via softmaxing over the available N experts. Hence, the probability of choosing the i-th expert for node u is given as: p i (u). A node's representation calculated by the chosen expert is then used as input for an expert's individual linear transformation:</p><formula xml:id="formula_14">p i (u) = exp(r(u) i ) N j exp(r(u) j ) , r(u) = h T u W r<label>(13)</label></formula><formula xml:id="formula_15">h u,i * = E i * (u) T W i * + b i * ,<label>(14)</label></formula><p>where W i * ∈ R d×d denotes the weight matrix of expert E i * (•), b i * denotes the bias term. The node's representation according to expert E i * (•), is denoted by h u,i * . The architecture on how the routing mechanism is integrated into our model is shown in figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Experimental Setting</head><p>Datasets ZINC <ref type="bibr" target="#b8">(Irwin et al. 2012</ref>) is one of the most popular realworld molecular dataset consisting of 250K graphs. A subset consisting of 10K train, 1K validation, and 1K test graphs is used in the literature as benchmark <ref type="bibr" target="#b3">(Dwivedi et al. 2020</ref>).</p><p>The task is to regress a molecular property known as the constrained solubility. For each molecular graph, the node features are types of heavy atoms, and edge features are types of bonds between them.</p><p>We also evaluate our models on ogbg-molhiv <ref type="bibr" target="#b6">(Hu et al. 2020a)</ref>. Each graph within the dataset represents a molecule, where nodes are atoms and edges are chemical bonds. The task is to predict the target molecular property that is cast as binary label, i.e., whether a molecule inhibits HIV replication or not. It is known that this dataset suffers from a de-correlation between validation and test set performance.</p><p>A benchmark dataset generated by the Stochastic Block Model (SBM) <ref type="bibr" target="#b0">(Abbe 2018</ref>) is PATTERN. The graphs within this dataset do not have explicit edge features. The task is to classify the nodes into 2 communities. The size of this dataset encompasses 14K graphs.</p><p>The benchmark datasets are summarized in table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Our implementation builds upon PyTorch <ref type="bibr" target="#b13">(Paszke et al. 2019)</ref>, DGL (Wang et al. 2019), and OGB <ref type="bibr" target="#b6">(Hu et al. 2020a)</ref>. We trained our models on a single GPU, an NVIDIA GeForce RTX 2080 Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configuration</head><p>We use the Adam optimizer <ref type="bibr" target="#b9">(Kingma and Ba 2015)</ref> with an initial learning rate selected in {10 −3 , 10 −4 }. We apply the same learning rate decay strategy for all models that half the learning rate if the validation loss does not improve over a fixed number of 5 epochs.</p><p>We tune the pairing (#heads, hidden dimension) ∈ {(4, 32), (8, 64), (8, 56))} and use READOUT ∈ {sum} as function for inference on the whole graph information. Batch Normalization and Layer Normalization are disabled, whereas residual connections are activated per default. For dropout we tuned the value to be ∈ {0, 0.01, 0.05, 0.07, 0.1} and a weight decay ∈ {0, 5e-5}. For the number of graph shells, i.e, number of experts being used, we report values ∈ {4, 6, 8, 10, 12}. As aggregation functions we use AGGREGATE ∈ {sum} and AGGREGATE µ ∈ {mean}. For LPE, the 8 smallest eigenvectors are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction Tasks</head><p>In the following series of experiments, we investigate the performance of the Graph Shell Attention mechanism on graph-level prediction tasks for the datasets ogbg-molhiv <ref type="bibr" target="#b6">(Hu et al. 2020a</ref>) and ZINC <ref type="bibr" target="#b8">(Irwin et al. 2012)</ref>, and a node-level classification task on PATTERN <ref type="bibr" target="#b0">(Abbe 2018)</ref>. We use commonly used metrics for the prediction tasks, i.e., mean absolute error (MAE) for ZINC, the ROC-AUC score on ogbg-molhiv, and the accuracy on PATTERN.</p><p>Competitors. We evaluate our architectures against stateof-the-art GNN models achieving competitive results. Our report subsumes the vanilla GCN <ref type="bibr" target="#b10">(Kipf and Welling 2017)</ref>, GAT <ref type="bibr" target="#b19">(Veličković et al. 2017</ref>) that includes additional attention heuristics, or more recent GNN architectures building on top of Transformer-enhanced models like SAN <ref type="bibr" target="#b11">(Kreuzer et al. 2021)</ref> and Graphormer <ref type="bibr" target="#b22">(Ying et al. 2021)</ref>. Moreover, we include GIN <ref type="bibr" target="#b20">(Xu et al. 2018a</ref>) that is more discriminative towards graph structures compared to GCN <ref type="bibr" target="#b10">(Kipf and Welling 2017)</ref>, GraphSage <ref type="bibr" target="#b4">(Hamilton, Ying, and Leskovec 2017)</ref>, and DGN <ref type="bibr" target="#b1">(Beaini et al. 2020)</ref> being more discriminative than standard GNNs w.r.t the Weisfeiler-Lehman 1-WL test.</p><p>Results. Tables <ref type="table" target="#tab_3">2, 3</ref>, and 4 summarize the performances of our SEA models compared to baselines on ZINC, ogbgmolhiv, and PATTERN. Vanilla GTL shows the results of our implementation of the GNN model including Graph Transformer Layers <ref type="bibr">(Dwivedi and Bresson 2021)</ref>. SEA-2-HOP includes the 2-hop connection within the input graph, whereas SEA-2-HOP-AUG process the input data the same way as the 2-HOP heuristic, but uses additional feedforward networks for computing Q, K, V values for the 2-hop neighbors.   For PATTERN, we observe the best result using the SEA-2-HOP model, beating all other competitors. On the other hand, distributing an aggregated value to neighboring nodes according to SEA-AGGREGATED yields a too coarse view for graphs following the SBM and loses local graph structure.</p><p>In the sense of Green AI <ref type="bibr" target="#b15">(Schwartz et al. 2020</ref>) that focuses on reducing the computational cost to encourage a reduction in the resources spent, our architecture reaches state-of-the-art performance on ogbg-molhiv while reducing the number of parameters being trained. Comparing SEA-AGGREGATED to the best result reported for Graphormer <ref type="bibr" target="#b22">(Ying et al. 2021)</ref>, our model economizes on 99.71% of the number of parameters while still reaching competitive results.</p><p>The results on ZINC enforces the argument of using individual experts compared to vanilla GTLs, where the best result is reported for SEA-2-HOP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Shells</head><p>Next, we examine the performance w.r.t the number of experts. Notably, increasing the number of experts correlated with the number of Graph Shells which are taken into account. Table <ref type="table" target="#tab_5">5</ref> summarizes the results where all other hyperparameters are frozen, and we only have a variable size in the number of experts. We train each model for 500 epochs and report the best-observed metrics on the test datasets. We apply an early stopping heuristic, where we stop the learning procedure if we have not observed any improvements w.r.t the evaluation metrics or if the learning rate scheduler reaches a minimal value which we set to 10 −6 . Each evaluation on the test data is conducted after 5 epochs, and the early stopping is effective after 10 consecutive evaluations on the test data with no improvements. First, note that increasing the number of experts also increases the model's parameters linearly. This is due to additional routings and linear layer being defined for each expert separately. Secondly, we report also the average running time in seconds [s] on the training data for each epoch. By construction, the running time correlates with the number of parameters that have to be trained. The number of parameters differs from one dataset to another with the same settings due to a different number of nodes and edges within the datasets and slightly differs if biases are used or not. Note that we observe better results of SEA-AGGREGATED by decreasing the embedding size from 64 to 32, which also applies for the PATTERN dataset in general. The increase of parameters of the augmented 2-hop architecture SEA-2-HOP-AUG is due to the additional feedforward layers being used for the k-hop neighbors to compute the inputs Q, K, V in the graph transformer layer. Notably, we also observe that similar settings apply for datasets where the structure is an important feature of the graph, like in molecules (ZINC + ogbg-molhiv). In contrast to that is the behavior on graphs following the stochastic block model (PATTERN). On the latter one, the best performance could be observed by including k-hop information, whereas an aggregation yields too simplified features to be competitive. For the real-world molecules datasets, we observe a tendency that more experts boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stretching Locality in SEA-K-HOP</head><p>In the last series of experiments, we investigate the influence of the parameter k for the SEA-K-HOP model. Generally, by increasing the parameter k, the model diverges to the full model being also examined for the SAN architecture explained in <ref type="bibr" target="#b11">(Kreuzer et al. 2021)</ref>. In short, the full setting takes edges into account that is given by the input data and also sends information over non-existent edges, i.e., the argumentation is on a full graph setting. In our model, we smooth the transition from edges being given in the input data to the full setting that naturally arises when k, the number of hops, is set to a sufficiently high number. The table 6 summarizes the results for the non-augmented model, i.e., no extra linear layers are used for each k-hop neighborhood.</p><p>The number of parameters stays the same by increasing k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion &amp; Outlook</head><p>We introduced the theoretical foundation for integrating an expert heuristic within transformer-based graph neural networks (GNNs). This opens a fruitful direction for future works that go beyond successive neighborhood aggregation (message-passing) to develop even more powerful architectures in graph learning.</p><p>We provide an engineered solution that allows selecting the most representative experts for nodes in the input graph. For that, our model exploits the idea of a routing layer, which enables to steer the nodes' representations towards the individual expressiveness of dedicated experts.</p><p>As experts process different subgraphs starting from a central node, we introduce the terminology of Graph Shell Attention (SEA), where experts solely process nodes that are in their respective receptive field. Therefore, we gain expressiveness by capturing varying short-and long-term dependencies expressed by individual experts.</p><p>In a thorough experimental study, we show on real-world benchmark datasets that the gained expressiveness yields competitive performance compared to state-of-the-art results while reducing the number of parameters. Additionally, we report a series of experiments that stress the number of graph shells that are taken into account.</p><p>In the future, we aim to work on more novel implementations and applications enhanced with the graph shell attention mechanism, e.g., where different hyperparameters are used for different graph shells.  given by the graph shells they are processing, i.e., nodes within their respective fields. Hence, standardization techniques applied on mini-batches smooth out the peculiarities captured by the experts. Consequently, we disabled normalization techniques and just kept the residual connections enabled in the computational flow of graph transformer layers (GTLs). Smaller batch sizes for PATTERN are applied due to a limited 11 GB GDDR6 memory used in NVIDIA's GeForce RTX 2080 Ti, which we used in our experiments. Notably, we also observed that disabling biases for feedforward networks yield better results. However, increasing the number of hops k ≥ 3 within the SEA-K-HOP model, learning also biases result in better performances. In contrast to experiments shown in the approach proposed by Kreuzer et al. <ref type="bibr" target="#b11">(Kreuzer et al. 2021)</ref>, we also get better results using Adam as optimizer instead of AdamW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notes on Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of Experts</head><p>We evaluate the distributions of the experts being chosen to compute the nodes' representations in the following. We set the number of experts to 8. Figure <ref type="figure" target="#fig_3">4</ref> summarizes the relative frequencies of the experts being chosen on the datasets ZINC, ogbg-molhiv, and PATTERN. Generally, the performance of the shell attention heuristic degenerates whenever we observe expert collapsing. In the extreme case, just one expert expresses the mass of all nodes, and the capability to distribute learning nodes' representations over several experts is not leveraged. To overcome expert collapsing, we can use a heuristic where in the early stages of the learning procedure, an additional epsilon parameter introduces randomness into the algorithm. Like a decaying greedy policy in Reinforcement Learning (RL), we choose a random expert with probability and choose the expert with the highest probability according to the routing layer with a probability of 1 − . The epsilon value slowly decays over time. This ensures that all experts' expressiveness is being explored to find the best matching one w.r.t to a node u and prevents getting stuck in a local optimum. The figure shows the distribution of experts that are relevant for the computation of the nodes' representations. For illustrative purposes, values below 1% are omitted. Generally, nodes are more widely distributed over all available experts in the molecular datasets like ZINC and ogbg-molhiv for all models compared to PAT-TERN following a stochastic block model. Therefore, various experts are capable of capturing individual topological characteristics of molecules better than vanilla graph neural networks for which over-smoothing might potentially occur. We also observe that the mass is distributed to only a subset of the available experts for the PATTERN dataset. Hence, the specific number of iterations is more expressive for nodes within graph structures following SBM. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Vanilla Graph Transformer Layer with edge features (Dwivedi and Bresson 2021); λ denotes the Laplacian eigenvectors for positional encodings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Three variants of SEA models; for each model, the respective fields of 3 experts are shown from left to right</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Routing mechanism to N experts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of 8 experts for models SEA-GNN, SEA-AGGREGATED, SEA-2-HOP, and SEA-2-HOP-AUG for datasets ZINC, ogbg-molhiv and PATTERN. Relative frequencies are shown for values ≥ 1%. Numbers attached to the slices refer to the respective experts.</figDesc><graphic url="image-12.png" coords="10,203.99,447.98,72.50,75.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary dataset statistics</figDesc><table><row><cell>Domain</cell><cell></cell><cell></cell><cell>Dataset</cell><cell>#Graphs Task</cell></row><row><cell cols="3">Chemistry: Real-world molecular graphs</cell><cell>ZINC OGBG-MOLHIV</cell><cell>12K 41K</cell><cell>Graph Regression Graph Classification</cell></row><row><cell cols="4">Mathematical Modeling: Stochastic Block Models PATTERN</cell><cell>14K</cell><cell>Node Classification</cell></row><row><cell></cell><cell>ZINC</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>#params.</cell><cell>MAE</cell><cell></cell></row><row><cell>GCN (Kipf and Welling 2017)</cell><cell>505K</cell><cell>0.367</cell><cell></cell></row><row><cell>GIN (Xu et al. 2018b)</cell><cell>509K</cell><cell>0.526</cell><cell></cell></row><row><cell>GAT (Veličković et al. 2017)</cell><cell>531K</cell><cell>0.384</cell><cell></cell></row><row><cell>SAN (Kreuzer et al. 2021)</cell><cell>508K</cell><cell>0.139</cell><cell></cell></row><row><cell>Graphormer-SLIM (Ying et al. 2021)</cell><cell>489K</cell><cell>0.122</cell><cell></cell></row><row><cell>Vanilla GTL</cell><cell>83K</cell><cell>0.227</cell><cell></cell></row><row><cell>SEA-GNN</cell><cell>347K</cell><cell>0.212</cell><cell></cell></row><row><cell>SEA-AGGREGATED</cell><cell>112K</cell><cell>0.215</cell><cell></cell></row><row><cell>SEA-2-HOP</cell><cell>430K</cell><cell>0.159</cell><cell></cell></row><row><cell>SEA-2-HOP-AUG</cell><cell>709K</cell><cell>0.189</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison to state-of-the-art on ZINC<ref type="bibr" target="#b8">(Irwin et al. 2012)</ref>; results are partially taken from<ref type="bibr" target="#b11">(Kreuzer et al. 2021;</ref><ref type="bibr" target="#b3">Dwivedi et al. 2020)</ref>; color coding (gold/silver/bronze)</figDesc><table><row><cell></cell><cell cols="2">OGBG-MOLHIV</cell></row><row><cell>Model</cell><cell cols="2">#params. %ROC-AUC</cell></row><row><cell>GCN-GRAPHNORM (Kipf and Welling 2017)</cell><cell>526K</cell><cell>76.06</cell></row><row><cell>GIN-VN (Xu et al. 2018b)</cell><cell>3.3M</cell><cell>77.80</cell></row><row><cell>DGN (Beaini et al. 2020)</cell><cell>114K</cell><cell>79.05</cell></row><row><cell>Graphormer-FLAG (Ying et al. 2021)</cell><cell>47.0M</cell><cell>80.51</cell></row><row><cell>Vanilla GTL</cell><cell>386K</cell><cell>78.06</cell></row><row><cell>SEA-GNN</cell><cell>347K</cell><cell>79.53</cell></row><row><cell>SEA-AGGREGATED</cell><cell>133K</cell><cell>80.18</cell></row><row><cell>SEA-2-HOP</cell><cell>511K</cell><cell>80.01</cell></row><row><cell>SEA-2-HOP-AUG</cell><cell>594K</cell><cell>79.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>: Comparison to state-of-the-art on ogbg-molhiv</cell></row><row><cell></cell><cell></cell><cell>(Hu et al. 2020a); results are partially taken from (Kreuzer</cell></row><row><cell></cell><cell></cell><cell>et al. 2021; Dwivedi et al. 2020); color coding (gold/sil-</cell></row><row><cell></cell><cell></cell><cell>ver/bronze)</cell></row><row><cell></cell><cell cols="2">PATTERN</cell></row><row><cell>Model</cell><cell>#params.</cell><cell>% ACC</cell></row><row><cell>GCN (Kipf and Welling 2017)</cell><cell>500K</cell><cell>71.892</cell></row><row><cell>GIN (Xu et al. 2018b)</cell><cell>100K</cell><cell>85.590</cell></row><row><cell>GAT (Veličković et al. 2017)</cell><cell>526K</cell><cell>78.271</cell></row><row><cell>GraphSage (Hamilton, Ying, and Leskovec 2017)</cell><cell>101K</cell><cell>50.516</cell></row><row><cell>SAN (Kreuzer et al. 2021)</cell><cell>454K</cell><cell>86.581</cell></row><row><cell>Vanilla GTL</cell><cell>82K</cell><cell>84.691</cell></row><row><cell>SEA-GNN</cell><cell>132K</cell><cell>85.006</cell></row><row><cell>SEA-AGGREGATED</cell><cell>69K</cell><cell>57.557</cell></row><row><cell>SEA-2-HOP</cell><cell>48K</cell><cell>86.768</cell></row><row><cell>SEA-2-HOP-AUG</cell><cell>152K</cell><cell>86.673</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparison to state-of-the-art on PATTERN; results are partially taken from<ref type="bibr" target="#b11">(Kreuzer et al. 2021;</ref><ref type="bibr" target="#b3">Dwivedi et al. 2020)</ref>; color coding (gold/silver/bronze) ;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Influence of the number of experts applied on various SEA models; best configurations are highlighted in green ;</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ZINC</cell><cell></cell><cell></cell><cell>OGBG-MOLHIV</cell><cell></cell><cell>PATTERN</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell>#experts</cell><cell cols="2">#params</cell><cell>MAE</cell><cell>time/</cell><cell cols="2">#params. %ROC-AUC</cell><cell>time/</cell><cell>#params.</cell><cell>% ACC</cell><cell>time/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>epoch</cell><cell></cell><cell></cell><cell>epoch</cell><cell>epoch</cell></row><row><cell cols="2">SEA-GNN</cell><cell></cell><cell>4 6</cell><cell>183K 266K</cell><cell></cell><cell>0.385 0.368</cell><cell>13.60 20.93</cell><cell>182K 263K</cell><cell>79.24 78.24</cell><cell>49.21 68.67</cell><cell>48K 69K</cell><cell>78.975 82.117</cell><cell>58.14 82.46</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>349K</cell><cell></cell><cell>0.212</cell><cell>26.24</cell><cell>345K</cell><cell>79.53</cell><cell>84.35</cell><cell>90K</cell><cell>82.983</cell><cell>108.41</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>433K</cell><cell></cell><cell>0.264</cell><cell>31.63</cell><cell>428K</cell><cell>79.35</cell><cell>107.11</cell><cell>111K</cell><cell>84.041</cell><cell>133.73</cell></row><row><cell></cell><cell></cell><cell></cell><cell>12</cell><cell>516K</cell><cell></cell><cell>0.249</cell><cell>38.26</cell><cell>511K</cell><cell>79.18</cell><cell>122.99</cell><cell>132K</cell><cell>85.006</cell><cell>168.47</cell></row><row><cell cols="3">SEA-AGGREGATED</cell><cell>4 6</cell><cell>49K 70K</cell><cell></cell><cell>0.257 0.308</cell><cell>31.24 44.61</cell><cell>48K 69K</cell><cell>77.87 79.21</cell><cell>60.98 86.26</cell><cell>48K 69K</cell><cell>57.490 57.557</cell><cell>99.10 106.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>91K</cell><cell></cell><cell>0.249</cell><cell>57.89</cell><cell>90K</cell><cell>77.19</cell><cell>86.93</cell><cell>90K</cell><cell>54.385</cell><cell>131.57</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>112K</cell><cell></cell><cell>0.215</cell><cell>73.49</cell><cell>111K</cell><cell>77.48</cell><cell>102.40</cell><cell>111K</cell><cell>57.221</cell><cell>173.74</cell></row><row><cell></cell><cell></cell><cell></cell><cell>12</cell><cell>133K</cell><cell></cell><cell>0.225</cell><cell>87.08</cell><cell>132K</cell><cell>80.18</cell><cell>124.08</cell><cell>132K</cell><cell>57.270</cell><cell>206.73</cell></row><row><cell cols="2">SEA-2-HOP</cell><cell></cell><cell>4 6</cell><cell>182K 265K</cell><cell></cell><cell>0.309 0.213</cell><cell>14.28 20.13</cell><cell>180K 263K</cell><cell>76.30 77.27</cell><cell>43.51 59.82</cell><cell>48K 69K</cell><cell>86.768 86.706</cell><cell>94.04 138.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>347K</cell><cell></cell><cell>0.185</cell><cell>24.91</cell><cell>345K</cell><cell>76.61</cell><cell>79.56</cell><cell>90K</cell><cell>86.707</cell><cell>178.64</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>430K</cell><cell></cell><cell>0.159</cell><cell>32.68</cell><cell>428K</cell><cell>78.38</cell><cell>95.69</cell><cell>111K</cell><cell>86.680</cell><cell>232.91</cell></row><row><cell></cell><cell></cell><cell></cell><cell>12</cell><cell>513K</cell><cell></cell><cell>0.188</cell><cell>38.73</cell><cell>511K</cell><cell>80.01</cell><cell>112.93</cell><cell>132K</cell><cell>86.699</cell><cell>269.71</cell></row><row><cell cols="3">SEA-2-HOP-AUG</cell><cell>4 6</cell><cell>248K 363K</cell><cell></cell><cell>0.444 0.350</cell><cell>16.86 24.84</cell><cell>248K 363K</cell><cell>77.21 75.19</cell><cell>48.65 70.05</cell><cell>65K 94K</cell><cell>84.889 85.141</cell><cell>124.96 203.38</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>478K</cell><cell></cell><cell>0.285</cell><cell>31.48</cell><cell>476K</cell><cell>76.55</cell><cell>90.78</cell><cell>123K</cell><cell>86.660</cell><cell>270.85</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>594K</cell><cell></cell><cell>0.205</cell><cell>39.25</cell><cell>594K</cell><cell>79.08</cell><cell>109.91</cell><cell>152K</cell><cell>86.673</cell><cell>363.58</cell></row><row><cell></cell><cell></cell><cell></cell><cell>12</cell><cell>709K</cell><cell></cell><cell>0.189</cell><cell>46.51</cell><cell>707K</cell><cell>77.52</cell><cell>133.48</cell><cell>181K</cell><cell>86.614</cell><cell>421.46</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ZINC</cell><cell cols="2">OGBG-MOLHIV</cell><cell cols="2">PATTERN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">#exp. k</cell><cell cols="2">#prms MAE #prms</cell><cell>%ROC-</cell><cell cols="2">#prms %ACC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AUC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>265K 0.213</cell><cell>263K</cell><cell>77.27</cell><cell>69K</cell><cell>86.768</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6</cell><cell>3</cell><cell>266K 0.191</cell><cell>263K</cell><cell>76.15</cell><cell>69K</cell><cell>86.728</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SEA-K-HOP</cell><cell></cell><cell>4</cell><cell>266K 0.316</cell><cell>263K</cell><cell>73.48</cell><cell>69K</cell><cell>86.727</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>430K 0.159</cell><cell>428K</cell><cell>78.38</cell><cell>111K</cell><cell>86.680</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>3</cell><cell>433K 0.171</cell><cell>428K</cell><cell>74.67</cell><cell>111K</cell><cell>86.765</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>433K 0.239</cell><cell>428K</cell><cell>73.72</cell><cell>111K</cell><cell>86.725</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Influence of parameter k for the SEA-K-HOP model; best configuration for each model is highlighted in green ;</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter settings</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ZINC</cell><cell></cell><cell></cell><cell cols="2">OGBG-MOLHIV</cell><cell></cell><cell></cell><cell cols="2">PATTERN</cell><cell></cell></row><row><cell>Param.</cell><cell cols="2">SEA-GNN SEA-AGG</cell><cell>SEA-2-</cell><cell>SEA-2-</cell><cell cols="2">SEA-GNN SEA-AGG</cell><cell>SEA-2-</cell><cell>SEA-2-</cell><cell cols="2">SEA-GNN SEA-AGG</cell><cell>SEA-2-</cell><cell>SEA-2-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>HOP</cell><cell>HOP-AUG</cell><cell></cell><cell></cell><cell>HOP</cell><cell>HOP-AUG</cell><cell></cell><cell></cell><cell>HOP</cell><cell>HOP-AUG</cell></row><row><cell>batch size</cell><cell>64</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell></row><row><cell>optimizer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Adam</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>learning rate (lr)</cell><cell></cell><cell>10 −3</cell><cell></cell><cell></cell><cell></cell><cell>10 −3</cell><cell></cell><cell></cell><cell></cell><cell>10 −4</cell><cell></cell><cell></cell></row><row><cell>lr reduce factor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>momentum</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>emb.size</cell><cell>64</cell><cell>32</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell># heads</cell><cell>8</cell><cell>4</cell><cell>8</cell><cell>8</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell></row><row><cell># experts</cell><cell>8</cell><cell>10</cell><cell>10</cell><cell>12</cell><cell>8</cell><cell>12</cell><cell>12</cell><cell>10</cell><cell>12</cell><cell>6</cell><cell>4</cell><cell>10</cell></row><row><cell>dropout</cell><cell>0.07</cell><cell>0.01</cell><cell>0.07</cell><cell>0.07</cell><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell>0.00</cell><cell></cell><cell></cell></row><row><cell>READOUT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sum</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer norm.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>false</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>batch norm.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>false</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>residual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>true</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>use bias</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>false</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>use edge features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>false</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LPE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>true</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LPE dim</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In the following, we provide additional information about the hyperparameters being used in our experiments. We also discuss the distribution of the experts being selected to learn the node representations of the input graph.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community Detection and Stochastic Block Models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Abbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Commun. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="162" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Directional Graph Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<idno>CoRR, abs/2010.02863</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Generalization of Transformer Networks to Graphs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>CoRR, abs/2101.03961</idno>
		<imprint>
			<date type="published" when="2020">2020. 2021</date>
		</imprint>
	</monogr>
	<note>Benchmarking Graph Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Expander Graphs and Their Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hoory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="439" to="562" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<title level="m">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Transformer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020b. 2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
	<note>ISBN 9781450370233</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ZINC: A Free Tool to Discover Chemistry for Biology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03893</idno>
		<title level="m">Rethinking Graph Transformers with Spectral Attention</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Universal Self-Attention Network for Graph Classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11855</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>; D'alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12559" to="12571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
	<note>ISBN 9781510860964</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
	</analytic>
	<monogr>
		<title level="m">Graph Attention Networks. 6th International Conference on Learning Representations</title>
				<imprint>
			<publisher>Wu</publisher>
			<date type="published" when="2017">2017. 2019. 2021</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
	<note type="report_type">Highly-Performant Package for Graph Neural Networks. arXiv preprint</note>
	<note>Z.; Pan,</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>CoRR, abs/1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>CoRR, abs/1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Do Transformers Really Perform Bad for Graph Representation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05234</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph Transformer Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>; D'alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A Data-Driven Graph Generative Model for Temporal Interaction Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="401" to="411" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>ISBN 9781450379984</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
