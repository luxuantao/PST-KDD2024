<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A general sample complexity analysis of vanilla policy gradient</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-23">23 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Gower</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alessandro</forename><surname>Lazaric</surname></persName>
						</author>
						<title level="a" type="main">A general sample complexity analysis of vanilla policy gradient</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-23">23 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2107.11433v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The policy gradient (PG) is one of the most popular methods for solving reinforcement learning (RL) problems. However, a solid theoretical understanding of even the "vanilla" PG has remained elusive for long time. In this paper, we apply recent tools developed for the analysis of SGD in non-convex optimization to obtain convergence guarantees for both REINFORCE and GPOMDP under smoothness assumption on the objective function and weak conditions on the second moment of the norm of the estimated gradient. When instantiated under common assumptions on the policy space, our general result immediately recovers existing O( -4 ) sample complexity guarantees, but for wider ranges of parameters (e.g., step size and batch size m) with respect to previous literature. Notably, our result includes the single trajectory case (i.e., m = 1) and it provides a more accurate analysis of the dependency on problem-specific parameters by fixing previous results available in the literature. We believe that the integration of state-of-the-art tools from nonconvex optimization may lead to identify a much broader range of problems where PG methods enjoy strong theoretical guarantees.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The policy gradient (PG) is one of the most popular reinforcement learning (RL) methods to compute policies that maximize long-term rewards <ref type="bibr" target="#b27">(Williams, 1992;</ref><ref type="bibr" target="#b25">Sutton et al., 2000)</ref>. The success of PG methods is due to their simplicity and versatility, as they can be readily implemented to solve a wide range of problems (including in environments where the Markov assumption is not verified) and they can be effectively paired with other techniques to obtain more sophisticated algorithms such as the actor-critic <ref type="bibr">(Konda &amp;</ref> 1 Facebook AI Research 2 LTCI, T?l?com Paris 3 Institut Polytechnique de Paris.</p><p>Correspondence to: Rui Yuan &lt;ruiyuan@fb.com&gt;.</p><p>Workshop on "Reinforcement learning theory" at the 38 th International Conference on Machine Learning, 2021. Copyright 2021 by the author(s). <ref type="bibr" target="#b11">Tsitsiklis, 2000;</ref><ref type="bibr" target="#b15">Mnih et al., 2016)</ref>, natural PG <ref type="bibr" target="#b9">(Kakade, 2002)</ref>, trust-region based variants <ref type="bibr" target="#b21">(Schulman et al., 2015;</ref><ref type="bibr" target="#b2">2017)</ref>, variance-reduced PG <ref type="bibr" target="#b17">(Papini et al., 2018;</ref><ref type="bibr" target="#b23">Shen et al., 2019;</ref><ref type="bibr">Xu et al., 2020b)</ref>, etc.</p><p>Unlike value-based methods, a solid theoretical understanding of even the "vanilla" PG has remained elusive for long time. Recently, a more complete theory of PG has been derived by leveraging the RL structure of the problem together with tools from convex and non-convex optimization. By using a gradient domination property of the cumulative reward, the global convergence of PG with the exact full gradient is established for linear-quadratic regulators <ref type="bibr" target="#b5">(Fazel et al., 2018)</ref> and Markov Decision Process (MDP) with constrained tabular parametrization <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref> or with soft-max tabular parametrization <ref type="bibr" target="#b14">(Mei et al., 2020)</ref>. <ref type="bibr">Zhang et al. (2020b)</ref> also establishes the global convergence with the exact full gradient by leveraging the hidden convex structure of the cumulative reward and shows that all local optimums are in fact global optimums under certain assumptions. To improve sample efficiency, <ref type="bibr" target="#b17">Papini et al. (2018)</ref>; <ref type="bibr">Xu et al. (2020a;</ref><ref type="bibr" target="#b30">b)</ref> introduce stochastic variance reduced gradient techniques <ref type="bibr" target="#b8">(Johnson &amp; Zhang, 2013;</ref><ref type="bibr" target="#b16">Nguyen et al., 2017;</ref><ref type="bibr" target="#b4">Fang et al., 2018)</ref> to policy optimization, and they have studied the sample complexity of PG methods to achieve a first-order stationary point (FOSP). Later, <ref type="bibr" target="#b13">Liu et al. (2020)</ref> and <ref type="bibr" target="#b32">Zhang et al. (2021)</ref> unify the lines of work on global convergence and variance reduction in PG. However, these works require either the exact full gradient updates or large batch sizes per iteration. By doing a regret minimization analysis, <ref type="bibr">Zhang et al. (2020a)</ref> shows that it is possible to allow a single sampled trajectory (i.e., mini-batch size m = 1) for the convergence. However, their setting is restricted to soft-max policy and does not use "vanilla" PG but a modified version with re-projection meant to guarantee a sufficient level of policy randomization.</p><p>In this paper, we apply recent tools developed for the analysis of stochastic gradient descent (SGD) in non-convex optimization <ref type="bibr" target="#b10">(Khaled &amp; Richt?rik, 2020)</ref> to obtain FOSP convergence guarantees for both REINFORCE and GPOMDP under smoothness assumption on the objective function and weak conditions on the second moment of the norm of the estimated gradient. When instantiated under common assumptions on the policy space, our general result immediately recovers existing O( -4 ) sample complexity guarantees <ref type="bibr" target="#b13">(Liu et al., 2020)</ref>, but for wider ranges of parameters (e.g., step size and batch size) with respect to previous literature. Notably, our result includes the single trajectory case (i.e., m = 1) and it provides a more accurate analysis of the dependency on problem-specific parameters by fixing previous results available in the literature. We believe that the integration of state-of-the-art tools from non-convex optimization may lead to identify a much broader range of problems where PG methods enjoy strong theoretical guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Markov decision process (MDP). We consider a continuous MDP M = {S, A, P, R, ?, ?}, where S is a state space; A is an action space; P is a Markovian transition model, where P(s | s, a) defines the transition density from state s to s under action a; R is the reward function, where R(s, a)</p><formula xml:id="formula_0">def = E s ?P(?|s,a) [R(s, a, s )] ? [-R max , R max ]</formula><p>is the expected reward for state-action pair (s, a); ? ? [0, 1) is the discount factor; and ? is the initial state distribution. The agent's behavior is modeled as a policy ? ? ? S A , where ?(? | s) is the density distribution over A in state s ? S. We consider the infinite-horizon discounted setting.</p><p>Let p(? | ?) be the distribution induced by the policy ? on the set T of all possible trajectories, that is</p><formula xml:id="formula_1">p(? | ?) = ?(s 0 ) ? t=0 ?(a t | s t )p(s t+1 | s t , a t ). (1) Let R(? ) = ? t=0 ? t R(s t ,</formula><p>a t ) be the total discounted reward accumulated along trajectory ? , then we define the performance function</p><formula xml:id="formula_2">J(?) = E ? ?p(?|?,M) [R(? )] def = E ? ?p(?|?) [R(? )] . (2)</formula><p>Policy gradient. PG is a class of methods designed to compute the policy maximizing the total reward J(?) by gradient ascent. We introduce a class of parametrized policies ? ? = {? ? : ? ? ?}, with the assumption that ? ? is differentiable w.r.t. ?. For simplicity, we consider ? ? R d . We denote J(?) = J(? ? ) and p(?</p><formula xml:id="formula_3">| ?) = p ? (? ) = p(? | ? ? ).</formula><p>We also define J * = sup ? J(?) the optimal expected total reward and ? * ? arg sup ? J(?) the parameter of the optimal policy. In the most general case, J(?) is a non-convex function of the parameter.</p><p>The gradient ?J(?) is derived as follows:</p><formula xml:id="formula_4">?J(?) = R(? )?p(? | ?)d? (3) = R(? ) (?p(? | ?)/p(? | ?)) p(? | ?)d? = E ? ?p(?|?) [R(? )? log p(? | ?) | M] (1) = E ? ? t=0 ? t R(s t , a t ) ? t =0 ? ? log ? ? (a t | s t ) .</formula><p>Since it is not possible to execute all possible trajectories up to infinity to compute the full gradient ?J(?), one has to resort to an empirical estimate of the gradient by sampling m truncated trajectories</p><formula xml:id="formula_5">? i = (s 0 , a 0 , r 0 , s 1 , ? ? ? , s H-1 , a H-1 , r H-1</formula><p>) obtained by executing ? ? for a fixed horizon H. Then the gradient estimator is computed as</p><formula xml:id="formula_6">? m J(?) = 1 m m i=1 H-1 t=0 ? t R(s i t , a i t ) ? H-1 t =0 ? ? log ? ? (a i t | s i t ). (4)</formula><p>The estimator (4) is known as the REINFORCE gradient estimator <ref type="bibr" target="#b27">(Williams, 1992)</ref>.</p><p>However, the REINFORCE estimator can be simplified by leveraging the fact that future actions do not depend on past rewards. This leads to the alternative formulation</p><formula xml:id="formula_7">?J(?) = E ? ? t=0 t k=0 ? ? log ? ? (a k | s k ) ? t R(s t , a t ) ,<label>(5)</label></formula><p>which leads to the following gradient estimator</p><formula xml:id="formula_8">? m J(?) = 1 m m i=1 H-1 t=0 t k=0 ? ? log ? ? (a i k | s i k ) ? t R(s i t , a i t ),<label>(6)</label></formula><p>which is known as GPOMDP <ref type="bibr" target="#b1">(Baxter &amp; Bartlett, 2001)</ref>.</p><p>Notice that both REINFORCE and GPOMDP are the truncated versions of unbiased gradient estimators. More precisely, they are unbiased for the gradient of the truncated performance function</p><formula xml:id="formula_9">J H (?) def = E ? H-1 t=0 ? t R(s t , a t ) .</formula><p>Equipped with gradient estimators, vanilla policy gradient simply updates the policy parameters as</p><formula xml:id="formula_10">? k+1 = ? k + ? ? m J(? k ),<label>(7)</label></formula><p>with a step size ? &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Non-convex optimization under ABC assumption</head><p>We use ? m J(?) to denote either of the truncated gradient estimators defined in (4) or (6). All following analyses rely on the following smoothness assumption.</p><p>Assumption 3.1 (Smoothness). There exists L &gt; 0 such that, for all ?, ? ? R d , we have</p><formula xml:id="formula_11">|J(? ) -J(?) -?J(?), ? -? | ? L 2 ? -? 2 . (8)</formula><p>We also make use of the recently introduced ABC assumption <ref type="bibr" target="#b10">(Khaled &amp; Richt?rik, 2020)</ref> <ref type="foot" target="#foot_0">1</ref> which bounds the second moment of the norm of the gradient estimators using the norm of the truncated full gradient, the suboptimality gap and an additive constant. Assumption 3.2 (ABC). The stochastic gradient satisfies</p><formula xml:id="formula_12">E ? m J(?) 2 ? 2A(J * -J(?))+B ?J H (?) 2 +C,<label>(9)</label></formula><p>for some A, B, C ? 0 and all ? ? R d .</p><p>The ABC assumption effectively summarizes a number of popular and more restrictive assumptions commonly used in non-convex optimization. Indeed, the bounded variance of the stochastic gradient assumption <ref type="bibr" target="#b6">(Ghadimi &amp; Lan, 2013)</ref>, the gradient confusion assumption <ref type="bibr" target="#b19">(Sankararaman et al., 2020)</ref>, the sure-smoothness assumption <ref type="bibr" target="#b12">(Lei et al., 2020)</ref> and different variants of strong growth assumptions proposed in <ref type="bibr" target="#b20">(Schmidt &amp; Roux, 2013;</ref><ref type="bibr" target="#b26">Vaswani et al., 2019;</ref><ref type="bibr" target="#b3">Bottou et al., 2018)</ref> can all be seen as specific cases of Asm. 3.2. The ABC assumption has been shown to be the weakest among all existing assumptions to provide convergence guarantees for SGD for the minimization of non-convex smooth functions.</p><p>In order to apply this result to our case, we need an additional assumption to bound the error due to the truncation of the horizon as follows.</p><p>Assumption 3.3. There exists D, D &gt; 0 such that, for all ? ? R d , we have</p><formula xml:id="formula_13">| ?J H (?), ?J H (?) -?J(?) | ? D? H , (10) ?J H (?) -?J(?) ? D ? H . (11)</formula><p>While we specifically need those conditions to hold as an assumption, we notice that they are reasonable since we have</p><formula xml:id="formula_14">|J(?) -J H (?)| ? Rmax 1-? ? H by the definition of J(?) and J H (?).</formula><p>When H is large, the difference between J(?) and J H (?) is negligible. However, Asm. 3.3 is still necessary.</p><p>In fact, the forthcoming convergence results is built on the first-order stationary point. Once we find a stationary point ? such that ?J H (?) is closed to 0, we need (11) to claim the first-order stationary point convergence.</p><p>Equipped with these assumptions, we can adapt Thm. 2 in <ref type="bibr" target="#b10">(Khaled &amp; Richt?rik, 2020</ref>) and obtain the following guarantee.</p><p>Proposition 3.4. Suppose that Assumption 3.1, 3.2 and 3.3 are satisfied. We choose a constant step size ? such that ? ? 0, 2 LB where B can be zero.</p><formula xml:id="formula_15">a Let ? 0 def = J * -J(? 0 ). If A &gt; 0, then PG satisfies min 0?t?T -1 E ?J(? t ) 2 ? 2? 0 (1 + L? 2 A) T ?T (2 -LB?)<label>(12)</label></formula><formula xml:id="formula_16">+ LC? 2 -LB? + 2D(3 -LB?) 2 -LB? + D 2 ? H ? H . If A = 0, we have E ?J(? U ) 2 ? 2? 0 ?T (2 -LB?)<label>(13)</label></formula><formula xml:id="formula_17">+ LC? 2 -LB? + 2D(3 -LB?) 2 -LB? + D 2 ? H ? H , where ? U is uniformly sampled from {? 0 , ? 1 , ? ? ? , ? T -1 }. a We set 1 0 = ?.</formula><p>While the proof of Prop. 3.4 is integrating the bias coming from the truncated estimators in the proof of Thm. 2 in <ref type="bibr" target="#b10">(Khaled &amp; Richt?rik, 2020)</ref>, we provide the full proof in App. B for completeness. Prop. 3.4 provides a very general characterization of the performance of PG as a function of all the constants involved in the assumptions on the problem and policy gradient estimator.</p><p>From ( <ref type="formula" target="#formula_15">12</ref>) we can derive the sample complexity of PG (see also Cor. 1 in <ref type="bibr" target="#b10">(Khaled &amp; Richt?rik, 2020)</ref>). If we set the parameters as</p><formula xml:id="formula_18">? = min 1 ? LAT , 1 LB , 2LC , T ? 12? 0 L 2 max B, 12? 0 A 2 , 2C 2 ,<label>(14)</label></formula><formula xml:id="formula_19">H = O(log -1 ), then min 0?t?T -1 E ?J(? t ) 2 = O( 2 ).</formula><p>First, the iteration complexity ( <ref type="formula" target="#formula_18">14</ref>) recovers the exact full gradient case. That is, considering H = ? (i.e. J H = J) and ? m J(?) = ?J(?) in ( <ref type="formula" target="#formula_10">7</ref>), we have Asm. 3.2 and 3.3 hold automatically with A = C = D = D = 0 and B = 1.</p><p>Consequently, we require T = O( -<ref type="foot" target="#foot_1">2</ref> ) iterations to reach an -stationary point. Thus, for any policy and MDP that satisfy the smoothness property (Asm. 3.1), the exact full PG converge in O( -2 ) iterations. Notice that this is the standard rate of convergence for gradient descent on nonconvex function minimizations without any other assumptions <ref type="bibr" target="#b2">(Beck, 2017)</ref>. Under special cases, <ref type="bibr" target="#b0">Agarwal et al. (2021)</ref> also establishes a O( -2 ) rate of convergence for the exact full gradient in the constrained tabular parametrized policy. By leveraging the hidden convex structure using composite optimization tools with additional assumptions where the constrained tabular parametrized policy satisfies, <ref type="bibr">Zhang et al. (2020b)</ref> and <ref type="bibr" target="#b32">Zhang et al. (2021)</ref> obtain an improved convergence rate O( -1 ) for the exact full gradient. <ref type="bibr" target="#b14">Mei et al. (2020)</ref> also establishes the same convergence rate O( -1 ) for the true gradient in the soft-max policy by using a gradient domination property (Lojasiewicz inequality). However, these convergence rates are only conceptual, as we can rarely access the exact full gradient for the update in practice.</p><p>In a more general case, i.e. A, C, D, D are not all 0, the iteration complexity ( <ref type="formula" target="#formula_18">14</ref>) shows that with T H = O( -4 ) samples (i.e., single-step interaction with the environment and single sampled trajectory per iteration), the vanilla policy gradient guarantees to converge to a first-order stationary point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Convergence under the Lipschitz and smooth policy assumptions</head><p>In this section, we instantiate this general statement under (more restrictive) common assumptions on the policy space and we recover existing results for wider ranges of the parameters and more accurate dependencies.</p><p>4.1. Sufficient conditions for Asm. 3.1, 3.2 and 3.3</p><p>We consider a Lipschitz and smooth policy.</p><p>Assumption 4.1 (Lipschitz and smooth policy). There exists constants G, F &gt; 0 such that for every action a ? A and every state s ? S, the gradient and Hessian of log ? ? (a | s) satisfy</p><formula xml:id="formula_20">? ? log ? ? (a | s) ? G, (15) ? 2 ? log ? ? (a | s) ? F. (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>This assumption is widely adopted in the analysis of variance-reduced PG methods, e.g. <ref type="bibr" target="#b23">(Shen et al., 2019;</ref><ref type="bibr">Xu et al., 2020a;</ref><ref type="bibr" target="#b30">b;</ref><ref type="bibr" target="#b7">Huang et al., 2020;</ref><ref type="bibr" target="#b18">Pham et al., 2020;</ref><ref type="bibr" target="#b13">Liu et al., 2020;</ref><ref type="bibr" target="#b32">Zhang et al., 2021)</ref> and it is a relaxation of the one in <ref type="bibr" target="#b17">(Papini et al., 2018)</ref>, which assumes that</p><formula xml:id="formula_22">? ??i log ? ? (a | s) and ? 2</formula><p>??i??j log ? ? (a | s) are bounded element-wise. Such assumption is reasonable. For instance, Gaussian policy under the mild condition that the actions and the state feature vectors are bounded satisfies this assumption <ref type="bibr">(Xu et al., 2020b)</ref>.</p><p>Asm. 4.1 directly implies the smoothness of J(?) as well as the ABC and the truncated gradient assumptions for any PG estimator as illustrated in the following lemmas.</p><p>Lemma 4.2. Under Asm. 4.1, J(?) is L-smooth, namely ? 2 J(?) ? L for all ? which is a sufficient condition of Asm. 3.1, with</p><formula xml:id="formula_23">L = 2G 2 R max (1 -?) 3 + F R max (1 -?) 2 . (<label>17</label></formula><formula xml:id="formula_24">)</formula><p>Remark. The smoothness constant ( <ref type="formula" target="#formula_23">17</ref>) is different as compared to recent work such as Proposition 4.2 (2) in <ref type="bibr">(Xu et al., 2020b)</ref>, Proposition 1 (2) in <ref type="bibr" target="#b7">(Huang et al., 2020)</ref>, Lemma B.1 in <ref type="bibr" target="#b13">(Liu et al., 2020)</ref> and Lemma 3.1 in <ref type="bibr" target="#b18">(Pham et al., 2020)</ref>. This difference is due to a recurring mistake in a crucial step in bounding the Hessian. 2 Compared to existing bounds, our result reveals an additional term depending on (1 -?) -3 which dominates the term F Rmax</p><p>(1-?) 2 derived in <ref type="bibr">(Xu et al., 2020b)</ref> whenever ? is close to 1.</p><formula xml:id="formula_25">Lemma 4.3. Under Asm. 4.1, Asm. 3.2 holds with A = 0, B = 1 -1 m and C = ? 2 g m , that is, E ? m J(?) 2 ? 1 - 1 m ?J H (?) 2 + ? 2 g m ,<label>(18)</label></formula><p>where m is the mini-batch size, and ? g = HGRmax 1-? when using REINFORCE gradient estimator or ? g = GRmax (1-?) 2 when using GPOMDP gradient estimator.</p><p>Bounded variance of the gradient estimator. Interestingly, from (18) we immediately obtain</p><formula xml:id="formula_26">Var ? m J(?) = E ? m J(?) 2 -?J H (?) 2 (18) ? ? 2 g -?J H (?) 2 m ? ? 2 g m ,<label>(19)</label></formula><p>which was used as an assumption in <ref type="bibr" target="#b17">(Papini et al., 2018;</ref><ref type="bibr">Xu et al., 2020b;</ref><ref type="bibr" target="#b7">Huang et al., 2020;</ref><ref type="bibr" target="#b18">Pham et al., 2020;</ref><ref type="bibr" target="#b13">Liu et al., 2020)</ref>, while it can be directly deduced from Asm. 4.1.</p><p>Lemma 4.4. Under Asm. 4.1, Asm. 3.3 holds with</p><formula xml:id="formula_27">D = D GR max (1 -?) 2 ,<label>(20)</label></formula><formula xml:id="formula_28">D = 1 (1 -?) 2 + H 1 -? GR max . (21)</formula><p>As a by-product, in Lemma D.1 in the appendix we also show that J(?) is Lipschitz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sample complexity of the vanilla policy gradient</head><p>Now we can establish the sample complexity of policy gradient for Lipschitz and smooth policies as an immediate corollary of Proposition 3.4 and Lemma 4.2, 4.3 and 4.4.</p><p>Corollary 4.5. Suppose that Assumption 4.1 is satisfied. Let ? 0 def = J * -J(? 0 ). Any PG method with a mini-batch sampling of size m and step size</p><formula xml:id="formula_29">? ? 0, 2 L (1 -1/m) ,<label>(22)</label></formula><p>we have</p><formula xml:id="formula_30">E ?J(? U ) 2 ? 2? 0 ?T 2 -L? 1 -1 m + L? 2 g ? m 2 -L? 1 -1 m + 2D 3 -L? 1 -1 m 2 -L? 1 -1 m + D 2 ? H ? H , (<label>23</label></formula><formula xml:id="formula_31">)</formula><p>where D, D &gt; 0 are provided in Lemma 4.4.</p><p>We first notice that we impose no restriction on the batch size and when m = 1, by ( <ref type="formula" target="#formula_29">22</ref>) we have that ? ? (0, ?), i.e., the guarantee holds for any choice of the step size. This greatly extends the range of parameters for which PG is guaranteed to converge w.r.t. existing literature.</p><p>As in Prop. 3.4, we can then derive explicit sample complexity guarantees. For any accuracy level , if we set the parameters as (the detailed derivation is provided in App. F.1)</p><formula xml:id="formula_32">m ? 1; 2? 2 g 2 , T s.t. T m ? 8? 0 L? 2 g 4 , ? = 2 m 2L? 2 g , H = O (log (1/ ) / log (1/?)) , then E ?J(? U ) 2 = O( 2 )</formula><p>. This shows that it is possible to have the vanilla policy gradient methods converge with a mini-batch size per iteration that can actually vary from 1 to O( -2 ), while the sample complexity remains the same as known in the literature, i.e., O( -4 ) <ref type="bibr" target="#b13">(Liu et al., 2020)</ref>.</p><p>This result is novel compared to <ref type="bibr" target="#b17">(Papini et al., 2018;</ref><ref type="bibr">Xu et al., 2020b;</ref><ref type="bibr" target="#b13">Liu et al., 2020;</ref><ref type="bibr" target="#b32">Zhang et al., 2021)</ref> that do not allow a single trajectory sampled per iteration. The only existing analysis that allows m = 1 we are aware of is <ref type="bibr">(Zhang et al., 2020a)</ref>. However, <ref type="bibr">Zhang et al. (2020a)</ref> does not study the vanilla policy gradient. Instead, they add an extra phased learning step to enforce the exploration of the MDP and used a decreasing step size. Moreover, their result is restricted to the soft-max policy parametrization with a log-barrier regularization, which makes their analysis less general. Our results show that such extra phased learning step is unnecessary, the step size can be constant and our convergence theory is satisfied for a much larger class of parametrized policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We believe the generality of Prop. 3.4 opens the possibility to identify a broader set of configurations (i.e., MDP and policy space) for which PG is guaranteed to converge.</p><p>In particular, we notice that Asm. 4.1 despite being very common, is somehow restrictive, as general policy spaces defined by e.g., a multi-layer neural network, may not satisfy it, unless some restriction on the parameters is imposed. Another interesting venue of investigation is whether it is possible to identify counterparts of the ABC assumption for variance-reduced versions of PG and for the improved analysis of <ref type="bibr">(Zhang et al., 2020b;</ref><ref type="bibr">2021)</ref> leveraging composite optimization tools. For those better sample complexity results, it remains an open question whether we still have convergence guarantee for one single sampled trajectory per iteration with a constant step size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>Here we provide the missing proofs from the main paper and some additional noteworthy observations made in the main paper. Each proposition and lemma have a respect section with its proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Auxiliary Lemmas</head><p>Lemma A.1. For all ? ? [0, 1) and any strictly positive integer H, we have that</p><formula xml:id="formula_33">H-1 t=0 (t + 1)? t ? ? t=0 (t + 1)? t = 1 (1 -?) 2 .</formula><p>Proof. The first part of the inequality is trivial. We now prove the second part of the inequality. Let</p><formula xml:id="formula_34">S def = ? t=0 (t + 1)? t . We have ?S = ? t=0 (t + 1)? t+1 = ? t=1 t? t .</formula><p>Thus, the subtraction of the above two equations gives</p><formula xml:id="formula_35">(1 -?)S = ? t=0 (t + 1)? t - ? t=1 t? t = 1 + ? t=1 (t + 1 -t)? t = ? t=0 ? t = 1 1 -? .</formula><p>Finally, the proof follows by dividing 1 -? on both hand side.</p><p>Lemma A.2. For all ? ? [0, 1) and any strictly positive integer H, we have that</p><formula xml:id="formula_36">? t=0 (t + 1) 2 ? t ? 2 (1 -?) 3 . Proof. Let S def = ? t=0 (t + 1) 2 ? t . We have ?S = ? t=0 (t + 1) 2 ? t+1 = ? t=1 t 2 ? t .</formula><p>Thus, the subtraction of the above two equations gives</p><formula xml:id="formula_37">(1 -?)S = ? t=0 (t + 1) 2 ? t - ? t=1 t 2 ? t = 1 + ? t=1 ((t + 1) 2 -t 2 )? t = 1 + ? t=1 (2t + 1)? t = ? t=0 (2t + 1)? t = 2 ? t=0 (t + 1)? t - ? t=0 ? t Lemma A.1 = 2 (1 -?) 2 - 1 1 -? ? 2 (1 -?) 2 .</formula><p>Finally, the proof follows by dividing 1 -? on both hand side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Proposition 3.4</head><p>Proof. We start with L-smoothness of J, which implies</p><formula xml:id="formula_38">J(? t+1 ) ? J(? t ) + ?J(? t ), ? t+1 -? t - L 2 ? t+1 -? t 2 = J(? t ) + ? ?J(? t ), ? m J(? t ) - L? 2 2 ? m J(? t ) 2 . (<label>24</label></formula><formula xml:id="formula_39">)</formula><p>Taking expectations conditioned on ? t , we get</p><formula xml:id="formula_40">E t [J(? t+1 )] ? J(? t ) + ? ?J(? t ), ?J H (? t ) - L? 2 2 E t ? m J(? t ) 2 (9) ? J(? t ) + ? ?J H (? t ) + (?J(? t ) -?J H (? t )) , ?J H (? t ) - L? 2 2 2A(J * -J(? t )) + B ?J H (? t ) 2 + C = J(? t ) + ? 1 - LB? 2 ?J H (? t ) 2 -L? 2 A(J * -J(? t )) - LC? 2 2 + ? ?J H (? t ), ?J(? t ) -?J H (? t ) (10) ? J(? t ) + ? 1 - LB? 2 ?J H (? t ) 2 -L? 2 A(J * -J(? t )) - LC? 2 2 -?D? H . (<label>25</label></formula><formula xml:id="formula_41">)</formula><p>Subtracting J * from both sides gives</p><formula xml:id="formula_42">-(J * -E t [J(? t+1 )]) ? -(1 + L? 2 A)(J * -J(? t )) + ? 1 - LB? 2 ?J H (? t ) 2 - LC? 2 2 -?D? H . (<label>26</label></formula><formula xml:id="formula_43">)</formula><p>Taking the total expectation and rearranging, we get</p><formula xml:id="formula_44">E [J * -J(? t+1 )] + ? 1 - LB? 2 E ?J H (? t ) 2 ? (1 + L? 2 A)E [J * -J(? t )] + LC? 2 2 + ?D? H . (27) Letting ? t def = E [J * -J(? t )] and r t def = E ?J H (? t )</formula><p>2 , we can rewrite the last inequality as</p><formula xml:id="formula_45">? 1 - LB? 2 r t ? (1 + L? 2 A)? t -? t+1 + LC? 2 2 + ?D? H . (<label>28</label></formula><formula xml:id="formula_46">)</formula><p>We now introduce a sequence of weights w -1 , w 0 , w 1 , ? ? ? , w T -1 based on a technique developed by <ref type="bibr" target="#b24">(Stich, 2019)</ref>. Let w -1 &gt; 0. Define w t = wt-1 1+L? 2 A for all t ? 0. Notice that if A = 0, we have</p><formula xml:id="formula_47">w t = w t-1 = ? ? ? = w -1 . Multiplying (28) by w t /?, 1 - LB? 2 w t r t ? w t (1 + L? 2 A) ? ? t - w t ? ? t+1 + LC? 2 w t + D? H w t = w t-1 ? ? t - w t ? ? t+1 + LC? 2 + D? H w t . (<label>29</label></formula><formula xml:id="formula_48">)</formula><p>Summing up both sides as t = 0, 1, ? ? ? , T -1 and using telescopic sum, we have,</p><formula xml:id="formula_49">1 - LB? 2 T -1 t=0 w t r t ? w -1 ? ? 0 - w T -1 ? ? T + LC? 2 + D? H T -1 t=0 w t ? w -1 ? ? 0 + LC? 2 + D? H T -1 t=0 w t .<label>(30)</label></formula><p>Let W T =</p><p>T -1 t=0 w t . Dividing both sides by W T , we have,</p><formula xml:id="formula_50">1 - LB? 2 min 0?t?T -1 r t ? 1 W T ? 1 - LB? 2 T -1 t=0 w t r t ? w -1 W T ? 0 ? + LC? 2 + D? H .<label>(31)</label></formula><p>Note that,</p><formula xml:id="formula_51">W T = T -1 t=0 w t ? T -1 t=0 min 0?i?T -1 w i = T w T -1 = T w -1 (1 + L? 2 A) T .<label>(32)</label></formula><p>Using this in (31),</p><formula xml:id="formula_52">1 - LB? 2 min 0?t?T -1 r t ? (1 + L? 2 A) T ?T ? 0 + LC? 2 + D? H .<label>(33)</label></formula><p>However, we have</p><formula xml:id="formula_53">E ?J(? t ) 2 = E ?J(? t ) -?J H (? t ) + ?J H (? t ) 2 = E ?J H (? t ) 2 + 2E [ ?J H (? t ), ?J(? t ) -?J H (? t ) ] + E ?J(? t ) -?J H (? t ) 2 (10)+(11) ? E ?J H (? t ) 2 + 2D? H + D 2 ? 2H .<label>(34)</label></formula><p>Substituting r t in ( <ref type="formula" target="#formula_52">33</ref>) by E ?J(? t ) 2 and using (34), we get</p><formula xml:id="formula_54">1 - LB? 2 min 0?t?T -1 E ?J(? t ) 2 ? (1 + L? 2 A) T ?T ? 0 + LC? 2 + D? H + 1 - LB? 2 2D? H + D 2 ? 2H .</formula><p>Our choice of step size guarantees that no matter B &gt; 0 or B = 0, we have 1 -LB? 2 &gt; 0. Dividing both sides by 1 -LB? 2 and rearranging yields the proposition's claim.</p><p>If A = 0, we know that {w t } t?-1 is a constant sequence. In this case, W T = T w -1 . Dividing both sides of (30) by W T , we have,</p><formula xml:id="formula_55">1 - LB? 2 1 T T -1 t=0 r t ? ? 0 ?T + LC? 2 + D? H .<label>(35)</label></formula><p>Similarly, substituting r t in ( <ref type="formula" target="#formula_55">35</ref>) by E ?J(? t ) 2 and using (34), we get</p><formula xml:id="formula_56">1 - LB? 2 E ?J(? U ) 2 = 1 - LB? 2 1 T T -1 t=0 E ?J(? t ) 2 ? ? 0 ?T + LC? 2 + D? H + 1 - LB? 2 2D? H + D 2 ? 2H .</formula><p>Dividing both sides by 1 -LB? 2 and rearranging yields the proposition's claim.</p><p>C. Proof of Lemma 4.2</p><p>Proof. We know that</p><formula xml:id="formula_57">? 2 J(?) (5) = ? ? E ? ? t=0 ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) = ? ? p(? | ?) ? t=0 ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) d? = ? ? p(? | ?) ? t=0 ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) d? + p(? | ?) ? t=0 ? t R(s t , a t ) t k=0 ? 2 ? log ? ? (a k | s k ) d? = p(? | ?)? ? log p(? | ?) ? t=0 ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) d? + p(? | ?) ? t=0 ? t R(s t , a t ) t k=0 ? 2 ? log ? ? (a k | s k ) d? = E ? ? ? ? ? log p(? | ?) ? t=0 ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) ? ? +E ? ? t=0 ? t R(s t , a t ) t k=0 ? 2 ? log ? ? (a k | s k ) (1) = E ? ? ? ? t =0 ? ? log ? ? (a t | ? t ) ? t=0 ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) ? ? 1 + E ? ? t=0 ? t R(s t , a t ) t k=0 ? 2 ? log ? ? (a k | s k ) 2 . (<label>36</label></formula><formula xml:id="formula_58">)</formula><p>We aim to bound two terms separately. The second term can be bounded easily. That is,</p><formula xml:id="formula_59">2 ? E ? ? t=0 ? t |R(s t , a t )| t k=0 ? 2 ? log ? ? (a k | s k ) ? E ? F R max ? t=0 (t + 1)? t = F R max (1 -?) 2 ,<label>(37)</label></formula><p>where the second line is obtained by using |R(s t , a t )| ? R max and ? 2 ? log ? ? (a k | s k ) ? F from Assumption 4.1; the last line is obtained by Lemma A.1.</p><p>To bound the first term, we use the following notation x 0:t def = (x 0 , x 1 , ? ? ? , x t ) with {x t } t?0 a sequence of random variables. Similar to the derivation of GPOMDP, we notice that future actions do not depend on past rewards and past actions. That is, for 0 ? t &lt; t among terms of the two sums in 1 , we have</p><formula xml:id="formula_60">E ? ? ? ? ? log ? ? (a t | s t ) ? ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) ? ? = E s 0:t ,a 0:t ? ? ? ? log ? ? (a t | s t ) ? ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) ? ? = E s 0:t ,a 0:(t -1) ? ? E a t ? ? ? ? log ? ? (a t | s t ) ? ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) s 0:t , a 0:(t -1) ? ? ? ? = E s 0:t ,a 0:(t -1) ? ? E a t ? ? log ? ? (a t | s t ) s t ? ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) ? ? = E s 0:t ,a 0:(t -1) ? ? ? ? (a t | s t )? ? log ? ? (a t | s t )da t ? ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) ? ? = E s 0:t ,a 0:(t -1) ? ? ? ? ? ? (a t | s t )da t ? ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) ? ? = E s 0:t ,a 0:(t -1) ? ? ? ? ? ? ? ? (a t | s t )da t =1 ?? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) ? ? ? ? = 0.<label>(38)</label></formula><p>Thus, 1 can be simplified. We have</p><formula xml:id="formula_61">1 = E ? ? ? t t =0 ? ? log ? ? (a t | ? t ) ? t=0 ? t R(s t , a t ) t k=0 ? ? log ? ? (a k | s k ) ? ? = E ? ? ? ? t=0 ? t R(s t , a t ) t t =0 ? ? log ? ? (a t | ? t ) t k=0 ? ? log ? ? (a k | s k ) ? ? . (<label>39</label></formula><formula xml:id="formula_62">)</formula><p>Now we can bound 1 easily. That is,</p><formula xml:id="formula_63">1 ? E ? ? ? ? t=0 ? t |R(s t , a t )| t t =0 ? ? log ? ? (a t | ? t ) 2 ? ? ? E ? ? ? ? t=0 ? t |R(s t , a t )| t t =0 ? ? log ? ? (a t | ? t ) 2 ? ? ? E ? G 2 R max ? t=0 (t + 1) 2 ? t ? 2G 2 R max (1 -?) 3 (40)</formula><p>where the third line is obtained by using |R(s t , a t )| ? R max and ? ? log ? ? (a t | s t ) ? G from Assumption 4.1; the last line is obtained by Lemma A.2.</p><p>Finally, combining the bounds of 1 and 2 yields the lemma's claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Lemma 4.3</head><p>In this section, we aim to prove Lemma 4.3. It is beneficial to first show that J is Lipschitz. </p><formula xml:id="formula_64">(i) ? m J(?) is L g -Lipschitz continuous ;</formula><p>(ii) The gradient estimator is bounded, i.e. ? m J(?) ? ? g .</p><formula xml:id="formula_65">(iii) J(?) is ?-Lipschitz, namely ?J(?) ? ? with ? = GRmax (1-?) 2 . Furthermore, if ? m J(?) is a REINFORCE gradient estimator, then L g = HF Rmax 1-? and ? g = HGRmax 1-? ; if ? m J(?) is a GPOMDP gradient estimator, then L g = F Rmax</formula><p>(1-?) 2 and ? g = ?.</p><p>The results with GPOMDP gradient estimator were already proposed in Proposition 4.2 in <ref type="bibr">(Xu et al., 2020b)</ref>. We include them for the completeness of the properties of a general vanilla policy gradient estimator.</p><p>Proof. To prove (i), let ? m J(?) be a REINFORCE gradient estimator. From (4), we have</p><formula xml:id="formula_66">? ? m J(?) = 1 m m i=1 H-1 t=0 H-1 t =0 ? t R(s i t , a i t ) ? 2 ? log ? ? (a i t | s i t ) ? 1 m m i=1 H-1 t =0 ? t R(s i t , a i t ) H-1 t=0 ? 2 ? log ? ? (a i t | s i t ) ? HF R max H-1 t =0 ? t ? HF R max 1 -? ,<label>(41)</label></formula><p>where the third line is obtained by using R(s i t , a i t ) ? R max and ? 2 ? log ? ? (a i t | s i t ) ? F from Assumption 4.1. In this case, L g = HF Rmax 1-? . Let ? m J(?) be a GPOMDP gradient estimator. From (6), we have</p><formula xml:id="formula_67">? ? m J(?) = 1 m m i=1 H-1 t=0 ? t R(s i t , a i t ) t k=0 ? 2 ? log ? ? (a i k | s i k ) ? 1 m m i=1 H-1 t=0 ? t R(s i t , a i t ) t k=0 ? 2 ? log ? ? (a i k | s i k ) ? F R max H-1 t=0 (t + 1)? t Lemma A.1 ? F R max (1 -?) 2 , (<label>42</label></formula><formula xml:id="formula_68">)</formula><p>where similarly, the third line is obtained by using R(s</p><formula xml:id="formula_69">i t , a i t ) ? R max and ? 2 ? log ? ? (a i k | s i k ) ? F from Assumption 4.1. In this case, L g = F Rmax (1-?) 2 .</formula><p>The proof for (ii) is verbatim. We simply replace ? ?</p><formula xml:id="formula_70">m J(?) by ? m J(?), ? 2 ? log ? ? (a i t | s i t ) by ? ? log ? ? (a i t | s i t ) and F by G. If ? m J(?) is a REINFORCE gradient estimator, we have ? g = HGRmax 1-? ; if g(? | ?) is a GPOMDP gradient estimator, then ? g = GRmax (1-?) 2 . To prove (iii), notice that ?J(?) (5) = E ? ? t=0 t k=0 ? ? log ? ? (a k | s k ) ? t R(s t , a t ) ? E ? ? t=0 t k=0 ? ? log ? ? (a k | s k ) ? t |R(s t , a t )| ? E ? GR max ? t=0 (t + 1)? t Lemma A.1 = GR max (1 -?) 2 ,<label>(43)</label></formula><p>where similarly, the third line is obtained by using</p><formula xml:id="formula_71">|R(s t , a t )| ? R max and ? ? log ? ? (a k | s k ) ? G from Assumption 4.1. Thus, ?J(?) ? ? with ? = GRmax (1-?) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. The proof</head><p>Now we show the proof of Lemma 4.3.</p><p>Proof. We denote g(? | ?) a stochastic gradient estimator of one single sampled trajectory ? . Thus ? m J(?) = 1 m m i=1 g(? i | ?). Both ? m J(?) and g(? | ?) are unbiased estimators of J H (?). We have</p><formula xml:id="formula_72">E ? m J(?) 2 = E ? ? 1 m m-1 i=0 g(? i | ?) 2 ? ? = E ? ? 1 m m-1 i=0 g(? i | ?) -?J H (?) + ?J H (?) 2 ? ? = ?J H (?) 2 + E ? ? 1 m m-1 i=0 (g(? i | ?) -?J H (?)) 2 ? ? = ?J H (?) 2 + 1 m 2 m-1 i=0 E g(? i | ?) -?J H (?) 2 = ?J H (?) 2 + E g(? 1 | ?) 2 -?J H (?) 2 m ? ?J H (?) 2 + ? 2 g -?J H (?) 2 m ,<label>(44)</label></formula><p>where the third and the fourth lines are obtained by ?J H (?) = E [g(? i | ?)] , and the last line is obtained by Lemma D.1 (ii).</p><p>If ? m J(?) is a REINFORCE gradient estimator, then ? g = HGRmax</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-?</head><p>; if ? m J(?) is a GPOMDP gradient estimator, then ? g = GRmax</p><p>(1-?) 2 . By rearranging, we obtain the lemma's claim. </p><formula xml:id="formula_73">? ?J H (?) ? D ? H ? D GR max (1 -?) 2 ? H ,<label>(46)</label></formula><p>where the last line is obtained by Lemma D.1 (iii). Thus D = D GRmax (1-?) 2 .</p><p>F. Proof of Corollary 4.5</p><p>Proof. From Lemma 4.2, we know that J is L-smooth. Consider policy gradient with a mini-batch sampling of size m. From Lemma 4. and a mini-batch sampling of size m. From (23), we have</p><formula xml:id="formula_74">E ?J(? U ) 2 ? 2? 0 ?T 2 -L? 1 -1 m + L? 2 g ? m 2 -L? 1 -1 m + 2D 3 -L? 1 -1 m 2 -L? 1 -1 m + D 2 ? H ? H ? 2? 0 ?T + L? 2 g ? m + 6D + D 2 ? H ? H ,<label>(47)</label></formula><p>where the second inequality is obtained by</p><formula xml:id="formula_75">1 2-L?(1-1 m ) ? 1 with ? ? 0, 1 L(1-1 m ) .</formula><p>To get E ?J(? U ) 2 = O( 2 ), it suffices to have</p><formula xml:id="formula_76">O( 2 ) ? 2? 0 ?T + L? 2 g ? m<label>(48)</label></formula><p>and</p><formula xml:id="formula_77">O( 2 ) ? 6D + D 2 ? H ? H<label>(49)</label></formula><p>respectively. To make the right hand side of (49) smaller than 2 , we need H? H = O( 2 ). Thus, we require</p><formula xml:id="formula_78">H = O log 1 / log 1 ? .</formula><p>To make the right hand side of (48) smaller than 2 , we require</p><formula xml:id="formula_79">L? 2 g ? m ? 2 2 ?? ? ? 2 m 2L? 2 g .<label>(50)</label></formula><p>Similarly, for the first term of the right hand side of (48), we require</p><formula xml:id="formula_80">2? 0 ?T ? 2 2 ?? 4? 0 2 T ? ?.<label>(51)</label></formula><p>Combine the two inequality, we get</p><formula xml:id="formula_81">4? 0 2 T ? ? ? 2 m 2L? 2 g . (<label>52</label></formula><formula xml:id="formula_82">)</formula><p>This implies</p><formula xml:id="formula_83">T m ? 8? 0 L? 2 g 4 . (<label>53</label></formula><formula xml:id="formula_84">)</formula><p>The condition on the step size ? ? 0,</p><formula xml:id="formula_85">1 L(1-1 m )</formula><p>requires the mini-batch size satisfy</p><formula xml:id="formula_86">2 m 2L? 2 g &lt; 1 L 1 -1 m =? m ? 2? 2 g 2 .</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>D. 1 .</head><label>1</label><figDesc>Lipschitz continuity of J(?) Lemma D.1. If Assumption 4.1 holds, for any m trajectories ? i and ? ? R d , we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>3, we have Assumption 3.2 holds with A = 0, B = 1 -1 m and C = ? 2 g /m. Assumption 3.3 is verified as well by Lemma 4.4 with appropriate D and D . By Proposition 3.4, plugging A = 0, B = 1 -1 m and C = ? 2 g /m in (13) yields the corollary's claim with step size ? ? 0,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where the third line is obtained by using|R(s t , a t )| ? R max and ? ? log ? ? (a k | s k ) ? G from Assumption 4.1. Thus D = GR max .Next, by inequality of Cauchy-Swartz we have| ?J H (?), ?J H (?) -?J(?) | ? ?J H (?) ?J H (?) -?J(?)</figDesc><table><row><cell>Proof. From (5), we have</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>t</cell><cell></cell></row><row><cell>?J(?) -?J H (?)</cell><cell cols="4">= ? ? E ? t=H k=0 ? E ? GR max (t + 1)? t</cell></row><row><cell></cell><cell></cell><cell cols="2">t=H</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">?</cell><cell></cell></row><row><cell></cell><cell>=</cell><cell>GR max ? H</cell><cell cols="2">(t + 1 + H)? t</cell></row><row><cell></cell><cell></cell><cell cols="2">t=0</cell><cell></cell></row><row><cell></cell><cell>Lemma A.1 =</cell><cell>1 (1 -?) 2 +</cell><cell>H 1 -?</cell><cell>GR max ? H ,</cell><cell>(45)</cell></row><row><cell cols="3">1 (1-?) 2 + H 1-? (11)</cell><cell></cell><cell></cell></row></table><note><p>E. Proof of Lemma 4.4 ? log ? ? (a k | s k ) ? t R(s t , a t ) ? E ? ? t=H ? t |R(s t , a t )| t k=0 ? ? log ? ? (a k | s k )</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>While Khaled &amp; Richt?rik (2020)  refers to this assumption as expected smoothness, we prefer the alternative name ABC to avoid confusion with the smoothness of J.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In the proof in Sect. C,Xu et al. (2020b)  rely on the step? 2 ? J(?) = E? [? ? g(? | ?)],which is not correct since the operators ? ? and E [?] are not commutative in this case as the density p(? | ?) of E [?] depends on ? as well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>= O( 2 ). Here the total sample complexity is T m ? H = O( -4 ).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the theory of policy gradient methods: Optimality, approximation, and distribution shift</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">98</biblScope>
			<biblScope unit="page" from="1" to="76" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Infinite-horizon policy-gradient estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.806</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<idno type="ISSN">1076-9757</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="319" to="350" />
			<date type="published" when="2001-11">Nov 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">First-Order Methods in Optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philadelphia</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>SIAM-Society for Industrial and Applied Mathematics</publisher>
			<pubPlace>PA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimization methods for large-scale machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<idno type="DOI">10.1137/16M1080173</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<idno type="ISSN">0036-1445</idno>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="311" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spider: Nearoptimal non-convex optimization via stochastic pathintegrated differential estimator</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="689" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global convergence of policy gradient methods for the linear quadratic regulator</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mesbahi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on optimization</title>
		<idno type="ISSN">1052-6234</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentumbased policy gradient methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A natural policy gradient</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Better theory for sgd in the nonconvex world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richt?rik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Actor-critic algorithms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>M?ller</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent for nonconvex learning without bounded gradient assumptions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2019.2952219</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4394" to="4400" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Basar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7624" to="7636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the global convergence rates of softmax policy gradient methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SARAH: A novel method for machine learning problems using stochastic recursive gradient</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tak??</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic variance-reduced policy gradient</title>
		<author>
			<persName><forename type="first">M</forename><surname>Papini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Binaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Canonaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4026" to="4035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A hybrid stochastic policy gradient algorithm for reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tran-Dinh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Calandra</surname></persName>
		</editor>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020-08">Aug 2020</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="26" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The impact of neural network overparameterization on gradient confusion and stochastic gradient descent</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Sankararaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fast convergence of stochastic gradient descent under a strong growth condition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">Jul 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hessian aided policy gradient</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unified optimal analysis of the (stochastic) gradient method</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>M?ller</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<meeting>the Twenty-Second International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2019-04">Apr 2019</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="16" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An improved convergence analysis of stochastic variance-reduced policy gradient</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 35th Uncertainty in Artificial Intelligence Conference</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Gogate</surname></persName>
		</editor>
		<meeting>The 35th Uncertainty in Artificial Intelligence Conference</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sample efficient policy gradient methods with recursive variance reduction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sample efficient reinforcement learning with reinforce</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Variational policy gradient method for reinforcement learning with general utilities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4572" to="4583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On the convergence and sample efficiency of variancereduced policy gradient method</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
