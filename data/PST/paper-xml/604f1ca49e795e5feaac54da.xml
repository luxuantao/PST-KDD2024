<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Aus-tralian Artificial Intelligence Institute</orgName>
								<orgName type="laboratory">ReLER Lab</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<postCode>2007</postCode>
									<settlement>Syd-ney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
							<email>zhulinchao7@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310007</postCode>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hehe</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310007</postCode>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310007</postCode>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
							<email>iexumingliang@zzu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310007</postCode>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TMM.2021.3057503</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A CTION recognition in videos [1]- [6] has attracted increas- ing interests in computer vision researches. A major thrust for this problem is to develop discriminative features to capture both the static information and motion information from videos, which has been applied to other potential tasks such as question answering <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, skeleton-based action recognition <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, video summarization, video indexing <ref type="bibr" target="#b10">[11]</ref>.</p><p>First, typical video pooling methods treat each local feature equally. However, as videos are temporally ordered sequences, the same frame-level representations may have different semantic meanings in a different context. For example, in Fig. <ref type="figure">1</ref>, frame Fig. <ref type="figure">1</ref>. A sequence of frames showing a man doing push-ups. Frame (a) and frame (e) are similar in appearance but have different semantic meanings.</p><p>(a) and (e) are almost the same in appearance. However, when treated as a sequence, frame (a) means the man starts to push up, while frame (e) means he just did a push-up. The encoded representation should be capable of reflecting the semantic difference. Mean pooling and Vector of Locally Aggregated Descriptors (VLAD) encoding <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> fail to take the sequence order into account, ignoring sequence order information during the aggregation procedure. In the typical video classification models based on Recurrent Neural Networks (RNN) <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, e.g., Long Short-Term Memory (LSTM) <ref type="bibr" target="#b16">[17]</ref>, Gated Recurrent Unit (GRU) <ref type="bibr" target="#b17">[18]</ref>, convolutional features are first fed into RNN, and aggregation models are directly applied to the outputs of RNNs. In these models, the state of the last step has seen the whole video, while the state of the first step only observes a single frame at the beginning of the video. In this way, the output features are not encoded equally. It introduces a prior importance weight to the aggregation model, i.e., later outputs are more important than the previous outputs. To generate features that are from the same distributions as well as encoding the temporal context, we borrow an unsupervised context reconstruction block from <ref type="bibr" target="#b18">[19]</ref>. We use the same structure, but extend it by training the reconstruction loss jointly with the supervised loss and considering neighboring frames. This module learns to reconstruct neighboring frames for better context exploration. The final encoded context-aware features at each step are of equal importance.</p><p>Second, multi-scale features from convolutional layers are not well exploited. Xu et al. <ref type="bibr" target="#b11">[12]</ref> found that fusing the features of different layers can boost the classification performance. They simply used late fusion to aggregate the features from different layers. However, the features from different layers do not influence each other during training. It only takes single scale features at the aggregation stage. To effectively leverage cross-layer features from multiple scales, we propose to learn to aggregate features from different layers in a unified way. We first learn to decide the weight of each layer, where the features from different layers will be assigned with a weight to identify its significance. The weighted features will be further aggregated with a single aggregation model. We introduce a center-guided attention to extract shared knowledge from different videos. It can thus generate a single global representation that aggregates cross-layer features.</p><p>In this paper, we make the following contributions. First, we propose a Temporal Cross-Layer Correlation network to uncover local and global structures from video data. It enables context exploration and better facilitates temporal feature learning.</p><p>Second, we propose a cross-layer attention method and a center-guided attention method to leverage the features from multi-level granularity in a unified way. Thus, the features from different scales can benefit each other to learn a more discriminative representation.</p><p>Third, our evaluation on three datasets shows the effectiveness of the proposed method. Our extensive studies demonstrate that our Temporal Cross-Layer Correlation (TCLC) network is effective for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Action recognition has been extensively studied and we briefly review recent advances in this field. One of the most successful hand-craft features for action recognition is improved Dense Trajectories (iDT). Dense Trajectories <ref type="bibr" target="#b0">[1]</ref> and improved Dense Trajectories <ref type="bibr" target="#b1">[2]</ref> were introduced for action recognition with considerable performance boost over other features. These hand-craft features firstly utilize dense sampled interest points to track objects. Along with the trajectories cubic, local descriptors such as Histogram of Oriented Gradients (HOG), Histogram of Optical Flow (HOF) and Motion Boundary Histogram (MBH) are used to describe the spatio-temporal grids. These methods utilize trajectories tracking but requires complex warping and optical flow calculation, and it is computationally intensive. Aggregation methods were used to encode local descriptors and the Fisher Vector encoded iDT feature outperforms bag-of-words method significantly. In these tasks, recurrent neural networks have been widely deployed to generate sentences based on the visual content.</p><p>On a par with the success of Dense Trajectories, Convolutional Neural Networks (ConvNets) have been brought into the field of video modeling, motivated by its tremendous success on image analysis, such as object recognition <ref type="bibr" target="#b19">[20]</ref>. Recent progress on action recognition has shown that proper exploitation of ConvNets is possible to improve the classification performance. Specially, Simonyan and Zisserman <ref type="bibr" target="#b2">[3]</ref> introduced two-stream ConvNets which are composed of two convolutional networks for action recognition. The spatial network captures the spatial appearance of objects and humans, and the temporal network models the motions by feeding stacked optical flow to the neural networks directly. By fusing these two networks, it can match the performance of improved Dense Trajectories. With the emergence of large-scale video datasets like Kinetics <ref type="bibr" target="#b20">[21]</ref>, 3D convolution neural networks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref> have shown strong performance in spatio-temporal representation learning. <ref type="bibr" target="#b5">[6]</ref> used inflated 3D convolution from 2D convolution, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref> decomposited 3D convolution into a 2D spatial convolution and a 1D temporal convolution. Equipped with 3D convolution, using only the RGB stream can outperform frame-based two-stream methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Tran et al. <ref type="bibr" target="#b26">[27]</ref> tackled action recognition with a spatio-temporal feature learning method using a deep 3D convolutional network and demonstrated that an architecture with 3 × 3 × 3 convolution kernels of the best performing architecture. <ref type="bibr" target="#b25">[26]</ref> designed a cheap network for efficient video classification. Carreira et al. <ref type="bibr" target="#b5">[6]</ref> introduced I3D to adapt the Inception-V1 architecture and inflates the 2D filter to 3D filters. Qiu et al. <ref type="bibr" target="#b24">[25]</ref> replaced the 3D convolution with spatial and temporal convolutions using a residual connection style. Similarly, Tran et al. <ref type="bibr" target="#b22">[23]</ref> decomposed spatial and temporal convolution with a (2+1)D block, which can double the number of non-linearities. Xie et al. <ref type="bibr" target="#b23">[24]</ref> used a similar approach which replaces some 3D convolutions with 2D convolutions. However, these methods grasp average pooling to get the global video representation which would potentially hurt the performance. Our proposed approach can be readily plugged into these structures to generate more discriminative representations. Besides, Liu et al. <ref type="bibr" target="#b27">[28]</ref> introduce a temporal adaptive module that learns local importance weights to adaptively enhance local location and utilizes global kernel weights to adaptive aggregate video-level features. Zhang et al. <ref type="bibr" target="#b28">[29]</ref> introduce a 4D convolution network to aggregate video-level features. These studies focus on learning from the convolutional layers while our method introduces cross-layer aggregation using recurrent layers.</p><p>Zhao et al. <ref type="bibr" target="#b29">[30]</ref> encoded frame sequences via a visual dictionary to generate a sequence of high-level semantics for self-supervised video feature learning. Leveraging the attention mechanism for videos has been studied by <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Dong et al. <ref type="bibr" target="#b14">[15]</ref> leveraged three modalities to learn a spatio-temporal attention for video segment aggregation. These methods ignored the multi-scale structure of convolutional features. We introduce a cross-layer attention for learning features from different layers.</p><p>Some works have been conducted on encoding deep features to a global representation. <ref type="bibr" target="#b11">[12]</ref> showed that VLAD encoded ConvNet activations can significantly boost the performance compared with average pooling in event detection. Ac-tionVLAD <ref type="bibr" target="#b12">[13]</ref> has been used for video data modeling by leveraging learnable VLAD encoding. These methods regard videos as individual clips, without considering that neighboring clips are related. Recurrent neural networks have been developed in modeling sequential data <ref type="bibr" target="#b18">[19]</ref>. However, they ignored that the neighboring clips are more related. <ref type="bibr" target="#b30">[31]</ref> also found that the similarity between two samples is related to the distribution of surrounding samples and labels. In this paper, we aggregate context-aware and cross-layer video features. We use a reusable context prediction block for video temporal modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TEMPORAL CROSS-LAYER CORRELATION FOR ACTION RECOGNITION</head><p>Our framework is composed of two modules. The first module is an RNN-based context reconstruction network which maps ordered inputs to a context space where sequence order and sequence context are considered. The second module is the attention module which reweights the residuals (cross-layer attention) and leverages hidden centers/descriptors from different layers (center-guided attention). The entire system is a unified network which can be trained end-to-end and ensures the final feature, i.e., the output of the attention layer to be discriminative. To train the network effectively for the action recognition task, we also propose a simple data augmentation method which can suppress overfitting for small video datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Local Correlated Neighborhood Reconstruction</head><p>As previously described, video sequences are context relevant. We follow the RNN training described in <ref type="bibr" target="#b18">[19]</ref> to train the context reconstruction. Instead of training reconstruction module in separate stage <ref type="bibr" target="#b18">[19]</ref> (i.e., unsupervised context reconstruction and supervised fine-tuning), we extend it and incorporate the reconstruction module into a joint end-to-end learning framework. The original context-reconstruction <ref type="bibr" target="#b18">[19]</ref> considered the reconstruction at the sequence-level, e.g., use a past sequence to reconstruct the future sequence. We consider the reconstruction of more correlated neighboring frames. Specifically, given a sequence X = {x 1 , x 2 , . . . , x n }, the RNN hidden state at time step t is defined as h t = σ(h t−1 , x t ), where σ is a nonlinear activation function. It defines a recurrent hidden state whose activation at each time step is dependent on the previous time steps. Long Short-Term Memory (LSTM) <ref type="bibr" target="#b16">[17]</ref> and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b17">[18]</ref> are two popular architectures in modeling language and both of them have been used to model video data <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Following <ref type="bibr" target="#b18">[19]</ref>, we use the GRU architecture as it has shown similar performance to LSTM but with only two gates (update gate and reset gate) and no memory cells. The GRU can be applied to a sequence of inputs with variable lengths. The next state h t is computed by,</p><formula xml:id="formula_0">r t = σ(W xr x t + W hr h t−1 ),<label>(1)</label></formula><formula xml:id="formula_1">z t = σ(W xz x t + W hz h t−1 ),<label>(2)</label></formula><formula xml:id="formula_2">ht = tanh(W x hx t + W h h(r t h t−1 )),<label>(3)</label></formula><formula xml:id="formula_3">h t = (1 − z t ) h t−1 + z t ht , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where x t is the input, r t is the reset gate, z t is the update gate, ht is the interval state, h t is the proposed state and is elementwise multiplication. W * * are learnable weights for training. In order to model the context, one GRU is trained to predict the next frame and another GRU is enabled to predict the previous frame which are combined together to form the bi-directional network <ref type="bibr" target="#b18">[19]</ref>. This bi-direction process forces the encoded representation to be aware of the future frame as well as the past frame. Specifically, given the input x t at time step t, the model is asked to predict x t+1 and x t−1 , respectively. x t is the features extracted from convolutional layers. <ref type="bibr" target="#b13">[14]</ref> used LSTM to perform classification directly, we instead introduce this prediction block before the classification. As the learned features contain more information than the original feature, we can obtain more expressive representation for the classification task. Following <ref type="bibr" target="#b18">[19]</ref>, we choose the huber loss for reconstruction,</p><formula xml:id="formula_5">(x, x) = ⎧ ⎪ ⎨ ⎪ ⎩ 1 2 (x − x) 2 for |x − x| ≤ δ, δ |x − x| − 1 2 δ 2 otherwise.</formula><p>(5)</p><p>x is the target vector and x is the predicted vector. We set δ = 0.5 in all experiments. xt is calculated by W o h t , where W o is the output linear projection matrix. The output of this prediction block is computed by,</p><formula xml:id="formula_6">o t = concat(x t + h t fw , x t + h t bw ).<label>(6)</label></formula><p>We stop the gradient to be backpropagated to lower layers at h t fw and h t bw . In this way, the gradient of the classification loss will not affect the prediction block, which can stabilize the training procedure. A shortcut connection <ref type="bibr" target="#b19">[20]</ref> is introduced to connect the input features and the output features directly, which leverages the bottom convolutional layer features and represents local information about the current frame. The architecture is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross-Layer Attention</head><p>The idea of aggregating features from multiple convolutional layers has been used in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. These works focus on the task of segmentation and edge detection, and they do not explicitly model the layer importance. We introduce a layer importance factor that considers the relations between layers before cross-layer fusion. We use the features from different convolutional layers as the inputs to the context-aware reconstruction block. The output of the reconstruction block can be denoted as O i , where i is the index of the convolutional layer. The output sequences {O 1 , O 2 , ..., O l } are then fed to the cross-layer attention module.</p><p>We first decide the importance weight for each layer. Then, all the features are aggregated together with the attention model. The layer importance weight β can be calculated by,</p><formula xml:id="formula_7">m i = 1 n n k=1 o k i ,<label>(7)</label></formula><formula xml:id="formula_8">q = W qp concat(m i ),<label>(8)</label></formula><formula xml:id="formula_9">β i = exp(m i , q)) l i=1 exp(m i , q)) , (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>Authorized licensed use limited to: BEIHANG UNIVERSITY. Downloaded on November 14,2022 at 08:12:46 UTC from IEEE Xplore. Restrictions apply. where m i is the mean vector of outputs from convolutional layer i, W qp is the learnable weight, q is the global vector aggregated all information from different layers, β i is the final weight for layer i. The outputs from different layers are first projected to the same dimension using a linear layer. Then, the outputs are weighted by β.</p><formula xml:id="formula_11">ōk i = β i W ip o k i . (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>The overall framework is shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Center-Guided Attention</head><p>The classical VLAD encoding method has been found useful for video aggregation <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Given a set of inputs X = {x 1 , x 2 , . . ., x T } and centers C = {c 1 , . . ., c K }. We denote a set of inputs as X = {x 1 , x 2 , . . ., x T } and centers generated by the k-means clustering on X as C = {c 1 , . . ., c K }, where T denotes the length of input sequence and K denotes the number of centers. T does not necessarily need to be the same as K. VLAD encodes the inputs as follows,</p><formula xml:id="formula_13">u k = i:NN(x i )=c k x i − c k , (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>where x i is assigned to the center c k if NN(x i ) = c k and NN(•) is a nearest neighbor operation which is practically the unsupervised k-means algorithm. Concatenating u k over all K centers, we can get the feature vector of size DK where D is the dimension of x i . Several normalization methods are proposed to yield better performance. Power normalization, often signed square rooting (SSR), is usually used to convert each element into sign(x i ) |x i | form. The intra-normalization method normalizes representations for each center, followed by the 2 normalization for the whole feature vector. The assignment between the descriptor x i and the c k can be denoted as α ik , where we have α ik = 1 if x i 's nearest center is c k and α ik = 0 otherwise. We have K k=1 α ik = 1 if only one nearest neighbor is considered.</p><p>Girdhar et al. <ref type="bibr" target="#b12">[13]</ref> proposed to learn the assignment with a softmax layer. The assignment is derived from input x. In this paper, we propose to soften the hard VLAD weight assignment α ik with the soft attention mechanism <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. The soft attention mechanism has become a powerful method in image caption generation <ref type="bibr" target="#b33">[34]</ref>, and has also been extensively used in video analysis <ref type="bibr" target="#b14">[15]</ref>. Visual attention has also been used in image recognition, where the neural network learns to pay attention to parts of the image for detection and recognition. It mainly utilizes the softmax operation which can be formulated as,</p><formula xml:id="formula_15">e(x, h) = tanh(Wx + h),<label>(12)</label></formula><formula xml:id="formula_16">α i ({x i , . . ., x N }, h) = exp(e(x i , h)) N j=1 exp(e(x j , h)) ,<label>(13) x</label></formula><formula xml:id="formula_17">= T i=1 α i (X, h)x i , (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>where the x is the weighted average over {x 1 , x 2 ,..., x T } and the weights are computed by the function of x i and the state h.</p><p>For the captioning or machine translation tasks, the state h is usually the decoder output. Different from the unsupervised k-means training, we treat the centers as hidden variables within a unified neural network framework and learn the centers during backpropagation training process. In this paper, we simply regard centers as variables that are independent to the inputs and any distributions with a prior assumption.</p><p>To assign the weights between the centers and the descriptors, we find that there are two methods to set the weights. The first method corresponds to the classical VLAD nearest neighbor assignment, where each input is assigned to centers (Fig. <ref type="figure" target="#fig_3">4</ref> schema (a)). The representation is computed by,</p><formula xml:id="formula_19">u k = T i=1 α k (C, x i )(x i − c k )<label>(15)</label></formula><p>For each input x i , we will have K k=1 α ik = 1. In the second method, the center will weigh across the inputs and assign weights to each of them (Fig. <ref type="figure" target="#fig_3">4</ref> schema (b)), specially,</p><formula xml:id="formula_20">v k = T i=1 α i (X, c k )(x i − c k )<label>(16)</label></formula><p>Authorized licensed use limited to: BEIHANG UNIVERSITY. Downloaded on November 14,2022 at 08:12:46 UTC from IEEE Xplore. Restrictions apply. Fig. <ref type="figure">5</ref>. Illustration of sampling convolutional feature maps as data augmentation. We propose the random sampling method, which generates sequences by randomly selecting a spatial location at each time step. At the testing stage, aligned sampling is used to generate consistent results across different evaluations which samples at the same location for each sequence. For a feature map of size a × a, we sample a × a sequences in total and average representations from sequences to generate the final feature.</p><p>For each center c k , we will have T i=1 α ik = 1. We will compare the effects of both methods in Section IV-C. The output of the aggregation model can be directly fed to a classification layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Video Data Augmentation</head><p>In contrast to the activations from fully connected layers, the ConvNets activations from the lower convolutional layer contain more spatial information. To get the frame-level representation, simple mean pooling of convolutional activations will eliminate the spatial information.</p><p>However, for action recognition on small datasets, there is limited number of labeled instances compared with the image recognition tasks. It could make the modern neural network models overfit easily, for example, there are only 9537 training videos in the UCF-101 split 1 <ref type="bibr" target="#b35">[36]</ref> and 3570 for HMDB-51 split 1 <ref type="bibr" target="#b36">[37]</ref>. We introduce an augmentation approach which randomly samples convolutional activations at each time step (Fig. <ref type="figure">5(d</ref>)), and it mimics the trajectory tracking in a random way. We found that this method can be treated as a data augmentation method for video data. At the testing stage, we sample with the aligned sampling method (Fig. <ref type="figure">5(c</ref>)) which will output deterministic results across different evaluations. It samples the activations at the same spatial location on feature maps (Fig. <ref type="figure">5(c)</ref>). The final representation is obtained by averaging representations of the sampled sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we show the effectiveness of our architecture and compare it with a set of carefully designed baselines and other video classification models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Our experiments are conducted on three commonly used action recognition datasets. The UCF-101 <ref type="bibr" target="#b35">[36]</ref> dataset consists of 13 320 realistic action videos in 101 categories, which are collected from YouTube. We compare the effectiveness of our framework using the first split and report the mean average accuracy over three splits. The HMDB-51 dataset <ref type="bibr" target="#b36">[37]</ref> consists of 6766 action videos, collected from various sources, and are divided into 51 categories. The mean average accuracy over three splits will also be reported. The Kinetics dataset <ref type="bibr" target="#b20">[21]</ref> is a large human action recognition dataset collected from YouTube. It is a relatively new dataset and contains 400 classes and 240K training examples. The videos are trimmed to be around 10 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>For UCF-101 and HMDB-51, we sample videos at the frame rate of 5 FPS and extract the spatial features from the activations of mixed_7c, mixed_6e, and mixed_5 d layer from the Inception-v3 <ref type="bibr" target="#b37">[38]</ref> single crop model. The Inception-v3 is pre-trained on ImageNet and finetuned on UCF-101. Similarly, for HMDB-51, the Inception-V3 is first pre-trained on ImageNet and then fine-tuned on HMDB-51. For the Kinetics dataset, we extract pre-trained features from the I3D architecture <ref type="bibr" target="#b5">[6]</ref>. We use the 32-frame clip as the input to extract the clip-level features, using the activations from Mixed_5c, Mixed_4f and Mixed_3c. Before feeding the convolutional activations directly to GRU, we apply dimension reduction to 256 followed by Batch Normalization. For Kinetics, we additionally average across the temporal dimension features. Specifically, we use the architecture of FC256-BN-ReLU-FC256-BN and these layers are shared among time steps. Two layers of GRUs with cell size of 256 are stacked for sequence mapping, while no dropout is used between the GRU layers. We initialize the centers with normal distribution of standard deviation 0.001 and the other weights are initialized with Xavier uniform initialization. We use the batch size of 128 and optimize with ADAM optimizer at the learning rate of 0.0001. We clip the gradient of the GRU weights at the norm of 5. We apply fixed weight decay of 0.0005 to weight matrices. The evaluations are performed using a moving average of all weights <ref type="bibr" target="#b37">[38]</ref>,</p><formula xml:id="formula_21">W i+1 = decay × W i + (1 − decay) × w (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>where W i is the weight which will be used for evaluation and the w is current updated weight. We use decay of 0.999 through  all experiments. The entire network is trained end-to-end with TensorFlow <ref type="bibr" target="#b38">[39]</ref> distributed machine learning system on four NVidia V100 GPU. During context modeling, we used batch size of 128, and the sequence length is 30. The total memory computation is 24 GB × 4, where 24 GB is the memory cost per GPU and 4 is the number of GPUs we used. During inference, we used memory consumption in the same setting is around 15 GB × 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of the TCLC Structure</head><p>We now analyze the effectiveness of our method quantitatively. We will evaluate three key components of our model by comparing them to several baselines on the first split of UCF-101 and HMDB-51 using RGB only following <ref type="bibr" target="#b2">[3]</ref>. We make other components fixed during comparisons. In all the experiments, we utilize power normalization, intra-normalization and 2 normalization to the encoded vector. We set the sequence length to 30 by default. For videos with less than 30 sequences, we pad zeros after the last frame. We randomly sample continuous 30 sequences if the video sequences are more than 30. Without specification, we empirically set the number of centers to 10.</p><p>We first show the result of the mean pooling method in Table I to establish the baselines for the Inception-v3 architecture. For the mean pooling, we average convolutional feature maps across spatial locations to obtain the frame-level feature, and we average the frame features over video sequences to obtain the video level representation which is later classified by a linear SVM classifier. We validate that 2 normalization on global video representation will lead to better classification results <ref type="bibr" target="#b11">[12]</ref> and apply it to our models directly.</p><p>Compared to average pooling (average clip-level predictions from all clips), our method introduces more computational cost due to the implementation of additional blocks. On an NVIDIA V100 GPU with Intel i7 CPU, the total cost of 100 Kinetics videos under the 32-frame setting is 72.8 s, while our method takes 86.5 s.</p><p>1) The Effectiveness of Context Modeling: We show that the bi-directional context modeling improves the classification performance by comparing it with the spatial-only model where temporal context is eliminated. Another baseline that only predicts the next frame is used to show the benefits of modeling context in both the future and the past. To show the effectiveness of neighboring reconstruction, we compare to a sequence-level reconstruction method proposed in <ref type="bibr" target="#b18">[19]</ref>. We use the sequence length to 30 as <ref type="bibr" target="#b18">[19]</ref>. We use the schema (a) described in Section III-C and the random sampling method described in Section III-D for these methods. The result is shown in Table <ref type="table" target="#tab_1">II</ref>.</p><p>As can be seen, the classification result is improved by a large margin with the context modeling method compared with the mean pooling method, especially on the HMDB-51 dataset. Compared to <ref type="bibr" target="#b18">[19]</ref>, our neighboring reconstruction outperforms it by 0.6% on HMDB-51, showing the benefits of neighborhood reconstruction. Note that the feature dimensions of these methods are in the same magnitude. For the mean pooling, the feature dimension is 2048 and the residual attention encoding, in which 10 centers are used, has feature dimension of 2560. In addition, it shows that our context-aware method works well for both datasets and the bi-directional context modeling approach is capable of capturing more information than the method predicts only the future frame.</p><p>2) Comparison With Unsupervised VLAD Encoding: We compare our method with the classical VLAD encoding. To train the unsupervised VLAD encoding, we mainly follow the settings in <ref type="bibr" target="#b11">[12]</ref>. We first apply PCA with whitening to the convolutional activations and use PCA dimension of 256. We utilize VLAD-k and set k to 5. We apply power normalization, intra-normalization and 2 normalization to the encoded vector, as they show good performance in <ref type="bibr" target="#b11">[12]</ref>.</p><p>We first use 10 centers for the unsupervised VLAD encoding and compare it with our approach with default settings. The result is shown in Table <ref type="table" target="#tab_1">III</ref>. As can be seen, our method with 10 center outperforms the unsupervised VLAD with a large margin which shows the discriminative ability of our representation. When we increase the number of centers in the unsupervised VLAD encoding method, the classification result gets better as shown in Table <ref type="table" target="#tab_1">III</ref>. However, more than 100 centers with final feature dimension of more than 20 000 are usually required to achieve comparable performance to our center-guided attention model which has feature dimension around 2000. After 2 normalization, the high dimensional representation becomes rather sparse. Our compact representation is more suitable in the case of video retrieval where large number of video representations Fig. <ref type="figure">6</ref>. The distribution of the softmax activations over time. The x-axis is the iteration number and the y-axis is the value of activations. We use the Ten-sorBoard <ref type="bibr" target="#b38">[39]</ref> to visualize the distributions which shows the percentage of the data in different regions. The line in the center shows the median of the data, the next two lines cover the 68th percentile, the next two lines up show the 95th percentile, the next two lines show the 99.7th percentile, and the final two lines show the 100th percentile. For example, at round 10 k iteration in (c), we can observe that all activations are in [0.0, 1.0], 99.7% are in [0.0, 0.93] and 95% are in [0.0, 0.01]. Best view in need to be stored and queried. At the same time, our model can be plugged to different network architectures for aggregation. Our model with feature dimension of 256 which uses only 1 center can achieve good classification performance. Note that the performance of our model drops when 20 centers are used to aggregate inputs. We suspect it is because of model overfitting.</p><p>3) The Analysis of Assignment Methods: We analyze two assignment schemas described in Section III-C. Schema (a) corresponds to the classical nearest neighbor assignment where the correspondences between the input and the centers are measured. Schema (b), however, means that the center will weigh across inputs and assign weights to each of them.</p><p>We use the random sampling method and for simplicity, we utilize the spatial-only model which directly takes the convolutional activations as inputs to the multi-scale attention layer. We first show the activations of the soft assignment weights for both schemas in Fig. <ref type="figure">6</ref>. As can be seen, these two assignment methods have different meanings. The assignments of both methods start with a uniform weighting. For schema (a), where 10 centers need to be assigned, the weights start with 1 10 (Fig. <ref type="figure">6</ref> bottom). For schema (b), each center needs to weight the 30 inputs and the weight starts with 1 30 (Fig. <ref type="figure">6</ref> top). During the training process, the assignments of the input to the centers in schema (a) gradually pay more attention to a specific center, while for schema (b), the assignments distribute between 0.0 and 0.06, which means that the center is more prone to average the inputs.   The results of both methods in encoding inputs are shown in Table <ref type="table" target="#tab_3">IV</ref>. It shows that schema (a) performs slightly better than schema (b). We will use schema (a) for subsequent experiments as it is a more reasonable structure and has similar behaviour to the classical encoding approach.</p><p>We apply the nearest neighbor loss on schema (a) which guides the model to generate the nearest neighour assignment between the input and the centers. It is computed by</p><formula xml:id="formula_23">NNL = T i=1 K k=1 a ik x i − c k 2 (18)</formula><p>The classification result is shown in Table <ref type="table" target="#tab_3">IV</ref>. It shows that schema (a) with nearest neighbor loss has a better classification result than the one without the nearest neighbor loss. We show the cross-entropy loss during the training on HMDB-51 split 1 in Fig. <ref type="figure" target="#fig_4">7</ref>. It shows that additional nearest neighbor loss will lead to faster convergence and lower classification loss.</p><p>4) Cross-Layer Attention: We evaluate our method with only a single layer output, and a baseline with outputs from three layers. When three layers are utilized, we assign the same weight to each layer's output ("Equal importance"). The results are shown in Table <ref type="table">.</ref> V. The results demonstrate that the benefits of our cross-layer attention in dynamically incorporating activations from multiple layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Convolutional Feature Maps Sampling:</head><p>We show the effectiveness of our data augmentation method by comparing it with the mean sampling and the weighted sampling method.   In this experiment, we use the BiRNN to model the context and by default, we use the schema (a) for residual attention. For the weighted sampling, we utilize the spatial attention method which generates a weighted average representation over inputs and the result is shown in Table <ref type="table" target="#tab_4">VII</ref>.</p><p>During training, we found all these methods overfit on UCF-101 and HMDB-51. However, the random sampling suppresses the overfitting problem and it shows that the random sampling method outperforms the mean sampling method with a large margin. For the spatial weighted sampling, we find it overfits more severely when limited labeled data is provided. This experiment shows that our simple random sampling method is an effective data augmentation method for video modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results and Comparison With Video Classification Models</head><p>We now compare our full model with other video classification models. Mean average accuracy is reported for both UCF-101 and HMDB-51 datasets. The final results are reported using the two-stream network, where optical flow is incorporated via late fusion. The results are shown in Table <ref type="table" target="#tab_7">VIII</ref>. In <ref type="bibr" target="#b5">[6]</ref>, it shows the benefits of pre-training networks' weights on Kinetics for initialization. Our model achieves the best performance on UCF-101 and HMDB-51 dataset, which shows the power of our approach in generating discriminative representation. We outperform ActionVLAD by a large margin, indicating the model's capability in aggregating multi-scale features. We show the results using pre-trained weights on Kinetics in Table <ref type="table" target="#tab_8">IX</ref>. It validates that pre-training is an effective method. Meanwhile, the results demonstrate that our method is promising when a stronger initialization is used.</p><p>We also report the accuracy on the validation set of Kinetics (Table <ref type="table">X</ref>). On the Kinetics dataset, it also shows an improvement over the I3D baseline. Our results are also competitive to other video classification models. To fairly compare to other state-of-the-art methods, we evaluate three TCLC models which consist of a 16-frames model, a 32-frames model, and a 64-frames model. Compared to SlowFast <ref type="bibr" target="#b46">[47]</ref>, we obtain a comparable result to it under the 64-frames setting. This result demonstrates the effectiveness of our method in modeling temporal correlations. SlowFast with the 128-frames setting uses ResNet-101 as the backbone and it leverages additional non-local block. To evaluate the results on 128-frame, we extract the features from "SlowFast (128-frames, ResNet-101)". Our method outperforms "SlowFast (128-frames, ResNet-101)" by 0.3% on Kinetics-400. Our 32-frames model also outperforms Non-local Net (32-frames) under the same backbone, while our 64-frames TCLC achieves comparable results to Non-local Net with 128-frames. This demonstrates that our TCLC is capable of modeling temporal correlations for better action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have proposed a unified temporal correlated neighborhood exploration framework to encode sequences into a compact and discriminative representation. The video sequence context is considered and modeled with an RNN by predicting the next frame and the previous frame, respectively. We introduce a novel cross-layer attention and a center-guided attention mechanism to aggregate features from multiple scales. We showed the benefits of our three key components by comparing them with a set of baselines and the results confirmed their superiority. Our full model achieved competitive performance among the state-of-the-art models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Temporal Cross-Layer Correlation Mining for Action Recognition Linchao Zhu , Hehe Fan , Yawei Luo, Mingliang Xu , Member, IEEE, and Yi Yang , Member, IEEE Abstract-Neighboring frames are more correlated compared to frames from further temporal distances. In this paper, we aim to explore the temporal correlations among neighboring frames and exploit cross-layer multi-scale features for action recognition. First, we present a Temporal Cross-Layer Correlation (TCLC) framework for temporal correlation learning. The unified framework uncovers both local and global structures from video data, enabling a better exploration of temporal context and assisting cross-layer spatio-temporal feature learning. Second, we propose a novel cross-layer attention and a center-guided attention mechanism to integrate features with contextual knowledge from multiple scales. Our method is a two-stage process for effective cross-layer feature learning. The first stage incorporates the crosslayer attention module to decide the importance weight of the convolutional layers. The second stage leverages the center-guided attention mechanism to aggregate local features from each layer for the generation of a final video representation. We leverage global centers to extract shared semantic knowledge among videos. We evaluate TCLC on three action recognition datasets, i.e., UCF-101, HMDB-51 and Kinetics. Our experimental results demonstrate the superiority of our proposed temporal correlation mining method. Index Terms-Deep learning, video feature learning, video classification, action recognition, frame correlation mining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The local correlated neighborhood reconstruction process. The input features are encoded to predict the future feature and the past feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Our unified framework for aggregating video frame sequences. The convolutional activations are pushed into the context-aware reconstruction block to predict the next frame and the previous frame. The outputs of the reconstruction block are encoded by the cross-layer attention block, which decides the weight for different layers. The network can be trained end-to-end through backpropagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Two methods to assign the weights. Schema (a) traverses each center, and we have K k=1 α ik = 1. Schema (b) traverses each input, and we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a): Cross entropy loss changes over time. When nearest neighour loss is enabled, it will converge faster and have lower classification loss. (b) Validation accuracy over time.</figDesc><graphic url="image-2.png" coords="7,47.03,66.65,234.50,235.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I RESULT</head><label>I</label><figDesc>OF MEAN POOLING ON CONVOLUTIONAL FEATURE MAPS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF TWO ASSIGNMENT METHODS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V THE</head><label>V</label><figDesc>EFFECTIVENESS OF CROSS-LAYER ATTENTION. "EQUAL IMPORTANCE" DENOTES THE ASSIGNED WEIGHTS TO EACH LAYER'S OUTPUT ARE THE SAME</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII COMPARISON</head><label>VIII</label><figDesc>WITH MODELS USING MEAN AVERAGE ACCURACY OVER THREE SPLITS ON UCF-101 AND HMDB-51 DATASETS. NOTE WE DO NOT USE KINETICS PRE-TRAINED WEIGHTS IN THIS SETTING</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc>WITH MODELS ON UCF-101 AND HMDB-51 USING KINETICS PRE-TRAINED WEIGHTS. ONLY RGB INPUTS ARE USED IN THIS SETTING. "*" INDICATES THAT OPTICAL FLOW INFORMATION IS USED TABLE X COMPARISON WITH MODELS ON KINETICS DATASET</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: BEIHANG UNIVERSITY. Downloaded on November 14,2022 at 08:12:46 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-stream multi-class fusion of deep networks for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
				<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling spatialtemporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
				<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond text QA: Multimedia answer generation by harvesting web information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="426" to="441" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimedia question answering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="72" to="78" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusing geometric features for skeleton-based action recognition using multilayer LSTM networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2330" to="2343" />
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical soft quantization for skeleton-based human action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2020.2990082</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia, p. 1</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive video indexing with statistical active learning</title>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="27" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A discriminative CNN video representation for event detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1798" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3165" to="3174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unified spatio-temporal attention networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="416" to="428" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7834" to="7843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bidirectional multirate reconstruction for temporal modeling in videos</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2653" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d Human activity recognition with reconfigurable convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
				<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3 d residual networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A 2 -nets: Double attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tam: Temporal adaptive module for video recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06803</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">V4D: 4 D convolutional neural networks for video-level representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Videowhisper: Toward discriminative unsupervised video feature learning with attentionbased recurrent neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2080" to="2092" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond distance measurement: Constructing neighborhood similarity for video annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="476" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
				<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pooling the convolutional layers in deep convnets for video action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1839" to="1849" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE conf. comput. vis. pattern recognition</title>
				<meeting>IEEE conf. comput. vis. pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
	</analytic>
	<monogr>
		<title level="m">2015, software available from tensorflow.org</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks for Action Recognition in Videos</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="284" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Holistic large scale video understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="593" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7024" to="7033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">TEA: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
