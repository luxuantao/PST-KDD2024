<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yubin</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Input Image Binary Label/Aesthetic Score</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Input Image Binary Label/Aesthetic Score</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Input Image Binary Label/Aesthetic Score</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5C562DECC229CA61E3E1590C8A724A7B</idno>
					<idno type="DOI">10.1109/MSP.2017.2696576</idno>
					<note type="submission">Handcrafted Features Simple Image Features Generic Features Nongeneric Features Deep Features Generic Deep Features Learned Aesthetic Deep Features Classification Naïve Bayes SVM Deep Neural Network Regression Linear Regressor SVR Customized Regressor Component 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>T his article reviews recent computer vision techniques used in the assessment of image aesthetic quality. Image aesthetic assessment aims at computationally distinguishing high-quality from low-quality photos based on photographic rules, typically in the form of binary classification or quality scoring. A variety of approaches has been proposed in the literature to try to solve this challenging problem. In this article, we summarize these approaches based on visual feature types (hand-crafted features and deep features) and evaluation criteria (data set characteristics and evaluation metrics). The main contributions and novelties of the reviewed approaches are highlighted and discussed. In addition, following the emergence of deep-learning techniques, we systematically evaluate recent deep-learning settings that are useful for developing a robust deep model for aesthetic scoring.</p><p>Experiments are conducted using simple yet solid baselines that are competitive with the current state of the art. Moreover, we discuss the possibility of manipulating the aesthetics of images through computational approaches. We hope that this article might serve as a comprehensive reference for future research on the study of image aesthetic assessment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aesthetic Assessment Through Computer Vision</head><p>The aesthetic quality of an image is judged by commonly established photographic rules, which can be affected by numerous factors, including the different uses of lighting <ref type="bibr" target="#b0">[1]</ref>, contrast <ref type="bibr" target="#b1">[2]</ref>, and image composition <ref type="bibr" target="#b2">[3]</ref> [see Figure <ref type="figure" target="#fig_0">1(a)</ref>]. These human judgments, given in an aesthetic evaluation setting, are the result of human aesthetic experience, i.e., the interaction between emotional-valuation, sensory-motor, and meaning-knowledge neural systems, as demonstrated in a systematic neuroscience study by Chatterjee et al. <ref type="bibr" target="#b3">[4]</ref>. From the beginning of psychological aesthetics studies by Fechner <ref type="bibr" target="#b4">[5]</ref> to modern neuroaesthetics, researchers have argued that there is a certain connection between human aesthetic experience and the sensation caused by visual stimuli, regardless of source, culture, and experience <ref type="bibr" target="#b5">[6]</ref>, which is supported by activations in specific regions of the visual cortex <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>. For example, humans' general reward circuitry produces pleasure when they look at beautiful objects <ref type="bibr" target="#b10">[11]</ref>, and the subsequent aesthetic judgment consists of the appraisal of the valence of such Image Aesthetic Assessment ©graphicstock perceived objects <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>. These activations in the visual cortex can be attributed to the processing of various early, intermediate, and late visual features of the stimuli, including orientation, shape, color grouping, and categorization <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Artists intentionally incorporate such features to facilitate desired perceptual and emotional effects in viewers, forming a set of guidelines as they create artworks to induce desired responses in the nervous systems of perceivers <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. And modern photographers, to make their work appealing to as large an audience as possible, now also resort to certain well-established photographic rules <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> when they capture images.</p><p>As the volume of visual data available online grows at an exponential rate, the capability of automatically distinguishing high-quality images from low-quality ones is in increasing demand in real-world image searching and retrieving applications. When a person enters a particular keyword in an image search engine, it is expected that the system will return professional photographs instead of random snapshots. For example, when a user enters the words "mountain scenery," the person will expect to see colorful, pleasing mountain views or well-captured mountain peaks instead of gray or blurry mountain snapshots.</p><p>The design of these intelligent systems can potentially be facilitated by insights from neuroscience studies, which show that human aesthetic experience is a kind of information processing that includes five stages: perception, implicit memory integration, explicit classification of content and style, cognitive mastering, and evaluation, which together ultimately produce aesthetic judgment and aesthetic emotion <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. However, it is nontrivial to computationally model this process. Challenges in the task of judging the quality of an image include 1) computationally modeling the intertwined photographic rules, 2) knowing the aesthetic differences in images from different image genres (e.g., close-shot object, profile, scenery, and night scenes), 3) knowing the type of techniques used in photo capturing (e.g., high-dynamic range, black and white, and depth of field), and 4) obtaining a large amount of human-annotated data for robust testing.</p><p>To address these challenges, computer vision researchers typically cast this problem as a classification or regression problem. Early studies started with distinguishing typical snapshots from professional photographs by trying to model the well-established photographic rules using lowlevel features <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. These systems typically involve a training set and a testing set consisting of both high-quality and low-quality images. The system robustness is judged by the model performance on the testing set using a specified metric, such as accuracy. These rule-based approaches are intuitive, as they try to explicitly model the criteria that humans use in evaluating the aesthetic quality of an image. However, more recent studies <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b25">[26]</ref> have shown that using a data-driven approach is more effective, as the amount of training data available grows from a couple of hundred images to millions. Besides, transfer learning from source tasks with sufficient amounts of data to a target task with relatively fewer training data is also proven feasible, with many successful attempts showing promising results through deep-learning methods <ref type="bibr" target="#b26">[27]</ref> with network finetuning, where image aesthetics are implicitly learned in a data-driven manner.</p><p>As summarized in Figure <ref type="figure" target="#fig_0">1</ref>(b), the majority of the aforementioned computer vision approaches for image aesthetic assessment can be categorized based on image representations (e.g., handcrafted features and learned features) and classifiers/regressors training (e.g., support vector machine <ref type="bibr">[SVM]</ref> and neural network learning approaches). To the best of our knowledge, no up-to-date survey covers the state-of-the-art methodologies involved in image aesthetic assessment. The last review was published in 2011 by Joshi et al. <ref type="bibr" target="#b27">[28]</ref>, and no deep learning-based methods were covered. Some reviews on image-quality assessment have been published <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. In those efforts, image-quality metrics regarding the differences between a noise-tempered sample and the original high-quality image were proposed, including but not limited to mean squared error, structural similarity index (SSIM) <ref type="bibr" target="#b30">[31]</ref>, and visual information fidelity (VIF) <ref type="bibr" target="#b31">[32]</ref>. Nevertheless, their main focus was on distinguishing noisy images from clean ones in terms of a different quality measure rather than artistic/photographic aesthetics.</p><p>In this article, we contribute a thorough overview of the field of image aesthetic assessment. Meanwhile, we also cover the basics of deep-learning methodologies. Specifically, as different data sets exist and evaluation criteria vary in the image aesthetics literature, we do not aim to directly compare the system performance of all of the reviewed works; instead, we point out in the survey their main contributions and novelties in model designs, and give potential insights for future directions in this field of study. In addition, following the recent emergence of deep-learning techniques and the effectiveness of the data-driven approach in learning better image representation, we systematically evaluate different techniques that could facilitate the learning of a robust deep classifier for aesthetic scoring. Our study covers topics such as data preparation, fine-tuning strategies, and multicolumn deep architectures, which we believe to be useful for researchers working in this domain.</p><p>In particular, we summarize useful insights on how to alleviate the potential problem of data distribution bias in a binary classification setting and show the effectiveness of rejecting false-positive predictions using our proposed convolutional neural network (CNN) baselines, as revealed by the balanced accuracy metric. We also review the most commonly used publicly available image aesthetic assessment data sets for this problem and draw connections between image aesthetic assessment and image aesthetic manipulation, including image enhancement, computational photography, and automatic image cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The deep neural network</head><p>The deep neural network belongs to the family of deep-learning methods that are tasked to learn feature representation in a data-driven approach. While shallow models (e.g., SVM and boosting) showed success in earlier studies concerning relatively smaller amounts of data, they require highly engineered feature designs in solving machine-learning problems. Common architectures in deep neural networks consist of a stack of parameterized individual modules that we call layers, such as the convolution layer and the fully connected layer. The architecture design of stacking layers on top of layers is inspired by the hierarchy in the human visual cortex ventral pathway, offering different levels of abstraction for the learned representation in each layer. Information propagation among layers in feedforward deep neural networks typically follows a sequential pattern. A forward operation (•)</p><p>F is defined respectively in each layer to propagate the input x it receives and produces an output y to the next layer. For example, the forward operation in a fully connected layer with learnable weights W can be written as</p><formula xml:id="formula_0">( ) • . W x x y F w x ij i = = = / (1)</formula><p>This is typically followed by a nonlinear function, such as sigmoid</p><formula xml:id="formula_1">( ) exp z y 1 1 = + -<label>(2)</label></formula><p>or the rectified linear unit ( , ), max z y 0 = which acts as the activation function and produces the net activation output . z To learn the weights W in a data-driven manner, we need to have the feedback information that reports the current performance of the network. Essentially, we are trying to tune the knobs W to achieve a learning objective. For example, given an objective t for the input ,</p><p>x we want to minimize the squared error between the net output z and t by defining a loss function :</p><formula xml:id="formula_2">L . L z t 2 1 2 = -<label>(3)</label></formula><p>To propagate this feedback information to the weights, we define the backward operation for each layer using gradient backpropagation <ref type="bibr" target="#b32">[33]</ref>. We hope to get the direction W D to update the weights W to better suit the training objective (i.e., to minimize L ):</p><p>,</p><formula xml:id="formula_3">W W W ! hD -</formula><p>where h is the learning rate. In our example, W D can be easily derived based on the chain rule:</p><formula xml:id="formula_4">( )• ( ( ) ) ( ) • . W W W x exp exp L z L y z y z t y y 1 2 2 2 2 2 2 2 2 2 D = = = - -+ -<label>(4)</label></formula><p>In practice, researchers resort to batch stochastic gradient descent or more advanced learning procedures that compute more stable gradients, as averaged from a batch of training examples {( ,</p><formula xml:id="formula_5">) | } x x t X i i i !</formula><p>to train deeper and deeper neural networks with continually increasing numbers of layers. We refer readers to <ref type="bibr" target="#b26">[27]</ref> for an in-depth overview of additional deep-learning methodologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-quality metrics</head><p>Image-quality metrics are defined in an attempt to quantitatively measure the objective quality of an image. This is typically used in image restoration applications (superresolution <ref type="bibr" target="#b33">[34]</ref>, deblurring <ref type="bibr" target="#b34">[35]</ref>, and deartifacting <ref type="bibr" target="#b35">[36]</ref>), where we have a default high-quality reference image for comparison. However, these quality metrics are not designed to measure the subjective nature of human-perceived aesthetic quality (see examples in Figure <ref type="figure" target="#fig_2">2</ref>). Directly applying these objective quality metrics to our domain of image aesthetic assessment may produce misleading results, as can be seen from the measured values in Figure <ref type="figure" target="#fig_2">2</ref>(b). Interest in developing more robust metrics has increased in the research community, as a means to assess the more subjective quality of image aesthetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A typical pipeline</head><p>Most existing image-quality assessment methods take a supervised learning approach. A typical pipeline assumes a set of training data , , x y</p><formula xml:id="formula_6">[ , ] i i i N 1 ! " , from which a function : ( ) f g X Y "</formula><p>is learned, where ( ) x g i denotes the feature representation of image .</p><p>xi The label yi is either {0, 1} for binary classification (when f is a classifier) or a continuous score range for regression (when f is a regressor). Following this formulation, a pipeline can be broken into two main components, as shown in Figure <ref type="figure" target="#fig_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature extraction</head><p>The first component of an image aesthetics assessment system aims at extracting robust feature representations describing the aesthetic aspect of an image. Such features are assumed to model the photographic/artistic aspect of images to distinguish images of different qualities. Numerous efforts have been made to design features that are robust enough for the intertwined aesthetic rules. The majority of feature types can be classified into handcrafted features and deep features. Conventional approaches <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b48">[49]</ref> typically adopt handcrafted features to computationally model the photographic rules (e.g., lighting and contrast), global image layout (the rule of thirds), and typical objects (e.g., human profiles, animals, and plants) in images. In more recent work, generic deep features <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> and learned deep features <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b58">[59]</ref> exhibit stronger representation power for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision phase</head><p>The second component of an image aesthetics assessment system provides the ability to perform classification or regression for the given aesthetic task. The naïve Bayes classifier, SVM, boosting, and deep classifier are typically used for binary classification of high-quality and low-quality images, whereas regressors like support vector regressors (SVRs) are used in ranking or scoring images based on their aesthetic quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data sets</head><p>The assessment of image aesthetic quality assumes a standard training set and testing set containing both high-quality and low-quality image examples, as previously mentioned. Judging the ground-truth aesthetic quality of a given image is, however, a subjective task. As such, it is inherently challenging to obtain a large amount of such annotated data. Most of the earlier papers <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> on image aesthetic assessment collect a small amount of private image data. These data sets typically contain from a few hundred to a few thousand images, with binary labels or aesthetic scoring for each image. Yet such data sets where the model performance is evaluated are not publicly available. Much research effort has later been made to contribute publicly available image aesthetic data sets of larger scale for more standardized evaluation of model performance. In the following, we introduce those data sets that are most frequently used in performance benchmarking for image aesthetic assessment. The Photo.net data set and the DPChallenge data set are introduced in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b59">[60]</ref>, respectively. These two data sets can be considered the earliest attempts to construct large-scale image databases for image aesthetic assessment. The Photo.net data set contains 20,278 images, with at least ten score ratings per image. The ratings range from zero to seven, with seven assigned to the most aesthetically pleasing photos. Typically, images uploaded to Photo.net are rated as somewhat pleasing, with the peak of the global mean score skewing to the right in the distribution <ref type="bibr" target="#b27">[28]</ref>. The more challenging DPChallenge data set contains diverse ratings. The DPChallenge data set contains 16,509 images in total, and was later replaced by the Aesthetic Visual Analysis (AVA) data set, where a significantly larger number of images derived from DPChallenge.com are collected and annotated.</p><p>The Chinese University of Hong Kong-PhotoQuality (CUHK-PQ) data set is introduced in <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b60">[61]</ref>. It contains 17,690 images collected from DPChallenge.com and amateur photographers. All of the images are given binary aesthetic labels and grouped into seven scene categories, i.e., animals, plants, static, architecture, landscape, humans, and night. The standard training and testing set from this data set are random partitions of a 50-50 split or a fivefold cross-validation partition, where the overall ratio of the total number of positive examples and that of the negative examples is around : . 13 Sample images are shown in Figure <ref type="figure" target="#fig_4">3</ref>.</p><p>The AVA data set <ref type="bibr" target="#b48">[49]</ref> contains ~, 250 000 images in total. These images are obtained from DPChallenge.com and labeled by aesthetic scores. Specifically, each image receives 78 549 votes of scores ranging from one to ten. The average score of an image is commonly taken to be its groundtruth label. As such, it contains more challenging examples, as images that lie within the center score range could be aesthetically ambiguous [Figure <ref type="figure" target="#fig_5">4(a)</ref>]. For the task of binary aesthetic quality classification, images with an average score higher than a threshold of 5 v + are treated as positive examples, and images with an average score lower than 5 v are treated as negative ones. Additionally, the AVA data set contains 14 style attributes and more than 60 category attributes for a subset of images. There are two typical training and testing splits from this data set, i.e., 1) a large-scale standardized partition with ~, 230 000 training images and ~, 20 000 testing images using a hard threshold of , 0 v = and 2) an easier partition modeling that of CUHK-PQ by taking those images whose score ranking is at the top 10% and the bottom 10%, resulting in ~, 25 000 images for training and ~, 25 000 images for testing. The ratio of the total number of positive examples to that of the negative examples is around : . 12 5 Apart from these two standard benchmarks, more recent research also introduces new data sets that take into consideration the data-balancing issue. The Image Aesthetic Data Set (IAD) introduced in <ref type="bibr" target="#b54">[55]</ref>   examples is around . : . 1 07 1 The Aesthetic and Attributes Database (AADB) <ref type="bibr" target="#b24">[25]</ref> also contains a balanced distribution of professional and consumer photos, with a total of 10,000 images. Eleven aesthetic attributes and annotators' IDs are provided. A standard partition with 8,500 images for training, 500 images for validation, and 1,000 images for testing is proposed <ref type="bibr" target="#b24">[25]</ref>.</p><p>The trend toward creating data sets of even larger volume and higher diversity is essential for boosting the research progress in this field of study. To date, the AVA data set serves as a canonical benchmark for performance evaluation of image aesthetic assessment, as it is the first largescale data set with detailed annotation. Still, the distribution of positive and negative examples in the data set also plays a role in the effectiveness of trained models, as false-positive predictions are as harmful as having a low recall rate in image retrieval and searching applications. In the following, we review major attempts in the literature to build systems for the challenging task of image aesthetic assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conventional approaches with handcrafted features</head><p>The conventional option for image quality assessment is to hand-design good feature extractors, which requires a considerable amount of engineering skill and domain expertise. Next we review a variety of approaches that exploit hand-engineered features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple image features</head><p>Global features are first explored by researchers to model the aesthetic aspect of images. The works by Datta et al. <ref type="bibr" target="#b20">[21]</ref> and Ke et al. <ref type="bibr" target="#b36">[37]</ref> are among the first to cast aesthetic understanding of images into a binary classification problem. <ref type="bibr">Datta et</ref> al. <ref type="bibr" target="#b20">[21]</ref> combine low-level and high-level features that are typically used for image retrieval and train an SVM classifier for binary classification of images in terms of aesthetic quality. Ke et al. <ref type="bibr" target="#b36">[37]</ref> propose global edge distribution, color distribution, hue count, and low-level contrast and brightness indicators to represent an image; then they train a naïve Bayes classifier based on such features. An even earlier attempt by Tong et al. <ref type="bibr" target="#b19">[20]</ref> adopts boosting to combine global lowlevel simple features (blurriness, contrast, colorfulness, and saliency) to classify professional photographs and ordinary snapshots.</p><p>All of these pioneering works present the very first attempts to computationally model the global aesthetic aspect of images using handcrafted features. Even in a recent work, Ayd in The distribution of positive and negative examples in the data set also plays a role in the effectiveness of trained models.</p><p>et al. <ref type="bibr" target="#b61">[62]</ref> construct image aesthetic attributes by sharpness, depth, clarity, tone, and colorfulness. An overall aesthetics rating score is heuristically computed based on these five attributes. Improving upon these global features, later studies adopt global saliency to estimate aesthetic attention distribution. Sun et al. <ref type="bibr" target="#b37">[38]</ref> make use of a global saliency map to estimate visual attention distribution to describe an image, and they train a regressor to output the quality score of an image based on the rateof-focused-attention region in the saliency map. You et al. <ref type="bibr" target="#b38">[39]</ref> derive similar attention features based on a global saliency map and incorporate a temporal activity feature for video quality assessment. Regional image features <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref> later prove to be effective in complementing the global features. Luo et al. <ref type="bibr" target="#b39">[40]</ref> extract regional clarity contrast, lighting, simplicity, composition geometry, and color harmony features based on the subject region of an image. Wong et al. <ref type="bibr" target="#b62">[63]</ref> compute exposure, sharpness, and texture features on salient regions and global images, as well as features depicting the subject-background relationship of an image. Nishiyama et al. <ref type="bibr" target="#b40">[41]</ref> extract bags-of-color patterns from local image regions with a grid-sampling technique. While <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and <ref type="bibr" target="#b62">[63]</ref> adopt the SVM classifier, Lo et al. <ref type="bibr" target="#b41">[42]</ref> build a statistical modeling system with coupled spatial relations after extracting color and texture features from images, where a likelihood evaluation is used for aesthetic quality prediction. These methods focus on modeling image aesthetics from local image regions that are potentially most attractive to humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image composition features</head><p>Image composition in a photograph typically relates to the presence and position of a salient object. The rule of thirds, low depth of field, and opposing colors are the common techniques for composing a good image where the salient object is made outstanding (see Figure <ref type="figure" target="#fig_6">5</ref>). To model such aesthetic aspects, Bhattacharya et al. <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b63">[64]</ref> propose compositional features using relative foreground position and a visual weight ratio to model the relations between foreground objects and the background scene; then an SVR is trained. Wu et al. <ref type="bibr" target="#b64">[65]</ref> propose the use of Gabor filter responses to estimate the position of the main object in images, and then extract low-level hue, saturation, value (HSV)-color features from global and central image regions. These features are fed to a soft-SVM classifier with sigmoidal softening to distinguish images of ambiguous quality. Dhar et al. <ref type="bibr" target="#b43">[44]</ref> cast high-level features into describable attributes of composition, content, and sky illumination and combine low-level features to train an SVM classifier. Lo et al. <ref type="bibr" target="#b65">[66]</ref> propose the combination of layout composition, edge composition features with an HSV color palette, HSV counts, and global features (textures, blur, dark channel, and contrasts). SVM is used as the classifier.</p><p>The representative work by Tang et al. <ref type="bibr" target="#b44">[45]</ref> gives a comprehensive analysis of the fusion of global features and regional features. Specifically, image composition is estimated by global hue composition and scene composition, and multiple types of regional features extracted from subject areas are proposed, such as dark channel feature, clarity contrast, lighting contrast, composition geometry of the subject region, spatial complexity and human-based features. An SVM classifier is trained on each of the features for comparison, and the final model performance is substantially enhanced by combining all of the proposed features. It is shown that regional features can effectively complement global features in modeling the image aesthetics.</p><p>A more recent approach by image composition features is proposed by Zhang et al. <ref type="bibr" target="#b66">[67]</ref>, where image descriptors that characterize local and global structural aesthetics from multiple visual channels are designed. The spatial structure of the image local regions is modeled using graphlets, and they are connected based on atomic region adjacency. To describe such atomic regions, visual features from multiple visual channels [such as color moment, histogram of oriented gradients (HOG), and saliency histogram] are used. The global spatial layout of the photo is also embedded into graphlets using a Grassmann manifold. The importance of the two kinds of graphlet descriptors is dynamically adjusted, capturing the spatial composition of an image from multiple visual channels. The final aesthetic prediction of an image is generated by a probabilistic model using the postembedding graphlets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General-purpose features</head><p>Yeh et al. <ref type="bibr" target="#b45">[46]</ref> make use of scale-invariant feature transform (SIFT) descriptors and propose relative features by matching a query photo to photos in a gallery group. General-purpose imagery features like bag of visual (BOV) words <ref type="bibr" target="#b67">[68]</ref> and Fisher vector (FV) <ref type="bibr" target="#b68">[69]</ref> are explored in <ref type="bibr" target="#b46">[47]</ref>- <ref type="bibr" target="#b48">[49]</ref>. Specifically, SIFT and color descriptors are used as the local descriptors upon which a Gaussian mixture model (GMM) is trained. The statistics up to the second order of this GMM distribution are The rule of thirds, low depth of field, and opposing colors are the common techniques for composing a good image where the salient object is made outstanding.</p><p>then encoded using the BOV words or FV. Spatial pyramid is also adopted, and the per-region encoded FVs are concatenated as the final image representation. These methods ([47]- <ref type="bibr" target="#b48">[49]</ref>) represent an attempt to implicitly model photographic rules by encoding them in generic content-based features, which is competitive with or even outperforms simple handcrafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task-specific features</head><p>Task-specific features is a term that refers to features in image aesthetic assessment that are optimized for a specific category of photos, which can be efficient when the use-case or task scenario is fixed or known beforehand. Explicit information (such as human facial characteristics, geometry tag, scene information, or intrinsic character component properties) is exploited based on the different task nature. Li et al. <ref type="bibr" target="#b69">[70]</ref> propose a regression model that targets only consumer photos with faces. Face-related social features (such as facial expression features, facial pose features, and relative facial position features) and perceptual features (facial distribution symmetry, facial composition, and pose consistency) are specifically designed for measuring the quality of images with faces, and it is shown in <ref type="bibr" target="#b69">[70]</ref> that for this task they complement conventional handcrafted features (brightness contrast, color correlation, clarity contrast, and background color simplicity). Support vector regression is used to produce aesthetic scores for images.</p><p>Lienhard et al. <ref type="bibr" target="#b70">[71]</ref> study particular facial features for evaluating the aesthetic quality of headshot images. To design features for face/headshots, the input image is divided into subregions (the eyes, mouth, global face, and entire image regions). Low-level features (sharpness, illumination, contrast, dark channel, and hue and saturation in the HSV color space) are computed from each region. These pixellevel features assume the human way of perceiving a facial image and hence can reasonably model the headshot images. SVM with Gaussian kernel is used as the classifier.</p><p>Su et al. <ref type="bibr" target="#b71">[72]</ref> propose bag of aesthetics-preserving features for scenic/landscape photographs. Specifically, an image is decomposed into n n # spatial grids; then low-level features in HSV-color space as well as local binary patterns, HOG, and saliency features are extracted from each patch. The final feature is generated by a predefined patch-wise operation to exploit the landscape composition geometry. AdaBoost is used as the classifier. These features aim at modeling only landscape images and may be limited in their representation power in general image aesthetic assessment.</p><p>Yin et al. <ref type="bibr" target="#b72">[73]</ref> build a scene-dependent aesthetic model by incorporating the geographic location information with GIST descriptors and spatial layout of saliency features for scene aesthetic classification (such as bridges, mountains, and beaches). SVM is used as the classifier. The geographic location information is used to link a target scene image to relevant photos taken within the same geocontext; then these relevant photos are used as the training partition to the SVM. The authors' proposed model requires input images with geographic tags and is also limited to scenic photos. For scene images without geo-context information, SVM trained with images from the same scene category is used.</p><p>Sun et al. <ref type="bibr" target="#b73">[74]</ref> design a set of low-level features for aesthetic evaluation of Chinese calligraphy. They target the handwritten Chinese character on a plain white background; hence, conventional color information is not useful in this task. Global shape features, extracted based on standard calligraphic rules, are introduced to represent a character. In particular, the authors consider alignment and stability, distribution of white space, stroke gaps, and a set of component layout features while modeling the aesthetics of handwritten characters. A backpropagation neural network is trained as the regressor to produce an aesthetic score for each given input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep-learning approaches</head><p>The powerful feature representation learned from a large amount of data has shown an ever-improving performance in the tasks of recognition, localization, retrieval, and tracking, surpassing the capability of conventional handcrafted features <ref type="bibr" target="#b74">[75]</ref>. Since the work by Krizhevsky et al. <ref type="bibr" target="#b74">[75]</ref>, where CNNs are adopted for image classification, a great degree of interest has arisen in learning robust image representations through deep-learning approaches. Recent works in the literature of image aesthetic assessment using deep-learning approaches to learn image representations can be broken down into two major schemes: 1) adopting generic deep features learned from other tasks and training a new classifier for image aesthetic assessment and 2) learning aesthetic deep features and training a classifier directly from image aesthetics data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generic deep features</head><p>A straightforward approach to employing deep-learning aims is to adopt generic deep features learned from other tasks and train a new classifier on the aesthetic classification task. Dong et al. <ref type="bibr" target="#b49">[50]</ref> propose adopting the generic features from the penultimate layer output of AlexNet <ref type="bibr" target="#b74">[75]</ref> with spatial pyramid pooling. Specifically, the , ( ) ( ) , fc SpatialPyramid 4 096 7 <ref type="bibr" target="#b5">6</ref> 24 576 # = -d i m e n s i o n a l feature is extracted as the generic representation for images; then an SVM classifier is trained for binary aesthetic classification. Lv et al. <ref type="bibr" target="#b50">[51]</ref> also adopt the normalized 4,096-dimension fc7 output of AlexNet <ref type="bibr" target="#b74">[75]</ref> for feature representation. They propose to learn the relative ordering relationship of images of different aesthetic quality. They use SVM rank <ref type="bibr" target="#b75">[76]</ref> to train a ranking model for image pairs of { , }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I I</head><p>ighQuality owQuality H L</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learned aesthetic deep features</head><p>Features learned with single-column CNNs ! layers. Then the weights learned through this stage are transferred back to the conv i 5 in the proposed parallel architecture, with the weights from conv1 to conv4 reused from AlexNet in the fully connected layer randomly reinitialized. Subsequently, the CNN is further finetuned end to end. Upon convergence, the network produces a strong response in the conv i 5 layer feature map when the input image is of category { ,..., } i 1 7 !</p><p>. This shows the potential in exploiting image category information when learning the aesthetic presentation.</p><p>Tian et al. <ref type="bibr" target="#b53">[54]</ref> train a CNN with four convolution layers and two fully connected layers to learn aesthetic features from the data. The output size of the two fully connected layers is set to 16 instead of 4,096 as in AlexNet. The authors propose that such a 16-dimension representation is sufficient to model only the top 10% and bottom 10% of the aesthetic data, which are relatively easy to classify compared to the full data. Based on this efficient feature representation learned from the CNN, the authors propose a query-dependent aesthetic model as the classifier. Specifically, for each query image, a querydependent training set is retrieved based on predefined rules (visual similarity, image tags association, or a combination of both). Subsequently, an SVM is trained on this retrieved training set. It shows that the features learned from the aesthetic data outperform the generic deep features learned in the ImageNet task.</p><p>The deep multipatch aggregation (DMA)-net is proposed in <ref type="bibr" target="#b23">[24]</ref>, where information from multiple image patches is extracted by a single-column CNN that contains four convolution layers and three fully connected layers, with the last layer outputting a softmax probability. Each randomly sampled image patch is fed into this CNN. To combine multiple feature outputs from the sampled patches of one input image, a statistical aggregation structure is designed to aggregate the features from the orderless sampled image patches by multiple poolings (minimum, maximum, median, and averaging). An alternative aggregation structure is also designed based on sorting. The final feature representation effectively encodes the image based on regional image information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features learned from multicolumn CNNs</head><p>The Rating Pictorial Aesthetics using Deep Learning (RAPID) model by Lu et al. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b54">[55]</ref> can be considered to be the first attempt to train CNNs with aesthetic data. They use an AlexNet-like architecture where the last fully connected layer is set to output two-dimensional probability for aesthetic binary classification. Both global image and local image patches are considered in their network input design, and the best model is obtained by stacking a global-column and a local-column CNN to form a double-column CNN, where the feature representation (the penultimate layers' fc7 output) from each column is concatenated before the fc8 layer (classification layer). (Figure <ref type="figure">7</ref> shows a typical multicolumn CNN.) Standard stochastic gradient descent is used to train the network with softmax loss. Moreover, the authors further boost the performance of the network by incorporating image style information using a style-column or semantic-column CNN. Then the style-column CNN is used as the third input column, forming a three-column CNN with style/semantic information. Such a multicolumn CNN exploits the data from both the global and local image aspects.</p><p>Mai et al. <ref type="bibr" target="#b25">[26]</ref> propose stacking five columns of Visual Geometry Group (VGG)-based networks using an adaptive spatial pooling layer. The adaptive spatial pooling layer is  <ref type="bibr" target="#b55">[56]</ref> propose a multicolumn CNN model called brain-inspired deep networks (BDN) that shares similar structures with RAPID. In RAPID, a style attribute prediction CNN is trained to predict 14 style attributes for input images. This attribute CNN is treated as one additional CNN column, which is then added to the parallel input pathways of a global image column and a local patch column. In BDN, 14 different style CNNs are pretrained, and they are parallel cascaded and used as the input to a final CNN for rating distribution prediction, where the aesthetic quality score of an image is subsequently inferred. The BDN model can be considered as an extended version of RAPID that exploits each of the aesthetic attributes using learned CNN features, hence enlarging the parameter space and learning capability of the overall network.</p><p>Zhang et al. 6 Hence, the final concatenated feature learned in this manner is ( ) D 128 1 # + dimensional. A probabilistic model containing four layers is trained for aesthetic quality classification.</p><p>Kong et al. <ref type="bibr" target="#b24">[25]</ref> propose learning aesthetic features assisted by the pair-wise ranking of image pairs as well as the image attribute and content information. Specifically, a Siamese architecture that takes image pairs as input is adopted, where the two base networks of the Siamese architecture adopt the AlexNet configurations (the 1,000-class classification layer fc8 from the AlexNet is removed). In the first stage, the base network is pretrained by fine-tuning from aesthetic data using the Euclidean loss regression layer instead of the softmax classification layer. After that, the Siamese network ranks the loss for every sampled image pair. Upon convergence, the fine-tuned base network is used as a preliminary feature extractor.</p><p>In the second stage, an attribute prediction branch is added to the base network to predict image attribute information. Then the base network continues to be fine-tuned in a multitask manner by combining the rating regression Euclidean loss, attribute classification loss, and ranking loss.</p><p>In the third stage, yet another content classification branch is added to the base network to predict a predefined set of category labels. Upon convergence, the softmax output of the content category prediction is used as a weighting vector for weighting the scores produced by each feature branch (the aesthetic branch, attribute branch, and content branch).</p><p>In the final stage, the base network and all of the added output branches are fine-tuned jointly, with the content classification branch frozen. Effectively, such aesthetic features are learned by considering both the attribute and category content information, and the final network produces image scores for each given image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features learned with multitask CNNs</head><p>Kao et al. <ref type="bibr" target="#b57">[58]</ref> propose three category-specific CNN architectures: one for object, one for scene, and one for texture. The scene CNN takes a warped global image as input. It has five convolution layers and three fully connected layers, with the last fully connected layer producing a two-dimensional softmax classification. The object CNN takes both the warped global image and the detected salient region as input. It is a two-column CNN combining global composition and salient information. The texture CNN takes 16 randomly cropped patches as input. Category information is predicted using a three-class SVM classifier before feeding images to a category-specific CNN. To alleviate the use of the SVM classifier, an alternative architecture with a warped global image as input is trained with a multitask approach, where the main task is aesthetic classification and the auxiliary task is scene category classification. (A typical multitask CNN is illustrated in Figure <ref type="figure">8</ref>.) Kao et al. <ref type="bibr" target="#b58">[59]</ref> propose learning image aesthetics in a multitask manner. Specifically, AlexNet is used as the base network. Then the 1,000-class fc8 layer is replaced by a two-class aesthetic prediction layer and a 29-class semantic prediction layer. The loss balance between the aesthetic prediction task and the semantic prediction task is determined empirically. Moreover, another branch containing two fully connected layers for aesthetic prediction is added to the second convolution layer (conv2 of AlexNet). By linking an added gradient flow from the aesthetic task directly to the convolutional layers, one expects to learn better low-level convolutional features. This strategy shares a similar spirit with the deeply supervised net <ref type="bibr" target="#b76">[77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation criteria and existing results</head><p>Different metrics for performance evaluation of image aesthetic assessment models are used across the literature: classification accuracy <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b62">[63]</ref>- <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b72">[73]</ref> reports the proportion of correctly classified results; precision-and-recall (PR) curve <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b65">[66]</ref> considers the degree of relevance of the retrieved items and the retrieval rate of relevant items, which is also widely adopted in image search or retrieval applications; Euclidean distance or residual sum-of-squares error between the ground-truth score and aesthetic ratings <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b73">[74]</ref> and correlation ranking <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b45">[46]</ref> are used for performance evaluation in score regression frameworks; receiver-operating characteristic (ROC) curve <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref> and area under the curve <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b65">[66]</ref> concerns the performance of binary classifiers when the discrimination threshold is varied; mean average precision <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b54">[55]</ref> is the average precision (AP) across multiple queries, which is usually used to summarize the PR curve for the given set of samples. These are among the typical metrics for evaluating model effectiveness for image aesthetic assessment (see Table <ref type="table" target="#tab_2">1</ref> for a summary). Subjective evaluation by conducting human surveys is also seen in <ref type="bibr" target="#b61">[62]</ref>,</p><p>where human evaluators are asked to give subjective aesthetic attribute ratings. We find that it is not feasible to directly compare all methods, as different data sets and evaluation criteria are used across the literature. To this end, we try to summarize, respectively, the released results reported on the two standard data sets, namely the CUHK-PQ (Table <ref type="table" target="#tab_3">2</ref>) and AVA data sets (Table <ref type="table" target="#tab_4">3</ref>), and to present the results on other data sets in Table <ref type="table" target="#tab_5">4</ref>. To date, the AVA data set (standard partition) is considered to be the most challenging by the majority of the reviewed work.</p><p>The overall accuracy metric appears to be the most popular metric. It can be written as Overall accuracy .</p><formula xml:id="formula_7">P N TP TN = + +<label>(5)</label></formula><p>This metric alone could be biased and far from ideal, as a naïve predictor that predicts all examples as positive would already reach about ( )/( ) % k k k 14 0 14 6 70 + + = classification accuracy. To complement such a metric when evaluating models on imbalanced testing sets, an alternative balanced accuracy metric <ref type="bibr" target="#b77">[78]</ref> can be adopted:</p><formula xml:id="formula_8">Balanced accuracy . P TP N TN 2 1 2 1 = + `j j<label>(6)</label></formula><p>Balanced accuracy equally considers the classification performance on different classes <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref>. While the overall accuracy in (5) offers an intuitive sense of correctness by reporting the proportion of correctly classified samples, the balanced accuracy in (6) combines the prevalence-independent statistics of sensitivity and specificity. A low balanced accuracy will be observed if a given classifier tends to predict only the dominant class. For the naïve predictor mentioned above, the balanced accuracy would give a proper number In this regard, in the following sections where we discuss our findings on a proposed strong baseline, we report both overall classification accuracy and balanced accuracy to get a more reasonable measure of baseline performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on deep-learning settings</head><p>It is evident from Table <ref type="table" target="#tab_4">3</ref> that deep learning-based approaches dominate the performance of image aesthetic assessment. The effectiveness of learned deep features in this task has motivated us to take a step back to consider how a CNN works to understand the aesthetic quality of an image. It is worth noting that training a robust deep aesthetic scoring model is nontrivial, and often we found that the devil is in the details. To this end, we design a set of systematic experiments based on a baseline one-column CNN and a two-column CNN, and evaluate different settings from minibatch formation to complex multicolumn architecture. The results are reported on the widely used AVA data set.</p><p>We observe that by carefully training the CNN architecture, the two-column CNN baseline reaches comparable or  even better performance than state-of-the-art methods, and the one-column CNN baseline acquires the strong capability to suppress false-positive predictions while having competitive classification accuracy. We hope the experimental results will facilitate the design of future deep-learning models for image aesthetic assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulation and the base CNN structure</head><p>The supervised CNN learning process involves a set of training data , x y</p><formula xml:id="formula_9">[ , ] i i i N 1 ! "</formula><p>, from which a nonlinear mapping function :</p><p>f X Y " is learned through backpropagation <ref type="bibr" target="#b32">[33]</ref>. Here, xi is the input to the CNN and y T i ! is its corresponding ground-truth label. For the task of binary classification, , y 0 1 i ! " , is the aesthetic label corresponding to image .</p><p>xi The convolutional operations in such a CNN can be expressed as</p><formula xml:id="formula_10">( ) ( * ( ) , ), , , ..., w b max F X F X k D 0 12 k k k k 1 ! = + - " ,,<label>(7)</label></formula><p>where ( ) F X X 0 = is the network input and D is the depth of the convolutional layers. The operator * denotes the convolution operation. The operations in the Dl fully connected layers can be formulated in a similar manner. To learn the ( ) D D + l network weights W using the standard backpropagation with stochastic gradient descent, we adopt the cross-entropy classification loss, which is formulated as</p><formula xml:id="formula_11">( ) { ( | ; ) ( ) ( ( | ; )) ( )} W W W W x x log log L n t p y t t py t 1 1 1 t i n i i i i 1 z =- = + - - = + = t t / / (8) ( | ; ) ( ) ( ) , w w w x x x exp exp p y t t T i i i t t i t T T = = ! l t / (9) where { , } t 0 1 T ! =</formula><p>is the ground truth. This formulation is in accordance with prior successful model frameworks, such as AlexNet <ref type="bibr" target="#b74">[75]</ref> and VGG-16 <ref type="bibr" target="#b79">[80]</ref>, which are also adopted as the base network in some of our reviewed approaches.</p><p>The original last fully connected layer of these two networks is for the 1,000-class ImageNet object recognition challenge. For aesthetic quality classification, a two-class aesthetic classification layer to produce a softmax predictor is needed [see Figure <ref type="figure" target="#fig_10">9(a)</ref>]. Following typical CNN approaches, the input size is fixed to , 224 224 3 # # which is cropped from globally warped 256 256 3 # # images. Standard data augmentation, such as mirroring, is performed. All of the baselines are implemented based on the Caffe package <ref type="bibr" target="#b80">[81]</ref>. For clarity of presentation in the following sections, we name all of our fine-tuned baselines Deep Aesthetic Net (DAN), with the corresponding suffix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training from scratch versus fine-tuning</head><p>Fine-tuning from a trained CNN has been proven in <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b82">[83]</ref> to be an effective initialization approach. The RAPID base network <ref type="bibr" target="#b22">[23]</ref> uses global image patches and trains a network structure from scratch that is similar to AlexNet. For a fair comparison of similar-depth networks, we first select AlexNet pretrained with the ILSVRC-2012 training set (1.2 million images) and fine-tune it with the AVA training partition. As shown in Table <ref type="table" target="#tab_6">5</ref>, fine-tuning from the vanilla AlexNet yields better performance than simply training the RAPID base network from scratch. Moreover, the DAN model fine-tuned from VGG-16 [see Figure <ref type="figure" target="#fig_10">9(a)</ref>] yields the best performance in both balanced accuracy and overall accuracy. It is worth pointing out that other more recent and deeper models, such as ResNet <ref type="bibr" target="#b83">[84]</ref>, Inception-ResNet <ref type="bibr" target="#b84">[85]</ref>, and PolyNet <ref type="bibr" target="#b85">[86]</ref>, could serve as pretrained models. Nevertheless, owing to the typically small size of aesthetic data sets, precautions need be taken during the fine-tuning process. Plausible methods include freezing some earlier layers to prevent overfitting <ref type="bibr" target="#b82">[83]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minibatch formation</head><p>Minibatch formation directly affects the gradient direction toward which stochastic gradient descent brings down the training loss in the learning process. We consider two types of minibatch formation and reveal the impact of this difference on image aesthetic assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random sampling</head><p>By randomly selecting examples for minibatches <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Balanced formation</head><p>Another approach is to enforce a balanced number of positives and negatives in each of the minibatches, i.e., for each iteration of backpropagation, the gradient is computed from a balanced number of positive examples and negative examples. Table <ref type="table" target="#tab_7">6</ref> compares the performance of these two strategies. We observe that although the model fine-tuned with randomly sampled minibatches reaches a higher overall accuracy, its performance is inferior to the one fine-tuned with balanced minibatches, as evaluated using balanced accuracy. To keep track of both true-positive prediction rates and true-negative prediction rates, balanced accuracy is adopted to measure the model robustness on the data imbalance issue. Network fine-tuning in the rest of the experiments is performed with balanced minibatches, unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triplet pretraining and multitask learning</head><p>Apart from directly training using the given training data pairs , , x y</p><formula xml:id="formula_12">[ , ] i i i N 1 ! "</formula><p>, one could utilize richer information inherent in the data or auxiliary sources to enhance the learning performance. We discuss two popular approaches next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining using triplets</head><p>The triplet loss is inspired by Dimensionality Reduction by Learning an Invariant Mapping <ref type="bibr" target="#b88">[89]</ref> and large margin nearest neighbor <ref type="bibr" target="#b89">[90]</ref>. It is widely used in many recent vision studies <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b90">[91]</ref>- <ref type="bibr" target="#b92">[93]</ref> and aims to bring data of the same class closer while moving data of different classes further away. This loss is particularly suitable to our task; i.e., the absolute aesthetic score of an image is arguably subjective, but the general relationship that beautiful images are close to each other while the opposite images should be apart is obvious.   To enforce such a relationship in an aesthetic embedding, one needs to generate minibatches of triplets for deep feature learning, i.e., an anchor , x a positive instance x ve + of the same class, and a negative instance x ve -of a different class. Furthermore, we found it useful to constrain each image triplet to be selected from the same image category. In addition, we observed better performance by introducing triplet loss in the pretraining stage and continuing with conventional supervised learning on the triplet-pretrained model. Table <ref type="table" target="#tab_8">7</ref> shows that the DAN model pretrained with triplets gives better performance.</p><p>We further visualize some categories in the learned aesthetic embedding space in Figure <ref type="figure" target="#fig_11">10</ref>. It is interesting to observe that the embedding learned with triplet loss demonstrates much better aesthetic grouping in comparison to that without the use of triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multitask learning with image category prediction</head><p>Can aesthetic prediction be facilitated provided that a model understand to which category the image belongs? Following the work in <ref type="bibr" target="#b93">[94]</ref>, where auxiliary information is used to regularize the learning of the main task, we investigate the potential benefits of using image categories as an auxiliary label in training the aesthetic quality classifier.</p><p>Specifically, given an image labeled with main task label , y where y 0 = for low-quality images and y 1 = for highquality ones, we provide an auxiliary label c C ! denoting one of the image categories, such as animals, landscape, portraits, and so forth. In total, we include 30 image categories. To learn a classifier for the auxiliary class, a new fully connected layer is attached to the fc7 of the vanilla VGG-16 structure to predict a softmax probability for each category class. The modified one-column CNN baseline architecture is shown in Figure <ref type="figure" target="#fig_10">9(b)</ref>. The loss function in ( <ref type="formula">8</ref>) is now changed to   (c) t is the auxiliary prediction from the network. Solving the above loss function, the DAN model performance from this multitask learning strategy is observed to have surpassed the previous one (Table <ref type="table" target="#tab_8">7</ref>). It is worth noting that the category annotation of the AVA-training partition is not complete, with about 25% of the images not having categories labeled. For those training instances without categories labeled, the auxiliary loss ( ) W Laux c due to missing labels is ignored.</p><formula xml:id="formula_13">( ) ( ), W W L L L multi ask aux t c = +<label>(10) ( ) { ( | ; ) ( ) ( |</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triplet pretraining + multitask learning</head><p>Combining triplet pretraining and multitask learning, the final one-column CNN baseline reaches a balanced accuracy of 73.59% on the challenging task of aesthetic classification. The results for different fine-tuning strategies is summarized in Table <ref type="table" target="#tab_8">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Note that it is nontrivial to boost the overall accuracy at the same time as we try not to overfit the baseline to a certain data distribution. Still, compared with other released results in Table <ref type="table" target="#tab_9">8</ref>, with careful training, a one-column CNN baseline yields a strong capability of rejecting false positives while attaining a reasonable overall classification accuracy. We show some qualitative classification results as follows.</p><p>Figures <ref type="figure" target="#fig_14">11</ref> and<ref type="figure" target="#fig_15">12</ref> show the qualitative results of aesthetic classification by the one-column CNN baseline, using DAN-1 (triplet pretrained + multitask). Note that these examples are correctly classified neither by BDN <ref type="bibr" target="#b55">[56]</ref> nor by DMA-net <ref type="bibr" target="#b23">[24]</ref>. False-positive test examples (Figure <ref type="figure" target="#fig_4">13</ref>) by the DAN-1 baseline still show a somewhat high-quality image trend, with high color contrast or depth of field, while false-negative testing examples (Figure <ref type="figure" target="#fig_5">14</ref>) mostly reflect low image tones. Both quantitative and qualitative results suggest the importance of minibatch formation and fine-tuning strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multicolumn deep architecture</head><p>State-of-the-art approaches <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> for image aesthetic classification typically adopt multicolumn CNNs (Figure <ref type="figure">7</ref>) to enhance the learning capacity of the model. In particular, these approaches benefit from learning multiscale image information (e.g., global image versus local patches) or utilizing image semantic information (e.g., image styles). To incorporate insights from previous successful approaches, we prepared another two-column CNN baseline (DAN-2) (see Figure <ref type="figure" target="#fig_6">15</ref>) with a focus on the more apparent approach of using local image patches as a parallel input column. Both <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b23">[24]</ref> utilize CNNs trained with local image patches as alternative columns in their multibranch network, with performance evaluated using overall accuracy. For fair comparison, we prepared local image patches of size 224 224 3 # # following <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b23">[24]</ref>, and we fine-tuned one DAN-1 model from the vanilla VGG-16 (ImageNet) with such local patches. Another branch is the original DAN-1 model, fine-tuned with globally warped input by triplet pretraining and multitask learning (see the section "Triplet Pretraining and Multitask Learning"). We performed separate experiments where minibatches of these local image patches were taken from either random sampling or the balanced formation.</p><p>As shown in Table <ref type="table" target="#tab_9">8</ref>, the DAN-1 model fine-tuned with local image patches performs less well under the metric of balanced accuracy compared to the original DAN-1 model fine-tuned with globally warped input in both random minibatch learning and balanced minibatch learning. We conjecture that local patches contain The authors of <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b57">[58]</ref>, and <ref type="bibr" target="#b58">[59]</ref> have not released detailed results.</p><p>The absolute aesthetic score of an image is arguably subjective, but the general relationship that beautiful images are close to each other while the opposite images should be apart is obvious.  FIGURE <ref type="bibr" target="#b12">13</ref>. Some examples with a negative ground truth that are wrongly classified by the DAN-1 baseline. High color contrast or depth of field is observed in these testing cases <ref type="bibr" target="#b48">[49]</ref>.</p><p>FIGURE <ref type="bibr" target="#b13">14</ref>. Some examples with a positive ground truth that are wrongly classified by the DAN-1 baseline. Most of these images are of low image tones <ref type="bibr" target="#b48">[49]</ref>.</p><p>no global and compositional information as compared to globally warped input. Nevertheless, such a drop in accuracy is not observed under the overall accuracy metric.</p><p>We next evaluated the two-column CNN baseline DAN-2 using the DAN-1 model fine-tuned with local image patches and the one fine-tuned with globally warped input. We have two variants here, depending on whether we employ random or balanced minibatches. We observed that DAN-2 trained with random minibatches attains the highest overall accuracy on the AVA standard testing partition compared to the previous state-of-the-art methods (see Table <ref type="table" target="#tab_9">8</ref>). (Some other works <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b94">[95]</ref>- <ref type="bibr" target="#b96">[97]</ref> on AVA data sets use only a small subset of images for evaluation, which is not directly comparable to the canonical state of the art on the AVA standard partition; see Table <ref type="table" target="#tab_4">3</ref>).</p><p>Interestingly, we observed the balanced accuracy of the two variants of DAN-2 degrades when compared to the respective DAN-1 trained on globally warped input. This observation raises the question of whether local patches necessarily benefit the performance of image aesthetic assessment. We analyzed the cropped local patches more carefully and found that these patches were inherently ambiguous. Thus, the model trained with such inputs could easily become biased toward predicting local patch input to be of high quality, which also explains the performance differences in the two complementary evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model depth and layer-wise effectiveness</head><p>Determining the aesthetics of images from different categories takes varying photographic rules. We understand that it is not easy to determine some image genres' aesthetic quality in general. It would be interesting to perform a layer-by-layer analysis and track to what degree a deep model has learned image aesthetics in its hierarchical structure. We conducted this experiment using the one-column CNN baseline DAN-1 (triplet pretrained + multitask). We used layer features generated by this baseline model and trained an SVM classifier to perform aesthetic classification on the AVA testing images and then evaluated the performance of different layer features across different image categories.</p><p>Features extracted from the convolutional layers of the model were aggregated into a convolutional Fisher representation, as done in <ref type="bibr" target="#b97">[98]</ref>. Specifically, to extract features from the dth convolutional layer, note that the output feature maps of = 6 @ we obtained the feature representation of the local patch region . L A dictionary codebook was created using GMM from all of the , vL L Itrain ! " , and an FV representation is subsequently computed using this codebook to describe an input image. The obtained convolutional Fisher representation is used for training SVM classifiers.</p><p>We compared features from layer conv3_1 to fc7 of the DAN-1 baseline and reported selected results that we find interesting in Figure <ref type="figure" target="#fig_0">16</ref>. We obtained the following results: 1) Model depth is important: More abstract aesthetic representation can be learned in deeper layers. The performance of aesthetic assessment can generally be benefited from model depth. This observation aligns with that in general object recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Different categories demand different model depths:</head><p>The aesthetic classification accuracy on images belonging to the black and white category are generally lower than the accuracy on images in the landscape category across all of the layer features. Sample classification results are shown in confusion matrix ordering (see Figure <ref type="figure" target="#fig_0">17</ref>). High-quality black-and-white images show subtle details that should be considered when assessing their aesthetic level, whereas high-quality landscape images differentiate from those low-quality ones in a more apparent way. Similar observations are found, e.g., in the humorous and rural categories.</p><p>The observation explains why it could be inherently difficult for the baseline model to judge whether images from some specific categories are aesthetically pleasing or not, revealing yet another challenge in the assessment of image aesthetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From generic aesthetics to user-specific taste</head><p>Individual users may hold different opinions on the aesthetic quality of any single image. One may consider that all of the images in Figure <ref type="figure" target="#fig_4">13</ref> are of high quality to some extent, even though the average scores by the data set annotators say otherwise. Coping with individual aesthetic bias is a challenging problem. We may follow the idea behind transfer learning <ref type="bibr" target="#b82">[83]</ref> and directly model the aesthetic preference of individual users by transferring the learned aesthetic features to fitting personal taste. In particular, we consider that the DAN-1 baseline network has already captured a sense of generic aesthetics in the aforementioned learning process; so to adapt to personal aesthetic preferences, one can include additional data sources for positive training samples that are user specific, such as the user's personal photographic album or the collection of photos that the user "liked" on social media. As such, our proposed baseline can be further fine-tuned with personal-taste data for individual users and become a personalized aesthetic classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image aesthetic manipulation</head><p>A task closely related to image aesthetic assessment is image aesthetics manipulation, the aim of which is to improve the aesthetic quality of an image. A full review of the techniques of image aesthetics manipulation in the literature is beyond the scope of this article. Still, we make an attempt to connect image aesthetic assessment to a broader topic surrounding image aesthetics by focusing on one of the major aesthetic enhancement operations, i.e., automatic image cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aesthetics-based image cropping</head><p>Image cropping improves the aesthetic composition of an image by removing undesired regions, increasing its aesthetic value. A majority of cropping schemes in the literature can be divided into three main approaches. Attention/saliency-based approaches <ref type="bibr" target="#b98">[99]</ref>- <ref type="bibr" target="#b100">[101]</ref> typically extract the primary subject region in the scene of interest according to attention scores or saliency maps as the image crops. Aesthetics-based approaches <ref type="bibr" target="#b101">[102]</ref>- <ref type="bibr" target="#b103">[104]</ref> assess the attractiveness of some proposed candidate crop windows with low-level image features and rules of photographic composition. However, simple handcrafted features are not robust for modeling the huge aesthetic space. The state-of-the-art method is the change-based approach proposed by Yan et al. <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b105">[106]</ref>, which aims to   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rural Landscape</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Black and White Humorous</head><note type="other">Prediction High Quality High Quality Low Quality Low Quality Actual Class Prediction High Quality High Quality Low Quality Low Quality Actual Class FIGURE 17.</note><p>A layer-by-layer analysis of classification results using the best layer features on (a) black-and-white category images and (b) landscape category images <ref type="bibr" target="#b48">[49]</ref>.</p><p>account for what is removed and changed by cropping itself and trying to incorporate the influence of the starting composition of the initial image in the ending composition of the cropped image. This approach produces reasonable crop windows, but the time cost of producing an image crop is prohibitively expensive because of the time spent in evaluating large numbers of crop candidates. Automatic thumbnail generation is also closely related to automatic image cropping. Huang et al. <ref type="bibr" target="#b106">[107]</ref> target visual representativeness and foreground recognizability when cropping and resizing an image to generate its thumbnail. Chen et al. <ref type="bibr" target="#b107">[108]</ref> aim at extracting the most visually important region as the image crop. Nevertheless, the aesthetics aspects of cropping are not taken into prime consideration in these approaches.</p><p>In the next section, we show that high-quality image crops can already be produced from the last convolutional layer of the aesthetic classification CNN. Optionally, this convolutional response can be utilized as the input to a cropping regression layer for learning more precise cropping windows from additional crop data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Plausible formulations based on deep models</head><p>Fine-tuning a CNN model for the task of aesthetic quality classification (see the "Experiments on Deep-Learning Settings" section) can be considered as a learning process in which the fine-tuned model tries to understand the metric of image aesthetics. We hypothesize that the same metric is applicable to the task of automatic image cropping. We discuss two possible variants as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAN-1 (original) without cropping data</head><p>Without utilizing additional image cropping data, a CNN such as the one-column CNN baseline DAN-1 can be tweaked to produce image crops with minor modifications, removing the fully connected layers. That leaves us with a neural network that is fully convolutional where the input can be of arbitrary size, as shown in Figure <ref type="figure" target="#fig_18">18</ref>(b). The output of the last convolutional layer of the modified model is 14 14 512 # # dimensional, where the 512 feature maps contain the responses/activations corresponding to the input. To generate the final image crop, we take an average of the 512 feature maps and resize it to the input image size. After that, a binary mask is generated by suppressing the feature map values below a threshold. The output crop window is produced by taking a rectangle convex hull from the largest connected region of this binary mask. = w e follow insights in <ref type="bibr" target="#b110">[111]</ref> and add a window regression layer to learn a mapping from the convolutional response [see Figure <ref type="figure" target="#fig_18">18(c)</ref>]. As such, we can predict a more precise cropping window by learning this extended regressor from such crop data by a Euclidean loss function:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAN-1 (regression) with cropping data</head><formula xml:id="formula_14">( ) , W L n Y Y 1 crop crop i i i n 2 1 = - = t /<label>(12)</label></formula><p>where Y crop i t is the predicted crop window for input image .</p><formula xml:id="formula_15">x crop i</formula><p>To learn the regression parameters for this additional layer, the image cropping data set by Yan et al. <ref type="bibr" target="#b104">[105]</ref> is used for further fine-tuning. Images in the data set are labeled with ground-truth crops by professional photographers. Following the evaluation criteria in <ref type="bibr" target="#b104">[105]</ref>, a fivefold cross-validation approach is adopted for evaluating the model performance on High-quality image crops can already be produced from the last convolutional layer of the aesthetic classification CNN.</p><p>all images in the data set. Note that there are only a few hundred images in each training fold; hence, a direct fine-tuning by simply warping the few hundred images of input to 224 224 3 # # could be vulnerable to overfitting. To this end, we fix the weights in the convolutional layers of the DAN-1 (regression) network and learn only the weights for the crop window regression layers. Also, a systematic augmentation approach is adopted as follows. First, the input images are randomly jittered for a few pixels ( ), 5 # and mirroring is performed ( ). 2 # Second, we warp the images to have their longer side equal to 224, hence keeping their aspect ratio. We further downscale the images using a scale of %, %, %, % C 50 <ref type="bibr" target="#b59">60</ref>  -and the fine-tuning process converges at around the second epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aesthetics-based image cropping</head><p>As shown in Figure <ref type="figure" target="#fig_19">19</ref>, we observe that the convolutional response of the vanilla VGG-16 (ImageNet) for object recognition typically finds a precise focus of the salient object in view, while the one-column CNN baseline, i.e., the DAN-1 (original) for aesthetic quality classification, outputs an aes-thetically oriented salient region where both the object in view and its object composition are revealed. Compared to the cropping performance using the vanilla VGG-16, image crops from our DAN-1 (original) baseline already have the capability of removing unwanted regions while preserving the aesthetically salient part in view (see Figure <ref type="figure" target="#fig_19">19</ref>). The modified CNN, i.e., the DAN-1 (regression), further incorporates aesthetic composition information in its crop window regression layer, which serves to refine the crop coordinates for more precise crop generation.</p><p>Following the evaluation settings in <ref type="bibr" target="#b104">[105]</ref> and <ref type="bibr" target="#b105">[106]</ref>, we use the average overlap ratio and average boundary displacement error to quantify the performance of automatic image cropping. A higher overlap and a lower displacement between the generated crop and the corresponding ground truth indicate a more precise crop predictor. As shown in Table <ref type="table" target="#tab_14">9</ref>, directly using the DAN-1 (original) baseline responses to construct image crops already gains competitive cropping performance, while fine-tuning the DAN-1 (regression) with cropping data further boosts the performance and even surpasses the previous state-of-theart method <ref type="bibr" target="#b104">[105]</ref> on this data set, especially in terms of the boundary displacement error. Last but not least, it is worth noting that the CNN-based cropping approach takes merely ~0.2 s for generating an output image crop on a graphics processing unit and ~2 s on a central processing unit (compared to ~11 s on CPU in <ref type="bibr" target="#b104">[105]</ref>).  *There are separate ground-truth annotations by three different photographers in the cropping data set of <ref type="bibr" target="#b106">[107]</ref>. The first number is the average overlap ratio (higher is better). The second number (shown in parentheses) is the average boundary displacement error (lower is better). Bold values signify the best performance by the corresponding methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. (a) Some high-quality images following well-established photographic rules (top row: color harmony; middle row: single salient object and low depth of field; bottom row: black-and-white portraits with decent lighting contrast). (b) A typical flow of image aesthetic assessment systems. SVM: support vector machine; SVR: support vector regressor.</figDesc><graphic coords="3,52.85,84.98,236.30,177.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b), i.e., a feature extraction component and a decision component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. Quality measurements by peak signal-to-noise ratio (PSNR), SSIM<ref type="bibr" target="#b30">[31]</ref>, and VIF<ref type="bibr" target="#b31">[32]</ref> (a higher measurement is better, typically made against a referencing ground-truth high-quality image). Although these are good indicators for measuring the quality of images in image restoration applications, such as the images in (a), they do not reflect human-perceived aesthetic values, as shown by the measurements for the building images in (b).</figDesc><graphic coords="5,373.49,509.91,105.62,144.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>contains 1.5 million images derived from DPChallenge and Photo.net. Similar to AVA, images in the IAD data set are scored by annotators. Positive examples are selected from those images with a mean score larger than a threshold. All IAD images are used for model training, and the model performance is evaluated on AVA in [55]. The ratio of the number of positive examples to that of the negative</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3. Some sample images in the CUHK-PQ data set [45]. (a) Distinctive differences can be visually observed between the high-quality (grouped in the green-framed box) and low-quality images (grouped in the red-framed box). (b) The number of images in the CUHK-PQ data set.</figDesc><graphic coords="6,237.95,551.18,170.90,114.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 4 .</head><label>4</label><figDesc>FIGURE 4. Some sample images in the AVA data set [49]. (a) Images in the green-framed box are labeled with a mean score of &gt;5. Images in the red-framed box are labeled with a mean score of &lt;5. The image groups on the right are ambiguous, with a somewhat neutral scoring around five. (b) The number of images in the AVA data set.</figDesc><graphic coords="7,232.12,543.88,170.54,115.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 5 .</head><label>5</label><figDesc>FIGURE 5. (a) An image composition with low depth of field, a single salient object, and the rule of thirds [49]. (b) An image of low aesthetic quality [45].</figDesc><graphic coords="8,410.86,591.70,103.94,72.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc><ref type="bibr" target="#b56">[57]</ref> propose a two-column CNN for learning aesthetic feature representation. The first column CNN1 ^h takes image patches as input, and the second column CNN2 ^h takes a global image as input. Instead of randomly sampling image patches, given an input image, a weakly supervised learning algorithm is used to project a set of D textual attributes learned from image tags to highly responsive image regions. Such image regions in images are then fed to the input of CNN .1 This CNN1 contains four convolution layers and one fully connected layer fc5 ^h at the bottom. Then a parallel group of D output branches , the D textual attributes are connected on top. The size of the feature maps of each of the fc i 6 is of 128 dimensions. A similar CNN2 takes a globally warped image as input, producing one more 128-dimension feature vector from fc .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 FIGURE 8 .</head><label>28</label><figDesc>FIGURE 8. A typical multitask CNN consists of a main task (task 1) and multiple auxiliary tasks, only one of which is shown here (task 2)<ref type="bibr" target="#b48">[49]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>we select from a distribution of the training partition. Since the number of positive examples in the AVA training partition is almost twice that of the negative examples [Figure 4(b)], models trained with such minibatches may bias toward predicting positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>FIGURE 9 .</head><label>9</label><figDesc>FIGURE 9. (a) The structure of the chosen base network for our systematic study on aesthetic quality classification. (b) The structure of the one-column CNN baseline with multitask learning [49].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>FIGURE 10 .</head><label>10</label><figDesc>FIGURE 10. Aesthetic embeddings of AVA images (testing partition) learned by triplet loss, visualized using t-SNE<ref type="bibr" target="#b83">[84]</ref>: (a) ordinary supervised learning without triplet pretraining and multitask learning, (b) triplet pretrained, and (c) combined triplet pretraining and multitask learning. t-SNE: t-distributed stochastic neighbor embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>label corresponding to each auxiliary class c C ! and y aux c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>FIGURE 11 .</head><label>11</label><figDesc>FIGURE 11. Some positive examples (high-quality images) that are wrongly classified by BDN and DMA-net but correctly classified by the DAN-1 baseline [49].</figDesc><graphic coords="18,52.49,211.29,87.86,118.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>FIGURE 12 .</head><label>12</label><figDesc>FIGURE 12. Some negative examples (low-quality images) that are wrongly classified by BDN and DMA-net but correctly classified by the DAN-1 baseline [49].</figDesc><graphic coords="18,339.41,557.08,84.50,127.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>FIGURE 18 .</head><label>18</label><figDesc>FIGURE 18. (a) The originally proposed one-column CNN baseline. (b) A tweaked CNN made by removing all of the fully connected layers. (c) A modified CNN incorporating a crop-regression layer to learn cropping coordinates [49].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>FIGURE 19 .</head><label>19</label><figDesc>FIGURE 19. The layer response differences of the last convolutional layer. The images in each row correspond to (a) the input image with ground-truth crop, (b) the feature response of the vanilla VGG, (c) the image crops obtained via the feature responses of the vanilla VGG, (d) the feature response of the DAN-1 (original) model, (e) the image crops obtained via the DAN-1 (original) model, (f) the four-coordinates window estimated by the DAN-1 (regression) network, and (g) the cropped image generated by the DAN-1 (regression) [107].</figDesc><graphic coords="24,52.49,74.00,462.14,225.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 6  illustrates a typical single-column CNN.) In particular, the last layer of the CNN for aesthetic classification is modified to output two-dimensional softmax probabilities. This CNN is trained from scratch using aesthetic data, and the penultimate layer (fc7) output is used as the feature representation. To further analyze the effectiveness of the features learned from other tasks, Peng et al. analyze different pretraining and fine-tuning strategies and evaluate the performance of different combinations of the concatenated fc7 features from the eight CNNs.</figDesc><table><row><cell cols="2">Wang et al. [53] propose a CNN that is modified from</cell></row><row><cell cols="2">the AlexNet architecture. Specifically, the conv5 layer of</cell></row><row><cell cols="2">AlexNet is replaced by a group of seven convolutional layers</cell></row><row><cell cols="2">(with respect to different scene categories), which are stacked</cell></row><row><cell cols="2">in a parallel manner with mean pooling before feeding to the</cell></row><row><cell cols="2">fully connected layers, i.e., , , conv conv conv " human landscape 5 3 5 4 5 conv , 5 1 n ight 5 ---The fully connected layers fc6 and fc7 are modified to output , , conv animal architecture 5 2 --, . conv conv plant static 5 6 5 7 --,</cell></row><row><cell cols="2">512 feature maps instead of 4,096 for more efficient param-</cell></row><row><cell cols="2">eter learning. The 1,000-class softmax output is changed to</cell></row><row><cell cols="2">two-class softmax (fc8) for binary classification. The advan-</cell></row><row><cell cols="2">tage of this CNN using such a group of seven parallel con-</cell></row><row><cell cols="2">volutional layers is to exploit the aesthetic aspects in each of</cell></row><row><cell cols="2">the seven scene categories. During pretraining, a set of images</cell></row><row><cell cols="2">belonging to one of the scene categories is used for each of</cell></row><row><cell>the</cell><cell>( { , ..., }) conv i 1 7 i 5</cell></row></table><note><p><p><p>Peng et al.</p><ref type="bibr" target="#b51">[52]</ref> </p>propose to train CNNs of AlexNet-like architecture for eight different abstract tasks (emotion classification, artist classification, artistic style classification, aesthetic classification, fashion style classification, architectural style classification, memorability prediction, and interestingness prediction). (</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>to allow arbitrary-sized images as input; specifically, it pools a fixed-length output, given different receptive field sizes, after the last convolution layer. By varying the kernel size of the adaptive pooling layer, each subnetwork effectively encodes multiscale image information. Moreover, to potentially exploit the aesthetic aspect of different image categories, a scene categorization CNN outputs a scene category posterior for each input image. Then a final scene-aware aggregation layer processes such aesthetic features (category posterior and multiscale VGG features) and outputs the final classification label. The design of this multicolumn network has the advantage of being able to exploit the multiscale composition of an image in each subcolumn by adaptive pooling, yet the multiscale VGG features may contain redundant or overlapping information, which could potentially lead to network overfitting.Wang et al.</figDesc><table><row><cell>Convolution</cell><cell>Fully Connected</cell></row><row><cell>Convolution</cell><cell>Fully Connected</cell></row></table><note><p><p><p><p><p>Output FIGURE 6. The architecture of a typical single-column CNN</p><ref type="bibr" target="#b48">[49]</ref></p>. Output</p>FIGURE 7. A typical multicolumn CNN (a two-column architecture is shown as an example) [49].</p>designed</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 . An overview of typical evaluation criteria.</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="4">Formula</cell><cell></cell><cell></cell><cell>Remarks</cell></row><row><cell>Overall accuracy</cell><cell cols="4">P N TP TN + +</cell><cell></cell><cell></cell><cell>Accounting for the proportion of correctly classified samples.</cell></row><row><cell>Balanced accuracy</cell><cell>2 1</cell><cell cols="2">P TP</cell><cell>+</cell><cell>2 1</cell><cell cols="2">N TN</cell><cell>Averaging precision and true negative prediction for imbalanced distribution.</cell></row><row><cell>PR curve</cell><cell cols="7">p TP FP TP = +</cell><cell>,</cell><cell>r TP FN TP = +</cell><cell>Measuring the relationship between precision and recall.</cell></row><row><cell>Euclidean distance</cell><cell></cell><cell>i /</cell><cell cols="5">( Y Y i i -t</cell><cell>)</cell><cell>2</cell><cell>Measuring the difference between the ground-truth score and aesthetic ratings. Y: ground-truth score, : Y t predicted score.</cell></row><row><cell>Correlation ranking</cell><cell cols="6">( , cov rg rg rg rg X X Y v v</cell><cell>Y</cell><cell>)</cell><cell>Measuring the statistical dependence between the ranking of aesthetic prediction and ground truth. rg X , rg Y : rank variables, : v standard deviation, cov: covariance.</cell></row><row><cell>ROC curve</cell><cell cols="7">tpr TP FN TP = +</cell><cell>,</cell><cell>fpr FP TN FP = +</cell><cell>Measuring model performance change by true positive rate and false positive rate when the binary discrimination threshold is varied.</cell></row><row><cell>Mean AP</cell><cell>n 1</cell><cell>i n</cell><cell></cell><cell cols="4">( precision</cell><cell>() i</cell><cell>recall # D</cell><cell>()) i</cell></row></table><note><p><p><p>/</p>The averaged AP values, based on precision and recall. precision(i ) is calculated among the first i predictions, : i recall D ^h change in recall.</p>TP: true positive, TN: true negative, P: total positive, N: total negative, FP: false positive, FN: false negative, tpr: true positive rate, fpr: false positive rate.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . The methods evaluated on the CUHK-PQ data set.</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Data Set</cell><cell>Metric</cell><cell>Result</cell><cell>Training-Testing Remarks</cell></row><row><cell>Su et al. (2011) [72]</cell><cell>CUHK-PQ</cell><cell>Overall accuracy</cell><cell>92.06%</cell><cell>1,000 training, 3,000 testing</cell></row><row><cell>Marchesotti et al. (2011) [47]</cell><cell>CUHK-PQ</cell><cell>Overall accuracy</cell><cell>89.90%</cell><cell>50-50 split</cell></row><row><cell>Zhang et al. (2014) [67]</cell><cell>CUHK-PQ</cell><cell>Overall accuracy</cell><cell>90.31%</cell><cell>50-50 split, 12,000 subset</cell></row><row><cell>Dong et al. (2015) [50]</cell><cell>CUHK-PQ</cell><cell>Overall accuracy</cell><cell>91.93%</cell><cell>50-50 split</cell></row><row><cell>Tian et al. (2015) [54]</cell><cell>CUHK-PQ</cell><cell>Overall accuracy</cell><cell>91.94%</cell><cell>50-50 split</cell></row><row><cell>Zhang et al. (2016) [57]</cell><cell>CUHK-PQ</cell><cell>Overall accuracy</cell><cell>88.79%</cell><cell>50-50 split, 12,000 subset</cell></row><row><cell>Wang et al. (2016) [53]</cell><cell>CUHK-PQ</cell><cell>Overall accuracy</cell><cell>92.59%</cell><cell>4:1:1 partition</cell></row><row><cell>Lo et al. (2012) [66]</cell><cell>CUHK-PQ</cell><cell>Area under ROC curve</cell><cell>0.93</cell><cell>50-50 split</cell></row><row><cell>Tang et al. (2013) [45]</cell><cell>CUHK-PQ</cell><cell>Area under ROC curve</cell><cell>0.9209</cell><cell>50-50 split</cell></row><row><cell>Lv et al. (2016) [51]</cell><cell>CUHK-PQ</cell><cell>Mean AP</cell><cell>0.879</cell><cell>50-50 split</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 . The methods evaluated on the AVA data set.</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Data Set</cell><cell>Metric</cell><cell>Result</cell><cell>Training-Testing Remarks</cell></row><row><cell>Marchesotti et al. (2013) [48]</cell><cell>AVA</cell><cell>ROC curve</cell><cell>tpr: 0.7, fpr: 0.4</cell><cell>Standard partition</cell></row><row><cell>AVA handcrafted features (2012) [49]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>68.00%</cell><cell>Standard partition</cell></row><row><cell>Spatial pyramid pooling (SPP) (2015) [24]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>72.85%</cell><cell>Standard partition</cell></row><row><cell>RAPID (full method) (2014) [23]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>74.46%</cell><cell>Standard partition</cell></row><row><cell>Peng et al. (2016) [52]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>74.50%</cell><cell>Standard partition</cell></row><row><cell>Kao et al. (2016) [58]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>74.51%</cell><cell>Standard partition</cell></row><row><cell>RAPID (improved version) (2015) [55]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>75.42%</cell><cell>Standard partition</cell></row><row><cell>DMA-net (2015) [24]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>75.41%</cell><cell>Standard partition</cell></row><row><cell>Kao et al. (2016) [59]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>76.15%</cell><cell>Standard partition</cell></row><row><cell>Wang et al. (2016) [53]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>76.94%</cell><cell>Standard partition</cell></row><row><cell>Kong et al. (2016) [25]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>77.33%</cell><cell>Standard partition</cell></row><row><cell>BDN (2016) [56]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>78.08%</cell><cell>Standard partition</cell></row><row><cell>Zhang et al. (2014) [67]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>83.24%</cell><cell>10% subset, 12.5k*2</cell></row><row><cell>Dong et al. (2015) [50]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>83.52%</cell><cell>10% subset, 19k*2</cell></row><row><cell>Tian et al. (2016) [54]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>80.38%</cell><cell>10% subset, 20k*2</cell></row><row><cell>Wang et al. (2016) [53]</cell><cell>AVA</cell><cell>Overall accuracy</cell><cell>84.88%</cell><cell>10% subset, 25k*2</cell></row><row><cell>Lv et al. (2016) [51]</cell><cell>AVA</cell><cell>Mean AP</cell><cell>0.611</cell><cell>10% subset, 20k*2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 . The methods evaluated other data sets.</head><label>4</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Data Set</cell><cell>Metric</cell><cell>Result</cell></row><row><cell>Tong et al. (2004) [20]</cell><cell>29,540-image private set</cell><cell>Overall accuracy</cell><cell>95.10%</cell></row><row><cell>Datta et al. (2006) [21]</cell><cell>3,581-image private set</cell><cell>Overall accuracy</cell><cell>75%</cell></row><row><cell>Sun et al. (2009) [38]</cell><cell>600-image private set</cell><cell>Euclidean distance</cell><cell>3.5135</cell></row><row><cell>Wong et al. (2009) [63]</cell><cell>3,161-image private set</cell><cell>Overall accuracy</cell><cell>79%</cell></row><row><cell>Bhattacharya (2010, 2011) [43], [64]</cell><cell>~650-image private set</cell><cell>Overall accuracy</cell><cell>86%</cell></row><row><cell>Li et al. (2010) [70]</cell><cell>500-image private set</cell><cell>Residual sum-of-squares error</cell><cell>2.38</cell></row><row><cell>Wu et al. (2010) [65]</cell><cell>10,800-image private set from Flickr</cell><cell>Overall accuracy</cell><cell>~83%</cell></row><row><cell>Dhar et al. (2011) [44]</cell><cell>16,000-image private set from DPChallenge</cell><cell>PR curve</cell><cell>-</cell></row><row><cell>Nishiyama et al. (2011) [41]</cell><cell>12,000-image private set from DPChallenge</cell><cell>Overall accuracy</cell><cell>77.60%</cell></row><row><cell>Lo et al. (2012) [42]</cell><cell>4,000-image private set</cell><cell>ROC curve</cell><cell>tpr: 0.6, fpr: 0.3</cell></row><row><cell>Yeh et al. (2012) [46]</cell><cell>309-image private set</cell><cell>Kendalls Tau-b measure</cell><cell>0.2812</cell></row><row><cell>Aydin et al. (2015) [62]</cell><cell>955-image subset from DPChallenge.com</cell><cell>Human survey</cell><cell>-</cell></row><row><cell>Yin et al. (2012) [73]</cell><cell>13,000-image private set from Flickr</cell><cell>Overall accuracy</cell><cell>81%</cell></row><row><cell>Lienhard et al. (2015) [71]</cell><cell>Human Face Scores 250-image data set</cell><cell>Overall accuracy</cell><cell>86.50%</cell></row><row><cell>Sun et al. (2015) [74]</cell><cell>1,000-image Chinese handwriting</cell><cell>Euclidean distance</cell><cell>-</cell></row><row><cell>Kong et al. (2016) [25]</cell><cell>AADB data set</cell><cell>Spearman ranking</cell><cell>0.6782</cell></row><row><cell>Zhang et al. (2016) [57]</cell><cell>PNE</cell><cell>Overall accuracy</cell><cell>86.22%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 . Training from scratch versus fine-tuning.</head><label>5</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Balanced Accuracy</cell><cell>Overall Accuracy</cell></row><row><cell>RAPID (global) [23]</cell><cell>-</cell><cell>67.8</cell></row><row><cell>DAN-1</cell><cell>68.0</cell><cell>71.3</cell></row><row><cell>(fine-tuned from AlexNet)</cell><cell></cell><cell></cell></row><row><cell>DAN-1</cell><cell>72.8</cell><cell>74.1</cell></row><row><cell>(fine-tuned from VGG-16)</cell><cell></cell><cell></cell></row></table><note><p><p><p>Using a one-column CNN baseline (DAN-1) fine-tuned on AlexNet and VGG-16, both of which are pretrained on the ImageNet data set. The authors in</p><ref type="bibr" target="#b22">[23]</ref> </p>have not released detailed classification results.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 . The effects of minibatch formation.</head><label>6</label><figDesc></figDesc><table><row><cell>Minibatch Formation</cell><cell cols="2">Balanced Accuracy Overall Accuracy</cell></row><row><cell>DAN-1 (randomly sampled)</cell><cell>70.39</cell><cell>77.65</cell></row><row><cell cols="2">DAN-1 (balanced formation) 72.82</cell><cell>74.06</cell></row></table><note><p>Using a one-column CNN baseline (DAN-1) with VGG-16 as the base network.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 . Triplets pretraining and multitask learning.</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell>Balanced</cell><cell>Overall</cell></row><row><cell>Methods</cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell>DAN-1</cell><cell>72.82</cell><cell>74.06</cell></row><row><cell>DAN-1 (triplet pretrained)</cell><cell>73.29</cell><cell>75.32</cell></row><row><cell>DAN-1 (multitask-aesthetic and category)</cell><cell>73.39</cell><cell>75.36</cell></row><row><cell>DAN-1 (triplet pretrained + multitask)</cell><cell>73.59</cell><cell>74.42</cell></row><row><cell cols="3">Using a one-column CNN baseline (DAN-1) with VGG-16 as the base network.</cell></row><row><cell>Balanced minibatch formation is used.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 . A comparison of aesthetic quality classification between our proposed baselines and previous state-of-the-art methods on the canonical AVA testing partition.</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell>Balanced</cell><cell>Overall</cell></row><row><cell>Previous Work</cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell>AVA handcrafted features (2012) [49]</cell><cell>-</cell><cell>68.00</cell></row><row><cell>SPP (2015) [24]</cell><cell>-</cell><cell>72.85</cell></row><row><cell>RAPID (full method) (2014) [23]</cell><cell>-</cell><cell>74.46</cell></row><row><cell>Peng et al. (2016) [52]</cell><cell>-</cell><cell>74.50</cell></row><row><cell>Kao et al. (2016) [58]</cell><cell>-</cell><cell>74.51</cell></row><row><cell>RAPID (improved version) (2015) [55]</cell><cell>61.77</cell><cell>75.42</cell></row><row><cell>DMA-net (2015) [24]</cell><cell>62.80</cell><cell>75.41</cell></row><row><cell>Kao et al. (2016) [59]</cell><cell>-</cell><cell>76.15</cell></row><row><cell>Wang et al. (2016) [53]</cell><cell>-</cell><cell>76.94</cell></row><row><cell>Kong et al. (2016) [25]</cell><cell>-</cell><cell>77.33</cell></row><row><cell>Mai et al. (2016) [26]</cell><cell>-</cell><cell>77.40</cell></row><row><cell>BDN (2016) [56]</cell><cell>67.99</cell><cell>78.08</cell></row><row><cell>Proposed Baseline Using Random Minibatches</cell><cell></cell><cell></cell></row><row><cell>DAN-1 (VGG-16, AVA global warped input)</cell><cell>70.39</cell><cell>77.65</cell></row><row><cell>DAN-1 (VGG-16, AVA local patches)</cell><cell>68.70</cell><cell>77.60</cell></row><row><cell>Two-column DAN-2</cell><cell>69.45</cell><cell>78.72</cell></row><row><cell>Proposed Baseline Using Balanced Minibatches</cell><cell></cell><cell></cell></row><row><cell>DAN-1 (VGG-16, AVA global warped input)</cell><cell>73.59</cell><cell>74.42</cell></row><row><cell>DAN-1 (VGG-16, AVA local patches)</cell><cell>71.40</cell><cell>75.8</cell></row><row><cell>Two-column DAN-2</cell><cell>73.51</cell><cell>75.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc># is the size of each of the K output maps. Denote M k as the kth output map. Specifically, a point M ,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">224 × 224</cell><cell>fc6 4,096</cell><cell>fc7 4,096</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56 × 56</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>14 × 14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fc7-new</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8,182</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28 × 28</cell><cell>fc8-1 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Aesthetic</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>112 × 112</cell><cell>Prediction</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">224 × 224</cell><cell>fc6 4,096</cell><cell>fc7 4,096</cell><cell>fc8-2 30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56 × 56</cell><cell>Category Prediction</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>14 × 14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28 × 28</cell></row><row><cell cols="6">this dth layer are of size</cell><cell cols="3">w h K # #</cell><cell>,</cell><cell>112 × 112 where w h</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">i j k in output map M k is computed</cell></row><row><cell cols="10">from a local patch region L of the input image I using the</cell></row><row><cell cols="10">forward propagation. By aligning all such points into a vector</cell></row><row><cell>v</cell><cell>L</cell><cell>, M M , i j 1</cell><cell>,..., i j , 2</cell><cell>M</cell><cell>, ..., i j , k</cell><cell>M</cell><cell>, i j K</cell><cell>,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Can aesthetic prediction</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>be facilitated provided</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>that a model understand</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>to which category the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>image belongs?</cell></row></table><note><p><p><p><p>FIGURE 15</p>. The structure of the two-column CNN baseline with multitask learning</p><ref type="bibr" target="#b48">[49]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>A layer-by-layer analysis showing the difficulties of understanding aesthetics across different categories. From the learned feature hierarchy and the classification results, we observe that image aesthetics in the landscape and rural categories can be judged reasonably by the proposed baselines, yet the more ambiguous humorous and black-and-white images are inherently difficult for the model to handle (see also Figure17).</figDesc><table><row><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Balanced Accuracy</cell><cell>0.75 0.7 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell>p 3</cell><cell>p 4</cell><cell>p 5</cell><cell>6</cell><cell>f c 7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Layer Feature</cell><cell></cell><cell></cell></row><row><cell cols="7">FIGURE 16. DAN-2 trained with random</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">minibatches attains the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">highest overall accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">on the AVA standard</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">testing partition compared</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">to the previous state-of-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">the-art methods.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 . The performance on automatic image cropping.</head><label>9</label><figDesc></figDesc><table><row><cell>Previous Work</cell><cell>*Photographer 1</cell><cell>Photographer 2</cell><cell>Photographer 3</cell></row><row><cell>Park et al. [111]</cell><cell>0.6034 (0.1062)</cell><cell>0.5823 (0.1128)</cell><cell>0.6085 (0.1102)</cell></row><row><cell>Yan et al. [108]</cell><cell>0.7487 (0.0667)</cell><cell>0.7288 (0.0720)</cell><cell>0.7322 (0.0719)</cell></row><row><cell>Wang et al. [112]</cell><cell>0.7823 (0.0623)</cell><cell>0.7697 (0.0617)</cell><cell>0.7725 (0.0701)</cell></row><row><cell>Yan et al. [107]</cell><cell>0.7974 (0.0528)</cell><cell>0.7857 (0.0567)</cell><cell>0.7723 (0.0594)</cell></row><row><cell>Proposed Baselines</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vanilla VGG-16 (ImageNet)</cell><cell>0.6971 (0.0580)</cell><cell>0.6841 (0.0618)</cell><cell>0.6715 (0.0613)</cell></row><row><cell>DAN-1 (original) (AVA training partition)</cell><cell>0.7637 (0.0437)</cell><cell>0.7437 (0.0493)</cell><cell>0.7360 (0.0495)</cell></row><row><cell>DAN-1 (regression) (cropping data fine-tuned)</cell><cell>0.8059 (0.0310)</cell><cell>0.7750 (0.0375)</cell><cell>0.7725 (0.0377)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and potential directions</head><p>Models with competitive performance on image aesthetic assessment have been seen in the literature, yet the state of research in this field is far from saturated. Challenging issues include the ground-truth ambiguity due to neutral image aesthetics and how to effectively learn category-specific image aesthetics from the limited amount of auxiliary data information. Image aesthetic assessment can also benefit from an even larger volume of data, with richer annotations, where every single image is labeled by more users with diverse backgrounds. A large and more diverse data set will facilitate the learning of future models and potentially allow more meaningful statistics to be captured.</p><p>In this work, we systematically review major attempts on image aesthetic assessment in the literature and further propose an alternative baseline to investigate the challenging problem of understanding image aesthetics. We also discuss an extension of image aesthetic assessment to the application of automatic image cropping by adapting the learned aestheticclassification CNN for the task of aesthetics-based image cropping. We hope that this survey can serve as a comprehensive reference source and inspire future research in understanding image aesthetics and fostering many potential applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Complete Guide to Light and Lighting in Digital Photography (A Lark Photography Book)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Sterling Publishing Company</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Design and Form: The Basic Course at the Bauhaus and Later</title>
		<author>
			<persName><forename type="first">J</forename><surname>Itten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Photography</forename><surname>London</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Pearson</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neuroscience of aesthetics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vartanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. New York Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">1369</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="194" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Fechner</surname></persName>
		</author>
		<title level="m">Vorschule der Aesthetik</title>
		<meeting><address><addrLine>Wiesbaden, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Breitkopf &amp; Härtel</publisher>
			<date type="published" when="1876">1876</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clive Bell&apos;s &apos;significant form&apos; and the neurobiology of aesthetics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zeki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Human Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">730</biblScope>
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The brain&apos;s specialized systems for aesthetic and perceptual judgment</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ishizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Euro. J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1413" to="1420" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Naturalizing aesthetics: Brain areas for aesthetic appraisal across sensory modalities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tisdelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="250" to="258" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The experience of emotion</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ochsner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="373" to="403" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The structure of emotion: Evidence from neuroimaging studies</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="83" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The neural correlates of subjective pleasantness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kühn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gallinat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="294" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A model of aesthetic appreciation and aesthetic judgments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Leder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Belke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oeberst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Augustin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. J. Psychol</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="489" to="508" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prospects for a cognitive neuroscience of visual aesthetics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin Psychol. and the Arts</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="60" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Functional neuroanatomy of the human visual system: A review of functional MRI studies</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Greenlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">T</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pediatric Ophthalmology</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="119" to="138" />
		</imprint>
	</monogr>
	<note>Genetics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual cortex in humans</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wandell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inner Vision: An Exploration of Art and the Brain</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zeki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nash</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Oxford Univ. Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The artist as neuroscientist</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cavanagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">434</biblScope>
			<biblScope unit="issue">7031</biblScope>
			<biblScope unit="page" from="301" to="307" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Digital Photographer&apos;s Handbook</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Dorling Kindersley Publishing</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Photographer&apos;s Eye: Composition and Design for Better Digital Photos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CRC</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification of digital photos taken by photographers or home users</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Multimedia Information Processing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Aizawa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</editor>
		<editor>
			<persName><surname>Satoh</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3331</biblScope>
			<biblScope unit="page" from="198" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing photo composition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="469" to="478" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RAPID: Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep multi-patch aggregation network for image style, aesthetics, and quality estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Photo aesthetics ranking network with attributes and content adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="662" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Composition-preserving deep photo aesthetics assessment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Subjective and objective quality assessment of image: A survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Ebrahimi</forename><surname>Moghadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Majlesi J. Elect. Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on different approaches used in image quality assessment</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Prabavathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Sci. and Network Security</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A visual information fidelity approach to video quality assessment</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Workshop Video Processing and Quality Metrics for Consumer Electronics</title>
		<meeting>1st Int. Workshop Video essing and Quality Metrics for Consumer Electronics</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="23" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzsky</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>Neural Information Processing Systems Foundation</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Photo assessment based on computational visual attention model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="541" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment based on visual attention analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perkis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Hannuksela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="561" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Photo and video quality evaluation: Focusing on the subject</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf, Computer Vision (ECCV)</title>
		<meeting>European Conf, Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="386" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aesthetic quality classification of photographs based on color harmony</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A statistic approach for photo quality assessment</title>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Information Security and Intelligence Control</title>
		<meeting>IEEE Int. Conf. Information Security and Intelligence Control</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="107" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A framework for photo-quality assessment and enhancement based on visual aesthetics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia, 2010</title>
		<meeting>ACM Int. Conf. Multimedia, 2010</meeting>
		<imprint>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Relative features for photo quality assessment</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2861" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Assessing the aesthetic quality of photographs using generic image descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1784" to="1791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning beautiful (and ugly) attributes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf. (BMVC)</title>
		<meeting>British Machine Vision Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">AVA: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Photo quality assessment with DCNN that understands image well</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia Modeling</title>
		<meeting>Int. Conf. Multimedia Modeling</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="524" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning relative aesthetic quality with a pairwise approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia Modeling</title>
		<meeting>Int. Conf. Multimedia Modeling</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Toward correlating and solving abstract tasks using convolutional neural networks</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Applications Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conf. Applications Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A multi-scene deep learning model for image aesthetic evaluation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process.: Image Commun</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="511" to="518" />
			<date type="published" when="2016-09">Sept. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Query-dependent aesthetic model with deep learning for photo quality assessment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rating image aesthetics using deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2021" to="2034" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dolcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04155</idno>
		<title level="m">Brain-inspired deep networks for image aesthetics assessment</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Describing human aesthetic perception by deeply-learned attributes from Flickr</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07699</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hierarchical aesthetic quality assessment using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process.: Image Commun</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="500" to="510" />
			<date type="published" when="2016-09">Sept. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Visual aesthetic quality assessment with multitask deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04970</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Algorithmic inferencing of aesthetics and emotion in natural images: An exposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2206" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Automated aesthetic analysis of photographic images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Ayd In</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Saliency-enhanced image aesthetics class prediction</title>
		<author>
			<persName><forename type="first">L.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="997" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A holistic approach to aesthetic enhancement of photographs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Computing, Commun., and Applicat</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The good, the bad, and the ugly: Predicting aesthetic image labels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Pattern Recognition (ICPR)</title>
		<meeting>IEEE Int. Conf. Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1586" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Assessment of photo aesthetics with efficiency</title>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Pattern Recognition (ICPR)</title>
		<meeting>IEEE Int. Conf. Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2186" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fusion of multichannel local and global structural cues for photo aesthetics evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1419" to="1429" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV) Workshop on Statistical Learning in Computer Vision</title>
		<meeting>European Conf. Computer Vision (ECCV) Workshop on Statistical Learning in Computer Vision<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Aesthetic quality assessment of consumer photos with faces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Loui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3221" to="3224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Low level features for quality assessment of facial images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lienhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ladret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Computer Vision Theory and Applications (VISAPP)</title>
		<meeting>Int. Conf. Computer Vision Theory and Applications (VISAPP)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scenic photo quality assessment with bag of aesthetics-preserving features</title>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1213" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Assessing photo quality with geo-context and crowdsourced photos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visual Communications and Image Processing Conf</title>
		<meeting>IEEE Visual Communications and Image essing Conf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Aesthetic visual quality evaluation of Chinese handwritings</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Intelligence</title>
		<meeting>Int. Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2510" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C L</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Neural Information Processing Systems Foundation</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Training linear SVMs in linear time</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</title>
		<meeting>ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015-02">Feb 2015</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The balanced accuracy and its posterior distribution</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Brodersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Pattern Recognition (ICPR)</title>
		<meeting>IEEE Int. Conf. Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3121" to="3124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
	<note>Neural Information Processing Systems Foundation</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Inception-v4, Inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Polynet: A pursuit of structural diversity in very deep networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Efficient mini-batch training for stochastic optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</title>
		<meeting>ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The general inefficiency of batch training for gradient descent learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1429" to="1451" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><surname>Platt</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
	<note>Neural Information Processing Systems Foundation</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Visual link retrieval in a database of paintings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Seguin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Striolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="753" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Self-restraint object recognition by model based CNN learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="654" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based CNN with improved triplet loss function</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Visual aesthetic quality assessment with a regression model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1583" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A comprehensive aesthetic quality assessment method for natural images using basic rules of photography</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mavridaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="887" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Multi-level photo quality assessment with multi-view features</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="308" to="319" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Hybrid CNN and dictionarybased models for scene recognition and domain adaptation</title>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Saliency based automatic image cropping using support vector machine classifier</title>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Meghrajani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Innovations Information, Embedded and Communication Systems</title>
		<meeting>Int. Conf. Innovations Information, Embedded and Communication Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Scale and object aware image thumbnailing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Saliency based image cropping</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mazzola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Analysis and Processing</title>
		<meeting>Int. Conf. Image Analysis and essing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Sensation-based photo cropping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="669" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Learning to photograph</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia, 2010</title>
		<meeting>ACM Int. Conf. Multimedia, 2010</meeting>
		<imprint>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Probabilistic graphlet transfer for photo cropping</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="802" to="815" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Change-based image cropping with exclusion and compositional features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Learning the change for automatic image cropping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="971" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Automatic thumbnail generation based on visual representativeness and foreground recognizability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="253" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Automatic image cropping: A computational complexity study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Modeling photo composition and its application to photo re-arrangement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2741" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Learning an aesthetic photo cropping cascade</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Applications Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conf. Applications Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
