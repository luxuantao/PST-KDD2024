<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Describing Visual Scenes using Transformed Dirichlet Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
							<email>esuddert@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
							<email>torralba@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
							<email>willsky@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Describing Visual Scenes using Transformed Dirichlet Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">81E138AD4D86FCFDC091023FD8E32BB0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes. In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image. Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data. For visual scenes, mixture components describe the spatial structure of visual features in an object-centered coordinate frame, while transformations model the object positions in a particular image. Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler. Applied to a dataset of partially labeled street scenes, we show that the TDP's inclusion of spatial structure improves detection performance, flexibly exploiting partially labeled training images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we develop methods for analyzing the features composing a visual scene, thereby localizing and categorizing the objects in an image. We would like to design learning algorithms that exploit relationships among multiple, partially labeled object categories during training. Working towards this goal, we propose a hierarchical probabilistic model for the expected spatial locations of objects, and the appearance of visual features corresponding to each object. Given a new image, our model provides a globally coherent explanation for the observed scene, including estimates of the location and category of an a priori unknown number of objects. This generative approach is motivated by the pragmatic need for learning algorithms which require little manual supervision and labeling. While discriminative models may produce accurate classifiers, they typically require very large training sets even for relatively simple categories <ref type="bibr" target="#b0">[1]</ref>. In contrast, generative approaches can discover large, visually salient categories (such as foliage and buildings <ref type="bibr" target="#b1">[2]</ref>) without supervision. Partial segmentations can then be used to learn semantically interesting categories (such as cars and pedestrians) which are less visually distinctive, or present in fewer training images. Moreover, generative models provide a natural framework for learning contextual relationships between objects, and transferring knowledge between related, but distinct, visual scenes. The principal challenge in developing hierarchical models for scenes is specifying tractable, scalable methods for handling uncertainty in the number of objects. This issue is entirely ignored by most existing models. We address this problem using Dirichlet processes <ref type="bibr" target="#b2">[3]</ref>, a tool from nonparametric Bayesian analysis for learning mixture models whose number of components is not fixed, but instead estimated from data. In particular, we extend the recently proposed hierarchical Dirichlet process (HDP) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> framework to allow more flexible sharing of mixture components between images. The resulting transformed Dirichlet process (TDP) is naturally suited to our scene understanding application, as well as many other domains where "style and content" are combined to produce the observed data <ref type="bibr" target="#b5">[6]</ref>.</p><p>We begin in Sec. 2 by reviewing several related generative models for objects and scenes. Sec. 3 then introduces Dirichlet processes and develops the TDP model, including MCMC methods for learning and inference. We specialize the TDP to visual scenes in Sec. 4, and conclude in Sec. 5 by demonstrating object recognition and segmentation in street scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generative Models for Objects and Scenes</head><p>Constellation models <ref type="bibr" target="#b6">[7]</ref> describe single objects via the appearance of a fixed, and typically small, set of spatially constrained parts (see Fig. <ref type="figure" target="#fig_0">1</ref>). Although they can successfully recognize objects in cluttered backgrounds, they do not directly provide a mechanism for detecting multiple object instances. In addition, it seems difficult to generalize the fixed set of constellation parts to problems where the number of objects is uncertain.</p><p>Grammars, and related rule-based systems, were one of the earliest approaches to scene understanding <ref type="bibr" target="#b7">[8]</ref>. More recently, distributions over hierarchical tree-structured partitions of image pixels have been used to segment simple scenes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. In addition, an image parsing <ref type="bibr" target="#b10">[11]</ref> framework has been proposed which explains an image using a set of regions generated by generic or object-specific processes. While this model allows uncertainty in the number of regions, and hence the number of objects, the high dimensionality of the model state space requires good, discriminatively trained bottom-up proposal distributions for acceptable MCMC performance. We also note that the BLOG language <ref type="bibr" target="#b11">[12]</ref> provides a promising framework for reasoning about unknown objects. As of yet, however, the computational tools needed to apply BLOG to large-scale applications are unavailable.</p><p>Inspired by techniques from the text analysis literature, several recent papers analyze scenes using a spatially unstructured bag of features extracted from local image patches (see Fig. <ref type="figure" target="#fig_0">1</ref>). In particular, latent Dirichlet allocation (LDA) <ref type="bibr" target="#b12">[13]</ref> describes the features x ji in image j using a K component mixture model with parameters θ k . Each image reuses these same mixture parameters in different proportions π j (see the graphical model of Fig. <ref type="figure">2</ref>). By appropriately defining these shared mixtures, LDA may be used to discover object cat-egories from images of single objects <ref type="bibr" target="#b1">[2]</ref>, categorize natural scenes <ref type="bibr" target="#b13">[14]</ref>, and (with a slight extension) parse presegmented captioned images <ref type="bibr">[15]</ref>. While these LDA models are sometimes effective, their neglect of spatial structure ignores valuable information which is critical in challenging object detection tasks. We recently proposed a hierarchical extension of LDA which learns shared parts describing the internal structure of objects, and contextual relationships among known groups of objects <ref type="bibr" target="#b15">[16]</ref>. The transformed Dirichlet process (TDP) addresses a key limitation of this model by allowing uncertainty in the number and identity of the objects depicted in each image. As detailed in Sec. 4 and illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the TDP effectively provides a textural model in which locally unstructured clumps of features are given global spatial structure by the inferred set of objects underlying each scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical Modeling using Dirichlet Processes</head><p>In this section, we review Dirichlet process mixture models (Sec. 3.1) and previously proposed hierarchical extensions (Sec. 3.2). We then introduce the transformed Dirichlet process (TDP) (Sec. 3.3), and discuss Monte Carlo methods for learning TDPs (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dirichlet Process Mixture Models</head><p>Let θ denote a parameter taking values in some space Θ, and H be a measure on Θ. A Dirichlet process (DP), denoted by DP(γ, H), is then a distribution over measures on Θ, where the concentration parameter γ controls the similarity of samples G ∼ DP(γ, H) to the base measure H. Samples from DPs are discrete with probability one, a property highlighted by the following stick-breaking construction <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_0">G(θ) = ∞ k=1 β k δ(θ, θ k ) β k ∼ Beta(1, γ) β k = β k k-1 =1 (1 -β )<label>(1)</label></formula><p>Each parameter θ k ∼ H is independently sampled, while the weights β = (β 1 , β 2 , . . .) use Beta random variables to partition a unit-length "stick" of probability mass.</p><p>In nonparametric Bayesian statistics, DPs are commonly used as prior distributions for mixture models with an unknown number of components <ref type="bibr" target="#b2">[3]</ref>. Let F (θ) denote a family of distributions parameterized by θ. Given G ∼ DP(γ, H), each observation x i from an exchangeable data set x is generated by first choosing a parameter θi ∼ G, and then sampling x i ∼ F ( θi ). Computationally, this process is conveniently described by a set z of independently sampled variables z i ∼ Mult(β) indicating the component of the mixture G(θ) (see eq. ( <ref type="formula" target="#formula_0">1</ref>)) associated with each data point x i ∼ F (θ zi ).</p><p>Integrating over G, the indicator variables z demonstrate an important clustering property.</p><p>Letting n k denote the number of times component θ k is chosen by the first (i -1) samples,</p><formula xml:id="formula_1">p (z i | z 1 , . . . , z i-1 , γ) = 1 γ + i -1 k n k δ(z i , k) + γδ(z i , k)<label>(2)</label></formula><p>Here, k indicates a previously unused mixture component (a priori, all unused components are equivalent). This process is sometimes described by analogy to a Chinese restaurant in which the (infinite collection of) tables correspond to the mixture components θ k , and customers to observations x i <ref type="bibr" target="#b3">[4]</ref>. Customers are social, tending to sit at tables with many other customers (observations), and each table shares a single dish (parameter).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Dirichlet Processes</head><p>In many domains, there are several groups of data produced by related, but distinct, generative processes. For example, in this paper's applications each group is an image, and the data are visual features composing a scene. Given J groups of data, let x j = (x j1 , . . . , x jnj ) denote the n j exchangeable data points in group j.</p><p>Hierarchical Dirichlet processes (HDPs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> describe grouped data with a coupled set of mixture models. To construct an HDP, a global probability measure G 0 ∼ DP(γ, H) is first chosen to define a set of shared mixture components. A measure G j ∼ DP(α, G 0 ) is then independently sampled for each group. Because G 0 is discrete (as in eq. ( <ref type="formula" target="#formula_0">1</ref>)), groups G j will reuse the same mixture components θ k in different proportions:</p><formula xml:id="formula_2">x ji θ z ji π j α H k J n j K x ji θ t ji π j k jt β α H γ k J n j ∞ ∞ x ji θ t ji π j k jt jt β α φ H R γ k k J n j ∞ ∞ ρ θ t ji π j β α φ H R γ k k J n j ∞ w ji λ η O o k jt jt ∞ y ji o ji ρ LDA</formula><formula xml:id="formula_3">G j (θ) = ∞ k=1 π jk δ(θ, θ k ) π j ∼ DP(α, β)<label>(3)</label></formula><p>In this construction, shared components improve generalization when learning from few examples, while distinct mixture weights capture differences between groups.</p><p>The generative process underlying HDPs may be understood in terms of an extension of the DP analogy known as the Chinese restaurant franchise <ref type="bibr" target="#b3">[4]</ref>. Each group defines a separate restaurant in which customers (observations) x ji sit at tables t ji Each table shares a single dish (parameter) θ, which is ordered from a menu G 0 shared among restaurants (groups).</p><p>Letting k jt indicate the parameter θ kjt assigned to table t in group j, we may integrate over G 0 and G j (as in eq. ( <ref type="formula" target="#formula_1">2</ref>)) to find the conditional distributions of these indicator variables:</p><formula xml:id="formula_4">p (t ji | t j1 , . . . , t ji-1 , α) ∝ t n jt δ(t ji , t) + αδ(t ji , t)<label>(4)</label></formula><formula xml:id="formula_5">p (k jt | k 1 , . . . , k j-1 , k j1 , . . . , k jt-1 , γ) ∝ k m k δ(k jt , k) + γδ(k jt , k)<label>(5)</label></formula><p>Here, m k is the number of tables previously assigned to θ k . As before, customers prefer tables t at which many customers n jt are already seated (eq. ( <ref type="formula" target="#formula_4">4</ref>)), but sometimes choose a new table t. Each new table is assigned a dish k j t according to eq. ( <ref type="formula" target="#formula_5">5</ref>). Popular dishes are more likely to be ordered, but a new dish θk ∼ H may also be selected.</p><p>The HDP generative process is summarized in the graphical model of Fig. <ref type="figure">2</ref>. Given the assignments t j and k j for group j, observations are sampled as x ji ∼ F (θ zji ), where z ji = k jtji indexes the shared parameters assigned to the table associated with x ji .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transformed Dirichlet Processes</head><p>In the HDP model of Fig. <ref type="figure">2</ref>, the group distributions G j are derived from the global distribution G 0 by resampling the mixture weights from a Dirichlet process (see eq. ( <ref type="formula" target="#formula_3">3</ref>)), leaving the component parameters θ k unchanged. In many applications, however, it is difficult to define θ so that parameters may be exactly reused between groups. Consider, for example, a Gaussian distribution describing the location at which object features are detected in an image. While the covariance of that distribution may stay relatively constant across object instances, the mean will change dramatically from image to image (group to group), depending on the objects' position relative to the camera.</p><p>Motivated by these difficulties, we propose the Transformed Dirichlet Process (TDP), an extension of the HDP in which global mixture components undergo a set of random transformations before being reused in each group. Let ρ denote a transformation of the parameter vector θ ∈ Θ, φ ∈ Φ the parameters of a distribution Q over transformations, and R a measure on Φ. We begin by augmenting the DP stick-breaking construction of eq. ( <ref type="formula" target="#formula_0">1</ref>) to create a global measure describing both parameters and transformations:</p><formula xml:id="formula_6">G 0 (θ, ρ) = ∞ k=1 β k δ(θ, θ k )q(ρ | φ k ) θ k ∼ H φ k ∼ R<label>(6)</label></formula><p>As before, β is sampled from a stick-breaking process with parameter γ. For each group, we then sample a measure G j ∼ DP(α, G 0 ). Marginalizing over transformations ρ, G j (θ) reuses parameters from G 0 (θ) exactly as in eq. ( <ref type="formula" target="#formula_3">3</ref>). Because samples from DPs are discrete, the joint measure for group j then has the following form:</p><formula xml:id="formula_7">G j (θ, ρ) = ∞ k=1 π jk δ(θ, θ k ) ∞ =1 ω jk δ(ρ, ρ jk ) ∞ =1 ω jk = 1<label>(7)</label></formula><p>Note that within the j th group, each shared parameter vector θ k may potentially be reused multiple times with different transformations ρ jk . Conditioning on θ k , it can be shown that</p><formula xml:id="formula_8">G j (ρ | θ k ) ∼ DP(αβ k , Q(φ k ))</formula><p>, so that the proportions ω jk of features associated with each transformation of θ k follow a stick-breaking process with parameter αβ k .</p><p>Each observation x ji is now generated by sampling ( θji , ρji ) ∼ G j , and then choosing x ji ∼ F ( θji , ρji ) from a distribution which transforms θji by ρji . Although the global family of transformation distributions Q(φ) is typically non-atomic, the discreteness of G j ensures that transformations are shared between observations within group j.</p><p>Computationally, the TDP is more conveniently described via an extension of the Chinese restaurant franchise analogy (see Fig. <ref type="figure">2</ref>). As before, customers (observations) x ji sit at tables t ji according to the clustering bias of eq. ( <ref type="formula" target="#formula_4">4</ref>), and new tables choose dishes according to their popularity across the franchise (eq. ( <ref type="formula" target="#formula_5">5</ref>)). Now, however, the dish (parameter) θ kjt at table t is seasoned (transformed) according to ρ jt ∼ q(ρ jt | φ kjt ). Each time a dish is ordered, the recipe is seasoned differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning via Gibbs Sampling</head><p>To learn the parameters of a TDP, we extend the HDP Gibbs sampler detailed in <ref type="bibr" target="#b3">[4]</ref>. The simplest implementation samples table assignments t, cluster assignments k, transformations ρ, and parameters θ, φ. Let t -ji denote all table assignments excluding t ji , and define k -jt , ρ -jt similarly. Using the Markov properties of the TDP (see Fig. <ref type="figure">2</ref>), we have</p><formula xml:id="formula_9">p t ji = t | t -ji , k, ρ, θ, x ∝ p t | t -ji f x ji | θ kjt , ρ jt (8)</formula><p>The first term is given by eq. ( <ref type="formula" target="#formula_4">4</ref>). For a fixed set of transformations ρ, the second term is a simple likelihood evaluation for existing tables, while new tables may be evaluated by marginalizing over possible cluster assignments (eq. ( <ref type="formula" target="#formula_5">5</ref>)).</p><p>Because cluster assignments k jt and transformations ρ jt are strongly coupled in the posterior, a blocked Gibbs sampler which jointly resamples them converges much more rapidly:</p><formula xml:id="formula_10">p k jt = k, ρ jt | k -jt , ρ -jt , t, θ, φ, x ∝ p k | k -jt q (ρ jt | φ k ) tji=t f (x ji | θ k , ρ jt )</formula><p>For the models considered in this paper, F is conjugate to Q for any fixed observation value. We may thus analytically integrate over ρ jt and, combined with eq. ( <ref type="formula" target="#formula_5">5</ref>), sample a Left: Four of 50 "images" used for training. Center: Global distribution G0(θ) for the HDP, where ellipses are covariance estimates and intensity is proportional to prior probability. Right: Global TDP distribution G0(θ, ρ) over both clusters θ (solid) and translations ρ of those clusters (dashed). new cluster assignment kjt . Conditioned on kjt , we again use conjugacy to sample ρjt . We also choose the parameter priors H and R to be conjugate to Q and F , respectively, so that standard formulas may be used to resample θ, φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Data TDP HDP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transformed Dirichlet Processes for Visual Scenes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Context-Free Modeling of Multiple Object Categories</head><p>In this section, we adapt the TDP model of Sec. 3.3 to describe the spatial structure of visual scenes. Groups j now correspond to training, or test, images. For the moment, we assume that the observed data x ji = (o ji , y ji ), where y ji is the position of a feature corresponding to object category o ji , and the number of object categories O is known (see Fig. <ref type="figure">2</ref>). We then choose cluster parameters θ k = (ō k , µ k , Λ k ) to describe the mean µ k and covariance Λ k of a Gaussian distribution over feature positions, as well as the single object category ōk assigned to all observations sampled from that cluster. Although this cluster parameterization does not capture contextual relationships between object categories, the results of Sec. 5 demonstrate that it nevertheless provides an effective model of the spatial variability of individual categories across many different scenes.</p><p>To model the variability in object location from image to image, transformation parameters ρ jt are defined to translate feature position relative to that cluster's "canonical" mean µ k :</p><p>p o ji , y ji | t ji = t, k j , ρ j , θ = δ(o ji , ōkjt ) × N y ji ; µ kjt + ρ jt , Λ kjt (9) We note that there is a different translation ρ jt associated with each table t, allowing the same object cluster to be reused at multiple locations within a single image. This flexibility, which is not possible with HDPs, is critical to accurately modeling visual scenes. Density models for spatial transformations have been previously used to recognize isolated objects <ref type="bibr" target="#b16">[17]</ref>, and estimate layered decompositions of video sequences <ref type="bibr" target="#b17">[18]</ref>. In contrast, the proposed TDP models the variability of object positions across scenes, and couples this with a nonparametric prior allowing uncertainty in the number of objects.</p><p>To ensure that the TDP scene model is identifiable, we define p (ρ jt | k j , φ) to be a zeromean Gaussian with covariance φ kjt . The parameter prior R is uniform across object categories, while R and H both use inverse-Wishart position distributions, weakly biased towards moderate covariances. Fig. <ref type="figure" target="#fig_1">3</ref> shows a 2D synthetic example based on a single object category (O = 1). Following 100 Gibbs sampling iterations, the TDP correctly discovers that the data is composed of elongated "bars" in the upper right, and round "blobs" in the lower left. In contrast, the learned HDP uses a large set of global clusters to discretize the transformations underlying the data, and thus generalizes poorly to new translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detecting Objects from Image Features</head><p>To apply the TDP model of Sec. 4.1 to images, we must learn the relationship between object categories and visual features. As in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>, we obtain discrete features by vector quantizing SIFT descriptors <ref type="bibr" target="#b18">[19]</ref> computed over locally adapted elliptical regions. To improve discriminative power, we divide these elliptical regions into three groups (roughly circu-lar, and horizontally or vertically elongated) prior to quantizing SIFT values, producing a discrete vocabulary with 1800 appearance "words". Given the density of feature detection, these descriptors essentially provide a multiscale over-segmentation of the image.</p><p>We assume that the appearance w ji of each detected feature is independently sampled conditioned on the underlying object category o ji (see Fig. <ref type="figure">2</ref>). Placing a symmetric Dirichlet prior, with parameter λ, on each category's multinomial appearance distribution η o ,</p><formula xml:id="formula_11">p w ji = b | o ji = o, w -ji , t, k, θ ∝ c bo + λ<label>(10</label></formula><p>) where c bo is the number of times feature b is currently assigned to object o. Because a single object category is associated with each cluster, the Gibbs sampler of Sec. 3.4 may be easily adapted to this case by incorporating eq. ( <ref type="formula" target="#formula_11">10</ref>) into the assignment likelihoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analyzing Street Scenes</head><p>To demonstrate the potential of our TDP scene model, we consider a set of street scene images (250 training, 75 test) from the MIT-CSAIL database. These images contain three "objects": buildings, cars (side views), and roads. All categories were labeled in 112 images, while in the remainder only cars were segmented. Training from semi-supervised data is accomplished by restricting object category assignments for segmented features. Fig. <ref type="figure">4</ref> shows the four global object clusters learned following 100 Gibbs sampling iterations. There is one elongated car cluster, one large building cluster, and two road clusters with differing shapes. Interestingly, the model has automatically determined that building features occur in large homogeneous patches, while road features are sparse and better described by many smaller transformed clusters. To segment test images, we run the Gibbs sampler for 50 iterations from each of 10 random initializations. Fig. <ref type="figure">4</ref> shows segmentations produced by averaging these samples, as well as transformed clusters from the final iteration. Qualitatively, results are typically good, although foliage is often mislabeled as road due to the textural similarities with features detected in shadows across roads.</p><p>For comparison, we also trained an LDA model based solely on feature appearance, allowing three topics per object category and again using object labels to restrict the Gibbs sampler's assignments <ref type="bibr" target="#b15">[16]</ref>. As shown by the ROC curves of Fig. <ref type="figure">4</ref>, our TDP model of spatial scene structure significantly improves segmentation performance. In addition, through the set of transformed car clusters generated by the Gibbs sampler, the TDP explicitly estimates the number of object instances underlying each image. These detections, which are not possible using LDA, are based on a single global parsing of the scene which automatically estimates object locations without a "sliding window" <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have developed the transformed Dirichlet process, a hierarchical model which shares a set of stochastically transformed clusters among groups of data. Applied to visual scenes, TDPs provide a model of spatial structure which allows the number of objects generating an image to be automatically inferred, and lead to improved detection performance. We are currently investigating extensions of the basic TDP scene model presented in this paper which describe the internal structure of objects, and also incorporate richer contextual cues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A scene with faces as described by three generative models. Constellation: Fixed parts of a single face in unlocalized clutter. LDA: Bag of unlocalized face and background features. TDP: Spatially localized clusters of background clutter, and one or more faces (in this case, the sample contains one face and two background clusters). Note: The LDA and TDP images are sampled from models learned from training images, while the Constellation image is a hand-constructed illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of hierarchical models learned via Gibbs sampling from synthetic 2D data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Graphical representations of the LDA, HDP, and TDP models for sharing mixture components θ k , with proportions πj, among J groups of exchangeable data xj = (xj1, . . . , xjn j ). LDA directly assigns observations xji to clusters via indicators zji. HDP and TDP models use "table" indicators tji as an intermediary between observations and assignments kjt to an infinite global mixture with weights β. TDPs augment each table t with a transformation ρjt sampled from a distribution parameterized by φ k jt . Specializing the TDP to visual scenes (right), we model the position yji and appearance wji of features using distributions ηo indexed by unobserved object categories oji.</figDesc><table><row><cell>Hierarchical DP</cell><cell>Transformed DP</cell><cell>Visual Scene TDP</cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Funding provided by the National Geospatial-Intelligence Agency NEGI-1582-04-0004, the National Science Foundation NSF-IIS-0413232, the ARDA VACE program, and a grant from BAE Systems.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Top right: Global model G0 describing object shape (solid) and expected transformations (dashed). Bottom right: ROC curves comparing TDP feature segmentation performance to an LDA of feature appearance. Left: Four test images (first row), estimated segmentations of features into object categories (second row), transformed global clusters associated with each image interpretation (third row), and features assigned to different instances of the transformed car cluster (fourth row).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discovering objects and their location in images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian density estimation and inference using mixtures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Escobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">430</biblScope>
			<biblScope unit="page" from="577" to="588" />
			<date type="published" when="1995-06">June 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical Dirichlet processes</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno>653</idno>
		<imprint>
			<date type="published" when="2004-10">October 2004</date>
		</imprint>
		<respStmt>
			<orgName>U.C. Berkeley Statistics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical Dirichlet processes</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 17</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Bayesian approach to unsupervised one-shot learning of object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Experiments in interpretation-guided segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Barrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intel</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="241" to="274" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image modeling with position-encoding dynamic trees</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="859" to="871" />
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spatial random tree grammars for modeling hierarchal structure in images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Submitted to IEEE Tran. PAMI</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BLOG: Probabilistic models with unknown objects</title>
		<author>
			<persName><forename type="first">B</forename><surname>Milch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolobov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 19</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1352" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matching words and pictures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1107" to="1135" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning hierarchical models of scenes, objects, and parts</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from one example through shared densities on transforms</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning flexible sprites in video layers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
