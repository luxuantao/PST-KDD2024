<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximizing reliability with energy conservation for parallel task scheduling in a heterogeneous cluster</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-02-26">26 February 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Longxin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">National Supercomputing Center</orgName>
								<address>
									<settlement>Changsha</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hunan University</orgName>
								<address>
									<postCode>410082</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kenli</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">National Supercomputing Center</orgName>
								<address>
									<settlement>Changsha</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hunan University</orgName>
								<address>
									<postCode>410082</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuming</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">National Supercomputing Center</orgName>
								<address>
									<settlement>Changsha</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hunan University</orgName>
								<address>
									<postCode>410082</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">National Supercomputing Center</orgName>
								<address>
									<settlement>Changsha</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hunan University</orgName>
								<address>
									<postCode>410082</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Kavli Institute for Astrophysics and Space Research</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Research Institute of Information Technology</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keqin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">National Supercomputing Center</orgName>
								<address>
									<settlement>Changsha</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hunan University</orgName>
								<address>
									<postCode>410082</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York</orgName>
								<address>
									<postCode>12561</postCode>
									<settlement>New Paltz</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Maximizing reliability with energy conservation for parallel task scheduling in a heterogeneous cluster</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-02-26">26 February 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">ED8D24439F13743EF5C5C4B5B0129E02</idno>
					<idno type="DOI">10.1016/j.ins.2015.02.023</idno>
					<note type="submission">Received 29 June 2014 Received in revised form 7 February 2015 Accepted 14 February 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Cluster computing Energy consumption List scheduling Parallel application Reliability 2</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>28</head><p>A heterogeneous computing system in a cluster is a promising computing platform, which 29 attracts a large number of researchers due to its high performance potential. High system 30 reliability and low power consumption are two primary objectives for a data center. 31 Dynamic voltage scaling (DVS) has been proved to be the most efficient technique and is 32 exploited widely to realize a low power system. Unfortunately, transient fault is inevitable 33 during the execution of an application while applying the DVS technique. Most existing 34 scheduling algorithms for precedence constrained tasks in a multiprocessor computer sys-35 tem do not adequately consider task reliability. In this paper, we devise a novel Reliability 36 Maximization with Energy Constraint (RMEC) algorithm, which incorporates three impor-37 tant phases, including task priority establishment, frequency selection, and processor 38 assignment. The RMEC algorithm can effectively balance the tradeoff between high relia-39 bility and energy consumption. Our rigorous performance evaluation study, based on both 40 randomly generated task graphs and the graphs of some real-world applications, shows 41 that our scheduling algorithm surpasses the existing algorithms in terms of system relia-42 bility enhancement and energy consumption saving.</p><p>43</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays, big data applications are progressively becoming the major focus of attention due to the enormous increment of data generation and storage that has taken place in the recent years. The data size is constantly increasing, as of 2012 ranging from a few dozen terabytes to many petabytes of data in a single data set <ref type="bibr" target="#b36">[37]</ref>. The major features of big data are high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery, and process optimization <ref type="bibr" target="#b4">[5]</ref>. Many real-world areas such as telecommunications, health care, pharmaceutical, Internet search, financial and business informatics generate massive amounts of data. Tmall <ref type="bibr" target="#b37">[38]</ref>, which is the largest online shopping site in Asia, has become an indispensable part of daily life of Chinese. During the Chinese billion). About 188 million of transactions were conducted on this day. It contributes to the powerful support provided by Aliyun. In the data center of Aliyun, there are about 40 thousand jobs to be run on a cluster which is composed of 3000 nodes to process the 1.5 petabyte transaction records everyday, and then to output 20 terabyte results.</p><p>A data center garnered significant support and encouragement by its participants, who spanned industry, government labs, and academia <ref type="bibr" target="#b10">[11]</ref>. To meet the needs of economic development, national centers for supercomputing have sprung up. The ranking of the top supercomputers in the world show a major trend in heterogeneous architectures. They are superior by power efficiency rather than speed. Such high end computing facilities can consume a very large amount of power, although they provide high performance computing solutions for scientific and engineering applications. For instance, operating a middle-sized data center (i.e., a university data center) demands 80,000 kW power <ref type="bibr" target="#b33">[34]</ref>. In addition, high power consumption usually leads to expensive cooling costs. Furthermore, keeping computing facilities running on high power for a long time will result in high temperature of computing systems, which further degrades systems' availability and reliability.</p><p>While performance/hardware-cost has increased dramatically with the advancement of electronic technology, power consumption in computer systems has also increased according to Moore's law <ref type="bibr" target="#b35">[36]</ref>. Such increased energy consumption causes severe ecological, economic, and technical issues. Hence, it is not very hard to image the size of adverse environmental footprint left by the heterogeneous computing systems (HCS) in a cluster. This issue has attracted extensive research activities in recent years, with the growing advocacy of green computing systems. Some hardware technologies <ref type="bibr" target="#b32">[33]</ref>, such as energy-efficient monitors, low-power microprocessors and processors consisting of multiple processing element cores and a selective-associative cache memory, are employed to address the energy consumption problems. Comprehensive surveys can be found in Refs. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Task scheduling problems are classic and important. A large number of excellent algorithms are proposed. Huang et al. <ref type="bibr" target="#b14">[15]</ref> proposed three types of fuzzy models to solve the Fuzzy Time-dependent Project Scheduling Problem (FTPSP) while guaranteeing resource constraints. Under the dynamic grid environment, Kołodziej and Khan <ref type="bibr" target="#b21">[22]</ref> introduced a Hierarchic Genetic Strategy based Scheduler (HGS-Sched) to achieve fast reductions in makespan and flowtime in the concurrent search of the optimization domain. An adaptive scoring method was used to schedule jobs in grid environment by Chang et al. <ref type="bibr" target="#b6">[7]</ref>.</p><p>With the rapid development of society, reducing processor energy consumption has been a critically important and pressing research issue in recent years. The dynamic voltage and frequency scaling (DVFS) technique <ref type="bibr" target="#b34">[35]</ref> is widely recognized as the basis of numerous energy management solutions. It exploits the fact that the dynamic power consumption is a strictly convex function of the CPU frequency, and attempts to conserve energy by reducing clock speed and supply voltage at active state. Benefiting from DVFS, various energy-aware task scheduling and resource management methods have emerged as promising studies for sustainable computing. Many excellent strategies and approaches have been developed, but their scope is restricted to unique processor systems <ref type="bibr" target="#b41">[42]</ref>, homogeneous computing systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b42">43]</ref>, and battery based embedded systems <ref type="bibr" target="#b30">[31]</ref>.</p><p>Numerous algorithms have been devised to accomplish speedup for parallel applications in the form of directed acyclic graphs (DAG). It is generally appreciated that a task scheduling problem is NP-hard <ref type="bibr" target="#b11">[12]</ref>. Usually, scheduling algorithms aim to map tasks onto proper processors and sort tasks in an appropriate sequence, so that task precedence constraints are met and the minimum scheduling length can be achieved. A significant number of existing studies are devised for homogeneous systems, such as the well known Dynamic-Level Scheduling (DLS) algorithm <ref type="bibr" target="#b26">[27]</ref>. Recently, a few diverse list scheduling algorithms have been developed to handle heterogeneous systems, for instance, Constrained Earliest Finish Time (CEFT) algorithm <ref type="bibr" target="#b19">[20]</ref>, Critical-Path-On-a-Processor (CPOP) algorithm <ref type="bibr" target="#b31">[32]</ref>, and Heterogeneous Earliest Finish Time (HEFT) algorithm <ref type="bibr" target="#b31">[32]</ref>. Among these algorithms, HEFT and CPOP have been proven to be very promising algorithms with their demonstrated low complexity and performance effective capability. It is widely accepted that a major challenge in scheduling is to diminish interprocessor communication cost. Node duplication is an effective solution that has been exploited to deal with the above described problem. Based on this, recently reported algorithms, such as HCPFD <ref type="bibr" target="#b13">[14]</ref> and HLD <ref type="bibr" target="#b2">[3]</ref>, were proposed for HCS. They improve the performance by taking account of limited effective duplication into them. Idle time slots, scattered among the processors, are exploited for duplicating the immediate parent of a child task so as to make its start time earlier.</p><p>The comparison analysis of HCPFD <ref type="bibr" target="#b13">[14]</ref> and HLD <ref type="bibr" target="#b2">[3]</ref> shows that they significantly outperform other algorithms, for instances, DLS <ref type="bibr" target="#b26">[27]</ref> and HEFT <ref type="bibr" target="#b31">[32]</ref>. However, the duplication technique aims to reduce the schedule length at the expense of sacrificing more energy and higher complexity. With respect to the promotion of system reliability, a number of hardware and software based techniques for hypercube (HC) fault tolerance was developed by Abd-El-Barr and Gebali <ref type="bibr" target="#b0">[1]</ref>, which can be exploited to enhance the system reliability and fault tolerance aspects of existing hypercube multi-computer networks (HCNs). Dogan and Özgüner developed three reliability cost functions that were incorporated into making dynamic level (DL) and introduced a Reliable Dynamic Level Scheduling (RDLS) algorithm <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Tang et al. proposed a Hierarchical Reliability-Driven Scheduling (HRDS) algorithm in a grid computing system <ref type="bibr" target="#b29">[30]</ref>. Wang et al. developed scheduling heuristics which reduce energy consumption of parallel task execution in a cluster by using the DVFS mechanism <ref type="bibr" target="#b33">[34]</ref>.</p><p>Unfortunately, most of these approaches are on the basis of simple system models, which do not precisely reflect the real parallel computation systems. One of the assumptions, i.e., a node never fails during execution, may lead to some problems.</p><p>In real systems, the transition faults in task execution are inevitable and may have an adverse impact on the running applications. Studies in <ref type="bibr" target="#b40">[41]</ref> show that it is critical to design an accurate schedule with consideration of task reliability. Low power consumption and high system reliability, availability, and utility are the main concerns of modern high-performance computing system development. A number of studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b24">25]</ref> revealed the interplay between energy consumption and system reliability. However, these approaches are exclusively confined to the embedded systems. Li and Xu et al. analyzed the performance of heuristic power allocation and scheduling algorithms for precedence constrained parallel tasks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39]</ref>. Lee and Zomaya developed two energy-conscious scheduling algorithms which effectively balance the quality of schedules and energy consumption using dynamic voltage scaling (DVS) <ref type="bibr" target="#b22">[23]</ref>. Tang et al. designed a reliability-driven scheduling architecture and proposed a reliability-aware scheduling algorithm for precedence constrained tasks <ref type="bibr" target="#b28">[29]</ref>. Notwithstanding, none of them incorporates energy consumption and reliability together. In most cases, the scheduling length is not always as small as possible. It is crucial to run an application with higher reliability and lower energy consumption.</p><p>In this paper, we treat maximizing reliability with energy conservation for precedence constrained tasks in a heterogeneous cluster with dynamically variable voltage and speed as an combinatorial optimization problem. Our scheduling problem comprises three nontrivial subproblems, namely, precedence constraining, energy conserving, and reliability maximizing.</p><p>Precedence Constraining. Compared to an independent task set, parallel tasks with precedence constraints make devise and analysis of heuristic scheduling algorithms particularly complicated.</p><p>Energy Conserving. Processors should provide appropriate powers and energy efficient execution speeds, such that the schedule length is modest and the energy consumption is minimal.</p><p>Reliability Maximizing. Tasks should be run at relatively high speeds without exceeding the maximum frequency of processors, such that the system reliability can be achieved optimal.</p><p>The above subproblems should be solved efficiently so that heuristic algorithms with overall fine performance can be explored. By adopting the DVS technique, three algorithms are presented in this paper, i.e., the Reliability-aware Heterogeneous Earliest Finish Time (RHEFT) algorithm, the Reliability-aware Critical-Path-On-a-Processor (RCPOP) algorithm, and a novel Reliability Maximization with Energy Constraint (RMEC) algorithm. Algorithms RHEFT and RCPOP are two intuitive strategies for system reliability enhancement. The main purpose is to lead to the presentation of algorithm RMEC. Each of these three algorithms comprises the following three phases. The task priority establishment phase builds a proper topological order for the application tasks. The frequency selection phase chooses an energy efficient frequency to execute each task. The processor assignment phase allocates a candidate task to a suitable processor, so as to get higher total system reliability with lower total energy consumption.</p><p>The rest of the paper is organized as follows. Section 2 presents the system, energy, application, and reliability models used in this paper. Based on the previously introduced reliability and energy models, Section 3 develops the RMEC algorithm and other two revised algorithms RHEFT and RCPOP. A simple motivational example is presented in Section 4. Extensive experimental results are discussed in Section 5. Finally, Section 6 gives concluding remarks and mentions future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Models</head><p>In this section, we introduce the system, power, application, fault and reliability models used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">System model</head><p>The system model used in this paper comprises a set PE of p heterogeneous cores/processors in a cluster. Each of them is available for DVFS technology; namely, each core can run at different speeds (i.e., different supply voltage levels). Each processor pe which belongs to set PE has f different available frequency levels (AFLs). It follows a random and uniform distribution among the four different sets of operation voltages/frequency. As clock frequency switching overhead takes an inappreciable amount of time (e.g., 10-150 ls <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref>), such overhead is neglected in our paper. Besides, the communications among the processors are supposed to perform at the same speed on all links without contention. Our paper is based on the premise that the target system consists of a set of fully connected processors, which implies that each processor has a direct communication link to every other processor. Inter-processor communication is performed by a dedicated communication subsystem in such a way that is completely free of contention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Power model</head><p>Generally, with the DVFS technique, the clock frequency is reduced alongside with the supply voltage for the approximately linear relationship between the supply voltage and operation frequency <ref type="bibr" target="#b5">[6]</ref>. For the promising capability of energy saving, the DVFS technique is adopted in our study. It enables a processor dynamically adjust available frequency levels. To present a system-level power model, we adopt the classic one proposed in <ref type="bibr" target="#b43">[44]</ref>. And the system power consumption is given as follows <ref type="bibr" target="#b40">[41]</ref>:</p><formula xml:id="formula_0">P ¼ P s þ h P ind þ P d ð Þ¼P s þ h P ind þ C eff f a À Á ;<label>ð1Þ</label></formula><p>where P s is the static power consumption, P ind refers to the frequency-independent active power, and P d represents the frequency-dependent dynamic power. The static power term, including the power to maintain the basic circuits, keeps the clock working and the memory staying in sleep mode, can be removed only by turning off the whole system. P ind is a constant, independent of system operation frequency (i.e., the power consumption occurs while accessing external devices like main memory, I/O, and so on), can be decreased to a very small value by setting the system to standby mode <ref type="bibr" target="#b5">[6]</ref>. P d is the dynamic power dissipation, the dominant component of energy consumption in widely popular CMOS technology. It can be given by P d ¼ C eff Á V 2 dd Á f , where C eff is the effective loading capacitance, V dd is the supply voltage, and f is the clock frequency. Since f / v c ð0 &lt; c &lt; 1Þ <ref type="bibr" target="#b23">[24]</ref>, in other words, v / f 1=c , we reckon that the frequency dependent active power consumption is P d / f a , where a ¼ 1 þ 2=c P 3. In our studies, we have P d ¼ C eff f a . And h indicates the system mode and represents whether active power consumption is occurred present. Particularly, h ¼ 1 signifies that the system is active currently.</p><p>Otherwise, h ¼ 0 refers to a sleep mode that the system is in. In the context of this paper, all frequencies are normalized with respect to the maximum frequency f max (i.e., f max ¼ 1:0). And the energy consumption of task v i can be calculated according to Eq. ( <ref type="formula" target="#formula_1">2</ref>),</p><formula xml:id="formula_1">E i ðf i Þ ¼ P ind i Á c i f i þ C eff Á c i Á f 2 i ;<label>ð2Þ</label></formula><p>where c i is the computational cost at executing frequency of f i . The total energy E total consumed by processors during the execution of tasks in a task set is hence estimated by</p><formula xml:id="formula_2">E total ¼ X n i¼1 E i ðf i Þ:<label>ð3Þ</label></formula><p>For simplicity, only processor energy consumption is considered in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Application model</head><p>Generally, a parallel application program consisting of precedence constrained tasks can be represent by a directed acyclic graph (DAG). A DAG, G ¼ hT; Ei, where T is the task set which comprises jTj tasks that can be executed on any available processors. Set E is composed of the edges which represent task precedence constrains. An edge w i;j 2 E between task node i and node j, both of which perform on different processors, denotes the intertask communication.</p><p>For each node s i in a given task graph, the direct predecessors of which are denoted by parentðs i Þ, which is a set</p><formula xml:id="formula_3">parentðs i Þ ¼ f8s p 2 Tje p;i 2 Eg.</formula><p>And its direct successors are denoted as childðs i Þ. If a task has no any predecessor, namely,</p><formula xml:id="formula_4">parentðs i Þ ¼ ;</formula><p>, it is termed as an entry task. Likewise, if a task has no any successor, namely, childðs i Þ ¼ ;, it is called an exit task.</p><p>Without loss of generality, we assume that a DAG in our study has (or can be transformed to have) exactly one entry task s entry and one exit task s exit . The notations about the system and application models used in this paper are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The weight on task s i is denoted as w i , which represents the computation cost. In addition, the execution time of task s i on processor pe j refers to w i;j and its average computation cost is denoted by w i . Similarly, the weight c i;j assigned to an edge represents the communication cost between two tasks s i and s j . However, the communication occurs only when the two nodes are scheduled to two distinct processors. In other word, there is no communication cost provided that the two nodes are assigned to the same processor.</p><p>Consider the graph with eleven nodes as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the edges, which are labeled with weights, reflect the communication costs of corresponding nodes in different processors. In our study, the target system comprises of a set PE of p The task s i 2 T executed on processor pe j 2 PE c i;j</p><p>The communication cost between node s i and node s j w i</p><p>The average computational time of a task when executed on different processors</p><formula xml:id="formula_5">childðs i Þ The set of immediate successors of task s i parentðs i Þ The set of immediate predecessors of task s i ESTðs i ; pe j Þ</formula><p>The earliest execution start time of task s i on processor pe j EFTðs i ; pe j Þ The earliest execution finish time of task s i on processor pe j</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFLs</head><p>Available The total number of nodes of a special DAG heterogeneous processors. And each one is DVFS enabled. As shown in Table <ref type="table" target="#tab_2">2</ref>, each core can run at different AFLs. For each processor pe i 2 PE, the voltage-relative frequency AFLs is selected randomly among the distinct sets. The execution costs of each node on different processors are shown in Table <ref type="table" target="#tab_4">3</ref>, under the condition that each task runs at the maximum available frequency. According to the previous study <ref type="bibr" target="#b16">[17]</ref>, frequency switching takes a negligible amount to time, about 189-300 ls.</p><p>These overheads are not taken into account in this study while applying the DVFS technique. Besides, communications among processors are also considered to perform at the same speed on all links without the limitation of bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Fault model</head><p>While an application is executing, a fault maybe hard to avoid owing to various reasons, such as hardware failure, software bugs, devices exposed to extreme temperatures, and external interference. As a consequence, transient faults occur more frequently than permanent failures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>. In this paper, we will pay more attention to transient faults in our study, and devise a feasible and efficient scheduling algorithm with the DVFS technology to maximize overall system reliability.</p><p>Extensive works have been done for fault management. Generally, the transient fault is modeled by a Poisson distribution with an average arrival rate k <ref type="bibr" target="#b39">[40]</ref>. Following most previous studies, we assume that transient faults happen during the execution of each task independently. Nevertheless, with the effect of dynamic voltage and frequency scaling, transient faults' average arrival rate will depend on the system processing frequency f, and v is the corresponding voltage. Hence, the fault rate can be modeled as follows:</p><formula xml:id="formula_6">kðf Þ ¼ k 0 Á gðf Þ:<label>ð4Þ</label></formula><p>In the above equation, we have gðf max Þ ¼ 1, where f max ¼ 1:0. Traditionally, it has been recognized as an exponential relationship between the transient fault rate and the circuit's critical cost <ref type="bibr" target="#b17">[18]</ref>. We adopt the exponential model proposed in <ref type="bibr" target="#b43">[44]</ref> for our scheduling model and experiment analysis. It can be expressed as  </p><formula xml:id="formula_7">kðf Þ ¼ k 0 Á gðf Þ ¼ k 0 Á 10 dð1Àf Þ 1Àf min ;<label>ð5Þ</label></formula><p>where k 0 stands for the average fault rate as mentioned before, d is a constant greater than zero, which represents the dependency of fault rate on frequency and voltage scaling. It can be seen easily that the fault rate will increase exponentially when the frequency decreases for energy conservation. In other words, kðf Þ is a strictly decreasing function. Hence, the maximum average fault rate is k max ¼ k 0 Á 10 d , which corresponds to the minimum available frequency.</p><p>Definition 1. The reliability of a task is the probability of executing the task successfully. If the transient fault follows a</p><p>Poisson distribution, the reliability of node s i with the corresponding computation cost</p><formula xml:id="formula_8">c i is [44] R i ðf i Þ ¼ e Àkðf i ÞÂ c i f i ;<label>ð6Þ</label></formula><p>where f i denotes the processing frequency.</p><p>Definition 2. The system reliability R sys denotes the probability of successfully executing an entire task set which consists of n tasks:</p><formula xml:id="formula_9">R sys ¼ P n i¼1 R i ðf i Þ:<label>ð7Þ</label></formula><formula xml:id="formula_10">2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Problem description</head><p>The problem to be solved in this paper can be formally described as follows. Assume that we are given a DAG which comprises tasks with precedence constraints, and processors in a heterogeneous cluster which support different frequency levels.</p><p>Then, the problem to be addressed in this paper is to assign a property execution voltage (or frequency) of an available processor for each ready task in a right order, while assuring the maximum system reliability and guaranteeing the total energy consumption not exceeding a given energy E Ã . Obviously, it is a combinatorial optimization problem, whose formulation is given as follows:</p><p>Maximize: R sys</p><formula xml:id="formula_11">¼ P n i¼1 R i ðf i Þ; subject to : f min 6 f i 6 f max ; ð8 i : 1 6 i 6 nÞ;<label>ð8Þ</label></formula><formula xml:id="formula_12">E total 6 E Ã :<label>ð9Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A motivational example</head><p>As an illustration, Fig. <ref type="figure" target="#fig_1">2</ref> presents the schedule obtained by the RMEC algorithm for the sample DAG in Fig. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The proposed algorithms</head><p>Reliability is critical for an application, sometimes higher system reliability is even more important than shorter schedule length. For a real application, a more useful objective is to assure that the total energy consumption does not exceed E Ã and the system reliability is maximized. In a cluster, task scheduling consists of two major stages, i.e., task priority calculation and task-processor pair selection. In the first stage, we use the efficient URank to calculate the task priorities, which can guarantee the topological order of a given DAG. In the second phase, there are more feasible strategies to decide the task-processor pair while fulfilling the energy constraint.</p><p>In this section, three schemes are developed to solve the problem of maximizing the reliability with energy conservation for parallel tasks in a heterogeneous cluster. The first scheme, namely the Reliability-aware Heterogeneous Earliest Finish Time (RHEFT) algorithm, is to choose the proper frequency to maximize the task execution reliability with the earliest finish time on a heterogeneous processor. The executing frequency for each task depends on the remaining energy and system reliability maximizing. The second scheme to be presented is the Reliability-aware Critical-Path-On-a-Processor (RCPOP) algorithm. In comparison with the RHEFT algorithm, the RCPOP algorithm only differs in the phase of processor selection after finding the efficient frequency for each ready task. Under such a scheme, the distinguishing feature is to find a processor with the highest reliability which can be allocated to the critical path of the application on it. In addition, a new scheme, which is referred to as the Reliability Maximization with Energy Constraint (RMEC) algorithm, picks out the best combination of task-processor according to the maximum reliability and energy conservation during the first round task scheduling.</p><p>After the pre-scheduling, the available time slots are reclaimed to further reduce the energy.</p><p>Before introducing the details of the above three schemes, we firstly present the characteristics of high reliability and energy with task executing frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Relationship between reliability and energy</head><p>In this section, we will present a Reliability Maximization with Energy Constraint (RMEC) algorithm. The RMEC algorithm which will be presented in the following aims at achieving high reliability with the condition of energy constraint and without increasing makespan during the scheduling procedure. Due to the frequency-independent active power, the power consumption no longer varies monotonically with the increasing of frequency. As shown in Eq. ( <ref type="formula" target="#formula_1">2</ref>), it can be easily to deduce that E i ðf i Þ is a strictly convex function and is minimum when</p><formula xml:id="formula_13">f i ¼ f ee ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi P ind =2C eff 3 p</formula><p>(energy efficient frequency) <ref type="bibr" target="#b40">[41]</ref>. Therefore, lower frequencies may not always be of benefit for energy saving and there must exist an optimal voltage-frequency pair for each candidate task to achieve minimum energy consumption. On the other hand, as the processing frequency increases, the reliability of task improves monotonically. It is safely to draw such a conclusion that high reliability and low energy consumption are two contradicting elements for scheduling. Searching a good strategy for the tradeoff of them is a worthwhile work. We will attempt to schedule tasks aiming at improving system reliability and preserving energy consumption.</p><p>Definition 3. The immediate neighboring frequency f in of f ee on processor pe j , is the one which consumes less energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>According to the expression</head><formula xml:id="formula_14">f i ¼ f ee ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi P ind =2C eff 3 p</formula><p>; f ee only depends on P ind . The specific P ind value for each task follows a uniform distribution and is determined randomly with its range between 0.2 and 2.0. So it is not difficult to find that f ee varies on different processors for each task s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Critical phases</head><p>Each of the three algorithms which will be presented in the next part, is mainly composed of three phases. In the following, we introduce two important phases as outlined below.</p><p>Task priority establishment phase. To meet the requirement of task scheduling, a prior order is established in this phase.</p><p>Each task is set with its URank, which is computed recursively according to the expression where childðs i Þ is the set of immediate children of task node s i . The rank is computed recursively by traversing from the bottom of a DAG to the top. It should be apparent to draw such a conclusion that URankðs exit Þ ¼ w exit . Similarly, the downward rank DRank is defined as</p><formula xml:id="formula_15">URankðs i Þ ¼ w i þ max s j 2childðs i Þ c i;j þ URankðs j Þ À Á ;<label>ð10Þ</label></formula><formula xml:id="formula_16">DRankðs i Þ ¼ max s j 2parentðs i Þ w j þ c i;j þ DRankðs j Þ À Á ;<label>ð11Þ</label></formula><p>where parentðs i Þ is the set of direct parents of task node s i . As there is no parent for the entry node, it is easy to conclude that the DRank of the entry node is equal to zero. As shown in Table <ref type="table" target="#tab_4">3</ref>, the computation cost of each node which is performed on a special processor with the maximum available processing speed (AFL 0) is obtained. According to the computation cost given in Table <ref type="table" target="#tab_4">3</ref>, a priority queue for the simple DAG shown in Fig. <ref type="figure" target="#fig_0">1</ref> is maintained for the following three algorithms. The values of task priority using the DRank and URank method are presented in Table <ref type="table" target="#tab_6">4</ref>. DRank is also effective but less efficient than URank while forming a priority queue during the task priority establishment phase. Both of them can guarantee the priority constraint while scheduling. Furthermore, they are two indispensable elements to search the critical path of a DAG. In what follows, we use the URank to present our algorithms.</p><p>Processor frequency selection phase. As mentioned above, the ''best'' frequency-voltage pair to achieve lower energy and higher reliability appears as nearest neighbors of f ee on a processor for each task node. A binary search tree is used to find these two immediate neighbors, which has time complexity of Oðlog jTjÞ, where jTj is the number of task nodes.</p><p>By calculating the active energy using Eq. ( <ref type="formula" target="#formula_1">2</ref>), the neighbor frequency which consumes less energy will be selected to perform a data ready task node. For the purpose of performance comparison, two other revised algorithms which are based on two well known algorithms will be presented in order to find the optimal system reliability. The first revised algorithm is the Reliability-aware Heterogeneous Earliest Finish Time (RHEFT) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1. RHEFT</head><p>Require: A DAG G ¼ hV; Ei and a set PE of DVS available processors. Ensure: A schedule S of G onto PE.</p><p>1: compute URank of s i 2 V by traversing the graph from the exit node 2: compute energy-efficient frequency for each node in set V 3: sort the tasks in a non-increasing order by URankðs i Þ value and establish a priority queue Queue URank for the sorted tasks 4: while the priority queue Queue URank is not empty do 5: s i the head node in Queue URank ; 6: for each processor pe j 2 PE do 7:</p><formula xml:id="formula_17">for 8f j;k 2 F do 8:</formula><p>find the immediate neighboring frequency f in of f ee on processor pe j for task s i , mark the one which consumes less energy 9:</p><p>if the summation of energy consumption satisfies E total 6 E Ã then 10:</p><formula xml:id="formula_18">f j;k f in 11: else 12: f j;k f max 13:</formula><p>end if 14:</p><p>end for 15: end for 16: assign the marked frequency f in to task s i on the marked processor 17: compute the reliability of task s i using Eq. ( <ref type="formula" target="#formula_8">6</ref>) 18: compute the energy consumption for task s i using Eq. (2) 19: delete the head node s i in Queue URank</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20: end while</head><p>The RHEFT algorithm (Algorithm 1) is an application scheduling algorithm with bounded number of heterogeneous processors. It consists of three major phases, i.e., the priority establishment phase to provide a valid topological order of application tasks, the frequency selection phase to choose a feasible and efficient frequency to perform the data ready task, and the processor assignment phase to allocate the candidate task to the ''best'' processor in the order of priorities and low energy consumption.</p><p>As shown in Algorithm 1, Step 1 and Step 2 primarily calculate the URank and energy-efficient frequency for each task, respectively. The priority queue is constructed in Step 3. Steps 4-20 are outside loop of the RHEFT to guarantee each task node is scheduled in a right order. In Step 8, a suitable immediate neighboring frequency is selected, and temporarily marked in the memory. Steps 9-13 guarantee that the total energy consumption does not exceed E Ã .</p><p>After traversing all the processors in Steps 6-15, the most suitable frequency and the ''best'' processor are produced.</p><p>Then the algorithm allocates a task node to the best processor and assigns it with suitable execution frequency f in in</p><p>Step 16. The reliability and energy consumption of a task are calculated in Steps 17 and 18, respectively. The RHEFT algorithm takes Oðlog f Þ time for the frequency selection phase in Step 8. The RHEFT algorithm has a OðjTj Â p Â log f Þ time complexity for jTj task nodes and p processors, where each processor supports f levels of DVS enabled frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">The Reliability-aware Critical-Path-On-a-Processor (RCPOP) Algorithm</head><p>Another revised algorithm is the Reliability-aware Critical-Path-On-a-Processor (RCPOP) algorithm. In this algorithm, the DRank and URank of each node is computed firstly. A priority queue of the application graph is formed according to the decreasing order of nodes' URank values. The task nodes on the critical path, which has the maximum summation of DRank and URank, should be found and stored to a sorted list. Then in Step 9, energy-efficient frequency f ee is derived. The critical processor which takes the least computational cost for all task nodes on the critical path is selected in Step 10. Steps 12-28 are the main part of a loop. While scheduling a task from the priority queue, a node on the critical path will only be assigned to the critical processor. Then the algorithm picks out a ''best'' immediate neighboring frequency f in for it under the constraint of total computation consumption. The nodes in other paths can also be assigned to the critical processor provided that there is a free and big enough slack time slot to accommodate it. Otherwise, it will be assigned to a non-critical processor. The latter procedure is the same as the CP nodes.</p><p>For all ready tasks, the feasible frequency searching takes time Oðlog f Þ. So the complexity of the RCPOP algorithm is add node s i to CP set list CP 7: end if 8: end for 9: compute energy-efficient frequency for each node in set T 10: select the CP processor P CP which have the minimal summation computation cost of nodes in list CP 11: sort the tasks in a non-increasing order by URankðs i Þ value and establish a priority queue Queue URank for tasks in order 12: while not all nodes in Queue URank have been scheduled do 13: s i the head node in Queue URank 14: if s i 2 list CP then 15: assign processor P CP to s i , and compute the immediate neighboring frequency f in of f ee on processor 16: else 17: assign the processor which consumes less energy with immediate neighboring frequency of f in its f ee for s i 18: end if 19: if the summation of energy consumption satisfies E total 6 E Ã then 20:</p><formula xml:id="formula_19">OðjTj Â p Â lg f Þ.</formula><p>f j;k f in 21: else 22:</p><p>f j;k f max 23: end if 24: assign task s i to the marked processor and specify its executing frequency f in 25: compute reliability of the task node s i using Eq. ( <ref type="formula" target="#formula_8">6</ref>) 26: compute energy consumption of the task node s i using Eq. ( <ref type="formula" target="#formula_1">2</ref>) 27: delete the head node s i in Queue URank 28: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">The Reliability Maximization with Energy Constraint (RMEC) Algorithm</head><p>Likewise, three phases are incorporated into the RMEC algorithm. The task priority queue consists of Steps 1 and 2. The topological order of tasks is established in this phase. The optimal frequency selection phase comprises Steps 5-13. It is notable that Steps 7-11 assure the RMEC algorithm within the energy constraint, as well as improve the reliability of each candidate task during execution. It is in the inner loop body of the algorithm. Inspired by the energy-conscious scheduling method proposed in <ref type="bibr" target="#b22">[23]</ref>, the reliability maximum energy conservative (RME) strategy is to choose a processor and frequency combination that has the maximum value, as shown in Eq. ( <ref type="formula" target="#formula_20">12</ref>), RME s i ; pe j ; f j;k ; pe l ; f l;m</p><formula xml:id="formula_20">À Á ¼ À R s i ; pe j ; f j;k À Á À R s i ; pe l ; f l;m À Á R s i ; pe j ; f j;k À Á ! þ E s i ; pe j ; f j;k À Á À E s i ; pe l ; f l;m À Á E s i ; pe j ; f j;k À Á À min E s i ; pe j ; f j;k À Á À E s i ; pe l ; f l;m À Á È É ! ;<label>ð12Þ</label></formula><p>where s i is the data ready task which is in the head of the priority queue, f j;k and f l;m are the available discrete frequency on the candidate processors pe j and processor pe l , respectively. Rðs i ; pe j ; f j;k Þ and Rðs i ; pe l ; f l;m Þ denote the reliability of task s i on processor pe j at frequency f j;k and that of task s i on processor pe l at frequency of f l;m , respectively. Similarly, Eðs i ; pe j ; f j;k Þ and Eðs i ; pe l ; f l;m Þ represent the energy consumption of task s i on processor pe j at frequency f j;k and that of task s i on processor pe l at frequency of f l;m , respectively. search the best combination of processor pe j and frequency level f k of which keeps the max value REMðs i ; pe j ; f k ; pe t ; f t Þ . pe t and f t are used to temporarily store the best combination of processor and processing frequency respectively 13: end for 14: assign the recorded best combination of processor pe t and processing frequency f t to node s i 15: compute the node reliability of task s i 16: end while 17: let S 0 denotes the scheduling derived from the above procedure 18: while the scheduling list S 0 is not empty do 19: v 0 i the head node in scheduling list S 0 20: for each processor pe 0 j in set PE do 21:</p><p>while there is an available slack slot in the processor which satisfies the precedence priority of tasks do 22:</p><p>compute makespan, node reliability 23:</p><p>if makespan does not increase, node reliability promotes and there is an available time slot to accommodate v 0 i then 24: replace v 0 i with the better combination processor pe 0 j and frequency f 0 k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25:</head><p>end if 26:</p><p>update node reliability and compute the node energy consumption 27:</p><p>end while 28: end for 29: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INS 11416</head><p>No. of Pages 19, Model 3G</p><p>Steps 14-15 are included to realize the third phase, i.e., assigning a node in the marked processor with the optimum frequency. As slack occurs inevitably due to the inter-processor communication for a DAG, Steps 17-29 are used to utilize the slot times of processors without increasing the schedule length and degrading reliability. This step can be taken as local optimum without sacrificing time complexity consumption. So the complexity of the RMEC algorithm is Oð2 Â jTj Â p Â log f Þ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and results</head><p>RHEFT and RCPOP are two algorithms with straightforward minds for reliability maximization. In the comparison with the RMEC algorithm, they are inferior in performance. Hence, in this section, we not only compare the three proposed algorithms, but also compare the performance of the RMEC algorithm with two other excellent reliability-aware algorithms, i.e., HRDS <ref type="bibr" target="#b29">[30]</ref> and RDLS <ref type="bibr" target="#b9">[10]</ref>. These two algorithms are proven to perform well for parallel task scheduling in a heterogeneous computing system. The following comparison is intended not only to present quantitative results, but also to qualitatively analyze the results. To measure the performance of these scheduling algorithms, two extensive sets of task graphs, i.e., randomly generated and real-world applications, are used in our study. The three real parallel applications are the fast Fourier transformation (FFT), the LU-decomposition, and Gaussian-elimination. In addition, plenty of variables are made for these task sets in some more comprehensive cases.</p><p>Before making a comparison with algorithms RDLS and HRDS, we firstly give a brief introduction of them. HRDS <ref type="bibr" target="#b29">[30]</ref> refers to the Hierarchical Reliability-Driven Scheduling algorithm in a hierarchical grid computing system. Its main objective is to achieve high reliability and shorter schedule length by considering the task reliability overhead while mapping a task to the appropriate processor. The algorithm proceeds in two phases. During the first phase of establishing task priority, the system task overhead LCðs j ; pe j Þ is taken into account, where LCðs j ; pe j Þ is the sum of the task reliability cost and the corresponding earliest finish time of task s j on processor pe j . During the second phase, the algorithm selects the processor pe m which costs the minimal overhead LCðs j ; pe j Þ to perform task s j . That is to say, for a given task s i in the task set T, HRDS allocates the processor pe m with minimal LCðs j ; pe m Þ in the mapping phase of list scheduling.</p><p>While the Reliable Dynamic Level Scheduling (RDLS) algorithm <ref type="bibr" target="#b9">[10]</ref> is based on the DLS <ref type="bibr" target="#b8">[9]</ref>, it promotes the best suited resources to the application to reduce the execution time of that application. During the task priority calculation phase, RDLS adds the last item Cðs i ; pe j Þ to capture the reliability, which could impact the allocation decision and improve the reliability of application. It is noteworthy that the Cðs i ; pe j Þ is a cost function, which could promote resources reliability at a high level to maximize the reliability of an application. Akin to the DLS algorithm, the RDLS algorithm finds a suitable task-processor pair with the highest dynamic level. The RDLS algorithm minimizes the makespan and enhances the reliability of the application to some extent.</p><p>Specially, the random task graph set consists of 100 base task graphs generated with 80 different sizes, eleven CCRs, and five processor heterogeneity settings. For each configuration, the specific P ind value is determined randomly according to a uniform distribution in the range of [0.2, 2.0]. For simplicity, the value of C eff is set to one throughout the whole experiments.</p><p>Each of the three real-world applications is conducted in the benchmark that involves 7250 task graphs. Hence the total amount is 21,750.</p><p>Our experiments are carried out using a workstation at National Supercomputing Center in Changsha. It is equipped with an Intel Core i3-540 dual-core CPU, 8 GB DRAM, and 500 GB hard disk, respectively. The machine runs with Windows 7</p><p>(64 bit OS) SP1. We designed a simulator which has implemented well known algorithms, including HCPFD <ref type="bibr" target="#b13">[14]</ref>, HLD <ref type="bibr" target="#b2">[3]</ref>,</p><p>RDLS <ref type="bibr" target="#b9">[10]</ref>, and so on. Based on this framework, we utilize the well-known benchmark involving the large number of FFT, GE, Laplace (introduced below) graphs to implement our proposed algorithms to verify the performance.</p><p>In our study, each processor is assumed to execute a task from the prior queue without preemption. The computation and communication costs are generated and follow a uniform distribution for each task node, with the mean value to the specific average cost of computation and communication, respectively. The occurrence of transient fault follows a Poisson distribution, and the equation is given by Eq. ( <ref type="formula" target="#formula_7">5</ref>), where d ¼ 3 and k 0 ¼ 10 À9 . For the three real applications, the number of tasks can range from about 14 to 1024 as the input number of points and the matrix sizes are varied, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Scheduling Length Ratio (SLR)</head><p>Makespan, or scheduling length, is defined as</p><formula xml:id="formula_21">makespan ¼ FT s exit ð Þ;<label>ð13Þ</label></formula><p>where FTðs exit Þ is the earliest finish time of the exit task in the scheduling queue.</p><p>For most scheduling algorithms, makespan is one of the main metrics of performance evaluation. Since a large set of application graphs with different properties are used, it is necessary to normalize the makespan to the lower bound, which is named as Scheduling Length Ratio (SLR). Without considering the communication cost, the calculation expression of SLR can be expressed as:</p><formula xml:id="formula_22">SLR ¼ makespan P s i 2CP min pe j 2PE w i;j È É:<label>ð14Þ</label></formula><formula xml:id="formula_23">5.1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Energy Consumption Ratio (ECR)</head><p>In our study, we consider energy consumption and the probability of failure for a scheduled task set as another two more important performance metrics. For a given task set, the ECR is defined as:</p><formula xml:id="formula_24">ECR ¼ E total P s i 2CP min pe j 2PE P ind i Á c i f i þ C ef Á c i Á f 2 i n o ;<label>ð15Þ</label></formula><p>where E total is the total energy consumption of the scheduled tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Probability of Failure (POF)</head><p>The probability of failure is one of the two primary performance metrics for our comparison. Formally, the POF value of a task set with the allocated frequency to each specific task by a scheduling algorithm is defined as:</p><formula xml:id="formula_25">POF ¼ 1 À R ¼ 1 À P n i¼1 R i f i ð Þ:<label>ð16Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Randomly generated DAG</head><p>In our study, we first considered the random DAG graphs, which are generated with certain probability for an edge between any two nodes of the graph. Such kind of weighted application DAGs with various characteristics that have close relationship with several input parameters can be presented briefly as follows.</p><p>DAG size ðsÞ. This is the number of task nodes in a graph.</p><p>Communication to computation ratio ðCCRÞ. In some experiments, CCR is used to characterize the workload of the task graph. It is the ratio which equals to the average of communication cost over the average of computation cost. With the help of CCR, one can judge the importance of communication or computation in the task graph. A DAG with very low value can be considered as a computation intensive application. On the contrary, a DAG with a high CCR value is a communication intensive application.</p><p>Average out degree (d). This is the average value of the out degree of a DAG graph.</p><p>Computation capacity heterogeneity factor ðhÞ <ref type="bibr" target="#b7">[8]</ref>. Heterogeneity basically reflects the variance of processing speed. A high h indicates wider margin in the computation costs for a task, and vice versa. And each task has the same computation cost if this factor is set to 0. The average computation cost of each task wðs i Þ is selected randomly from a uniform distribution generator with a mean value W which can be specified by a user. And its range is</p><formula xml:id="formula_26">wðs i Þ Â ð1 À h 2 Þ; wðs i Þ Â ð1 þ h 2 Þ h i .</formula><p>The value of W has no influence on the performance of scheduling.</p><p>In each of our experiments, graphs are created with the combination of the above characteristics. The graph size ranges from 50 to 500 nodes, with step by 50. The communication edge between two nodes is generated with identical probability and computed based on the average number of edges for each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Random application performance results</head><p>The goal of these experiments is to assess the performance of the proposed algorithms with two other excellent algorithms, i.e., HRDS and RDLS.</p><p>The experimental results of the first set of simulation studies are shown in Figs. <ref type="figure" target="#fig_2">3</ref><ref type="figure">4</ref><ref type="figure">5</ref>, where each data point comes from the average value of experiment data based on the algorithms running for 500 times. As can be seen from these three figures, RMEC outperforms the other four algorithms significantly for applications with different sizes and diverse CCR values. For Fig. <ref type="figure" target="#fig_2">3</ref>, where the application is computation intensive with CCR ¼ 0:5, the average SLR for both RMEC and HRDS are pretty close. This is due to the fact that SLR is not the most important metric in this study. To achieve high system reliability even at the expense of modest performance loss in some cases, RMEC tries its best to improve the reliability of a task with the minimum energy consumption to the greatest extent. It can also be observed from Fig. <ref type="figure" target="#fig_2">3</ref> that RMEC outperforms HRDS and RDLS in terms of average ECR and average POF. We attribute the marginally better performance of RMEC over other four algorithms to the fact that RMEC is a reliability adaptive strategy and assigns each task intelligently to a processor with an appropriate execution frequency according to its computational time and execution reliability. In the processor allocation phase,  RHEFT chooses the processor with the earliest finish time, and RCPOP identifies the critical processor for critical tasks. Both of them do not take enough account for reliability. Meanwhile, HRDS pays excessive attention to system task overhead LCðs j ; pe j Þ in the phase of processor selecting. And the cost function of RDLS cannot accurately reflect the importance of the efficient frequency selection for an application's reliability. Thus, their ECR and POF are all worse than RMEC.</p><p>The comparisons for random graphs with larger CCR values are shown in Figs. <ref type="figure">4</ref> and<ref type="figure">5</ref>, respectively. The gaps among the five algorithms shrink gradually with the CCR value increasing. For both CCR ¼ 1:0 and CCR ¼ 5:0, the improvement of RMEC evinces significantly, with respect to the quality of average ECR and average POF.     analysis, we can get a conclusion that our algorithms perform better as the CCR increases, especially the RMEC algorithm. In other words, it is more suitable for data intensive applications.</p><p>Fig. <ref type="figure" target="#fig_6">6</ref> reveals the performance of the five algorithms for various numbers of processors, where the random graph has 500 task nodes and CCR equals to five. The RMEC strategy performs consistently better than other four algorithms with the number of processors increasing. The margins of the average SLR among the five algorithms decrease gradually. As the number of processors increases, the average of ECR and the average of POF show a slight downward trend. At some point, especially for 12 processors in Fig. <ref type="figure" target="#fig_6">6</ref>, the increase in the number of processors is of no use to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Performance analysis on graphs of real-world applications</head><p>Without loss of generality, it is necessary to use real-world applications to evaluate the performance of our algorithms. To this end, three representative real-world applications will be used to test the performance of our algorithms. These three applications are real-world problems, i.e., fast Fourier transformation (FFT), LU decomposition, and Gaussian elimination.  A fast Fourier transform is an algorithm to compute the discrete Fourier transform and its inverse transform. FFT computations are very fast and widely used in many applications in science, engineering, and mathematics. The computation of FFT is comprised of two operations, i.e., the input vector and the butterfly operation. Fig. <ref type="figure" target="#fig_8">7</ref>(a) shows a FFT task graph with four points. It consists of two parts. The tasks above the dashdotted line are the recursive call tasks and the ones below are the butterfly operation tasks <ref type="bibr" target="#b7">[8]</ref>. The parameters used for experiment are shown in Table <ref type="table" target="#tab_8">5</ref>. The total number of tasks used in the evaluation follows the expression, S 2n ¼ 2 2n ; ðn P 2Þ, where n is the number of levels in a FFT graph. As the size of n ranges from 4 to 10 with the step by 1, the input FFT points change from 4 to 24 with the step by 4, and the corresponding number of task nodes in a FFT graph varies as 16, 32, 64, 128, 256, and 512. For each kind of configuration, which combines the three parameters in Table <ref type="table" target="#tab_8">5</ref> together, we test the five Algorithms 500 times in such a combination. The ultimate results are evaluated with the selected average value from the output, involving the average value of SLR, ECR, and POF with the increment of CCR.  The experiment result are presented in Fig. <ref type="figure" target="#fig_9">8</ref>(b) and (c). As can be observed from these two figures, algorithms RHEFT, RCPOP, and RMEC are able to produce competitive ECR and POF over HRDS and RDLS. Due to the comprehensive precedence constraints of most task nodes as the size of graph grows, the variety of both ECR and POF for the three algorithms are not large. In such a case, an increment of number for the processor will not bring any benefit for improving performance.</p><p>The overall performance improvement of the RMEC strategy for FFT graphs is 4.87%, 6.19%, 8.43%, 17.64% better than RHEFT, RCPOP, HRDS, and RDLS respectively, in terms of the average POF. For the average ECR, RMEC performs 3.21%, 5.12%, 6.79%, 15.02% better than RHEFT, RCPOP, HRDS, and RDLS. For the average SLR, RMEC performs À0.60%, 1.55%, 1.87%, 11.54% better than RHEFT, RCPOP, HRDS, and RDLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">LU decomposition</head><p>LU decomposition, as shown in Fig. <ref type="figure" target="#fig_8">7</ref>(b), is used wisely in solving mathematical equations and works by transforming a matrix into a production of lower and upper triangular matrices. Table <ref type="table" target="#tab_9">6</ref> shows the used parameters for experiment of LU decomposition task graphs. The total number of task nodes S n complies with the expression, S n ¼ n 2 þ3n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>; ðn P 3Þ, where n is the size. With the size of input matrix varies from 5 to 30, the task node number changes from 20 to 495. With regard to the number of processors and the value of CCR, they vary from 3 to 15 with step by 3, and 0.5, 1, 2, 3, 4, . . . , 9, 10, respectively.</p><p>During the experiments, we choose the combination of the three parameters in Table <ref type="table" target="#tab_9">6</ref>, and make the algorithms execute 500 times under the condition of every combination. The following result is collected from the average value of the output.</p><p>As shown in Fig. <ref type="figure" target="#fig_10">9</ref>(b) and (c), the RMEC strategy outperforms HRDS and RDLS strategies for all values of the CCR parameter. For both small and large CCR values, the RMEC algorithm consistently produces very low ECR values, especially for small CCR values. The overall performance improvement of the RMEC strategy for LU decomposition graphs is 2.67%, 3.59%, 7.37%, 6.25% better than RHEFT, RCPOP, HRDS, and RDLS respectively, in terms of the average POF. For the average ECR, RMEC performs 3.77%, 4.29%, 5.69%, 6.74% better than RHEFT, RCPOP, HRDS, and RDLS. For the average SLR, RMEC performs À1.04%, 3.34%, 0.23%, 5.68% better than RHEFT, RCPOP, HRDS, and RDLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">Gaussian-elimination</head><p>The Gaussian-elimination, a well known method, is widely used in mathematics for solving system of linear equations.</p><p>Let n be the size of a matrix which depicts the Gaussian-elimination task graph. The total number of tasks S n in this DAG is equal to n 2 þnÀ2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>; ðn P 2Þ, where n is the size of a matrix in a GE graph. Fig. <ref type="figure" target="#fig_8">7</ref>(c) shows a Gaussian elimination graph with matrix size 5. Table <ref type="table" target="#tab_10">7</ref> shows the corresponding parameters used in the experiment of Gaussian elimination graphs. As the size of a matrix varies from 5 to 31 with an increment step by 1, the total number of task nodes ranges from 14 to 495. Similarly as above, after 500 times execution under the combination of the there parameters in Table <ref type="table" target="#tab_10">7</ref>, the final comparison for the five algorithms with the average SLR, ECR, POF for different CCR is given below.</p><p>It can be seen from Fig. <ref type="figure" target="#fig_11">10</ref>(b) and (c) that the RHEFT, RCPOP, and RMEC strategies surpass the HRDS and RDLS scheduling strategies for all the CCR values, in terms of the average ECR and the average POF. As CCR increases, the average ECR for each of the five algorithms grows slightly. The overall performance of the RMEC algorithm for the Gaussian elimination graph is 3.81%, 4.84%, 6.94%, 8.40% better than RHEFT, RCPOP, HRDS, and RDLS respectively, in terms of the average POF. For the average ECR, the RMEC performs 4.53%, 4.96%, 6.38%, 7.63% better than RHEFT, RCPOP, HRDS, and RDLS, respectively. For the average SLR, RMEC performs À1.53%, 3.77%, À1.31%, 9.96% than RHEFT, RCPOP, HRDS, and RDLS, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper aims at incorporating task reliability while applying the DVS technique to achieve low power consumption in a heterogeneous computing system of a cluster. Since most traditional researches lack for the consideration of transient failure while exploiting the DVS technique, we believe that it is mandatory to devise and implement highly reliable scheduling heuristics to satisfy the requirement of precedence constrained tasks, while pursuing high performance as well as achieving energy efficiency. Thus, we design scheduling algorithms that can incorporate task reliability enhancement and energy saving for directed acyclic graphs. They can achieve high system reliability.</p><p>The performance of the RMEC algorithm is compared with four other reliability-aware algorithms. All of them take proper consideration of improvement for system reliability in a multiprocessor computing system. The comparison is based on a large number of randomly generated DAGs and three real-world applications, which include fast Fourier transformation (FFT), Gaussian elimination, and LU-decomposition. The simulation results show that the proposed RMEC algorithm significantly surpasses other algorithms in terms of system reliability and energy consumption.</p><p>Robustness, high system reliability, and low energy consumption are crucial to a heterogeneous computing system in a cluster. One planned future research is to detect and recover a failed task while scaling frequency to execute tasks. This extension may come up with one or more shared recovery blocks in each processor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A simple precedence-constrained application DAG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Scheduling the task graph in Fig. 1 using the RMEC algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 3 .</head><label>3</label><figDesc>The proposed algorithms 4.3.1. The Reliability-aware Heterogeneous Earliest Finish Time (RHEFT) Algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 .</head><label>2</label><figDesc>RCPOP Require: A DAG G ¼ hT; Ei and a set PE of DVS available processors. Ensure: A schedule S of G onto PE. 1: compute URankðs i Þ of each node s i 2 T by traversing the graph upward from the exit node 2: compute DRankðs i Þ of each node s i 2 T by traversing the graph downward from the entry node 3: CP j j ¼ Maxf8s i 2 TjURankðs i Þ þ DRankðs i Þg 4: for each node s i 2 T do 5: if the summation of URankðs i Þ and DRankðs i Þ equals to CP j j then 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Average SLR, ECR, and POF of the five algorithms for CCR ¼ 0:5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Average SLR, ECR, and POF of the five algorithms for CCR ¼ 1:0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Average SLR, ECR, and POF of the five algorithms for CCR ¼ 5:0 and DAG size = 500.</figDesc><graphic coords="14,78.34,394.13,388.65,194.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) FFT with four points (b) LU-decomposition task graph (c) Gaussian elimination task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Three kinds of real-world DAGs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Average SLR, ECR, and POF for the FFT task graph with different CCR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Average SLR, ECR, and POF for the LU task graph with different CCR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. Average SLR, ECR, and POF for the GE task graph with different CCR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Definitions of notations.</figDesc><table><row><cell>Notation</cell><cell>Definition</cell></row><row><cell>PE</cell><cell>A set of processing elements</cell></row><row><cell>V</cell><cell>A set of supply voltages</cell></row><row><cell>F</cell><cell>A set of supply frequencies</cell></row><row><cell>w i;j</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Voltage-relative frequency pairs.</figDesc><table><row><cell>Level</cell><cell>Pair 1</cell><cell></cell><cell>Pair 2</cell><cell></cell><cell>Pair 3</cell><cell></cell></row><row><cell></cell><cell>Voltage</cell><cell>Frequency</cell><cell>Voltage</cell><cell>Frequency</cell><cell>Voltage</cell><cell>Frequency</cell></row><row><cell>0</cell><cell>1.75</cell><cell>1.0</cell><cell>1.5</cell><cell>1.0</cell><cell>2.2</cell><cell>1.00</cell></row><row><cell>1</cell><cell>1.50</cell><cell>0.8</cell><cell>1.4</cell><cell>0.9</cell><cell>1.9</cell><cell>0.85</cell></row><row><cell>2</cell><cell>1.40</cell><cell>0.7</cell><cell>1.3</cell><cell>0.8</cell><cell>1.6</cell><cell>0.65</cell></row><row><cell>3</cell><cell>1.20</cell><cell>0.6</cell><cell>1.2</cell><cell>0.7</cell><cell>1.3</cell><cell>0.50</cell></row><row><cell>4</cell><cell>1.00</cell><cell>0.5</cell><cell>1.1</cell><cell>0.6</cell><cell>1.0</cell><cell>0.35</cell></row><row><cell>5</cell><cell>0.90</cell><cell>0.4</cell><cell>1.0</cell><cell>0.5</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell></cell><cell>0.9</cell><cell>0.4</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Computation costs on different processors.</figDesc><table><row><cell>INS 11416</cell><cell></cell><cell></cell><cell>No. of Pages 19, Model 3G</cell></row><row><cell>6</cell><cell cols="2">L. Zhang et al. / Information Sciences xxx (2015) xxx-xxx</cell><cell></cell></row><row><cell>Node number</cell><cell>P 1</cell><cell>P 2</cell><cell>P 3</cell></row><row><cell>0</cell><cell>1 2</cell><cell>1 5</cell><cell>1 3</cell></row><row><cell>1</cell><cell>1 1</cell><cell>1 6</cell><cell>1 0</cell></row><row><cell>2</cell><cell>1 4</cell><cell>1 2</cell><cell>1 5</cell></row><row><cell>3</cell><cell>9</cell><cell>13</cell><cell>7</cell></row><row><cell>4</cell><cell>1 5</cell><cell>2 0</cell><cell>1 7</cell></row><row><cell>5</cell><cell>7</cell><cell>9</cell><cell>1 5</cell></row><row><cell>6</cell><cell>1 3</cell><cell>1 2</cell><cell>1 1</cell></row><row><cell>7</cell><cell>1 1</cell><cell>9</cell><cell>1 8</cell></row><row><cell>8</cell><cell>1 3</cell><cell>1 8</cell><cell>1 5</cell></row><row><cell>9</cell><cell>9</cell><cell>12</cell><cell>17</cell></row><row><cell>10</cell><cell>12</cell><cell>10</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Task priorities.</figDesc><table><row><cell>Node number</cell><cell>URank</cell><cell>DRank</cell></row><row><cell>0</cell><cell>136.33</cell><cell>0.00</cell></row><row><cell>1</cell><cell>96.67</cell><cell>27.33</cell></row><row><cell>2</cell><cell>101.00</cell><cell>22.33</cell></row><row><cell>3</cell><cell>106.00</cell><cell>30.33</cell></row><row><cell>4</cell><cell>101.33</cell><cell>22.33</cell></row><row><cell>5</cell><cell>69.33</cell><cell>67.00</cell></row><row><cell>6</cell><cell>64.33</cell><cell>53.00</cell></row><row><cell>7</cell><cell>70.00</cell><cell>53.67</cell></row><row><cell>8</cell><cell>39.00</cell><cell>97.33</cell></row><row><cell>9</cell><cell>37.33</cell><cell>86.33</cell></row><row><cell>10</cell><cell>12.67</cell><cell>123.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Specially, as CCR equals to five, it implies that the cost of communication part dominates the whole application. RMEC clearly exceeds RHEFT, RCPOP, HRDS, and RDLS HRDS, and RDLS. RMEC is a reliability-aware and energy conservative algorithm. It obtains nice ECR and POF at the expense of slight longer makespan. The results distinctly demonstrate that RMEC surpasses HRDS and RDLS. For the above</figDesc><table><row><cell>by (3.31%, 5.74%, 7.62%, 14.05%) in terms of the average ECR, and (3.14%, 6.22%, 11.23%, 14.05%) in terms of the average POF,</cell></row><row><cell>respectively. With respect to average SLR, RMEC has slightly lower performance (À2.95%, 1.10%, À3.68%, 3.89%) over RHEFT,</cell></row><row><cell>RCPOP,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>Configuration parameters for the FFT task graphs.</figDesc><table /><note><p><p>Parameter</p>Possible values CCR 0.5, 1, 2, 3, . . . , 10 Number of processors 3, 6, 9, 12, 15 Number of points 4, 8, 12, 16, 20, 24</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>Configuration parameters for the LU decomposition task graphs.</figDesc><table><row><cell>Parameter</cell><cell>Possible values</cell></row><row><cell>CCR</cell><cell>0.5, 1, 2, 3, . . . , 10</cell></row><row><cell>Number of processors</cell><cell>3, 6, 9, 12, 15</cell></row><row><cell>Size</cell><cell>5, 6, 7, . . . , 29, 30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc>Configuration parameters for the Gaussian elimination task graphs.</figDesc><table><row><cell>INS 11416</cell><cell></cell><cell>No. of Pages 19, Model 3G</cell></row><row><cell>16</cell><cell cols="2">L. Zhang et al. / Information Sciences xxx (2015) xxx-xxx</cell></row><row><cell cols="2">Parameter</cell><cell>Possible values</cell></row><row><cell>CCR</cell><cell></cell><cell>0.5, 1, 2, 3, . . . , 10</cell></row><row><cell cols="2">Number of processors</cell><cell>3, 6, 9, 12, 15</cell></row><row><cell>Size</cell><cell></cell><cell>5, 6, 7, . . . , 29, 30, 31</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Please cite this article in press as: L. Zhang et al., Maximizing reliability with energy conservation for parallel task scheduling in a heterogeneous cluster, Inform. Sci. (2015), http://dx.doi.org/10.1016/j.ins.2015.02.023</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>L. Zhang et al. / Information Sciences xxx (2015) xxx-xxx</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to express their gratitude to the anonymous reviewers for their insightful comments. The research was partially funded by the Key Program of National Natural Science Foundation of China (Grant Nos. 61133005, 61432005), the National Natural Science Foundation of China (Grant Nos. 61370095, 61472124), the National Science Foundation for Distinguished Young Scholars of Hunan (Grant No. 12JJ1011), and the Innovation Fund Designated for Graduate Students of Hunan Province (No. CX2013B142).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reliability analysis and fault tolerance for hypercube multi-computer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abd-El-Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gebali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">276</biblScope>
			<biblScope unit="page" from="295" to="318" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Energy-efficient algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Albers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="86" to="96" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dealing with heterogeneity through limited duplication for scheduling precedence constrained task graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="479" to="491" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of design techniques for system-level dynamic power management</title>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bogliolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">De</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. VLSI Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="316" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Importance of &apos;big data&apos;: A Definition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Gartner, Stamford, CT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Energy efficient CMOS microprocessor design</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Burd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Brodersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Twenty-Eighth Hawaii International Conference on System Sciences</title>
		<meeting>of the Twenty-Eighth Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="288" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An adaptive scoring job scheduling algorithm for grid computing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page" from="79" to="89" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A high performance algorithm for static task scheduling in heterogeneous distributed computing systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Daoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="409" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal and suboptimal reliable scheduling of precedence-constrained tasks in heterogeneous distributed computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özgüner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Workshops on Parallel Processing</title>
		<meeting>of International Workshops on Parallel essing</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matching and scheduling algorithms for minimizing execution time and failure probability of applications in heterogeneous computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özgüner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="308" to="323" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The green500 list: year one</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scogland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Parallel &amp; Distributed Processing</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Intractability</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<date type="published" when="1979">1979</date>
			<publisher>Freeman</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance-constrained distributed DVS scheduling for scientific applications on power-aware clusters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Cameron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2005 ACM/IEEE Conference on Supercomputing</title>
		<meeting>of the 2005 ACM/IEEE Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Janec ˇek, A high performance, low complexity algorithm for compile-time task scheduling in heterogeneous systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hagras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="653" to="670" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A fuzzy time-dependent project scheduling problem</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">246</biblScope>
			<biblScope unit="page" from="100" to="114" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Intel Pentium m Processor Datasheet</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="&lt;http://download.intel.com/support/processors/mobile/pm/sb/25261203.pdf&gt;" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ample: an adaptive multi-performance processor for low-energy embedded applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishitobi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kunitake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kaneda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muroyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Application Specific Processors (SASP 2008)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Design optimization of time-and cost-constrained fault-tolerant distributed embedded systems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Izosimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Design Automat. Test Europe Conf. (DATE&apos;05)</title>
		<meeting>Design Automat. Test Europe Conf. (DATE&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="864" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Siva Ram Murthy, Task allocation algorithms for maximizing reliability of distributed computing systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kartik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="719" to="724" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scheduling for heterogeneous systems using constrained critical paths</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="175" to="193" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Power aware scheduling of bag-of-tasks applications with deadline constraints on DVS-enabled clusters</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th IEEE International Symposium on Cluster Computing and the Grid (CCGRID&apos;07)</title>
		<meeting>of the 8th IEEE International Symposium on Cluster Computing and the Grid (CCGRID&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-level hierarchic genetic-based scheduling of independent jobs in dynamic heterogeneous grid environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kołodziej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Energy conscious scheduling for distributed computing systems under different operating conditions</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1374" to="1381" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scheduling precedence constrained tasks with reduced processor energy on multiprocessor computers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="page" from="1668" to="1681" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The interplay of power management and fault recovery in real-time systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Melhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mossé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elnozahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="231" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic voltage scaling techniques for distributed microsensor networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Furrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chandrakasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Society Workshop on VLSI</title>
		<meeting>of IEEE Computer Society Workshop on VLSI</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="43" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A compile-time scheduling heuristic for interconnection-constrained heterogeneous processor architectures</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Sih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="187" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feedback-controlled reliability-aware power management for real-time embedded systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahapatra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Design Automat</title>
		<meeting>Design Automat</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="185" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reliability-aware scheduling strategy for heterogeneous distributed computing systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Veeravalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="941" to="952" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A hierarchical reliability-driven scheduling algorithm in grid systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H M</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="535" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time task mapping and scheduling for collaborative in-network processing in DVS-enabled wireless sensor networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boangoat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ekici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özgüner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Parallel and Distributed Processing Symposium (IPDPS&apos;06)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance-effective and low-complexity task scheduling for heterogeneous computing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Topcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hariri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="260" to="274" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Power reduction techniques for microprocessor systems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Venkatachalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv. (CSUR)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="237" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Energy-aware parallel task scheduling in a cluster</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kołodziej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zomaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1670" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scheduling for reduced cpu energy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobile Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="449" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Moore&apos;s law</title>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="&lt;http://en.wikipedia.org/wiki/Moore&apos;s_law&gt;" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Big data</title>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="&lt;http://en.wikipedia.org/wiki/Big_data&gt;" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tmall</forename><surname>Wikipedia</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<ptr target="&lt;http://www.tmall.com/&gt;" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A genetic algorithm for task scheduling on heterogeneous computing systems using multiple priority queues</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">270</biblScope>
			<biblScope unit="page" from="255" to="287" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Energy-aware adaptive checkpointing in embedded real-time systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chakrabarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Design Automat</title>
		<meeting>Design Automat</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="918" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On maximizing reliability of real-time embedded applications under hard energy constraint</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Industr. Inf</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="316" to="328" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Energy-aware modeling and scheduling for dynamic voltage scaling with statistical real-time guarantee</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="358" to="372" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scheduling with dynamic voltage/speed adjustment using slack reclamation in multiprocessor real-time systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Melhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Childers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="686" to="700" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The effects of energy management on reliability in real-time embedded systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Melhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mossé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference on Computer Aided Design (ICCAD&apos;04)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
