<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object detection using spatial histogram features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
							<email>hmzhang@jdl.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<addrLine>No. 92, west Da-zhi street</addrLine>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@jdl.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<addrLine>No. 92, west Da-zhi street</addrLine>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@jdl.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Debin</forename><surname>Zhao</surname></persName>
							<email>dbzhao@jdl.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<addrLine>No. 92, west Da-zhi street</addrLine>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object detection using spatial histogram features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EE3AD4EFCE30D772705511B9C6045D99</idno>
					<idno type="DOI">10.1016/j.imavis.2005.11.010</idno>
					<note type="submission">Received 5 April 2005; received in revised form 22 November 2005; accepted 23 November 2005</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object detection</term>
					<term>Spatial histogram features</term>
					<term>Feature selection</term>
					<term>Histogram matching</term>
					<term>Support vector machine</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an object detection approach using spatial histogram features. As spatial histograms consist of marginal distributions of an image over local patches, they can preserve texture and shape information of an object simultaneously. We employ Fisher criterion and mutual information to measure discriminability and features correlation of spatial histogram features. We further train a hierarchical classifier by combining cascade histogram matching and support vector machine. The cascade histogram matching is trained via automatically selected discriminative features. A forward sequential selection method is presented to construct uncorrelated and discriminative feature sets for support vector machine classification. We evaluate the proposed approach on two different kinds of objects: car and video text. Experimental results show that the proposed approach is efficient and robust in object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In computer vision community, object detection has been a very challenging research topic. Given an object class of interest T (target, such as pedestrian, human face, buildings, car or text) and an image P, object detection is the process to determine whether there are instances of T in P, and if so, return locations where instances of T are found in the image P. The main difficulty of object detection arises from high variability in appearance among objects of the same class. An automatic object detection system must be able to determine the presence or absence of objects with different sizes and viewpoints under various lighting conditions and complex background clutters.</p><p>Many approaches have been proposed for object detection in images under cluttered backgrounds. In most approaches, the object detection problem is solved within a statistical learning framework. First, image samples are represented by a set of features, and then learning methods are used to identify objects of interest class. In general, these approaches can be classified as two categories: global appearance-based approaches and component-based approaches.</p><p>Global appearance-based approaches consider an object as a single unit and perform classification on the features extracted from the entire object. Many statistical learning mechanisms are explored to characterize and identify object patterns. Rowley et al. <ref type="bibr" target="#b0">[1]</ref> and Carcia and Delakis <ref type="bibr" target="#b1">[2]</ref> use neural network approaches as classification methods in face detection. Based on wavelet features, Osuna et al. <ref type="bibr" target="#b2">[3]</ref> and Papageprgiou and Poggio <ref type="bibr" target="#b3">[4]</ref> adopt support vector machines to locate human faces and cars. Schneiderman and Kanade <ref type="bibr" target="#b4">[5]</ref> use Naı ¨ve Bayes rule for face and non-face classification. Recently, boosting algorithms are applied to detect frontal faces by Viola and Jones <ref type="bibr" target="#b5">[6]</ref>, then are extended for multi-view face detection by Li et al. <ref type="bibr" target="#b6">[7]</ref> and for text detection by Chen and Yuille <ref type="bibr" target="#b7">[8]</ref>. Other learning methods used in object detection include probabilistic distribution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, principal components analysis <ref type="bibr" target="#b10">[11]</ref> and mixture linear subspaces <ref type="bibr" target="#b11">[12]</ref>.</p><p>Component-based methods treat an object as a collection of parts. These methods first extract some object components, and then detect objects by using geometric information. Mohan et al. <ref type="bibr" target="#b12">[13]</ref> propose an object detection approach by components. In their approach, a person is represented by components such as head, arms, and legs, and then support vector machine classifiers are used to detect these components and decide whether a person is present. Naquest and Ullman <ref type="bibr" target="#b13">[14]</ref> use fragments as features and perform object recognition with informative features and linear classification. Agarwal et al. <ref type="bibr" target="#b14">[15]</ref> extract a part vocabulary of side-view cars using an interest operator and learn a Sparse Network of Winnows classifier to detect side-view cars. Fergus et al. <ref type="bibr" target="#b15">[16]</ref> and Leibe et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> also use interest operators to extract objects' parts and perform detection by probabilistic representation and recognition on many object classes, such as motorbikes, human faces, airplanes, and cars.</p><p>As opposed to a majority of the above approaches, the problem of detecting multi-class objects and multi-view objects has been recently gained great attention in computer vision community. Schneiderman and Kanade <ref type="bibr" target="#b4">[5]</ref> train multiple view-based detectors for profile face detection and car detection. Lin and Liu <ref type="bibr" target="#b18">[19]</ref> propose a multi-class boosting approach to directly detect faces of many scenarios, such as multi-view faces, faces under various lighting conditions, and faces with partial occlusions. Amit et al. <ref type="bibr" target="#b19">[20]</ref> use a coarse to fine strategy for multi-class shape detection with an application of reading license plates. There are 37 object classes to be recognized, including 26 letters, 10 digits, and 1 special symbol. Li et al. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> propose methods to learn a geometric model of a new object category using a few examples and detect multi-class objects by a Bayesian approach. To improve efficiency, Torralba et al. <ref type="bibr" target="#b22">[23]</ref> introduce an algorithm for sharing features across object classes for multi-class object detection. Tu et al. <ref type="bibr" target="#b23">[24]</ref> propose an image parsing framework to combine image segmentation, object detection, and recognition for scene understanding.</p><p>One visual task related to object detection is object recognition, whose goal is to identify specific object instances in images. Local descriptor-based methods are increasingly used for object recognition. Schiele <ref type="bibr" target="#b24">[25]</ref> proposes to use Gaussian derivatives as local characteristics to create a multidimensional histogram as object representation, and then perform the task to recognize many 3D objects. Lowe <ref type="bibr" target="#b25">[26]</ref> develops an object recognition system that uses SIFT descriptors based on local orientation histograms. However, these methods are designed to recognize a specific object rather than in generalization to categorize the object class.</p><p>Feature extraction for object representation plays an important role in automatic object detection systems. Previous methods have used many representations for object feature extraction, such as raw pixel intensities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27]</ref>, edges <ref type="bibr" target="#b27">[28]</ref>, wavelets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29]</ref>, rectangle features <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, and local binary pattern <ref type="bibr" target="#b29">[30]</ref>. However, what kinds of features are stable and flexible for object detection still remains an open problem.</p><p>Motivated by the observation that objects have texture distribution and shape configuration, we propose spatial histogram based features (termed as spatial histogram features) to represent objects. As spatial histograms consist of marginal distributions of an image over local patches, the information about texture and shape of the object can be encoded simultaneously. In contrast to most features previously used, spatial histogram features are specific to the object class, since discriminative information of the object class is embedded into these features through measuring image similarity between the object class and the non-object class. In addition, computation cost of spatial histogram features is low. Our previous work <ref type="bibr" target="#b30">[31]</ref> shows that spatial histogram features are effective and efficient to detect human faces in color images.</p><p>Based on object representation of spatial histogram features, we present an object detection approach using a coarse to fine strategy in this paper. Our approach uses a hierarchical object detector combining cascade histogram matching and a support vector machine to detect objects, and learns informative features for the classifier. First, we employ Fisher criterion to measure the discriminability of each spatial histogram feature, and calculate features correlation using mutual information. Then, a training method for cascade histogram matching via automatically selecting discriminative features is proposed. Finally, we present a forward sequential selection algorithm to obtain uncorrelated and discriminative features for support vector machine.</p><p>Unlike methods which use interest operators to detect parts prior to recognition of the object class, we apply the proposed object detector at anywhere in image scale space. Therefore, our method does not need figure-ground segmentation or object parts localization. In contrast to most systems which are designed to detect a single object class, our method can be applied to any type of object classes with widely varying texture patterns and varying spatial configurations. Extensive experiments on two different kinds of objects (car and video text) are conducted to evaluate the proposed object detection approach.</p><p>The rest of the paper is organized as follows. Section 2 outlines the proposed object detection approach. Section 3 describes spatial histogram features for object representation and provides quantitative measurement of spatial histogram features. Section 4 presents the methods of selecting informative features for object detection. Section 5 gives experiment results of car detection and video text detection. Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview of the proposed object detection approach</head><p>The proposed approach is designed to detect multiple object instances of different sizes at different locations in an input image. Take car detection as an example, the overall architecture of the object detection approach is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. One essential component of the proposed approach is an object detector, which uses spatial histogram features as object representation. We call the object detector as spatial histogram features-based object detector (hereinafter referred as 'SHFbased object detector'). In our approach, the SHF-based object detector is formed as a hierarchical classifier which combines cascade histogram matching and support vector machine.</p><p>For object detection process, we adopt an exhaustive window search strategy to find multiple object instances in an input image. The process of object detection contains three phases: image pyramid construction (Step 1), object detection at different scales (Step 2), and detection results fusion (Step 3).</p><p>Initially, an image pyramid is constructed from the original image in the Step 1. The detector is applied at every location in the image in order to detect object instances anywhere in the input image. To detect objects larger than the fixed size, the input image is repeatedly reduced in scale, and the detector is applied at each scale. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the image pyramid is constructed by subsampling with a factor 1.2. Consequently, the proposed approach can detect objects of different scales. In particular, a small window (subwindow) with the fixed size scans the pyramid of images at different scales and each subimage is verified whether it contains an object instance.</p><p>In Step 2, all subimages are passed to the SHF-based object detector. Firstly, spatial histogram features are generated from the image patches. Secondly, cascade histogram matching is performed to all subwindow images for coarse classification. It eliminates a large number of subimages in background as nonobjects and provides almost all subwindows of object instances to the fine detection stage. Finally, the support vector machine classification is applied to each remained window to identify whether or not it contains an object instance. If a subimage is indicated as an object instance by the object detector, it is mapped to the image of corresponding scale at the pyramid.</p><p>Step 3 is a stage for detection results fusion, in which overlapped object instances of different scales are merged into final detection results. Since the object detector is insensitive to small changes in translation and scale, multiple detections usually occur around each object instance in the pyramid images. We use a grouping method to combine overlapping detections into final detection results. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, all detections at different scales are firstly mapped to the original image scale, resulting in a detection map. The detection map is a binary image, which contains object candidate regions and background. A grouping algorithm is applied to label the detection map into disjoint regions. In addition, some very small regions are removed because very small regions usually correspond to false detections. Each region yields a single final detection. The position and size of each final detection is the average of the positions and sizes of all detections in the region. As a result, the number of object instances and their locations and scales are reported in the output image.</p><p>The SHF-based object detector is constructed through a coarse to fine strategy. For any input image patch of the fixed size, the SHF-based object detector initially produces spatial histogram features from the image path, and then perform hierarchical classification with cascade histogram matching method and support vector machine. As a result, the SHF-based object detector generates an output that indicates whether or not the input image patch is an object instance. In the object detection process, cascade histogram matching method and support vector machine play different roles. Cascade histogram matching quickly locates object candidate instances, and support vector machine precisely verifies the object candidate instances. During training stage, a lot of samples of object and non-object are used to select informative spatial histogram features and to train the SHF-based object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatial histogram features</head><p>Object representation and feature extraction are essential to object detection. In this section, we describe a novel object pattern representation combining texture and spatial structures. Specially, we model objects by their spatial histograms over local patches and extract class specific features for object detection. Moreover, we quantitatively analyze spatial histogram features by discriminating feature analysis and features correlation measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial histograms</head><p>In our approach, a subwindow contains a grey sample image with a certain size. Local binary pattern (LBP) is used to preprocess sample images. Local binary pattern is a relatively new and simple texture model and it has been proved to be a very powerful feature in texture classification <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. LBP is invariant against any monotonic transformation of the gray scale. As illustrated in Fig. <ref type="figure">2</ref>, basic LBP operator uses neighborhood intensities to calculate the region central pixel value.</p><p>The 3!3 neighborhood pixels are signed by the value of center pixel:</p><formula xml:id="formula_0">sðg 0 ; g i Þ Z 1; g i R g 0 ; 0; g i ! g 0 ; 1% i% 8: (<label>(1)</label></formula><p>The signs of the eight differences are encoded into an 8-bit number to obtain LBP value of the center pixel:</p><formula xml:id="formula_1">LBPðg 0 Þ Z X 8 iZ1 sðg 0 ; g i Þ2 iK1 :<label>(2)</label></formula><p>For any sample image, we compute histogram-based pattern representation as follows. First we apply variance normalization on the gray image to compensate the effect of different lighting conditions, then we use basic local binary pattern operator to transform the image into an LBP image, and finally we compute histogram of the LBP image as representation. Fig. <ref type="figure">3</ref> shows a sample image of a side-view car and a non-car sample image, their LBP images and histograms.</p><p>It is easy to prove that histogram, a global representation of image pattern, is invariant to translation and rotation. However, histogram is not adequate for object detection since it does not encode spatial distribution of objects. For some non-object images and object images, their histograms can be very similar or even identical, making histogram not sufficient for object detection.</p><p>In order to enhance discrimination ability, we introduce spatial histograms, in which spatial templates are used to encode spatial distribution of object patterns. As illustrated in Fig. <ref type="figure">4</ref>, we use an image window with a fixed size (width, height) to sample object patterns in image scale space, and then encode pattern spatial distribution by spatial templates. Each template is a binary rectangle mask, shown in Fig. <ref type="figure">5</ref>. We denote each template as rt(x,y,w,h), where (x,y) is the location of the top left position of the mask, while w and h are the width and height of the mask, respectively.</p><p>For a single spatial template rt(x,y,w,h), we model subimage within the masked window by histogram. We call this kind of histograms as spatial histograms. For a sample image P, its spatial histogram associated with the template rt(x,y,w,h) is denoted as SH rt(x,y,w,h) (P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Object features extracted from spatial histograms</head><p>A lot of methods can be used to measure similarity between two histograms, such as quadratic distance, Chi-square distance and histogram intersection <ref type="bibr" target="#b24">[25]</ref>. In this paper, we adopt histogram intersection for its stability and computational inexpensiveness. Similarity measurement by intersection of two histograms <ref type="bibr" target="#b33">[34]</ref> is calculated as</p><formula xml:id="formula_2">DðH 1 ; H 2 Þ Z P K iZ1 minðH i 1 ; H i 2 Þ P K iZ1 H i 1 ;<label>(3)</label></formula><p>where H 1 and H 2 are two histograms, and K is the number of bins in the histograms.</p><p>Suppose a database with n object samples and a spatial template, we represent object histogram model over the spatial template by the average spatial histogram of the object training samples, defined as SH rtðx;y;w;hÞ Z 1 n</p><p>X n jZ1 SH rtðx;y;w;hÞ ðP j Þ;</p><p>where P j is an object training sample, and rt(x,y,w,h) is the spatial template. For any sample P, we define its spatial histogram feature f rt(x,y,w,h) (P) as its distance to the average object histogram, given by f rtðx;y;w;hÞ ðPÞ Z DðSH rtðx;y;w;hÞ ðPÞ; SH rtðx;y;w;hÞ Þ:</p><p>An object pattern is encoded by a spatial template set {rt(1),.,rt(m)}, where m is the number of spatial templates. Therefore, an object sample is represented by a spatial histogram feature vector in the spatial histogram feature space: F Z ½f rtð1Þ ; .; f rtðmÞ :</p><p>As the masks can vary in positions and sizes in the image window, the exhaustive set of spatial histogram features is very large. Therefore, the spatial histogram feature space completely encodes texture and spatial distributions of objects. In addition, spatial histogram feature is a kind of object class specific features, since it encodes sample's similarity to object histogram models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature discriminating ability</head><p>Each type of spatial histogram feature has discriminating ability between object pattern and non-object pattern. To demonstrate this property, we take a spatial histogram feature of side-view car pattern as an example. The size of sample image is 100!40 pixels. The spatial template is rt <ref type="bibr" target="#b39">(40,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b19">20)</ref>, which is within the 100!40 image window and locates a 20!20 mask in position <ref type="bibr" target="#b39">(40,</ref><ref type="bibr" target="#b19">20)</ref>. The car model over this spatial template SH rtð40;20;20;20Þ car is generated by 200 car samples. The spatial histogram feature f rt (40,20,20,20) is the testing feature.</p><p>Fig. <ref type="figure">6</ref> shows the testing feature's distribution over an image sample set containing 2000 car samples and 15,000 non-car samples. In horizontal axis, feature value stands for the value range of the testing feature. In vertical axis, frequency reflects the feature value's distribution of car class samples and non-car class samples. For each class, frequency is the numbers of the samples with feature values over the total number of the corresponding class samples.</p><p>As shown in Fig. <ref type="figure">6</ref>, we use a threshold to classify car and non-car on the testing feature. By setting the threshold to 0.7, we retain 99.1% car detection rate with false alarm rate 45.1% and threshold 0.8 produces 93.8% car detection rate with false alarm rate 12.1%.</p><p>We adopt Fisher criterion to measure discriminating ability of each spatial histogram feature. For a spatial histogram feature f j (1%j%m), suppose that we have a set of N samples x 1 ,x 2 ,.,x N , where each x i is a scalar value of the spatial histogram feature. In the data set, N 1 samples are in object subset labelled u 1 and N 2 samples in non-object subset labelled u 2 . Between-class scatter S b is the distance between the two classes given by</p><formula xml:id="formula_6">S b Z ðm 1 Km 2 Þ 2 ;<label>(7)</label></formula><p>where m i Z 1</p><formula xml:id="formula_7">N i P x2u i</formula><p>x; i 2f1; 2g. Within-class scatter S i for each class is computed as</p><formula xml:id="formula_8">S i Z 1 N i X x2u i ðxKm i Þ 2 ; i 2f1; 2g;<label>(8)</label></formula><p>then total within-class scatter S w is defined by</p><formula xml:id="formula_9">S w Z S 1 C S 2 :<label>(9)</label></formula><p>Thus, Fisher criterion of the spatial histogram feature f j is the ratio of the between-class to the total within-class scatter, given by</p><formula xml:id="formula_10">Jðf j Þ Z S b S w :<label>(10)</label></formula><p>The greater Fisher criterion is, the more discriminative the spatial histogram feature is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Features correlation measurement</head><p>An efficient feature set requires not only each feature has strong discriminating ability, but also they are mutually independent. There are many methods to calculate features correlation, such as mutual information <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> and correlation coefficient <ref type="bibr" target="#b36">[37]</ref>. We employ mutual information to measure features correlation, since mutual information is a natural indicator of statistical dependence between random variables and takes into account the amount of information shared between variables.</p><p>A spatial histogram feature of samples is a random variable, which expresses distance between each sample's spatial histogram and associated object histogram model. For any spatial histogram feature f j (1%j%m), we denote it as a variable X, its entropy is defined as</p><formula xml:id="formula_11">HðXÞ ZK ð pðxÞlog 2 pðxÞdx;<label>(11)</label></formula><p>where p(x) is probability density of the variable. In our approach, we adopt a Parzen window density estimate technology <ref type="bibr" target="#b37">[38]</ref> to approximate the probability density using a set of training samples of the feature f i . Given two spatial histogram features f 1 and f 2 , the mutual information of these two features is defined by</p><formula xml:id="formula_12">Iðf 1 jf 2 Þ Z Hðf 1 Þ C Hðf 2 ÞKHðf 1 ; f 2 Þ;<label>(12)</label></formula><p>where H(f 1 ,f 2 ) is the joint entropy of f 1 and f 2 .</p><p>It is obvious that I(f 1 jf 2 )ZI(f 2 jf 1 ) and 0% Iðf 1 jf 2 Þ% Hðf 1 Þ. Therefore, we calculate feature correlation between the two features f 1 and f 2 as</p><formula xml:id="formula_13">Corrðf 1 ; f 2 Þ Z Iðf 1 jf 2 Þ Hðf 1 Þ :<label>(13)</label></formula><p>Let F s be a feature subset, we calculate the correlation between a feature f m ;F s and F s as follows:</p><formula xml:id="formula_14">Corrðf m ; F s Þ Z maxfCorrðf m ; f k Þjc f k 2F s g:<label>(14)</label></formula><p>Fig. <ref type="figure">6</ref>. Feature distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning informative features for object detection</head><p>We apply a hierarchical classification using cascade histogram matching and support vector machine to object detection. Since the spatial histogram feature space is high dimensional as mentioned in Section 3, it is crucial to get a compact and informative feature subset for efficient classification. In this section, the selection methods of informative features based on discriminability and features correlation are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cascade histogram matching</head><p>Histogram matching is a direct method for object recognition. In this method, a histogram model of an object pattern is first generated for one spatial template. If the histogram of a sample is close to the model histogram under a certain threshold, the sample is classified as an object pattern. Let P is a sample and its spatial histogram feature with one template rt(x,y,w,h) is f rt(x,y,w,h) (P), P is classified as object pattern if f rt(x,y,w,h) (P)Rq, otherwise P is classified as nonobject pattern. q is the threshold for classification.</p><p>Histogram matching with one spatial template is far from acceptable by an object detection system. We select most informative spatial histogram features and combine them in a cascade form to perform histogram matching. We call this classification method as cascade histogram matching. If n spatial histogram features f 1 ,.,f n with associated classification thresholds q 1 ,.,q n are selected, the decision rule of cascade histogram matching is as follows:</p><formula xml:id="formula_15">HðPÞ Z 1 object; ifðf 1 ðPÞR q 1 Þo .o ðf n ðPÞR q n Þ; 0 non-object; otherwise: (<label>(15)</label></formula><p>We measure each feature's contribution by its Fisher criterion and detection rate, and propose a training method for cascade histogram matching. Detection rate is the classification accuracy on a positive object samples set. This method selects a discriminative feature set F select with a classification threshold set ThreSet to construct a cascade histogram matching classifier.</p><p>Suppose that we have (1) spatial histogram features space FZ{f 1 ,.,f m }, (2) positive and negative training sets: SP and SN, (3) positive and negative validation sets: VPZ fðx 1 ; y 1 Þ; .; ðx n ; y n Þg and VNZ fðx 0 1 ; y 0 1 Þ; .; ðx 0 k ; y 0 k Þg, where x i and x 0 i are samples with m-dimensional spatial histogram feature vectors, y i Z1 and y 0 i Z 0, (4) acceptable detection rate: D. The method for training cascade histogram matching is given in the following procedure:</p><p>(1) Initialization: </p><formula xml:id="formula_16">F select Z f,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Support vector machine for object detection</head><p>Cascade histogram matching is the coarse object detection stage and it can obtain high detection rate, however, the false positive rate is still high. For the sake of improvement of detection performance, we employ support vector machine (SVM) classification as a fine object detector.</p><p>An SVM <ref type="bibr" target="#b38">[39]</ref> performs pattern recognition for a two-class problem by determining the separating hyper plane that maximizes the distance to the closest points of a training set. In our approach, we first adopt an SVM method as the evaluation classifier in selecting informative spatial histogram features, and then use the selected feature set to train an SVM for object detection using the Libsvm software <ref type="bibr" target="#b39">[40]</ref>.</p><p>By integrating discriminability and features correlation, we use a forward sequential selection method to iteratively select a feature subset F select for classification. Initially, F select is set to be empty. In each iteration, this method firstly chooses an uncorrelated spatial histogram feature with large Fisher criterion, then uses a classifier to evaluate the performance of the selected feature subset, and finally adds a feature which has maximum classification accuracy to F select .</p><p>Suppose that we have (1) a spatial histogram features space FZ{f 1 ,.,f m }, (2) a training sample set sZ{(x 1 ,y 1 ),.,(x n ,y n )} and a testing sample set vZ fðx 0 1 ; y 0 1 Þ; .; ðx 0 k ; y 0 k Þg,where x i and x 0 i are samples with m-dimensional spatial histogram feature vectors, y i 2{0.1} and y 0 i 2f0:1g for negative and positive samples, respectively. The selection of feature subset F select is performed as the following procedure: </p><formula xml:id="formula_17">MinCorr Z minfCorrðf ; F select Þjf 2F ori g; MaxCorr Z maxfCorrðf ; F select Þjf 2F ori g Thre Z MinCorr * ð1KaÞ C MaxCorr * a; ; 8 &gt; &lt; &gt; :</formula><p>here a is a balance weight (0!a!1), we choose aZ0.2 in experiments;</p><p>(5) Find f 0 2F ori with large Fisher criterion as below: AccðpreÞ% e (e is a small positive constant), the procedure exits and returns F select that contains the selected features, otherwise process following steps: (a) Acc(pre)ZAcc(cur), F select ZF select g{f 0 }, F ori Z F ori \{f 0 }, (b) Go to (3) and continue next iteration step.</p><formula xml:id="formula_18">f 0 Z arg max</formula><p>After running the above feature selection algorithm, we train an SVM classifier for object detection in images using the selected feature set F select . The SVM classifier and the cascade histogram matching constitute the final object detector based on the coarse to fine strategy as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>In order to evaluate the effectiveness of the proposed approaches, we conduct experiments of two different object detection tasks. One is to detect side-view car, which has semirigid structure with special componential configuration. The other is text detection in video frames. Text region is mainly a texture pattern without any obvious componential structure.</p><p>Some performance measures are used to evaluate object detection systems: (1) detection rate is defined as the number of correct detections over the total number of positives in data set;</p><p>(2) false positive rate is the number of false positives over the total number of negatives in data set; (3) precision is the number of correct detections over the sum of correct detections and false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Car detection</head><p>Side-view cars consist of distinguishable parts such as wheels, car doors, and car windows. These parts are arranged in a relatively fixed spatial configuration. Side-view cars have enormous changes in configurations because of various design styles. We build a training image database with 2725 car samples and 14,968 non-car samples, each 100!40 pixel in size. 500 car sample images are from the training image set from the UIUC image database for car detection <ref type="bibr" target="#b40">[41]</ref>. Other car images are collected from video frames and web sites. We also construct a validation set containing 1225 car images and 7495 non-car images for training cascade histogram matching and selection of informative classification features for SVM. Some car samples are shown in Fig. <ref type="figure">7</ref>.</p><p>The exhaustive spatial template set within 100!40 image window is very large: 3,594,591. However, this spatial template set is over complete, and most spatial templates are with small and meaningless size or mutually overlapped. To reduce redundant and meaningless spatial templates, the mask is moved in steps of size 5 pixels in the horizontal and vertical directions and only those spatial templates, whose masks are multiple times the size of 10!10, are used in car detection. In total, 270 spatial templates in a 100!40 image sample are evaluated to extract spatial histogram features. In our experiment, we use the sample database to train cascade histogram matching. As a result, 15 spatial templates (see Fig. <ref type="figure">8</ref>) are learned for cascade histogram matching to reject most non-car instances in the coarse detection stage. In order to improve the detection accuracy, 25 spatial templates are learned for SVM classification with RBF (Radial Basis Function) kernel in the fine detection stage to perform object verification.</p><p>We test our system on two test image sets from the UIUC image database for car detection <ref type="bibr" target="#b40">[41]</ref> To get understanding of the overall performance of the car detection system, we report the receiver operating characteristics (ROC) curves and recall-precision curves (RPC) of test sets A and B as shown in Fig. <ref type="figure" target="#fig_7">9</ref> and Fig. <ref type="figure" target="#fig_8">10</ref>. These curves are obtained by changing classification thresholds of cascade histogram matching and SVM, and then running our car detection system on the test image sets.</p><p>For analysis of the performance of different steps in the coarse to fine object detection system, we conduct experiments using two schemes on test set A. The first scheme is to use cascade histogram matching without SVM classification, the second is to use the combination of cascade histogram matching with SVM classification. Table <ref type="table">1</ref> is the results of experiments using two sets of different classification thresholds. The experimental results show that cascade histogram matching method gets high detection rates, however false positive rate is still high and detection precision is low. As the fine object detection method, the SVM classification improves the detection precisions without significant loss of detection rates.</p><p>In Table <ref type="table">2</ref> and Table <ref type="table">3</ref>, the experimental results on test sets A and B are compared with the results reported on the same data sets from Agrawal et al. <ref type="bibr" target="#b14">[15]</ref>. From the experimental results, our car detection approach outperforms the results reported by Agrawal et al. <ref type="bibr" target="#b14">[15]</ref> with higher detection rate and lower false detections number.  Table <ref type="table">4</ref> shows the experimental results of RPC equal error rate on the test set A and scale invariance from different car detection systems <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. Except Agrawal et al. <ref type="bibr" target="#b14">[15]</ref> and our work, many systems conduct experiments only on test set A which contains cars roughly in single-scale. Our system achieves a performance of recall-precision curve (RPC) equal error rate 92.5%, which outperforms Agrawal et al. <ref type="bibr" target="#b14">[15]</ref>, Fergus et al. <ref type="bibr" target="#b15">[16]</ref> and Leibel et al. <ref type="bibr" target="#b17">[18]</ref>. The system of Leibel et al. <ref type="bibr" target="#b16">[17]</ref> obtains a performance of equal error rate 97.5% by using a complete and flexible representation of the object class with an implicit shape model. Based on detection confidences over pixels, the system can combine object categorization and segmentation. However, it is only capable of detecting sideview cars in single-scale with a small tolerance for scale changes. Although the accuracy is lower than that of Leibel et al. <ref type="bibr" target="#b16">[17]</ref>, our system still compares favorably to Leibel et al. <ref type="bibr" target="#b16">[17]</ref> and can detect objects in multi-scales.</p><p>In Fig. <ref type="figure" target="#fig_3">11</ref>, some car detection results are given. These images contain highly variable side-view cars with different sizes under complex backgrounds. As shown in Fig. <ref type="figure" target="#fig_9">12(a</ref>) and (b), some cars are far small than 100!40, so they are often missing detected. Some false car detections are presented in Fig. <ref type="figure" target="#fig_9">12(c</ref>) and (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Video text detection</head><p>Text detection is the process of detecting and locating regions that contain texts from a given image. We apply the proposed approach to detect text in video frames. A text block pattern is an image window 50!20 in size. A text region classifier is constructed using spatial histogram features. We detect text lines by two steps. First, we scan the image at multiple scales by the text region classifier to produce a text region map. Second, we segment text regions into distinct text lines using vertical segmentation algorithm similar to <ref type="bibr" target="#b41">[42]</ref>.</p><p>We build a training database with 2936 text images extracted form video frames and 12,313 non-text images, each 50!20 pixels in size. We also construct a validation set containing 2012 text images and 6865 non-text images for training cascade histogram matching and learning of informative classification features for SVM. Some text samples are shown in Fig. <ref type="figure" target="#fig_10">13</ref>.</p><p>After     using two sets of different classification thresholds. These results show that high detection rates and low detection precision are obtained during the cascade histogram matching stage. In the fine object detection stage, the SVM improves detection precisions without significant loss of detection rates.</p><p>In Table <ref type="table" target="#tab_3">6</ref>, the experimental results on the video text detection test set are compared with the results reported on the same date set from Hua et al. <ref type="bibr" target="#b43">[44]</ref>. From the experimental results, our text detection approach outperforms the approach reported by Hua et al. <ref type="bibr" target="#b43">[44]</ref> with higher detection rate.  In Fig. <ref type="figure" target="#fig_12">15</ref>, some video text detection results are given. These examples include text lines with considerable variations in contrast, intensity and texture. Some text lines are missing detected because of their small size or low contrast. Some false detected text lines are similar to text in contrast and texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance time</head><p>We implement the object detection methods on a conventional PC. Operating on 320!240 pixel images, the side-view car detection method proceeds at 9 frames per second on a Pentium IV 3.2 GHz CPU. For 320!240 pixel images, the text detection method has an average detection speed of 5 frames per second using a Pentium IV 3.2 GHz CPU. These results show that the proposed object detection methods are suitable for practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have presented a spatial histogram feature-based object detection approach. This method automatically selects informative spatial histogram features, and learns a hierarchical classifier by combining cascade histogram matching and a support vector machine to detect objects in images. Extensive object detection experiments show high detection rates with relatively low numbers of false detections. These results illustrate the high discriminant power of spatial histogram features and the effectiveness and robustness of the hierarchical object detection approach.</p><p>The proposed approach is able to detect not only the objects that consist of distinguishable parts in spatial configurations, such as side-view cars, but also the objects without fixed part-based configurations, such as text lines in video frames. In summary, the results show that the object representation using spatial histogram features is general to different kinds of object classes, and our feature selection methods are efficient to extract informative classspecific features for object detection.</p><p>As a direct extension of this work, we are currently investigating spatial histogram features for multi-class objects detection. The ongoing experiments dealing with multi-view human face detection are very encouraging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Architecture of the proposed object detection approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Neighborhood for basic LBP computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Spatial distribution is encoded by spatial histograms in image scale space.</figDesc><graphic coords="5,95.47,71.22,414.38,128.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 )</head><label>1</label><figDesc>Find f* with maximum Fisher criterion, F select Z{f*} and F ori ZF\{f*}; (2) Set classification accuracy: Acc(pre)Z0; (3) Compute Fisher criterion J(f) and feature correlation Corr(f,F select ) on the training sample set s, for each feature f2F ori ; (4) Compute Thre as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ÞjCorrðf j ; F select Þ% Threg; (6) Train an evaluation classifier C on the training set s, using f 0 and F select . It should be noted that C can be any type of classifier, such as artificial neural network, Naı ¨ve Bayes classifier, and nearest neighbor classifier. In our experiment, we use SVM as the evaluation classifier. (7) Evaluate the classifier C on the testing samples set v, and compute the classification accuracy: Here, C(x) is classification output by the classifier C using f 0 and F select , and C(x)2{0,1}; (8) If the classification accuracy satisfy condition: AccðcurÞK</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Some samples of training car images.</figDesc><graphic coords="8,142.64,71.22,300.25,126.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>. The first set (Test Set A) consists of 170 images containing 200 cars with roughly the same size as in the training images. The second set (Test Set B) consists of 108 images containing 139 cars with different sizes. The test sets are difficult for detection since they contain partially occluded cars, and cars that have low contrast with backgrounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. (a) ROC and (b) RPC curves obtained on UIUC car detection test set A.</figDesc><graphic coords="9,134.53,71.22,336.21,125.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. (a) ROC and (b) RPC curves obtained on UIUC car detection test set B.</figDesc><graphic coords="9,134.59,602.04,336.24,127.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Car detection results: missing detections (a and b) and false detections (c and d).</figDesc><graphic coords="12,100.12,71.22,385.55,204.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Some training samples of text images.</figDesc><graphic coords="12,175.75,317.27,234.01,60.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. (a) ROC curve and (b) RPC curve obtained on video text test set.</figDesc><graphic coords="12,124.61,412.91,336.24,126.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Text detection results on (a) the video text set and (b) still images.</figDesc><graphic coords="13,83.45,235.18,438.47,494.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Perform histogram matching with f t on the validation set VZVPgVN, find a threshold q t such that the detection rate d on the positive validation set VP is greater than D, Here, H(x) is the classification output by histogram matching with f t and q t , and H(x)2{0,1}; (6) If the classification accuracy satisfy condition: AccðcurÞK AccðpreÞ% e (e is a small positive constant), the procedure exits and returns F select and ThreSet, otherwise process following steps: (a) Acc(pre)ZAcc(cur), SNZf, F select ZF select g{f t }, FZF\{f t }, ThreSetZThreSetg{q t }, tZtC1; (b) Bootstrap: perform cascade histogram matching with F select and ThreSet on an image set containing no target objects, put false detections into SN; (c) Go to (2) and continue next iteration step.</figDesc><table><row><cell cols="4">criterion value, i.e.</cell></row><row><cell>f t Z arg max</cell><cell cols="4">fJðf j Þjf j 2Fg;</cell></row><row><cell>f j</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(4) i.e. dRD;</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">(5) Compute the classification accuracy on the negative</cell></row><row><cell cols="3">validation set VN</cell><cell></cell></row><row><cell cols="2">AccðcurÞ Z 1K</cell><cell>1 k</cell><cell>iZ1 X k</cell><cell>jHðx 0 i ÞKy 0 i j:</cell></row></table><note><p><p><p>ThreSetZØ, tZ0, and set two classification accuracy parameters to zero, i.e. Acc(pre)Z 0, Acc(cur)Z0;</p>(2) Compute Fisher criterion J(f) using training sample sets SP and SN, for each feature f2F;</p>(3) Find spatial histogram feature f t which has maximal Fisher</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>reducing redundant spatial templates, 130 spatial templates are evaluated to extract spatial histogram features. We use the training data set to select informative spatial histogram features for text detection. As a result, 17 spatial templates are learned for cascade histogram matching. 32 spatial histogram features are learned for an SVM with RBF kernel to improve text detection performance.The final text detection system is tested on a video text detection test set used in Hua et al.'s work<ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. The video text set contains 45 video frames extracted from the MEPG-7 Video Content Set. There are totally 158 text blocks in the 45 frames and 128 of them are human-recognizable. Hua et al.'s work<ref type="bibr" target="#b43">[44]</ref> and our work use the 128 human-recognizable text blocks as ground truth data. The video text detection test set is Fig.14reports the ROC curve and RPC curve that are obtained on the video text detection test set. These curves characterize the overall performance of the video text detection system. The maximum detection rate is 96.8% with 10 false detections and the RPC equal error rate is 95.3%.Similar to car detection, we also conduct experiments to analyze performance of cascade histogram matching and support vector machine. Table5shows results of experiments</figDesc><table><row><cell>Table 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Car detection results on test set A</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">No. of correct detections</cell><cell cols="2">No. of false detections</cell><cell>Detection rate (%)</cell><cell>Precision (%)</cell></row><row><cell>Without SVM</cell><cell>164</cell><cell></cell><cell>187</cell><cell></cell><cell>82.0</cell><cell>46.7</cell></row><row><cell>With SVM</cell><cell>158</cell><cell></cell><cell>11</cell><cell></cell><cell>79.0</cell><cell>93.4</cell></row><row><cell>Without SVM</cell><cell>196</cell><cell></cell><cell>441</cell><cell></cell><cell>98.0</cell><cell>30.7</cell></row><row><cell>With SVM</cell><cell>193</cell><cell></cell><cell>45</cell><cell></cell><cell>96.5</cell><cell>81.1</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Comparison of car detection results on test set A</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">No. of correct detections</cell><cell cols="2">No. of false detections</cell><cell>Detection rate (%)</cell><cell>Precision (%)</cell></row><row><cell>Agarwal et al. [15]</cell><cell>183</cell><cell></cell><cell>557</cell><cell></cell><cell>91.50</cell><cell>24.73</cell></row><row><cell>Our approach</cell><cell>193</cell><cell></cell><cell>45</cell><cell></cell><cell>96.50</cell><cell>81.1</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Comparison of car detection results on test set B</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">No. of correct detections</cell><cell cols="2">No. of false detections</cell><cell>Detection rate (%)</cell><cell>Precision (%)</cell></row><row><cell>Agarwal et al. [15]</cell><cell>112</cell><cell></cell><cell>1216</cell><cell></cell><cell>80.58</cell><cell>8.43</cell></row><row><cell>Our approach</cell><cell>120</cell><cell></cell><cell>37</cell><cell></cell><cell>86.33</cell><cell>76.43</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Comparison of car detection results on test set A reported in literature</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Agarwal et al. [15]</cell><cell cols="2">Fergus et al. [16]</cell><cell>Leibel et al. [17]</cell><cell>Leibel et al. [18]</cell><cell>Our approach</cell></row><row><cell>ERR</cell><cell>77.0%</cell><cell>88.5%</cell><cell></cell><cell>97.5%</cell><cell>91.0%</cell><cell>92.5%</cell></row><row><cell>Scale inv.</cell><cell>Yes</cell><cell>Yes</cell><cell></cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell></row></table><note><p>Fig. 11. Some experiment results of car detection on (a) UIUC test set A, (b) UIUC test set B, and (c) some other digital photos. H. Zhang et al. / Image and Vision Computing 24 (2006) 327-341 now available at http://www.cs.cityu.edu.hk/(liuwy/PE_VT-Detect/.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5</head><label>5</label><figDesc>Text detection results on the video text set</figDesc><table><row><cell></cell><cell>No. of text blocks</cell><cell>No. of correct detec-</cell><cell>No. of false detections</cell><cell>Detection rate (%)</cell><cell>Precision (%)</cell></row><row><cell></cell><cell></cell><cell>tions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Without SVM</cell><cell>128</cell><cell>78</cell><cell>112</cell><cell>60.9</cell><cell>41.0</cell></row><row><cell>With SVM</cell><cell>128</cell><cell>72</cell><cell>2</cell><cell>56.3</cell><cell>97.2</cell></row><row><cell>Without SVM</cell><cell>128</cell><cell>119</cell><cell>237</cell><cell>93.0</cell><cell>33.4</cell></row><row><cell>With SVM</cell><cell>128</cell><cell>114</cell><cell>5</cell><cell>89.0</cell><cell>95.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6</head><label>6</label><figDesc>Comparison of text detection results on the video text set</figDesc><table><row><cell>Method</cell><cell>No. of text blocks</cell><cell>No. of correct detec-</cell><cell>No. of false detections</cell><cell>Detection rate (%)</cell><cell>Precision (%)</cell></row><row><cell></cell><cell></cell><cell>tions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hua et al. [44]</cell><cell>128</cell><cell>117</cell><cell>6</cell><cell>91.4</cell><cell>95.1</cell></row><row><cell>Our approach</cell><cell>128</cell><cell>122</cell><cell>6</cell><cell>95.3</cell><cell>95.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>H. Zhang et al. / Image and Vision Computing 24 (2006) 327-341</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the anonymous reviewers for their invaluable comments and suggestions. They also would like to thank Dr Jie Yang for his helpful discussion. This research is partially sponsored by Natural Science Foundation of China under contract No. 60332010, the 100 Talents Program of CAS, ShangHai Municipal Sciences and Technology Committee (No. 03DZ15013), and ISVISION Technologies Co., Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional face finder: a neural architecture for fast and robust face detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1408" to="1423" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training support vector machines: an application to face detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1997 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 1997 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A training object system: car detection in static images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Papageprgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT AI Memo</title>
		<imprint>
			<biblScope unit="issue">180</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A statistical method for 3D object detection applied to faces and cars</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2000 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2001 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical learning of multi-view face detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh European Conference on Computer Vision</title>
		<meeting>the Seventh European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="366" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Example-based learning for view-based human face detection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Bayesian discriminating features method for face detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="725" to="740" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face detection in color images using principal component analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Menser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Congress on Image Processing and its Applications</title>
		<meeting>the Seventh International Congress on Image Processing and its Applications</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face detection using mixtures of linear subspaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the Fourth IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="70" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Example-based object detection in images by components</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="349" to="361" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object recognition with informative features and linear classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Naquest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Computer Vision</title>
		<meeting>the Ninth International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to detect objects in images via a sparse, part-based representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1475" to="1490" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object class recognition by unsurpervise scale-invariant learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Ninth International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combined object categorization and segmentation with an implicit shape model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV2004 Workshop on Statistical Learning in Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scale-invariant object categorization using a scaleadaptive mean-shift search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DAGM&apos;04 Annual Pattern Recognition Symposium</title>
		<meeting>the DAGM&apos;04 Annual Pattern Recognition Symposium<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3175</biblScope>
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust face detection with multi-class boosting</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR 2005)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="680" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A coarse-to-fine strategy for multiclass shape detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1606" to="1621" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A bayesian approach to unsupervised oneshot learning of object categories</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Computer Vision</title>
		<meeting>the Ninth International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshop on Generative-Model Based Vision</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sharing features: efficient boosting procedures for multiclass object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="762" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image parsing: unifying segmentation, detection and recognition</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="140" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object recognition using multidimensional receptive field histograms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I.N.P. Grenoble. English translation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Texture-based approach for text detection in images using support vector machines and continuously adaptive mean shift algorithm</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1631" to="1639" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time face detection using edge-orientation matching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Christian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference Audio-and Video-Based Biometric Person Authentication</title>
		<meeting>the Third International Conference Audio-and Video-Based Biometric Person Authentication</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face detection using quantized skin color regions merging and wavelet packet analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tziritas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="277" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A discriminative feature space for detecting and recognizing faces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietika ¨inen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="797" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatial histogram features for face detection in color images, 5th Pacific Rim Conference on Multimedia</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3331</biblScope>
			<biblScope unit="page" from="377" to="384" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rotation-invariant texture classification using feature distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pietika ¨inen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ma ¨enpa ¨, Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietika ¨inen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Macine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature selection for classifying high-dimensional numerical data</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimating optimal feature subsets using efficient estimation of high-dimensional mutual information</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W S</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="213" to="224" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<title level="m">Linear Statistical Inference and Its Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Input feature selection by mutual information based on parzen window</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1667" to="1671" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Libsvm-a library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="www.csie.ntu.edu.tw/cjlin/libsvm" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<ptr target="http://l2r.cs.uiuc.edu/cogcomp/Data/Car/" />
		<title level="m">UIUC Image Database for Car Detection</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Localizing and segmenting text in images and videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wernicked</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="236" to="268" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An automatic performance evaluation protocol for video text detection algorithms</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="507" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatic performance evaluation protocol for video text detection algorithms</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
