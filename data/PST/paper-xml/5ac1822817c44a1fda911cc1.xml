<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Collocated Robot Teleoperation with Augmented Reality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hooman</forename><surname>Hedayati</surname></persName>
							<email>hooman.hedayati@colorado.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Colorado Boulder Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Walker</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Colorado Boulder Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
							<email>daniel.szafir@colorado.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Colorado Boulder Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Collocated Robot Teleoperation with Augmented Reality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F9B182332CE154141B9D85EB0B6A5B94</idno>
					<idno type="DOI">10.1145/3171221.3171251</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Aerial robots</term>
					<term>drones</term>
					<term>teleoperation</term>
					<term>augmented reality</term>
					<term>mixed reality</term>
					<term>interface design</term>
					<term>aerial photography</term>
					<term>free-flying robot</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: In this research, we explore how to leverage augmented reality (AR) to improve robot teleoperation. We present the design and evaluation of 3 AR design prototypes: (A) the Frustum design augments the environment giving users a clear view of what real-world objects are within the robot's FOV; (B) the Callout design augments the robot like a thought-bubble, attaching a panel with the live video feed above the robot; (C) the Peripheral design provides a window with the live video feed fixed in the user's periphery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Robotic teleoperation, in which a user manually operates a robot, typically requires a high level of operator expertise and may impose a considerable cognitive burden. However, it also affords a high degree of precision and may require little autonomy on the part of the robot. As a result, teleoperation is still a dominant paradigm for human-robot interaction in many domains, including the operation of surgical robots for medical purposes <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref>, robotic manipulators in space exploration <ref type="bibr" target="#b21">[22]</ref>, and aerial robots for disaster response <ref type="bibr" target="#b31">[32]</ref>. Even in future systems, where robots have achieved a greater degree of autonomy than in current human-robot teams, teleoperation may still have a role <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38]</ref>. For instance, in "shared control" and "user-directed guarded motion" paradigms, robots enable users to input direct teleoperation commands, but use these commands in an attempt to infer user intentions, rather than following received inputs exactly, particularly if received inputs might lead to unsafe operation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>A substantial body of research has explored human performance issues in various forms of robotic teleoperation interfaces and mixed teleoperation/supervisory control systems (see <ref type="bibr" target="#b2">[3]</ref> for a survey). In particular, prior work has highlighted the issue of perspectivetaking-the notion that poor perceptions of the robot and its working environment may degrade situational awareness and thus have a Session Tu-2: Best Paper Nominees I HRI <ref type="bibr">'18, March 5-8, 2018</ref>, Chicago, IL, USA detrimental effect on operation effectiveness <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>. This can be a problem for both remote teleoperation (as found in space exploration) and even when operator and robot are collocated (as may happen in search-and-rescue or building inspection scenarios).</p><p>In this work, we are interested in designing new interfaces that better support perspective-taking, particularly within the context of operating aerial robots. Aerial robots show significant promise in data-gathering, inspection, and recording tasks across a variety of domains (see <ref type="bibr" target="#b36">[37]</ref> for a survey), although they are typically restricted to operating within line-of-sight range <ref type="bibr" target="#b31">[32]</ref>. Such operations often require trained experts because users must rapidly and accurately synthesize information provide directly from the robot (e.g., via a live camera feed) with an understanding of where the robot is located within the larger context of the environment.</p><p>Current interface designs may exacerbate this problem as live robot camera feeds are typically presented in one of two ways: viewed directly in display glasses or on a traditional screen (e.g., a mobile device, tablet, or laptop computer). While video display glasses may help users achieve an egocentric understanding of what the robot can see, they may degrade overall situational awareness by removing a third-person perspective that can aid in understanding operating context, such as identifying obstacles and other surrounding objects that are not in direct view of the robot. On the other hand, routing robot camera feeds through traditional displays means that, at any point in time, the operator can only view the video stream on their display or the robot in physical space. As a result, operators must make constant context switches between monitoring the robot's video feed and monitoring the robot, leading to a divided attention paradigm that bears similarities to texting while driving.</p><p>We seek to address this challenge by designing new interface paradigms and interaction techniques that better support robot operation. Our approach is inspired by research exploring the integration of teleoperation and virtual reality <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref> and prior work envisioning that augmented reality might someday afford new ways of facilitating information exchange between people and robots <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>. Recent advances in the development of consumer-grade, see-through augmented reality head-mounted displays (ARHMDs) are now making this vision possible, creating an exciting new design space for the field of human-robot interaction. We provide an in-depth examination of this design space within the context of supporting collocated robot teleoperation, specifically for aerial robots.</p><p>In this paper, we address the research question of: "how might augmented reality support collocated user teleoperation for collecting environmental data?" The answer to this question is important to many fields (e.g. search-and-rescue, inspecting construction sites, monitoring manufacturing logistics, etc). We outline relevant work that informed our design process and present a framework for designing augmented reality feedback to enhance HRI. We then describe the development of three new ARHMD interfaces that provide novel forms of visual feedback to operators aimed at reducing the need for context switching and better supporting user perspective-taking. Each design represents a unique fusion for embedding information about the robot's live camera feed directly into the user's view of the robot and environment. We evaluate these designs within a laboratory experiment with 48 participants. This study examined participant abilities at teleoperating a collocated aerial robot while completing a line-of-site environmental inspection task using our new designs compared with an industry standard interface in common use today. We conclude with a discussion of how augmented and mixed reality technologies may represent promising new tools for designing robot control interfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In this work, we explore the design of augmented reality interfaces that enhance collocated robot teleoperation, specifically within the context of aerial robots. Below, we review current aerial robot control interfaces and provide a brief primer on augmented reality technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Aerial Robot Interfaces</head><p>Currently, most interfaces for aerial robots take one of two forms: direct teleoperation interfaces, where robots are controlled by a user via joysticks (or similar device) with a video display, or a higher-level supervisory interface that allows users to plot waypoints outlining desired robot path <ref type="bibr" target="#b37">[38]</ref>. In this work, we are primarily interested in designing new interface techniques to support the former paradigm.</p><p>Teleoperation interfaces for aerial robots often require that users have a great deal of skill to be able to pilot a robot with potentially unfamiliar degrees of freedom while monitoring a live robot video feed. As an example of the skill required, the Federal Aviation Administration (FAA) in the United States initially considered regulations that would have required commercial aerial robot operators to obtain a pilot's license <ref type="bibr" target="#b14">[15]</ref> and still today requires that such robots be operated within line-of-sight range.</p><p>A great deal of research has sought to improve teleoperation paradigms. For example, certain interfaces provide the operator with a first-person view of the robot's video feed via display glasses. While this may help in certain tasks, it can also degrade overall situational awareness as the operator loses all contextual knowledge of the surrounding environment outside of the robot's immediate field of view. Other interfaces combine a live video display with virtual map data, often mixing teleoperation with forms of autonomous waypoint navigation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. Still other approaches have looked to develop control systems using multi-modal interfaces <ref type="bibr" target="#b34">[35]</ref> including exotic designs where the user wears a head-mounted display (HMD) and the robot is controlled via a "floating head" metaphor, changing orientation in sync with the user <ref type="bibr" target="#b15">[16]</ref>. Finally, other research has advanced the notion of "perceived first-order control," which allows users to "nudge" an aerial robot using gestures on a mobile touchscreen, supporting more precise and safe operation <ref type="bibr" target="#b32">[33]</ref>.</p><p>In this work, we seek to extend such prior research. One limitation of many existing systems is that they often focus on remote teleoperation, rather than collocated teleoperation, even though there are many deployments in which collocated control might be useful (e.g., monitoring construction sites, over-the-hill reconnaissance, factory logistics management, environmental surveys of mills, etc.) or even required due to line-of-sight regulations. In addition, we know of no current interface that enable users to view information collected by the robot (e.g., live video feed) while monitoring the robot itself directly within the operating environment. Instead, traditional interfaces present a robot video feed and other sensor information on a display (e.g., mobile device), requiring that users choose between </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Augmented Reality</head><p>Augmented reality (AR) technology overlays computer graphics onto real world environments in real time <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. AR interfaces have three main features: (1) users can view real and virtual objects in a combined scene, (2) users receive the impression that virtual objects are actually expressed and embedded directly in the real world, and (3) the virtual objects can be interacted with in real-time <ref type="bibr" target="#b0">[1]</ref>. This contrasts with purely virtual environments or other parts of the mixed reality continuum, such as augmented virtuality in which real-world objects are mixed into a virtual environment <ref type="bibr" target="#b22">[23]</ref>.</p><p>Early AR systems were often custom-made in research laboratories and were quite limited in display fidelity, rendering speed, support for interaction, and generalizability. However, recent advancements in augmented reality head-mounted display (ARHMD) technology is creating an ecosystem of standardized, consumergrade see-through ARHMDs. For example, the HoloLens and Meta 2 ARHMDs both afford high resolution stereographic virtual imagery displayed at a 60Hz refresh rate, built-in gesture tracking, depth sensing, and integration with standard development tools such as Unity and Unreal Engine. This advance in hardware accessibility is creating new opportunities for exploring AR as an interaction medium for enhancing HRI. Although we are not the first to recognize that AR holds potential for improving human-robot interactions (see <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36]</ref>), we believe that this area is critically understudied and represents a fairly nascent research space, especially within the context of examining how the capabilities provided by modern ARHMD hardware might afford novel methods for teleoperation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN</head><p>We undertook an iterative design process to explore the space of how ARHMD technology might mediate collocated human-robot interaction by providing feedback to support aerial robot teleoperation. Our design process began with an analysis of the potential of augmented and mixed reality technologies. Synthesizing information from past work in robot interface design with research across the mixed reality continuum, we developed a high-level framework for considering how ARHMD feedback might enhance HRI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Design Framework for AR-HRI</head><p>Our framework classifies potential interface designs for augmenting human-robot interactions with virtual imagery into three main categories, regarding whether an AR interface provides supplemental information in a manner (1) augmenting the environment, (2) augmenting the robot, or (3) augmenting the user interface. Augmenting the environment: In this paradigm, interfaces can display information regarding robot operations and data collected by the robot as virtual imagery that is directly embedded into the context of the operational environment, using an environment-as-canvas metaphor. For example, objects that the user/robot has inspected (or plans to inspect) might be highlighted, or information might be added to better indicate the robot's field-of-view. This paradigm affords a full three-dimensional "canvas" to utilize for virtual imagery, rather than a two-dimensional display typical in mobile, tablet, or laptop interfaces.</p><p>Augmenting the robot: In this archetype, virtual imagery may be attached directly to the robot platform in a robot-as-canvas metaphor. This technique may provide contextually relevant cues to an operator in a more fluid manner than traditional interfaces. For example, rather than displaying a battery indicator on a 2D display, requiring that the operator take their eyes off the robot to check status, virtual imagery might instead provide indication icons directly above the robot in physical space, enabling the operator to maintain awareness of both robot position and status. Augmenting the user interface: In this paradigm, virtual imagery is displayed as an overlay directly in front of the user to provide an interface to the physical world, inspired by "window-on-the-world" AR applications <ref type="bibr" target="#b8">[9]</ref> and heads-up display technologies used for pilots <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>. This interface-as-canvas metaphor may take a great deal of inspiration from traditional 2D interface designs, for instance providing supplemental information regarding robot bearing, attitude, GPS coordinates, or connection quality in the user's periphery while maintaining their view of the robot and preserving situational awareness of the environment.</p><p>These design paradigms may provide benefits over traditional interfaces. For example, ARHMDs support stereographic cues that can better leverage human depth perception, as opposed to monocular cues in traditional interfaces. Moreover, these paradigms enable interfaces that provide feedback directly in the context of where the robot is actually operating, reducing the need for context-switching between monitoring the robot and monitoring operations data and thus helping to resolving the perspective-taking problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Creating Reference Prototypes</head><p>Inspired by prior work in robot interface design, we used the framework described above to develop several interface prototypes that provide sample references within the overall space of how augmented reality might support collocated/line-of-sight robot teleoperation. Although ARHMD interfaces might provide feedback on many different aspects relevant to teleoperation, we focused our design exploration specifically on how to convey information about the robot's camera, as this is typically the most critical information for aerial robots operators. We developed three primary prototypes, each of which falls within one of the paradigms in our design framework.</p><p>We refer to these three design prototypes as Frustum, an example of augmenting the environment, Callout, an example of augmenting the robot, and Peripherals, an example of augmenting the user interface. These designs are each based on prior robot interface designs or other metaphors that may be common to user experiences, adjusted and extended to take advantage of ARHMD technology. Each offers potential tradeoffs in terms of how they support perspectivetaking, the total information conveyed, potential scalability across interaction distances, and possibility for user distraction and/or interface overdraw. While our work targets the context of collocated aerial robot teleoperation, we believe that our design framework and methodology, along with the main design metaphors and interface techniques we develop, may also provide utility for other robotic platforms (e.g., ground-mobile robots, underwater robots, or industrial robotic manipulators) and even higher-level supervisory or collaborative control systems. Frustum: The Frustum design provides an example of augmenting the environment. This design has a spatial focus as it provides virtual imagery that displays the robot's camera frustum as a series of lines and points, similar to what might be seen emanating from a virtual camera in computer graphics and modeling applications (e.g., Maya, Unity, etc.). The virtual frustum provides information on the robot's aspect ratio, orientation, and position, while explicitly highlighting what objects in the environment are within the robot's field-of-view (Figure <ref type="figure">1-A</ref>). An additional benefit of this design is the ability to see a robot's sensor frustum even if there is inherently no real-time feedback and/or if taking aggregate measurements (e.g. LiDAR, air quality sensor, spectrometer, etc.). Callout: The Callout design represents an example of augmenting the robot itself with virtual imagery. It uses a metaphor inspired by callouts, speech balloons, and thought bubbles, along with prior work rendering real robot camera feeds within the context of virtual representations of the robot environment <ref type="bibr" target="#b27">[28]</ref>. As in <ref type="bibr" target="#b27">[28]</ref>, the robot's live camera feed is displayed on a panel with an orientation corresponding to the orientation of the camera on the physical robot, enabling information in the video to be spatially similar to the corresponding physical environment (Figure <ref type="figure">1-B</ref>). This feed also provides implicit information regarding the distance between the robot and operator for a perspective projection transformation is applied to the video callout panel based on the calculated offset between the user and the robot. As a result, panel size is proportional to operating distance, just as perceived sizes of real-world objects are proportional to viewer distance. While this design decision may impair long-range operations, we speculate that it may better support scalability and fan-out for operating multiple robots and provides a more realistic embedding of the information directly in the context of where the robot is collecting data at any given time. Alternate variations on this design might instead use billboarding, such that the callout panel always faces the operator. While this would ensure the operator always has a direct view of the robot's video stream, it may remove potentially useful orientation cues. Another design variation might use an orthographic projection, rather than perspective, such that the video panel always remains at a fixed size to the operator regardless of robot distance (this would create a similar effect to the Peripherals design, discussed below). Peripherals: Our final design illustrates a potential method for an augmented-reality user interface that provides contextual information regarding the robot's camera feed in an egocentric manner. This design displays a live robot video feed within a fixed window within the user's view (Figure <ref type="figure">1-C</ref>). Designers may specify fixed parameters regarding window size and location (e.g., directly in front of the user or in their peripheral view), or provide support for dynamic interaction that allows the user to customize window width, height, position, and even opacity. In this work, we placed the window in the user's periphery, fixed within the upper-right corner of the user's view, in a manner inspired by ambient and peripheral displays <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>We conducted a 4 × 1 between-participants experiment to evaluate how our designs might affect user teleoperation of a collocated flying robot. The study tasked participants with operating a Parrot Bebop quadcopter to take several pictures in a laboratory environment as an analog to aerial robot inspection and survey tasks. The independent variable in this study corresponded to what type of teleoperation interface the participant used (four levels: Frustum, Callout, and Peripherals designs plus a baseline). In the baseline condition, participants still wore an ARHMD (to control for possible effects of simply wearing a HMD), but did not see any augmented reality imagery. Instead, participants used the Freeflight Pro application <ref type="foot" target="#foot_0">1</ref> , the official piloting interface supplied by Parrot for the Bebop robot (the platform used in this experiment, detailed in §4.2). Dependent variables included objective measures of task completion and subjective ratings of operator comfort and confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task and Environment</head><p>Our overall experimental design was inspired by contexts where free-flying robots might assist with environmental inspections and surveys within human environments, as is already common practice among drone hobbyists and might soon be found within domains including disaster response <ref type="bibr" target="#b31">[32]</ref>, operations on-board the International Space Station <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37]</ref>, and journalism <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>. In the study, participants operated an aerial robot in a shared environment. The environment measured 5m×5m×3m and contained motion tracking cameras that we utilized for precisely tracking the robot to ensure ARHMD visualizations were displayed at the appropriate locations for the Frustum and Callout designs (motion tracking was not needed for the baseline condition or the Peripherals design).</p><p>Two inspection targets adorned the walls of the experimental environment in the form of rectangular frames colored in pink and purple with an orange outline (targets can be seen in Figure <ref type="figure">1</ref>). The larger pink target is 1.78m × 1.0m and positioned 1.3m from the ground to its bottom edge. The smaller purple target is 1.35m×0.76m and positioned 0.34m from the ground to its bottom edge. The aspect ratio of the targets exactly matches that of the robot's camera. This gives participants the ability to capture exact photos of the targets. It is relatively easier to capture a perfect image of the pink target as it is larger and higher off the ground, while the purple target is more challenging due to its smaller size and shorter height from the ground (operating an aerial robot close to the ground is more challenging due to instabilities resulting up-drafts reflected off the ground). To capture a perfect image of the purple target requires the robot to fly in closer proximity to both the wall and ground, which increases chances of crashes due to operator error. Participants were tasked with piloting an aerial robot to take photos of the targets in a set order, first inspecting the larger pink target and then the smaller purple target. Participants were directed to prioritize capturing imagery as fast as possible, as precisely as possible, and with the fewest number of total pictures as possible. In terms of precision, participants were instructed to record a photo that captured the entirety of the pink or purple target region, with as little additional imagery (e.g., the orange target frame or other parts of the scene) in the photo as possible.</p><p>Participants were allowed to determine when they had captured what they believed to be a suitable image such that they could then move on to the next target. Overall, this task mimicked an environmental inspection mission, requiring that participants operate a line-of-sight robot to take off from a set starting position, inspect a series of targets in order, and land the robot within six minutes. If participants crashed the robot it was reset to the starting position, after which they were allowed to continue performing the task as long as time remained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Apparatus and Implementation</head><p>Robotic Platform: We used a Parrot Bebop quadcopter as our aerial robot for this experiment (Figure <ref type="figure" target="#fig_0">2</ref>). The Bebop is a popular consumer-grade "drone" appropriate for flying indoors and outdoors with a digitally stabilized 14MPixel HD camera and autonomous hovering capabilities. ARHMD Platform: We used a Microsoft HoloLens as our ARHMD.</p><p>The HoloLens is a wireless, optical see-through stereographic augmented reality HMD with a 30°× 17.5°FOV, an inertial measurement unit, depth sensor, ambient light sensor, and several cameras and microphones that support voice input, gesture recognition, and head tracking. The HoloLens was chosen due to its emerging popularity, ease of access, ability to support hands-free AR, and high potential as a model for future consumer ARHMD systems.</p><p>Implementing Teleoperation Interfaces: Participants in the baseline condition still wore the HoloLens ARHMD, but received no augmented reality visualizations. Instead, they controlled the robot via the "Free Flight Pro" application on an iPad. The Free Flight application represents the default control software for the Bebop robot and is also developed by Parrot (the Bebop manufacturer). It is a popular application (with an average 3.8 rating from 24,654 reviews on the Android App Store) and is highly representative of modern aerial robot control interfaces used in practice today that aim to provide users with an intuitive control scheme. The application provides touchscreen controls for taking off/landing, positioning/orienting the robot, and recording pictures/video, all overlaid on the live video feed from the robot (Figure <ref type="figure" target="#fig_0">2</ref>). In the absence of user input, the application ensures the robot continues to hover and automatically lands the robot upon detecting low battery. Unfortunately, limitations in the robot platform and Freeflight application prevented us from using it as the control input in the other experimental conditions: if Freeflight is connected to the robot, it is not possible to also stream the robot's video feed to any other device. This makes it impossible to use the Freeflight interface with the Callout or Peripherals designs. Instead, in the AR conditions, participants received AR feedback while operating the robot using a wireless Xbox One controller. We set the button/joystick mappings on the Xbox controller to match the touchscreen controls in the Freeflight Pro application and calibrated Xbox controller sensitivity to make it as similar to the Freeflight controller as possible (Figure <ref type="figure" target="#fig_0">2</ref>). As with the Freeflight Pro app, in the absence of user input the robot would continue to hover in place.</p><p>The augmented reality visuals for the Frustum, Callout, and Peripherals designs were implemented using the Unity game engine and deployed as an application that ran on the Microsoft HoloLens. The Frustum and Callout designs require a real-time understanding of robot position, such that virtual imagery can be correctly displayed in the environment relative to the robot (Frustum) or directly as an attachment to the robot itself (Callout). To accomplish this, we used motion tracking cameras to precisely localize the robot, feeding in robot position and orientation values to the HoloLens application. The Peripherals design did not rely on the motion tracking setup, however both the Callout and Peripherals designs displayed a live video feed from the robot's camera as a virtual object in augmented reality. To accomplish this, the robot's video stream was wirelessly broadcast to the HoloLens by routing it through a desktop computer. With this method, we were able to achieve an average framerate of 15 frames per second (FPS), slightly slower than the ~30FPS provided to participants in the baseline condition by the Freeflight Pro application.</p><p>Each of the three main designs we tested have several parameters that may be tuned by designers or users; however, we fixed each of these parameters to control for potential variance in our experiment. The Frustum was displayed as red lines in a wireframe view, as opposed to a shaded or highlighted region to minimize potential occlusion of the environment or task targets. The Callout was designed to emanate from the top of the robot, such that the video feed always appeared 13.5cm above the center of the Bebop as the operator flew it throughout the environment. The Peripherals design set the camera feed window to the upper-right corner of the user's view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Participants</head><p>We recruited a total of 48 participants (28 males, 19 females, 1 self-reported non-binary) from the University of Colorado Boulder campus to participate in our study, which was approved by our university IRB. Males and females were evenly distributed across conditions. Average participant age was 22.2 (SD = 7.2), with a range of 18 -58. On a seven-point scale, participants reported a moderate prior familiarity with both aerial robots (M = 3.48, SD = 1.6) and ARHMDs (M = 3.38, SD = 1.75).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Procedure</head><p>The study took approximately 30 minutes and consisted of five main phases: (1) introduction, (2) calibration, (3) training, (4) task, and (5) conclusion.</p><p>(1) First, participants were given a high-level overview of the experiment and signed a consent form, then were led into the experimental space. (2) Participants were then fitted with the HoloLens and either given an iPad running the Freeflight Pro application or an Xbox controller depending on condition. At this point, the appropriate HoloLens application was also started based on condition (Frustum, Callout, or Peripherals, with no AR application for baseline participants). ( <ref type="formula">3</ref>) Participants then received instructions on the controls (i.e., button-maps for the Freeflight and Xbox controllers) and had two minutes to practice piloting the robot. After two minutes were up, the robot landed and was placed in a fixed starting position for all participants. (4) Participants then completed the main task of inspecting the targets in order, as detailed in §4.1. Participants received six minutes to pilot the robot such that it took off from a fixed starting location, captured images of the targets, and then landed, simulating an environmental inspection mission. If participants crashed the robot, it was reset to the starting position and participants could continue the task if time still remained. ( <ref type="formula">5</ref>) Once participants completed the task or the six minutes allotted to the task ran out, participants were given a post-survey on their experience and then debriefed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Measures &amp; Analysis</head><p>We used objective, behavioral, and subjective measurements to characterize the utility of our interface designs. We measured several objective aspects of task accuracy including: accuracy-measured by how well participant photographs captured the inspection target, each of which consisted of a visible, uniform grid of 297mm × 210mm rectangles, which allowed us to measure the accuracy rate by comparing the rectangle grid in a perfect shot with the photos captured by participants; completion time-measured by the total flight time (lower times mean more efficient performance); and operational errors-the number of times they crashed the robot or otherwise caused it to land prematurely.</p><p>We also recorded first-person and third-person video to analyze behavioral patterns in participant actions. Two coders annotated video data from each interaction based on when participants were able to view the robot and when they were not. Data was divided evenly between coders, with an overlap of 15% of the data coded by both. Inter-rater reliability analysis revealed substantial agreement between raters (Cohen's κ = .92) <ref type="bibr" target="#b17">[18]</ref>. This coding enabled us to calculate distracted gaze shifts-the number of times the participant was distracted looking away from the robot during the task; and distraction time-the total time spent not looking at the robot. We were interested in both measures as many small gaze shifts might be just as detrimental as fewer but longer periods of distraction.</p><p>We constructed several 7-point scales using Likert-type questionnaire items to capture subjective participant responses. These scales measured how the interface designs affected participant comfort (3 items, Cronbach's α = .86), confidence (5 items, Cronbach's α = .95), and perceptions of task difficulty (5 items, Cronbach's α = .93) while operating the robot. Participants also gave open-ended responses regarding their experiences.</p><p>Finally, we had all participants evaluate perceived usability using the System Usability Scale (SUS), an industry standard ten-item attitude survey. SUS scores below 68 are considered below average, scores above 68 are considered above average, and scores above 80.3 are considered in the top 10th percentile.</p><p>We analyzed data from our objective and subjective measures using a one-way Analysis of Variance (ANOVA) with experimental condition (i.e., teleoperation interface) as a fixed effect. Post-hoc tests used Tukey's Honestly Significant Difference (HSD) to control for Type I errors in compared effectiveness across each interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>Objective Results -We analyzed our objective task metrics to confirm that our designs were useful in allowing participants to teleoperate a collocated aerial robot more effectively. We found a significant main effect of design on task performance scores for accuracy, F(3, 44) = 25.01, p &lt; .0001. Tukey's HSD revealed that the Frustum (M = 63.2%) and Callout (M = 67.0%) interfaces significantly improved inspection performance over the baseline interface (M = 31.33%), with the Peripheral design (M = 81.1%) showing even further benefits by significantly outperforming both Frustum and Callout (all post-hoc results with p &lt; .0001). We also found a significant main effect of design on task completion time, F(3, 44) = 3.83, p = .016. Post-hoc comparisons against the baseline (M = 239.70s) revealed that participants were able to complete the task significantly faster using the Frustum (M = 140.69s), p = .017, and Peripherals (M = 154.44s), p = .050, but not the Callout (M = 191.09s), p = .434. Examining occurrences when users crashed the robot, we found a significant effect of interface design on operational errors, F(3, 44) = 9.24, p &lt; .001 with each of our AR designs significantly reducing the number of crashes compared to the baseline (Frustum: M = .250, p &lt; .0001; Callout: M = .667, p = .003; Peripherals: M = .584, p = .001; Baseline: M = 2.17).</p><p>We next turned to our behavioral metrics to understand user distraction. We found a significant main effect of interface design on number of distracted gaze shifts, F(3,44) = 40.28, p &lt; .001, and on total distraction time, F(3, 44) = 48.72, p &lt; .001. Post-hoc tests showed that all three AR designs significantly decreased both the number and length of distractions compared to the baseline (all comparisons at p &lt; .0001).</p><p>Subjective Results -Participants rated the several facets regarding their experiences teleoperating an aerial robot. We found a significant effect of interface design on user comfort working with the robot, F(3, 44) = 8.12, p &lt; .001, with users more comfortable using the Frustum (M = 5.58), p = .002, Callout (M = 5.87), p &lt; .001, and Peripherals (M = 5.14), p = .019, designs than the current Freeflight interface (M = 3.64). We also found a significant effect of design on confidence operating the robot F(3, 44) = 7.93, p &lt; .001. Posthoc comparisons against the baseline (M = 3.0) revealed that users were significantly more confident using the Frustum (M = 5.02) and Callout (M = 5.0) design, both p &lt; .001, but not the Peripherals (M = 4.0), p = .179. Finally, we found a significant effect of design on perceived task difficulty, F(3, 44) = 4.17, p = .011, with participants finding the task significantly easier using the Frustum (M = 4.03), p = .027, Callout (M = 4.13), p = .016, designs and marginally easier using Peripherals (M = 3.78), p = .082, designs than the baseline (M = 2.5). Perceived interface usability was evaluated with the SUS. We found a significant effect of interface design on SUS total score, F(3, 44) = 7.38, p &lt; .001. Tukey's HSD revealed each of our AR designs had significantly higher usability ratings over the baseline (Frustum: M = 80.21, p &lt; .001; Callout: M = 76.88, p = .003; Peripherals: M = 71.04, p = .041; Baseline: M = 55.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Our new interface designs that provided users with augmented reality feedback while teleoperating an aerial robot demonstrated significant improvements over a modern interface that is representative of popular designs currently in use. We found that each of our designs enabled users to complete an inspection task faster and more accurately than with the robot's default control interface, while also leading to safer operation with fewer crashes. Overall, users rated our designs as largely more favorable in terms of usefulness and their own comfort and confidence when operating the robot. It is possible that differences in using a physical Xbox controller rather than touchscreen controls could be partially responsible for our results. However, we do not believe this to be the case, as we took great care to match the Xbox control mappings and sensitivities to the touchscreen controls provided by the Freeflight Pro application. Moreover, we are unaware of any prior research that suggests simply altering whether a controller is physical or touch-based might lead to results as extreme as we found. Instead, we believe the reason behind the success of our designs can principally be found within our behavioral measures: our interface designs enabled users to get live video feedback without taking their eyes off the robot, whereas current designs force users to make context switches that sacrifice either situational awareness of the robot in the environment or their ability to closely monitor the robot's camera feed at any given time.</p><p>In our experiment, as in real-world deployments, users needed an understanding of both of these aspects to successfully complete their task. This was especially important as participants attempted to inspect the smaller, pink target that was closer to the ground. The size and placement of this target required that participants navigate fairly close to both the wall and ground, which can lead to difficulties for the robot's internal stabilization mechanism, potentially causing the robot to drift while hovering. Observing our experimental recordings, we see that participants using the Frustum, Callout, and Peripherals designs were able to quickly notice drift and re-stabilize the robot. However, participants in the baseline condition often took much longer to realize the robot was drifting as they were staring at the tablet which provided the robot video feed, rather than monitoring the robot itself. By the time these participants noticed the drift, it was often too late to correct, leading to crashes, landings, or loops of overcorrection, giving participants the impression that the robot was difficult to control: P8[Baseline]: "I found it fairly difficult and had a hard time getting precise shots with the robot. At times, it seemed that the drone was moving out of my control and I was forced to land it" P15[Baseline]: "The environment interfering with how the robot could move was the biggest challenge; when it got too close to walls it would spin or move unpredictably. The precise task was hard due to this interference." P16[Baseline]: "It was difficult to make minor adjustments and get close to the wall" On the other hand, participants in the AR conditions, operating the exact same robot with the same flight capabilities and stabilization limitations, found operation much easier: P12[Frustum]: "It was easier than I had thought. . . " P20[Callout]: "That wasn't so hard. . . " P40[Peripherals]: "Flying was very easy. . . I felt very comfortable."</p><p>In addition to this improvement in overall utility, our results also revealed an interesting tradeoff between objective performance and subjective user preference. Overall, participants used the Peripherals design to achieve significantly better task performance than in any other condition. However, users felt no more comfortable Session Tu-2: Best Paper Nominees I HRI'18, March 5-8, 2018, Chicago, IL, USA using Peripherals than they did using the baseline interface, felt that Peripherals only made the task marginally easier, and gave the Peripherals the lowest SUS score of the three AR designs. On the other hand, the Frustum and Callout design still offered significant improvements over the baseline in terms of task performance, but were rated much better in terms of subjective preferences. This result might inform future designs: for non-critical use cases (e.g., hobby flying, entertainment purposes, etc.), a Frustum or Callout design might improve both objective and subjective usability; however, for mission-critical applications, a Peripheral design might optimize performance at a small cost to perceived ease-of-use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Limitations and Future Work</head><p>While our results are promising, we recognize that this study represents only an initial exploration into the design of AR interfaces for robot teleoperation. Our participants gave us several suggestions to further improve our designs, often involving the lack of a visual indication for when an image had been captured. Our systems could easily be modified to provide such feedback, possibly leveraging the idiom now common in mobile devices where images briefly freeze to indicate a photo has been taken. Many participants found this missing feature to be a downside, indicating that teleoperation interface designers should take special care in including this functionality in future systems (current interfaces, including the Freeflight Pro used as our baseline, also lack this feature):</p><p>P24[Callout]: ". . . the lack of a visual indicator when a picture was taken meant I didn't know if I'd done so or not." P27[Peripherals]: "It was fairly easy to line up the rectangles for a brief moment, but found the lack of feedback after taking a photo difficult; I didn't know whether the photo had actually been captured upon pressing 'B' on the remote." P30[Frustum]: "I felt like I was blind with respect to whether I was capturing a good image with the appropriate field of view."</p><p>Aside from such improvements to our current designs, we also recognize that our models represent only a small subset of the rich design space regarding how interface designers might leverage AR feedback to improve robot teleoperation. We found our framework, which divides models across augmenting the environment, the robot, or the user interface useful in inspiring and structuring our design choices, however this framework may need further extensions and development to truly serve as an optimal taxonomy or predictive model for the AR-HRI design space. Other perspectives may also prove useful, for example thinking about "what" feedback may be presented to the user, compared with "how" such information is rendered. In this work, we focused on better ways of transmitting information about what is in the robot's field of view, as this will often be the most critical information for robot operators, and examined these effects in isolation to determine potential tradeoffs that might inspire the design of future systems. However, there is no reason our designs couldn't be combined (e.g., provide a Frustum and Peripheral feedback). Moreover, there is a variety of other information that might be important to convey, such as robot battery life, GPS coordinates, or readings from onboard sensor payloads.</p><p>As an additional limitation, we explicitly scoped this research within examining collocated aerial robot teleoperation. This decision was based on three considerations: line-of-site is currently mandated for aerial robots, interactions involving collocated flight are already common among hobbyists, and our collocated inspection task mimics the context of many envisioned aerial robot deployments, such as free-flyers operating within factories <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> or the International Space Station <ref type="bibr" target="#b1">[2]</ref>. Although we used an aerial robot in this experiment, we believe that our results could generalize to many other robots (e.g. ground robots, robotic arms, etc.). Future systems might explore the broader generalizability of our designs, for instance examining collocated interaction at longer distances, exploring how to convey additional robot data besides camera information, investigating augmented reality feedback for remote teleoperation, or studying scalability for operating multiple robots. Finally, future research might improve certain limitations of our current implementation. For instance, future work might remove our reliance on motion capture for precise robot localization by integrating our designs with simultaneous localization and mapping (SLAM) software. Alternatively, studies might improve upon our networking solution to increase the framerate of the Callout and Peripheral designs while presenting higher resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this work, we explored the design of augmented reality interfaces that support collocated robot teleoperation of aerial robots. We propose a framework for structuring the design of AR interfaces that support HRI and use this framework to develop three new teleoperation models that leverage ARHMD technology to provide visual feedback on robot camera capabilities. We conducted a 48participant user study comparing these designs against a baseline teleoperation system without AR feedback that is currently in common use. We found that our designs significantly improved objective measures of teleoperation performance and speed while reducing crashes. We attribute this enhanced performance to the support our systems provide for synthesizing information about the physical location of the robot with robot camera information, compared to traditional interfaces whose use leads to divided attention interactions. We believe that our designs represent a new paradigm that can reduce the detrimental effects of such constant context switching. Our subjective results revealed an interesting tradeoff, where participants preferred designs that moderately improved performance over the best-performing design. Overall, our results show that AR technologies hold great promise for mediating human-robot interactions and we hope our research can serve as a catalyst for additional explorations within this space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top: the Parrot Bebop robot used in this study. Bottomleft: the Freeflight Pro application used as our baseline condition. Bottom-right: the Xbox controller used in our AR conditions.</figDesc><graphic coords="5,59.39,174.19,229.07,103.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Objective results show that the augmented reality interface designs improved task performance in terms of accuracy and number of crashes, while minimizing distractions in terms of number of gaze shifts and total time distracted. (*), (**), and (***), denote comparisons with p &lt; .05, p &lt; .01, and p &lt; .001 respectively.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://itunes.apple.com/us/app/freeflight-pro/id889985763?mt=8 Session Tu-2: Best Paper Nominees I HRI'18, March 5-8,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2018, Chicago, IL, USA</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGMENT</head><p>This work was supported by an Early Career Faculty grant from NASA's Space Technology Research Grants Program under award NNX16AR58G. We thank Andrew Gorovoy and Adrienne Stenz for their help in our research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recent Advances in Augmented Reality</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Azuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohan</forename><surname>Baillot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhold</forename><surname>Behringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blair</forename><surname>Macintyre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="34" to="47" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Astrobee: Developing a Free-flying Robot for Session Tu-2: Best Paper Nominees I HRI</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Bualat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Provencher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trey</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allison</forename><surname>Zuniga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIAA SPACE 2015 Conference and Exposition</title>
		<meeting><address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-03-05">2015. March 5-8, 2018</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note>USA the International Space Station</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human Performance Issues and User Interface Design for Teleoperated Robots</title>
		<author>
			<persName><forename type="first">Jessie Yc</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">C</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1231" to="1245" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Corcoran</surname></persName>
		</author>
		<ptr target="http://www.abc.net.au/news/2012-02-21/drone-journalism-takes-off/3840616" />
		<title level="m">Drone Journalism Takes Off</title>
		<imprint>
			<date type="published" when="2014-02">2014. February 2014</date>
			<biblScope unit="page" from="2018" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eye-q: Eyeglass Peripheral Display for Subtle Intimate Notifications</title>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Costanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elan</forename><surname>Samuel A Inverso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pattie</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human-Computer Interaction with Mobile Devices and Services</title>
		<meeting>the ACM Conference on Human-Computer Interaction with Mobile Devices and Services</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">World embedded interfaces for human-robot interaction</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngkwan</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Payton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Hawaii International Conference on</title>
		<meeting>the 36th Annual Hawaii International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>System Sciences (HICSS&apos;03)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Policy-blending Formalism for Shared Control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><forename type="middle">S</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="790" to="805" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Comparing Situation Awareness for Two Unmanned Aerial Vehicle Human Interface Approaches</title>
		<author>
			<persName><forename type="first">Jill</forename><forename type="middle">L</forename><surname>Drury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Richer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Rackliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Goodrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Windows on the World: 2D Windows for 3D Augmented Reality</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blair</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliot</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th annual ACM Symposium on User Interface Software and Technology (UIST&apos;93</title>
		<meeting>the 6th annual ACM Symposium on User Interface Software and Technology (UIST&apos;93</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="145" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smart SPHERES: A Telerobotic Free-Flyer for Intravehicular Activities in Space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Micire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Provencher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mittman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AIAA Space&apos;13</title>
		<meeting>AIAA Space&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Situation Awareness in an Augmented Reality Cockpit: Design, Viewpoints and Cognitive Glue</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Foyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Hooey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Becky</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Human Computer Interaction</title>
		<meeting>the 11th International Conference on Human Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supporting Wilderness Search and Rescue using a Camera-equipped Mini UAV</title>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">S</forename><surname>Michael A Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damon</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Gerhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><surname>Humphrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="89" to="110" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human-Robot Collaboration: A Literature Review and Augmented Reality Approach in Design</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Scott A Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluating the Augmented Reality Human-Robot Collaboration System</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Scott A Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Billinghurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems Technologies and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="130" to="143" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Drew</forename><surname>Harwell</surname></persName>
		</author>
		<ptr target="https://www.washingtonpost.com/blogs/the-switch/wp/2014/11/25/this-government-rule-could-cripple-commercial-drone-flight/" />
		<title level="m">This government rule could cripple commercial drone flight</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flying Head: A Headsynchronization Mechanism for Flying Telepresence</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuya</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Reality and Telexistence</title>
		<imprint>
			<publisher>ICAT</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Peripheral Vision Annotation: Noninterference Information Presentation Method for Mobile Augmented Reality</title>
		<author>
			<persName><forename type="first">Yoshio</forename><surname>Ishiguro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Augmented Human International Conference</title>
		<meeting>the ACM Augmented Human International Conference</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun Rekimoto. 2011</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Measurement of Observer Agreement for Categorical Data</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977">1977. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Paula</forename><surname>Lavigne</surname></persName>
		</author>
		<title level="m">Eyes in the Sports Sky. ESPN website</title>
		<imprint>
			<date type="published" when="2014">2014. 2014. 2018-01-05</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Review and Analysis of Avionic Helmet-Mounted Displays</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="110901" to="110901" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Baxter&apos;s Homunculus: Virtual Reality Spaces for Teleoperation in Manufacturing</title>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">J</forename><surname>Jeffrey I Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Fay</surname></persName>
		</author>
		<author>
			<persName><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="179" to="186" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Influence of Perspective-taking and Mental Rotation Abilities in Space Teleoperation</title>
		<author>
			<persName><forename type="first">Alejandra</forename><surname>Menchaca-Brandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">M</forename><surname>Oman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Natapoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE International Conference on Human-Robot Interaction (HRI&apos;07)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Taxonomy of Mixed Reality Visual Displays</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumio</forename><surname>Kishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="1321" to="1329" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Applications of Augmented Reality for Human-Robot Communication</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Drascic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Grodski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;93)</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;93)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1467" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Design of Mobile Robot Teleoperation System based on Virtual Reality</title>
		<author>
			<persName><forename type="first">Masmoudi</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaddour</forename><forename type="middle">El</forename><surname>Boudadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khelf</forename><surname>Loukil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahane</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Amine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Control, Engineering &amp; Information Technology (CEIT&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Robin R</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Disaster Robotics</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Airline Head-up Display Systems: Human Factors Considerations</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Nicholl</surname></persName>
		</author>
		<ptr target="https://ssrn.com/abstract=2384101" />
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ecological Interfaces for Improving Mobile Robot Teleoperation</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Curtis W Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><surname>Ricks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="927" to="941" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Haptic Interface Design for Minimally Invasive Telesurgical Training and Collaboration in the Presence of Time Delay</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Shahin S Nudehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moji</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><surname>Ghodoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 42nd IEEE Conference on Decision and Control</title>
		<meeting>42nd IEEE Conference on Decision and Control</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4563" to="4568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Methods for Haptic Feedback in Teleoperated Robotassisted Surgery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><surname>Okamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Industrial Robot: An International Journal</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="499" to="508" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Augmented Reality-Enhanced Structural Inspection Using Aerial Robots</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Papachristos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Alexis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Intelligent Control (ISIC&apos;16)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semiautonomous Teleoperation of UAVs in Search and Rescue Scenarios</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fj Perez-Grau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ragel</surname></persName>
		</author>
		<author>
			<persName><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><surname>Viguria</surname></persName>
		</author>
		<author>
			<persName><surname>Ollero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Unmanned Aircraft Systems (ICUAS&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1066" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Collaborative exploration with a Micro Aerial Vehicle: A Novel Interaction Method for Controlling a MAV with a Hand-Held Device. Advances in Human-Computer Interaction</title>
		<author>
			<persName><forename type="first">David</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">L</forename><surname>Cummings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012. 2012</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AmbiGlasses-Information in the Periphery of the Visual Field</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Poppinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><surname>Henze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jutta</forename><surname>Fortmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilko</forename><surname>Heuten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Boll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mensch &amp; Computer</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semiautonomous human-UAV interfaces for fixed-wing mini-UAVs</title>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randal</forename><forename type="middle">W</forename><surname>Beard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;04)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2457" to="2462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Third Point of View Augmented Reality for Robot Intentions Visualization</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Ruffaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Brizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Tecchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Bacinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Augmented Reality</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
		</author>
		<title level="m">Human Interaction with Assistive Free-Flying Robots</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>The University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Designing Planning and Control Interfaces to Support User Collaboration with Flying Robots</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="514" to="542" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Medical Robotics and Computer-Integrated Surgery</title>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Russell H Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Menciassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Fichtinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Fiorini</surname></persName>
		</author>
		<author>
			<persName><surname>Dario</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Handbook of Robotics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1657" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adaptive View Management for Drone Teleoperation in Complex 3D Structures</title>
		<author>
			<persName><forename type="first">John</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Photchara</forename><surname>Ratsamee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoshi</forename><surname>Kiyokawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pakpoom</forename><surname>Kriangkomol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Orlosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Mashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Uranishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haruo</forename><surname>Takemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI&apos;17)</title>
		<meeting>the ACM International Conference on Intelligent User Interfaces (IUI&apos;17)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
