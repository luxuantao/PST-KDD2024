<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Baidu Tech Report 2020 MASKED LABEL PREDICTION: UNIFIED MASSAGE PASSING MODEL FOR SEMI-SUPERVISED CLASSIFICA-TION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-08">8 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
							<email>shiyunsheng01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
							<email>huangzhengjie@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
							<email>fengshikun01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Baidu Tech Report 2020 MASKED LABEL PREDICTION: UNIFIED MASSAGE PASSING MODEL FOR SEMI-SUPERVISED CLASSIFICA-TION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-08">8 Sep 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2009.03509v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional network (GCN) and label propagation algorithms (LPA) are both message passing algorithms, which have achieved superior performance in semi-supervised classification. GCN performs feature propagation by a neural network to make predictions, while LPA uses label propagation across graph adjacency matrix to get results. However, there is still no good way to combine these two kinds of algorithms. In this paper, we proposed a new Unified Massage Passaging model (UniMP) that can incorporate feature propagation and label propagation with a shared message passing network, providing a better performance in semi-supervised classification. First, we adopt a graph Transformer network jointly label embedding to propagate both the feature and label information. Second, to train UniMP without overfitting in self-loop label information, we propose a masked label prediction method, in which some percentage of training examples are simply masked at random, and then predicted. UniMP conceptually unifies feature propagation and label propagation and be empirically powerful. It obtains new state-of-the-art semi-supervised classification results in Open Graph Benchmark (OGB). Our implementation is available online</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>There are various scenarios in the world, e.g., recommending related news and products, discovering new drugs, or predicting social relations, which can be described as graph structures. Many methods have been proposed to optimize these graph-based problems and achieved significant success in many related domains. Such as predicting the properties of nodes <ref type="bibr" target="#b24">(Yang et al., 2016;</ref><ref type="bibr" target="#b8">Kipf &amp; Welling, 2016)</ref>, links <ref type="bibr" target="#b4">(Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b0">Battaglia et al., 2018)</ref>, and graphs <ref type="bibr" target="#b2">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b14">Niepert et al., 2016;</ref><ref type="bibr">Bojchevski et al., 2018)</ref>.</p><p>In the task of semi-supervised node classification, we are required to learn with labeled examples and then make predictions for those unlabeled ones. To better classify the node labels in the graph, based on the Laplacian smoothing assumption <ref type="bibr" target="#b11">(Li et al., 2018;</ref><ref type="bibr" target="#b23">Xu et al., 2018b)</ref>, the massage passing models were proposed to aggregate the information from its connected neighbored nodes in the graph, aggregating more information to produce a more robust prediction for unlabeled nodes. Generally, there are two kinds of practical methods to implement message passing model, the graph convolutional neural network (GCN) <ref type="bibr" target="#b8">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b5">Hamilton et al., 2017;</ref><ref type="bibr" target="#b23">Xu et al., 2018b;</ref><ref type="bibr" target="#b12">Liao et al., 2019;</ref><ref type="bibr" target="#b22">Xu et al., 2018a;</ref><ref type="bibr" target="#b15">Qu et al., 2019)</ref> and the label propagation algorithm (LPA) <ref type="bibr" target="#b28">(Zhu, 2005;</ref><ref type="bibr" target="#b27">Zhu et al., 2003;</ref><ref type="bibr" target="#b26">Zhang &amp; Lee, 2007;</ref><ref type="bibr" target="#b18">Wang &amp; Zhang, 2007;</ref><ref type="bibr" target="#b7">Karasuyama &amp; Mamitsuka, 2013;</ref><ref type="bibr" target="#b3">Gong et al., 2016;</ref><ref type="bibr" target="#b13">Liu et al., 2019)</ref>. GCN combines graph structures by propagating and aggregating features through several neural layers, which gets predictions from feature propagation. While LPA only uses the adjacency matrix of the graph, making predictions for unlabeled instances by label propagation.</p><p>Since GCN and LPA are based on the same assumption, making semi-supervised classifications by information propagation, there is an intuition that incorporating them together can boost performance. Some superior studies have proposed graph models based on it. For example, APPNP <ref type="bibr" target="#b9">(Klicpera et al., 2019)</ref> and <ref type="bibr">TPN Liu et al. (2019)</ref> integrate GCN and LPA by concatenating them together, and GCC-LPA <ref type="bibr" target="#b20">(Wang &amp; Leskovec, 2019)</ref> uses LPA to regularize their GCN model. However, shown as Tabel 1, there methods still can not propagate feature and label within a massage passing model in both training and prediction procedure. To unify the feature propagation and label propagation, there are two issues needed to be solved: 1. It is not easy to aggregate feature and label information. Since the feature is represented by embedding, while the label is a one-hot vector, which are not in the same vector space. In addition, there are the difference between their massage passing ways, GCN can propagate the information by different ways likes GraphSAGE <ref type="bibr" target="#b5">(Hamilton et al., 2017)</ref>, <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref> and GAT <ref type="bibr" target="#b17">(Veličković et al., 2017)</ref>, but label propagation can only pass the massage by adjacency matrix.</p><p>2. Supervised training a unified massage passing model with feature and label propagation will overfit in self-loop label information inevitably, which makes the label leakage in training time and causing poor performance in prediction.</p><p>In this work, inspired by several advantage developments in NLP <ref type="bibr" target="#b19">(Wang et al., 2018;</ref><ref type="bibr" target="#b16">Vaswani et al., 2017;</ref><ref type="bibr" target="#b1">Devlin et al., 2018)</ref>, we prosed a new Unified Massage Passing model (UniMP) with Masked Label Prediction that can settle the aforementioned issues. UniMP is a multi-layer graph Transformer network, jointly using label embedding to transform label information into the same vector space as node features. After that, UniMP propagates node features like the previous graph attention based network <ref type="bibr" target="#b17">(Veličković et al., 2017;</ref><ref type="bibr" target="#b25">Zhang et al., 2018)</ref>. Meanwhile, the multi-head attentions are used as transition matrix for propagating label vector, so that each node can aggregate both feature and label information from its neighbors, making a more precise prediction. To supervised training UMP without overfitting in label information, we draw lessons from BERT <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref>, proposing a Masked Label Prediction method (Masked Word Prediction for pre-training in BERT), which randomly masks some training instances' label embedding vectors and then predicts them. This training method perfectly simulates the procedure of transducing label information from labeled to unlabeled examples in the graph.</p><p>We conduct experiments on three datasets in the Open Graph Benchmark (OGB), where our new methods achieve new state-of-the-art results in all tasks, gaining 82.56% ACC in ogbn-products (1.66% absolute improvement), 86.42% ROC-AUC in ogbn-proteins (0.62% absolute improvement) and 73.11% ACC in ogbn-arxiv (0.37% absolute improvement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">UNIFIED MASSAGE PASSING MODEL</head><p>We first introduce our notation about graph. We donate a graph as G = (V, E), where V is the nodes with |V | = n and E is the edges with |E| = m. The nodes are described by the feature matrix X ∈ R n×f , which usually are dense vectors with f demension, and the target class matrix Y ∈ R n×c , with the number of classes c. The adjacency matrix A = [a i,j ] ∈ R n×n is used to describe graph G, and the diagonal degree matrix is denoted by</p><formula xml:id="formula_0">D = diag(d 1 , d 2 , ..., d n ) , where d i = j a i,j is the degree of node i. A nomalized adjacency matrix is difined as D −1 A or D − 1 2 AD − 1 2</formula><p>, and in this paper, we use the first format as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">FEATURE PROPAGATION AND LABEL PROPAGATION</head><p>In semi-supervised node classification of graph, based on the Laplacian smoothing assumption, the GCN transforms and propagates node features X across the graph by several layers, including linear layers and nonlinear activation to learn the mapping: X → Y . The feature propagation scheme of GCN in layer l is:</p><formula xml:id="formula_1">H l+1 = σ(D −1 AH l W l ) Y = sof tmax(H L ) (1)</formula><p>where the σ is an activation function such as Tanh, Sigmoid or ReLu, W l is the trainable weight in the l-th layer, and the H l is the l-th layer represenations of nodes, when H 0 is equal to node input features X. Finally, a sof tmax classifier is applied on the output reprentation to make prediction for Y .</p><p>As for LPA, it also assumes the labels between each connected node are smoothing and propagates the labels iteratively across the graph. Given an initial label matrix Ŷ 0 , which consists of one-hot label indicator vector ŷ0 i for the labeled nodes or zeros vectors for the unlabeled. An simple iteration equation of LPA is formulated as following:</p><formula xml:id="formula_2">Ŷ (k+1) = D −1 A Ŷ (k)<label>(2)</label></formula><p>Labels are propagated from each other nodes through a normalized adjacency matrix D −1 A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">UNIFIED MASSAGE PASSING MODEL</head><p>To combine the aforementioned feature propagation and label propagation together, we employ a graph Transformer model, jointly using label embedding to construct our unified massage passing model.</p><p>Graph Transformer Network. Different from the previous attention-based GCN netwroks <ref type="bibr" target="#b17">(Veličković et al., 2017;</ref><ref type="bibr" target="#b25">Zhang et al., 2018)</ref>, which have modified the vanilla multi-head Attention more or less, we convert the vanlilla multi-head attention of Transformer <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> directly from NLP to graph learning. Specifically, we calculate multi-head Attentions for each edge e = (t, s) as following:</p><formula xml:id="formula_3">MT-A l i = sof tmax( Q ⊺ i (K i + E i ) √ d ) Q i = H l [t]W q i + b q i K i = H l [s]W k i + b k i E i = H e W e i + b e<label>(3)</label></formula><p>Our model also takes into account the case of edge features if its have been given. For the i-th head attention, we firstly transform the source H l [s] and target node features</p><formula xml:id="formula_4">H l [t] into Query vector Q i ∈ R d C and Key vector K i ∈ R d C respectively using different trainable parameters W q i , W k i , b q i , b k i .</formula><p>The edge features H e will be encoded and added into Key vector as extended additional information. Similar to the original, we calculate the scaled dot product between Query and Key vector as their attention result.</p><p>After getting the graph multi-head attention, we make a massage aggregation from the source to the target nodes:</p><formula xml:id="formula_5">H (l+1) = ReLu(layerNorm(( C i=1 (MT-A i (V i + E i )) + R)) V i = H[s] l W q i + b v i R = H l W r + b r (4)</formula><p>where the is the concatenation operation, and C is the head number. Comparing with the equation (1), The multi-head Attention replaces the original normalized adjacency matrix as transition matrix for massage passing, and the source node is transformed to V i by parameters W v i and b v i . In addition, we also apply a residual connections on it by transforming H l to R.</p><p>Specially, similar to GAT, if we apply the graph Transformer on the output layer of the network, we will employ averaging for multi-head output. Moreover, we aggregate the graph output and the linear ouput together by the fusion gate α.</p><formula xml:id="formula_6">H L = (1 − α) ĤL + αR ĤL = 1 C C i=1 (MT-A i (V i + E i ) α = sigmoid(W [ ĤL ; R; ĤL − R])<label>(5)</label></formula><p>Label Embedding and Propagation. We propose to embed the label information into the same space as node features: Ŷ ∈ R n×c → Ĉn×f , which consists of the label embedding vector for labeled nodes and zeros vectors for the unlabeled. To combine the label propagation into graph Transformer, we propose a unified massage passing model by simply adding the node feature and label feature together as propagation feature (H 0 = X + Ĉ) ∈ R n×f . We can prove that by mapping partially-labeled Ŷ and node feature X into the same space, our model is unifying both label propagation and feature propagation into a shared massage passing framework. Let's take Ĉ = Ŷ W (l) and A * to be Laplacian Matrix D − 1 2 AD − 1 2 or an attentive matrix which is learnt by attention technique. Then we can find that:</p><formula xml:id="formula_7">H (0) = X + Ŷ W (l) H (k) = σ(((1 − α)A * + αI)H (k−1) W (k) )<label>(6)</label></formula><p>where α can be the gated attention or a pre-defined hyper-parameters like APPNP <ref type="bibr" target="#b9">Klicpera et al. (2019)</ref>. For simplification, we let σ function as identity function, then we can get:</p><formula xml:id="formula_8">H (k) = ((1 − α)A * + αI) k (X + Ŷ W (l) )W (1) W (2) . . . W (k) = ((1 − α)A * + αI) k XW + ((1 − α)A * + αI) k Ŷ W (l) W<label>(7)</label></formula><p>Let's take W = W (1) W (2) . . . W (k) , then we can find that our model can be approximately decomposed into feature propagation ((1 − α)A * + αI) k XW and label propagation ((1 − α)A * + αI) k Ŷ W (l) W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MASKED LABEL PREDICTION</head><p>Previous works on GCN seldom consider using the partially-observed labels in both training and inference stages. To utilize the partially-observed labels, we adopt masked label prediction strategies to avoid label leakage in the training stage. We also provide a new prospect of masked label predictions in graph semi-supervised learning as sequential label prediction. As shown in Equ. 8, most GNNs model the label prediction task independently with only the node features and the adjacent matrix. The partially-observed labels are only used as the target in the training phase and they will be discarded in the inference stage. The discard of the training labels assumes that the node feature and adjacent matrix can fit the training label perfectly. However, in practice, we find that for some datasets, using only node features and the adjacent matrix to predict the labels is still underfitting.</p><formula xml:id="formula_9">p(Y |X, A) = v∈V p(y v |X, A). (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>Let U be the subset of V which have partially-observed labels Ŷ , then Equ. 8 can be reformulated as follows:</p><formula xml:id="formula_11">P (Y V −U |X, A) = v∈V −U p(y v |X, Ŷ , A). (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>With Equ. 9, we can perform label propagation on ground truths instead of approximation by X and A. In order to avoid label leakage, in training stage, we adopt Masked Label Prediction which randomly masks a part of the observed labels and try to learn to recover them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We propose a Unified Massage Passing model (UniMP) for semi-supervised node classification, which incorporates the feature propagation and label propagation by a joint graph Transformer netwrok and label propagation. We perform extensive experiments on the Node Property Prediction of Open Graph Benchmark (OGBN), which includes a several various challenging and large-scale datasets, splitted in the procedure that closely matches the real-world application. We compare our model with others sate-of-the-art models in ogbn-products, ogbn-proteins and ogbn-arxiv three node property prediction datasets, with a ablation study. We also conduct a comprehensive study on the hyper-parameter sensitivity and provide an analysis of feature propagation and label propagation in our unified model. Datasets. Traditional graph datasets have been shown to be limited and unable to provide a reliable evaluation and rigorous comparison among methods <ref type="bibr" target="#b6">(Hu et al., 2020)</ref> due to several issues including their small scale nature, non-negligible duplication or leakage rates, unrealistic data splits, etc. Consequently, we conduct our experiments on the recently released datasets of Open Graph Benchmark (OGB) <ref type="bibr" target="#b6">(Hu et al., 2020)</ref>, which overcome the main drawbacks of commonly used datasets and thus are much more realistic and challenging. OGB datasets cover a variety of real-world applications and span several important domains ranging from social and information networks to biological networks, molecular graphs, and knowledge graphs. They also span a variety of predictions tasks at the level of nodes, graphs, and links/edges. In this work, we performed our experiments on the three OGBN datasets for getting credible result. We introduce these three datasets briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head><p>ogbn-Products. As shown in Table <ref type="table" target="#tab_1">2</ref>, the ogb-product is an undirected and unweighted graph, representing an Amazon product co-purchasing network. The goal of this task is to predict the category of a product in a multi-class classification setup, where the 47 top-level categories are used for target labels. To match the real-world application, it conducts a splitting on the dataset based on salesranking, where the top 10% for training, next top 2% for validation, and the rest for testing.</p><p>ogbn-Proteins. As shown in Table <ref type="table" target="#tab_1">2</ref>, The ogbn-proteins dataset is an undirected, weighted, and typed (according to species) graph. Nodes represent proteins, and edges indicate different types of biologically meaningful associations between proteins, e.g., physical interactions, co-expression or homology. The task is to predict the presence of protein functions in a multi-label binary classification setup, where there are 112 kinds of labels to predict in total. The performance is measured by the average of ROC-AUC scores across the 112 tasks. It conducts the splitting by species.</p><p>ogbn-Products. As shown in Table <ref type="table" target="#tab_1">2</ref>, The ogbn-arxiv dataset is a directed graph, representing the citation network between all Computer Science (CS) ARXIV papers indexed by MAG <ref type="bibr" target="#b21">(Wang et al., 2020)</ref>. The task is to predict the 40 subject areas of ARXIV CS papers,11 e.g., cs.AI, cs.LG, and cs.OS, which are manually determined (i.e., labeled) by the papers authors and ARXIV moderators. This dataset were splitted by Time.</p><p>Implementation Details. Following the previous studies, we evaluate our model in the various datasets with different sampling methods to get a credible result as the SOTA methods did <ref type="bibr" target="#b10">(Li et al., 2020)</ref>. In ogbn-products dataset, we use NeighborSampling with size = 10 for each layer to sample the subgraph during training and use full-batch for inferrence. In ogbn-proteins dataset, we use Random Partition to split the dense graph into subgraph to train and test our model. The number of partitions is 10 for training and 5 for test. As for small-size ogbn-arxiv dataset, we just apply full batch for both training and test. We set the hyper-parameter of our model for each dataset in Table <ref type="table" target="#tab_2">3</ref>, and the label rate means the percentage of labels we preserve during using masked label prediction. We use Adam optimizer with lr = 0.001 to train our model. Specially, we set weight decay to 0.0005 for our model in small-size ogbn-arxiv dataset to prevent overfitting. We implement all our models by PGL<ref type="foot" target="#foot_0">1</ref> and PaddlePaddle<ref type="foot" target="#foot_1">2</ref> and run all experiments on a single NVIDIA V100 32GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS</head><p>The experimental results are shown in Tabel 4, Tabel 5, and Tabel 6. We run the results for each dataset 10 times and report the mean and standard deviation as required. Results show that our models outperform all other baselines models in three ogbn dataset by significant margins, in which we gain 82.56% ACC in ogbn-products (1.66% absolute improvement), 86.42% ROC-AUC in ogbnproteins (0.62% absolute improvement) and 73.11% ACC in ogbn-arxiv (0.37% absolute improvement).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUTION AND FUTURE WORK</head><p>We first propose a unified massage passing model, UniMP, which jointly performs feature propagation and label propagation within a graph Transformer network to make semi-supervised classification. Furthermore, we propose a masked label prediction method to supervised training our model, preventing it from overfitting in self-loop label information. Experimental results show that unimp outperforms the previous state-of-the-art models on three main OGBN datasets: ogbn-products, ogbn-proteins and ogbn-arxiv by a large margin, and ablation studies demonstrate the effectiveness of unifying feature propagation and label propagation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparision between massage passing models</figDesc><table><row><cell>Training</cell><cell>Prediction</cell></row><row><cell cols="2">Feature Label Feature Label</cell></row><row><cell>LPA</cell><cell></cell></row><row><cell>GCN</cell><cell></cell></row><row><cell>APPNP</cell><cell></cell></row><row><cell>GCN-LPA</cell><cell></cell></row><row><cell>UniMP (Ours)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics of OGB node property prediction</figDesc><table><row><cell>Name</cell><cell>Node</cell><cell>Edges</cell><cell cols="3">Tasks Split Rate Split Type</cell><cell>Task Type</cell><cell>Metric</cell></row><row><cell cols="3">ogbn-Products 2, 449, 029 61, 859, 140</cell><cell>1</cell><cell cols="4">10\02\88 Sales rank Multi-class class Accuracy</cell></row><row><cell cols="5">ogbn-Proteins 132, 534 39, 561, 252 112 65\16\19</cell><cell>Species</cell><cell>Binary class</cell><cell>ROC-AUC</cell></row><row><cell>ogbn-arxiv</cell><cell>169, 343</cell><cell>1, 166, 243</cell><cell>1</cell><cell>78\08\14</cell><cell>Time</cell><cell cols="2">Multi-class class Accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The detailed hyperparamerter of our model</figDesc><table><row><cell></cell><cell>ogbn-products</cell><cell>ogbn-proteins</cell><cell>ogbn-arxiv</cell></row><row><cell cols="4">sampling method NeighborSampling Random Partition Full-batch</cell></row><row><cell>num layers</cell><cell>3</cell><cell>7</cell><cell>3</cell></row><row><cell>hidden size</cell><cell>128</cell><cell>64</cell><cell>128</cell></row><row><cell>num heads</cell><cell>4</cell><cell>4</cell><cell>2</cell></row><row><cell>dropout</cell><cell>0.3</cell><cell>0.1</cell><cell>0.3</cell></row><row><cell>lr</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>weight decay label rate</cell><cell>*  0.625</cell><cell>*  0.5</cell><cell>0.0005 0.625</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results for ogbn-products</figDesc><table><row><cell>Model</cell><cell>Test Accuracy</cell><cell>Validation Accuracy</cell><cell>Params</cell></row><row><cell>GCN-Cluster GAT-Cluster GAT-NeighborSampling GraphSAINT DeeperGCN UniMP unimp-w/o LP</cell><cell>0.7897 ± 0.0036 0.7923 ± 0.0078 0.7945 ± 0.0059 0.8027 ± 0.0026 0.8090 ± 0.0020 0.8256 ± 0.0031 0.8023 ± 0.0026</cell><cell>0.9212 ± 0.0009 0.8985 ± 0.0022 --0.9238 ± 0.0009 0.9308 ± 0.0017 0.9286 ± 0.0017</cell><cell>206, 895 1, 540, 848 1, 751, 574 331, 661 253, 743 1, 475, 605 1, 470, 905</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results for ogbn-proteins</figDesc><table><row><cell>Model</cell><cell>Test Accuracy</cell><cell>Validation Accuracy</cell><cell>Params</cell></row><row><cell>GaAN GeniePath-BS MWE-DGCN DeepGCN DeeperGCN unimp unimp-w/o LP</cell><cell>0.7803 ± 0.0073 0.7825 ± 0.0035 0.8436 ± 0.0065 0.8496 ± 0.0028 0.8580 ± 0.0017 0.8642 ± 0.0008 0.8611 ± 0.0017</cell><cell>--0.8973 ± 0.0057 0.8921 ± 0.0011 0.9106 ± 0.0016 0.9175 ± 0.0007 0.9128 ± 0.0007</cell><cell>-316, 754 538, 544 2, 374, 456 2, 374, 568 1, 909, 104 1, 879, 664</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results for ogbn-arxiv</figDesc><table><row><cell>Model</cell><cell>Test Accuracy</cell><cell>Validation Accuracy</cell><cell>Params</cell></row><row><cell>DeeperGCN GaAN DAGNN JKNet GCNII unimp unimp-w/o LP</cell><cell>0.7192 ± 0.0016 0.7197 ± 0.0024 0.7209 ± 0.0025 0.7219 ± 0.0021 0.7274 ± 0.0016 0.7311 ± 0.0021 0.7225 ± 0.0015</cell><cell>0.7262 ± 0.0014 --0.7335 ± 0.0007 -0.7450 ± 0.0005 0.7367 ± 0.0012</cell><cell>1, 471, 506 1, 471, 506 1, 751, 574 331, 661 2, 148, 648 473, 489 468, 369</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/PaddlePaddle/PGL</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/PaddlePaddle/Paddle</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work can be advance from the following perspectives:</p><p>• We will replace the current graph Transformer network with other GCN models and conduct more experiments to investigate a more universal massage passing model. • Currently, our model only performs in transductive learning. We will extend the current UniMP model to support both transductive and inductive learning in semi-supervised classification.</p><p>• We can design a more efficient training method to reduce our training time for the model while gaining a better performance.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<idno>arXiv:1803.00816</idno>
	</analytic>
	<monogr>
		<title level="m">Generating graphs via random walks</title>
				<editor>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</editor>
		<editor>
			<persName><surname>Netgan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Label propagation via teaching-tolearn and learning-to-teach</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1452" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Manifold-based similarity adaptation for label propagation</title>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1547" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gunnemann</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="3538" to="3545" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01484</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Gmnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06214</idno>
		<title level="m">Graph markov neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Label propagation through linear neighborhoods</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04174</idno>
		<title level="m">Joint embedding of words and labels for text classification</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hyperparameter learning for graph based semi-supervised learning algorithms</title>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1585" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
				<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
