<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Anomaly Detection With Sparse Coding Inspired Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weixin</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dongze</forename><surname>Lian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lixin</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Jinhui Tang is with Nanjing University of Science and Engineering</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Lixin Duan is with University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">is with School of Information Science and Technology</orgName>
								<orgName type="institution">Xi Peng is with Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country>China. • S. Gao</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">W</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Tech-nology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country>China,</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>200050</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Anomaly Detection With Sparse Coding Inspired Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">05C03043252F146EF616EC06FCCA9A4E</idno>
					<idno type="DOI">10.1109/TPAMI.2019.2944377</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2019.2944377, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sparse Coding</term>
					<term>Anomaly Detection</term>
					<term>Stacked Recurrent Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an anomaly detection method that is based on a sparse coding inspired Deep Neural Networks (DNN). Specifically, in light of the success of sparse coding based anomaly detection, we propose a Temporally-coherent Sparse Coding (TSC), where a temporally-coherent term is used to preserve the similarity between two similar frames. The optimization of sparse coefficients in TSC with the Sequential Iterative Soft-Thresholding Algorithm (SIATA) is equivalent to a special stacked Recurrent Neural Networks (sRNN) architecture. Further, to reduce the computational cost in alternatively updating the dictionary and sparse coefficients in TSC optimization and to alleviate hyperparameters selection in TSC, we stack one more layer on top of the TSC-inspired sRNN to reconstruct the inputs, and arrive at an sRNN-AE. We further improve sRNN-AE in the following aspects: i) rather than using a predefined similarity measurement between two frames, we propose to learn a data-dependent similarity measurement between neighboring frames in sRNN-AE to make it more suitable for anomaly detection; ii) to reduce computational costs in the inference stage, we reduce the depth of the sRNN in sRNN-AE and, consequently, our framework achieves real-time anomaly detection; iii) to improve computational efficiency, we conduct temporal pooling over the appearance features of several consecutive frames for summarizing information temporally, then we feed appearance features and temporally summarized features into a separate sRNN-AE for more robust anomaly detection. To facilitate anomaly detection evaluation, we also build a large-scale anomaly detection dataset which is even larger than the summation of all existing datasets for anomaly detection in terms of both the volume of data and the diversity of scenes. Extensive experiments on both a toy dataset under controlled settings and real datasets demonstrate that our method significantly outperforms existing methods, which validates the effectiveness of our sRNN-AE method for anomaly detection. Codes and data have been released at https://github.com/StevenLiuWen/sRNN TSC Anomaly Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A NOMALY detection is an important task in computer vision, and it has many potential applications in video surveillance, activity recognition and scene understanding, etc. However, anomaly detection is also an extremely challenging task. 1 Because of the unbounded and rare nature of anomalies, it is extremely expensive and sometimes infeasible to collect different types of abnormal events. For example, spontaneous car combustion is rare, and it is difficult to collect or simulate this kind of anomaly. Consequently, it seems infeasible to formulate anomaly detection with a binary classification framework because if some types of abnormal events are not included in the training set, the test phase may misclassify these kinds of anomalies. Further, considering the rare and unbounded nature of anomaly detection as well as to simplify the data collection procedure, only normal data is given in the training set in the common setup, with anomaly detection 1. This paper focuses on frame-level anomaly detection, and such framelevel prediction can meet the requirement of video surveillance. here aiming at discovering abnormal events in the test set.</p><p>To tackle anomaly detection when only normal data is given, an intuitive approach is to model the distribution of regular patterns, where data that does not agree with the distribution of regular patterns are classified as irregular. Recently, with the success of Convolutional Neural Networks, people leverage deep Convolutional Auto-Encoder <ref type="bibr" target="#b0">[1]</ref> or Convolutional LSTM Auto-Encoder <ref type="bibr" target="#b1">[2]</ref> to model the normal distribution in the training set, and irregular patterns will be distinguished by large reconstruction errors. These deep learning solutions are very efficient in the testing phase, but they rely on some delicately designed deep neural network architectures, and the principles for network design are still not well formulated. In addition, dictionary learning based approaches <ref type="bibr" target="#b2">[3]</ref>  <ref type="bibr" target="#b3">[4]</ref>, especially sparse coding based approaches, have been proposed and have shown their expertise in tackling such a task. Specifically, sparse coding based approaches encode regular patterns with a dictionary. Regular patterns can be linearly reconstructed by the entries in the dictionary with small reconstruction errors. In contrast, irregular patterns would lead to large reconstruction errors. However, the dictionary learning procedure during training is very time consuming for sparse coding based anomaly detection, and the optimization of sparse coefficients in the test phase is also very time-consuming, which restricts the deployment of these methods in real applications. Further, frame-wise sparse coding does not consider the coherence among neighboring frames for normal events.</p><p>Recently, Wisdom et al. <ref type="bibr" target="#b4">[5]</ref> have shown that actually the optimization of sparse coding with Iterative Soft-Thresholding Algorithm (ISTA) is essentially a special type of deep neural network. Motivated by the success of sparse coding based anomaly detection and the interpretation of ISTA with deep learning, we propose a sparse coding inspired Deep Neural Networks (DNN) framework for anomaly detection. Specifically, we add a temporally-coherent term into the sparse coding objective that utilizes the similarity between neighboring frames to weight the distance between their corresponding sparse coefficients. Then we arrive at a Temporally-coherent Sparse Coding (TSC). The optimization of sparse coefficients in TSC with Sequence Iterative Soft-Thresholding Algorithm (SISTA) <ref type="bibr" target="#b4">[5]</ref> in essence is a special type of stacked Recurrent Neural Networks (sRNN). To reduce the expensive computational costs in alternatively optimizing the dictionary and sparse coefficients, as well as to avoid the hyperparameters selection in TSC, we propose stacking one more layer on top of the TSC counterpart sRNN, arriving at an sRNN Auto-Encoder (sRNN-AE). With this sRNN-AE, the dictionary, the reconstruction coefficients and all of the hyperparameters can be automatically learned. In addition, the testing phase for each video frame is equivalent to a forward pass in this sRNN-AE. Further, to reduce computational costs in the test phase, we reduce the depth of the sRNN in sRNN-AE. Such a shallow architecture not only alleviates the gradient vanishing/exploding effect in the optimization of the DNN, thus improving the performance of sRNN-AE, but also improves efficiency in the test phase. Another advantage of sRNN-AE is that rather than using a predefined similarity measurement between neighboring frames, motivated by the kernel trick <ref type="bibr" target="#b5">[6]</ref>, we propose mapping the features of two frames to a new feature space with multi-layer perception and using the inner product of the new features as a similarity measurement. Experiments validate the effectiveness of such a similarity learning module in sRNN-AE.</p><p>Our sRNN-AE works as a classifier and its architecture is inspired by the ISTA based optimization of sparse coding. Besides a good classifier, a discriminative video representation is also desired for the good performance of anomaly detection. In real scenes, anomalies can be caused by unseen objects, namely appearance, unusual moving patterns, namely motion. Therefore, we propose extracting both appearance and motion features as the input of sRNN-AE. Inspired by the success of two-stream CNNs for video representation in activity recognition <ref type="bibr" target="#b6">[7]</ref>, in this paper, we propose learning a separate sRNN-AE with different features for anomaly detection. In two-stream CNNs, the stream corresponding to motion takes optical flow as input, where the calculation of optical flow is time-consuming. Considering that the difference among neighboring frames also characterizes the motion of objects <ref type="bibr" target="#b7">[8]</ref>, we propose conducting temporal pooling over appearance features of several consecutive frames for summarizing information temporally. Such a strategy would reduce the costs in calculating optical flow and motion features extraction with optical flow based CNN. Experiments validate the effectiveness and efficiency of this type of feature-aggregation based temporal representation.</p><p>It is desirable to learn an anomaly detection model which works well under multiple scenes. However, almost all existing datasets only contain videos captured by one camera with a fixed view, so these datasets lack scene diversity. Further, a large-scale dataset is in high demand for the evaluation of deep learning based anomaly detection approaches. In this paper, we build a new large-scale anomaly detection dataset. We set up multiple cameras with different view angles to capture real events in the teaching, research and living areas of our campus, and we name our new dataset the ShanghaiTech Campus anomaly detection dataset. To the best of our knowledge, our dataset is the largest one in terms of volume of frames, scene diversity, as well as viewing angles.</p><p>Contribution: We summarize our contributions of this work as follows: i) We design a sparse coding inspired sRNN-AE framework for anomaly detection, which alleviates the hyperparameters selection and dictionary training in TSC. Further, similarity can also be automatically learned in sRNN-AE; ii)we propose an appearance features based temporal characterization strategy. Then we propose learning a separate sRNN-AE for both spatial and temporal features for anomaly detection; iii) we collect a large-scale anomaly detection dataset, which greatly facilitates the evaluation of anomaly detection algorithms.</p><p>This paper is an extension of our previous work <ref type="bibr" target="#b8">[9]</ref>. We extend the framework in the following aspects: i) motivated by the kernel trick, we introduce a similarity learning module in sRNN-AE, which demonstrates its effectiveness over a predefined similarity for anomaly detection; ii) we propose an appearance features based temporal characterization strategy, and propose learning a separate sRNN-AE using both spatial and temporal features for anomaly detection; iii) more details of our implementation are given, and more experiments are conducted for performance evaluation.</p><p>The rest of this paper is organized as follows: In Section 2, we introduce work related to anomaly detection. In Section 3, we first briefly revisit sparse coding based anomaly detection and introduce the TSC formulation. Based on the optimization of TSC, we arrive at an sRNN based framework for anomaly detection. In Section 4, extensive experiments under both controlled and uncontrolled settings are conducted to validate the effectiveness of our work. We also evaluate the different components of our sRNN-AE algorithm with an ablation study in this section. We conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Most existing work on anomaly detection can be categorized into two steps: i) Feature extraction; One can leverage hand-crafted or deep learning based features. ii) Normal distribution learning; In this phase, a distribution is learned over the normal data of the training set, so that abnormal data of the test set will have a large reconstruction error over this distribution.</p><p>Hand Craft Feature and Distribution Modeling. Early work utilizes low-level trajectory features to represent regular patterns <ref type="bibr" target="#b9">[10]</ref>. However, these methods are not robust in complex or crowded scenes. In order to solve this problem, spatial-temporal features, such as histograms of oriented gradients (HOG) <ref type="bibr" target="#b10">[11]</ref> and histograms of oriented flows (HOF) <ref type="bibr" target="#b11">[12]</ref> have been widely leveraged. Based on these spatial-temporal features, Zhang et al.. <ref type="bibr" target="#b12">[13]</ref> model the normal patterns with a Markov random field (MRF). Adam et al. <ref type="bibr" target="#b13">[14]</ref> fit the regular histograms of optical flow in local regions with an exponential distribution. To represent local optical flow patterns, Kim and Grauman <ref type="bibr" target="#b14">[15]</ref> utilize a mixture of probabilistic PCA model. <ref type="bibr">Leyva et al. [16]</ref> propose an online framework by leveraging Gaussian Mixture Models, Markov Chains, and Bag-of-Words for anomaly detection.</p><p>Sparse Coding Based Anomaly Prediction. Dictionary learning based approaches are widely used in anomaly detection <ref type="bibr" target="#b3">[4]</ref> [3] [17] <ref type="bibr" target="#b17">[18]</ref>. A fundamental assumption of these methods is that any feature can be linearly represented as a linear combination of the bases of a dictionary that encodes regular patterns of the training set. <ref type="bibr" target="#b3">[4]</ref> [3] <ref type="bibr" target="#b16">[17]</ref> use the reconstruction error to determine whether a frame is abnormal or not. Ren et al. <ref type="bibr" target="#b17">[18]</ref> point out that reconstruction errors, such as those in least squares, do not take a sparsity term into consideration, while in fact, they do help to improve anomaly detection accuracy. To avoid this, Ren et al. <ref type="bibr" target="#b17">[18]</ref> propose two solutions, i.e. maximum coordinate (MC) and nonzero concentration (NC), to detect anomalies. However, sparse reconstruction based methods are usually time-consuming during the optimization of sparse coefficients. To solve this problem, Jia et al. <ref type="bibr" target="#b2">[3]</ref> propose to discard the sparse constraint and learn multiple dictionaries to encode the patches at multiple scales, which inevitably leads to additional costs in the training phase.</p><p>Deep Learning Based Anomaly Detection. Deep learning approaches have demonstrated their success for image classification <ref type="bibr" target="#b18">[19]</ref> [20], object detection <ref type="bibr" target="#b20">[21]</ref>  <ref type="bibr" target="#b21">[22]</ref>, as well as anomaly detection <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, Hasan et al.propose a 2D convolutional Auto-Encoder (Conv-AE) by stacking frames in channels to model regular frames. Such a 2D convolution, however, cannot characterize spatial and temporal information very well, as shown in activity recognition <ref type="bibr" target="#b23">[24]</ref>. In light of the capabilities of convolutional neural networks (ConvNets) to represent spatial features and the strong capabilities of recurrent neural networks (RNN) and long short term memory (LSTM) to model temporal patterns, <ref type="bibr">[25] [26]</ref> [2] make attempts to leverage a convolutional LSTM Auto-Encoder (ConvLSTM-AE) to characterize both appearance and motion information. Ryota et al. <ref type="bibr" target="#b26">[27]</ref> combine both detection and the recounting of abnormal events. Sabokrou et al. <ref type="bibr" target="#b27">[28]</ref> leverage a pretrained Fully Convolutional Neural Networks (FCNs) to training an unsupervised FCN for anomaly detection. Sultani et al. <ref type="bibr" target="#b28">[29]</ref> apply multiple instance learning (MIL) for anomaly detection, but in their setting both normal and abnormal videos are equally provided in the training set, and in many scenarios the acquisition of different types of abnormal data is very expensive and even infeasible.</p><p>Even though Auto-Encoder based methods have shown some good performance for anomaly detection, they may be prone to learn an identity mapping, and fail to detect the abnormal events. In order to prevent learning a trivial solution, a generative model can be utilized to model a normal distribution. In <ref type="bibr" target="#b29">[30]</ref>, Thomas et al.apply a GAN <ref type="bibr" target="#b30">[31]</ref> model to detect anomalies in medical images. More specifically, they first train a generator from the latent space to the image space, by fooling a discriminator. Once the generator and discriminator are trained using the training set only with normal data, all of their parameters are fixed. Further, for a query sample, they leverage gradient descent to search for the optimal latent variable to reconstruct the sample. Normal samples would cause a low reconstruction residual error while abnormal ones would cause a higher value. In addition, pixel errors between the query and reconstruction indicate lesions.</p><p>Although RNNs or LSTMs are powerful and effective for processing sequential data, they are actually "black boxes" whose internal structures are hard to interpret. Recently, Scott et al. <ref type="bibr" target="#b4">[5]</ref> show that a special type of RNN actually enforces a sparse constraint on features. Inspired by the work of sparse coding based anomaly detection and interpretable RNNs, we propose a TSC and its sRNN-AE counterpart for anomaly detection.</p><p>Methods without a Training Phase. Except for those methods mentioned above, there are also others methods without a training phase for anomaly detection. In <ref type="bibr" target="#b31">[32]</ref>, Giomo et al.propose the direct estimation of the discriminability of frames with references to the context in the test video, without any training set. In addition to that, a similar setting is adopted in <ref type="bibr" target="#b32">[33]</ref>, which trains a binary classifier to distinguish between two consecutive video sequences while removing the most discriminant features at each step. The higher training accuracy rates of the intermediately obtained classifiers represent abnormal events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head><p>In this section, we first revisit sparse coding based anomaly detection. To model the coherence between neighboring frames for normal events, Temporally-coherent Sparse Coding (TSC) is introduced, then we show that the optimization of TSC with the Sequential Iterative Soft-Thresholding Algorithm (SISTA) is equivalent to a special type of stacked Recurrent Neural Networks (sRNN). Further, to reduce the time cost in training and inference stage as well as alleviate the hyperparameter selection in TSC, we reduce the number of layers in sRNN and stack one more layer on top of sRNN to reconstruct the input, which arrives at an sRNN-AE. We also propose to learn similarity with a multilayer perceptron within the sRNN-AE framework, which further improves anomaly detection accuracy. Finally, we will show how to combine spatial and temporal features for real time anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Revisit of Sparse Coding Based Anomaly Detection</head><p>Sparse coding based anomaly detection aims to learn a dictionary to encode all normal events with small reconstruction errors <ref type="bibr">[4] [3]</ref>. Mathematically, we denote a feature corresponding to a normal input as x i , then it is desirable that x i can be linearly reconstructed by a dictionary A with a small reconstruction error i , i.e., x i = Aα i + i . Under the assumption that i ∼ N (0, σ 2 I), and α i ∼ Laplace(0, 2σ 2 /λ), we arrive at the following objective function:</p><formula xml:id="formula_0">min A,αi 1 2 x i -Aα i 2 2 + λ α i 1<label>(1)</label></formula><p>In this formulation, the first term corresponds to a reconstruction error, where it measures how well the feature can be reconstructed by the dictionary. The second term corresponds to a sparsity term while λ balances the sparsity and the reconstruction error. A larger λ corresponds to an even more sparse solution. To avoid trivial solutions to the problem, usually an L2 norm constraint is imposed on each column of A: A(:, j) ≤ 1. By alternatively optimizing the dictionary and the sparse coefficients on the training set <ref type="bibr" target="#b3">[4]</ref>, a dictionary can be learned that encodes all normal patterns. In the test phase, when a feature comes in, we first compute its sparse coefficients based on the dictionary A. Then, based on its reconstruction error, we can classify whether it belongs to normal or abnormal events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporally-coherent Sparse Coding (TSC) for Anomaly Detection</head><p>One advantage of sparse coding based anomaly detection is that it learns a dictionary to encode all normal events with small reconstruction errors, thus an abnormal event is associated with a large reconstruction error. It does not consider, however, the temporal coherence between neighboring frames within normal/abnormal events. Further, as shown in previous works <ref type="bibr" target="#b2">[3]</ref> [34], with sparse coding, similar features may be encoded as dissimilar sparse codes, i.e., locality information is lost. To preserve the similarity we propose a Temporally-coherent Sparse Coding (TSC) model. Specifically, if two neighboring frames are similar, it is desirable that their sparse coefficients are similar as well. To achieve this goal, we use the similarity between neighboring frames to weight the distance between their sparse coefficients. We denote the similarity between the t-th frame and (t -1)-th frame as S t-1,t , which can be either predefined or learned with a datadriven approach. Then we use S t-1,t to weight α t -α t-1</p><formula xml:id="formula_1">2 2</formula><p>and substitutes the temporally coherent constraint into the sparse coding objective function, which gives the objective function of TSC:</p><formula xml:id="formula_2">min A,αt T t=1 x t -Aα t 2 2 +λ 1 α t 1 + λ 2 S t,t-1 α t -α t-1 2 2 s.t. A(:, i) ≤ 1<label>(2)</label></formula><p>This objective 2 is not convex. Following the classical optimization strategy in sparse coding <ref type="bibr" target="#b34">[35]</ref> [36], we can alternatively update A and α t (t = {1, . . . , T }). Optimization of A. When all α t (t = {1, . . . , T }) are fixed, the objective function corresponding to A can be written as follows:</p><formula xml:id="formula_3">min A T t=1 x t -Aα t 2 2 s.t. A(:, i) ≤ 1 (3)</formula><p>Then, we use a projected gradient descent algorithm to optimize A.</p><p>Optimization of α t . When A is fixed, we arrive at the following objective function w.r.t. reconstruction coefficients of all features:</p><formula xml:id="formula_4">min αt T t=1 x t -Aα t 2 2 + λ 1 α t 1 + λ 2 S t,t-1 α t -α t-1 2 2 (4)</formula><p>After that, we update α t (t = {1, . . . , T }) with a Sequential Iterative Soft-Thresholding Algorithm(SISTA) <ref type="bibr" target="#b4">[5]</ref> whose main steps are algorithm 1. In this algorithm, soft b (x) = max(xb, 0) = ReLU(x -b), K corresponds to the steps of the ISTA algorithm. γ is a hyperparameter.</p><p>Algorithm 1 Sequential iterative soft-thresholding algorithm. Input: extracted feature x 1:T , hyper-parameter λ 1 , λ 2 , γ, initial α0 , the steps of ISTA K 1: for t = 1 to T do 2:</p><formula xml:id="formula_5">α0 t = α t-1<label>3</label></formula><p>:</p><formula xml:id="formula_6">for k = 1 to K do 4: z = [I -1 γ (A T A + S t-1,t λ 2 I)]α k-1 t + 1 γ A T x t 5: α(k) t = soft λ1/γ (z + St-1,tλ2 γ α t-1 ) 6:</formula><p>end for 7:</p><p>α t = αK t 8: end for 9: return α 1:T ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interpreting TSC with a Stacked RNN (sRNN)</head><p>A traditional RNN is based on the assumption that h t = f (x t , h t-1 ), which introduces a recurrent structure. Many previous work <ref type="bibr">[37] [38]</ref> shows that by stacking multiple RNNs on top of each other, the performance of classification or regression can be further boosted. We denote x t as an input at time t and denote h k t as an output of hidden nodes in the k-th layer at time t. σ b is the nonlinear activation function parameterized by b. In this paper, we choose σ b (x) = soft b (x). Mathematically, the stacked RNN (sRNN) can be written as follows:</p><formula xml:id="formula_7">h (k) t = σ b (W (1) h (1) t-1 + V x t ), k = 1, σ b (W (k) h (k) t-1 + U (k) h (k-1) t ), k &gt; 1.</formula><p>(5)</p><p>The first layer accepts the last moment output at the same layer h 1 t-1 and the current moment input x t as its inputs. Similarly, the rest of the stacked layers accept the last moment output h k t-1 at the same layer and the previous layer output h k-1 t at the same moment as their inputs.</p><p>It should be noticed that the sRNN mapped from TSC is slightly different from that of the formulation in <ref type="bibr" target="#b4">(5)</ref>. For the stacked layers, slightly different from Equation ( <ref type="formula">6</ref>), they also accept the current moment input x t as its inputs.</p><formula xml:id="formula_8">h (k) t = σ b (W (k) h K t-1 + U (k) h (k-1) t + V x t ), k &gt; 1. (6)</formula><p>By comparing the optimization procedure in Algorithm 1 with the stacked RNN, we can see that Equation ( <ref type="formula" target="#formula_2">2</ref>) can be interpreted with an sRNN: The K steps in the Sequential Iterative Soft-Thresholding Algorithm correspond to the number of layers in the sRNN. Comparing the proposed sRNN to classical RNN <ref type="bibr" target="#b36">[37]</ref>, the difference between them is that x t is fed into all sRNN layers in our sRNN, while the vanilla RNN only takes x t as its input in the first layer. Further, S t,t-1 takes x t and x t-1 as inputs, which means that h k t also depends on the input of the last moment x t-1 . Additionally, S t,t-1 is the input of each hidden state h k t . We illustrate the stacked RNN in our problem in Figure <ref type="figure">1</ref>.</p><p>More specifically, the mapping from the variables in TSC to the variables in sRNN in Equation ( <ref type="formula">5</ref>) is:</p><formula xml:id="formula_9">W (1) = I - λ 2 γ A T A<label>(7)</label></formula><formula xml:id="formula_10">W (k) = S t-1,t λ 2 γ I, k &gt; 1 U (k) = I - 1 γ (A T A + S t-1,t λ 2 I), k &gt; 1 V = 1 γ A T b = λ 1 /γ h (k) t = α k t</formula><p>To demonstrate the mapping, we copy line 4 and 5 in Algorithm 1 here and denote each replacement under each component in Equation <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_11">z = [I - 1 γ (A T A + S t-1,t λ 2 I)] U (k) αk-1 t h (k-1) t + 1 γ A T V x t (8) α(k) t = soft σ λ 1 /γ b (z + S t-1,t λ 2 γ W (k) α t-1 h K t-1</formula><p>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">sRNN Auto-Encoder</head><p>First, as shown in Figure <ref type="figure">7</ref>, TSC is sensitive to the weight of the sparsity term and is a temporally-coherent term. In addition to that, different datasets prefer different parameters. Therefore, it is desirable to derive a data-dependent way to automatically learn these parameters. Second, the training of TSC is done by the alternative optimization of the dictionary and the sparse coefficients, which is also time-consuming, while it is observable that dictionary learning is equivalent to learning the sRNN. Thirdly, if the number of layers in sRNN (K) is very high, our network is identical with TSC, which guarantees that all α t 's are sparse. A very deep sRNN, however, is very time-consuming in the inference stage. To tackle these problems, we first reduce the number of layers in sRNN. Then, we propose the training of the sRNN with an Auto-Encoder (sRNN-AE), i.e., we use the last layer output (h K t ) of the sRNN to reconstruct the input x t with the mapping function parameterized by Z, i.e., xt = Zh K t . We denote the parameters in the sRNN as θ = {A, λ 1 , λ 2 , Z, α 0 , γ}. Finally, we can simultaneously optimize all parameters including the dictionary, hyperparameters and reconstruction coefficients in the following way:</p><formula xml:id="formula_12">min θ T t=1 x t -Zh K t 2 F + β θ 2 F (9)</formula><p>To solve Equation ( <ref type="formula">9</ref>), we use a min-batch based Stochastic Gradient Descent (SGD) algorithm. Specifically, we use the RM-SPROP <ref type="bibr" target="#b38">[39]</ref> based SGD method, and set the weight for the weight decay term as β = 0.005. Further, a larger K will inevitably introduce a higher computational cost. Therefore, rather than using a very large K, we use a small one (K=3). As shown in the experiments section, such a shallow architecture achieves much better performance than all other existing methods. Our sRNN has two advantages: i) we can learn all of the parameters in the sRNN rather than choosing the hyperparameters in TSC; ii) the architecture of our sRNN is not deep. In the test phase, we can get α t = h K t in one forward pass, which greatly accelerates anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Similarity Measurement</head><p>The similarity measurement is a key factor for the performance of TSC and sRNN-AE. One simple way to obtain it is to directly define the similarity between neighboring frames with some commonly used functions, such as the Gaussian function, which is defined as follows:<ref type="foot" target="#foot_0">2</ref> </p><formula xml:id="formula_13">S t-1,t = exp(- x t -x t-1 2 2 δ 2 ) (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>It is desirable, however, to learn a data-driven similarity measurement for different data.</p><p>Inspired by the kernel trick of SVM <ref type="bibr" target="#b5">[6]</ref>, we can define the similarity as follows:</p><formula xml:id="formula_15">S t-1,t = κ(x t , x t-1 ) = φ(x t ) T φ(x t-1 )<label>(11)</label></formula><p>Here κ(•, •) is a kernel function, and φ(•) is some mapping function, usually unknown. In this paper, we leverage a datadriven approach to learn the mapping function φ(•) within sRNN-AE. Specifically, we leverage a multi-layer perceptron with the ReLU activation function as the φ(•) function. We denote the parameters of φ(•) as w φ and put it into the trainable parameter set of sRNN-AE. In this way, the mapping function φ(•) can be automatically learned with other parameters in sRNN-AE in an end-to-end learning manner. We also normalize the output of the multi-layer perceptron with an l 2 normalization to make the length of the output be 1. In this way, the similarity S t-1,t ∈ [0, 1]. In the following sections, without specification, the similarity in sRNN-AE is trained in this way, as shown in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">The Combination of Spatial and Temporal Features for Anomaly Detection</head><p>Both TSc and sRNN-AE are actually used to model the distribution over normal patterns and discover abnormal ones. Hence, they function as classifiers. In addition, their performance is feature dependent. Since anomalies can be caused by unseen or unexpected objects or unusual motion patterns, it is desirable to combine both spatial and temporal features for anomaly detection. In action recognition, the prevalent method is to use two-stream CNNs <ref type="bibr" target="#b6">[7]</ref> [40] <ref type="bibr" target="#b40">[41]</ref> for explicitly characterizing appearance and motion information, respectively. These works have shown the effectiveness of the two-streams solution over 2D convolution by stacking frames in channels for feature extraction. Therefore, we also propose to extract the spatial and temporal features separately. For appearance features, we use a spatial ConvNet (ResNet) pretrained with the UCF101 dataset <ref type="bibr" target="#b41">[42]</ref> to extract appearance features. For motion features, one way is to feed the optical flow features to a temporal ConvNet. However, the extraction of optical flow is time-consuming, which is not desirable. Following the work of <ref type="bibr" target="#b7">[8]</ref>, we propose to conduct pooling over the appearance features of several consecutive frames and use this as temporal features. But a bit different from <ref type="bibr" target="#b7">[8]</ref> where max pooling, sum pooling, histogram of time series gradients pooling are all used. In our experiments, we find that max pooling already corresponds to good performance. Thus we only use max pooling. Further, we use multiple patches at multiple scales for appearance representation, and do the max pooling for features corresponding to patches at different scales. Thus, these pooled features over patches at different scales encode the spatial change of some objects (spatial features) over time, which also gathers information temporally. As shown in Table <ref type="table" target="#tab_7">7</ref>, our solution greatly accelerates anomaly detection, and also improves accuracy.</p><p>Sampling multiple patches at multiple scales has been shown to be a very effective way for improving anomaly detection <ref type="bibr" target="#b2">[3]</ref>. We also use the same strategy on videos-based anomaly detection. Specifically, for both spatial and temporal features, we gradually partition the feature map over spatial dimensions into increasingly finer regions: 1 × 1, 2 × 2, and 4 × 4. We use max pooling over each sub-region. Thus the feature dimension of all sub-regions are the same. Rather than learning multiple dictionaries for features at different scales <ref type="bibr" target="#b2">[3]</ref>, which brings additional computational costs, features at all scales share the same dictionary in our method. For features at multiple scales, we only enforce a temporal coherent constraint for features at the same scale and spatial location.</p><p>After extracting spatial and temporal features, there are two possible ways to combine them. One way is to directly stack the spatial and temporal features at the same moment and feed them to one sRNN-AE, which is referred to as early fusion. Another way is to feed them to separate sRNN-AE algorithms and combine the outputs of each sRNN-AE for anomaly detection, which is referred to as late fusion. Previous work <ref type="bibr" target="#b42">[43]</ref> has shown that late fusion achieves a better performance for video classification, and our experiments also demonstrate a similar phenomenon for anomaly detection, as shown in Table <ref type="table">9</ref>.</p><p>The whole pipeline of our proposed anomaly detection system is demonstrated in Figure <ref type="figure" target="#fig_1">3</ref>. For an input video, we sample 4 continuous frames with an interval of 1. Then, a pretrained ResNet is used to extract spatial and temporal features. We adopt features extracted from different regions and conduct spatial pyramid pooling <ref type="bibr" target="#b43">[44]</ref> over them, achieving 21 feature vectors for each frame. Further, we use temporal pooling over these 4 frames for temporal information summarization, where temporal pooling is an element-wise maximum operation. Further, spatial and temporal features go through two separate sRNN-AE algorithms to achieve 21 reconstruction errors for each modality. For each modality, we take the maximum reconstruction error of the 21 patches. Finally, two reconstruction errors are normalized to scores and linearly combined with a weight, as shown in Section 3.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Anomaly Detection on Testing Data</head><p>In the training phase, we can learn the dictionary A which encodes normal events well. In the test phase, we feed the feature of each patch corresponding to the t-th frame into our special sRNN. With one forward pass, we can get α t . We denote the feature of the i-th patch in the t-th frame as x t,i , where super-script s and t correspond to spatial and temporal, respectively, then we can calculate the reconstruction error corresponding to the i-th patch in the t-th frame as follows:</p><formula xml:id="formula_16">l s (t, i) = x s t,i -A s α s t,i 2 2 l t (t, i) = x t t,i -A t α t t,i<label>2 2 (12)</label></formula><p>As for sRNN-AE model, the reconstruction error can be measured as follows</p><formula xml:id="formula_17">l s (t, i) = x s t,i -xs t,i 2 2 l t (t, i) = x t t,i -xt t,i<label>2 2 (13)</label></formula><p>Next, we pick the maximum reconstruction error among all patches within this frame as the frame level reconstruction error, i.e., l(t) = max i l(t, i). The reason for using the maximum reconstruction error as a measurement of anomaly is that it is robust to the small changes in spatial and temporal directions. In other words, if the patch corresponding to the maximum reconstruction error is normal, then the frame should be a normal event. Otherwise, if the patch with the maximum reconstruction error is abnormal, then the frame is highly likely to be abnormal. Further, following the work of <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b24">[25]</ref>, and after calculating all frame level reconstruction errors for a testing video, we normalize the errors to the range [0, 1] and calculate a regularity score for each frame based on the following equations:</p><formula xml:id="formula_18">s s (t) = 1 - l s (t) -min k:1...T l s (k) max k:1...T l s (k) -min k:1...T l s (k) s t (t) = 1 - l t (t) -min k:1...T l t (k) max k:1...T l t (k) -min k:1...T l t (k)<label>(14)</label></formula><p>where T means the length of a video. A smaller s(t) means that the t-th frame more likely corresponds to an abnormal event. Finally, we combine spatial and temporal scores with a weight β that refers to the final score of a frame t is s(t) = s a (t)+βs m (t), where β ∈ [0, 1]. This is because anomalies can usually be easily discovered by appearance changes. Further, it is hard to properly characterize motion features compared to appearance features. Thus spatial anomaly detection is more robust than temporal anomaly detection, as shown in Table <ref type="table" target="#tab_8">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In Section 4.1, we first introduce measurements used in all experiments. We empirically evaluate our proposed method under a controlled setting on a synthesized dataset in Section 4.2. Then, we compare our methods with other state-of-the-art methods on real anomaly detection datasets as well as our new ShanghaiTech  anomaly dataset in Section 4.3. Different parameters in TSC and sRNN-AE are also empirically evaluated in Section 4.4. In addition, in order to describe the effectiveness of our proposed sRNN-AE, compared with other variant RNN-AE formulations, we conduct some experiments in Section 4.5. Two options for the similarity definition between neighboring codes will be discussed in Section 4.6. Different combinations of spatial and temporal streams will be discussed in Section 4.7. Finally, the running time will be reported as well in Section 4.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Measurements. We can predict whether an abnormal event occurs based on s(t). One can set a threshold and if the score of a frame is smaller than the threshold, the frame can be categorized as an abnormal case. Obviously a higher threshold may cause a higher false negative ratio, while a lower one may lead to more false alarms. By changing the threshold gradually, we can arrive at an ROC curve. The Area Under the Curve (AUC) is a commonly used measurement for detecting irregularity <ref type="bibr" target="#b33">[34]</ref>. In this paper, we use frame-level AUC to evaluate the performance of different methods. Implementation Details. In our implementation, the learning rate for sRNN-AE is 0.00001. Many stacked RNNs including LSTMs illustrated in Equation ( <ref type="formula">5</ref>) contain different trainable parameters such as W, U, V . However, our proposed method interpreting TSC with a stacked RNN finally result in only one trainable parameter A, which means all gradients will be accumulated into A. As shown in Fig. <ref type="figure">1</ref>, if the number of blocks contributing to the calculation of the gradient of a trainable parameter in a vanilla stacked RNN is T , the number of blocks contributing to the calculation of the gradient of A in TSC counterpart sRNN is T × K × 2, where T is time steps, K is the number of stacked layers and 2 means that each cell accept the current moment input x t as input. In our experiments, K can be larger than 10. Thus, we use a small learning rate for all ablation studies. The training sequence length is 10. The batch size in the training phase is 4. The dimension of the fully-connected layer in the trainable  similarity measurement is 512. We fix the number of iterations of sRNN-AE to 20,000 for all datasets. In the training phase, we leverage a ResNet pretrained on UCF101 for feature extraction <ref type="bibr" target="#b41">[42]</ref>. Then we fix the pretrained ResNet in the feature extraction module and use a RMSPROP based SGD method to train the anomaly detection module for sRNN-AE. Specifically, for the spatial ResNet for appearance feature extraction, its architecture is the same with that in <ref type="bibr" target="#b39">[40]</ref>, then a pooling over the appearance features is conducted to summarizing information temporally.</p><p>For the anomaly detection, we train the TSC with the (2) and normalize each column of A to be 1 to avoid the trivial solution in each iteration. We train the sRNN-AE with the Equation <ref type="formula">9</ref>. It is worth noting that we optimize A, α, λ 1 and λ 2 in sRNN-AE rather than U , V and W because different from vanilla sRNN, in our sRNN-AE, U , V , and W depend on A, α, λ 1 , and λ 2 , as shown in Equation <ref type="formula" target="#formula_9">7</ref>. In other words, the vanilla sRNN cannot characterize the dependencies between different layers. After training Spatial and Temporal sRNN-AEs, we add the normal scores of these two streams together. The weights corresponding to the spatial and temporal scores are fixed to be 1 and 0.5, respectively on all datasets. The whole pipeline is implemented with the Tensorflow framework <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluate with A Synthesized Dataset</head><p>Anomaly in Appearance. To evaluate the performance of our method for the anomalies caused by a sudden change in appearance, we deploy experiments on a synthesized Moving-MNIST dataset. Specifically, we randomly choose two digits from the MNIST dataset, and put them in the center of a black image whose size is 225×225 pixels. Then in the next 19 frames, the digits randomly move horizontally or vertically. In this way, we can get a sequence with 20 frames. In our experiments, we synthesize 10,000 sequences for training data and train the network. For each testing sequence, 5 consecutive frames are randomly occluded by randomly inserting a 3×3 white box. We generate 3,000 sequences in total as test data. Then we use the intensity of the images as features and normalized them with an l 2 normalization. Anomaly in Motion. We also evaluate the performance of our methods for the anomalies caused by a sudden change in motion. Specifically, we randomly choose two digits from the MNIST dataset, and put them in the center of a black image whose size is 225×225 pixels. In the first 10 frames, these two digits move together in a straight direction. After that, in each one of the next 10 frames, they move separately in two random directions. We can then also get a sequence with 20 frames. We generate the same amount as for anomalies in appearance.</p><p>The performance of the different methods are shown in Table 1. We can see that both TSC and sRNN-AE outperform Conv-AE when the anomalies are caused by either motion or appearance. Further, sRNN-AE outperforms TSC by around 2% and 3% for the anomalies caused by appearance and motion, respectively. We also show a sample with an anomaly caused by appearance in Figure <ref type="figure" target="#fig_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation with Real Anomaly Datasets</head><p>We also evaluate our TSC and sRNN-AE with real anomaly detection datasets. It is desirable that the trained anomaly detection model can be directly applied in multiple scenes with multiple viewing angles. Most of existing datasets, however, only contain videos captured with one fixed angle camera, and they lack diversity of scenes and viewing angles. To increase scene diversity, we build a new anomaly detection dataset, which is named the ShanghaiTech Campus dataset. To the best of our knowledge, it is the biggest dataset for anomaly detection, which is even bigger than the sum of all existing datasets except for the LV in terms of the volume of data and the diversity of scenes. Further we introduce more anomalies caused by sudden motion in this dataset, such as chasing and brawling, which are not included in existing datasets. These characteristics make our dataset more suitable for real scenarios. We show some samples of our dataset in Figure <ref type="figure" target="#fig_3">5</ref> and list some statistics of different datasets in Table <ref type="table" target="#tab_1">2</ref>.</p><p>Specifically, we conduct experiments on our new proposed dataset as well as the two recently most used datasets, including ShanghaiTech Campus, CUHK Avenue <ref type="bibr" target="#b2">[3]</ref>, UCSD Ped2 <ref type="bibr" target="#b33">[34]</ref>, Subway <ref type="bibr" target="#b13">[14]</ref> and LV <ref type="bibr">[50]</ref>.It is worth noting that for the UCSD pedestrian datasets, Ped1 is more frequently used for pixelwise anomaly detection <ref type="bibr" target="#b22">[23]</ref>, while our work focuses on framelevel prediction, so we only conduct experiments on Ped2. For the Entrance dataset, ConvLSTM-AE <ref type="bibr" target="#b1">[2]</ref> removes timestamps embedded in videos because the timestamps in videos usually leads to large reconstruction errors and hurts reconstruction based methods. But timestamps may appear at different places of videos, and it is trivial to remove it. Therefore, we rerun the ConvLSTM-AE without removing timestamps for fair comparison with our method. To better understand the differences between our dataset and existing anomaly detection datasets, we briefly summarize all anomaly detection datasets as follows:</p><p>• The CUHK Avenue <ref type="bibr" target="#b2">[3]</ref> dataset contains 16 training videos and 21 testing videos with a total of 47 abnormal events, including throwing objects, loitering and running. The apparent size of people may change because of the camera position and angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The UCSD Pedestrian 2 (Ped2) <ref type="bibr" target="#b33">[34]</ref> dataset contains 16 training videos and 12 testing videos with 12 abnormal events. All of these abnormal cases are about vehicles such as bicycles and cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The Subway <ref type="bibr" target="#b13">[14]</ref> dataset is 2 hours long in total. There are two categories, i.e.Entrance and Exit. Unusual events contain walking in wrong directions and loitering. More importantly, this dataset was recorded in an indoor environment while the above ones were recorded in an outdoor environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The LV dataset [50] is a challenging dataset, where all videos are collected online and abnormal events are realistic. Following the setting of [50], an abnormal frame is labelled as a true positive when at least 20% of abnormal regions of a frame is correctly detected, otherwise it is a false positive.</p><p>• Our ShanghaiTech Campus dataset has 13 scenes with complex lighting conditions and camera angles. It contains 130 abnormal events and over 270, 000 training frames. Moreover, the pixel level ground truth of abnormal events is also annotated in our dataset.</p><p>Baselines. Besides comparing our method with other state-ofthe-art anomaly detection methods, including Conv-AE <ref type="bibr" target="#b0">[1]</ref>, Del et al. <ref type="bibr" target="#b31">[32]</ref>, Unmasking <ref type="bibr" target="#b32">[33]</ref> and Hinami et al. <ref type="bibr" target="#b26">[27]</ref>, we further design another two baselines to evaluate how well does the proposed feature extraction module do directly on the anomaly detection module without the sRNN-AE counterpart. Specifically, on all datasets, we firstly extract appearance feature with dimensionality of 2048 for each frame, then give a testing frame, we calculate its similarity/distance to the training/normal frames for anomaly detection. Since there are too many training frames, and it is very time consuming to do the frame-wise comparison between each testing and training pair, and it is also very time consuming to do the sorting. To reduce the computational complexity, we propose two solutions: i) Nearest Subspace: we use K-means to cluster training data into a dictionary A with size of 1000×2048, where 1000 is the dictionary size and 2048 is the dimensionality of feature. In the testing phase, we calculate the distance between testing frame and training with Nearest Subspace distance: min α y -Aα 2 for each testing frame feature y, where α is a coefficient of linear combination, and the optimal α * = (A T A) -1 A T y. After that, the reconstruction errors are normalized as normal scores. ii) OC-SVM: we train a one-class SVM with all training data for anomaly detection.</p><p>We list the performance of different methods on these datasets in Table <ref type="table" target="#tab_2">3</ref> and<ref type="table">Table 4</ref>. It clearly shows that both our methods outperform all existing methods, including Conv-AE <ref type="bibr" target="#b0">[1]</ref>, Del et al. <ref type="bibr" target="#b31">[32]</ref>, Unmasking <ref type="bibr" target="#b32">[33]</ref> and Hinami et al. <ref type="bibr" target="#b26">[27]</ref>, which are stateof-the-art methods for anomaly detection. Further, we can see that the extracted features are discriminative for anomaly detection, but both Nearest Subspace and one-class SVM based classifier is not as as good as our sRNN-AE and TSC on all datasets. Specifically, since our dataset contains multiple scenes which makes our dataset more realistic and challenging, the performance on our dataset is not as good as that on Avenue, Ped2, Entrance and Exit. Further, on all datasets, our sRNN-AE outperforms TSC, which validates the effectiveness of sRNN-AE. The reasons contributing to the improvement of sRNN-AE are two-fold: i) sRNN-AE can automatically learn the weights of the sparsity term and the temporally-coherent term. ii) the trainable similarity module leans a data dependent similarity, which is better than predefined similarities. The results in Table <ref type="table">4</ref> also show that our Finally, we show the change of the score (s(t)), the similarities between neighboring frames (S t,t-1 ) and the distances between sparse codes of neighboring frames ( α t -α t-1 ) for some normal and abnormal events on the Ped2 and ShanghaiTech datasets in Figure <ref type="figure">6</ref>. We can see that some smooth similarities and distances can be found for the frames within normal or abnormal events, which agrees with the motivation of our TSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Effect of Different Hyper-Parameters in TSC and sRNN-AE</head><p>In this subsection, we conduct some experiments on the effect of different hyperparameters in TSC and sRNN-AE. Since dictionary training and coefficient optimization is very time-consuming, experiments conducted in this subsection are only based on appearance features.</p><p>Weight of the Sparsity Term(λ 1 ) in TSC. λ 1 in Equation ( <ref type="formula" target="#formula_2">2</ref>) controls the sparsity of α t . As shown in Algorithm 1, α k t is optimized based on a soft-thresholding operator. The bigger λ 1 is, the more sparse α t will be. We fix λ 2 and the dictionary size to 2.0 and 2048 × 2048, respectively, and change λ 1 to observe how this parameter affects the AUC on Ped2, Avenue and ShanghaiTech. As shown in Figure <ref type="figure">7</ref>(a), a bigger λ 1 improves the AUC on Avenue but reduces the performance for the Ped2 and ShanghaiTech datasets.</p><p>Weight of the Temporally-coherent Term (λ 2 ) in TSC. λ 2 in Equation (2) controls the smoothness of the sparse codes between neighboring frames. Figure <ref type="figure">7</ref>(b) demonstrates that different datasets may be affected differently by λ 2 . For example, Ped2 and Avenue prefers a larger λ 2 but ShanghaiTech prefers a smaller λ 2 .</p><p>Dictionary Size. We show the change of TSC performance with respect to the change of dictionary size on the Avenue dataset in Figure <ref type="figure">7</ref>(c). We can see that a larger dictionary does not always improve AUC and that the optimal dictionary size varies for different datasets. In addition, we report the performance of different dictionary sizes in sRNN-AE. We can see that sRNN-AE always outperforms TSC when dictionary size varies. For saving both training and testing time, we set dictionary size to 2048 for all datasets.</p><p>Number of Layers in sRNN-AE. The optimization of the SISTA algorithm requires a very large K to achieve a sparse solution with a small reconstruction error. Fewer iterative steps may harm the optimization of TSC. A larger K means a deeper sRNN, as the counterpart of TSC.</p><p>However, a very deep sRNN-AE may lead to gradient vanishing or explosion, which is harder to optimize. To validate how K affects the performance of our sRNN-AE, we set it to different values <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30)</ref>, respectively. The sparsity and AUC of sRNN-AE with different numbers of layers on Avenue are shown in Figure <ref type="figure">8</ref>. The sparsity (percentage of zero entries) of 3-layers-based sRNN-AE and that of 30-layers-based sRNN-AE is 80.0% and 90.0%, respectively, while the AUC for 3-layersbased sRNN-AE and 30-layers-based sRNN-AE is 83.48% and 79.19%, respectively. This experiment shows that sparsity does not necessarily lead to better performance in sRNN-AE. In our experiments, we set K = 3 for all datasets. Such a shallow architecture also accelerates the inference of α t (h t ) in the test phase. We also show the change of the objective with respect to iteration in Figure <ref type="figure">9</ref>. We can see that our sRNN-AE converges at around 10,000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison between sRNN-AE and Other Types of RNN</head><p>Our sRNN-AE is a special type of Recurrent Neural Networks (RNN) based Auto-Encoder. To verify the effectiveness of such a sparse coding inspired sRNN, we also compare our sRNN-AE with the LSTM based Auto-Encoder, where the same features are used, and the ConvLSTM based Auto-Encoder which extracts features from raw pixels. The AUC of these methods on Avenue, Ped2 and ShanghaiTech is listed in Table <ref type="table" target="#tab_3">5</ref>. We can see that our sRNN-AE also outperforms these two baselines. In addition, our sRNN-AE can be well interpreted compared to other types of RNN based Auto-Encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#Frame</head><p>Normality Anomaly Fig. <ref type="figure">6</ref>. Scores, similarities and distances between neighboring codes of two video samples on the Ped2 and ShanghaiTech. We can see that the similarities between neighboring frames can be kept for normal events. We highlight the abnormal events with red boxes. (Best viewed in color) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Similarity Measurement</head><p>To verify the effectiveness of the similarity learning module in sRNN-AE, we also use a Gaussian kernel based similarity measurement and fix λ 2 in sRNN-AE and learn other parameters, including λ 1 . The results of the Gaussian kernel based and trainable similarity measurement are shown in Table <ref type="table" target="#tab_5">6</ref>. We can see that the trainable similarity measurement achieves a better performance than the predefined similarity method, which verifies the importance of trainable data dependent similarity. We also show a pair of normal and abnormal images in Figure <ref type="figure" target="#fig_6">10</ref>. We can  see that our trainable similarity better characterizes similarities for both normal and abnormal image pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">The Combination of Spatial and Temporal Features</head><p>Optical Flow Based Motion Features vs. Our Temporal Aggregated Features. In our implementation, we use a ResNet <ref type="bibr">[51]</ref> for appearance feature extraction and use temporal pooling over the appearance features of 4 consecutive frames as temporal features. We also compare our temporal features with optical flow based ones, as done in two-streams CNNs for action recognition.</p><p>For optical flow based motion features, we also use a ConvNet pretrained with the UCF101 dataset for motion feature extraction. The performance based on these two types of motion features is shown in Table <ref type="table" target="#tab_7">7</ref>. We can see that our temporal features is more effective than optical flow based ones. The possible reason for   this is that some types of anomalies are caused by a very fast movement, while optical flow estimation for very fast movement is not easy and usually inaccurate. As a result, the performance of optical flow based motion estimation is reduced. Further, our temporal feature extraction strategy is much faster than optical flow based motion feature extraction, which guarantees real-time anomaly detection.</p><formula xml:id="formula_19">• -• • • • = 50 • • -• • • • = 55 !"##$!% #$&amp;$'!<label>(</label></formula><p>Early Fusion vs. Late Fusion We also list the performance of anomaly detection based early fusion and late fusion in Table <ref type="table">9</ref>. We can see that late fusion always outperforms early fusion, which agrees with the findings for action recognition <ref type="bibr" target="#b42">[43]</ref>. There are two possible reasons for this. First, when the number of nodes are the same in hidden layers for both modalities, two separate sRNN-AE algorithms reduces the number of parameters by a half compared with early fusion. Thus, late fusion facilitates the training of a more robust sRNN-AE. Second, late fusion is more plausible for anomaly detection because humans infer anomalies either by appearance or motion, thus it may be more suitable to combine the spatial and temporal anomaly scores at the final stage during anomaly detection. On the one hand, for early fusion, the nodes in hidden layers may receive signals from both spatial and temporal directions. On the other hand, late fusion enforces that the hidden nodes only receive signals from one type of feature, making judgments merely based on one type of feature.</p><p>The Combination of Spatial Normal Scores and Temporal Normal Scores. Appearance is a strong cue for anomaly detection (for example, if unexpected objects appear, then we have confidence to say such a phenomenon is abnormal.). In addition, CNN has shown its expertise for appearance feature extraction, thus many existing work only leverages appearance for anomaly detection <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>. Temporal features representation is not as good as spatial features. Therefore, spatial normal scores are more reliable than those temporal normal scores, as shown in Table <ref type="table" target="#tab_8">8</ref>. Thus it is more reasonable to combine normal scores inferred by spatial features and temporal features with a weight. Here we show the change of the AUC with respect to β on Avenue and Ped2 in Figure <ref type="figure">11</ref> where we can see that β = 0.5 corresponds to a higher AUC. Thus we simply fix β = 0.5 on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Running Time</head><p>We report training and inference time of Avenue in Table <ref type="table" target="#tab_9">10</ref>. It is obvious that sRNN-AE with 3 layers is much faster than TSC with 30 layers. This means our proposed sRNN-AE effective and efficient for anomaly detection. Further, if the time for feature extraction is included, our sRNN-AE can run at a speed of 10 FPS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a TSC framework for anomaly detection which preserves the similarities between frames within normal and abnormal events. Our TSC can be interpreted with a special sRNN. By optimizing all parameters in sRNN-AE simultaneously, we avoid nontrivial parameter selection and reduce the computational cost for inferring the reconstruction coefficients in the test phase. Further, we propose a multi-layer perceptron based similarity measurement in sRNN-AE which learns a data dependent similarity, which demonstrates better performance than predefined similarity measurement. In addition, we propose combining the spatial and temporal features in a late fusion manner which further improves performance. Considering the fact that most anomaly detection datasets only contain one scene with the same view angle, we build a new dataset which is the most challenging one in terms of data volume and scene diversity. Extensive experiments on both synthesized datasets and real datasets validate the effectiveness of sRNN-AE for anomaly detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. The blue boxes represent the input xt. The green boxes represent hidden states h k t or coding vectors α k t . The orange circles are similarities between neighboring frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The whole pipeline of our proposed anomaly detection. It consists of a feature extraction module and an anomaly detection module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A sample with an anomaly caused by appearance on the Moving-MNIST dataset.</figDesc><graphic coords="7,298.31,19.82,302.80,170.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Some samples from our new proposed dataset and other datasets. The first tow rows represent some samples from the UCSD Ped1, UCSD Ped2, CUHK Avenue and Subway Entrance and Subway Exit datasets, respectively. The last two rows represent normal and abnormal scenes from our proposed dataset (ShanghaiTech Campus).</figDesc><graphic coords="8,154.90,255.58,98.53,55.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. AUC and sparsity of different numbers of layers in SRNN-AE on the Avenue and the ShanghaiTech dataset. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The learned similarity with two different strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>AUC on Moving-MNIST dataset.</figDesc><table><row><cell></cell><cell>Conv-AE</cell><cell>TSC</cell><cell>sRNN-AE</cell></row><row><cell>Spatial Anomaly Temporal Anomaly</cell><cell>74.30% 60.02%</cell><cell>88.19% 65.47%</cell><cell>90.11% 68.51%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Comparision of our dataset with other released datasets.</figDesc><table><row><cell>Dataset</cell><cell>Total</cell><cell>Training</cell><cell>#Frames Testing</cell><cell>Regularity</cell><cell>Irregularity</cell><cell>#Abnormal Events</cell><cell>#Scenes</cell></row><row><cell>Our Dataset</cell><cell>317,398</cell><cell>274,515</cell><cell>42,883</cell><cell>300,308</cell><cell>17,090</cell><cell>130</cell><cell>13</cell></row><row><cell>CUHK Avenue</cell><cell>30,652</cell><cell>15,328</cell><cell>15,324</cell><cell>26,832</cell><cell>3,820</cell><cell>47</cell><cell>1</cell></row><row><cell>UCSD Ped2</cell><cell>4,560</cell><cell>2,550</cell><cell>2,010</cell><cell>2,924</cell><cell>1,636</cell><cell>12</cell><cell>1</cell></row><row><cell>UCSD Ped1</cell><cell>14,000</cell><cell>6,800</cell><cell>7,200</cell><cell>9,995</cell><cell>4,005</cell><cell>40</cell><cell>1</cell></row><row><cell>Subway Entrance</cell><cell>136,524</cell><cell>20,000</cell><cell>116,524</cell><cell>134,124</cell><cell>2,400</cell><cell>66</cell><cell>1</cell></row><row><cell>Subway Exit</cell><cell>72,401</cell><cell>7,500</cell><cell>64,901</cell><cell>71,681</cell><cell>720</cell><cell>19</cell><cell>1</cell></row><row><cell>LV</cell><cell>309,940</cell><cell>127,500</cell><cell>182,440</cell><cell></cell><cell>68,989</cell><cell>34</cell><cell>30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>AUC of different methods on the Avenue, Ped2, Entrance, Exit and our dataset (ShanghaiTech Campus).</figDesc><table><row><cell></cell><cell></cell><cell>Avenue</cell><cell>Ped2</cell><cell>Entrance</cell><cell>Exit</cell><cell cols="2">Our dataset</cell></row><row><cell></cell><cell>MPPCA [34]</cell><cell>N/A</cell><cell>69.30%</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>MPPC+SFA [34]</cell><cell>N/A</cell><cell>61.30%</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>HOFME [46]</cell><cell>N/A</cell><cell>87.50%</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>Conv-AE [1]</cell><cell>74.50%</cell><cell>81.10%</cell><cell>91.00%</cell><cell>80.20%</cell><cell>60.85%</cell><cell></cell></row><row><cell></cell><cell>Del et al.. [32]</cell><cell>78.30%</cell><cell>N/A</cell><cell>69.10%</cell><cell>82.40%</cell><cell>N/A</cell><cell></cell></row><row><cell cols="2">ConvLSTM-AE [2]</cell><cell>77.00%</cell><cell>88.10%</cell><cell>84.30%</cell><cell>87.7%</cell><cell>55.00%</cell><cell></cell></row><row><cell></cell><cell>Unmasking [33]</cell><cell>80.60%</cell><cell>82.20%</cell><cell>71.30%</cell><cell>86.30%</cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>Hinami et al. [27]</cell><cell>N/A</cell><cell>92.20%</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>Nearest Subspace</cell><cell>76.67%</cell><cell>83.87%</cell><cell>77.04%</cell><cell>83.34%</cell><cell>65.33%</cell><cell></cell></row><row><cell></cell><cell>OC-SVM</cell><cell>78.23%</cell><cell>78.58%</cell><cell>78.34%</cell><cell>81.56%</cell><cell>53.11%</cell><cell></cell></row><row><cell></cell><cell>TSC</cell><cell>80.56%</cell><cell>91.03%</cell><cell>84.24%</cell><cell>87.54%</cell><cell>67.94%</cell><cell></cell></row><row><cell></cell><cell>sRNN-AE</cell><cell>83.48%</cell><cell>92.21%</cell><cell>85.38%</cell><cell>89.73%</cell><cell>69.63%</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">AUC on the LV dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Luet al. [3]</cell><cell>Biswas et al. [47]</cell><cell>Reddyet al. [48]</cell><cell>Javan et al. [49]</cell><cell>Conv-AE [1]</cell><cell>ConvLSTM-AE [2]</cell><cell>TSC</cell><cell>SRNN-AE</cell></row><row><cell>11.20%</cell><cell>15.10%</cell><cell>32.50%</cell><cell>42.70%</cell><cell>33.64%</cell><cell>39.41%</cell><cell>55.34%</cell><cell>58.27%</cell></row><row><cell cols="4">sRNN-AE achieves AUC of 58.27% which is much better than</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">the state-of-the-art one of 42.7%, which further demonstrates the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">effectiveness of our proposed method.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 5</head><label>5</label><figDesc>AUC of different RNN variants on Avenue, Ped2 and ShanghaiTech datasets.</figDesc><table><row><cell></cell><cell>Avenue</cell><cell>Ped2</cell><cell>ShanghaiTech</cell></row><row><cell>LSTM-AE</cell><cell>75.33%</cell><cell>83.62%</cell><cell>53.30%</cell></row><row><cell>ConvLSTM-AE</cell><cell>77.00%</cell><cell>88.10%</cell><cell>55.00%</cell></row><row><cell>sRNN-AE</cell><cell>83.48%</cell><cell>92.21%</cell><cell>69.63%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Fig. 7. The change of AUC w.r.t. λ 1 , λ 2 and dictionary size . (a) and (b) are conducted on Avenue and Ped2 datasets. (c) is conducted on Avenue. (Best viewed in color)</figDesc><table><row><cell></cell><cell>0.95</cell><cell>Ped2</cell><cell>Avenue</cell><cell cols="3">ShanghaiTech</cell><cell>0.95</cell><cell>Ped2</cell><cell>Avenue</cell><cell cols="2">ShanghaiTech</cell><cell>0.83</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.82</cell></row><row><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.81</cell></row><row><cell>AUC</cell><cell>0.75 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AUC</cell><cell>0.75 0.8</cell><cell></cell><cell></cell><cell></cell><cell>AUC</cell><cell>0.8</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.79</cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.78</cell><cell>sRNN-AE</cell><cell>TSC</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.77</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>1024</cell><cell>2048</cell><cell>3072</cell><cell>4096</cell><cell>5120</cell></row><row><cell></cell><cell></cell><cell></cell><cell>λ1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>λ2</cell><cell></cell><cell></cell><cell></cell><cell>Dictionary Size</cell></row><row><cell></cell><cell></cell><cell cols="3">(a) AUC versus λ 1 (λ 2 =2.0)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) AUC versus λ 2 (λ 1 =0.2)</cell><cell cols="3">(c) AUC versus dictionary size (λ 1 =0.2, λ 2 =2.0)</cell></row><row><cell></cell><cell></cell><cell>0.95</cell><cell cols="2">Sparsity on Avenue</cell><cell></cell><cell cols="2">AUC on Avenue</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Sparsity on ShanghaiTech</cell><cell cols="3">AUC on ShanghaiTech</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>AUC / Sparsity</cell><cell>0.75 0.8 0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>AUC of different similarity measurement on Avenue, Ped2 and ShanghaiTech datasets.</figDesc><table><row><cell></cell><cell>Avenue</cell><cell>Ped2</cell><cell>ShanghaiTech</cell></row><row><cell>Gaussian kernel</cell><cell>81.71%</cell><cell>91.03%</cell><cell>68.00%</cell></row><row><cell>Trainable similarity</cell><cell>82.58%</cell><cell>91.20%</cell><cell>69.63%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>AUC and average inference time of different temporal features on three datasets.</figDesc><table><row><cell></cell><cell>Optical flow</cell><cell>Appearance aggregation</cell><cell>Appearance+Optical flow</cell><cell>Appearance+Appearance</cell></row><row><cell></cell><cell>based only</cell><cell>based only</cell><cell>based</cell><cell>aggregation based</cell></row><row><cell>Avenue</cell><cell>81.84%</cell><cell>82.42%</cell><cell>82.20%</cell><cell>83.48%</cell></row><row><cell>Ped2</cell><cell>86.43%</cell><cell>88.82%</cell><cell>88.84%</cell><cell>92.21%</cell></row><row><cell>ShanghaiTech</cell><cell>66.25%</cell><cell>68.16%</cell><cell>67.12%</cell><cell>69.63%</cell></row><row><cell>average inference time</cell><cell>5 FPS</cell><cell>10 FPS</cell><cell>3 FPS</cell><cell>10 FPS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8</head><label>8</label><figDesc>AUC of only spatial and only temporal features on three datasets.</figDesc><table><row><cell></cell><cell>Spatial features</cell><cell>Temporal features</cell></row><row><cell>Avenue</cell><cell>82.58%</cell><cell>82.42%</cell></row><row><cell>Ped2</cell><cell>91.20%</cell><cell>88.82%</cell></row><row><cell>ShanghaiTech</cell><cell>69.63%</cell><cell>68.16%</cell></row><row><cell></cell><cell>TABLE 9</cell><cell></cell></row><row><cell cols="3">AUC of different feature fusions on three datasets.</cell></row><row><cell></cell><cell>Early fusion</cell><cell>Late Fusion</cell></row><row><cell>Avenue</cell><cell>82.90%</cell><cell>83.48%</cell></row><row><cell>Ped2</cell><cell>89.80%</cell><cell>92.21%</cell></row><row><cell>ShanghaiTech</cell><cell>68.60%</cell><cell>69.63%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10</head><label>10</label><figDesc>Running time of Conv-AE with feature extraction, TSC and sRNN-AE without feature extraction on Avenue dataset. Avenue Fig. 11. The AUC with different β on Avenue and Ped2 dataset.</figDesc><table><row><cell></cell><cell>K</cell><cell>Training</cell><cell>Inference</cell></row><row><cell>Conv-AE</cell><cell>N/A</cell><cell>6 hours</cell><cell>30 FPS</cell></row><row><cell>TSC</cell><cell>30</cell><cell>30 hours</cell><cell>7 FPS</cell></row><row><cell>sRNN-AE</cell><cell>3</cell><cell>1.2 hours</cell><cell>152 FPS</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>δ 2 = 100 in our experiments. It is worth mentioning that since S t-1,t is multiplied by λ 2 , thus we can set δ to any value and tune λ 2 accordingly.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported in part by the National Key Research and Development Program of China under Grant 2016YFB1001001 and NSFC (No. 61502304).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional lstm for anomaly detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interpretable recurrent neural networks using sequential sparse recovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08033</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Goal-based trajectory analysis for unusual behaviour detection in intelligent surveillance</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Zelek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="230" to="240" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Navneet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semisupervised adapted hmms for unusual event detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: a spacetime mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video anomaly detection with compact feature sets for online performance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leyva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3463" to="3478" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A comprehensive study of sparse codes on abnormality detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04026</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01553</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using spatiotemporal autoencoder</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Anomaly detection in video using predictive convolutional long short-term memory networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Medel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00390</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00866</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Center for Research in Computer Vision (CRCV), University of Central Florida (UCF)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A discriminative framework for anomaly detection in large videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">249</biblScope>
			<biblScope unit="page">250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">rmk-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">801</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2067" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06573</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7445" to="7454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSDI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Histograms of optical flow orientation for abnormal events detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Snoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="2013">2013. 2013</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>in Performance Evaluation of Tracking and Surveillance (PETS)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Real time anomaly detection in h. 264 compressed videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG), 2013 Fourth National Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed, size and texture</title>
		<author>
			<persName><forename type="first">V</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="55" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An on-line, real-time learning method for detecting anomalies in videos using spatio-temporal compositions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Roshtkhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1436" to="1452" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
