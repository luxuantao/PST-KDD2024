<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FAST MONTE CARLO ALGORITHMS FOR MATRICES II: COMPUTING A LOW-RANK APPROXIMATION TO A MATRIX *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
							<email>drinep@cs.rpi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ravi</forename><surname>Kannan</surname></persName>
							<email>kannan@cs.yale.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06520</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>AND</roleName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
							<email>mahoney@cs.yale.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06520</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FAST MONTE CARLO ALGORITHMS FOR MATRICES II: COMPUTING A LOW-RANK APPROXIMATION TO A MATRIX *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0A06F6A8B8D56D041293E67970D35BD0</idno>
					<idno type="DOI">10.1137/S0097539704442696</idno>
					<note type="submission">Received by the editors April 5, 2004; accepted for publication (in revised form) November 17, 2005;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>randomized algorithms</term>
					<term>Monte Carlo methods</term>
					<term>massive data sets</term>
					<term>singular value decomposition AMS subject classification. 68W20</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many applications, the data consist of (or may be naturally formulated as) an m × n matrix A. It is often of interest to find a low-rank approximation to A, i.e., an approximation D to the matrix A of rank not greater than a specified rank k, where k is much smaller than m and n. Methods such as the singular value decomposition (SVD) may be used to find an approximation to A which is the best in a well-defined sense. These methods require memory and time which are superlinear in m and n; for many applications in which the data sets are very large this is prohibitive. Two simple and intuitive algorithms are presented which, when given an m × n matrix A, compute a description of a low-rank approximation D * to A, and which are qualitatively faster than the SVD. Both algorithms have provable bounds for the error matrix A -D * . For any matrix X, let X F and X 2 denote its Frobenius norm and its spectral norm, respectively. In the first algorithm, c columns of A are randomly chosen. If the m × c matrix C consists of those c columns of A (after appropriate rescaling), then it is shown that from C T C approximations to the top singular values and corresponding singular vectors may be computed. From the computed singular vectors a description D * of the matrix A may be computed such that rank(D * ) ≤ k and such that</p><p>holds with high probability for both ξ = 2, F . This algorithm may be implemented without storing the matrix A in random access memory (RAM), provided it can make two passes over the matrix stored in external memory and use O(cm + c 2 ) additional RAM. The second algorithm is similar except that it further approximates the matrix C by randomly sampling r rows of C to form a r × c matrix W . Thus, it has additional error, but it can be implemented in three passes over the matrix using only constant additional RAM. To achieve an additional error (beyond the best rank k approximation) that is at most A 2 F , both algorithms take time which is polynomial in k, 1/ , and log(1/δ), where δ &gt; 0 is a failure probability; the first takes time linear in max(m, n) and the second takes time independent of m and n. Our bounds improve previously published results with respect to the rank parameter k for both the Frobenius and spectral norms. In addition, the proofs for the error bounds use a novel method that makes important use of matrix perturbation theory. The probability distribution over columns of A and the rescaling are crucial features of the algorithms which must be chosen judiciously.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction.</head><p>We are interested in developing and analyzing fast Monte Carlo algorithms for performing useful computations on large matrices. In this paper we consider the singular value decomposition (SVD); in two related papers we consider matrix multiplication and a new method for computing a compressed approximate decomposition of a large matrix <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Since such computations generally require time which is superlinear in the number of nonzero elements of the matrix, we expect our algorithms to be useful in many applications where data sets are modeled by matrices and are extremely large. In all these cases, we assume that the input matrices are prohibitively large to store in random access memory (RAM) and thus that only external memory storage is possible. Our algorithms will be allowed to read the matrices a few, e.g., one, two or three, times and keep a small randomly chosen and rapidly computable "sketch" of the matrices in RAM; computations will then be performed on this "sketch." We will work within the framework of the pass-efficient computational model, in which the scarce computational resources are the number of passes over the data, the additional RAM space required, and the additional time required <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>In many applications, the data consist of (or may be naturally formulated as) an m × n matrix A which is either low-rank or is well approximated by a low-rank matrix <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22]</ref>. In these application areas, e.g., latent semantic indexing, DNA microarray analysis, facial and object recognition, and web search models, the data may consist of m points in R n . Let A ∈ R m×n be the matrix with these points as rows. Two methods for dealing with such high-dimensional data are the SVD (and the related principal components analysis) and multidimensional scaling <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>. Thus, it is often of interest to find a low-rank approximation to A, i.e., an approximation D, of rank no greater than a specified rank k, to the matrix A, where k is much smaller than m and n. For example, this rank reduction is used in many applications of linear algebra and statistics as well as in image processing, lossy data compression, text analysis, and cryptography <ref type="bibr" target="#b5">[6]</ref>. The SVD may be used to find an approximation to A which is the best in a well-defined sense <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, but it requires a superlinear (in m and n) polynomial time dependence that is prohibitive for many applications in which the data sets are very large. Another method that has attracted interest recently is the traditional "random projection" method where one projects the problem into a randomly chosen low-dimensional subspace <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20]</ref>. This dimensional reduction requires performing an operation that amounts to premultiplying the given m × n matrix A by an s × m matrix which takes time dependent in a superlinear manner on m + n.</p><p>In this paper we present two simple and intuitive algorithms which, when given an m × n matrix A, compute a description of a low-rank approximation D * to A, and which are qualitatively faster than the SVD. Both algorithms have provable bounds for the error matrix A -D * . For any matrix X, let X F and X 2 denote its Frobenius norm and its spectral norm (as defined in section 3.1), respectively. In the first algorithm, the LinearTimeSVD algorithm of section 4, c columns of A are randomly chosen. If the m × c matrix C consists of those c columns of A (after appropriate rescaling), then it is shown that from C T C approximations to the top singular values and corresponding singular vectors of A may be computed. From the computed singular vectors a description D * of the matrix A may be computed such that rank(D * ) ≤ k and such that</p><formula xml:id="formula_0">A -D * 2 ξ ≤ min D:rank(D)≤k A -D 2 ξ + poly(k, 1/c) A 2 F (1)</formula><p>holds with high probability for each of ξ = 2, F . This algorithm may be implemented without storing the matrix A in RAM, provided it can make two passes over the matrix stored in external memory and use O(cm + c 2 ) additional RAM. The second algorithm, the ConstantTimeSVD algorithm of section 5, is similar except that it further approximates the matrix C by randomly sampling r rows of C to form an r ×c </p><formula xml:id="formula_1">A -D * 2 2 1/ 2 1/ 4 k 4 / 3 A -D * 2 F k/ 2 k 2 / 4 k 4 / 3</formula><p>matrix W . Thus, it has additional error but can be implemented in three passes over the matrix using additional RAM that is O(cr), i.e., that is constant for constant c and r. To achieve an additional error that is at most A 2 F , both algorithms take time which is polynomial in k, 1/ , and log(1/δ), where δ &gt; 0 is a failure probability; see Table <ref type="table" target="#tab_0">1</ref> for a summary of the dependence of the sampling complexity on k and . The first algorithm takes time linear in max(m, n) and the other takes time independent of m and n. Our bounds improve previously published results with respect to the rank parameter k for both the Frobenius and the spectral norms. In addition, the proofs for the error bounds use a novel method that makes important use of matrix perturbation theory. The probability distribution over columns of A and the rescaling are crucial features of the algorithms which must be chosen judiciously.</p><p>It is worth emphasizing how this work fits into recent work on computing low-rank matrix approximations. In the original work of Frieze, Kannan, and Vempala <ref type="bibr" target="#b15">[16]</ref> (see also <ref type="bibr" target="#b16">[17]</ref>) it was shown that by working with a randomly chosen and constant-sized submatrix of A, one could obtain bounds of the form (1) for the Frobenius norm (and thus indirectly for the spectral norm). To achieve an additional error that is at most A 2 F , the size of the submatrix was a constant with respect to m and n but depended polynomially on k and 1/ ; although the submatrix was constant-sized, its construction (in particular, the construction of the sampling probabilities) required space and thus time that was linear in m + n. In this work, we modify the algorithm of <ref type="bibr" target="#b15">[16]</ref> so that both the construction of and the computation on the constant-sized submatrix requires only constant additional space and time; thus, it fits within the framework of the pass-efficient model of data-streaming computation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. In addition, we provide a different proof of the main result of <ref type="bibr" target="#b15">[16]</ref> for the Frobenius norm and improve the polynomial dependence on k. Our proof method is quite different than that of <ref type="bibr" target="#b15">[16]</ref>; it relies heavily on the approximate matrix multiplication result of <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b10">[11]</ref> and it uses the Hoffman-Wielandt inequality. In addition, we provide a proof of a direct and significantly improved bound with respect to the spectral norm. Since these results are technically quite complex, we also present the corresponding proofs for both norms in the linear additional space and time framework <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. These latter results have been presented in the context of clustering applications <ref type="bibr" target="#b9">[10]</ref>, but are included here for completeness and to provide motivation and clarity for the more complex constant time results. Table <ref type="table" target="#tab_0">1</ref> provides a summary of our results, for both the linear and the constant time models, and shows the number of rows and columns to be sampled sufficient to ensure, with high probability, an additional error of A 2 F in (1); see section 6 for more discussion. In other related work, Achlioptas and McSherry have also computed low-rank approximations using somewhat different sampling techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. The primary focus of their work was in introducing methods to accelerate orthogonal iteration and Lanczos iteration, which are two commonly used methods for computing low-rank approximations to a matrix. Also included in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> is a comparison of their methods with those of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> and thus with the results we present here. Our algorithms and those of <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> come with mathematically rigorous guarantees of the running time and of the quality of the approximation produced. As far as we know, so-called incremental SVD algorithms, which bring as much of the data as possible into memory, compute the SVD, and then update this SVD in an incremental fashion with the remaining data, do not come with such guarantees.</p><p>In section 2 several applications areas that deal with large matrices are discussed, and in section 3 we provide a review of relevant linear algebra, the pass-efficient model, and an approximate matrix multiplication result that will be used extensively. Then, in section 4 our linear additional space and time approximation algorithm, the LinearTimeSVD algorithm, is presented and analyzed; in section 5 our constant additional space and time approximation algorithm, the ConstantTimeSVD algorithm, is presented and analyzed. Finally, in section 6 a discussion and conclusion are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Some applications.</head><p>There are numerous applications in which the data are well approximated by a low-rank matrix. In this section we discuss several such applications to provide motivation for our algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Latent semantic indexing.</head><p>Latent semantic indexing (LSI) is a general technique for analyzing a collection of documents which are assumed to be related <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref>. Approaches to retrieving textual information from databases that depend on a lexical match between words in the query and words in the document can be inaccurate, both because often users want to retrieve information on the basis of conceptual content and because individual words do not in general provide reliable evidence about the conceptual topic of a document. LSI is an alternative matching method that attempts to overcome problems associated with lexical matching; it does so by assuming that there is some underlying or latent semantic structure that is partially obscured by variability in word choice and then using techniques such as SVD to remove the noise and estimate this latent structure.</p><p>Suppose that there are m documents and n terms which occur in the documents. Latent semantic structure analysis starts with a term-document matrix, e.g., a matrix A ∈ R m×n , where A ij is frequency of the jth term in the ith document. A topic is modeled as an n-vector of nonnegative reals summing to 1, where the jth component of a topic vector is interpreted as the frequency with which the jth term occurs in a discussion of the topic. By assumption, the number of topics that the documents are about is small relative to the number of unique terms n. It can be argued that, for a given k, finding a set of k topics which best describe the documents corresponds to keeping only the top k singular vectors of A; most of the important underlying structure in the association of terms and documents will then be kept and most of the noise or variability in word usage will be removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DNA microarray data.</head><p>DNA microarray technology has been used to study a variety of biological processes since it permits the monitoring of the expression levels of thousands of genes under a range of experimental conditions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>. Depending on the particular technology, either the absolute or the relative expression levels of m genes, which for model organisms may constitute nearly the entire genome, are probed simultaneously by a single microarray. A series of n arrays probe genome-wide expression levels in n different samples, i.e., under n different experimental conditions. The data from microarray experiments may thus be represented as a matrix A ∈ R m×n , where A ij represents the expression level of gene i under experimental condition j. From this matrix, both the relative expression level of the ith gene under every condition and also the relative expression level of every gene under the jth condition may be easily extracted.</p><p>This matrix is low-rank and thus a small number of eigengenes and corresponding eigenarrays (left and right singular vectors) are sufficient to capture most of the gene expression information. Removing the rest, which correspond to noise or experimental artifacts, enables meaningful comparison of the expression levels of different genes. When processing and modeling genome-wide expression data, the SVD and its lowrank approximation provides a framework such that the mathematical variables and operations suggest assigned biological meaning, e.g., in terms of cellular regulatory processes and cellular states, that may be hidden in the original data due to experimental noise or hidden dependencies. Expression data has been used for inference tasks such as to identify genes based on coexpression, predict regulatory elements, and reverse-engineer transcription networks, but this inference is difficult with noise or dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Eigenfaces and facial recognition.</head><p>Applications of SVD and low-rank approximations in computer vision include pattern estimation, image compression and restoration, and facial and object recognition, where the concept of eigenfaces has been useful <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>The goal of facial recognition is to recognize a certain face given a database of photographs of human faces under variations in lighting conditions and pose viewpoints. A common approach is to represent the database as a matrix in which the rows of the matrix are the images represented as vectors. Thus, if there are m images, each of which is of size n × n, the matrix A ∈ R m×n 2 represents the database of images, where A ij is the jth pixel value in the ith image. Typically, m n 2 and, although many of the singular vectors are needed for very accurate reconstruction of an image, often only a few of the singular vectors are needed to extract the major appearance characteristics of an image. The right singular vectors of the matrix A are known as eigenfaces since they are the principal components or eigenvectors of the associated correlation matrix of the set of face images. The eigenfaces are computed and they are used to project the database of photographs to a lower-dimensional space that spans the significant variations among known facial images. Then, given a new image, it is projected to the same low-dimensional space, and its position is then compared to the images in the database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Web search model.</head><p>The problem of how to extract information from the network structure of a hyperlinked environment such as the World Wide Web was considered by <ref type="bibr">Kleinberg [22]</ref>. This is of interest, for example, if one wants to find web pages that are relevant to a given query and one is using a keyword-based web search program, since there is no obvious endogenous measure of an authoritative page that would favor it under a text-based ranking system.</p><p>Starting with a set of pages returned by a text-based search engine, a document is defined to be an authority if many other documents returned by the search point to it, i.e., have a hypertext link to it. A document is defined to be a hub if it points to many other documents. More generally, suppose n documents are returned by the search engine. Then, a matrix A ∈ R m×n is defined, where A ij is 1 or 0 depending on whether the ith document points to the jth document. Kleinberg attempts to find two n-vectors, x and y, where x i is the hub weight of document i and y j is the authority weight of document j. He then argues that it is desirable to find max |x|=|y|=1 x T Ay, where | • | denotes the Euclidean length, since in maximizing x, y one expects the hub weights and authority weights to be mutually consistent. This is simply the problem of finding the singular vectors of A. Since A is large, he judiciously chooses a submatrix of A and computes only the singular vectors of it. In the case when the key word has multiple meanings, not only the top but also some of the other singular vectors with large singular values are interesting. Thus, it is of interest to find the k largest singular vectors form some small k. This is the problem we are considering, and we also find the singular vectors of a submatrix, but a randomly chosen one.</p><p>3. Review of relevant background. This section contains a review of linear algebra that will be useful throughout the paper; for more detail, see <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8]</ref> and the references therein. This section also contains a review of the pass-efficient model of data-streaming computation (which provides a framework within which our SVD results may be viewed) and a matrix multiplication result that will be used extensively in our proofs; see <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.</head><p>Review of linear algebra. For a vector x ∈ R n we let x i , i = 1, . . . , n, denote the ith element of x and we let |x| = ( n i=1 |x i | 2 ) 1/2 . For a matrix A ∈ R m×n we let A (j) , j = 1, . . . , n, denote the jth column of A as a column vector and A (i) , i = 1, . . . , m, denote the ith row of A as a row vector; thus, if</p><formula xml:id="formula_2">A ij denotes the (i, j)th element of A, A ij = (A (j) ) i = (A (i) ) j . The range of an A ∈ R m×n is range(A) = {y ∈ R m : y = Ax for some x ∈ R n } = span(A (1) , . . . , A (n) ).</formula><p>The rank of A, rank(A), is the dimension of range(A) and is equal to the number of linearly independent columns of A; since this is equal to rank(A T ) it also equals the number of linearly independent rows of A. The null space of A is</p><formula xml:id="formula_3">null(A) = {x ∈ R n : Ax = 0}.</formula><p>For a matrix A ∈ R m×n we denote matrix norms by A ξ , using subscripts to distinguish between various norms. Of particular interest will be the Frobenius norm, which is defined by</p><formula xml:id="formula_4">A F = m i=1 n j=1 A 2 ij . (2)</formula><p>If Tr (A) is the matrix trace which is the sum of the diagonal elements of A, then A 2 F = Tr(A T A) = Tr(AA T ). Also of interest is the spectral norm, which is defined by</p><formula xml:id="formula_5">A 2 = sup x∈R n , x =0 |Ax| |x| . (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>Both of these norms are submultiplicative and unitarily invariant and they are related to each other as</p><formula xml:id="formula_7">A 2 ≤ A F ≤ √ n A 2 .</formula><p>Both of these norms provide a measure of the "size" of the matrix A. Note that if A ∈ R m×n , then there exists an x ∈ R n such that |x| = 1 and</p><formula xml:id="formula_8">A T Ax = A 2 2 x and that if {x 1 , x 2 , . . . , x n } is any basis of R n and if A ∈ R m×n , then A 2 F = n i=1 Ax i 2 . If A ∈ R m×n , then there exist orthogonal matrices U = [u 1 u 2 . . . u m ] ∈ R m×m and V = [v 1 v 2 . . . v n ] ∈ R n×n where {u t } m t=1 ∈ R m and {v t } n t=1 ∈ R n are such that U T AV = Σ = diag(σ 1 , . . . , σ ρ ),<label>(4)</label></formula><p>where Σ ∈ R m×n , ρ = min{m, n}, and</p><formula xml:id="formula_9">σ 1 ≥ σ 2 ≥ • • • ≥ σ ρ ≥ 0. Equivalently, A = U ΣV T .</formula><p>The three matrices U , V , and Σ constitute the SVD of A. The σ i are the singular values of A and the vectors u i , v i are the ith left and the ith right singular vectors, respectively. The columns of U and V satisfy the relations Av i = σ i u i and A T u i = σ i v i . For symmetric matrices the left and right singular vectors are the same. The singular values of A are the nonnegative square roots of the eigenvalues of A T A and of AA T ; furthermore, the columns of U , i.e., the left singular vectors, are eigenvectors of AA T and the columns of V , i.e., the right singular vectors, are eigenvectors of A T A.</p><p>The SVD can reveal important information about the structure of a matrix. If we define r by</p><formula xml:id="formula_10">σ 1 ≥ σ 2 ≥ • • • ≥ σ r &gt; σ r+1 = • • • = σ ρ = 0, then rank(A) = r, null(A) = span(v r+1 , . . . , v ρ</formula><p>), and range(A) = span(u 1 , . . . , u r ). If we let U r ∈ R m×r denote the matrix consisting of the first r columns of U , V r ∈ R r×n denote the matrix consisting of the first r columns of V , and Σ r ∈ R r×r denote the principal r × r submatrix of Σ, then</p><formula xml:id="formula_11">A = U r Σ r V T r = r t=1 σ t u t v t T . (5)</formula><p>Note that this decomposition property provides a canonical description of a matrix as a sum of r rank-one matrices of decreasing importance. If k ≤ r and we define</p><formula xml:id="formula_12">A k = U k Σ k V T k = k t=1 σ t u t v t T , (6) then A k = U k U T k A = ( k t=1 u t u t T )A and A k = AV k V T k = A( k t=1 v t v t T</formula><p>), i.e., A k is the projection of A onto the space spanned by the top k singular vectors of A. Furthermore, the distance (as measured by both • 2 and • F ) between A and any rank-k approximation to A is minimized by A k , i.e., min</p><formula xml:id="formula_13">D∈R m×n :rank(D)≤k A -D 2 = A -A k 2 = σ k+1 (A) (7)</formula><p>and min</p><formula xml:id="formula_14">D∈R m×n :rank(D)≤k A -D 2 F = A -A k 2 F = r t=k+1 σ 2 t (A). (8)</formula><p>Thus, A k constructed from the k largest singular triplets of A is the optimal rank-k approximation to A with respect to both • F and • 2 . More generally, one can also show that A 2 = σ 1 and that</p><formula xml:id="formula_15">A 2 F = r i=1 σ 2 i .</formula><p>From the perturbation theory of matrices it is known that the size of the difference between two matrices can be used to bound the difference between the singular value spectrum of the two matrices <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8]</ref>. In particular, if</p><formula xml:id="formula_16">A, E ∈ R m×n , m ≥ n, then max t:1≤t≤n |σ t (A + E) -σ t (A)| ≤ E 2 (9) and n k=1 (σ k (A + E) -σ k (A)) 2 ≤ E 2 F . (10)</formula><p>The latter inequality is known as the Hoffman-Wielandt inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Review of the pass-efficient model.</head><p>The pass-efficient model of datastreaming computation is a computational model that is motivated by the observation that in modern computers the amount of disk storage, i.e., sequential access memory, has increased very rapidly, while RAM and computing speeds have increased at a substantially slower pace <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>. In the pass-efficient model the three scarce computational resources are number of passes over the data and the additional RAM space and additional time required by the algorithm. The data are assumed to be stored on a disk, to consist of elements whose size is bounded by a constant, and to be presented to an algorithm on a read-only tape. See <ref type="bibr" target="#b12">[13]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Review of matrix multiplication.</head><p>The BasicMatrixMultiplication algorithm to approximate the product of two matrices is presented and analyzed in <ref type="bibr" target="#b12">[13]</ref>. When this algorithm is given as input two matrices, A ∈ R m×n and B ∈ R n×p , a probability distribution {p i } n i=1 , and a number c ≤ n, it returns as output two matrices, C and R, such that CR ≈ AB; C ∈ R m×c is a matrix whose columns are c randomly chosen columns of A (suitably rescaled) and R ∈ R c×p is a matrix whose rows are the c corresponding rows of B (also suitably rescaled). An important aspect of this algorithm is the probability distribution {p i } n i=1 used to choose column-row pairs. Although one could always use a uniform distribution, superior results are obtained if the probabilities are chosen judiciously. In particular, a set of sampling probabilities {p i } n i=1 are nearly optimal probabilities if they are of the form <ref type="bibr" target="#b10">(11)</ref> and are the optimal probabilities (with respect to approximating the product AB) if they are of the form <ref type="bibr" target="#b10">(11)</ref> with β = 1. In <ref type="bibr" target="#b12">[13]</ref> we prove the following theorem.</p><p>Theorem 1. Suppose A ∈ R m×n , B ∈ R n×p , c ∈ Z + such that 1 ≤ c ≤ n, and {p i } n i=1 are such that p i ≥ 0, n i=1 p i = 1 and such that for some positive constant β ≤ 1</p><formula xml:id="formula_17">p k ≥ β A (k) B (k) n k =1 A (k ) B (k ) . (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>Construct C and R with the BasicMatrixMultiplication algorithm of <ref type="bibr" target="#b12">[13]</ref> and let CR be an approximation to AB. Then</p><formula xml:id="formula_19">E AB -CR 2 F ≤ 1 βc A 2 F B 2 F . (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>Furthermore, let δ ∈ (0, 1) and η = 1 + (8/β) log(1/δ). Then, with probability at least</p><formula xml:id="formula_21">1 -δ, AB -CR 2 F ≤ η 2 βc A 2 F B 2 F . (13)</formula><p>In <ref type="bibr" target="#b12">[13]</ref> it is shown that after one pass over the matrices nearly optimal probabilities can be constructed. In the present paper, we will be particularly interested in the case that B = A T . In this case, using the Select algorithm of <ref type="bibr" target="#b12">[13]</ref> random samples can be drawn according to nearly optimal probabilities using O(1) additional space and time. LinearTimeSVD Algorithm.</p><formula xml:id="formula_22">Input: A ∈ R m×n , c, k ∈ Z + such that 1 ≤ k ≤ c ≤ n, {p i } n i=1 such that p i ≥ 0 and n i=1 p i = 1. Output: H k ∈ R m×k and σ t (C), t = 1, . . . , k. 1. For t = 1 to c, (a) Pick i t ∈ 1, . . . , n with Pr [i t = α] = p α , α = 1, . . . , n. (b) Set C (t) = A (it) / √ cp it .</formula><p>2. Compute C T C and its SVD; say  </p><formula xml:id="formula_23">C T C = c t=1 σ 2 t (C)y t y t T . 3. Compute h t = Cy t /σ t (C) for t = 1, . . . , k. 4. Return H k , where H (t) k = h t , and σ t (C), t = 1, . . . , k.</formula><formula xml:id="formula_24">R n V {v i } Σ A G G R m U {u i } R n A G G R m R m H(=U C ) {h i } o o R c C y y C T C Q Q R c Y (=V C ) {y i } o o Σ C y y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Linear time SVD approximation algorithm.</head><p>4.1. The algorithm. Given a matrix A ∈ R m×n we wish to approximate its top k singular values and the corresponding singular vectors in a constant number of passes through the data and O(cm + c 2 ) additional space and O(c 2 m + c 3 ) additional time. The strategy behind the LinearTimeSVD algorithm is to pick c columns of the matrix A, rescale each by an appropriate factor to form a matrix C ∈ R m×c , and then compute the singular values and corresponding left singular vectors of the matrix C, which will be approximations to the singular values and left singular vectors of A, in a sense we make precise later. These are calculated by performing an SVD of the matrix C T C to compute the right singular vectors of C and from them calculating the left singular vectors of C.</p><p>The LinearTimeSVD algorithm is described in Figure <ref type="figure" target="#fig_0">1</ref>; it takes as input a matrix A and returns as output an approximation to the top k left singular values and the corresponding singular vectors. Note that by construction the SVD of C is C = HΣ C Y T . A diagram illustrating the action of the LinearTimeSVD algorithm is presented in Figure <ref type="figure" target="#fig_1">2</ref>. The transformation represented by the matrix A is shown along with its SVD, and the transformation represented by the matrix C is also shown along with its SVD. It will be shown that if the probabilities {p i } n i=1 are chosen judiciously, then the left singular vectors of C are with high probability approximations to the left singular vectors of A.</p><p>In section 4.2 we discuss running-time issues, and in section 4.3 we will prove the correctness of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of the implementation and running time.</head><p>Assuming that nearly optimal sampling probabilities (as defined in section 3.3) are used, then in the LinearTimeSVD algorithm the sampling probabilities p k can be used to select columns to be sampled in one pass and O(c) additional space and time using the Select algorithm of <ref type="bibr" target="#b12">[13]</ref>. Given the elements to be sampled, the matrix C can then be constructed in one additional pass; this requires additional space and time that is O(mc). Given C ∈ R m×c , computing C T C requires O(mc) additional space and O(mc 2 ) additional time, and computing the SVD of C T C requires O(c 3 ) additional time. Then computing H k requires k matrix-vector multiplications for a total of O(mck) additional space and time. Thus, overall O(cm + c 2 ) additional space and O(c 2 m + c 3 ) additional time are required by the LinearTimeSVD algorithm. Note that the "description" of the solution that is computable in the allotted additional space and time is the explicit approximation to the top k singular values and corresponding left singular vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of the sampling step. Approximating</head><formula xml:id="formula_25">A by A k = U k U T k A incurs an error equal to A -A k 2 F = r t=k+1 σ 2 t (A) and A -A k 2 = σ k+1 (A)</formula><p>, since A k is the "optimal" rank-k approximation to A with respect to both • F and • 2 . We will show that in addition to this error the matrix H k H T k A has an error that depends on AA T -CC T F . Then, using the results of Theorem 1, we will show that this additional error depends on A 2 F . We first consider obtaining a bound with respect to the Frobenius norm. Theorem 2. Suppose A ∈ R m×n and let H k be constructed from the Linear-TimeSVD algorithm. Then</p><formula xml:id="formula_26">A -H k H T k A 2 F ≤ A -A k 2 F + 2 √ k AA T -CC T F .</formula><p>Proof. Recall that for matrices X and Y ,</p><formula xml:id="formula_27">X 2 F = Tr(X T X), Tr (X + Y ) = Tr (X) + Tr (Y ), and also that H T k H k = I k . Thus, we may express A -H k H T k A 2 F as A -H k H T k A 2 F = Tr (A -H k H T k A) T (A -H k H T k A) = Tr A T A -2A T H k H T k A + A T H k H T k H k H T k A = Tr A T A -Tr A T H k H T k A = A 2 F -A T H k 2 F . (<label>14</label></formula><formula xml:id="formula_28">)</formula><p>We may relate A T H k 2 F and k t=1 σ 2 t (C) by the following:</p><formula xml:id="formula_29">A T H k 2 F - k t=1 σ 2 t (C) ≤ √ k k t=1 A T h t 2 -σ 2 t (C) 2 1/2 = √ k k t=1 A T h t 2 -C T h t 2 2 1/2 = √ k k t=1 h t T (AA T -CC T )h t 2 1/2 ≤ √ k AA T -CC T F . (<label>15</label></formula><formula xml:id="formula_30">)</formula><p>The first inequality follows by applying the Cauchy-Schwarz inequality; the last inequality follows by writing AA T and CC T with respect to a basis containing {h t } k t=1 . By again applying the Cauchy-Schwarz inequality, noting that σ 2 t (X) = σ t (XX T ) for a matrix X, and applying the Hoffman-Wielandt inequality <ref type="bibr" target="#b9">(10)</ref>, we may also relate k k=1 σ 2 t (C) and k k=1 σ 2 t (A) by the following:</p><formula xml:id="formula_31">k t=1 σ 2 t (C) - k t=1 σ 2 t (A) ≤ √ k k t=1 σ 2 t (C) -σ 2 t (A) 2 1/2 = √ k k t=1 σ t (CC T ) -σ t (AA T ) 2 1/2 ≤ √ k m t=1 σ t (CC T ) -σ t (AA T ) 2 1/2 ≤ √ k CC T -AA T F . (<label>16</label></formula><formula xml:id="formula_32">)</formula><p>Combining the results of ( <ref type="formula" target="#formula_29">15</ref>) and ( <ref type="formula" target="#formula_31">16</ref>) allows us to relate A T H k 2 F and k t=1 σ 2 t (A) by the following:</p><formula xml:id="formula_33">A T H k 2 F - k t=1 σ 2 t (A) ≤ 2 √ k AA T -CC T F . (<label>17</label></formula><formula xml:id="formula_34">)</formula><p>Combining ( <ref type="formula" target="#formula_33">17</ref>) with ( <ref type="formula" target="#formula_27">14</ref>) yields the theorem.</p><p>We next prove a similar result for the spectral norm; note that the factor √ k is not present.</p><p>Theorem 3. Suppose A ∈ R m×n and let H k be constructed from the Linear-TimeSVD algorithm. Then</p><formula xml:id="formula_35">A -H k H T k A 2 2 ≤ A -A k 2 2 + 2 AA T -CC T 2 .</formula><p>Proof. Let H k = range(H k ) = span(h 1 , . . . , h k ) and let H m-k be the orthogonal complement of H k . Let x ∈ R m and let x = αy + βz, where y ∈ H k , z ∈ H m-k , and</p><formula xml:id="formula_36">α 2 + β 2 = 1; then A -H k H T k A 2 = max x∈R m ,|x|=1 x T (A -H k H T k A) = max y∈H k ,|y|=1,z∈H m-k ,|z|=1,α 2 +β 2 =1 (αy T + βz T )(A -H k H T k A) ≤ max y∈H k ,|y|=1 y T (A -H k H T k A) + max z∈H m-k ,|z|=1 z T (A -H k H T k A) (18) = max z∈H m-k ,|z|=1 z T A . (<label>19</label></formula><formula xml:id="formula_37">)</formula><p>Inequality <ref type="bibr" target="#b17">(18)</ref>  </p><formula xml:id="formula_38">z T A 2 = z T CC T z + z T AA T -CC T z ≤ σ 2 k+1 (C) + AA T -CC T 2 (20) ≤ σ 2 k+1 (A) + 2 AA T -CC T 2 (21) = A -A k 2 2 + 2 AA T -CC T 2 . (<label>22</label></formula><formula xml:id="formula_39">)</formula><p>Inequality <ref type="bibr" target="#b19">(20)</ref> follows since max z∈H m-k |z T C| occurs when z is the (k + 1)st left singular vector, i.e., the maximum possible in the H m-k subspace. Inequality <ref type="bibr" target="#b20">(21)</ref> follows since σ 2 k+1 (C) = σ k+1 (CC T ) and since by <ref type="bibr" target="#b8">(9)</ref> we have that</p><formula xml:id="formula_40">σ 2 k+1 (C) ≤ σ k+1 (AA T )+ AA T -CC T 2 ; (22) follows since A -A k 2 = σ k+1 (A).</formula><p>The theorem then follows by combining <ref type="bibr" target="#b18">(19)</ref> and <ref type="bibr" target="#b21">(22)</ref>.</p><p>Theorems 2 and 3 hold regardless of the sampling probabilities {p i } n i=1 . Since A -A k ξ , ξ = 2, F , is a property of the matrix A, the choice of sampling probabilities enters into the error of A -H k H T k A 2 ξ only through the term involving the additional error beyond the optimal rank-k approximation, i.e., the term AA T -CC T ξ . Although the additional error in Theorem 3 depends on AA T -CC T 2 , we note that AA T -CC T  2 ≤ AA T -CC T F and will use a bound for the latter quantity to bound the former in the following. Note that the prefactor of the additional error is 2 √ k for • 2 F , while that for • 2 2 is only 2. In the following theorem we specialize the sampling probabilities to be those that are nearly optimal; by choosing enough columns, the error in the approximation of the SVD can be made arbitrarily small. Theorem 4. Suppose A ∈ R m×n ; let H k be constructed from the Linear-TimeSVD algorithm by sampling c columns of A with probabilities</p><formula xml:id="formula_41">{p i } n i=1 such that p i ≥ β A (i) 2 / A 2 F for some positive β ≤ 1, and let η = 1 + (8/β) log(1/δ). Let &gt; 0. If c ≥ 4k/β 2 , then E A -H k H T k A 2 F ≤ A -A k 2 F + A 2 F , (<label>23</label></formula><formula xml:id="formula_42">)</formula><formula xml:id="formula_43">and if c ≥ 4kη 2 /β 2 , then with probability at least 1 -δ, A -H k H T k A 2 F ≤ A -A k 2 F + A 2 F . (<label>24</label></formula><formula xml:id="formula_44">)</formula><p>In addition, if c ≥ 4/β 2 , then</p><formula xml:id="formula_45">E A -H k H T k A 2 2 ≤ A -A k 2 2 + A 2 F , (<label>25</label></formula><formula xml:id="formula_46">)</formula><p>and if c ≥ 4η 2 /β 2 , then with probability at least 1δ,</p><formula xml:id="formula_47">A -H k H T k A 2 2 ≤ A -A k 2 2 + A 2 F . (26)</formula><p>Proof. By combining Theorems 2 and 3 with Theorem 1 we have that</p><formula xml:id="formula_48">E A -H k H T k A 2 F ≤ A -A k 2 F + 4k βc 1/2 A 2 F , (<label>27</label></formula><formula xml:id="formula_49">) E A -H k H T k A 2 2 ≤ A -A k 2 2 + 4 βc 1/2 A 2 F , (<label>28</label></formula><formula xml:id="formula_50">)</formula><p>and that with probability at least 1δ,</p><formula xml:id="formula_51">A -H k H T k A 2 F ≤ A -A k 2 F + 4η 2 k βc 1/2 A 2 F , (29) A -H k H T k A 2 2 ≤ A -A k 2 2 + 4η 2 βc 1/2 A 2 F . (<label>30</label></formula><formula xml:id="formula_52">)</formula><p>The theorem follows by using the appropriate value of c.</p><p>Note that alternatively one could sample rows instead of columns of a matrix; in this case, a modified version of the LinearTimeSVD algorithm leads to results analogous to Theorems 2 through 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Constant time SVD approximation algorithm.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The algorithm. Given a matrix</head><p>A ∈ R m×n we now wish to approximate its top k singular values and the corresponding singular vectors in a constant number of passes through the data and additional space and time that are O(1), independent of m and n. The strategy behind the ConstantTimeSVD algorithm is to pick c columns of the matrix A, rescale each by an appropriate factor to form a matrix C ∈ R m×c , and then compute approximations to the singular values and left singular vectors of the matrix C, which will then be approximations to the singular values and left singular vectors of A. In the LinearTimeSVD algorithm of section 4, the left singular vectors of the matrix C are computed exactly; as the analysis of section 4.2 showed, this computation takes additional space and time that is linear in m + n (assuming that c is constant). With the ConstantTimeSVD algorithm, in order to use only a constant O(1) additional space and time, sampling is performed again, drawing rows of C to construct a matrix W ∈ R w×c . The SVD of W T W is then computed; let</p><formula xml:id="formula_53">W T W = ZΣ W T W Z T = ZΣ 2 W Z T .</formula><p>The singular values and corresponding singular vectors so obtained are with high probability approximations to the singular values and singular vectors of C T C and thus to the singular values and right singular vectors of C. Note that this is simply using the LinearTimeSVD algorithm to approximate the right singular vectors of C by randomly sampling rows of C.</p><p>The ConstantTimeSVD algorithm is described in Figure <ref type="figure" target="#fig_2">3</ref>; it takes as input a matrix A and returns as output a "description" of an approximation to the top k left singular values and the corresponding singular vectors. This "description" of the approximations to the left singular vectors of A may, at the expense of one additional pass and linear additional space and time, be converted into an explicit ConstantTimeSVD Algorithm. </p><formula xml:id="formula_54">Input: A ∈ R m×n , c, w, k ∈ Z + such that 1 ≤ w ≤ m, 1 ≤ c ≤ n,</formula><formula xml:id="formula_55">) : t = 1, . . . , c}. (b) Set C (t) = A (it) / √ cp it . (Note that C is not explicitly constructed in RAM.) 2. Choose {q j } m j=1 such that q j = C (j) 2 / C 2 F . 3. For t = 1 to w, (a) Pick j t ∈ 1, . . . , m with Pr [j t = α] = q α , α = 1, . . . , m. (b) Set W (t) = C (jt) / √ wq jt .</formula><p>4. Compute W T W and its SVD. Say</p><formula xml:id="formula_56">W T W = c t=1 σ 2 t (W )z t z t T . 5. If a • F bound is desired, set γ = /100k, Else if a • 2 bound is desired, set γ = /100. 6. Let = min{k, max{t : σ 2 t (W ) ≥ γ W 2 F }}. 7.</formula><p>Return singular values {σ t (W )} t=1 and their corresponding singular vectors {z t } t=1 . approximation to the left singular vectors of A by using C = HΣ W Z T to compute H, whose columns are approximations of the left singular vectors of C. Note that γ in the ConstantTimeSVD algorithm is introduced to bound small singular values of C that may be perturbed by the second level of sampling; as indicated, the particular value of γ that is chosen depends on the norm bound which is desired. Note also that the probabilities {q j } m j=1 used in the algorithm are optimal (in the sense of section 3.3), as will be the probabilities {p i } n i=1 which will enter into Theorem 5. A diagram illustrating the action of the ConstantTimeSVD algorithm is presented in Figure <ref type="figure" target="#fig_3">4</ref>. The transformation represented by the matrix A is represented along with its SVD, and the transformation represented by the matrix C is also shown (but note that its SVD is not shown). The transformation represented by the matrix W , which is constructed from C with the second level of sampling, is also shown along with its SVD. In addition, approximations to the right singular vectors of C and to the left singular vectors of C calculated from C = HΣ W Z T are shown.</p><p>In section 5.2 we will show that this algorithm takes O(1), i.e., a constant with respect to m and n, additional space and time, assuming that c and w are constant. In section 5.3 we will state Theorem 5, which will establish the correctness of the algorithm; this theorem is the main result of this section and is the analogue of Theorem 4. Finally, in section 5.4 we will prove Theorem 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of the implementation and running time.</head><p>Assuming that optimal sampling probabilities (as defined in section 3.3) are used, then in the Con-stantTimeSVD algorithm the sampling probabilities p k can be used to select columns to be sampled in one pass and O(c) additional space and time using the Select algorithm of <ref type="bibr" target="#b12">[13]</ref>. Given the columns of A to be sampled, we do not explicitly construct the matrix C but instead perform a second level of sampling and select w rows of C with probabilities {q i } m i=1 (as described in the ConstantTimeSVD algorithm) in order to construct the matrix W . We do this by performing a second pass and using O(w) additional space and time, again using the Select algorithm. Then in a third pass we explicitly construct W ; this requires additional space and time that is O(cw). Then, given W , computing W T W requires O(cw) additional space and O(c 2 w) additional time, and computing the SVD of W T W requires O(c 3 ) additional time. The singular values and corresponding singular vectors thus computed can then be returned as the "description" of the solution. The total additional time for the ConstantTimeSVD algorithm is then O(c 3 + cw 2 ); this is a constant if c and w are assumed to be a constant. To explicitly compute Hk would require k matrix-vector multiplications which would require another pass over the data and O(mck) additional space and time.</p><formula xml:id="formula_57">R n V {v i } Σ A G G R m U {u i } R n A G G R m R m H { hi } o o R c C y y W Ô Ô W T W @ @ R c Z(=V W ) {z i } o o Σ W y y Σ W Ô Ô R w R w U W o o</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Statement of Theorem 5.</head><p>This subsection and the next provide an analysis of the ConstantTimeSVD algorithm similar to the analysis of the Linear-TimeSVD algorithm found in section 4.3. Recall that in section 4 we were interested in bounding</p><formula xml:id="formula_58">A -H k H T k A 2 ξ , where ξ = F, 2. In that case, H T k H k = I k , H k H T</formula><p>k was an orthonormal projection, and H k H T k A was our rank at most k approximation to A. In the constant time model, we do not have access to H k but instead to H , where the columns of H , i.e., ht = Cz t /σ t (W ), t = 1, . . . , , do not form an orthonormal set. However, by Lemma 2 of section 5.4.1, if C and W are constructed by sampling with optimal probabilities, then with high probability the columns of H are approximately orthonormal, HT H ≈ I , and H HT = t=1 htht T is approximately an orthonormal projection. Applying this to A, we will get our low-rank approximation. Note that in dealing with this nonorthonormality the original proof of <ref type="bibr" target="#b15">[16]</ref> contained a small error which was corrected in the journal version <ref type="bibr" target="#b16">[17]</ref>.</p><p>In this section and the next we use the following notation. Recall that the SVD of</p><formula xml:id="formula_59">W T W ∈ R c×c is W T W = c t=1 σ 2 t (W )z t z t T = ZΣ 2 W Z T , (<label>31</label></formula><formula xml:id="formula_60">)</formula><p>where Z ∈ R c×c . Define Z α,β ∈ R c×(β-α+1) to be the matrix whose columns are the αth through the βth singular vectors of W T W . Then</p><formula xml:id="formula_61">H = CZ 1, T,<label>(32)</label></formula><p>where T ∈ R × is the diagonal matrix with elements T tt = 1/σ t (W ). In addition, let the SVD of H be</p><formula xml:id="formula_62">H = B Σ H D T , (<label>33</label></formula><formula xml:id="formula_63">)</formula><p>and let us define the matrix Δ ∈ R × to be</p><formula xml:id="formula_64">Δ = T Z T 1, (C T C -W T W )Z 1, T. (<label>34</label></formula><formula xml:id="formula_65">)</formula><p>We will see that Δ is a measure of the degree to which the columns of H are not orthonormal.</p><p>Theorem 5 is the constant time analogue of Theorem 4 and is the main result of this section. Note that since the results from sampling at the second step, i.e., sampling from the matrix C to form the matrix W , depend on the samples chosen in the first sampling step, we do not state the following results in expectation, but instead state them with high probability.</p><p>Theorem 5. Suppose A ∈ R m×n ; let a description of H be constructed from the ConstantTimeSVD algorithm by sampling c columns of A with probabilities {p i } n i=1 and w rows of C with probabilities {q j } m j=1 where</p><formula xml:id="formula_66">p i = |A (i) | 2 / A 2 F and q j = |C (j) | 2 / C 2 F . Let η = 1 + 8 log(2/δ) and &gt; 0.</formula><p>If a Frobenius norm bound is desired, and hence the ConstantTimeSVD algorithm is run with γ = /100k, then by choosing c = Ω(k 2 η 2 / 4 ) columns of A and w = Ω(k 2 η 2 / 4 ) rows of C we have that with probability at least 1δ,</p><formula xml:id="formula_67">A -H HT A 2 F ≤ A -A k 2 F + A 2 F . (<label>35</label></formula><formula xml:id="formula_68">)</formula><p>If a spectral norm bound is desired, and hence the ConstantTimeSVD algorithm is run with γ = /100, then by choosing c = Ω(η 2 / 4 ) columns of A and w = Ω(η 2 / 4 ) rows of C we have that with probability at least</p><formula xml:id="formula_69">1 -δ, A -H HT A 2 2 ≤ A -A k 2 2 + A 2 F . (<label>36</label></formula><formula xml:id="formula_70">)</formula><p>Proof. See section 5.4 for the proof. Recall that in section 4 we first proved Theorems 2 and 3, which provided a bound on A -</p><formula xml:id="formula_71">H k H T k A 2 F and A -H k H T k A 2 2</formula><p>, respectively, for arbitrary probabilities, and then we proved Theorem 4 for the nearly optimal probabilities. Although a similar presentation strategy could be adopted in this section, in the interests of simplicity (due to the technically more complicated proofs in the constant time model) we instead immediately restrict ourselves in Theorem 5 to the case of optimal sampling probabilities and defer the proofs of the supporting lemmas to section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Proof of Theorem 5.</head><p>In this section, we prove Theorem 5. We start in section 5.4.1 with several lemmas that are common to both the Frobenius and spectral norms. Then in section 5.4.2 we provide the proof of (35). Finally, in section 5.4.3 we provide the proof of (36).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">General lemmas.</head><p>In this section, we prove four lemmas that are used in the proofs of both the Frobenius and spectral norm results.</p><p>First, we relate A -H HT A </p><formula xml:id="formula_72">A -H HT A 2 ξ ≤ 1 + 100 A -B B T A 2 ξ + 1 + 100 B B T -H HT 2 ξ A 2 ξ .</formula><p>Proof. By subadditivity and submultiplicitivity,</p><formula xml:id="formula_73">A -H HT A 2 ξ ≤ A -B B T A ξ + B B T -H HT ξ A ξ 2 .</formula><p>The lemma follows since (α + β)</p><formula xml:id="formula_74">2 ≤ (1 + ε)α 2 + (1 + 1/ε)β 2 for all ε ≥ 0.</formula><p>Second, although the vectors ht = Cz t /σ t (W ), t = 1, . . . , , do not in general form an orthonormal set, one would expect from their construction that if the matrix W T W is close to the matrix C T C, then with high probability they will be approximately orthonormal. Lemma 2 establishes that Δ, defined in (34), characterizes how far H is from having orthonormal columns and shows that the error introduced due to this nonorthonormality is bounded by a simple function of γ and the error introduced at the second level of sampling.</p><p>Lemma 2. When written in the basis with respect to Z,</p><formula xml:id="formula_75">HT H = I + Δ. Furthermore, for ξ = 2, F Δ ξ ≤ 1 γ W 2 F C T C -W T W ξ .</formula><p>Proof. Recall that H = CZ 1, T and that</p><formula xml:id="formula_76">T T Z T 1, W T W Z 1, T = I , so that HT H -I ξ = T T Z T 1, C T CZ 1, T -T T Z T 1, W T W Z 1, T ξ (37) = T T Z T 1, C T C -W T W Z 1, T ξ . (38)</formula><p>Using the submultiplicativity properties of the 2-norm, and in particular Proof. Since H = B Σ H D T , we have</p><formula xml:id="formula_77">AB ξ ≤ A 2 B ξ , (39) AB ξ ≤ A ξ B 2 , (40) for both ξ = 2, F , we get HT H -I ξ ≤ T T Z T 1, 2 C T C -W T W ξ Z 1, T 2 (41) ≤ T 2 2 C T C -W T W ξ (42) ≤ max t=1,..., 1/σ 2 t (W ) C T C -W T W ξ , (<label>43</label></formula><formula xml:id="formula_78">B B T -H HT ξ = B I -Σ 2 H B T ξ = I -Σ 2 H ξ = D I -Σ 2 H D T ξ = I -HT H ξ .</formula><p>Fourth, Lemma 4 considers the special case in which the probabilities</p><formula xml:id="formula_79">{p i } n i=1</formula><p>that are entered into the ConstantTimeSVD algorithm are optimal, as is the case for Theorem 5. </p><formula xml:id="formula_80">p i = Pr [i t = i] = |A (i) | 2 / A 2 F and q j = Pr [j t = j] = |C (j) | 2 / C 2 F . Then W F = C F = A F . Proof. If p i = A (i) 2 / A 2 F , we have that C 2 F = c t=1 C (t) 2 = c t=1 |A (i t ) | 2 cpi t = A 2 F . Similarly, if q j = C (j) 2 / C 2 F , we have that W 2 F = w t=1 W (t) 2 = w t=1 |C(i t )| 2 wqi t = C 2 F</formula><p>. The lemma follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Lemmas for the Frobenius norm proof.</head><p>In this section we prove (35). We do this by first proving lemmas sufficient to bound A -B B T A 2 F ; when this is combined with the lemmas of section 5.4.1 we obtain a bound on A-H HT A 2 F . The bound on A -B B T A 2 F depends on the error for the optimal rank-k approximation to A, i.e., A-A k 2 F , and additional errors that depend on the quality of the sampling approximations, i.e., on AA T -CC T F and C T C -W T W F . This will be the analogue of Theorem 2 applied to the constant additional space and time model. The result and associated proof will have a similar structure to that of Theorem 2, but will be more complicated due to the nonorthonormality of the vectors ht , t = 1, . . . , , and will involve additional error terms since two levels of approximation are involved.</p><p>We now prove several lemmas which will provide a bound for the first term in Lemma 1 when applied to the Frobenius norm. We first rewrite the A -B B T A 2 F term from Lemma 1. Note that Lemma 5 is the constant time analogue of <ref type="bibr" target="#b13">(14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 5.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A -B B</head><formula xml:id="formula_81">T A 2 F = A 2 F -B T A 2 F . Proof. A -B B T A 2 F = Tr A -B B T A T A -B B T A = Tr A T A -A T B B T A .</formula><p>Next, we want to provide a lower bound for B T A 2 F in terms of the singular values of W . We do so in several steps. First, we relate B T A 2 F to HT A 2 F . We note that the assumption Δ F &lt; 1 is made since in Theorem 5 optimal probabilities are used and sufficiently many columns and rows are drawn; if this assumption is dropped, then bounds of the form in Theorem 5 may be obtained with slightly worse sampling complexity. Lemma 6. If Δ F &lt; 1, then The lemma follows since Δ 2 ≤ Δ F &lt; 1 and by observing that 1 + x ≤ 1/(1x) for all x ≤ 1.</p><formula xml:id="formula_82">B T A 2 F ≥ (1 -Δ F ) HT A 2 F . Proof. Since H = B Σ H D T HT A 2 F = Σ H B T A 2 F ≤ Σ H 2 2 B T A 2 F = HT H 2 2 B T A 2 F ,<label>(44)</label></formula><p>Second, we relate HT A 2 F to HT C 2 F . Lemma 7.</p><formula xml:id="formula_83">HT A 2 F ≥ HT C 2 F -k + √ k Δ F AA T -CC T F .</formula><p>Proof. Since HT A 2 F = Tr( HT AA T HT ), we have that</p><formula xml:id="formula_84">HT A 2 F = Tr HT CC T HT + Tr HT (AA T -CC T ) HT ≥ HT C 2 F -AA T -CC T 2 H 2 F ,</formula><p>where the inequality follows since</p><formula xml:id="formula_85">Tr HT (AA T -CC T ) HT ≤ t ( HT ) (t) (AA T -CC T )( H ) (t) ≤ AA T -CC T 2 H 2 F .</formula><p>The lemma follows since • 2 ≤ • F and since</p><formula xml:id="formula_86">H 2 F = t=1 ht T ht = t=1 1 + Δ tt ≤ k + √ k Δ F . Third, we relate HT C 2 F to t=1 σ 2 t (W ). Lemma 8. HT C 2 F ≥ t=1 σ 2 t (W ) - 2 √ γ C T C -W T W F . Proof. Since HT C 2 F = C T H 2 F = C T CZ 1, T 2 F , we have HT C 2 F ≥ W T W Z 1, T F -(C T C -W T W )Z 1, T F 2 ≥ ⎛ ⎝ t=1 σ 2 t (W ) 1/2 - 1 √ γ W F (C T C -W T W ) F ⎞ ⎠ 2 ,</formula><p>where the second inequality uses that XZ F ≤ X F for any matrix X if the matrix Z has orthonormal columns. By multiplying out the right-hand side and ignoring terms that reinforce the inequality, the lemma follows since ( t=1 σ 2 t (W ))  </p><formula xml:id="formula_87">σ 2 t (W ) ≥ k t=1 σ 2 t (A)- √ k AA T -CC T F - √ k C T C -W T W F -(k-)γ W 2 F .</formula><p>Proof. Recalling the Hoffman-Wielandt inequality, we see that</p><formula xml:id="formula_88">k t=1 σ 2 t (C) -σ 2 t (A) ≤ √ k k t=1 σ 2 t (C) -σ 2 t (A) 2 1/2 ≤ √ k k t=1 σ t (CC T ) -σ t (AA T ) 2 1/2 ≤ √ k AA T -CC T F , (46) and, similarly, that k t=1 σ 2 t (W ) -σ 2 t (C) ≤ √ k k t=1 σ 2 t (W ) -σ 2 t (C) 2 1/2 ≤ √ k k t=1 σ t (W W T ) -σ t (CC T ) 2 1/2 ≤ √ k C T C -W T W F . (<label>47</label></formula><formula xml:id="formula_89">)</formula><p>By combining (46) and (47) we see that </p><formula xml:id="formula_90">k t=1 σ 2 t (W ) - k t=1 σ 2 t (A) ≤ √ k AA T -CC T F + √ k C T C -W T W F . (48) Since σ 2 t (W ) &lt; γ W</formula><formula xml:id="formula_91">= AA T -CC T and E C T C = C T C -W T W .</formula><p>First, we establish a lower bound on B T A 2 F . By combining Lemmas 6 and 7 and dropping terms that reinforce the inequality, we have that</p><formula xml:id="formula_92">B T A 2 F ≥ HT C 2 F -Δ F HT C 2 F -k + √ k Δ F E AA T F .</formula><p>By combining this with Lemmas 8 and 9 and dropping terms that reinforce the inequality, we have that</p><formula xml:id="formula_93">B T A 2 F ≥ k t=1 σ 2 t (A) -k + √ k E AA T F - √ k + 2 √ γ E C T C F -Δ F k t=1 σ 2 t (A) - √ k Δ F E AA T F -(k -)γ W 2 F . (<label>49</label></formula><formula xml:id="formula_94">)</formula><p>From Lemma 5 this immediately leads to the upper bound on</p><formula xml:id="formula_95">A -B B T A 2 F , A -B B T A 2 F ≤ A -A k 2 F + k + √ k E AA T F + √ k + 2 √ γ E C T C F + Δ F k t=1 σ 2 t (A) + √ k Δ F E AA T F + (k -)γ W 2 F .<label>(50)</label></formula><p>From Lemmas 1 and 3,</p><formula xml:id="formula_96">A -H HT A 2 F ≤ 1 + 100 A -B B T A 2 F + 1 + 100 Δ 2 F A 2 F . (51) Recall that γ = /100k, that k t=1 σ 2 t (A) ≤ A 2 F , that Δ F ≤ E C T C F /γ W 2 F</formula><p>by Lemma 2, and that W F = C F = A F by Lemma 4; (35) then follows by combining (50) and (51), using the sampling probabilities indicated in the statement of the theorem, and by choosing c, w = Ω(k 2 η 2 / 4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">Lemmas for the spectral norm proof.</head><p>In this section we prove (36). We do this by first proving lemmas sufficient to bound A -B B T A 2 2 ; when this is combined with the lemmas of section 5.4.1, we obtain a bound on A-H HT A 2  2 . The bound on A-B B T A 2 2 depends on the error for the optimal rank-k approximation to A, i.e., A -A k 2 2 , and additional errors that depend on the quality of the sampling approximations, i.e., on AA T -CC T 2 and C T C -W T W 2 . This will be the analogue of Theorem 3 applied to the constant additional space and time model. The result and associated proof will have a similar structure to that of Theorem 3, but will be more complicated due to the nonorthonormality of the vectors ht , t = 1, . . . , , and will involve additional error terms since two levels of approximation are involved.</p><p>We now prove three lemmas which will provide a bound for the first term in Lemma 1 when applied to the spectral norm. We first rewrite the A -B B T A 2 2 term from Lemma 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 10.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A -B B</head><formula xml:id="formula_97">T A 2 2 ≤ Z T k+1,c C T CZ k+1,c 2 + Z T +1,k C T CZ +1,k 2 + AA T -CC T 2 .</formula><p>Proof. In order to bound A-B B T A 2 we will project onto the subspace spanned by B and its orthogonal complement in a manner analogous to that used in the proof of Theorem 3. Let B = range(B ) and let B m-be the orthogonal complement of B . Let x = αy + βz, where y ∈ B , z ∈ B m-, and </p><formula xml:id="formula_98">α 2 + β 2 = 1. Then A -B B T A 2 = max x∈R m ,|x|=1 x T (A -B B T A) = max y∈B ,|y|=1,z∈B m-,|z|=1,α 2 +β 2 =1 (αy T + βz T )(A -B B T A) ≤ max y∈B ,|y|=1 y T (A -B B T A) + max z∈B m-,|z|=1 z T (A -B B T A) (52) = max z∈B m-,|z|=1 z T A . (<label>53</label></formula><formula xml:id="formula_99">A -B B T A 2 2 ≤ CZ k+1,c Z T k+1,c C T 2 + CZ +1,k Z T +1,k C T 2 + AA T -CC T 2 .</formula><p>The lemma follows since X T X 2 = XX T 2 for any matrix X.</p><p>We next bound the Z T k+1,c C T CZ k+1,c 2 term from Lemma 10; note that matrix perturbation theory is used in (59). </p><formula xml:id="formula_100">Z T k+1,c C T CZ k+1,c 2 ≤ σ 2 k+1 (W ) + C T C -W T W 2 . (<label>58</label></formula><formula xml:id="formula_101">)</formula><p>By a double application of (9), we see that</p><formula xml:id="formula_102">σ 2 k+1 (W ) ≤ σ 2 k+1 (A) + AA T -CC T 2 + C T C -W T W 2 . (<label>59</label></formula><formula xml:id="formula_103">)</formula><p>The lemma follows by combining (58) and (59) since A -A k 2 = σ k+1 (A).</p><p>Finally, we bound the Z T +1,k C T CZ +1,k 2 term from Lemma 10; note that if = k, it is unnecessary.</p><p>Lemma 12.</p><formula xml:id="formula_104">Z T +1,k C T CZ +1,k 2 ≤ C T C -W T W 2 + γ W 2 F .</formula><p>Proof. First note that (60)  <ref type="formula">62</ref>) and (63), using the sampling probabilities indicated in the statement of the theorem, and by choosing c, w = Ω(η 2 / 4 ).</p><formula xml:id="formula_105">Z T +1,k C T CZ +1,k 2 ≤ Z T +1,k C T C -W T W Z +1,k 2 + Z T +1,k W T W Z +1,k 2 . Since Z T +1,k C T C -W T W Z +1,k 2 ≤ C T C -W T W 2 Z +1,k<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and conclusion.</head><p>We have presented two algorithms to compute approximations to the SVD of a matrix A ∈ R m×n which do not require that A be stored in RAM, but for which the additional space and time required (in addition to a constant number of passes over the matrix) is either linear in m + n or is a constant independent of m and n; we have also proven error bounds for both algorithms with respect to both the Frobenius and spectral norms. Table <ref type="table" target="#tab_0">1</ref> in section 1 presents a summary of the dependence of the sampling complexity on k and . With the LinearTimeSVD algorithm, the additional error (beyond the optimal rank-k approximation) in the spectral norm bound can be made less than A 2 F by sampling Θ(1/ 2 ) columns, and the additional error in the Frobenius norm can be made less than A 2 F by sampling Θ(k/ 2 ) columns. Likewise, with the ConstantTimeSVD algorithm, the additional error in the spectral norm can be made less than A 2 F by sampling Θ(1/ 4 ) columns and rows, and the additional error in the Frobenius norm can be made less than A 2 F by sampling Θ(k 2 / 4 ) columns and rows. The results of <ref type="bibr" target="#b15">[16]</ref> require Θ(k 4 / 3 ) columns and rows for the Frobenius (and thus the spectral) norm bound.</p><p>Recent work has focused on developing new techniques for proving lower bounds on the number of queries a sampling algorithm is required to perform in order to approximate a given function accurately with a low probability of error <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. In <ref type="bibr" target="#b4">[5]</ref> these methods have been applied to the low-rank matrix approximation problem (defined as approximating the SVD with respect to the Frobenius norm) and to the matrix reconstruction problem. It is shown that any sampling algorithm that with high probability finds a good low-rank approximation requires Ω(m + n) queries. In addition, it is shown that even if the algorithm is given the exact weight distribution over the columns of a matrix, it will still require Ω(k/ 2 ) column queries to approximate A. Thus, the LinearTimeSVD algorithm (see also the original <ref type="bibr" target="#b9">[10]</ref>) is optimal with respect to Frobenius norm bounds for the rank parameter k and the Constant-TimeSVD algorithm (see also the original <ref type="bibr" target="#b15">[16]</ref>) is optimal with respect to Frobenius norm bounds up to polynomial factors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The LinearTimeSVD algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Diagram for the LinearTimeSVD algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The ConstantTimeSVD algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Diagram for the ConstantTimeSVD algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 ξ 2 ξ 2 ξLemma 1 .</head><label>2221</label><figDesc>, for ξ = 2, F , to A -B B T A plus an error term; we do so since the columns of B are orthonormal, which will allow us to bound A -B B T A using arguments similar to those used to bound A -H k H T k A For ξ = 2, F and for any &gt; 0,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>) since Z 1 , 2 = 1 . 2 F</head><label>1212</label><figDesc>The lemma follows since σ 2 t (W ) ≥ γ W for all t = 1, . . . , by the definition of . Third, we consider the second term in Lemma 1, B B T -H HT 2 ξ and show that it can be related to Δ ξ . Lemma 3. For ξ = 2, F B B T -H HT ξ = Δ ξ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Lemma 4 .</head><label>4</label><figDesc>Let A ∈ R m×n and let H be constructed from the Constant-TimeSVD algorithm by sampling c columns of A with probabilities {p i } n i=1 and w rows of C with probabilities {q j } m j=1 , where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 ≥ I 2 -</head><label>22</label><figDesc>using (39). From the triangle inequality HT H HT H -I 2 = |1 -Δ 2 | . (45)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>t=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Cz t z t T C T = t=1 σ 2 t</head><label>2</label><figDesc>) Inequality (52) follows since α, β ≤ 1 and (53) follows since y ∈ B and z ∈ B m-. To bound (53), let z ∈ B m-, |z| = 1; thenz T A 2 = z T (AA T )z = z T (CC T )z + z T (AA T -CC T )z = z T (CC T -CZ 1,k Z T 1,k C T )z + z T (CZ 1,k Z T 1,k C T )z + z T (AA T -CC T )z (54) = z T (CZ k+1,c Z T k+1,c C T )z + z T (CZ +1,k Z T +1,k C T )z + z T (AA T -CC T )z. (55) Equation (55) follows since I c = ZZ T = Z 1,k Z T 1,k + Z k+1,c Z T k+1,c and, since CZ 1, Z T 1, C T = t=1 (W ) ht ht T , implies that z T CZ 1, Z T 1, C T z = 0 (56)for z ∈ B m-. Thus, by combining (53) and (55)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 2+ 2 =</head><label>22</label><figDesc>C T CZ k+1,c 2 ≤ A -A k AA T -CC T 2 + 2 C T C -W T W 2 . Proof. First note that (57) Z T k+1,c C T CZ k+1,c 2 ≤ Z T k+1,c W T W Z k+1,c 2 + Z T k+1,c (C T C -W T W )Z k+1,c 2 . Since Z T k+1,c (C T C -W T W )Z k+1,c 2 ≤ C T C -W T W 2 Z k+1,c 2 C T C -W T W 2 and Z T k+1,c W T W Z k+1,c 2 = σ 2 k+1 (W ),it follows from (57) that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 = 2 F 2 2 ≤ A -A k 2 2 + 2 E 2 F 2 F</head><label>2222222</label><figDesc>C T C -W T W 2 and Z T +1,k W T W Z +1,k 2 = σ 2 +1 (W ),it follows from (60) thatZ T +1,k C T CZ +1,k 2 ≤ C T C -W T W 2 + σ 2 +1 (W ). (61)The lemma follows since σ 2 t (W ) &lt; γ W for all t = + 1, . . . , k.Now we combine these results in order to prove (36). Recall thatE AA T = AA T -CC T and E C T C = C T C -W T W . By combiningLemmas 10,<ref type="bibr" target="#b10">11</ref>, and 12, we have thatA -B B T A AA T 2 + 3 E C T C 2 + γ W that γ = /100, that • 2 ≤ • F , and that Δ 2 ≤ E C T C 2 /γ Wby Lemma 2; (36) follows by combining (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Summary of sampling complexity.</figDesc><table><row><cell>Additional error for:</cell><cell>LinearTimeSVD</cell><cell>ConstantTimeSVD</cell><cell>Ref. [16, 17]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>follows since α, β ≤ 1 and (19) follows since y ∈ H k and z ∈ H m-k . We next bound (19):</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and 1 ≤ k ≤ min(w, c), and {p i }</figDesc><table><row><cell>n i=1 such that p i ≥ 0 and</cell><cell>n i=1 p i = 1.</cell></row><row><cell cols="2">Output: σ t (W ), t = 1, . . . , and a "description" of H ∈ R m× .</cell></row><row><cell>1. For t = 1 to c,</cell><cell></cell></row><row><cell>(a) Pick i</cell><cell></cell></row></table><note><p>t ∈ 1, . . . , n with Pr [i t = α] = p α , α = 1, . . . , n, and save {(i t , p jt</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1/2 / W F ≤ 1. By combining Lemmas 6, 7, and 8, we have our desired bound on B T A 2 F in terms of the singular values of W . Finally, we use matrix perturbation theory to relate t=1 σ 2 t (W ) to</figDesc><table><row><cell>k t=1 σ 2 t (A).</cell></row><row><cell>Lemma 9.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We would like to thank the following individuals for comments and fruitful discussions: Dimitris Achlioptas, Alan Frieze, Mauro Maggioni, Frank McSherry, and Santosh Vempala. We would also like to thank Dimitris Achlioptas and Frank McSherry for providing us with a preprint of <ref type="bibr" target="#b0">[1]</ref>, i.e., the journal version of <ref type="bibr" target="#b1">[2]</ref>, which provides a useful comparison of their results with ours. We would like to thank the National Science Foundation for partial support of this work. Finally, we would like to thank an anonymous reviewer for carefully reading the paper and making numerous useful suggestions; in particular, the reviewer provided elegant, short proofs for Lemmas 2 and 6.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This author was supported in part by a grant from the NSF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast computation of low rank matrix approximations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast computation of low rank matrix approximations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Symposium on Theory of Computing</title>
		<meeting>the 33rd Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Singular value decomposition for genome-wide expression data processing and modeling</title>
		<author>
			<persName><forename type="first">O</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="page" from="10101" to="10106" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Bar-Yossef</surname></persName>
		</author>
		<title level="m">The Complexity of Massive Data Set Computations</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sampling lower bounds via information theory</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bar-Yossef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual ACM Symposium on Theory of Computing</title>
		<meeting>the 35th Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Matrices, vector spaces, and information retrieval</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Drmač</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Jessup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="335" to="362" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using linear algebra for intelligent information retrieval</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>O'brian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="573" to="595" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bhatia</surname></persName>
		</author>
		<title level="m">Matrix Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Soc. Inform. Sci</title>
		<imprint>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clustering in large graphs and matrices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vinay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Annual ACM-SIAM Symposium on Discrete Algorithms, SIAM, Philadelphia</title>
		<meeting>the 10th Annual ACM-SIAM Symposium on Discrete Algorithms, SIAM, Philadelphia</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="291" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast Monte-Carlo algorithms for approximate matrix multiplication</title>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the 42nd Annual IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="452" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pass efficient algorithms for approximating large matrices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the 14th Annual ACM-SIAM Symposium on Discrete Algorithms<address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication</title>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="132" to="157" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast Monte Carlo algorithms for matrices III: Computing a compressed approximate matrix decomposition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="184" to="206" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast Monte Carlo Algorithms for Matrices II: Computing a Low-Rank Approximation to a Matrix</title>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno>YALEU/DCS/TR-1270</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>New Haven, CT</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Yale University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast Monte-Carlo algorithms for finding low-rank approximations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the 39th Annual IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast Monte-Carlo algorithms for finding low-rank approximations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1025" to="1041" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Computations</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Johns Hopkins University Press</publisher>
			<pubPlace>Baltimore, MD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stable distributions, pseudorandom generators, embeddings and data stream computation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the 41st Annual IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two algorithms for nearest-neighbor search in high dimensions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual ACM Symposium on Theory of Computing</title>
		<meeting>the 29th Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="599" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the 9th Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="668" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bibby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Analysis</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Academic Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual learning and recognition of 3-d objects from appearance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latent semantic indexing: A probabilistic analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Symposium on Principles of Database Systems</title>
		<meeting>the 17th ACM Symposium on Principles of Database Systems</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Principal components analysis to summarize microarray experiments: Application to sporulation time series</title>
		<author>
			<persName><forename type="first">S</forename><surname>Raychaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific Symposium on Biocomputing</title>
		<meeting>the Pacific Symposium on Biocomputing</meeting>
		<imprint>
			<date type="published" when="2000">2000, 2000</date>
			<biblScope unit="page" from="455" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Matrix Perturbation Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Missing value estimation methods for DNA microarrays</title>
		<author>
			<persName><forename type="first">O</forename><surname>Troyanskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sherlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="520" to="525" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognitive Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="96" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Random projection: A new approach to VLSI layout</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the 39th Annual IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="389" to="395" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
