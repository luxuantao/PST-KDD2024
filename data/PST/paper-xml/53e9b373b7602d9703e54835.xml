<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Foveated Shot Detection for Video Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giuseppe</forename><surname>Boccignone</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Angelo</forename><surname>Chianese</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vincenzo</forename><surname>Moscato</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Picariello</surname></persName>
						</author>
						<title level="a" type="main">Foveated Shot Detection for Video Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C9C81D6CB67061C36D97278B20E03879</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Attentive Vision</term>
					<term>Video Segmentation</term>
					<term>Shot Detection</term>
					<term>Hard Cuts</term>
					<term>Dissolves</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We view scenes in the real world by moving our eyes three to four times each second, and integrating information across subsequent fixations (foveation points). By taking advantage of this fact, in this paper we propose an original approach to partitioning of a video into shots based on a foveated representation of the video. More precisely, the shotchange detection method is related to the computation, at each time instant, of a consistency measure of the fixation sequences generated by an ideal observer looking at the video. The proposed scheme aims at detecting both abrupt and gradual transitions between shots using a single technique, rather than a set of dedicated methods. Results on videos of various content types are reported and validate the proposed approach</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Detection of shot boundaries provides a base for nearly all video abstraction and high level video segmentation methods <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b26">[27]</ref>. In this paper, we propose a novel approach to partitioning of a video into shots based on a foveated representation of the video.</p><p>A shot is usually conceived in the literature as a series of interrelated consecutive frames taken contiguously by a single camera and representing a continuous action in time and space. In other terms, a shot is a subsequence generated by the camera from the time it "starts" recording images, to the time it "stops" recording <ref type="bibr" target="#b15">[16]</ref>. However, shot segmentation is illdefined. On the one hand, a video is generated by composing several shots by a process called editing, and due to edit activity different kinds of transitions from one shot to another, either abrupt or gradual, may take place. An abrupt transition, or hard cut, occurs between two consecutive frames and is the most common type. An example is provided in Fig. <ref type="figure">1</ref>. Fig. <ref type="figure">1</ref>. An example of hard cut effect from a TREC01 video. An abrupt transition occurs between the second and the third frame Gradual transitions such as fades, wipes and dissolves (see Fig. <ref type="figure" target="#fig_0">2</ref> below) are spread over several frames and are obtained using some spatial, chromatic or spatio-chromatic effect; these are harder to detect from a purely data analysis point of view because the difference between consecutive frames is smaller.</p><p>It has been observed <ref type="bibr" target="#b1">[2]</ref> from a study of video production techniques, that the production process originates several constraints, which can be useful for video edit classification in the framework of a model based approach to segmentation. But the use of such constraints implies high costs in designing shot models due to the high number of degrees of freedom available in shot production (for review and discussion, see <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b15">[16]</ref>). On the other hand, for the purposes of video retrieval, one would like to mark the case of any large visual change, whether camera stops or not (e.g., a large object entering the scene). Thus, from a general standpoint, shot detection should rely on the recognition of any significant discontinuity in the visual content flow of the video sequence <ref type="bibr" target="#b15">[16]</ref>. Meanwhile, the detection process should be unaffected by less significant changes within the same shot, like object/camera motion and lighting changes, which may contribute to missed or false detections. In such a complex scenario, despite the number of proposals in the literature, robust algorithms for detecting different types of boundaries have not been found, where robustness is related to both detection performance and stability with minimum parameter tuning <ref type="bibr" target="#b20">[21]</ref>.</p><p>At the heart of our ability to detect changes from one view of a scene to the next is the mechanisms of visual attention. Film makers have long had the intuition that changes to the visual details across cuts are not detected by audiences, particularly when editing allows for smooth transitions <ref type="bibr" target="#b50">[51]</ref>. In the movie Ace Ventura: When Nature Calls the pieces on a chess board disappear completely from one shot to the next. In Goodfellas a child is playing with blocks that appear and disappear across shots. In fact, almost every movie, and almost every cut, has some continuity mistake, yet, most of the time people are blind to these changes. It has been noted that change blindness is evident when mistakes occur far from the viewer's focus of attention <ref type="bibr" target="#b50">[51]</ref>.</p><p>The term attention captures the cognitive functions that are responsible for filtering out unwanted information and bringing to consciousness what is relevant for the observer <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Visual attention, in turn, is related to how we view scenes in the real world: moving our eyes (saccade) three to four times each second, and integrating information across subsequent fixations <ref type="bibr" target="#b59">[60]</ref>. Saccades represent overt shifts of spatial attention that can be performed either voluntarily (topdown), or induced automatically (bottom-up) by salient targets suddenly appearing in the visual periphery and allow an observer to bring targets of interest onto the fovea, the retinal region of highest spatial resolution. Eye movements, though being characterized by some degree of randomness <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b5">[6]</ref>, are likely to occur in a specific path (the scanpath, <ref type="bibr" target="#b36">[37]</ref>) so as to focus areas that are deemed important. The scanpath can be conceived as a visuomotor pattern resulting from the perceptual coupling of observer and observed scene. An example generated on the third frame of Fig. <ref type="figure">1</ref> is illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>. In the course of a scan we have a rich visual experience from which we abstract the meaning or gist of a scene. During next scan, if the gist is the same our perceptual system assumes the details are the same. Clearly, this "sketch" representation not only serves the information reduction purpose of filtering unwanted information, but also, by integrating the gist from one view to the next, to achieve the impression of a stable world. However, the lack of a detailed representation of the outside world from one view to the next can rise failures of change detection <ref type="bibr" target="#b50">[51]</ref>.</p><p>The background question which motivates this work is whether these mechanisms that are useful to prevent audiences noticing the transitions, can conversely be exploited to detect such transitions, and thus help for video segmentation. Intuitively, one could argue that if the playback speed is reduced (or, equivalently, the saccade rate increased) change blindness effects would be reduced too. This corresponds to introducing an ideal observer or agent, capable of tuning his saccadic rate. In some sense, this is akin to Gargi's experimental study of human ground-truthing, where most consistent results in marking shot changes were obtained when subjects performed such task at half speed after viewing the sequence once at full speed <ref type="bibr" target="#b15">[16]</ref>.</p><p>The rationale behind our approach is that perceptual capacity of an observer can be defined at two levels <ref type="bibr" target="#b37">[38]</ref>. At the first level there is the ability of the agent to explore the scene in ways mediated by knowledge of patterns of visuomotor behavior, that is the ability to exploit the interdependence between incoming sensory information and motor behavior (eye movements). At the second, higher level there is the accessing by the observer of information related to the nature of observer's own exploration.</p><p>For example, while viewing a video sequence, it is reasonable that in the presence of similar visual configurations, and in the absence of an habituation mechanism, an observer should consistently deploy attention to visually similar regions of interest and by following a similar motor pattern; clearly, when the gist of the world observed undergoes a significant change, the visuomotor pattern cannot be exploited further, since inconsistent, and a new scanpath will be generated. Such an intuitive assumption can be theoretically motivated on the basis that after an abrupt transition the video signal is governed by a new statistical process <ref type="bibr" target="#b27">[28]</ref>. Indeed, it has been shown <ref type="bibr" target="#b5">[6]</ref> that gaze-shift is strongly constrained by structure and dynamics of the underlying random field modelling the image. Quantitatively, if a measure M of attention consistency is defined, M should decrease down to a minimum value. For instance, this is what is likely to occur when a view abruptly changes.</p><p>On the other hand, a view change may occur across long delay intervals, as in gradual transitions. In this case, M should account for a behavior similar to that experienced in change blindness experiments, where subjects fail to detect a slow, global spatio-chromatic editing of a sequence presenting the same image <ref type="bibr" target="#b37">[38]</ref>, but suddenly succeed when the frame rate of presentation is increased, due to the reduction of the time lag between the first and the last frames of the transition. In this case the M function should vary smoothly across the interval, while decreasing rapidly if measured on the first and the last frames of the same interval. It is worth remarking that shots involved in a dissolve transition may have similar color distribution, which a color histogram would hardly detect <ref type="bibr" target="#b27">[28]</ref>, while differing in structural information that can be detected by appropriate algorithms (e.g., edge based); as in the case of hard cuts, the sequence of attention shifts can be suitably exploited, since its dynamics <ref type="bibr" target="#b5">[6]</ref> is strongly intermingled with the complexity of the statistical process modelling the signal (e.g., two-source model for a dissolve <ref type="bibr" target="#b27">[28]</ref>).</p><p>As regards the second level, namely the evaluation of information about the nature of visual exploration itself, it can be stated as an inference drawn by the observer from its own sensorimotor behavior under prior knowledge available. On such assumption, the problem of detecting a shot change given the change of the observer's behavior M, naturally leads to a Bayesian formulation, and can be conceived as a signal detection problem where the probability that a shot boundary B occurs, given a behavior M, is compared against the probability that a shot boundary is not present.</p><p>The introduction of this approach has several advantages, both theoretical and practical. First it allows to find a uniform method for treating both abrupt and gradual transitions. As discussed above, this result stems from relations occurring between the dynamics of gaze-shifts and statistical processes modelling the observed image <ref type="bibr" target="#b5">[6]</ref>; also, the method is well grounded in visual perception theories <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. As such, it is suitable to overcome usual shortcomings of other simpler techniques proposed so far (e.g, histogram manipulations). In this sense, higher robustness can be achieved, as regards per-formance and stability in detecting important visual changes while discarding negligible ones. Then, once the distinctive scanpath has been extracted from a frame, subsequent analysis needs only to process a sparse representation of the frame. Eventually, attentive analysis can, in perspective, provide a sound and unitary framework at higher levels of video content analysis. For instance, key frame selection/generation could be conceived in terms of average scanpath of shot frames; multimodal processing for deriving semantic properties of a scene, can be stated in terms of attentive audio/visual integration.</p><p>In Section 2, we briefly discuss background and related work on shot segmentation. In Section 3, we outline the model for foveated analysis of a video sequence. In Section 4 the computation of patterns of visuomotor behavior is discussed. In Section 5 we derive the procedure to calculate the M function and the boundary detection algorithm. Sections 6 presents the experimental protocol and results obtained. Some concluding remarks are given in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND RELATED WORK</head><p>Assume as input to a segmentation system a video sequence, that is a finite sequence of time parameterized images, (f (t 0 ), f (t 1 ), . . . f (t N )), where each image f (t n ) is called a frame. Each frame is a color image, namely a mapping from the discrete image support Ω ⊆ Z 2 to an m-dimensional range, f : Ω → Q ⊆ Z m ; in other terms, it is a set of singlevalued images, or channels, sharing the same domain, i.e., f (x, y) = (f i (x, y)) T , where the index i = 1, .., m, defines the i-th color channel and (x, y) denotes a point in the Ω lattice. Q = {q 1 , ..., q N } is the set of colors used in the image. Each frame displays a view, a snapshot, of a certain visual configuration representing an original world scene.</p><p>A time segmentation of a video f defined on the time interval [t 0 , t N ] is a partition of the video sequence into N b subsequences or blocks. One such partition can be obtained in two steps. First, a mapping T : Z m → F of the frame f (t n ) ∈ Z m to a representation T (f (t n )) ∈ F , F being a suitable feature space, is performed. Then, given two consecutive frames f (t n ) and f (t n+l ), where l ≥ 1 is the skip or interframe distance, a discriminant function D : F × F → R + is defined to quantify the visual content variation between T (f (t n )) and T (f (t n+l )), such that a boundary occurs at frame</p><formula xml:id="formula_0">f (t n ) if D(T (f (t n )), T (f (t n+l ))) &gt; T , where T is a suitable threshold.</formula><p>Thus, in principle, to solve the shot detection problem three steps must be undertaken: choose an appropriate mapping T ; define a robust discriminant function D; devise a (universal) threshold T .</p><p>As regards the first two points, different techniques have been used: pixel based methods, such as the mean absolute value of intensity between frames <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b38">[39]</ref>, or block matching <ref type="bibr" target="#b49">[50]</ref> [21], histograms difference <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b63">[64]</ref> motion difference <ref type="bibr" target="#b62">[63]</ref> and perceived motion energy <ref type="bibr" target="#b30">[31]</ref>, differential geometry <ref type="bibr" target="#b57">[58]</ref>.</p><p>For what concerns the third point, heuristically chosen thresholds have been proposed <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b38">[39]</ref>. However a fixed thresholding approach is not feasible especially when considering gradual transitions. In particular, dissolve effects are reputed the most common ones, but also the most difficult to detect <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b56">[57]</ref>. A dissolve can be obtained as a combination of fade-out and fade-in, superimposed on the same film strip; fade-out occurs when the visual information gradually disappears, leaving a solid color frame, while fade-in takes place when the visual information gradually appears from a solid color frame (refer again to Fig. <ref type="figure" target="#fig_0">2</ref>).</p><p>Dissolve detection is still a challenging problem. Few techniques have been published <ref type="bibr" target="#b28">[29]</ref>. Variable thresholding has been proposed in <ref type="bibr" target="#b60">[61]</ref> and <ref type="bibr" target="#b63">[64]</ref>, the latter relying on gaussian distribution of discontinuity values. For instance in <ref type="bibr" target="#b63">[64]</ref> the twin-comparison approach using a pair of thresholds, for detecting hard cuts and gradual transitions, respectively, has been introduced. More significant improvements have been achieved by recasting the detection problem in a statistical framework. A novel and robust approach has been presented by Lienhart <ref type="bibr" target="#b29">[30]</ref>, which relies on multiresolution analysis of time series of dissolve probabilities at various time scales; experiments achieved a detection rate of 75% and a false alarm rate of 16% on a standard test video set. Further, it has been recently argued that a statistical framework incorporating prior knowledge in model based <ref type="bibr" target="#b19">[20]</ref>, statistical approaches leads to higher accuracy for choosing shot boundaries <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUTLINE OF THE MODEL FOR FOVEATED VIDEO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ANALYSIS</head><p>The evaluation of attention consistency relies on the model of foveated analysis outlined in Fig. <ref type="figure">4</ref>.</p><p>In the preattentive stage, salient features are extracted by specific detectors operating in parallel for all points of the image, at different spatial scales, and organized in the form of contrast maps. In order to obtain such a representation different methods can be adopted, e.g., <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b13">[14]</ref>. It is worth remarking, that the model presented in this paper is unaffected by the method chosen to implement such preattentive stage. We experimented with schemes proposed in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and opted for the latter due to simplicity and limited computational complexity. Precisely, low-level vision features are derived from the original color image decomposed at several spatial scales using Gaussian <ref type="bibr" target="#b7">[8]</ref> and oriented pyramids (via convolution with Gabor filters <ref type="bibr" target="#b18">[19]</ref>). Note that pyramid computation is an O(|Ω|) method, where |Ω| represents the number of samples in the image support Ω.</p><p>The features considered are: brightness (I); color channels tuned to red (R), green (G), blue (B) and yellow (Y) hues; orientation (O). From color pyramids, red/green (RG) and blue/yellow (BY) pyramids are derived by subtraction. Then, from each pyramid a contrast pyramid is computed encoding differences between a fine and a coarse scale for a given feature. As a result, one contrast pyramid encodes for image intensity contrast, four encode for local orientation contrast, and two encode for red/green (RG) and blue/yellow (BY) contrast (see <ref type="bibr" target="#b21">[22]</ref>, for details).</p><p>Successively, this preattentive representation, undergoes specialized processing through a "Where" system devoted to Fig. <ref type="figure">4</ref>. A general model of attentive/foveated video analysis. At a lower level, the observer generates visuomotor patterns, related to the content of the sequence. At a higher level, the observer detects scene changes by judging his own visuomotor behavior in the context of prior knowledge available localizing objects, and a "What" system tailored for identifying them. Clearly, tight integration of these two information pathways is essential, and indeed attentive mechanisms play a fundamental role. A plausible assumption is that, in the "What" pathway, early layers provide feature extraction modules, whose activity is subjected to temporal modulation by the "Where" pathway and the related attention shifting mechanism, so that unmodulated responses are suppressed.</p><p>In the "Where" pathway, the preattentive contrast maps are combined into a master or saliency map <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b0">[1]</ref>, which is used to direct attention to the spatial location with the highest saliency (attention shifting stage). The region surrounding such location represents the current focus of attention (FOA), C s . By traversing spatial locations of decreasing saliency, a scanpath (C s ) s=1,2,.. is obtained by connecting a sequence of FOAs, and stored.</p><p>It is important to note that, in general and specifically in this work, such "working memory" retains either a representation of a set of visual features (measured at FOAs) and a motor map of how such features have been explored; indeed, the memory of an attentive system is a visuomotor trace of a world view <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b17">[18]</ref>, rather than a classical feature representation of the original scene, and any subsequent information-lookup task entails a prediction/confirmation upon such visuomotor scheme. Denote T (f (t)) the visuomotor trace (simply, the trace) of frame f (t).</p><p>At the higher perceptual level, the observer infers scene changes by judging his own visuomotor behavior. To this end, given two frames f (t) and f (t + l) (for notational simplicity, t = t n ), an effective procedure is needed to compute the function M(t)) which gauges the consistency between the two traces T (f (t)) and T (f (t + l)). A way to solve this problem, is suggested by experiments performed by Walker and Smith <ref type="bibr" target="#b58">[59]</ref>, who provided evidence that when observers are asked to make a direct comparison between two simultaneously presented pictures, a repeated scanning, in the shape of a FOA by FOA comparison, occurs. Using this procedure, which we name information look-up loop, the consistency M will eventually be calculated as the average of the local consistencies measured on pairs of FOAs, iteratively selected from the two traces T (f (t)) and T (f (t + l)) according to a "best fit" prediction strategy. The behavior of the M function, is then used by a detection module, based on Bayesian decision theory, which, under prior contextual knowledge available, infers from M the presence of a scene transition, either abrupt or gradual.</p><p>In sections IV and V, we detail how the different levels of our attentive system have been designed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LOW PERCEPTUAL LEVEL: GENERATION OF VISUOMOTOR PATTERNS</head><p>At this lower perceptual level, the agent observes the sequence and generates patterns of visuomotor behavior (traces).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Where system: from preattentive features to attention shifting</head><p>The goal of the "Where" system is to build a saliency map of the frame and define over this map the motor trace, that is the sequence of FOAs (C s ) s=1,2,...,K . To this end, the contrast features for intensity, color and orientation, obtained from the preattentive stage, are summed across scales (pyramid levels) into three separate contrast maps, one for intensity, one for color and one for orientation. Eventually, the three maps, normalized between 0 and 100, are linearly summed into a unique master map (for simplicity, we compute the latter as the average of the three maps), or saliency map (SM).</p><p>By using the SM map, the attention shifting mechanism could be implemented through a variety of ways (e.g., <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b53">[54]</ref>). One intuitive method for traversing spatial locations of decreasing saliency, is to use a winnertake-all (WTA) strategy <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b53">[54]</ref>, in which the most salient location "wins" and determines the setting of the FOA; the winner is subsequently inhibited in order to allow competition among less salient locations, for predicting the next FOA. A simple and efficient way of implementing such strategy is through a WTA neural network, e.g. an array of integrateand-fire neurons with global inhibition <ref type="bibr" target="#b21">[22]</ref>. It is worth noting that a WTA algorithm, due to fast convergence properties, has O(n) time complexity, n being the number of processing elements (neurons) of the network. In our scheme the number of neurons is constrained by the number of samples in the saliency map (each point of the map, represents the input to one neuron) . Since the map resides at an intermediate scale between the highest and the lowest resolution scales, namely at scale 4, a reduction factor 1:16 is achieved with respect to the original image, thus the time complexity of the WTA stage is given by |Ω|/16 time units.</p><p>This solution has the advantage of providing information on the fixation time spent on the FOA (the firing time of WTA neurons) and our model, differently from others proposed in the literature, explicitly exploits such information. After the "Where" processing, the frame f (t) is represented by a spatiotemporal, or motor trace representing the stream of foveation points (C t s (p s ; τ s )) s=1,2,...,K , where p s = (x s , y s ) is the center of FOA s, and the delay parameter τ s is the observation time spent on the FOA before a saccade shifts to C s+1 . As outlined in Fig. <ref type="figure" target="#fig_2">5</ref> the generation of spatio-temporal information is basically an information reduction step in which we assume that the "Where" system "projects" towards the "What" system and signals the F OA to be analyzed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The What pathway: properties encoding</head><p>In the "What" pathway, features are extracted from each highlighted F OA, relative to color, shape and texture. A F OA is represented in the intensity and color opponent pyramids, at the highest resolution scale. Note that in biological vision, the spatial support of the FOA is usually assumed as circular; here, for computational purposes, each FOA is defined on a square support D ps ⊆ Ω, centered on p s , of dimension |D p s | = 1  36 |Ω|. In the following we drop the τ parameter for sake of simplicity.</p><p>Color features. Given a set of representative colors</p><formula xml:id="formula_1">Q = {q 1 , ..., q B }, a color histogram h(C(p)) = {h b } of the FOA C(p) is defined on bins b ranging in [1, B],</formula><p>such that h b given for any pixel in D p , is the probability that the color of the pixel is q b ∈ Q. Here, B = 16 × 16 × 16 is used. For a three channel frame, the FOA histogram calculation time is |D p |×3.</p><p>Shape and texture features. A wavelet transform (W T ) of the FOA has been adopted <ref type="bibr" target="#b31">[32]</ref>. Denote the wavelet coefficients as w k l (x, y), where (x, y) ∈ D p , l indicates the decomposition level and k indexes the sub-bands. In our case, due to the limited dimension of the FOA, only a first level decomposition (l = 1) is considered, and in the sequel, for notational simplicity the index l is dropped. Decomposition gives rise to 4 subregions of dimension |D p |/4. Then, only the details components of the W T are taken into account, in order to characterize shape (edges) and texture. Namely, for k = 1, 2, 3, the detail sub-bands contain horizontal, vertical and diagonal directional information, respectively, and are represented by coefficient planes w k (x, y) k=1,2,3 . Next, the Wavelet Covariance Signature is computed, i.e. the feature vector of coefficient covariances Σ 2</p><formula xml:id="formula_2">C m s = {σ 2</formula><p>X,Y }, where:</p><formula xml:id="formula_3">σ 2 X,Y = x,y { 1 |D p |/4 3 k=1 X k (x, y)Y k (x, y)}.</formula><p>(1)</p><p>The pair (X k , Y k ) is in the set of coefficient plane pairs {(w k i , w k j )}, i and j being used to index the three channels, and (x, y) span over the sub-band lattice of dimension |D p |/4. Note that, the FOA wavelet representation at level 1 can be obtained through 2h|D p | operations, where h is the size of convolution filters (here h = 3) <ref type="bibr" target="#b31">[32]</ref>, while calculation of covariances can be accomplished in |D p | 2 operations. Clearly,</p><formula xml:id="formula_4">|Σ 2 | = 18.</formula><p>As summarized in Fig. <ref type="figure" target="#fig_2">5</ref>, the saccadic movements together with their resultant fixations, and feature analysis of foveated regions, allow the formation of the trace T (f (t)), briefly T (t), of the view observed in frame f (t):</p><formula xml:id="formula_5">T (t) = (T t s ) s=1,...,K<label>(2)</label></formula><p>where</p><formula xml:id="formula_6">T t s = (C t s , h b (C t s ), Σ 2 C t s</formula><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. HIGH PERCEPTUAL LEVEL: INFERENCE OF SCENE CHANGES FROM VISUOMOTOR BEHAVIOR</head><p>At this level, the observer evaluates the information regarding the nature of visual exploration itself and infers the presence of a shot boundary from its own sensorimotor behavior under prior knowledge available on the kinds of transitions he is dealing with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Attention consistency via information look-up</head><p>An agent observing views that present similar configurations of objects, will generate consistent traces until a transition occurs. An example of such behavior is provided in Fig. <ref type="figure" target="#fig_3">6</ref> where a trace of three FOAs is tracked in a subsequence embedding a hard cut. Formally, we need to define a measure of consistency M :</p><formula xml:id="formula_7">F × F → R + , such that M(T (m), T (n))</formula><p>, where the traces T (m) and T (n) have been generated by observing frames f (t m ) = f (m) and f (t m + l) = f (n). A strategy to solve this problem is to make a FOA by FOA comparison <ref type="bibr" target="#b58">[59]</ref>. This information look-up loop is summarized in the scheme of Fig. <ref type="figure" target="#fig_4">7</ref>. </p><p>where α, β, γ ∈ [0, 1], and by choosing the FOA s as s = arg max{M r,s }. Such "best fit" is retained and eventually used to compute M(T (m), T (n)) as the average of the first K FOA consistencies:</p><formula xml:id="formula_9">M = 1 K K f =1 M r,s f . (<label>4</label></formula><formula xml:id="formula_10">)</formula><p>This "best fit" strategy has been chosen in order to reduce the sensitivity of the algorithm both to the starting FOA point and to the fact that, in similar images, some FOAs could be missing due to lighting changes and noise, even if this is unlikely to occur for small interframe distances. Right-hand terms of Eq. 3, namely M r,s spatial , M r,s temporal , M r,s visual , account for local measurements of spatial temporal and visual consistency, respectively. These are calculated as follows.</p><p>Local spatial consistency. M r,s spatial is gauged through the 1 distance between homologous F OAs's centers:</p><formula xml:id="formula_11">d(p r , p s ) = |x r -x s | + |y r -y s |. (<label>5</label></formula><formula xml:id="formula_12">)</formula><p>The distance is "penalized" if, for the two frames, the displacement between the current F OA and the next one is not in the same direction:</p><formula xml:id="formula_13">d(p r , p s ) = d(p r , p s ) • e -∆(p r ,p s ) ,<label>(6)</label></formula><p>∆ being the difference of direction between two F OAs,</p><formula xml:id="formula_14">∆ = ζ•sgn[(x r -x r-1 )•(x s -x s-1 )]•sgn[(y r -y r-1 )•(y s -y s-1 )],<label>(7)</label></formula><p>where ζ is a penalization constant. Thus, after d(p r , p s ) normalization:</p><formula xml:id="formula_15">M r,s spatial = 1 -d(p r , p s ). (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>Local temporal consistency. M r,s temporal takes into account the difference of time that the observer gazes at two different fixation points. To this end the 1 distance is introduced:</p><formula xml:id="formula_17">d(τ r , τ s ) = |τ r -τ s |. (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>The distance measured in Eq. 9 is normalized with respect to the maximum fixation time of the scanpath. Then temporal consistency is calculated as</p><formula xml:id="formula_19">M r,s temporal = 1 -d(τ r , τ s ).<label>(10)</label></formula><p>Local visual consistency. M r,s visual is defined using either color and texture/shape properties. Evaluation of consistency in terms of color is performed by exploiting well known histogram intersection, which again is an 1 distance on the color space <ref type="bibr" target="#b52">[53]</ref>. Given the two color histograms h(C m r ) and h(C n s ), defined on the same number of bins b = [1, . . . , B],</p><formula xml:id="formula_20">d r,s col = B b=1 (min(h b (C m r ), h b (C n s )))/ B b=1 h b (C m r ),<label>(11)</label></formula><p>where</p><formula xml:id="formula_21">B b=1 h b (C m r ) is a normalization factor. Then, M r,s col = 1 -d r,s col . (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>Computational complexity for the histogram analysis part is proportional to the number of bins in the histogram, thus taking B time units. Shape and texture consistency is measured as</p><formula xml:id="formula_23">M r,s tex = 1 - 1 R |Σ 2 | i=1 |Σ 2 C m r [i] -Σ 2 C n s [i]| min(|Σ 2 C m r [i]|, |Σ 2 C n s [i]|) , (<label>13</label></formula><formula xml:id="formula_24">)</formula><p>where R is a normalization factor to bound the sum in [0, 1], and |Σ 2 | the number of features in the feature vector Σ 2 computed through Eq. 1. Eventually, F OA's visual content consistency is given from the weighted mean of terms calculated via Eqs. 12 and 13:</p><formula xml:id="formula_25">M r,s visual = µ 1 M r,s col + µ 2 M r,s tex . (<label>14</label></formula><formula xml:id="formula_26">)</formula><p>The computation cost of Eq. 3 is approximately linear in the number of histogram bins B, since |Σ 2 | = 18, and Eqs. 8 and 10, are performed in constant time units. Thus, the algorithm (cfr. Eq. 4) requires (2H + 1)BK operations, which means that, once H and B have been fixed as in our case, the AC algorithm is linear in the number of FOAs K; in particular, a value of H = 2 for the best fit window provides suitable results. The value of K = 10 was chosen either because, in this way, each F OA is only visited once, and for the bottomup importance of earliest FOAs <ref type="bibr" target="#b39">[40]</ref>. For what concerns the setting of equation parameters, considering again Eq. 3, we simply use α = β = γ = 1/3, granting equal informational value to the three kinds of consistencies; similarly, we set µ 1 = µ 2 = 1/2 in Eq.14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Using attention consistency and prior knowledge for detecting shot transitions</head><p>The observer's behavior can be formalized as the attention consistency gauged over subsequences of the video sequence f . To this end, let us generalize the local attention consistency measure M to a parametrized family M : F × F × N + → R + , which accounts for the attentive behavior over the full sequence f , namely (M(T (i), T (i + l))) i=0,l,...,N/l .</p><p>In such framework, the problem of inferring a shot change given the change of observation behavior M(t) can be conceived as a signal detection problem where the probability that a shot boundary B occurs, given a behavior M(t), P (B|M(t)), is compared against the probability that a shot boundary is not present, P ( B|M(t)). More precisely, the observer's judgement of his own behavior can be shaped in a Bayesian approach where detection becomes the inference between two hypotheses:</p><p>• H 0 : no shot boundary occurs between the two frames under analysis ( B) • H 1 : a shot boundary occurs between the two frames (B) In this setting the optimal decision is provided by a test where H 1 is chosen if p(M(t)|B)P (B) &gt; p(M(t)| B)P ( B) and H 0 is chosen, otherwise. Namely a cut occurs if:</p><formula xml:id="formula_27">L(t) &gt; P ( B) P (B) = 1 -P (B) P (B)<label>(15)</label></formula><p>where L(t) = p(M(t)|B) p(M(t)| B) represents a likelihood ratio. In general, the prior shot probability P (B) models shot boundaries as arrivals over discrete, nonoverlapping temporal intervals, and a Poisson process seems an appropriate prior <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b20">[21]</ref>, which is based on the number of frames elapsed since the last shot boundary. Hanjalic has suggested <ref type="bibr" target="#b20">[21]</ref> that the prior P (B) should be more conveniently corrected by a factor depending upon the structural context of the specific shot boundary, gauged through a suitable function.</p><p>It is possible to generalize this suggestion resorting to contextual Bayesian analysis <ref type="bibr" target="#b45">[46]</ref> in which an occurrence of the property B is detected by taking into account the behavior M(t) given a context E, that is a set of events</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>., e n } characterizing B. Namely, H 1 is chosen if p(M(t)|B, E)P (S|E) &gt; p(M(t)| B, E)P ( B|E).</head><p>Thus, a cut is detected according to the likelihood ratio</p><formula xml:id="formula_28">L(t) &gt; 1 -P (B|E) P (B|E) , (<label>16</label></formula><formula xml:id="formula_29">)</formula><p>where now the r.h.s. of Eq. defines the adaptive threshold:</p><formula xml:id="formula_30">T (t) = 1 -P (E|B)P (B) P (E|B)P (B) . (<label>17</label></formula><formula xml:id="formula_31">)</formula><p>The prior probability P (B) models the Poisson process of boundary arrival according to the cumulative probability</p><formula xml:id="formula_32">P (B) = 1 2 • λ(t) w=0 µ w</formula><p>w! exp(-µ) <ref type="bibr" target="#b20">[21]</ref>. As regards P (E|B), under weak coupling assumption <ref type="bibr" target="#b61">[62]</ref> of structural events e 1 , e 2 , ..., e n , we can set P (e 1 , e 2 , ..., e n |B) = i P (e i |B). The events that constitute the structural context can be described as follows.</p><p>Consider the behavior of function M for both abrupt and gradual transitions. An example is depicted in Fig. <ref type="figure" target="#fig_5">8</ref> related to a video sequence characterized by the presence of two hard cuts embedding a dissolve. The first event we deal with is a shape event: when the gist of the world observed abruptly changes (hard cut), M decreases down to a minimum value.</p><p>Thus, as regards hard cuts, to calculate the probability P (E|B), we use a sliding window of dimension W = 10, centered on the frame f (t), thus including all frames in the temporal interval [t -W 2 , t + W 2 ], chosen with the interframe distance l = 5. For each frame, we consider the probability that the difference between the first minimum of M, M min1 , and the second minimum M min2 detected within the temporal window, be significant</p><formula xml:id="formula_33">P (E|B) = P (shape|B cut ) = 1 1 + exp(β δ) (<label>18</label></formula><formula xml:id="formula_34">)</formula><p>where δ represents the normalized difference (M min1 -M min2 )/M min1 . On the contrary, during a dissolve, the difference between consecutive frames is reduced, and a frame is likely to be similar to the next one. Thus, the consistency function will vary smoothly across the transition interval. Indeed, the behavior of M along a dissolve region is of parabolic type, and can be more precisely appreciated in Fig. <ref type="figure">9</ref>, where M(t) decreases very slowly till a local minimum point (fade-out effect), then slowly increases (fade-in effect). Fig. <ref type="figure">9</ref>. Attention consistency M in a dissolve region and its parabolic fitting A second event, which we denote dM, stems from the fact that the first derivative function of M is approximately constant and about zero in those frames characterized by dissolve effects (see Fig. <ref type="figure">10</ref>). Clearly, previous events are not Fig. <ref type="figure">10</ref>. First derivative of M(t) in the same region shown in Fig. <ref type="figure">9</ref> sufficient to completely characterize the context of a dissolve region: in fact M could exhibit a similar trend, e.g. in shots featuring a slow zoom. Thus, the inconsistency between the edge frames, that is the first and last frames of an hypothetical dissolve region, must be taken into account. We denote this event a change event.</p><p>Summing up, in the case of dissolves we can assume:</p><formula xml:id="formula_35">P (E|B) = P (shape|B dis )P (dM|B dis )P (change|B dis )<label>(19)</label></formula><p>To calculate the probability P (E|B), we use a sliding window of dimension W = 20, centered on the frame f (t), which includes all frames in the temporal interval</p><formula xml:id="formula_36">[t -W 2 , t + W 2 ],</formula><p>chosen with the interframe distance l = 5. The first term on the r.h.s. of Eq.19 is defined as</p><formula xml:id="formula_37">P (shape|B dis ) = 1 1 + exp(β (d minP )) , (<label>20</label></formula><formula xml:id="formula_38">)</formula><p>where d minP represents the distance between the absolute minimum of M within the temporal window and the minimum of the parabolic fitting performed on M values occurring in the same window.</p><p>The second term P (dM|B dis ) accounting for the probability that derivative dM dt be close to zero, is modelled as</p><formula xml:id="formula_39">P (dM|B dis ) = exp(-k| dM dt -µ|), (<label>21</label></formula><formula xml:id="formula_40">)</formula><p>where µ is the mean value of dM dt within the time window. To compute derivatives, the M curve is preprocessed via median filtering <ref type="bibr" target="#b40">[41]</ref> in order to avoid noise boost-up.</p><p>The third term P (change|B dis ), representing the probability that the first and the last frame of the dissolve be different, is given by</p><formula xml:id="formula_41">P (change|B dis ) = 1 - 1 1 + exp(-β(M(T (f start ), T (f end )) -δ )) , (<label>22</label></formula><formula xml:id="formula_42">)</formula><p>where f start and f end are the first and last frame of the sliding window,</p><formula xml:id="formula_43">f start = f t-W 2 and f end = f t+ W 2</formula><p>respectively. The variation δ is defined as</p><formula xml:id="formula_44">δ = M min + (M max -M min )/2<label>(23)</label></formula><p>where M min and M max represent the absolute minimum and maximum values of the M function within the window, respectively.</p><p>The likelihood in Eq.16 is estimated, on training sequences, by computing the histograms of the M(t) values within a shot and at its boundaries, respectively; then, ideal distributions are derived in non parametric form through Parzen windows <ref type="bibr" target="#b11">[12]</ref> using kernels ξ(1 -M) exp(-(1 -M)) (boundaries) and 1 σ √ 2π exp(-((1 -M) -µ) 2 /2σ 2 ) (within shot) , where ξ = 2.5, µ = 1.1, σ = 0.4, are the estimated parameters. Eventually, the decision module can be outlined as in Fig. <ref type="figure" target="#fig_6">11</ref>. The input is represented by the M(t) sequence computed by applying the AC algorithm on the video sequence, together with contextual knowledge. Boundary detection is accomplished according to a two-step procedure, which we denote Inconsistency Detection (ID).</p><p>In a first step abrupt transitions are detected by means of Eqs. <ref type="bibr">16, 17, 18.</ref> At the end of this phase we obtain the positions of hard cuts, which partition the original video in a sequence of blocks representing candidate shots.</p><p>In a second step, the frames interested in dissolve effects are detected. For each block, dissolve regions are individuated by means of Eqs. <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22</ref>, computed through a sliding window centered on each frame of the block, chosen according to an interframe distance l = 5. Eventually, the output of the system is represented by the list of shot boundary positions, defining the shot segmentation of the original video.</p><p>The first step of the ID algorithm has complexity O(N/l), N being the number of frames of the video sequence. The second step is O(W N b L b /l), where W, N b , L b are the dimension of the sliding window, the number of blocks partitioned along the first step, and the maximum block length, respectively.</p><p>The dimensions of the sliding windows have been chosen by means of an analysis of ROC curves obtained for the training set in order to maximize true detections with respect to false alarms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS AND RESULTS</head><p>To evaluate the performance of the proposed shot detection algorithm, a database of video sequences has been obtained from documentaries and news belonging to TREC01 video repository and from famous movies. The database represents a total of 1694 cuts and 440 dissolves in approximately 166 min. of video. The selected sequences are complex with extensive graphical effects. Videos were captured at a rate of 30 frames/sec, 640 × 480 pixel resolution, and stored in AV I format. These video sequences are also characterized for presenting significant dissolve effects. For each sequence a ground-truth was obtained by three experienced humans using visual inspection <ref type="bibr" target="#b15">[16]</ref>.</p><p>To obtain an estimate of parameters for detection, the training set, shown in Table <ref type="table">I</ref>, has been used. Experiments for performance evaluation were carried out on a test set including a total of 1304 cuts and 336 dissolves in 130 min. of video, which is summarized in Table <ref type="table">II</ref>.</p><p>The comparison between the proposed algorithm's output and the ground truth relies on the well know recall and precision figures of merit <ref type="bibr" target="#b15">[16]</ref>: </p><formula xml:id="formula_45">recall = detects/(detects + M D) (<label>24</label></formula><formula xml:id="formula_46">)</formula><formula xml:id="formula_47">precision = detects/(detects + F A) (<label>25</label></formula><formula xml:id="formula_48">)</formula><p>where detects denotes the correctly detected boundaries, while M D and F A denote missed detections and false alarms, respectively. In other terms, at fixed parameters, recall measures the ratio between right detected shot changes and total shot changes in a video, while precision measures the ratio between right detected shot changes and the total shot changes detected by algorithm.</p><p>Results obtained are provided in Tables III and IV and summarized in Table <ref type="table">V</ref>. The proposed method achieves a 97% recall rate with a 95% precision rate on abrupt transitions, and a 92% recall rate with a 89% precision rate on gradual transitions (Table <ref type="table">V</ref>). In order to provide an idea about the quality of this results, we refer to the discussion published by Hanjalic <ref type="bibr" target="#b20">[21]</ref>. In particular, on dissolve detection, it is worth comparing with Lienahrt <ref type="bibr" target="#b29">[30]</ref> and works therein reported, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b62">[63]</ref>.</p><p>Also, Table <ref type="table">V</ref> provides results in terms of the F 1 metric, </p><formula xml:id="formula_49">F 1 = 2 × precision × recall/(precision + recall)</formula><p>, which is commonly used to combine precision and recall scores <ref type="bibr" target="#b47">[48]</ref>, F 1 being high only when both scores are high. Summing up, the method proposed here achieves an overall average F 1 performance of 0.91 when considering both kinds of transitions. This result can indicatively be compared to the performance of a recently proposed method <ref type="bibr" target="#b47">[48]</ref> that uses global and block wise histogram differences, camera motion likelihood, followed by k-nearest neighbor classification. Such method achieves an F 1 performance of 0.94 and 0.69, for hard cuts and gradual transitions, respectively, resulting in an average performance of 0.82; interestingly enough, this result is higher than average scores (0.82 and 0.79) obtained by the two best performing systems at 2001 TREC evaluation <ref type="bibr" target="#b47">[48]</ref>. It is worth noting that, in our case, the overall score of 0.91 also accounts for results obtained by processing movies included in our test set, which eventually resulted to be the most critical. For completeness sake, by taking into account only TREC01 video sequences, the overall performance of our method is 0.925.</p><p>As regards the efficiency of the method, recall that to obtain the visuomotor trace of the frame, main effort is spent on pyramid and WTA computation, which can be estimated as an O(|Ω|) step, where |Ω| represents the number of samples in the image support Ω, while FOA analysis involves lower time complexity, since each of the K FOAs is defined on a limited support with respect to the original image (1/36|Ω|) and only 10 FOAs are taken into account to form a trace. The AC algorithm is O(K), that is linear in the number of FOAs. The first step of ID algorithm has complexity O(N/l), N and l being the number of frames of the video sequence and the interframe distance, respectively. The second step is O(W N b L b /l), where W, N b , L b are the dimension of the sliding window, the number of blocks partitioned along the first step, and the maximum block length, respectively. From this analysis, by considering operations performed on a single frame, we can expect that most of the time will be spent in the low-level perception stage, while the AC and ID algorithms will have higher efficiency, the former only performing on a sparse representation of the frame (K = 10) and the latter working on M(t) values of the sliding window of dimension W . This is experimentally confirmed from the results obtained and reported in Table <ref type="table">VI</ref>. The system achieves a processing speed per frame of about 35 ms on the Pentium IV 2.4 GHz PC (1 GB RAM). It is worth noting that the current prototype has been implemented using the Java programming language, running in Windows XP operating system, without any specific optimization.</p><p>Clearly, for time critical applications, the bottleneck of the proposed method, that is the computing of visuomotor traces, could be easily reduced by resorting to existing hardware implementation of pyramidal representations ( <ref type="bibr" target="#b8">[9]</ref>) and more efficient realizations of the WTA scheme (e.g., in <ref type="bibr" target="#b3">[4]</ref> a network is presented, which has O(lg n) time complexity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we have defined a novel approach to partitioning of a video into shots based on a foveated representation of the video. To the best of our knowledge, foveation mechanisms have never been taken into account for video segmentation, while there are some recent applications to video compression (refer to <ref type="bibr" target="#b25">[26]</ref>). The motivation for the introduction of this approach stems from the fact that success or failure in the perception of changes to the visual details of a scene across cuts are related to the attentive performance of the observer <ref type="bibr" target="#b50">[51]</ref>. By exploiting the mechanism of attention shifting through saccades and foveations, the proposed shot-change detection method computes, at each time instant, a consistency measure M(t) of the foveation sequences generated by an ideal observer looking at the video. The problem of detecting a shot change given the change of consistency M(t) has been conceived as a Bayesian inference of the observer from his own visual behavior.</p><p>The main results achieved can be summarized as follows. The proposed scheme allows the detection of both cuts and dissolves between shots using a single technique, rather than a set of dedicated methods. Also, it is well grounded in visual perception theories and allows to overcome usual shortcomings of many other techniques proposed so far. In particular, features extracted are strictly related to the visual content of the frame; this, for instance is not true for simpler methods, such as histogram based methods, where, in general, totally different frames may have similar histograms (e.g., a frame generated by randomly flipping the pixels of another frame has the same histogram of the original one). Further, the FOA representation is robust with respect to smooth view changes: for instance, an object translating with respect a a background, gives rise to a sequence of similar visuomotor traces. Meanwhile, a large object entering the scene would be recognized as a significant discontinuity in the visual content flow of the video sequence; in this sense, the approach accounts for the more general definition of shot as a sequence of frames that was, or appears to be, continuously captured from the same camera <ref type="bibr" target="#b15">[16]</ref>. Once the distinctive scanpath has been extracted from a frame, subsequent feature analysis need only to process a sparse representation of the frame; note that for each frame, we consider 10 FOAs, each FOA being defined on a square support region whose dimension is 1/36 of the original image; further reduction is achieved at the detection stage, where only the M function is processed (cfr. Table <ref type="table">VI</ref>). Last, the perceptual capacity of an observer to account for his own visual behavior, naturally leads, in this framework, to a Bayesian decision formulation for solving the detection problem, in a vein similar to <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In particular, by resorting to recently proposed contextual Bayesian analysis <ref type="bibr" target="#b45">[46]</ref>, we have generalized some suggestions introduced in <ref type="bibr" target="#b20">[21]</ref> for exploiting structural information related to different types of transitions.</p><p>It is worth remarking that, with respect to the specific problem of gradual transitions, the present work focuses on dissolve detection. However, the detection scheme can be easily extended to other kinds of transitions; for instance, preliminary experiments performed on wipes (not reported here, because out of the scope of this paper) show a behavior of the M function characterized by a damped oscillatory pattern. Also, beyond the context of video segmentation, the proposed technique introduces some novelties per se with respect to the "Where" and "What" integration problem, the explicit use of the fixation time in building a visuomotor trace, and as regards the way to exploit the extracted information for comparing different views (information look-up problem).</p><p>Results on a test set representing a total of 1304 cuts and 336 dissolves in 130 min. of video, including videos of different kinds are reported and validate the proposed approach. The performance of the currently implemented system is characterized by a 97% recall rate with a 95% precision rate on abrupt transitions, and a 92% recall rate with a 89% precision rate on gradual transitions. Meanwhile it exhibits a constant quality of detection for arbitrary complex movie sequences with no need for tuning parameters. Interestingly enough, the system has been trained on a small data set with respect to the test set used.</p><p>However, the introduction of an attention based approach not only is motivated by performance in shot-detection, but in perspective it could constitute an alternative to traditional approaches, and overcome their limitations for high-level video segmentation. Consider, for instance, the issue of scene change detection by jointly exploiting video and audio information. Audio and pictorial information play different roles and, to some extent, complementary. When trying to detect a scene decomposition of the video sequence, the analysis of visual data may provides candidate cuts, which are successively validated through fusion with information extracted from audio data. How to perform such fusion, in a principled way, is unclear. However, behavioral studies and cognitive neuroscience have remarked the fundamental role of attention in integrating multimodal information <ref type="bibr" target="#b4">[5]</ref>; and the approach proposed here could serve as a sound basis for such integration. In this way, the low level and high level video analysis could share the processing steps, making the entire content analysis process more effective and efficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of dissolve effect</figDesc><graphic coords="1,312.98,185.71,249.46,118.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Scanpath eye-tracked from a human observer while viewing the third frame presented in Fig. 1. The scanpath has been graphically overlapped on the original image: circles represent fixations, and lines trace displacements (saccades) between fixations.</figDesc><graphic coords="2,94.79,212.80,159.92,108.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Generation of the visuomotor trace of a single frame. The scheme shows the selection of a FOA by the "Where" pathway, and the extraction of FOA information by the "What" pathway. For visualization purposes, the trace is represented as a graph-like structure: each node corresponds to a single FOA, and the arc joining two FOAs denotes a saccade.</figDesc><graphic coords="5,54.94,139.92,239.43,176.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Traces generated on four frames embedding an hard cut. The first four FOAs are shown for each frame. The red rectangle represents the first FOA of the trace. The trace sequence abruptly changes between frame 2 and 3</figDesc><graphic coords="5,337.87,525.25,199.76,136.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The information look-up loop for determining the attention consistency M related to frames m and n, by exploiting the visuomotor traces T (m), T (n).</figDesc><graphic coords="6,69.88,127.25,209.60,224.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Plot of M(t) function for a sequence characterized by one a dissolve region embedded between two abrupt transitions</figDesc><graphic coords="7,312.97,249.90,249.45,187.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The decision module for inferring boundary presence from M(t) behavior and prior/contextual knowledge</figDesc><graphic coords="8,311.98,521.68,256.40,157.70" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors are grateful to the Referees and Associate Editor, for their enlightening and valuable comments that have greatly improved the quality and clarity of an earlier version of this paper. This research was also funded by the Italian Ministero per l' Universita' e la Ricerca Scientifica e Tecnologica and by the INFM.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Humanoid Robots: A New Kind of Tool</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scassellati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Systems</title>
		<imprint>
			<biblScope unit="page" from="25" to="31" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on the use of pattern recognition methods for abstraction, indexing and retrieval of images and video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="945" to="965" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data and Model-Driven Gaze Control for an Active-Vision System</title>
		<author>
			<persName><forename type="first">G</forename><surname>Backer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mertshing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bollmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1415" to="1429" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal winner-take-all networks: a timebased mechanism for fast selection in neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Barnden</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="844" to="853" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Le sens du mouvement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berthoz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Odile Jacob</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaze shift as a constrained random walk</title>
		<author>
			<persName><forename type="first">G</forename><surname>Boccignone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Perception and Communication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Broadbent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
			<pubPlace>Pergamon, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Communication</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Pyramid-Based Front-End Processor for Dynamic Vision Applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="1188" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TopDown Guided Eye Movements</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Chernyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Systems, Man and Cybernetics -Part B</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="514" to="522" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Information theoretic sensor data selection for active object recognition and state estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pattern Classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Wiley and Sons, N.Y</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fade and Dissolve Detection in Uncompressed and Compressed Video Sequences</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Canagarjah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Entropy-based representation of image information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boccignone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Caelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1391" to="1398" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Furht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Smoliar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Norwell</surname></persName>
		</author>
		<author>
			<persName><surname>Kluwer</surname></persName>
		</author>
		<title level="m">Video and Image Processing in Multimedia Systems</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Performance characterization of video-shot change detection methods</title>
		<author>
			<persName><forename type="first">U</forename><surname>Gargi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circ. Sys. for Video Tech</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Saccadic Object Recognition with an Active Vision System</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Giefing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mallot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Eur. Conf. Art. Intell</title>
		<meeting>10th Eur. Conf. Art. Intell</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="803" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The objects of action and perception</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Goodale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Humphrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="181" to="207" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overcomplete steerable pyramid filters and rotation invariance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rakshit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="222" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Digital video segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weymouth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia94</title>
		<meeting>ACM Multimedia94</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="357" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shot-Boundary Detection: Unraveled and Resolved?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="90" to="105" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computational modelling of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Development of an automatic summary editing system for the audio visual resources</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kikukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kawafuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Inst. Electron., Inform., Commun. Eng</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="204" to="212" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object and scene analysis by saccadic eye-movements: an investigation with higher order statistics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rentschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hauske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zetsche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spat. Vis</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="201" to="214" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Foveated Video Compression with Optimal Rate Control</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pattichis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="977" to="992" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Model based video segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Systems Video Technol</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="533" to="544" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reliable transition detection in videos: a survey and practicioner&apos;s guide</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Image and Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="469" to="486" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Comparison of Automatic Shot Boundary Detection Algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SPIE 3656-29 Image and Video Processing</title>
		<meeting>of SPIE 3656-29 Image and Video essing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">VII</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reliable Dissolve Detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">4315</biblScope>
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A novel Video Key Frame Extraction Algorithm</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and System</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A wavelet tour of signal processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Academic Press</publisher>
			<pubPlace>NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attentive mechanisms for dynamic and static scene analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milanese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2428" to="2434" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic video indexing and fullvideosearch for object appearances</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Database Systems II</title>
		<imprint>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The functional visual field during picture viewing</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Loftus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journ. of. Exp. Psych., Human Learning and Memory</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="391" to="399" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Computational architectures for attention</title>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scanpaths in saccadic eye movements while viewing and recognising patterns</title>
		<author>
			<persName><forename type="first">D</forename><surname>Noton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="929" to="942" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Solving the &apos;real&apos; mysteries of visual perception: The world as an outside memory</title>
		<author>
			<persName><forename type="first">K</forename><surname>O'regan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. of Psychology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="461" to="488" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video browsing using brightness data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Otsuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tonomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE-IST VCIP91</title>
		<meeting>SPIE-IST VCIP91</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">1606</biblScope>
			<biblScope unit="page" from="980" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling the role of salience in the allocation of overt visual attention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parkhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Non-linear Filters</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Venetsanopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Algorithms for Defining Visual Regions-of-Interest: Comparison with Eye Fixations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Privitera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis ans Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="970" to="982" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic model of visual recognition predicts neural response properties in the visual cortex</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neur. Comp</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="721" to="763" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An optimal estimation approach to visual perception and learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P N</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1963" to="1989" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">To see or not to see: the need for attention to perceive changes in scenes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>O'regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Sc</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="368" to="373" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Priors, preferences and categorical percepts</title>
		<author>
			<persName><forename type="first">W</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Perception as Bayesian Inference</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Richards</surname></persName>
		</editor>
		<meeting><address><addrLine>MA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="93" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Controlling Eye Movements with Hidden Markov Models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Rimey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="47" to="65" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Suprvised Classification for Video Shot Segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICME 2003</title>
		<meeting>ICME 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scene analysis with saccadic eye movements: top-down and bottom-up modeling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Umkehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beinlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zetzsche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electronic Imaging</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scene change detection and content-based sampling of video sequences</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shahraray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IST-SPIE</title>
		<meeting>IST-SPIE</meeting>
		<imprint>
			<date type="published" when="1995-02">Feb. 1995</date>
			<biblScope unit="volume">2419</biblScope>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Change blindess</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cog. Sc</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="261" to="267" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">The psychology of attention</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Styles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Psychology Press</publisher>
			<pubPlace>Hove</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Modeling visual-attention via selective tuning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Art. Intell</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="507" to="545" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Digital video processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NJ</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spike-based strategies for rapid processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delorme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Rullen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="715" to="725" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">New Enhancements to Cut, Fade, and Dissolve detection Processes in Video Segmentation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dorai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Multimedia</title>
		<imprint>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Statistical models of video structure for content analysis and characterization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date>Jan 200</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Eye movement strategies involved in face perception</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Walker-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Findlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="313" to="326" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Eye movements and vision</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yarbus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>Plenum Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Rapid scene analysis on compressed video</title>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="533" to="544" />
			<date type="published" when="1995-12">Dec. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bayesian decision theory and psychophysics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Bulthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Perception as Bayesian Inference</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Richards</surname></persName>
		</editor>
		<meeting><address><addrLine>MA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="123" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A Feature-Based Algorith, for Detecting and Classifying Scene Breaks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="189" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Automatic partitioning of full-motion video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Smoliar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="28" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
