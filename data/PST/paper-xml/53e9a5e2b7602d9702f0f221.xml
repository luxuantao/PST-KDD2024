<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature selection based-on genetic algorithm for image annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-04-07">7 April 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jianjiang</forename><surname>Lu</surname></persName>
							<email>jjlu@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Command Automation</orgName>
								<orgName type="institution">PLA University of Science and Technology</orgName>
								<address>
									<postCode>210007</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianzhong</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Command Automation</orgName>
								<orgName type="institution">PLA University of Science and Technology</orgName>
								<address>
									<postCode>210007</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yafei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Command Automation</orgName>
								<orgName type="institution">PLA University of Science and Technology</orgName>
								<address>
									<postCode>210007</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature selection based-on genetic algorithm for image annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-04-07">7 April 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">51B6E5C109D86185227C264AA3450F07</idno>
					<idno type="DOI">10.1016/j.knosys.2008.03.051</idno>
					<note type="submission">Received 23 November 2007 Accepted 30 March 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Image annotation Feature selection Genetic algorithm k-Nearest neighbor classifier Multimedia content description interface</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning techniques for feature selection, which include the optimization of feature descriptor weights and the selection of optimal feature descriptor subset, are desirable to enhance the performance of image annotation systems. In our system, the multimedia content description interface (MPEG-7) image feature descriptors consisting of color descriptors, texture descriptors and shape descriptors are employed to represent low-level image features. We use a real coded chromosome genetic algorithm and k-nearest neighbor (k-NN) classification accuracy as fitness function to optimize the weights of MPEG-7 image feature descriptors. A binary one and k-NN classification accuracy combining with the size of feature descriptor subset as fitness function are used to select optimal MPEG-7 feature descriptor subset. Furthermore, a bi-coded chromosome genetic algorithm is used for the simultaneity of weight optimization and descriptor subset selection, whose fitness function is the same as that of the binary one. The experimental results over 2000 classified Corel images show that with the real coded genetic algorithm, the binary coded one and the bi-coded one, the accuracies of image annotation system are improved by 7%, 9% and 13.6%, respectively, comparing to the method without machine learning. Furthermore, 2 of 25 MPEG-7 feature descriptors are selected with the binary coded genetic algorithm and four with the bicoded one, which may improve the efficiency of system significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Content-based image retrieval computes relevance based on the visual similarity of low-level image features such as colors, textures and shapes <ref type="bibr" target="#b0">[1]</ref>. However, people prefer retrieving images according to high-level semantic content. The problem is that visual similarity is not semantic similarity. There is a gap between low-level image visual features and semantic meanings. Image annotation techniques are used to bridge the gap <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. As it is laborious, error prone, and subjective, to manually annotate a large collection of images, we hope to do the task automatically. Automatic image annotation is a process of automatically labeling images with a set of pre-defined keywords, which represent image high-level semantic content. Specifically, given a set of images where each image is captioned with keywords that describe image semantic content, we identify correlation between keywords and low-level image visual features. This association combining with a similarity measure is used to automatically annotate unlabeled images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Multimedia content description interface (MPEG-7) is one of the most famous multimedia metadata standards, which includes a number of image feature descriptors to represent low-level image features effectively <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. But, the contribution of each descriptor may not be the same for a domain specific image database when computing the similarity measure. The weight of each feature ought to be different according to its importance. Another disadvantage using MPEG-7 is that exhaustively searching the MPEG-7 feature descriptor space can impose a high computational cost. Furthermore, there are redundant feature descriptors in MPEG-7, which decrease the effectiveness of feature representation. In order to improve the performance of image annotation, we exclusively focus on machine learning techniques for feature selection which includes weight optimization which automatically adjusts the weight of each feature descriptor, optimal feature descriptor subset selection and the simultaneity of weight optimization and optimal feature descriptor subset selection.</p><p>For weight optimization, Wang et al. determined relevant features based on histogram analysis and assigned greater weight to relevant features as compared to less relevant features <ref type="bibr" target="#b7">[8]</ref>. Setia et al. formatted image annotation with keywords as a classification problem and used a Gaussian mixture model to weight the features effectively <ref type="bibr" target="#b8">[9]</ref>. In <ref type="bibr" target="#b9">[10]</ref>, Qi et al. applied likelihood normalization to optimize weights for Corel images automatically. They computed the MPEG-7 scalable color descriptor and the modified MPEG-7 edge histogram descriptor to represent the overall color and texture of the image. In the above discussed methods, machine learning algorithms for weight optimization are ignored and few features are considered.</p><p>In general, feature subset selection approaches can be grouped into two categories: filter method and wrapper method <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Independent of any machine learning algorithm, the filter model selects feature subsets to estimate the performance by some indirect assessments, such as distance measures. The wrapper model, on the contrary, uses the accuracy of a predetermined machine learning algorithm to determine the 'goodness' of the selected subsets, which will intuitively yield better performance. The method is computationally expensive for data with a large number of features.</p><p>Search strategy is a key problem in feature subset selection. To balance the tradeoff of result optimality and computational efficiency, a lot of search strategies such as complete, heuristic, and random search have been studied to generate candidate feature subsets for evaluation. Genetic algorithm (GA) is an effective random search approach of wrapper model <ref type="bibr" target="#b12">[13]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, feature selection based on GA whose fitness function combined the number of features to be used and the error rate of the Bayesian classifier was presented. Huang et al. presented a feature optimization approach based on GA and support vector machine (SVM) classification accuracy <ref type="bibr" target="#b12">[13]</ref>. Hern√°ndez et al. proposed using GA to select a set of suitable regions for the feature extraction in facial expression recognition system <ref type="bibr" target="#b14">[15]</ref>. In these methods, MPEG-7 image feature descriptors are ignored.</p><p>In <ref type="bibr" target="#b15">[16]</ref>, Hamdani et al. proposed a bi-coded chromosome genetic algorithm to simultaneously select feature subset and the importance rate of each feature. They combined the k-NN recognition rate and the size of the selected feature subset as fitness function. But, it is not used for image annotation.</p><p>We represent low-level image features with all the MPEG-7 feature descriptors and use GA for feature selection. Three schemes are considered: weight optimization with a real coded chromosome GA, the selection for optimal feature descriptor subset with a binary one, the simultaneity of weight optimization and the selection for optimal feature descriptor subset with a bi-coded chromosome GA motivated by <ref type="bibr" target="#b15">[16]</ref>. As we just want to investigate the methodology in feature selection, the fitness function takes into consideration k-nearest neighbor (k-NN) classification accuracy instead of SVM one, which is one of the best classification algorithm <ref type="bibr" target="#b16">[17]</ref>. In the first scheme, k-NN classification accuracy is considered as the fitness function. In the others, the fitness function takes into consideration k-NN classification accuracy combining with the size of feature descriptor subset <ref type="bibr" target="#b17">[18]</ref>. The experiments are performed over 2000 classified Corel images to validate the performance of the approaches.</p><p>The remainder of the paper is organized as follows. Section 2 describes the MPEG-7 image feature descriptors and the extraction of low-level image feature vectors. Section 3 introduces k-nearest neighbor classifier. Section 4 discusses the designs of the real coded chromosome GA, the binary coded one and the bi-coded one in our approaches. Section 5 describes the method of image annotation in our approaches. Section 6 illustrates the experimental results and analysis. Our conclusions and future work are given in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Low-level image feature vector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">MPEG-7 image feature descriptors</head><p>We extract low-level image features with MPEG-7 image feature descriptors. In MPEG-7, features such as color, texture and shape are used for the description of low-level image feature content. Each feature has its corresponding descriptors, i.e., color descriptors, texture descriptors and shape descriptors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. All the descriptors and descriptor numbers used in image feature vector extraction are listed in Table <ref type="table" target="#tab_1">1</ref>.</p><p>There are 25 descriptors used to extract low-level image feature vector, 12 for color, eight for texture and five for shape, respectively. These descriptors are color value, percent, variance, spatial coherency, scalable color, discrete cosine transform</p><formula xml:id="formula_0">'s DC coefficient of Y component, discrete cosine transform's AC coeffi- cient of Y component, discrete cosine transform's DC coefficient of Cb component, discrete cosine transform's AC coefficient of Cb component, discrete cosine transform's DC coefficient of Cr component, discrete cosine transform's AC coefficient of Cr com- ponent,</formula><p>color structure, intensity average, intensity standard deviation, energy, energy standard deviation, regularity, direction, scale, edge histogram, region shape, global curvature, prototype curvature, highest peak and peak. They form image feature descriptor set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image feature vector extraction and normalization</head><p>With the MPEG-7 image feature descriptor set, original low-level feature vectors of images in the database are extracted. Each image is represented by a vector X, X = (x 1 , x 2 , . . . , x k , . . . , x 25 ), where x k is the vector extracted with the kth feature descriptor, i.e., x k = {x k,1 , . . . ,x k,l , . . . x k,s }, s is the dimensional number of x k . The extraction algorithms are referred to <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>.</p><p>As different component x k,l has different range and different x k has different dimensional number, x k,l is normalized so that each feature contributes equally in computing the similarity measure based on Euclidean distance. Assume max k,l and min k,l are the maximum and the minimum of x k,l over the database, respectively. We normalize x k,l as follows:</p><formula xml:id="formula_1">x k;l ¬º x 0 k;l √Ä min k;l ffiffi s p √Ç √∞max k;l √Ä min k;l √û<label>√∞1√û</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">k-Nearest neighbor classifier</head><p>We use k-nearest neighbor classification accuracy as fitness function for GA. k-Nearest neighbor classifier is based on learning by analogy. It is carried out under the assumption that the similar images will belong to the same category <ref type="bibr" target="#b17">[18]</ref>. Given a set of d instance-label pairs (X i ,L i ), i = 1,2,. . . , d, where X i 2 R n , L i is the category label of X i . Each image represents a point in an n-dimension feature space and is used as a query image to compute the 'closeness' to the other images. k-Nearest neighbors that are the closest to the query image are returned. The query image is assigned with the most common category among its k nearest neighbors. The </p><formula xml:id="formula_2">k √Ä NN accuracy ¬º t d<label>√∞2√û</label></formula><p>where t is the number of images correctly classified, d the number of images in the set. 'Closeness' is defined in terms of similarity measure. Several similarity measures based on common distance functions such as Euclidean, Mahalonibis, and etc. are defined in <ref type="bibr" target="#b21">[22]</ref>. We use Euclidean distance, where the Euclidean distance between two points X = {x k,l } and Y = {y k,l } is defined as the following:</p><formula xml:id="formula_3">d√∞X; Y√û ¬º ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi X 25 k¬º1 X s l¬º1 √∞x k;l √Ä y k;l √û 2 v u u t<label>√∞3√û</label></formula><p>Considering different weight may be assigned to different feature descriptor, a weighted Euclidean distance is used to compute the similarity measure as the following:</p><formula xml:id="formula_4">d√∞X; Y√û ¬º ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi X 25 k¬º1 w k X s l¬º1 √∞x k;l √Ä y k;l √û 2 v u u t<label>√∞4√û</label></formula><p>where w k is the weight of kth feature descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Genetic algorithm for feature selection</head><p>Genetic algorithm, a general adaptive optimization search methodology based on a direct analogy to Darwinian natural selection and genetics in biological systems, is a promising alternative to conventional heuristic methods. GA works with a set of candidate solutions called a population. Based on the Darwinian principle of 'survival of the fittest', GA obtains the optimal solution after a series of iterative computations. GA generates successive populations of alternative solutions that are represented by an individual, i.e., a solution to the problem, until acceptable results are obtained. Associated with the characteristics of exploitation and exploration search, GA can deal with large search spaces efficiently, and hence has less chance to get local optimal solution than other algorithms. A fitness function is used to evaluate the quality of a solution. The crossover and mutation functions are the main operators that randomly impact the fitness value. Chromosomes are selected for reproduction by evaluating the fitness value. The fitter chromosomes have higher probability to be selected for GA operation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Real coded chromosome GA</head><p>We use a real coded chromosome GA to select optimal weight of each MPEG-7 feature descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Chromosome design and initial population</head><p>Let m as the number of feature descriptors,N the size of population. Commonly, population size N is 20 &lt; N &lt; 100. Chromosome of m genes is an individual, which is used to represent the weights of feature descriptors. In initial population P = {C 1 ,C 2 , . . . , C N }, all the genes of the first chromosome C 1 are '1', which means the weights of all the descriptors are equal. For the other individuals, we randomly generate a set of real numbers w k , where w k 2 [0, 1], k = 1,2,. . . , m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Fitness function design</head><p>k-NN classification accuracy k-NN_accuracy is used to evaluate the fitness of individuals. Fitness function Fit1 is designed as the following:</p><formula xml:id="formula_5">Fit1 ¬º k √Ä NN accuracy<label>√∞5√û</label></formula><p>4.1.3. Genetic operation GA searches for better solutions by genetic operation, including selection operation, crossover operation and mutation operation.</p><p>Selection operation is to select elitist individuals as parents in current population, which can generate offspring. Fitness values are used as criteria to judge whether individuals are elitist. We use elitist preserved model for selection operation. In order to acquire the fittest individual in the history when GA process ends, after crossover and mutation operations in each GA iteration, both parent and up-to-date solutions are put into a pool to select N individuals with the top highest fitness values to form the new population.</p><p>Crossover operation needs to be operated on two individuals with crossover rate P c . Crossover operation is as the following:</p><p>First, all of the parent individuals are combined and C 2 N pairs are obtained.</p><p>Second, randomly generate two numbers a(0 &lt; a &lt; m) and b(0 &lt; b &lt; m √Ä a), where m is the length of each chromosome, a is the start position of crossover operation, b is the crossover operation length.</p><p>Lastly, assume for each pair,C t 1 ¬º fw k g and C t 2 ¬º fw 0 k g, k = a + 1,. . . , a + b are two gene fractions for crossover respectively. The genes in the range [(a + 1), (a + b)] swap to generate two new individuals with crossover rate P c as follows:</p><formula xml:id="formula_6">C t√æ1 1 ¬º fw 1;k g; C t√æ1 2 ¬º fw 2;k g, where w 1;k ¬º c √Å w 0 k √æ √∞1 √Ä c√û √Å w k , w 2;k ¬º c √Å w k √æ √∞1 √Ä c√û √Å w 0</formula><p>k , where crossover factor c is a predefined constant. Mutation operation is very important in keeping the varieties of populations. We put the individuals generated in crossover operation into the pool with parent individuals. K the worst fit individuals are selected with a very small mutation rate P m . We randomly select four genes of each individual for mutation operation. Assume gene w k (w k 2 [0, 1]) is mutated, whose offspring is w 0 k . The mutation operation is as following:</p><formula xml:id="formula_7">w 0 k ¬º w k √æ D√∞t; 1 √Ä w k √û; g ¬º 0 w k √Ä D√∞t; w k √û; g ¬º 1<label>√∞6√û</label></formula><p>where g is a random number whose value is '0' or '1'. Function D(t ,y) returns a value in the range [0, y].</p><formula xml:id="formula_8">D√∞t; y√û ¬º y 1 √Ä r 1√Ä t M √∞ √û p<label>√∞7√û</label></formula><p>where t is iteration number, r is a random number in the range [0, 1], M is the number of the maximum iteration, and mutation parameter p is a predefined constant. This method adjusts the genetic algorithm process, which lets mutation operation has larger mutation ranges in earlier stage, and smaller ones in the later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Termination criteria</head><p>We proceed with the next generation until the process reaches the maximum iteration Gen max . When the process ends, the fittest individual is output as the optimal feature selection result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Binary coded chromosome GA</head><p>A binary coded chromosome GA is used to select the most effective combination of MPEG-7 descriptors. Namely, let m the number of initial feature descriptors. S = {s 1 ,s 2 , . . . ,s m } is a binary string which represents a subset subT of image feature descriptor set T = {t 1 ,t 2 , . . . , t m }. If s i , i = 1,2,. . . , m is equal to '1', t i 2 subT, otherwise t i R subT.</p><p>The binary GA scheme is the same as the real one, except chromosome design, initial population, fitness function, crossover operation and mutation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Chromosome design and initial population</head><p>All the genes of the first chromosome in initial population are '1'. The other chromosomes are randomly generated. Namely, each gene in a chromosome has value '0' or '1' randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Fitness function design</head><p>The classification accuracy of k-NN classifier and the number of selected feature descriptors are combined to evaluate the fitness of the individuals. Fitness function Fit2 is designed as the following:</p><formula xml:id="formula_9">Fit2 ¬º b √Ç k √Ä NN accuracy √æ √∞1 √Ä b√û √Ç Zeros=m<label>√∞8√û</label></formula><p>where k-NN_accuracy is k-NN classification accuracy, Zeros is the number of genes whose values are '0' in a binary chromosome, b(0 &lt; b &lt; 1) is a balance factor, which adjusts the importance of k-NN_accuracyand Zeros, m is the length of chromosome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Genetic operation</head><p>Selection operation in a binary GA is the same as that in a real one. We only discuss crossover operation and mutation operation in this section.</p><p>In a binary GA, commonly used crossover methods include onepoint crossover, two-point crossover, uniform crossover, and etc. We use two-point crossover. The selection of crossover start position and length is the same as that in Section 4.1.3. For each pair, the genes in the range [(a + 1), (a + b)] swap to generate two new individuals with crossover rate P c .</p><p>The genes for mutation operation are selected as that recited in Section 4.1.3 and their values are changed from '1' to '0' or '0' to '1', respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Bi-coded chromosome GA</head><p>Besides the optimization of weights with a real coded GA (algorithm 1) and the selection of optimal descriptor subset with a binary one (algorithm 2), respectively, we optimize weights and select optimal descriptor subset simultaneously with a bi-coded chromosome GA (algorithm 3).</p><p>Different from algorithms 1 and 2, in algorithm 3,N chromosome pairs form initial population. Each chromosome pair consisting of a real chromosome and a binary one represents an individual as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The real coded chromosome represents the weights of feature descriptors. The binary one represents the selection of MPEG-7 feature descriptor subset; which '1' corresponds to a selected feature descriptor and '0' to a non selected one. Each pair of chromosome represents a kind of solution. Namely, a subset of MPEG-7 feature descriptors is selected for GA operation, which is assigned with randomly generated weights.</p><p>The genetic operation of the bi-coded GA consists of two parts: genetic operation for the real coded chromosomes which is the same as that of algorithm 1; genetic operation for the binary one which is the same as that of algorithm 2. Fitness function is the same as that of algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Image annotation</head><p>Image annotation can be formatted as a classification problem. In annotation systems, the images are represented by either global features or local features <ref type="bibr" target="#b9">[10]</ref>. As the up-to-date segmentation results are fragile and erroneous, we do not segment image. We use global features of the whole image for image annotation <ref type="bibr" target="#b23">[24]</ref>. We annotate image with one keyword. The annotation process is as the following: for an image without caption, first, compute low-level feature vector of the image according to the results of genetic algorithm. Second, classify the image into a category with the k-NN classifier. Lastly, propagate the keyword of the corresponding category to the image. The unlabeled image is automatically annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental parameter settings</head><p>We use Corel image database to train and test our approaches. 2000 images with 20 categories are used, which include 'Africa', 'antiques', 'buses', 'beaches', 'car_rare', 'desert', 'dino_art', 'dogs', 'elephants', 'fashion1', 'food', 'horses', 'lizard_1', 'mountain', 'Rome', 'roses', 'skiing', 'sunsets', 'waterfall', and 'workship'. Each category contains 100 images. Fifty images are used for training, the others for test. The detail parameter settings for the algorithms are as the following: population size N = 30, the length of chromosome m = 25, maximum iteration Gen max = 500, crossover rate P c = 0.7, crossover factor c = 0.7, mutation rate P m = 0.02, mutation parameter p = 0.6, balance factor b = 0.6, two-point crossover, elitist preserved model. For k-NN classifier, we set k to 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental results and analysis</head><p>With algorithm 1, optimal weights are output. The optimal weights are 0.164070, 0.003319, 0.000582, 0.164306, 0.594099, 0.000002, 0.544852, 0.015854, 0.490207, 0.109505, 0.283766, 0.594099, 0.183565, 0.231806, 0.700675, 0.536733, 0.362269, 0.026603, 0.756442, 0.880811, 0.118129, 0.212261, 0.010711, 0.010788, 0.030377, representing the weights for color value, percent, variance, spatial coherency, scalable color, discrete cosine transform's DC coefficient of Y component, discrete cosine transform's AC coefficient of Y component, discrete cosine transform's DC coefficient of Cb component, discrete cosine transform's AC coefficient of Cb component, discrete cosine transform's DC coefficient of Cr component, discrete cosine transform's AC coefficient of Cr component, color structure, intensity average, intensity standard deviation, energy, energy standard deviation, regularity, direction, scale, edge histogram, region shape, global curvature, prototype curvature, highest peak and peak, respectively.</p><p>With algorithm 2, optimal feature descriptor subset is output. The subset includes color structure descriptor (CSD) and edge histogram descriptor (EHD). With algorithm 3, optimal feature descriptor subset and corresponding optimal weights are output. In algorithm 3, optimal feature descriptors are color value, discrete cosine transform's AC coefficient of Y component, CSD and EHD corresponding to the optimal weights 0.1888, 0.2349, 0.7177 and 0.4532, respectively.</p><p>With these feature selection results, we do image annotation task over test images. The accuracies of image annotation are listed in Table <ref type="table" target="#tab_2">2</ref>, comparing to the approach without GA feature selection (algorithm 4).</p><p>It can be seen from Table <ref type="table" target="#tab_2">2</ref> that the accuracies of algorithms 1-3 are 7%, 9% and 13.6%, respectively, higher than that of algorithm 4. Algorithm 3 is the best among the four algorithms and algorithm 2 is the second best one. Furthermore, with algorithm 2, only 2 of 25 MPEG-7 feature descriptors are selected; with algorithm 3, only four feature descriptors are selected. In algorithm 3, different descriptor is assigned with different weight according to their importance in similarity measure as well as redundant feature descriptors which decrease the performance of system are discarded. Hence, algorithm 3 has the best synthetical performance. In algorithm 3, not only the system is optimized for having the best accuracy, but also the input dimensional number is reduced to only 16% that of the original feature descriptor space. So, computing time is decreased and the efficiency of image annotation system is improved significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In order to improve the performance of image annotation system based on MPEG-7 low-level feature descriptors, the optimization of MPEG-7 feature descriptor weights or/and the selection of optimal descriptor subset with machine learning techniques are highly desirable. We do the task with genetic algorithm. Three schemes are used: weight optimization with a real coded chromosome GA, the selection of optimal feature descriptor subset with a binary one, the simultaneity of weight optimization and the selection for optimal feature descriptor subset with a bi-coded one. When optimizing weights, k-NN classification accuracy is used as the fitness function. In the other cases, the fitness function takes into consideration k-NN classification accuracy combining with the size of feature descriptor subset.</p><p>The experiments are performed over 2000 classified Corel images. With weight optimization, the accuracy of image annotation system is improved. With the selection for optimal feature descriptor subset, a higher accuracy is obtained and considering only 2 of 25 feature descriptors are selected, the efficiency and effectiveness of image annotation system may improve significantly. With the simultaneity of weight optimization and the selection for optimal feature descriptor subset, the best synthetical performance can be obtained: the highest accuracy and four feature descriptors selected.</p><p>As the study shows experimental results only over a small classified image database, which includes 2000 Corel images with 20 image classes, the generalization of the approaches needs to be validated over larger image databases and in more domains for the next work. Furthermore, genetic algorithm incorporating AdaBoost to learning weak classifiers for effective feature descriptor subsets and relative weights to form strong classifier for image annotation is considered in the future. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. A pair of bi-coded chromosomes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Color, texture and shape descriptors</figDesc><table><row><cell cols="2">Feature Descriptor</cell><cell>No.</cell></row><row><cell>Color</cell><cell>Dominant color: including color value, percent, variance and</cell><cell>4</cell></row><row><cell></cell><cell>spatial coherency in a clustering quantized color space</cell><cell></cell></row><row><cell></cell><cell>Scalable color: including a color histogram in quantized HSV color</cell><cell>1</cell></row><row><cell></cell><cell>space</cell><cell></cell></row><row><cell></cell><cell>Color layout: including discrete cosine transform's DC coefficients</cell><cell>6</cell></row><row><cell></cell><cell>and AC coefficients in YCbCr color space</cell><cell></cell></row><row><cell></cell><cell>Color structure: including quantized coefficients in HMMD color</cell><cell>1</cell></row><row><cell></cell><cell>space</cell><cell></cell></row><row><cell cols="2">Texture Homogeneous texture: including intensity average, intensity</cell><cell>4</cell></row><row><cell></cell><cell>standard deviation, energy, energy standard deviation</cell><cell></cell></row><row><cell></cell><cell>Texture browsing: including regularity, direction and scale</cell><cell>3</cell></row><row><cell></cell><cell>Edge histogram: including the spatial distribution of five types of</cell><cell>1</cell></row><row><cell></cell><cell>edge</cell><cell></cell></row><row><cell>Shape</cell><cell>Region shape: including a set of angular radial transform</cell><cell>1</cell></row><row><cell></cell><cell>coefficients</cell><cell></cell></row><row><cell></cell><cell>Contour shape: including global curvature, prototype curvature,</cell><cell>4</cell></row><row><cell></cell><cell>highest peak and peak</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>The comparison of accuracies</figDesc><table><row><cell>Accuracy (%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J. Lu et al. / Knowledge-Based Systems 21 (2008) 887-891</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>J. Lu et al. / Knowledge-Based Systems 21 (2008) 887-891</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This paper is supported the National High Technology Research and Development Program of China (No. 2007AA01Z126, 863 Program).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of content-based image retrieval with high-level semantics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="262" to="282" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An object-and user-driven system for semanticbased image annotation and retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Djordjevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Izquierdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="323" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward bridging the annotation-retrieval gap in image search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="24" to="35" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised learning of semantic classes for image annotation and retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="394" to="410" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation and object labeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Athanasiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mylonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="312" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Text of ISO/IEC 15938-3 multimedia content description interface part 3: visual, ISO/IEC</title>
		<idno>ISO/IEC/JTC1/SC29/WG11/N4062</idno>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The MPEG-7 visual standard for content description-an overview</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="696" to="702" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic image annotation and retrieval using weighted feature selection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="71" />
			<date type="published" when="2006">2006. 2006</date>
			<publisher>Springer Science + Business Media</publisher>
		</imprint>
	</monogr>
	<note>LLC</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature selection for automatic image annotation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Setia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4174</biblScope>
			<biblScope unit="page" from="294" to="303" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating multiple SVMs for automatic image annotation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="728" to="741" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient feature selection via analysis of relevance and redundancy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1205" to="1224" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A wrapper for feature selection based on mutual information</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE Conference on Pattern Recognition</title>
		<meeting>eeding of IEEE Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="618" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Consistency-based search in feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="155" to="176" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Similarity-based online feature selection in content-based image retrieval</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="702" to="712" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual learning of texture descriptors for facial expression recognition in thermal imagery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hern√°ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Olague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hammoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Trujillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="258" to="269" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed genetic algorithm with bicoded chromosomes and a new evaluation function for features selection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hamdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE Congress on Evolutionary Computation</title>
		<meeting>eeding of IEEE Congress on Evolutionary Computation</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="581" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A GA-based feature selection and parameters optimization for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Boosting the distance estimation application to the k-nearest neighbor classifier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="209" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Color and texture descriptors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="703" to="715" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MPEG-7 visual shape descriptor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="716" to="719" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">MPEG-7 visual experimentation model (XM), Version 10.0, ISO/IEC</title>
		<idno>ISO/IEC/JTC1/SC29/WG11</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">4063</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving similarity measures of histograms using smoothing projections</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kyrki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ilonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kalviainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2009" to="2019" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hybrid genetic algorithms for feature selection</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1424" to="1437" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic image annotation by incorporating feature hierarchy and boosting to scale up SVM classifiers</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="901" to="910" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
