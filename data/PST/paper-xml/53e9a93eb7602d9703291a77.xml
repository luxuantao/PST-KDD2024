<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph nodes clustering with the sigmoid commute-time kernel: A comparative study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-11-14">14 November 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luh</forename><surname>Yen</surname></persName>
							<email>luh.yen@uclouvain.be</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université catholique de Louvain</orgName>
								<orgName type="institution" key="instit2">ISYS</orgName>
								<address>
									<settlement>Louvain-la-Neuve</settlement>
									<region>LSM</region>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francois</forename><surname>Fouss</surname></persName>
							<email>francois.fouss@fucam.ac.be</email>
							<affiliation key="aff1">
								<orgName type="department">Management Department</orgName>
								<orgName type="institution" key="instit1">Facultés Universitaires Catholiques de Mons</orgName>
								<orgName type="institution" key="instit2">LSM</orgName>
								<address>
									<settlement>Mons</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Decaestecker</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratoire de Toxicologie et Laboratoire de l&apos;Image: Synthese et Analyse (LISA)</orgName>
								<orgName type="institution">Université Libre de Bruxelles</orgName>
								<address>
									<settlement>Bruxelles</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Francq</surname></persName>
							<email>pfrancq@ulb.ac.be</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Université Libre de Bruxelles</orgName>
								<orgName type="institution" key="instit2">IRIDIA</orgName>
								<address>
									<settlement>Bruxelles</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Saerens</surname></persName>
							<email>marco.saerens@uclouvain.be</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université catholique de Louvain</orgName>
								<orgName type="institution" key="instit2">ISYS</orgName>
								<address>
									<settlement>Louvain-la-Neuve</settlement>
									<region>LSM</region>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Belgian Scientific Research Funds (FNRS)</orgName>
								<address>
									<settlement>Bruxelles</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph nodes clustering with the sigmoid commute-time kernel: A comparative study</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-11-14">14 November 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">75DEE1A17760B9C9D035ED37DDAD4647</idno>
					<idno type="DOI">10.1016/j.datak.2008.10.006</idno>
					<note type="submission">Received 7 March 2008 Received in revised form 15 October 2008 Accepted 16 October 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Kernel clustering Graph mining Kernel hierarchical clustering Laplacian matrix Fiedler vector Commute-time distance Resistance distance Community detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work addresses the problem of detecting clusters in a weighted, undirected, graph by using kernel-based clustering methods, directly partitioning the graph according to a welldefined similarity measure between the nodes (a kernel on a graph). The proposed algorithms are based on a two-step procedure. First, a kernel or similarity matrix, providing a meaningful similarity measure between any couple of nodes, is computed from the adjacency matrix of the graph. Then, the nodes of the graph are clustered by performing a kernel clustering on this similarity matrix. Besides the introduction of a prototype-based kernel version of the gaussian mixtures model and Ward's hierarchical clustering, in addition to the already known kernel k-means and fuzzy k-means, a new kernel, called the sigmoid commute-time kernel (K S CT ) is presented. The joint use of the K S CT kernel matrix and kernel clustering appears to be quite effective. Indeed, this methodology provides the best results on a systematic comparison with a selection of graph clustering and communities detection algorithms on three real-world databases. Finally, some links between the proposed hierarchical kernel clustering and spectral clustering are examined.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">General introduction</head><p>This work presents a general methodology for clustering the nodes of a weighted, undirected, graph. Graph nodes clustering and ''community detection" are important issues that have been the subject of much recent works in various fields of science: applied mathematics, computer science, social science, physics, pattern recognition; see for instance <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b121">122]</ref>, and the related works cited later in this paper. It can be noticed that graph clustering and communities detection both have as objective the search and identification of dense subgroups within a network. A cluster, or community, is supposed to have dense node-to-node connections within the community, and sparse connections with nodes from another community. Usually, community detection algorithms are also able to detect the natural number of communities. This is not the case for most clustering algorithms for which the number of clusters has to be provided in advance. In this work, we are interested in clustering the nodes of a graph: there is no attempt to find the number of clusters, except for the hierarchical clustering method that incrementally decreases the number of groups by one and thus enumerates all the solutions. Detecting dense groups in a network has a number of direct applications in telecommunications, web mining, social networking, to name a few.</p><p>On the other hand, kernel-based algorithms <ref type="bibr" target="#b109">[110,</ref><ref type="bibr" target="#b111">112]</ref> are characterized by two important properties: they allow (i) to compute implicitly similarities in a high-dimensional space where the data are more likely to be well-separated and (ii) to compute similarities between structured objects that cannot be naturally represented by a simple set of features. The latter property will be illustrated in this paper, with the general objective of clustering nodes of a graph according to some similarity measure between them. It relies on two independent steps:</p><p>(1) First, define a similarity or kernel matrix from the adjacency matrix of the graph, capturing similarities between nodes.</p><p>(2) Then, use a kernel clustering algorithm in order to cluster the nodes of the graph, based on the kernel matrix defined in <ref type="bibr" target="#b0">(1)</ref>.</p><p>The kernel clustering algorithms proposed in this paper differ from existing ones (see <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b138">139]</ref>) by the fact that (i) a prototype vector is explicitly defined for each cluster in the sample space and (ii) the clustering problem is expressed as an optimization problem in the sample space that mimics the standard formulation in the feature space.</p><p>Besides that, we also propose a new kernel (or, more rigorously, a similarity matrix since it is not positive semi-definite) on a weighted, undirected, graph for nodes clustering. It is a straightforward extension of the commute-time (CT) kernel which has the property to take both direct and indirect links into account <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. The CT kernel does not need any parameter adjustment, has a nice interpretation in terms of random-walk on a graph, and is closely related to the Fiedler vector used for graph partitioning. The new kernel, called the sigmoid commute-time kernel (K S CT ), is defined as the element-wise sigmoid of the CT kernel. It is specifically designed for graph nodes clustering and will be compared to other kernels on a graph and nodes clustering algorithms in the experimental section. Even if the K S CT matrix is not a valid kernel matrix (it does not need to be positive semi-definite), convergence to a local minimum of the criterion can be proved. Notice that a new interpretation of the CT kernel, based on electrical networks, is also provided in this paper.</p><p>Thus, inspired by previous work <ref type="bibr" target="#b132">[133]</ref>, we introduce a general methodology for defining prototype-based kernel clustering algorithms working in the sample space. Defining prototypes for each cluster is quite natural since it allows to mimic the update rules of the k-means, the fuzzy k-means, the gaussian mixture model and Ward's hierarchical clustering in the sample space instead of the feature space. In addition to be very similar to the original agglomerative or hierarchical algorithms, these prototype-based methods could easily be extended to variable-metric or multi-prototype kernel clustering methods, in the same way as the original algorithms <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b65">66]</ref>. Finally, the resulting algorithms are very simple and natural, and the prototypes in the sample space have a clear, intuitive, interpretation. Therefore, this methodology allows to design in a straightforward manner kernel clustering algorithms, working in the sample space, from their feature space counterpart.</p><p>The performances are evaluated, in a systematic way, on three real-world graph mining problems. The proposed algorithms are tested on four different kernels, and compared to several other clustering algorithms coming from document clustering, spectral clustering and communities detection. Finally, the benefit of the sigmoid transformation on the kernel is also examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions and organization of the paper</head><p>The present paper is an extended and improved follow-up to an earlier paper <ref type="bibr" target="#b132">[133]</ref> which introduced our basic model as well as preliminary experimental results. It extends the theoretical results and contains substantially expanded experiments. In short, the paper has six contributions:</p><p>(1) A brief survey of community detection in networks is provided.</p><p>(2) A new interpretation of the commute-time kernel, based on electrical networks, is developed. While there exists a well-known electrical interpretation for the commute times <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b118">119]</ref>, no such interpretation is available for the commute-time kernel. (3) A natural two-step methodology for finding communities in a graph is introduced: (i) compute a kernel on a graph that represents similarities between the nodes, for instance the sigmoid commute-time kernel, and (ii) perform a kernel clustering on this kernel matrix. We show that this two-step procedure provides good results, significantly better than the other tested algorithms. (4) A sample-space prototype-based kernel formulation of the gaussian mixture model is introduced. In this framework, the objective function (typically the sum of within-cluster inertia) is directly optimized with respect to a prototype vector defined in the sample space, that is, the Euclidean space whose dimensionality equals the number of data samples. The sample-space prototype-based formulation has some interesting properties that make it attractive: (i) the kernel version can be derived in a generic, well-founded, way, and (ii) it mimics the original algorithm by translating it in the sample space, therefore defining a prototype for each cluster in the sample space. Finally, a kernel version of Ward's hierarchical clustering is also developed.</p><p>(5) It is shown that taking a sigmoid function of a graph kernel, and more specifically of the commute-time kernel (K S CT ), improves significantly the results with respect to the basic commute-time kernel (without sigmoid transformation). ( <ref type="formula" target="#formula_8">6</ref>) A comprehensive comparison of four kernels on a graph and four kernel clustering algorithms on three real-world networks is provided.</p><p>The paper is organized as follows. Section 2 discusses previous work. Section 3 introduces the sigmoid commute-time kernel (K CT ) on a graph that will be used as similarity measure for clustering the nodes. Section 4 derives our kernel version of the k-means, the fuzzy k-means and the gaussian mixture model while Section 5 introduces a kernel version of Ward's hierarchical clustering. Section 6 describes the experiments which consist in a systematic comparison on three real-world datasets. Section 7 is the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A brief survey of related work on kernel clustering and link-based community detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Kernel clustering</head><p>A recent survey on kernel clustering together with its relationships with spectral clustering can be found in <ref type="bibr" target="#b40">[41]</ref>. Notice that the present work does not investigate the problem of clustering data; rather, it is focussed on clustering the nodes of a graph.</p><p>When dealing with kernel clustering, three different spaces can be defined:</p><p>(1) The input space is the initial space in which the data are defined. Remember <ref type="bibr" target="#b111">[112]</ref> that a nonlinear mapping is applied on this input space in order to obtain a new feature space where the data are hopefully easier to separate (for instance, the classes are linearly separable). Each coordinate in the input space represents the measurement of a characteristic on individuals. This space is not relevant in our case since we are directly working on a graph structure, without defining any input space. (2) The feature space, corresponding to the image of the input space through the mapping. In our case, the feature space corresponds to the Euclidean embedding space preserving the distances between the nodes of the graph. Thus, each node of the graph corresponds to a vector in this embedding space. As usual, these node vectors do not need to be computed; only the elements of the kernel matrix (the inner products of the node vectors in the feature space) will be used in the clustering methods. The dimensionality of the feature space corresponds to the rank of the kernel matrix, which is also the dimension of the embedding space. (3) The sample space, corresponding to the Euclidean space having, as dimensionality, the number of data samples. In the present case, this is the number of nodes of the graph. Each coordinate in this space thus corresponds to an individual, that is, a node of the graph.</p><p>When using agglomerative clustering techniques, prototype vectors (such as centroids) are usually defined. The authors of <ref type="bibr" target="#b40">[41]</ref> broadly categorize the kernel clustering methods into three groups:</p><p>1. Methods looking for prototype vectors in the input space. 2. Methods looking for prototype vectors in the feature space. 3. Methods relying on one-class SVM.We add two additional families of methods to this list: 4. Methods looking for prototype vectors in the sample space. This is our technique of choice introduced in this paper. 5. Methods simply avoiding the computation of a prototype vector.</p><p>Most existing kernel clustering algorithms either compute prototypes in the input space <ref type="bibr" target="#b138">[139]</ref> or simply avoid the computation of a prototype vector <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b130">131]</ref>. Some authors define prototype vectors in the feature space instead, such as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b138">139]</ref>. To our knowledge, the only known attempts to define a prototype-based kernel clustering method in the sample space are <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b137">138]</ref> and, very recently <ref type="bibr" target="#b122">[123]</ref>.</p><p>In <ref type="bibr" target="#b49">[50]</ref>, Girolami directly optimizes the prototype vector by using a stochastic optimization method over a set of binary indicator variables. On the other hand, the algorithm developed in <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b82">83]</ref> is closely related to our work. It starts from the iterative Self-Organizing (or topographic) Map clustering method that iteratively updates the centroids in the feature space. These centroids are then re-expressed in the sample space as a linear combination of the feature vectors. This leads to an iterative updating rule for the parameters of the linear combination, which correspond to prototype vectors in the sample space. The work of Villa and Rossi <ref type="bibr" target="#b122">[123]</ref> also follows the same idea of adjusting prototypes in the sample space; this time with the objective of clustering nodes of a graph with a self-organizing map. In contrast, the method proposed in <ref type="bibr" target="#b137">[138]</ref> necessitates the inversion of the kernel matrix and lacks a clear, intuitive, interpretation. A discussion of this fact is provided in Section 4.1, where the kernel k-means is developed.</p><p>Finally, methods relying on one-class SVM <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref> are seeking for a minimum enclosing sphere in feature space containing almost all the data, and excluding outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Link-based community detection</head><p>As already stated before, the problem of community detection in a weighted graph has been the subject of a large amount of literature; see the recent survey of Schaeffer <ref type="bibr" target="#b108">[109]</ref>. Moreover, community detection is studied in various fields, such as computer science, social science, applied mathematics, pattern recognition and physics. For the sake of simplicity, we will categorize this work into three different approaches:</p><p>Similarity-based approaches. These techniques first compute some meaningful distances/similarities between the nodes of the graph and then exploit these distances with standard or specialized clustering algorithms in order to cluster the nodes. The methodology proposed in this paper belongs to this category. Top-down, divisive, and bottom-up, agglomerative, approaches. These techniques try to recursively cut the graph into parts which are as disconnected as possible. Spectral clustering approaches. These techniques use the spectrum of matrices derived from the adjacency matrix of the graph in order to embed the nodes in a Euclidean space and perform a clustering in this space.</p><p>Of course, most of these approaches are in fact interrelated; we now provide a short survey of the main features characterizing each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Similarity-based approaches</head><p>The primary objective of these approaches is to define meaningful similarities (or distances) between the nodes of the graph which take the form of a kernel matrix. Indeed, the similarity measure between every pair of nodes is often defined as a positive semi-definite matrix, and is therefore called a graph kernel. <ref type="foot" target="#foot_0">2</ref> Once these similarities between nodes have been computed, they can be exploited in order to find communities, outliers, etc. Similarity between nodes is also called relatedness in the literature and the most well-known quantities measuring relatedness are co-citation <ref type="bibr" target="#b114">[115]</ref> and bibliographic coupling <ref type="bibr" target="#b70">[71]</ref>.</p><p>More sophisticated measures of similarities have been proposed as well. In their pioneering work, Klein and Randic <ref type="bibr" target="#b72">[73]</ref> proposed to use the effective resistance between two nodes as a meaningful distance measure. They call this quantity the resistance distance between nodes. Indeed, it can be shown that the effective resistance is indeed a Euclidean distance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b72">73]</ref>. The close link between the effective resistance and the commute-time of a random walker on the graph was established in <ref type="bibr" target="#b17">[18]</ref>. The links between the Laplacian matrix and the commute-time (as well as the Fiedler vector) were studied in <ref type="bibr" target="#b106">[107]</ref>. Therefore, the resistance distance is sometimes called the commute-time distance. Notice that the resistance distance was already used by Cohen in the contex of graph drawing <ref type="bibr" target="#b22">[23]</ref>. Recently, Alves <ref type="bibr" target="#b2">[3]</ref> proposed to define a new random-walk on the graph where transition probabilities are proportional to the effective conductance between the nodes. A new deviation measure between the nodes is then derived from this random walk model.</p><p>On the other hand, Chebotarev and Shamis proposed in <ref type="bibr" target="#b18">[19]</ref> a similarity measure between nodes integrating indirect paths, based on the matrix-forest theorem. Interestingly, this quantity, which we called the ''regularized Laplacian kernel" <ref type="bibr" target="#b45">[46]</ref>, defines a kernel matrix and is related to the Laplacian matrix of the graph. It has been shown that this similarity measure yields a relative importance score intermediate between co-citation/bibliographic coupling and the HITS importance measures <ref type="bibr" target="#b64">[65]</ref>. Ito et al. <ref type="bibr" target="#b64">[65]</ref> further propose a modified version of the regularized Laplacian kernel, the modified regularized Laplacian kernel, by introducing a new parameter controlling importance and relatedness. Moreover, in <ref type="bibr" target="#b113">[114]</ref> it is shown that the regularized Laplacian kernel overcomes some limitations of the von Neumann kernel, when ranking linked documents. This modified regularized Laplacian kernel is also closely related to a graph regularization framework introduced by Zhou and Scholkopf in <ref type="bibr" target="#b141">[142]</ref>. While in <ref type="bibr" target="#b18">[19]</ref>, the authors prove some nice properties of their similarity measure, no experiment investigating its effectiveness was performed.</p><p>The exponential and von Neumann diffusion kernels, based this time on the adjacency matrix, are introduced in <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b111">112]</ref>. The defined kernel matrices are computed through a power series of the adjacency matrix of the graph; they are therefore closely related to graph regularization models <ref type="bibr" target="#b74">[75]</ref>.</p><p>Moreover, some authors recently considered similarity measures based on random-walk or electrical concepts. For instance, Harel &amp; Koren <ref type="bibr" target="#b59">[60]</ref> investigated the possibility of clustering data according to some random-walk related quantities, such as the probability of visiting a node before returning to the starting node. They showed that their algorithm is able to cluster arbitrary nonconvex shapes. White and Smyth <ref type="bibr" target="#b126">[127]</ref> investigated the use of the average first-passage time as a similarity measure between nodes. Their purpose was to generalize the random-walk approach of Page et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b96">97]</ref> by capturing a concept of ''relative centrality" of a given node with respect to some other node of interest. A recent study, comparing several measures (including the average first-passage time) for analyzing the proximity of nodes in a graph in the framework of co-authorship networks, is presented in <ref type="bibr" target="#b78">[79]</ref>. Another application of the Laplacian matrix for semi-supervised learning in the context of image segmentation is <ref type="bibr" target="#b52">[53]</ref>.</p><p>On the other hand, Kondor and Lafferty <ref type="bibr" target="#b74">[75]</ref> as well as Smola and Kondor <ref type="bibr" target="#b115">[116]</ref> defined a graph regularization model related to the graph PCA introduced in <ref type="bibr" target="#b106">[107]</ref>. This model results in the definition of a family of kernels on a graph that provide similarities between nodes, just as any other graph kernel <ref type="bibr" target="#b111">[112]</ref>. An interesting attempt to learn the regularization operator in the context of semi-supervised learning can be found in <ref type="bibr" target="#b143">[144]</ref>. The result is a kernel on a graph maximizing kernel alignment to the labeled data.</p><p>Still another approach has been investigated by Palmer and Faloutsos <ref type="bibr" target="#b97">[98]</ref> who define a similarity function between categorical attributes, called ''refined escape probability", based on random walks and electrical networks. They show that this quantity provides a reasonably good measure for clustering and classifying categorical attributes.</p><p>The ''commute-time" (CT) kernel has been introduced in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b106">107]</ref> and was inspired by the already mentioned work of Klein and Randic <ref type="bibr" target="#b72">[73]</ref> and Chandra et al. <ref type="bibr" target="#b17">[18]</ref>. It takes its name from the average commute-time, which is defined as the average number of steps a random walker, starting from a given node, will take before entering another node for the first time, and go back to the starting node. The CT kernel is defined as the inner product in a Euclidean space where the nodes are exactly separated by the commute-time or resistance distance. Another interpretation of the CT kernel, based on electrical network concepts, will be provided in this paper. The CT kernel is also closely related to the ''Fiedler vector" <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b88">89]</ref>, widely used for graph partitioning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b101">102]</ref> or clustering <ref type="bibr" target="#b33">[34]</ref>, as detailed in <ref type="bibr" target="#b45">[46]</ref>. An interesting method allowing to efficiently compute truncated commute-time neighbors appears in <ref type="bibr" target="#b107">[108]</ref>. Finally, a family of dissimilarity measure reducing to the shortestpath distance at one end and to the commute-time (or resistance) distance on the other hand is proposed in <ref type="bibr" target="#b133">[134]</ref>.</p><p>Almost at the same period, Qiu and Hancock <ref type="bibr" target="#b102">[103]</ref>, Ham et al. <ref type="bibr" target="#b58">[59]</ref>, Yen et al. <ref type="bibr" target="#b134">[135]</ref> as well as Brand <ref type="bibr" target="#b10">[11]</ref> defined the same CT embedding, preserving the commute-time distance, and applied it to image segmentation and multi-body motion tracking <ref type="bibr" target="#b102">[103]</ref>, to dimensionality reduction of manifolds <ref type="bibr" target="#b58">[59]</ref>, to clustering <ref type="bibr" target="#b134">[135]</ref> as well as to collaborative filtering <ref type="bibr" target="#b10">[11]</ref>, with interesting results. Qiu and Hancock <ref type="bibr" target="#b102">[103]</ref> also establish interesting links between the diffusion map <ref type="bibr" target="#b89">[90]</ref>, the Laplacian eigenmap <ref type="bibr" target="#b5">[6]</ref> and the commute-time embedding <ref type="bibr" target="#b102">[103,</ref><ref type="bibr" target="#b106">107]</ref>. On the other hand, Zhou <ref type="bibr" target="#b142">[143]</ref> uses the average first-passage time between two nodes as a dissimilarity index in order to cluster them. He studies various greedy clustering techniques based on this dissimilarity index. An application of the CT distance to semi-supervised learning is proposed in <ref type="bibr" target="#b32">[33]</ref>. In this paper, it is shown that the CT kernel is a discrete Green function (see also <ref type="bibr" target="#b102">[103]</ref> who made the same observation). This framework can be exploited in order to infer class labels in a semi-supervised setting; good results are reported on several databases. Average first-passage times and commute times were also exploited for inducing hidden Markov models from sequential data <ref type="bibr" target="#b13">[14]</ref>. Yet another similarity measure related to the average fist-passage time appears in <ref type="bibr" target="#b120">[121]</ref>. It is defined as the escape probability, that is the probability that a random walker starting from one node will visit the other node, before returning to the starting node. The resulting similarity is directed and closely related to the effective conductance between the two nodes. Also related is the measure investigated by Koren et al. <ref type="bibr" target="#b75">[76]</ref>. In this work, the authors propose to replace the effective conductance by a cycle-free effective conductance.</p><p>In two recent papers, Nadler et al. <ref type="bibr" target="#b89">[90]</ref> and Pons and Latapy <ref type="bibr" target="#b99">[100]</ref> proposed a well-formulated distance measure between nodes of a graph based on a continuous-time diffusion process, called the ''diffusion distance". A valid kernel, called the ''Markov diffusion kernel" has been derived from this diffusion distance in <ref type="bibr" target="#b46">[47]</ref>. An application of the diffusion distance to dimensionality reduction and graph visualization is provided in <ref type="bibr" target="#b76">[77]</ref>. The natural embedding induced by the diffusion distance is called the ''diffusion map" by Nadler et al. <ref type="bibr" target="#b89">[90]</ref>. Moreover, in <ref type="bibr" target="#b99">[100]</ref>, Pons and Latapy defined a hierarchical clustering approach for clustering the nodes according to the squared diffusion distance.</p><p>Two recent meaningful relatedness measures between nodes based on the PageRank procedure <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b96">97]</ref> were proposed in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b120">121]</ref>. In the two last works, the authors propose a random-walk with restart procedure while in the first work, Gori and Pucci define a random-walk process starting from the node of interest, controlled by some pre-computed correlation matrix between nodes. These two algorithms are inspired from the well-known PageRank procedure, adapted in order to provide relative similarities between nodes. Yet another PageRank-inspired algorithm defining similarities between nodes was proposed in <ref type="bibr" target="#b7">[8]</ref>. It provides a general way of computing similarities between the nodes of two different graphs. Applying this procedure to the same graph allows to find self-similarities, that is, similarities between nodes of the same graph. Finally, a similarity between nodes based on the number of different paths connecting two nodes, and therefore on the maximum flow/minimum cut appears in <ref type="bibr" target="#b79">[80]</ref>. This measure has been tested in two collaborative recommendation tasks <ref type="bibr" target="#b45">[46]</ref>, but did not perform well in this context. Some recent attempts to define similarity measures on nodes of a graph for clustering purposes are <ref type="bibr" target="#b122">[123,</ref><ref type="bibr" target="#b139">140]</ref> (performed in parallel with the present work). In <ref type="bibr" target="#b122">[123]</ref>, Villa et al. use a batch version of the kernel self-organizing map, introduced in <ref type="bibr" target="#b82">[83]</ref>, based on an exponential diffusion kernel, in order to cluster a medieval social network. On the other hand, Zhang and coworkers perform in <ref type="bibr" target="#b139">[140]</ref> a hierarchical clustering directly from the similarities provided by an exponential diffusion kernel. They illustrate the technique on social networks data, such as the karate club and the football team networks. Another recent attempt comes from the field of parallel and distributed processing. Meyerhenke et al. propose to cluster nodes of a graph by using a random-walk model <ref type="bibr" target="#b87">[88]</ref>. A nice feature of their algorithm is that it does not need to compute the full kernel matrix. Instead, their k-means-like procedure involves solving as many linear systems of equations as clusters at each iteration.</p><p>There are also several attempts to generalize graph kernels to directed graphs. For instance, an extension of the Laplacian matrix to directed graphs is proposed in <ref type="bibr" target="#b21">[22]</ref> while an extension of the regularized Laplacian kernel to directed graphs appears in <ref type="bibr" target="#b0">[1]</ref>. Zhou et al. <ref type="bibr" target="#b141">[142]</ref> used the regularized normalized Laplacian matrix defined in <ref type="bibr" target="#b21">[22]</ref> in the context of semi-supervised classification of labeled nodes of a directed graph while Chen et al. <ref type="bibr" target="#b19">[20]</ref> used the same kernel matrix, but this time unnormalized, for directed graph embedding. Zhao et al. <ref type="bibr" target="#b140">[141]</ref> propose a directed contextual distance and define a directed graph from which the Laplacian matrix is computed. It is then used for ranking and clustering images.</p><p>Notice that a comprehensive experimental comparison between seven kernels on a graph on two collaborative recommendation tasks appeared in <ref type="bibr" target="#b46">[47]</ref>. The main findings of this experimental comparison are that (i) Laplacian-based methods outperform adjacency-based methods, and (ii) the '' commute-time", the ''regularized Laplacian" and the ''Markov diffusion" kernels provide the best results, comparable to state-of-the-art collaborative recommendation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Top-down, divisive, and bottom-up, agglomerative, approaches</head><p>Several algorithms for partitioning a graph have been developed; only a few papers, the most relevant to the present work, will be mentioned (we refer to Schaeffer <ref type="bibr" target="#b108">[109]</ref>, Newman <ref type="bibr" target="#b92">[93]</ref> and Danon et al. <ref type="bibr" target="#b25">[26]</ref> for documented surveys). A seminal work in this direction was published by Kernighan and Lin <ref type="bibr" target="#b69">[70]</ref> in 1969. In this paper, the authors present a heuristic method for partitioning graphs into subsets of given sizes so as to minimize the sum of all the costs on all edges cuts. This method has been generalized and extended in various ways; see for instance <ref type="bibr" target="#b42">[43]</ref> for a review. However, most of these methods were restricted to partitions of equal size.</p><p>Yet another approach relies on the concept of a LS-set developed in social networks analysis <ref type="bibr" target="#b9">[10]</ref>. A nice property of a LSset is that an element of a LS-set has more links with other elements of the LS-set than with elements not belonging to the LS-set. It has been shown that the set of all LS-sets in a graph can be represented as a series of nested partitions of the set of nodes <ref type="bibr" target="#b80">[81]</ref>. Therefore, bottom-up hierarchical algorithms can be used to compute the LS-sets of a graph <ref type="bibr" target="#b9">[10]</ref>.</p><p>Recursive spectral bisection (RSB) is a spectral-based technique for finding a minimum cut graph bisection <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b144">145]</ref>. The bisection algorithm is often based on the second smallest (the smallest non-trivial) eigenvector of the Laplacian matrix, and has a number of interesting links with spectral clustering (see for instance <ref type="bibr" target="#b1">[2]</ref> and the next section). The eigenvector associated to the second smallest eigenvalue of the Laplacian matrix is known as the Fiedler vector <ref type="bibr" target="#b39">[40]</ref>.</p><p>More recently, in the context of image segmentation, Shi and Malik <ref type="bibr" target="#b112">[113]</ref> introduced the normalized cut (NCut) criterion to define an optimal bipartition of a graph (see also <ref type="bibr" target="#b67">[68]</ref> for a discussion). The criterion used to find the partition seeks a balance between the goal of clustering (finding tight clusters) and segmentation (finding well-separated clusters). Finding the optimal NCut is NP-complete but an approximation can be found by computing the eigenvalues of, for instance, the normalized Laplacian matrix, or simply the Laplacian matrix <ref type="bibr" target="#b112">[113]</ref>. This approach is thus closely related to spectral clustering (see the next section). Other related two-way graph partitioning criteria, such as the ratio cut <ref type="bibr" target="#b57">[58]</ref>, the MinMaxCut <ref type="bibr" target="#b31">[32]</ref> or the isometric cut <ref type="bibr" target="#b53">[54]</ref> were also investigated. The MinMaxCut seemed to provide the best results on experiments performed on the newsgroup database <ref type="bibr" target="#b31">[32]</ref>; see <ref type="bibr" target="#b29">[30]</ref> for a thorough review. Links between these bipartition techniques and spectral clustering are investigated in <ref type="bibr" target="#b123">[124]</ref> while <ref type="bibr" target="#b53">[54]</ref> contains a review of algebraic, spectral, geometric as well as heuristic techniques for graph partitioning.</p><p>Another approach aims to discover communities within graphs based on the notion of electrical voltage drop <ref type="bibr" target="#b128">[129]</ref>. It views the graph as an electrical network, applies a voltage of +1 at one node and 0 at another node. Then, the nodes are partitioned in two sets according to their voltage. Notice that the model proposed in this paper has also an interpretation in terms of electrical network (see Section 3.3). On the other hand, Van Dongen <ref type="bibr" target="#b121">[122]</ref> and Enright et al. <ref type="bibr" target="#b37">[38]</ref> developed a random-walk based technique, called the Markov cluster algorithm. This method simulates random walks within the graph by alternating two operators called expansion and inflation. These two operators are repeated until the graph is split into two separated components. Yet another method relies on information-theoretic concepts <ref type="bibr" target="#b104">[105]</ref>. It decomposes the network into smaller parts by finding an optimal compression of its topology.</p><p>Finally, Newman and Girvan <ref type="bibr" target="#b50">[51]</ref> recently introduced a new measure of the cohesion of communities in a social network, called the modularity, which became quickly popular, especially in physics. This measure, apart from the fact of having a clear interpretation, benefits from the nice property of being able to find the ''natural" number of clusters. Indeed, the modularity measure does not always increase when the number of clusters increases, as is the case for most clustering criteria. It is studied in an increasing number of publications <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b117">118]</ref>; for instance, it has been reformulated as a spin-glass statistical physics problem <ref type="bibr" target="#b103">[104]</ref> involving simulated annealing. A robust clustering algorithm based on simulated annealing was recently proposed in <ref type="bibr" target="#b131">[132]</ref>. Notice that clustering based on modularity optimization has also been recast into a spectral clustering problem <ref type="bibr" target="#b127">[128,</ref><ref type="bibr" target="#b93">94]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Spectral clustering approaches</head><p>There is also a vast literature on spectral graph theory (see for instance <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b108">109]</ref>) and several results about the spectrum of graphs are summarized in <ref type="bibr" target="#b88">[89]</ref>. As mentioned in <ref type="bibr" target="#b106">[107]</ref>, spectral techniques have been applied in a wide variety of contexts including high performance computing <ref type="bibr" target="#b101">[102]</ref>, image segmentation <ref type="bibr" target="#b112">[113]</ref>, web pages ranking <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b96">97]</ref>, information retrieval <ref type="bibr" target="#b26">[27]</ref>, RNA motif classification <ref type="bibr" target="#b48">[49]</ref>, data clustering <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b135">136]</ref>, and dimensionality reduction <ref type="bibr" target="#b5">[6]</ref>. Once more, only a few relevant work will be mentioned here.</p><p>Spectral clustering refers to a collection of techniques that cluster observations using eigenvectors of a derived affinity matrix containing similarities between the observations (see for instance <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b125">126]</ref>; see also the survey of von Luxburg <ref type="bibr" target="#b123">[124]</ref> and Ding <ref type="bibr" target="#b29">[30]</ref>). One of the primary application of spectral clustering is graph drawing and clustering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b106">107]</ref>; it therefore has a number of links with graph partitioning. Indeed, a large amount of work is devoted to the analysis of the relationships between spectral clustering and spectral approaches to graph partitioning. In this context, some relevant works are <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b135">136]</ref>; see <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b123">124]</ref> for a survey.</p><p>The related problem of the Laplacian eigenmap is a dimensionality reduction procedure that has been proposed by Belkin and Niyogi <ref type="bibr" target="#b5">[6]</ref>. It defines an embedding of the observations based on the generalized eigendecomposition of the Laplacian matrix. This appears to be the same embedding that is computed with the spectral clustering algorithm from Shi and Malik <ref type="bibr" target="#b112">[113]</ref>, as noted in <ref type="bibr" target="#b125">[126]</ref>, as well as the same embedding as proposed by Zien et al. in <ref type="bibr" target="#b144">[145]</ref>.</p><p>Smola and Kondor highlight several relationships between spectral graph theory and graph kernels <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b115">116]</ref>. In particular, they define a graph regularization which aims to emphasize the role of the smallest non-trivial eigenvalues of a regularized Laplacian matrix and, on the contrary, discard the largest ones. This, of course, has interesting links with the commute-time kernel (see <ref type="bibr" target="#b45">[46]</ref>). Moreover, Meila and Shi present an interesting link between spectral segmentation and Markov random walks <ref type="bibr" target="#b86">[87]</ref>. The random-walk model they introduced is identical to the one defined in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b106">107]</ref>, but different properties are stressed. Finally, Donetti and Munoz <ref type="bibr" target="#b33">[34]</ref> and Yen et al. <ref type="bibr" target="#b134">[135]</ref> use the few smallest non-trivial eigenvectors of the Laplacian matrix in order to define an embedding of the nodes. They then use classical clustering techniques in this space in order to find communities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The sigmoid commute-time kernel on a graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic notations and definitions</head><p>Let us consider a weighted, undirected, graph, G, composed of n nodes and with symmetric weights w ij P 0 between every couple of nodes, i and j, which are linked by an edge. The weight w ij of the edge connecting node i and node j should be set to some meaningful value, with the following convention: the more important the affinity between node i and node j, the larger the value of w ij , and consequently the easier the communication through the edge. The elements a ij of the adjacency matrix A of the graph are defined in a standard way as a ij = w ij if node i is connected to node j and 0 otherwise. Based on the adjacency matrix, the Laplacian matrix L of the graph is defined in the usual manner: L = D À A, where D = Diag(a i. ) is the outdegree matrix, with diagonal entries d ii ¼ ½D ii ¼ a i: ¼ P n j¼1 a ij . Furthermore, the volume of the graph is defined as</p><formula xml:id="formula_0">v g ¼ volðGÞ ¼ P n i¼1 d ii ¼ P n i;j¼1 a ij .</formula><p>We suppose that the graph has one single connected component; that is, any node can be reached from any other node of the graph. In this case, L has rank n À 1, where n is the number of nodes <ref type="bibr" target="#b20">[21]</ref>. Moreover, it can be shown that L is symmetric and positive semi-definite (see for instance <ref type="bibr" target="#b20">[21]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The sigmoid commute-time kernel</head><p>The ''commute-time" kernel (K CT ) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b106">107]</ref> takes its name from the average commute-time, n(i, j), which is defined as the average number of steps a random walker, starting in node i-j, will take before entering a node j for the first time, and go back to i. Indeed, we associate a Markov chain to the graph in the following obvious manner. A state is associated to every node (n in total), and the transition probabilities are given by p ij = a ij /a i. . One can show <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b106">107]</ref> that, in this case, the average commute-time can be computed thanks to nði; jÞ</p><formula xml:id="formula_1">¼ v g ðe i À e j Þ T L þ ðe i À e j Þ<label>ð1Þ</label></formula><p>where every node i of the graph is represented by a basis vector, e i (the ith column of the identity matrix I), in the Euclidean space R n and v g is the volume of the graph. L + is the Moore-Penrose pseudoinverse of the Laplacian matrix of the graph and is positive semi-definite. Thus, (1) is a Mahalanobis distance between the nodes of the graph and is referred to as the ''commutetime distance" or the ''resistance distance" because of a close analogy with the effective resistance in electrical networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b72">73]</ref>. One can further show that L + is the matrix containing the inner products of the node vectors in the Euclidean space where these node vectors are exactly separated by commute-time distances <ref type="bibr" target="#b45">[46]</ref>. In other words, the entries of L + can be viewed as similarities between nodes and L + is a kernel matrix:</p><formula xml:id="formula_2">K CT ¼ L þ<label>ð2Þ</label></formula><p>The intuition behind this kernel is the following: the kernel clustering will implicitly be performed in the natural embedding space in which the coordinates of the nodes are provided by the eigenvectors of L + , weighted by their corresponding eigenvalue. The coordinate that contributes most is the dominant eigenvector of L + (corresponding to the largest eigenvalue of L + ) and is, in fact, the same as the smallest non-trivial eigenvector of L (L and L + have the same set of eigenvectors, but inverse eigenvalues, except the zero eigenvalues which remain equal to zero; see for instance <ref type="bibr" target="#b45">[46]</ref>) which, in turn, is precisely the well-known Fiedler vector <ref type="bibr" target="#b39">[40]</ref>. It is well-known that this vector conveys some important information about the structure of the graph <ref type="bibr" target="#b20">[21]</ref>. It is therefore the basis of numerous graph partitioning or spectral clustering techniques (see for instance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b101">102]</ref> or the related work section above). Of course, the second coordinate also conveys some important information, albeit to a lesser extend, etc. The sigmoid commute-time kernel K S CT is obtained by applying a sigmoid transformation <ref type="bibr" target="#b109">[110]</ref> on K CT . In other words, each element of the kernel matrix is given by the formula</p><formula xml:id="formula_3">½K S CT ij ¼ 1=ð1 þ exp½Àal þ ij =rÞ<label>ð3Þ</label></formula><formula xml:id="formula_4">where l þ ij ¼ ½L þ ij ,</formula><p>that is, element i, j of the matrix L + , and r is a normalizing factor, corresponding to the standard deviation of the elements of L + . The parameter a will be set to a constant value determined by informal preliminary tests. The sigmoid function aims to normalize the range of similarities in the interval [0, 1] [110] and is similar to the hyperbolic tangent transformation, up to a rescaling factor. Notice, however, that, even if the sigmoid transformation of a kernel matrix is presented as a kernel in <ref type="bibr" target="#b109">[110]</ref>, it is not necessarily positive semi-definite. Thus, strictly speaking, it is not a valid kernel matrix (it is not guaranteed to be a Mercer kernel). One could argue that using the similarity matrix defined by (3) in kernel k-means algorithms is not technically sound. This is true, at least partially, but we will provide three arguments in favor of the use of the sigmoid CT similarity. Firstly, and most importantly, even if the similarity matrix is not positive semi-definite, the convergence of the kernel kmeans algorithms to a minimum of the criterion can be proved (see <ref type="bibr">Section 4.4)</ref>. The kernel k-means therefore iteratively minimizes this criterion, even if it becomes negative. Secondly, and this is a pragmatic reason, the systematic comparisons (see Section 6, describing the experiments) show that the sigmoid commute-time kernel (Eq. ( <ref type="formula" target="#formula_3">3</ref>)) performs much better than the commute-time kernel for clustering, basically because the range of values of the commute-time kernel has quite a large spread, which is reduced by applying the sigmoid function. This large spread of values causes the presence of a number of outliers and perturbs the k-means which is known to be quite sensitive to outliers. Thus, taking a sigmoid transformation appears to be quite beneficial. Finally, but this is largely of philosophical nature, and even maybe stupid, nothing prevents us to work in a complex feature space (as in general relativity) when the kernel matrix is symmetric but non-positive-definite. In this case, some coordinates (corresponding to real eigenvalues) are real while others (corresponding to negative eigenvalues) are imaginary. Although, some of the coordinates become imaginary, all the quantities of interest are nevertheless real numbers since they are obtained through inner products, providing real numbers. This, of course, introduces a lot of open questions that will not be considered in this paper -for instance the fact that distances could be negative.</p><p>Alternatively, a valid positive semi-definite kernel could be computed (see for instance <ref type="bibr" target="#b105">[106]</ref>) from the similarity matrix (3), but this adds an extra computational cost to the clustering procedure. Notice finally that no attempt has been made in order to adapt automatically the kernel parameters. Indeed, there exists some adaptive methods for choosing the kernel parameters, e.g. <ref type="bibr" target="#b136">[137]</ref>; this is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">An electrical network interpretation of the commute-time kernel</head><p>In the previous section, the commute-time kernel was justified by a random-walk model on a graph. But it is also wellknown that the commute times have an electrical interpretation <ref type="bibr" target="#b17">[18]</ref>; see also <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b118">119]</ref>. Basically, they are equivalent, up to a scaling factor, to the effective resistance <ref type="bibr" target="#b17">[18]</ref> between nodes in a derived electrical network.</p><p>We provide here an electrical interpretation of the commute-time kernel K CT = L + (as suggested in <ref type="bibr" target="#b45">[46]</ref>). Let us define an electrical network from the original graph G, where a capacity (the inverse of the resistance) a ij &gt; 0 is assigned to each existing link; the capacity being 0 otherwise. A voltage v i is associated to each node i as well as a current i ij to each link. We will show that each column of</p><formula xml:id="formula_5">L þ ; l þ k ¼ L þ e k ,</formula><p>corresponds to the column vector containing the values of the voltage v on each node of the network when (i) a unit current is injected (source) in node k, (ii) a current 1/n is removed (sink) from each node of the network, and (iii) the voltage is centered, v T e = 0, where v contains the v i and e is a column vector filled with 1s.</p><p>From Kirchhoff's first law, we have</p><formula xml:id="formula_6">X j2NðlÞ i lj ¼ sourceðlÞ þ sinkðlÞ ¼ dðl À kÞ À 1 n<label>ð4Þ</label></formula><p>where the sum is taken on all the neighbors of node l, that is, adjacent to l, N(l). On the other hand, Kirchhoff's second law simply states that the current traversing a link is directly related to the difference of voltage,</p><formula xml:id="formula_7">i ij = a ij (v i À v j ). Replacing this last equation in (4) provides X j2NðlÞ a lj ðv l À v j Þ ¼ dðl À kÞ À 1 n<label>ð5Þ</label></formula><p>Or, in matrix form,</p><formula xml:id="formula_8">Lv ¼ e e k<label>ð6Þ</label></formula><p>where e e k ¼ ðI À ee T =nÞe k is the projection of e k on the column space of L ( e e k is centered: the column space of L is the set of vectors orthogonal to e). The general solution to this linear system of equations is v ¼ L þ e e k þ ke where ke spans the null space of L. This indicates that the voltage is defined up to a constant value; only the differences of voltage are meaningful. We now pick up the voltage vector v that is centered, i.e. such that e T v = 0. Since L + is centered, L + e = 0 and the vector</p><formula xml:id="formula_9">L þ e e k ¼ L þ e k ¼ l þ k is also centered. Thus e T v ¼ e T L þ e e k þ</formula><p>ke T e ¼ ke T e, which is equal to zero only when k = 0. We therefore ob-</p><formula xml:id="formula_10">tain v ¼ l þ k .</formula><p>Consequently, the similarities provided by the commute-time kernel correspond to the centered voltages when a unit current is injected into the node of interest and uniformly removed from each node of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Sample-space prototype-based kernel clustering</head><p>We now introduce a procedure for deriving prototype-based kernel versions of agglomerative clustering algorithms, where the prototype vectors are defined in the sample space. It relies on three steps.</p><p>(1) Define a criterion, typically the sum of within-cluster inertia, depending on (i) prototype vectors in the feature space (denoted as g k ) and on (ii) membership values indicating the membership of each sample to the cluster (denoted as u ik ). ( <ref type="formula" target="#formula_2">2</ref>) Express the prototype vectors in the feature space in terms of the prototype vectors in the sample space (denoted as h k ; the well-known kernel trick) in the criterion. (3) Optimize the criterion with respect to the prototype vectors, h k , as well as the membership values, u ik . To this end, the standard two-step algorithm used in k-means clustering and variants is applied: (i) optimize the prototype vectors while fixing the membership values, and (ii) optimize the membership values while fixing the prototype vectors. This algorithm is related to block coordinate descent.</p><p>This procedure is now applied in order to derive various kernel-based agglomerative clustering algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Kernel k-means</head><p>The goal is to design an iterative algorithm aiming to minimize a criterion which, in the case of a standard k-means, can be defined, in the feature space, as the total within-cluster inertia:</p><formula xml:id="formula_11">Jðg 1 ; . . . ; g m Þ ¼ X m k¼1 X i2C k kx i À g k k 2<label>ð7Þ</label></formula><p>where the first sum is taken on the m clusters, while the second sum is taken on the nodes i belonging to cluster k, i 2 C k . In Eq. ( <ref type="formula" target="#formula_11">7</ref>), x i is the node vector corresponding to node i and g k is a prototype vector of cluster k in the feature space, while kx i À g k k is the Euclidean distance between the node vector and the cluster prototype it belongs to. The number of clusters, m, is provided a priori by the user. Here, the prototype vector g k of one cluster is defined as a representative of the node vectors belonging to this cluster k. Remember that the node vectors x i represent the nodes in an Euclidean embedding space preserving exactly the distances between nodes -the feature space. We denote by X the n Â p (p is the number of features in the feature space) data matrix containing the transposed node vectors as rows; that is, X = [x 1 , x 2 , . . . , x n ] T . Let us now define the following change of parameter:</p><formula xml:id="formula_12">g k ! X T h k<label>ð8Þ</label></formula><p>corresponding to the ''kernel trick" (see <ref type="bibr" target="#b111">[112]</ref>). It aims to express the prototype vectors, g k , as a linear combination of the node vectors, x i (the columns of X T ). The h k will be called the prototype vectors in the n-dimensional sample space. Now, recompute the within-class inertia in terms of the h k and the inner products:</p><formula xml:id="formula_13">Jðh 1 ; . . . ; h m Þ ¼ X m k¼1 X i2C k ðx i À g k Þ T ðx i À g k Þ ¼ X m k¼1 X i2C k ðx T i x i À 2x T i g k þ g T k g k Þ ¼ X m k¼1 X i2C k ðk ii À 2k T i h k þ h T k Kh k Þ ¼ X m k¼1 X i2C k ðe i À h k Þ T Kðe i À h k Þ<label>ð9Þ</label></formula><formula xml:id="formula_14">where K ¼ XX T ; k ii ¼ ½K ii ¼ x T i x i ; k i ¼ Xx i ¼ col i ðKÞ ¼ Ke i .</formula><p>The k-means iteratively minimizes J by proceeding in two steps, (1) re-allocation of the node vectors while keeping the prototype vectors fixed, and (2) re-computation of the prototype vectors, h k , while maintaining the cluster labels of the nodes fixed. Clearly, the re-allocation step minimizing J is</p><formula xml:id="formula_15">l i ¼ arg min k fðe i À h k Þ T Kðe i À h k Þg<label>ð10Þ</label></formula><p>where l i contains the cluster label of node i. For the computation of the prototype vector, taking the gradient of J with respect to h k and setting the result equal to 0 provides</p><formula xml:id="formula_16">Kh k ¼ 1 n k X i2C k k i ¼ K 1 n k X i2C k e i<label>ð11Þ</label></formula><p>where n k is the number of nodes belonging to cluster k. By looking carefully to Eq. ( <ref type="formula" target="#formula_16">11</ref>), we immediately observe from the left-hand side that Kh k is a linear combination of the k i , while the right-hand side is also an unknown linear combination of the k i . Therefore, one solution 3 to this linear system of equations is</p><formula xml:id="formula_17">h k ¼ 1 n k X i2C k e i<label>ð12Þ</label></formula><p>In other words, h k contains 1/n k if i 2 C k and 0 otherwise; it corresponds to a prototype vector defined for each cluster in the sample space. Therefore the prototype re-computation step is</p><formula xml:id="formula_18">h ki ¼ ½h k i ¼ 1=n k if node i 2 C k 0 otherwise<label>ð13Þ</label></formula><p>This two-step procedure (Eqs. ( <ref type="formula" target="#formula_15">10</ref>) and ( <ref type="formula" target="#formula_18">13</ref>)) is iterated until convergence, and is very similar to the standard k-means algorithm. For a given cluster k, the prototype vector h k contains the degrees of membership of each node to cluster k. For each cluster, these degrees of membership are positive and sum to one. Notice that Zhang and Chen proposed instead <ref type="bibr" target="#b137">[138]</ref> proposed to compute the inverse of K in Eq. ( <ref type="formula" target="#formula_16">11</ref>), which is not needed and does not allow to interpret h k as a membership prototype as in Eq. ( <ref type="formula" target="#formula_18">13</ref>). On the other hand, MacDonald and Fyfe <ref type="bibr" target="#b82">[83]</ref> derived their kernel self-organizing map algorithm (SOM) by translating the standard SOM from the feature space to the sample space, without expressing it as an optimization problem.</p><p>This generic methodology can easily be used in order to derive sample-space prototype-based kernel versions of other standard clustering algorithms, as demonstrated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Kernel fuzzy k-means</head><p>We now apply the same methodology for deriving a kernel fuzzy k-means. This time, the criterion is (see for instance <ref type="bibr" target="#b119">[120]</ref>)</p><formula xml:id="formula_19">Jðg 1 ; . . . ; g m ; UÞ ¼ X m k¼1 X n i¼1 u ik kx i À g k k 2 with X m k¼1 u 1=q ik ¼ 1 for all i<label>ð14Þ</label></formula><p>where the u ik define the degree of membership of node i to cluster C k . The parameter q &gt; 1 is controlling the degree of fuzzyness of the membership functions. As for the kernel k-means, we perform the change of parameters (see ( <ref type="formula" target="#formula_12">8</ref>)), leading to</p><formula xml:id="formula_20">Jðh 1 ; . . . ; h m ; UÞ ¼ X m k¼1 X n i¼1 u ik ðe i À h k Þ T Kðe i À h k Þ ð<label>15Þ</label></formula><p>We thus introduce the following Lagrange function</p><formula xml:id="formula_21">Lðh 1 ; . . . ; h m ; UÞ ¼ X m k¼1 X n i¼1 u ik ðe i À h k Þ T Kðe i À h k Þ þ X n i¼1 k i X m k¼1 u 1=q ik À 1 " #<label>ð16Þ</label></formula><p>Taking the gradient of L with respect to u ik and setting the result equal to zero allows to compute the membership function,</p><formula xml:id="formula_22">u ik ¼ ððe i À h k Þ T Kðe i À h k ÞÞ À1=ðqÀ1Þ P m l¼1 ððe i À h l Þ T Kðe i À h l ÞÞ À1=ðqÀ1Þ " # q<label>ð17Þ</label></formula><p>Moreover, taking the gradient with respect to h k provides</p><formula xml:id="formula_23">Kh k ¼ P n i¼1 u ik k i P n j¼1 u jk ¼ K P n i¼1 u ik e i P n j¼1 u jk<label>ð18Þ</label></formula><p>Thus, the re-computation of the prototype vectors is simply</p><formula xml:id="formula_24">h k ¼ P n i¼1 u ik e i P n j¼1 u jk ;<label>ð19Þ</label></formula><p>or component-wise,</p><formula xml:id="formula_25">h ki ¼ ½h k i ¼ u ik P n j¼1 u jk<label>ð20Þ</label></formula><p>Eqs. ( <ref type="formula" target="#formula_22">17</ref>) and ( <ref type="formula" target="#formula_25">20</ref>) are iterated until convergence. 3 The solution of Eq. ( <ref type="formula" target="#formula_16">11</ref>) is defined up to a vector lying in the null space of K, which has no practical influence since the h k are always multiplied by K. We decide to pick up the solution for which h</p><p>T k e ¼ 1 and h k P 0; that is, the elements of each column vector h k are positive and sum to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Kernel Gaussian mixtures model</head><p>Gaussian mixtures clustering <ref type="bibr" target="#b62">[63]</ref> can be obtained by imposing a predefined level of Kulback-Leibler divergence (KL; see for instance <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b68">69]</ref>) between the membership functions and the a priori probabilities of belonging to the clusters (denoted by p k ). We consequently introduce the following Lagrange function</p><formula xml:id="formula_26">PLðh 1 ; . . . ; h m ; U; PÞ ¼ X m k¼1 X n i¼1 u ik ðe i À h k Þ T Kðe i À h k Þ þ X n i¼1 k i X m k¼1 u ik À 1 " # þ l X n i¼1 X m k¼1 u ik log u ik p k þ KL " # þ g X m k¼1 p k À 1 " #<label>ð21Þ</label></formula><p>By taking the corresponding gradients with respect to u ik , h k , p k , we obtain</p><formula xml:id="formula_27">u ik ¼ p k exp À ðe i Àh k Þ T Kðe i Àh k Þ l h i P m l¼1 p l exp À ðe i Àh l Þ T Kðe i Àh l Þ l h i<label>ð22Þ</label></formula><formula xml:id="formula_28">p k ¼ P n i¼1 u ik n<label>ð23Þ</label></formula><formula xml:id="formula_29">h ki ¼ ½h k i ¼ u ik P n j¼1 u jk<label>ð24Þ</label></formula><p>The parameters p k are representing the a priori probabilities of belonging to a cluster and the membership functions, u ik , correspond to the a posteriori probabilities of membership, assuming a gaussian distribution of the node vectors within each cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Convergence of the kernel clustering algorithms and some computational issues</head><p>Even if the similarity matrix K is not a kernel (not positive semi-definite), the proposed kernel clustering algorithms converge to a local minimum of the criterion J (Eq. ( <ref type="formula" target="#formula_13">9</ref>) for the kernel k-means). We, however, assume that the similarity matrix K is symmetric and contains positive values on its diagonal, k ii P 0 for every i -a positive self-similarity. It is clear that, for each algorithm, both (1) the re-allocation of the nodes step and (2) the re-computation of the prototype vectors step reduce the criterion J, leading to a monotonic decrease of this criterion at each iteration t. Let us denote this decrease as (J(t) À J(t À 1)) = Àd t 6 0 with d t P 0. Now, by examining the constraints on the u ik and the h ki , it is easy to show that the criterion J is bounded from below. Remember that the membership values satisfy 0 6 u ik 6 1 while the prototype vectors satisfy 0 6 h ki 6 1 and P n i¼1 h ki ¼ 1. Now, a rough lower bound can easily be obtained, which is enough for proving convergence. Considering each term of the criterion J ¼ P m k¼1 P n i¼1 u ik h T k Kh k P mnmin ij ðk ij ; 0Þ for the last term. Therefore, the criterion J is bounded from below by J P J inf = mn (min ij (k ij , 0) À 2max ij (k ij )). Now, denoting D = J(0) À J inf , we must have P 1 t¼1 d t 6 D with each d t P 0, which implies lim t?1 d t = 0. Therefore, the clustering algorithm converges to a local minimum of the criterion J.</p><formula xml:id="formula_30">P n i¼1 u ik ðe i À h k Þ T Kðe i À h k Þ in</formula><p>On the other hand, when considering the sigmoid commute-time kernel, the graph kernel clustering procedures necessitate the knowledge of the inverse of the Laplacian matrix of the graph, which can be an issue when dealing with large graphs. First, the computation time for inverting the matrix is significant and, second, even if the original matrix is sparse, the inverse is usually dense and could not fit into main memory. One can conclude that our kernel-based methods do not scale well on large graphs. Therefore, density-based methods, which are exploiting local information, are probably more appropriate when dealing with large graphs.</p><p>One potential, partial, solution to this problem is to use a Cholesky factorization of the kernel matrix <ref type="bibr" target="#b45">[46]</ref> or, even better, a low-rank (or incomplete) Cholesky factorization (see for instance <ref type="bibr" target="#b41">[42]</ref>) which can be computed efficiently an remains sparse if the original matrix is sparse. Once this factorization is computed, every column of the matrix inverse can be obtained by simple back-substitution. However, while this technique exploits the sparseness of the network, it also increases the computation time. Designing kernel-based clustering algorithms able to mine large graphs will be tackled in further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Kernel hierarchical clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">A kernel version of Ward's hierarchical clustering</head><p>In this section, we propose a kernel version of Ward's bottom-up hierarchical clustering. The algorithm starts with one observation per cluster (the within-class inertia is zero) and then merges the clusters that lead to the smallest increase of the total within-cluster inertia (see for instance <ref type="bibr" target="#b119">[120,</ref><ref type="bibr" target="#b124">125]</ref>).</p><p>Initially, each observation forms a cluster, so that, in the feature space, g k = x k , for k 2 {1, 2, . . . , n}. By using the kernel trick defined by Eq. ( <ref type="formula" target="#formula_12">8</ref>), we thus have X T h k = x k ; pre-multiplying this equation by X, we obtain Kh k = k k = Ke k . Therefore, the h k are initialized by h k ¼ e k ; for each k 2 f1; 2; . . . ; ng ð25Þ Now, when merging two clusters, say cluster k and cluster l, the new centroid g m in the feature space is</p><formula xml:id="formula_31">g m ¼ n k g k þ n l g l n k þ n l<label>ð26Þ</label></formula><p>By applying the transformation (8), we obtain the update equation for the prototypes h k :</p><formula xml:id="formula_32">h m ¼ n k h k þ n l h l n k þ n l<label>ð27Þ</label></formula><p>where h m is the prototype vector for the resulting merged cluster (once more, if X T is not of full rank and there is more than one solution, the particular solution ( <ref type="formula" target="#formula_32">27</ref>) is chosen). We observe that if h k and h l verify h</p><p>T k e ¼ 1 and h</p><p>T l e ¼ 1, then h m also verifies h T m e ¼ 1. Since this property is initially true, it remains true during the merging procedure. In fact, element i of vector h k is simply 1/n k if individual i belongs to cluster k and zero otherwise.</p><p>Moreover, it is well-known that merging cluster k and cluster l results in an increase of total within-cluster inertia of DJ ¼ Jðafter mergeÞ À Jðbefore mergeÞ ¼ n k n l n k þn l kg k À g l k 2 (see for instance <ref type="bibr" target="#b119">[120]</ref>) where g k and g l are the centroid vectors of clusters k and l before merging them. When applying the standard transformation <ref type="bibr" target="#b7">(8)</ref>, we obtain</p><formula xml:id="formula_33">DJ ¼ n k n l n k þ n l kg k À g l k 2 ¼ n k n l n k þ n l ðg k À g l Þ T ðg k À g l Þ ¼ n k n l n k þ n l ðh k À h l Þ T Kðh k À h l Þ ð<label>28Þ</label></formula><p>The idea is thus to merge the two clusters k, l that result in the smallest increase in total within-class inertia, DJ, until one single cluster containing all the data remains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Kernel hierarchical clustering and spectral clustering</head><p>Let us now consider instead a top-down approach to hierarchical clustering: we want to split the data into two clusters leading to the maximum decrease in total within-cluster inertia. In other words, DJ = J(before split) À J(after split), must be maximum. Now, define a new prototype vector h ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi</p><formula xml:id="formula_34">n k n l =ðn k þ n l Þ p ðh k À h l Þ; it is clear that h T e = 0.</formula><p>Eq. ( <ref type="formula" target="#formula_33">28</ref>) can be rewritten as</p><formula xml:id="formula_35">DJ ¼ h T Kh<label>ð29Þ</label></formula><p>Now, assuming that the kernel K is centered (Ke = 0) and relaxing the fact that h has a special structure, we define the following related optimization problem: maximize the quadratic form <ref type="bibr" target="#b28">(29)</ref> with some additional constraints,</p><formula xml:id="formula_36">h Ã ¼ max h ðh T KhÞ; subject to h T e ¼ 0 and khk 2 ¼ 1<label>ð30Þ</label></formula><p>The solution of this problem is the first eigenvector (associated to the largest eigenvalue) of the kernel matrix K; since K is centered, the constraint h T e = 0 is automatically satisfied (for a symmetric matrix, the eigenvectors corresponding to distinct eigenvalues are orthogonal to each other and, when the matrix is centered, e is an eigenvector corresponding to eigenvalue 0). If we choose the commute-time kernel, K = L + , we obtain a standard graph partitioning or spectral clustering technique <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b125">126]</ref>. Indeed, in this case, h * is the Fiedler vector <ref type="bibr" target="#b39">[40]</ref> and corresponds to the scores obtained by projecting the nodes on the first principal component of the graph, according to the commute-time distance (see <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b106">107]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>The main objectives of this section are (i) to compare the introduced kernel-based algorithms to other well-known algorithms allowing to cluster nodes, (ii) to evaluate the added value of the sigmoid transformation of the kernel for graph clustering, (iii) to compare the use of the K S CT kernel with other well-known kernels on graph, and (iv) to compare the different kernel clustering procedures (k-means, fuzzy k-means, gaussian mixture, hierarchical).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets description</head><p>Systematic comparisons between the different algorithms are performed on three real-world graphs. A short description is provided for each graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">Newsgroup dataset</head><p>The Newsgroup dataset <ref type="foot" target="#foot_2">4</ref> is composed of about 20,000 unstructured documents, taken from 20 discussion groups (newsgroups) of the Usernet diffusion list. For our experiments, nine subsets related to different topics are extracted from the original database, as listed in Table <ref type="table" target="#tab_1">1</ref>. For each subset, 200 documents are sampled from different newsgroups. Thus, the first three subsets (G- <ref type="figure">2cl-A, G-2cl-B, G-2cl-C</ref>) contain 2 Â 200 = 400 documents sampled from two newsgroups topics, the next three subsets (G-3cl-A, G-3cl-B, G-3cl-C) contain 3 Â 200 = 600 documents sampled from three topics and the last three subsets (G-5cl-A, G-5cl-B, G-5cl-C) contain 5 Â 200 = 1000 documents sampled from five topics. The selected topics can be well-separated or related such as politics/mideast and politics/guns in subset G-5cl-A.</p><p>In order to reduce the high dimensionality of the feature space (terms), the following standard preprocessing steps are performed.</p><p>(1) Stopwords without useful information, as well as special characters, are eliminated <ref type="bibr" target="#b47">[48]</ref>. Only words containing usual letters are kept. Notice that e-mail addresses which appear in each document are also deleted, since they allow in many cases to trivially identify the document's newsgroup. (2) Porter's stemming algorithm <ref type="bibr" target="#b100">[101]</ref> is applied so that each term is reduced to its ''root".</p><p>(3) Terms that occur too few times (&lt;3 occurrences) or in too few documents (&lt;2 documents) are considered as no content-bearing and are eliminated. (4) The mutual information between terms and documents is computed. For a term y, the mutual information with each document x of the dataset <ref type="bibr" target="#b135">[136]</ref> is provided by</p><formula xml:id="formula_37">IðyÞ ¼ X x log½pðx; yÞ=pðxÞpðyÞ<label>ð31Þ</label></formula><p>Terms with a small value of mutual information (fixed at 20% of I(y)'s median) are eliminated. <ref type="bibr" target="#b4">(5)</ref> The term-document matrix W is constructed with the remaining terms and documents. Element [W] ij of the matrix contains the value of the tf.idf factor <ref type="bibr" target="#b84">[85]</ref> between the term i and the document j. (6) Each row of the term-document matrix W is normalized to 1.</p><p>Finally, the adjacency matrix defining the links between documents is given by the sum of all document-term-document paths connecting all pairs of documents through the terms they have in common. In other words, if W represents the termdocument matrix containing the tf.idf factors, the adjacency matrix of the resulting document-document graph is provided by A = W T W.</p><p>For example, the subset G-2cl-A is composed of 400 documents, and 2898 terms (stopwords being already eliminated). After the preprocessing step, the term-document matrix is composed of 1,490 terms and 400 documents. The conversion to a document-document adjacency matrix finally provides a graph composed of 400 document nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">IMDb dataset</head><p>This dataset is extracted from the well-known movie database website IMDb, as preprocessed by Neville et al. <ref type="bibr" target="#b91">[92]</ref> and Macskassy et al. <ref type="bibr" target="#b83">[84]</ref>. Movies released in the United States between 1996 and 2001 were gathered to form a network where the edge between two movie nodes are weighted by the number of production companies in common. A connected subgraph of 1126 nodes is extracted and labeled according to their success at their release (determined by more than $2 million in the first weekend box-office receipts) or not (low-revenue).</p><p>The clustering task is to discriminate the high-revenue movies from the low-revenue ones, based on their shared production companies. Table <ref type="table" target="#tab_2">2</ref> shows the number of nodes for each of the two clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3.">CORA dataset</head><p>First proposed by McCallum et al. <ref type="bibr" target="#b85">[86]</ref> and then preprocessed by Macskassy et al. <ref type="bibr" target="#b83">[84]</ref>, Cora is originally a computer research papers internet portal. From the database, 3385 documents were extracted from seven different topics to form a connected graph. Two documents are connected by an undirected edge if document cites the other one. The weight is set to one, or two if both papers cite mutually. Since a paper tends to cite papers from the same research topic, it should be possible to cluster the seven different research domains (shown in Table <ref type="table" target="#tab_3">3</ref>) based on the citation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental settings and parameter tuning</head><p>Let us now describe the experimental settings used in all our experiments. Suppose we have a graph of n nodes to be partitioned into m clusters. For all kernel clustering algorithms, the prototype vectors h k (k = 1,. . . , m) are initialized by randomly selecting m columns of the identity matrix I.</p><p>Besides the commute-time kernel, the other investigated kernels are the Laplacian exponential diffusion kernel <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b115">116]</ref>, the von Neumann diffusion kernel <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b111">112]</ref> and the regularized Laplacian kernel <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b115">116]</ref>. Their mathematical definition is provided below in this section.</p><p>Notice that some parameters need to be tuned:</p><p>The parameter a when computing the sigmoid transformation of the K S CT (see Eq. ( <ref type="formula" target="#formula_3">3</ref>)). The parameter q which controls the degree of fuzzyness for the kernel fuzzy k-means (see Eq. ( <ref type="formula" target="#formula_22">17</ref>)).</p><p>The parameter l for the kernel gaussian mixtures model (see Eq. ( <ref type="formula" target="#formula_27">22</ref>)).</p><p>The parameters tuning was performed on an independent dataset. It was extracted from the newsgroup dataset and is composed of 598 new documents sampled within the topic 'medicine/general', 'motor/autos' and 'politics/guns' of the newsgroup dataset. At first, the K S CT k-means (since only the parameter a needs to be tuned for this algorithm) is launched 30 times for various values of a. Based on the average clustering rate, a is set to 7 for the sigmoid transformation (this value provided the best results). Once the a value settled, the parameter value of the K S CT fuzzy k-means, K S CT entropy-based fuzzy k-means and K S CT gaussian mixture are varied and the best value is retained. q is therefore set to 1.2, while l is set to 0.02 for the K S CT gaussian mixtures.</p><p>The same methodology is then applied to the other kernels (see <ref type="bibr" target="#b46">[47]</ref> for another experimental comparison between these kernels on a graph) in order to fix the parameter values:</p><p>The parameter a for the Laplacian exponential diffusion kernel, defined as K LED = exp(ÀaL) (matrix exponential). The parameter a for the von Neumann diffusion kernel, defined as K VND = (I À aA) À1 . The parameter a for the regularized Laplacian kernel, defined as K RL = (I + aL) À1 .</p><p>Based on the averaged results obtained on 30 runs of the clustering procedure, a is set to 1 for K LED , 10 À5 for K VND , and 10 3 for K RL .</p><p>To assess the impact of the sigmoid transformation, changes in the clustering performance when applying the sigmoid transformation to those classical kernels are also evaluated. In this case, the parameters are tuned jointly with a, as described before for K S CT . Parameters a and a are therefore set, respectively, to 0.1 and 4 for the sigmoid transformation of the Laplacian exponential diffusion kernel (denoted as K S LED ), 0.1 and 3 for the sigmoid transformation of von Neumann diffusion kernel (denoted as K S VND ), 1 and 4 for the sigmoid transformation of the regularized Laplacian kernel (denoted as K S RL ).  )) for all the kernels with and without a sigmoid transformation: the commute-time kernel (K CT and K S CT ), the Laplacian exponential diffusion kernel (K LED and K LED S ), the von Neumann diffusion kernel (K VND and K S VND ), and the regularized Laplacian kernel (K RL and K S RL ). The performances are reported for all the kernel clustering methods (simple k-means (kmeans), fuzzy k-means (fuzzy), gaussian mixture (gaussian), and hierarchical clustering (hierar.)), which are compared to the spherical k-means (Spher.), Ng's spectral clustering (Ng spec.), Girvan's edge centrality-based method (Girvan), Donetti's spectral analysis (Donetti), Duch's extremal optimization <ref type="bibr">(Duch)</ref>, and Reichardt's simulated annealing <ref type="bibr">(Reichardt)</ref>. The standard deviations on the 30 runs are also indicated.  )) for all the kernels with and without a sigmoid transformation: the commute-time kernel (K CT and K S CT ), the Laplacian exponential diffusion kernel (K LED and K S LED ), the von Neumann diffusion kernel (K VND and K S VND ), and the regularized Laplacian kernel (K RL and K S RL ). The performances are reported for all the kernel clustering methods (simple k-means (kmeans), fuzzy k-means (fuzzy), gaussian mixture (gaussian), and hierarchical clustering (hierar.)), which are compared to the spherical k-means (Spher.), Ng's spectral clustering (Ng spec.), Girvan's edge centrality-based method (Girvan), Donetti's spectral analysis (Donetti), Duch's extremal optimization <ref type="bibr">(Duch)</ref>, and Reichardt's simulated annealing <ref type="bibr">(Reichardt)</ref>. The standard deviations on the 30 runs are also indicated.  )) for all the kernels with and without a sigmoid transformation: the commute-time kernel (K CT and K S CT ), the Laplacian exponential diffusion kernel (K LED and K S LED ), the von Neumann diffusion kernel (K VND and K S VND ), and the regularized Laplacian kernel (K RL and K S RL ). The performances are reported for all the kernel clustering methods (simple k-means (kmeans), fuzzy k-means (fuzzy), gaussian mixture (gaussian), and hierarchical clustering (hierar.)), which are compared to the spherical k-means (Spher.), Ng's spectral clustering (Ng spec.), Girvan's edge centrality-based method (Girvan), Donetti's spectral analysis (Donetti), Duch's extremal optimization <ref type="bibr">(Duch)</ref>, and Reichardt's simulated annealing <ref type="bibr">(Reichardt)</ref>. The standard deviations on the 30 runs are also indicated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results and discussions</head><p>In a first step, a full, systematic, comparison is carried out on the nine subsets of the newsgroup dataset by considering all the kernel clustering methods and all the other competing clustering methods. Then, in a second step, only the kernel clustering methods providing good performances on the newsgroup datasets are further investigated on the IMDb and the CORA datasets, and compared to the competing clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1.">Results on the newsgroup dataset</head><p>A full comparison was carried out on the nine subsets of the newsgroup dataset by considering the four different kernel clustering methods (i.e. kernel k-means, kernel fuzzy k-means, kernel gaussian mixture, and kernel hierarchical clustering) applied with the 8 (4 Â 2) different kernels (i.e., the commute-time kernel K CT , the Laplacian exponential diffusion kernel K LED , the von Neumann diffusion kernel K VND , and the regularized Laplacian kernel K RL ; with and without sigmoid transformation). The classification rate (after performing an optimal assignment of the clusters) as well as the adjusted Rand index <ref type="bibr" target="#b38">[39]</ref>, averaged on 30 runs, are reported in three different tables, Tables <ref type="table" target="#tab_4">4</ref><ref type="table" target="#tab_5">5</ref><ref type="table" target="#tab_6">6</ref>. The results for the two-classes subsets (G-2cl-A, G-2cl-B and G-2cl-C) are reported in Table <ref type="table" target="#tab_4">4</ref>, the results for the three-classes subsets (G-3cl-A, G-3cl-B and G-3cl-C) are listed in Table <ref type="table" target="#tab_5">5</ref> while the results for the five-classes subsets (G-5cl-A, G-3cl-B and G-3cl-C) are listed in Table <ref type="table" target="#tab_6">6</ref>.</p><p>All these kernel-based methods are compared to the performances obtained by the spherical k-means [29], Ng's spectral clustering <ref type="bibr" target="#b95">[96]</ref>, Girvan's edge centrality-based method [51], Donetti's spectral analysis <ref type="bibr" target="#b33">[34]</ref>, Duch's extremal optimization <ref type="bibr" target="#b36">[37]</ref>, and Reichardt's simulated annealing <ref type="bibr" target="#b103">[104]</ref>. The last four algorithms come from the communities detection domain and are able to determine the natural number of clusters, together with the cluster memberships. However, in order to allow a fair comparison of the clustering performances, the number of communities will be fixed. This adaptation can be easily obtained by stopping the iteration when the desired number of clusters is reached. These algorithms were chosen because they performed well in the comparative study published by Danon et al. <ref type="bibr" target="#b25">[26]</ref>.</p><p>As for the kernel-based methods, each algorithm is run 30 times (excepting the Reichardt's simulated annealing which is averaged only on 5 runs, due to its excessive execution time), and the classification rate as well as the adjusted Rand index, averaged on all runs, are computed. For all the k-means-based algorithms that are dependent on the initial conditions (such as the kernel k-means, the spherical k-means, etc.), each run consists in 50 trials: the clustering algorithm is launched 50 times with different initial conditions and the best clustering among these 50 trials, having the minimal within-class inertia, is sent back as the solution. Notice that Girvan's edge centrality-based method is only tested on the six smallest graphs, and Reichardt's simulated annealing on the seven smallest, due to their large execution time.  )) for all the kernels with and without a sigmoid transformation: the commute-time kernel (K CT and K S CT ), the Laplacian exponential diffusion kernel (K LED and K S LED ), the von Neumann diffusion kernel (K VND and K S VND ), and the regularized Laplacian kernel (K RL and K S RL ). The kernel-based algorithms are assessed when performing a kernel k-means (kmeans) and a kernel fuzzy k-means (fuzzy), and are compared to Ng's spectral clustering (Ng spec.), Donetti's spectral analysis (Donetti), Duch's extremal optimization (Dutch), and Reichardt's simulated annealing <ref type="bibr">(Reichardt)</ref>. The standard deviations on the 30 runs are also indicated. Globally, from Tables <ref type="table" target="#tab_4">4</ref><ref type="table" target="#tab_5">5</ref><ref type="table" target="#tab_6">6</ref>, we clearly observe that the sigmoid transformation improves the performances of all the tested kernels. Moreover, the kernel k-means and the kernel fuzzy k-means combined with the K S CT kernel obtain the best overall results on these datasets, with very stable performances. K S RL used with the kernel k-means also offers competitive results and, indeed, it can be shown that the regularized Laplacian kernel tends to an approximation of the commute-time kernel (up to an additive constant) when the parameter a increases. The kernel hierarchical clustering performs well on datasets with a low number of clusters, but its performance degrades when the number of clusters increases. This can be explained by the fact that hierarchical clustering uses a greedy technique that does not question previous decisions. These experiments also show that the K S CT gaussian mixtures model does not provide satisfactory results. This is probably due to the fact that the estimation of a gaussian distribution is quite sensible to outliers.</p><p>What concerns the community detection algorithms, the results show a similar trend as in <ref type="bibr" target="#b25">[26]</ref>, i.e. the best performances are obtained by Duch's extremal optimization and Reichardt's simulated annealing. <ref type="bibr" target="#b25">[26]</ref> also noticed the difficulty for Girvan's edge centrality-based method to discriminate communities when the graph fuzziness increases. We made the same observation in our experiments where Girvan's method provides bad results. For a small number of clusters, Reichardt's method offers competitive results, similar to the kernel k-means combined with K S CT . However, when the number of clusters increases (especially for five clusters), the performances of Reichardt's method drop down in comparison with K S</p><p>CT . An informal test on artificial graphs shows a decrease of performance for Duch's extremal optimization when the graph node's degree is low. On the other hand, the simulated annealing suffers from its large execution time. The results should be moderated by the fact that the number of clusters is arbitrarily fixed despite the ability of the community detection algorithms to propose a natural number of clusters.</p><p>Notice that an informal experiment was also performed, centering the von Neumann diffusion kernel and the regularized Laplacian kernel before applying the sigmoid transformation, without any performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2.">Results on the CORA and the IMDb datasets</head><p>Based on the results obtained on the newsgroup subsets, only the kernel k-means and the kernel fuzzy k-means, which obtained the best performance, are further tested on the CORA and IMDb datasets. The results are shown in Table <ref type="table" target="#tab_8">7</ref>. The conclusions are quite the same as for the Newsgroup dataset: the sigmoid commute-time kernel K S CT and the sigmoid regularized Laplacian kernel K S RL still obtain the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3.">General discussion of the results</head><p>It can further be observed that the K S CT fuzzy k-means provides slightly better results than the K S CT k-means on the Newsgroup datasets. This probably comes from the fact that this dataset is itself fuzzy. It is hard to define clear boundaries between the different topics: a discussion within a specific newsgroup can also be related to other domains. example of the results of a K S CT fuzzy clustering for the subset G-5cl-A. For each topic, the membership-probability value (cf. Eq. ( <ref type="formula" target="#formula_22">17</ref>)) of each document of the subset is displayed. It can be observed that the membership-probability value for the two topics on politics is not as clearly marked as for other subjects. It is probably due to the fact that, in politic, various subjects can be discussed. Moreover, a close examination of the membership functions shows that, for some documents, the membership value is almost equal for the five clusters. An example of such a document is shown here below. It is document 21759, picked from the topic religion/christian: Document 21759: The text contains 80 lines devoted to a defence of the doctrine of predestination as applied to the salvation of individuals. There is then a five-line post-script on the Balkans. It is natural and easy to keep the Subject line of the post that one is replying to, but when the focus shifts, keeping the same Subject can cause confusion.</p><p>The fuzzy membership vector for this document provided by the K S CT fuzzy k-means indicates that the document belongs to the topic computer/windowsx with a membership value of 0.25, cryptography/general with 0.20, politics/mideast with 0.13, politics/guns with 0.20 and religion/chritian with only 0.22. Actually, it indicates that the algorithm fails to determine the correct topic of this message.</p><p>Finally, we can conclude that the kernel k-means and the kernel fuzzy k-means based on the sigmoid commute-time kernel, as well as the kernel k-means based on the sigmoid regularized Laplacian kernel, outperform the other competing methods (the spherical k-means, the spectral clustering as well as the community detection algorithms) on the tested datasets. Moreover, both methods provide quite stable performances. One advantage of the commute-time over the regularized Laplacian kernel lies in the fact that it does not need any parameter adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and further works</head><p>We introduced a family of methods allowing to cluster the nodes of a weighted graph by exploiting the links between them. It is based on recently introduced kernels on a graph, combined with kernel clustering techniques. The provided results are promising; indeed, the proposed methodology outperforms the other tested clustering and community detection algorithms on several graph nodes clustering problems.</p><p>The main drawback is that the kernel-based method does not scale well on large graphs. This issue will be investigated in future work. In this respect, it would be interesting to tackle the clustering of large graphs through the concept of fractionation and refractionation <ref type="bibr" target="#b116">[117]</ref> or some other sampling methods <ref type="bibr" target="#b90">[91]</ref>. Another potential solution to this scaling problem is to use an incomplete Cholesky factorization (see for instance <ref type="bibr" target="#b41">[42]</ref>) which can be computed efficiently an remains sparse if the original matrix is sparse.</p><p>Further work will be devoted to the development and to the study of robust kernel clustering algorithms (inspired for instance by <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b129">130,</ref><ref type="bibr" target="#b138">139]</ref>) as well as the development of new kernel algorithms dealing, for instance, with variable geometrical shapes (for instance <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b60">61]</ref>). We will also investigate the possibility of automatically adapting the kernel parameters <ref type="bibr" target="#b136">[137]</ref>. Moreover, it would be interesting to study the behavior of the commute-time distance when the size of the network grows. It is interesting to examine if it scale logarithmically with the system size, as the shortest-path distance.</p><p>We also plan to develop kernel versions of other clustering algorithms and to tackle other challenging problems, such as the clustering of sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 shows anFig. 1 .</head><label>1</label><figDesc>Fig. 1. Membership values for each document and for each cluster of the G-5cl-A subset, as provided by the K S CT fuzzy k-means.</figDesc><graphic coords="19,103.52,413.92,338.40,259.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Document subsets used in our experiments. Nine subsets are selected from the Newsgroups dataset, with 2, 3 or 5 topics. For each subset, 200 documents are randomly selected from each topic.</figDesc><table><row><cell>Topic</cell><cell>Size</cell><cell>Topic</cell><cell>Size</cell><cell>Topic</cell><cell>Size</cell></row><row><cell>G-2cl-A</cell><cell></cell><cell>G-2cl-B</cell><cell></cell><cell>G-2cl-C</cell><cell></cell></row><row><cell>Politics/general</cell><cell>200</cell><cell>Computer/graphics</cell><cell>200</cell><cell>Space/general</cell><cell>200</cell></row><row><cell>Sport/baseball</cell><cell>200</cell><cell>Motor/motorcycles</cell><cell>200</cell><cell>Politics/mideast</cell><cell>200</cell></row><row><cell>G-3cl-A</cell><cell></cell><cell>G-3cl-B</cell><cell></cell><cell>G-3cl-C</cell><cell></cell></row><row><cell>Sport/baseball</cell><cell>200</cell><cell>Computer/windows</cell><cell>200</cell><cell>Sport/hockey</cell><cell>200</cell></row><row><cell>Space/general</cell><cell>200</cell><cell>Motor/autos</cell><cell>200</cell><cell>Religion/atheism</cell><cell>200</cell></row><row><cell>Politics/mideast</cell><cell>200</cell><cell>Religion/general</cell><cell>200</cell><cell>Medicine/general</cell><cell>200</cell></row><row><cell>G-5cl-A</cell><cell></cell><cell>G-5cl-B</cell><cell></cell><cell>G-5cl-C</cell><cell></cell></row><row><cell>Computer/windowsx</cell><cell>200</cell><cell>Computer/graphics</cell><cell>200</cell><cell>Computer/machardware</cell><cell>200</cell></row><row><cell>Cryptography/general</cell><cell>200</cell><cell>Computer/pchardware</cell><cell>200</cell><cell>Sport/hockey</cell><cell>200</cell></row><row><cell>Politics/mideast</cell><cell>200</cell><cell>Motor/autos</cell><cell>200</cell><cell>Medicine/general</cell><cell>200</cell></row><row><cell>Politics/guns</cell><cell>200</cell><cell>Religion/atheism</cell><cell>200</cell><cell>Religion/general</cell><cell>200</cell></row><row><cell>Religion/christian</cell><cell>200</cell><cell>Politics/mideast</cell><cell>200</cell><cell>Forsale/general</cell><cell>200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Connected subset extracted from the IMDb dataset. The dataset is composed of movies released in the United States between 1996 and 2001. Movies are classified to be a blockbuster or not, according to the box-office receipts.</figDesc><table><row><cell>Category</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Connected subset extracted from the CORA dataset. The dataset is composed of research papers published in seven different topics. Every paper of the subset cites at least one of the others papers.</figDesc><table><row><cell>Topic</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Newsgroup dataset (two-clusters subsets): Comparison of the clustering performances (classification rate in % (upper table) and adjusted Rand index<ref type="bibr" target="#b38">[39]</ref> (lower table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Newsgroup dataset (three-clusters subsets): Comparison of the clustering performances (classification rate in % (upper table) and adjusted Rand index (lower table</figDesc><table><row><cell>Classification rate</cell><cell>G-2cl-A</cell><cell></cell><cell></cell><cell></cell><cell>G-2cl-B</cell><cell></cell><cell></cell><cell></cell><cell>G-2cl-C</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Spher.</cell><cell></cell><cell>Ng spec.</cell><cell></cell><cell>Spher.</cell><cell></cell><cell>Ng spec.</cell><cell></cell><cell>Spher.</cell><cell></cell><cell>Ng spec.</cell><cell></cell></row><row><cell></cell><cell>91.8 ± 0.5%</cell><cell></cell><cell>94.5 ± 0.0%</cell><cell></cell><cell>81.5 ± 0.8%</cell><cell></cell><cell>93.0 ± 0.0%</cell><cell></cell><cell>94.8 ± 0.2%</cell><cell></cell><cell>95.7 ± 0.0%</cell><cell></cell></row><row><cell></cell><cell>Girvan</cell><cell>Donetti</cell><cell>Duch</cell><cell>Reichardt</cell><cell>Girvan</cell><cell>Donetti</cell><cell>Duch</cell><cell>Reichardt</cell><cell>Girvan</cell><cell>Donetti</cell><cell>Duch</cell><cell>Reichardt</cell></row><row><cell></cell><cell>50.0%</cell><cell>94.0%</cell><cell>95.2 ± 0.6%</cell><cell>96.6 ± 0.3%</cell><cell>50.5%</cell><cell>88.2%</cell><cell>89.7 ± 1.0%</cell><cell>95.6 ± 0.1%</cell><cell>50.1%</cell><cell>95.2%</cell><cell>95.4 ± 0.4%</cell><cell>96.5 ± 0.0%</cell></row><row><cell></cell><cell>kmeans</cell><cell>fuzzy</cell><cell>gaussian</cell><cell>hierar.</cell><cell>kmeans</cell><cell>fuzzy</cell><cell>gaussian</cell><cell>hierar.</cell><cell>kmeans</cell><cell>fuzzy</cell><cell>gaussian</cell><cell>hierar.</cell></row><row><cell>K CT</cell><cell>50.2 ± 0.0%</cell><cell>78.6 ± 2.5%</cell><cell>50.2 ± 0.0%</cell><cell>50.3%</cell><cell>50.3 ± 0.1%</cell><cell>62.7 ± 3.8%</cell><cell>50.3 ± 0.1%</cell><cell>50.5%</cell><cell>51.8 ± 2.9%</cell><cell>92.2 ± 3.6%</cell><cell>50.3 ± 0.0%</cell><cell>50.4%</cell></row><row><cell>K S CT</cell><cell>97.5 ± 0.0%</cell><cell>97.8 ± 0.0%</cell><cell>60.2 ± 3.0%</cell><cell>87.5%</cell><cell>90.6 ± 0.0%</cell><cell>91.5 ± 0.0%</cell><cell>62.7 ± 5.7%</cell><cell>90.2%</cell><cell>95.5 ± 0.0%</cell><cell>96.0 ± 0.0%</cell><cell>74.4 ± 4.9%</cell><cell>94.2%</cell></row><row><cell>K LED</cell><cell>50.1 ± 0.1%</cell><cell>97.3 ± 0.0%</cell><cell>50.1 ± 0.0%</cell><cell>50.3%</cell><cell>50.5 ± 0.1%</cell><cell>91.2 ± 1.8%</cell><cell>50.4 ± 0.0%</cell><cell>50.5%</cell><cell>50.4 ± 0.1%</cell><cell>95.2 ± 0.0%</cell><cell>50.2 ± 0.0%</cell><cell>50.4%</cell></row><row><cell>K S LED</cell><cell>60.5 ± 2.5%</cell><cell>91.7 ± 3.9%</cell><cell>56.4 ± 0.8%</cell><cell>57.0%</cell><cell>92.7 ± 0.4%</cell><cell>86.6 ± 2.7%</cell><cell>54.8 ± 0.3%</cell><cell>74.1%</cell><cell>95.3 ± 0.0%</cell><cell>96.2 ± 0.1%</cell><cell>57.9 ± 1.5%</cell><cell>92.5%</cell></row><row><cell>K VND</cell><cell>62.7 ± 2.2%</cell><cell>60.2 ± 2.4%</cell><cell>50.3 ± 0.0%</cell><cell>57.0%</cell><cell>61.0 ± 3.0%</cell><cell>58.6 ± 2.4%</cell><cell>50.3 ± 0.0%</cell><cell>52.0%</cell><cell>74.6 ± 3.5%</cell><cell>65.0 ± 2.6%</cell><cell>50.3 ± 0.1%</cell><cell>61.2%</cell></row><row><cell>K S VND</cell><cell>71.6 ± 5.0%</cell><cell>82.5 ± 5.7%</cell><cell>50.0 ± 0.0%</cell><cell>57.0%</cell><cell>82.0 ± 1.7%</cell><cell>87.2 ± 2.8%</cell><cell>50.3 ± 0.0%</cell><cell>52.0%</cell><cell>76.0 ± 5.2%</cell><cell>80.4 ± 5.2%</cell><cell>50.1 ± 0.0%</cell><cell>61.2%</cell></row><row><cell>K RL</cell><cell>50.3 ± 0.1%</cell><cell>81.1 ± 4.0%</cell><cell>50.0 ± 0.0%</cell><cell>50.3%</cell><cell>50.2 ± 0.1%</cell><cell>82.9 ± 3.4%</cell><cell>50.0 ± 0.0%</cell><cell>50.3%</cell><cell>50.4 ± 0.1%</cell><cell>95.6 ± 0.6%</cell><cell>50.1 ± 0.0%</cell><cell>50.4%</cell></row><row><cell>K S RL</cell><cell>97.5 ± 0.0%</cell><cell>97.5 ± 0.0%</cell><cell>59.3 ± 4.0%</cell><cell>89.3%</cell><cell>91.5 ± 0.1%</cell><cell>91.2 ± 0.0%</cell><cell>55.7 ± 3.5%</cell><cell>82.2%</cell><cell>95.7 ± 0.0%</cell><cell>95.7 ± 0.0%</cell><cell>68.6 ± 6.2%</cell><cell>92.7%</cell></row><row><cell>Adjusted Rand</cell><cell>Spher.</cell><cell></cell><cell>Ng spec.</cell><cell></cell><cell>Spher.</cell><cell></cell><cell>Ng spec.</cell><cell></cell><cell>Spher.</cell><cell></cell><cell>Ng spec.</cell><cell></cell></row><row><cell></cell><cell>0.70 ± 0.02%</cell><cell></cell><cell>0.79 ± 0.00%</cell><cell></cell><cell>0.40 ± 0.02%</cell><cell></cell><cell>0.74 ± 0.00%</cell><cell></cell><cell>0.80 ± 0.01%</cell><cell></cell><cell>0.84 ± 0.00%</cell><cell></cell></row><row><cell></cell><cell>Girvan</cell><cell>Donetti</cell><cell>Duch</cell><cell>Reichardt</cell><cell>Girvan</cell><cell>Donetti</cell><cell>Duch</cell><cell>Reichardt</cell><cell>Girvan</cell><cell>Donetti</cell><cell>Duch</cell><cell>Reichardt</cell></row><row><cell></cell><cell>0.00</cell><cell>0.77</cell><cell>0.82 ± 0.02</cell><cell>0.87 ± 0.01</cell><cell>0.00</cell><cell>0.58</cell><cell>0.63 ± 0.03</cell><cell>0.83 ± 0.00</cell><cell>0.00</cell><cell>0.82</cell><cell>0.83 ± 0.01</cell><cell>0.86 ± 0.00</cell></row><row><cell></cell><cell>kmeans</cell><cell>fuzzy</cell><cell>gaussian</cell><cell>hierar.</cell><cell>kmeans</cell><cell>fuzzy</cell><cell>gaussian</cell><cell>hierar.</cell><cell>kmeans</cell><cell>fuzzy</cell><cell>gaussian</cell><cell>hierar.</cell></row><row><cell>K CT</cell><cell>0.00 ± 0.00</cell><cell>0.34 ± 0.05</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.00 ± 0.00</cell><cell>0.11 ± 0.06</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.03 ± 0.05</cell><cell>0.75 ± 0.09</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell></row><row><cell>K S CT</cell><cell>0.90 ± 0.00</cell><cell>0.91 ± 0.00</cell><cell>0.07 ± 0.06</cell><cell>0.54</cell><cell>0.66 ± 0.00</cell><cell>0.69 ± 0.00</cell><cell>0.16 ± 0.10</cell><cell>0.65</cell><cell>0.83 ± 0.00</cell><cell>0.85 ± 0.00</cell><cell>0.31 ± 0.12</cell><cell>0.78</cell></row><row><cell>K LED</cell><cell>0.00 ± 0.00</cell><cell>0.89 ± 0.00</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.00 ± 0.00</cell><cell>0.69 ± 0.04</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.00 ± 0.00</cell><cell>0.82 ± 0.00</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell></row><row><cell>K S LED</cell><cell>0.91 ± 0.00</cell><cell>0.56 ± 0.00</cell><cell>0.82 ± 0.00</cell><cell>0.63</cell><cell>0.98 ± 0.00</cell><cell>0.58 ± 0.00</cell><cell>0.98 ± 0.00</cell><cell>0.90</cell><cell>0.71 ± 0.00</cell><cell>0.90 ± 0.00</cell><cell>0.83 ± 0.00</cell><cell>0.93</cell></row><row><cell>K VND</cell><cell>0.08 ± 0.02</cell><cell>0.06 ± 0.02</cell><cell>0.00 ± 0.00</cell><cell>0.02</cell><cell>0.07 ± 0.04</cell><cell>0.05 ± 0.02</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.28 ± 0.06</cell><cell>0.11 ± 0.03</cell><cell>0.00 ± 0.00</cell><cell>0.05</cell></row><row><cell>K S VND</cell><cell>0.26 ± 0.10</cell><cell>0.52 ± 0.12</cell><cell>0.00 ± 0.00</cell><cell>0.02</cell><cell>0.42 ± 0.04</cell><cell>0.58 ± 0.06</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.35 ± 0.11</cell><cell>0.45 ± 0.11</cell><cell>0.00 ± 0.00</cell><cell>0.05</cell></row><row><cell>K RL</cell><cell>0.00 ± 0.00</cell><cell>0.43 ± 0.09</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.00 ± 0.00</cell><cell>0.47 ± 0.08</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.00 ± 0.00</cell><cell>0.83 ± 0.02</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell></row><row><cell>K S RL</cell><cell>0.90 ± 0.00</cell><cell>0.90 ± 0.00</cell><cell>0.08 ± 0.08</cell><cell>0.62</cell><cell>0.69 ± 0.00</cell><cell>0.68 ± 0.00</cell><cell>0.05 ± 0.06</cell><cell>0.41</cell><cell>0.84 ± 0.00</cell><cell>0.84 ± 0.00</cell><cell>0.25 ± 0.13</cell><cell>0.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Newsgroup dataset (five-clusters subsets): Comparison of the clustering performances (classification rate in % (upper table) and adjusted Rand index (lower table</figDesc><table><row><cell>Classification rate</cell><cell>G-3cl-A</cell><cell></cell><cell></cell><cell></cell><cell>G-3cl-B</cell><cell></cell><cell></cell><cell></cell><cell>G-3cl-C</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>K CT K S CT K LED K S LED K VND K S VND K RL K S RL Adjusted Rand K CT CT K S</cell><cell>Spher. 89.2 ± 0.4% Girvan 33.7% kmeans 35.4 ± 2.0% 93.9 ± 0.1% 38.0 ± 3.7% 61.1 ± 0.9% 51.1 ± 2.5% 57.2 ± 1.4% 33.9 ± 0.1% 93.8 ± 0.1% Spher. 0.80 ± 0.01% Girvan 0.00 kmeans 0.01 ± 0.02 0.83 ± 0.00</cell><cell>Donetti 83.8% fuzzy 64.1 ± 2.6% 94.5 ± 0.0% 66.3 ± 2.7% 63.2 ± 0.0% 47.7 ± 2.0% 68.8 ± 3.4% 66.7 ± 1.7% 94.7 ± 0.0% Donetti 0.58 fuzzy 0.43 ± 0.03 0.85 ± 0.00</cell><cell>Ng spec. 92.7 ± 0.0% Duch 80.2 ± 2.2% gaussian 33.5 ± 0.0% 66.4 ± 2.0% 33.4 ± 0.0% 48.9 ± 1.5% 33.7 ± 0.0% 33.3 ± 0.0% 33.3 ± 0.0% 61.6 ± 0.9% Ng spec. 0.84 ± 0.00% Duch 0.52 ± 0.04 gaussian 0.00 ± 0.00 0.31 ± 0.02</cell><cell>Reichardt 94.6 ± 0.4% hierar. 33.5% 83.3% 33.7% 66.2% 54.7% 53.3% 33.7% 82.2% Reichardt 0.85 ± 0.01 hierar. 0.00 0.12</cell><cell>Spher. 86.7 ± 0.5% Girvan 33.8% kmeans 34.7 ± 1.7% 93.6 ± 0.0% 35.0 ± 2.0% 71.4 ± 1.5% 60.3 ± 2.2% 62.3 ± 3.7% 34.0 ± 0.3% 93.6 ± 0.0% Spher. 0.70 ± 0.01% Girvan 0.00 kmeans 0.01 ± 0.02 0.82 ± 0.00</cell><cell>Donetti 81.9% fuzzy 66.8 ± 3.1% 93.5 ± 0.0% 91.6 ± 0.0% 65.5 ± 0.1% 51.7 ± 2.6% 68.9 ± 4.4% 72.3 ± 3.6% 93.5 ± 0.0% Donetti 0.54 fuzzy 0.35 ± 0.04 0.84 ± 0.00</cell><cell>Ng spec. 92.0 ± 0.0% Duch 90.3 ± 0.7% gaussian 33.6 ± 0.0% 63.8 ± 0.8% 33.5 ± 0.0% 54.2 ± 2.1% 33.8 ± 0.0% 33.4 ± 0.0% 33.4 ± 0.0% 61.1 ± 1.3% Ng spec. 0.79 ± 0.00% Duch 0.74 ± 0.02 gaussian 0.00 ± 0.00 0.39 ± 0.02</cell><cell>Reichardt 93.6 ± 0.3% hierar. 33.9% 84.6% 34.1% 60.0% 61.9% 58.9% 34.1% 80.3% Reichardt 0.82 ± 0.01 hierar. 0.00 0.57</cell><cell>Spher. 87.4 ± 0.7% Girvan 33.8% kmeans 34.1 ± 0.3% 93.9 ± 0.1% 35.5 ± 2.0% 80.1 ± 3.1% 54.1 ± 2.5% 67.8 ± 2.6% 34.6 ± 0.9% 93.7 ± 0.1% Spher. 0.64 ± 0.02% Girvan 0.00 kmeans 0.00 ± 0.00 0.82 ± 0.00</cell><cell>Donetti 66.2% fuzzy 71.3 ± 4.0% 92.8 ± 0.0% 87.6 ± 0.0% 64.8 ± 0.4% 48.4 ± 2.1% 72.7 ± 4.2% 70.0 ± 4.2% 92.8 ± 0.0% Donetti 0.39 fuzzy 0.42 ± 0.05 0.82 ± 0.00</cell><cell>Ng spec. 81.7 ± 0.1% Duch 83.1 ± 1.7% gaussian 33.8 ± 0.0% 64.1 ± 1.9% 33.8 ± 0.0% 59.1 ± 1.6% 33.9 ± 0.0% 33.6 ± 0.0% 33.6 ± 0.0% 64.8 ± 1.0% Ng spec. 0.78 ± 0.00% Duch 0.58 ± 0.03 gaussian 0.00 ± 0.00 0.40 ± 0.04</cell><cell>Reichardt 93.8 ± 1.0% hierar. 33.8% 86.6% 33.9% 77.5% 39.0% 39.0% 33.9% 84.9% Reichardt 0.83 ± 0.02 hierar. 0.00 0.59</cell><cell>L. Yen et al. / Data &amp; Knowledge Engineering 68 (2009) 338-361</cell></row><row><cell>K LED</cell><cell>0.06 ± 0.06</cell><cell>0.41 ± 0.04</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.02 ± 0.03</cell><cell>0.77 ± 0.00</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.02 ± 0.03</cell><cell>0.67 ± 0.00</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell></cell></row><row><cell>K S LED</cell><cell>0.29 ± 0.01</cell><cell>0.43 ± 0.00</cell><cell>0.06 ± 0.01</cell><cell>0.34</cell><cell>0.43 ± 0.01</cell><cell>0.42 ± 0.00</cell><cell>0.16 ± 0.03</cell><cell>0.21</cell><cell>0.55 ± 0.03</cell><cell>0.42 ± 0.00</cell><cell>0.26 ± 0.03</cell><cell>0.42</cell><cell></cell></row><row><cell>K VND</cell><cell>0.09 ± 0.02</cell><cell>0.07 ± 0.02</cell><cell>0.00 ± 0.00</cell><cell>0.17</cell><cell>0.18 ± 0.02</cell><cell>0.11 ± 0.02</cell><cell>0.00 ± 0.00</cell><cell>0.23</cell><cell>0.12 ± 0.03</cell><cell>0.07 ± 0.02</cell><cell>0.00 ± 0.00</cell><cell>0.01</cell><cell></cell></row><row><cell>K S VND</cell><cell>0.18 ± 0.02</cell><cell>0.37 ± 0.04</cell><cell>0.00 ± 0.00</cell><cell>0.14</cell><cell>0.25 ± 0.05</cell><cell>0.37 ± 0.07</cell><cell>0.00 ± 0.00</cell><cell>0.20</cell><cell>0.33 ± 0.03</cell><cell>0.45 ± 0.06</cell><cell>0.00 ± 0.00</cell><cell>0.01</cell><cell></cell></row><row><cell>K RL</cell><cell>0.00 ± 0.00</cell><cell>0.43 ± 0.02</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.00 ± 0.00</cell><cell>0.42 ± 0.04</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell>0.00 ± 0.01</cell><cell>0.41 ± 0.05</cell><cell>0.00 ± 0.00</cell><cell>0.00</cell><cell></cell></row><row><cell>K S RL</cell><cell>0.82 ± 0.00</cell><cell>0.85 ± 0.00</cell><cell>0.35 ± 0.02</cell><cell>0.53</cell><cell>0.82 ± 0.00</cell><cell>0.82 ± 0.00</cell><cell>0.34 ± 0.04</cell><cell>0.49</cell><cell>0.82 ± 0.00</cell><cell>0.79 ± 0.00</cell><cell>0.44 ± 0.02</cell><cell>0.60</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>IMDb and CORA datasets: Comparison of the clustering performances (classification rate in % (upper table) and adjusted Rand index (lower table</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Notice that the same terminology (graph kernels) is also used in the totally different context of defining similarities between graphs instead of nodes (see for instance<ref type="bibr" target="#b61">[62]</ref>). L. Yen et al. / Data &amp; Knowledge Engineering 68 (2009)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Available from http://people.csail.mit.edu/jrennie/20Newsgroups/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>L. Yen et al. / Data &amp; Knowledge Engineering 68 (2009) 338-361</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their interesting remarks and suggestions that allowed us to improve significantly the quality of the paper.</p><p>This work was partially supported by the STRATEGO project funded by the Walloon Region, Belgium, and the OASIS+ project funded by the Belgian Science Policy.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spanning forests of a digraph and their applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chebotarev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automation and Remote Control</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="466" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Z</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectral partitioning with multiple eigenvectors</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unveiling community structures in weighted networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kernel-kohonen networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Andras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="135" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Resistance distance in graphs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Bapat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Mathematics Student</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Support vector clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="137" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A measure of similarity between graph vertices, with application to synonym extraction and web searching</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Dooren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="647" to="666" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Modern Graph Theory</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LS sets, lambda sets and other cohesive subsets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Borgatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="337" to="357" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A random walks perspective on maximizing satisfaction and profit</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 SIAM International Conference on Data Mining</title>
		<meeting>the 2005 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unifying theorem for spectral embedding and clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the Ninth International Workshop on Artificial Intelligence and Statistics<address><addrLine>Key West, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-01">January 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual Web search engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks and ISDN Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1-7</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning hidden markov models from first passage times</title>
		<author>
			<persName><forename type="first">J</forename><surname>Callut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Machine Learning Conference (ECML)</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<meeting>the European Machine Learning Conference (ECML)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A novel kernel method for clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Camastra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="801" to="804" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting communities in large networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Capocci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Servedio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caldarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Colaiori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A</title>
		<imprint>
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="page" from="669" to="676" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the optimality of the median cut spectral bisection graph partitioning method</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ciarlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Szeto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="943" to="948" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The electrical resistance of a graph captures its commute and cover times</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Ruzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tiwari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="574" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The forest metric for graph vertices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chebotarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Electronic Notes in Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="98" to="107" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Directed graph embedding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2707" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Spectral Graph Theory</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Laplacian and the Cheeger inequality for directed graph</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Combinatorics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Drawing graphs to convey proximity: an incremental arrangement method</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="229" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mining Graph Data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Holder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comparing community structure identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Danon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diaz-Guilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arenas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiments</title>
		<imprint>
			<biblScope unit="page">9008</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerweester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A unified view of kernel k-means, spectral clustering and graph cuts</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<idno>TR-04-25</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">UTCS Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Concept decompositions for large sparse text data using clustering</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spectral clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorial presented at the 16th European Conference on Machine Learning (ECML 2005)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Linearized cluster assignment via spectral ordering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning (ICML04</title>
		<meeting>the 21st International Conference on Machine Learning (ICML04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A min-max cut algorithm for graph partitioning and data clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First IEEE International Conference on Data Mining (ICDM</title>
		<meeting>the First IEEE International Conference on Data Mining (ICDM</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A learning framework using green&apos;s function and kernel regularization with application to recommender system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting network communities: a new systematic and efficient algorithm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Donetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="page">10012</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Clustering large graphs via the singular value decomposition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vinay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="9" to="33" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust kernel fuzzy clustering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Urahama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fuzzy Systems and Knowledge Discovery</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<meeting>Fuzzy Systems and Knowledge Discovery</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">3613</biblScope>
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Community detection in complex networks using extremal optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arenas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An efficient algorithm for large-scale detection of protein families</title>
		<author>
			<persName><forename type="first">A</forename><surname>Enright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Dongen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ouzounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1575" to="1584" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leese</surname></persName>
		</author>
		<title level="m">Cluster Analysis</title>
		<imprint>
			<publisher>Arnold Publishers</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A property of eigenvectors of nonnegative symmetric matrices and its applications to graph theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fiedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Czechoslovak Mathematical Journal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">100</biblScope>
			<biblScope unit="page" from="619" to="633" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey of kernel and spectral methods for clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Filippone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Camastra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Masulli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rovetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="176" to="190" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient SVM training using low-rank kernel representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="243" to="264" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Algorithms for graph partitioning: a survey</title>
		<author>
			<persName><forename type="first">P.-O</forename><surname>Fjallstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linkoping Electronic Articles in Computer and Information Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph clustering and minimum cut trees</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Flake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="385" to="408" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Method to find community structures based on information centrality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Latora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchiori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Random-walk computation of similarities between nodes of a graph, with application to collaborative recommendation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pirotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="369" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An experimental investigation of graph kernels on a collaborative recommendation task</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pirotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Data Mining (ICDM 2006)</title>
		<meeting>the Sixth International Conference on Data Mining (ICDM 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="863" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Fox</surname></persName>
		</author>
		<title level="m">Information Retrieval: Data Structures and Algorithms</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Frakes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>Lexical analysis and stoplists</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shiffeldrim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Laserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rag: Rna-as-graphs database -concepts, analysis, and features</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1285" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mercer kernel-based clustering in feature space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="780" to="784" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Community structure in social et biological networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Proceedings of the National Academy of Sciences of the USA</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7821" to="7826" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A random-walk based scoring algorithm with application to recommender systems for large-scale e-commerce</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Isoperimetric partitioning: A new algorithm for graph partitioning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1844" to="1866" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fuzzy topographic kernel clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Fuzzy-Neuro Systems Workshop</title>
		<meeting>the Fifth Fuzzy-Neuro Systems Workshop</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Functional cartography of complex metabolic networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guimera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Amaral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">433</biblScope>
			<biblScope unit="page" from="895" to="899" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fuzzy clustering with a fuzzy covariance matrix</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kessel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Decision and Control</title>
		<meeting>the IEEE Conference on Decision and Control</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="761" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A new spectral method for ratio cut partitioning and clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kahng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1074" to="1085" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A kernel view of the dimensionality reduction of manifolds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On clustering using random walks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on the Foundations of Software Technology and Theoretical Computer Science</title>
		<title level="s">Lecture Notes in Computer Sciences</title>
		<meeting>the Conference on the Foundations of Software Technology and Theoretical Computer Science</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2245</biblScope>
			<biblScope unit="page" from="18" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Hoppner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klawonn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Runkler</surname></persName>
		</author>
		<title level="m">Fuzzy Cluster Analysis</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cyclic pattern kernels for predictive graph mining</title>
		<author>
			<persName><forename type="first">T</forename><surname>Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Gaussian mixture pdf approximation and fuzzy c-means clustering with entropy regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ichihashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Fuzzy System Symposium (AFSS)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Lvq clustering and SOM using a kernel function</title>
		<author>
			<persName><forename type="first">R</forename><surname>Inokuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miyamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Fuzzy Systems</title>
		<meeting>the IEEE International Conference on Fuzzy Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1497" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Application of kernels to link analysis: first results</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Mining Graphs, Trees and Sequences, ECML/PKDD</title>
		<meeting>the Second Workshop on Mining Graphs, Trees and Sequences, ECML/PKDD</meeting>
		<imprint>
			<publisher>Pisa</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning semantic similarity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 2002)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On clusterings: good, bad and spectral</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="497" to="515" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Entropy Optimization Principles with Applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Kesavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">An efficient heuristic procedure for partitioning graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kernighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="307" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bibliographic coupling between scientific papers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Documentation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="25" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Evaluation of the performance of clustering algorithms in kernel-induced feature space</title>
		<author>
			<persName><forename type="first">D.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="607" to="611" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Randic, Resistance distance</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Chemistry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="81" to="95" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Diffusion kernels on graphs and other discrete structures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Machine Learning</title>
		<meeting>the 19th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Measuring and extracting proximity in networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="245" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Diffusion maps and coarse-graining: a unified framework for dimensionality reduction, graph partitioning, and data set parameterization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lafon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1393" to="1403" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Towards a robust fuzzy clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="215" to="233" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Node similarity in the citation graph</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Milos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="129" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">On the decomposition of networks in minimally interconnected subnetworks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Luccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuit Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="188" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectral embedding of graphs</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2213" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The kernel self organising map</title>
		<author>
			<persName><forename type="first">D</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fyfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies</title>
		<meeting>the Fourth International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="317" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Classification in networked data: a toolkit and a univariate case study</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Macskassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="935" to="983" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<title level="m">Introduction to Information Retrieval</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A random walks view of spectral segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A new diffusion-based multilevel algorithm for computing graph partitions of very high quality</title>
		<author>
			<persName><forename type="first">H</forename><surname>Meyerhenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Monien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sauerwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Parallel and Distributed Processing Symposium, (IPDPS&apos;08)</title>
		<meeting>the 22nd International Parallel and Distributed Processing Symposium, (IPDPS&apos;08)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Laplace eigenvalues of graphs -a survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mohar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="171" to="183" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Diffusion maps, spectral clustering and reaction coordinate of dynamical systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kevrekidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Indexed-based density biased sampling for clustering applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="37" to="63" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Collective classification with relational dependency networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KDD-2003 Workshop on Multi-Relational Data Mining (MRDM-2003)</title>
		<meeting>the KDD-2003 Workshop on Multi-Relational Data Mining (MRDM-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="77" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Detecting community structure in networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="321" to="330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Finding community structure in networks using the eigenvectors of matrices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">36104</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Fast algorithm for detecting community structure in networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girvan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">66133</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">On spectral clustering: analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 2001)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<idno>1999-0120</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Electricity based external similarity of categorical attributes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Seventh Pacific-Asia Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>PAKDD</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="486" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Automatic multimedia cross-modal correlation discovery</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="653" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Computing communities in large networks using random walks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Latapy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graph Algorithms and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="218" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Partitioning sparse matrices with eigenvectors of graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pothen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Liou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="430" to="452" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Clustering and embedding using commute times</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1873" to="1890" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Detecting fuzzy community structures in complex networks with a Potts model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reichardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bornholdt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">21</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">An information-theoretic framework for resolving community structure in complex networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosvall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bergstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the USA</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="7327" to="7331" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Going metric: Denoising pairwise data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">The principal components analysis of a graph, and its relationships to spectral clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Conference on Machine Learning</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<meeting>the 15th European Conference on Machine Learning<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">3201</biblScope>
			<biblScope unit="page" from="371" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A tractable approach to finding closest truncated-commute-time neighbors in large graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the 23rd Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Graph clustering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="64" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Learning with kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Partitioning networks by eigenvectors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Social Networks</title>
		<meeting>the International Conference on Social Networks</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<title level="m">Kernel Methods for Pattern Analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Normalised cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Matching and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Kernels as link analysis measures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Graph Data</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Cook</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Holder</surname></persName>
		</editor>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="283" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Co-citation in the scientific literature: a new measure of the relationship between two documents</title>
		<author>
			<persName><forename type="first">H</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="265" to="269" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Kernels and regularization on graphs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Learning Theory (COLT)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Warmuth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<meeting>the Conference on Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Hierarchical model-based clustering of large datasets through fractionation and refractionation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tantrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="315" to="326" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Community detection in complex networks using genetic algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tasgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bingol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">604419</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Random walks and the effective resistance of networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tetali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Theoretical Probability</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="101" to="109" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koutroumbas</surname></persName>
		</author>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Random walk with restart: fast solutions and applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="346" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Graph Clustering by Flow Simulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Dongen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Utrecht</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A comparison between dissimilarity SOM and kernel SOM for clustering the vertices of a graph</title>
		<author>
			<persName><forename type="first">N</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Self-Organizing Maps (WSOM 2007)</title>
		<meeting>the Sixth International Workshop on Self-Organizing Maps (WSOM 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Hierarchical grouping to optimize an objective function</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Segmentation using eigenvectors: a unifying view</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE International Conference on Computer Vision</title>
		<meeting>the Seventh IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="975" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Algorithms for estimating relative importance in networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">A spectral clustering approach to finding communities in graph</title>
		<author>
			<persName><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 SIAM International Conference on Data Mining</title>
		<meeting>the 2005 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Finding communities in linear time: a physics approach</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="331" to="338" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Alternative c-means clustering algorithms</title>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2267" to="2278" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Fuzzy c-means clustering algorithm based on kernel method</title>
		<author>
			<persName><forename type="first">Z.-D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCIMA &apos;03: Proceedings of the Fifth International Conference on Computational Intelligence and Multimedia Applications</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">A robust deterministic annealing algorithm for data clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="84" to="100" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Graph nodes clustering based on the commute-time kernel</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Decaestecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Francq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2007)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the 11th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4426</biblScope>
			<biblScope unit="page" from="1037" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">A family of dissimilarity measures between nodes generalizing both the shortest-path and the commutetime distances</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mantrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="785" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Clustering using a random walk-based distance measure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vanvyve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wouters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Symposium on Artificial Neural Networks (ESANN 2005)</title>
		<meeting>the 13th European Symposium on Artificial Neural Networks (ESANN 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="317" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Spectral relaxation for K-means clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1057" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Learning the kernel parameters in kernel minimum distance classifier</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="133" to="135" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Fuzzy clustering using kernel method</title>
		<author>
			<persName><forename type="first">D.-Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 International Conference on Control and Automation (ICCA)</title>
		<meeting>the 2002 International Conference on Control and Automation (ICCA)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="162" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">A novel kernelized fuzzy c-means algorithm with application in medical image segmentation</title>
		<author>
			<persName><forename type="first">D.-Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Graph kernels, hierarchical clustering, and network community structure: experiments and comparative analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="67" to="74" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Contextual distance for data perception</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the Eleventh IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Learning from labeled and unlabeled data on a directed graph</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning (ICML 2005</title>
		<meeting>the 22nd International Conference on Machine Learning (ICML 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Distance, dissimilarity index, and network community structure</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<date type="published" when="2003">061901. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Graph kernels by spectral transforms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semi-Supervised Learning</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="277" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Multilevel spectral hypergraph partitioning with arbitrary vertex sizes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1389" to="1399" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Luh Yen received the M.Sc. degree in electrical engineering in 2002 from the Université catholique de Louvain (UCL), Belgium. She is now a Ph.D. student in the department of ISYS laboratory at Université catholique de Louvain. Her research interests include graph mining, unsupervised classification and multivariate analysis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">D. degree in management sciences in 2007, all from the Université catholique de Louvain (UCL), Belgium</title>
	</analytic>
	<monogr>
		<title level="m">2007, he joined the Facultés Universitaires Catholiques de Mons (FUCaM), Belgium, as a professor in computing science. His main research interests include data mining and machine learning</title>
		<imprint/>
	</monogr>
	<note>François Fouss received the B.S. degree in management sciences in 2001, the M.S. degree in information systems in 2002 and the Ph. more precisely, collaborative recommendation, graph mining, and network analysis</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">She is also member of the Laboratory of Image Analysis and Synthesis (LISA) from the Faculty of Applied Sciences (U.L.B.) and teaches data analysis and machine learning to students in biomedical engineering. Her current research interests are data and image analysis applied to both cell biology and the search of new prognostic and diagnostic markers in the cancer field. -year experience in a private company, he came back to the Université Libre de Bruxelles (ULB) where he earned his Phd in 2003. His main research topic is the Internet: its technologies, its social aspects and its support as a knowledge sharing platform. Since 1997, he has been working on automatic communities&apos; detection and is the main contributor of the open source platform GALILEI for information management</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">L B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Christine Decaestecker obtained her MS degree in Pure Mathematics in 1984 from the Université Libre de Bruxelles</title>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">L B</forename></persName>
		</editor>
		<imprint/>
	</monogr>
	<note>where she also received her PhD in Pure Science in 1991 and her Agregation de l&apos;Enseignement Superieur thesis (qualification for university professorship) in 1997. He is currently involved in research on social networks analysis, search technologies, XML document clustering, genetic algorithms and software design</note>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">While remaining a part-time researcher at IRIDIA, he then worked as a senior researcher in the R&amp;D department of various industries, mainly in the fields of speech recognition, data mining, and artificial intelligence</title>
	</analytic>
	<monogr>
		<title level="m">Universite Libre de Bruxelles (ULB), Belgium) as a research assistant and received the Ph.D. degree in engineering in 1991</title>
		<title level="s">Marco Saerens received the B.S. degree in physics engineering, and the M.S. degree in theoretical physics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Université Libre de Bruxelles (ULB) ; Universite catholique de Louvain (UCL</orgName>
		</respStmt>
	</monogr>
	<note>) as a professor in computing science. His main research interests include artificial intelligence, machine learning, data mining, pattern recognition. and speech/language processing. He is a member of the IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
