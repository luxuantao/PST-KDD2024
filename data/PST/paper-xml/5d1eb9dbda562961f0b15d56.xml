<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bipartite Graph Neural Networks for Efficient Node Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-28">28 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chaoyang</forename><surname>He</surname></persName>
							<email>chaoyang.he@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tian</forename><surname>Xie</surname></persName>
							<email>xiet@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<email>yu.rong@hotmail.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanfang</forename><surname>Li</surname></persName>
							<email>yangfang.li@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>jzhuang@uta.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<email>xiangren@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
							<email>shahabi@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Users Items One-Hop Connection Two-Hop Connection</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bipartite Graph Neural Networks for Efficient Node Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-28">28 Sep 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1906.11994v2[cs.SI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing Graph Neural Networks (GNNs) mainly focus on general structures, while the specific architecture on bipartite graphs-a crucial practical data form that consists of two distinct domains of nodes-is seldom studied. In this paper, we propose Bipartite Graph Neural Network (BGNN), a novel model that is domain-consistent, unsupervised, and efficient. At its core, BGNN utilizes the proposed Inter-domain Message Passing (IDMP) for message aggregation and Intradomain Alignment (IDA) towards information fusion over domains, both of which are trained without requiring any supervision. Moreover, we formulate a multi-layer BGNN in a cascaded manner to enable multi-hop relation modeling while enjoying promising efficiency in training. Extensive experiments on several datasets of varying scales verify the effectiveness of BGNN compared to other counterparts. Particularly for the experiment on a large-scale bipartite graph dataset, the scalability of our BGNN is validated in terms of fast training speed and low memory cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs that characterize relations among data arise in various domains including drug discovery <ref type="bibr" target="#b23">(You et al., 2018;</ref><ref type="bibr" target="#b8">Jin, Barzilay, and Jaakkola, 2018)</ref>, social networks analysis <ref type="bibr" target="#b21">(Wang, Cui, and Zhu, 2016;</ref><ref type="bibr" target="#b16">Qiu et al., 2018)</ref>, and visual understanding <ref type="bibr" target="#b22">(Yang et al., 2018;</ref><ref type="bibr" target="#b20">Wan et al., 2018)</ref> to name a few. Amongst their varying forms, bipartite graphs belong to one type of the most central structures. A bipartite graph (depicted in Figure <ref type="figure" target="#fig_0">1</ref>) is a graph whose vertices are divided into two independent components such that every edge connects nodes from one component to the other. This specific type of structures is demanded to explore in many real applications-taking the E-commerce recommendation system as an example, users and products correspond to two distinct components, and how to take benefit of the buying correlations between them plays an important role in accurate and effective recommendation services <ref type="bibr" target="#b12">(Linden, Smith, and York, 2003)</ref>.</p><p>Early works on graphical structure data can be dated back to the research of graph embedding ( <ref type="bibr" target="#b15">Perozzi, Al-Rfou, and Skiena, 2014;</ref><ref type="bibr" target="#b4">Grover and Leskovec, 2016)</ref>, where graph topology and node relations are embedded as vector space. In light of the rapid advances of deep learning <ref type="bibr" target="#b11">(LeCun, Bengio, and Hinton, 2015)</ref>, current research attention has been paid to the application of deep neural networks on graphs, giving rise to a novel and also powerful kind of models, dubbed as Graph Neural Networks (GNNs) <ref type="bibr">(Gori, Monfardini, and Scarselli, 2005;</ref><ref type="bibr" target="#b17">Scarselli et al., 2009)</ref>. To their essential characteristics, GNNs recursively update each node's feature through aggregation (or message passing) of its neighbors, by which the patterns of graph topology and node features are both captured. While GNNs have exhibited tremendous progress, applying them to the particular vein, i.e. bipartite graphs, is never studied previously and requires delicate developments. The main challenge is that the features in two groups of bipartite graphs follow different distributions with different dimensions (for example, in Figure <ref type="figure" target="#fig_0">1</ref>, users and items have quite distinct characteristics). Therefore, it is insufficient to depict the consistency and correlation between two domains if we pack them as a global graph and leverage neighbor message passing as usual GNN does. One can extract two different sub-graphs for each domain by regarding two-hop neighbors as one-hop homogeneous connections within the same domain (note that each node and its two-hop neighbors are in the same group). Obviously, such a separation scheme is unable to conduct information</p><formula xml:id="formula_0">… U V IDA Adversarial Learning</formula><p>Inter-domain Message Passing (IDMP)</p><formula xml:id="formula_1">Intra-domain Alignment (IDA) Depth 1 Depth 2 D H k u→v H k−1 v Adversarial Loss Cascaded Training IDMP U V H k u H u→v H k v IDA H 1 u→v X v X u H 1 u IDA H 1 v→u IDMP u 1 IDMP v 1 IDA IDA IDMP v 2 IDMP u 2 H 2 u→v H k u→v H k v→u H 2 v→u H 2 v H 2 u IDA IDA IDMP v k IDMP u k H k−1 u H k−1 v Depth k H 1 v Figure 2: Illustration of BGNN.</formula><p>Given the inputs of two domains X u and X v , we obtain their unsupervised-embedded representations as H u and H v via inter-domain message passing and inter-domain distribution alignment. To enable multi-hop neighbor information aggregation, we stack multiple layers to formulate a deep BGNN whose layers are trained in a cascaded manner.</p><p>fusion across domains.</p><p>In this paper, we propose the Bipartite Graph Neural Network (BGNN) that is domain-consistent, unsupervised, and efficient. BGNN consists of two central operations: interdomain message passing and intra-domain alignment, as illustrated in Figure <ref type="figure">2</ref>. Suppose the two domains to be U and V , respectively. In each layer (depth) of BGNN, we formulate two kinds of information flows, one from U to V and the other from V to U , each of which is equipped with different weight filter. In this way, we attain inter-domain representation for each node. To enable domain fusion, one may concatenate the input feature of each node with its corresponding inter-domain representation as an enhanced output. Nevertheless, such a simple concatenation requires supervised signals for the training, which is not allowable in practice. Instead, we propose an intra-domain alignment trick to minimize the divergence between input features and inter-domain representations, by using adversarial models <ref type="bibr" target="#b3">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b14">Pan et al., 2018)</ref>, a kind of tool that has been applied successfully for distribution matching. In contrast to feature concatenation, our alignment trick is unsupervised and more practical for broader cases.</p><p>Another benefit of the intra-domain alignment loss is that it allows for cascaded training. By saying cascaded training, we mean the training of the upper layer (depth k + 1 in Figure <ref type="figure">2</ref>) begins only after the lower one (depth k in Figure <ref type="figure">2</ref>) has already been trained. This cascaded training is possible for BGNN since we can utilize the intra-domain alignment loss for unsupervised training layer by layer. Clearly, the cascaded training is more memory-efficient than the conventional end-to-end training paradigm, because it does not need to restore all intermediate activation maps of all neural layers. Besides, by cascaded training, the domain shift (i.e. the discrepancy between two domain features, which always exists during the early training phase) in lower layers will not be passed to higher ones; while in end-to-end training this kind of errors will accumulate as the depth increases.</p><p>We contrast the performance of our algorithm against several unsupervised graph learning counter-parts: Node2Vec <ref type="bibr" target="#b4">(Grover and Leskovec, 2016)</ref>, VGAE <ref type="bibr" target="#b9">(Kipf and Welling, 2016)</ref>, Graph-SAGE <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec, 2017)</ref>, and AS-GCN <ref type="bibr" target="#b7">(Huang et al., 2018)</ref>. For the evaluation, we apply a large-scale scale social network from Tencent Platform, and also construct three synthesized datasets based on the citation networks Cora, Citeseer, and PubMed <ref type="bibr" target="#b18">(Sen et al., 2008)</ref>. Upon all benchmarks, our method exhibits more expressive representation learning ability, higher classification accuracy, faster training speed, and lower memory cost than all compared models.</p><p>To sum up, our contributions are listed as follows.</p><p>• To the best of our knowledge, we are the first to apply graph neural networks (GNNs) for bipartite graph representation learning. • We propose a novel model termed as bipartite graph neural network (BGNN), which is expressive, unsupervised and resource-economic. • We experimentally evaluate the effectiveness of our method against state of the arts on several public benchmarks as well as one large-scale bipartite graph dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Graph representation learning (also termed as graph embedding or network embedding) is closely related to our work. It can be generally classified into two groups: random walksbased methods, and graph convolutional networks (GCN)based methods. DeepWalk <ref type="bibr" target="#b15">(Perozzi, Al-Rfou, and Skiena, 2014)</ref> and Node2vec <ref type="bibr" target="#b4">(Grover and Leskovec, 2016)</ref> are representative random walk-based methods to model homogeneous graphs. Some follow-up works embed 1storder, 2nd-order and even high-order proximities between vertices on homogeneous graphs, such as LINE <ref type="bibr" target="#b19">(Tang et al., 2015)</ref>, GraRep <ref type="bibr" target="#b0">(Cao, Lu, and Xu, 2015)</ref>, SDNE <ref type="bibr" target="#b21">(Wang, Cui, and Zhu, 2016)</ref>, DVNE <ref type="bibr" target="#b24">(Zhu et al., 2018)</ref>. However, these methods are developed for embedding homogeneous graph. Thus, when applying them to bipartite graph embedding, the distinct vertex feature information is ignored. Although we can view the bipartite graph as a special type of heterogeneous network and then apply heterogeneous graph embedding methods, they are still suboptimal. For example, Metapath2vec++ <ref type="bibr">(Dong, Chawla, and Swami, 2017)</ref> ignores the strength of the relations between vertices and treats the explicit and implicit relations as equally. While BiNE <ref type="bibr">(Gao et al.)</ref> is a random walk-based method customized for bipartite graph embedding, it lacks evaluation on comparison with GCNbased methods, and more significantly, its drawback on extending to large-scale graphs largely hinders its application in practice. To handle the complexity of graph data, Graph Convolutional Networks (GCN)-based methods have a rapid development recently. Among them, GCN <ref type="bibr" target="#b10">(Kipf and Welling, 2017)</ref>, VGAE <ref type="bibr" target="#b9">(Kipf and Welling, 2016)</ref>, GraphSAGE <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec, 2017)</ref> show state-of-the-art performance in many graph representation learning applications. In general, these methods perform a convolution by aggregating the neighbor nodes' information so that each node can learn a relationship between the nodes in the entire graph. However, the weakness for GCN-based methods is that same node attributes should be assumed. Our work is also relevant to those GCNbased methods that deal with the scalability issue on largescale graphs, like the node-wise sampling method Graph-SAGE <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec, 2017)</ref> and the ayerwise sampling method AS-GCN <ref type="bibr" target="#b7">(Huang et al., 2018)</ref>. Differ from these methods, our BGNN has better scale performance by proposing a cascaded training architecture.</p><p>3 Proposed model: BGNN</p><p>This section introduces Bipartite Graph Neural Networks (BGNN), a general extension of GNN to bipartite graphs. We propose two novel techniques for unsupervised domain information fusion, including inter-domain message passing and intra-domain alignment. We finally present the efficient version of BGNN to enable large-scale graph learning. The overall implementation framework is also summarized.</p><p>Bipartite Graphs Let G = (U, V, E) be a bipartite network, where U and V denote the set of the two domains of vertices (nodes). u i and v j denote the i-th and j-th vertex in U and V , respectively, where i = 1, 2, ..., M and j = 1, 2, ..., N . There are only inter-domain edges which are defined as E ⊆ U × V . e ij represents the edge between u i and v j . The adjacent matrix for set</p><formula xml:id="formula_2">U is B u ∈ R M×N and B v ∈ R N ×M for set V . B u(i,j) = 1 if e ij ∈ E, and B u(i,j) = 0 if e ij / ∈ E.</formula><p>The features of two sets of nodes can be formulated as X u and X v , respectively, where X u ∈ R M×P is a feature matrix with x u(i) ∈ R P representing the feature vector of node u i , and X v ∈ R N ×Q is similarly defined.</p><p>Overall Framework Here we introduce the overall framework of our model for better readability. In general, bipartite graph representation learning is to learn the embedding representations</p><formula xml:id="formula_3">H u ∈ R P ′ and H v ∈ R Q ′</formula><p>for nodes in group U and V , respectively. Let f emb be a general bipartite graph embedding model with parameters θ, then the representation of H u and H v is defined as follow:</p><formula xml:id="formula_4">H u , H v = f emb (X u , B u , X v , B v ; θ)<label>(1)</label></formula><p>GNN-based methods <ref type="bibr" target="#b10">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b5">Hamilton, Ying, and Leskovec, 2017)</ref> define neural propagation for representation learning on general graphs. When customized to bipartite graphs, the intra-domain connections are absent, so Eq. ( <ref type="formula" target="#formula_4">1</ref>) is divided into two terms as</p><formula xml:id="formula_5">H v→u = f u (X v , B u ) (2) H u→v = f v (X u , B v )<label>(3)</label></formula><p>In this paper, we term this inter-domain message passing (IDMP) which follows the blue color information flow in Figure <ref type="figure">2</ref>. More details are provided in § 3.1.</p><p>To encourage domain information fusion, we further propose an intra-domain alignment (IDA, the orange color information flow in Figure <ref type="figure">2</ref>) to enhance the inter-domain representations of Eq. ( <ref type="formula">2</ref>)-(3). We arrive at</p><formula xml:id="formula_6">Loss u = L adv (H v→u , X u ) (4) Loss v = L adv (H u→v , X v ) (5)</formula><p>where L adv is specified as an adversarial loss. We provide more explanations in § 3.2. After the training by minimizing Eq. ( <ref type="formula">4</ref>)-( <ref type="formula">5</ref>), we obtain the representations as H u and H v for two domains. This alignment, from the other perspective of understanding, is able to combine two distinct distributions in an unsupervised manner. To be specific, the representation H v→u is a function of X v , and it has also been driven towards the distribution of X u by minimizing the alignment loss, hence the aligned H u contains information from both X v and X u .</p><p>The embedding H u (resp. H v ) merely captures one-hop topology structure of B u (resp. B v ) as well as feature information from X u and X v . As presented in previous works <ref type="bibr" target="#b10">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b5">Hamilton, Ying, and Leskovec, 2017)</ref>, the one-hop aggregation is insufficient to characterize diverse graph structures; hence a multi-hop mechanism (or equivalently a deep network) is in demand. Other than leveraging typical end-to-end networks of multiple layers, this paper develops a cascaded architecture to drive multi-hop message passing. We will detail how the cascaded framework is formulated in § 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inter-Domain Message Passing (IDMP)</head><p>Formally, the adjacent matrix of bipartite graph is</p><formula xml:id="formula_7">A = 0 u,u B u B v 0 v,v<label>(6)</label></formula><p>For stability, we normalize</p><formula xml:id="formula_8">B u as Bu = I + D − 1 2 u B u D − 1 2 u</formula><p>, where D u is the degree matrix of B u . Similar normalization is done for B v . The IDMP process is defined as</p><formula xml:id="formula_9">H (k) v→u = σ( Bu H (k) v W (k) u ) H (k) u→v = σ( Bv H (k) u W (k) v ) (7)</formula><p>where as mentioned in Eq. ( <ref type="formula">2</ref></p><formula xml:id="formula_10">)-(3), H k v→u ∈ R M×Q ′ (resp. H (k) u→v ∈ R N ×P ′ ) are hidden features of the nodes in set U (resp. V ) aggregated from the features in V (resp. U ), k indicates the depth index (note that when k = 0, H (0) u = X u , H (0) v = X v are actually input features).</formula><p>As we can see from Eq.7, there are two important characteristics that differ IDMP from conventional GCNs: 1. IDMP only performs aggregation on each node's neighbor nodes without involving the node itself, while conventional GCN methods usually consider the self-loop computation; 2. The propagation is only one-hop-neighbor aware. The first characteristic motivates us to further design intra-domain alignment to take the self-input features into account, while the second one leads to our design on the cascaded training approach which enables multi-hop modeling and supports efficient training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intra-Domain Alignment (IDA)</head><p>We introduce IDA from the perspective of domain U . As mentioned previously in Eq. ( <ref type="formula">4</ref>), we design two types of alignment losses to align H v→u to H u . Our first alignment is to employ the work <ref type="bibr" target="#b3">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b1">Ganin et al., 2015)</ref> to the graph representation learning domain. A discriminator is trained to discriminate between vectors randomly sampled from H v→u and H u . In another way, IDMP (Inter-Domain Message Passing) is trained as generator to prevent the discriminator from making accurate predictions. As a result, this is a two-player min-max game, where the discriminator aims to maximize the ability to identify two distinct feature representations, and IDMP aims to prevent the discriminator from doing so by aligning the encoded representations H v→u (source) to H u (target). After training, they will reach a Nash equilibrium so that the output of IDMP can embed information from two distinct feature spaces (learned representation H u ).</p><p>Discriminator objective We define the parameters of the IDA discriminator as θ and the IDMP generator as φ. We denote P θ,φ (source = 1|z) as the probability that input vector z is from the source domain H v→u . As opposed to this, source = 0 means z is from the target domain H u . The discriminator loss function is as follows:</p><formula xml:id="formula_11">L D (θ|φ) = − M i=1 log P θ,φ (source = 0|h u(i) ) − N i=1 log P θ,φ (source = 1|h v→u(i) ) (8)</formula><p>Generator objective In generative setting, IDMP is trained so that the discriminator is unable to distinguish between the intra-domain features:</p><formula xml:id="formula_12">L G (φ|θ) = − N i=1 log P θ,φ (source = 0|h v→u(i) )<label>(9)</label></formula><p>During training, for every input sample, the discriminator and the generator are trained successively with gradient updates to optimize two networks, respectively. We call our model as BGNN-Adv when using this approach as IDA.</p><p>Another intuitive approach to do intra-domain alignment is to project the IDMP output and features to the same dimensions with a multi-layer perceptron (MLP). We term our model as BGNN-MLP when using MLP as IDA. This approach is relatively straight-forward, but we can compare it with BGNN-Adv to judge whether adversarial learning could align domain effectively. In our experiment section, we regard BGNN-MLP as a baseline method for comparison. Formally, we define the loss function for one set U as</p><formula xml:id="formula_13">L u = ||MLP(H (i) v→u ) − H (i−1) u || F (10)</formula><p>which is symmetric for set V . The multi-layer perceptron takes the IDMP output as its input and minimizes with original features in a Frobenius norm. In the experiment section, we show that on the downstream classification task, even this simple loss can gain improvement upon conventional GCN-based methods. BGNN-Adv is even better than BGNN-MLP in terms of model accuracy, which empirically proves that the adversarial approach can embed more information to boost graph embedding performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cascaded Architecture: Towards Efficient BGNN</head><p>In this section, we present the cascaded architecture design for our proposed BGNN model. In Figure <ref type="figure" target="#fig_1">3</ref>(a), we depict a detailed diagram to illustrate our cascaded training process to compare with the conventional end-to-end training paradigm. We regard the previous IDMP with IDA as one depth. Each depth (layer) is trained one-hop embedding in E epochs at a time. Then its final learned embedding is used as the input for the later depth training. This is in contrast to the conventional end-to-end training paradigm which propagates through multiple depths for a fixed number of hops to train the final embedding (Shown in Figure <ref type="figure" target="#fig_1">3(b)</ref>). This cascaded architecture can embed multi-hops information for bipartite graph like GCN-based methods performing in general graphs. In addition to this, system-wise advantages further strengthen our design choice. In general, cascaded training is more memory-efficient and also costs less training time. This is attributed to our four findings confirmed by experiments:</p><formula xml:id="formula_14">H (1) u H (1) v X u X v IDMP IDA E-epochs IDMP IDA E-epochs H (2) u H (2) v … sampling sampling (a) Our Cascaded Training X u X v sampling layer 1 E-epochs H (1) u H (1) v layer 2 H (2) u H (2) v …<label>(</label></formula><p>Only one depth training is alive. In Figure <ref type="figure" target="#fig_1">3</ref>(a), each depth in cascaded architecture takes the final embedding from the previous depth as input. This indicates we can destroy model instance (release unused memory) in previous depth and only keep one depth training alive in the entire training process. One may argue that multi-hop topology information can also be preserved by end-to-end multi-depth training paradigm, but it significantly increases the memory cost and training time on large-scale bipartite graphs.</p><p>No uncontrollable neighborhood expansion. On a large-scale graph, the neighborhood expansion of each node layer by layer will quickly demand a high memory cost and lead to low computation speed. To deal with this problem, node-wise sampling like Graph-SAGE <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec, 2017)</ref> directly samples neighbor nodes, while layer-wise sampling method like AS-GCN <ref type="bibr" target="#b7">(Huang et al., 2018)</ref> uses adaptive sampling to fix the number of nodes in each layer further. Compared to these methods, since the propagation of our architecture only happens one-hop, mini-batch sampling can be applied without neighborhood expansion, which speeds up the training and reduces memory cost.</p><p>Faster convergence speed. We also found that normally our model requires only three epochs on a large-scale dataset to achieve the best result. Some sampling methods, like the layer-wise sampling AS-GCN method, can speed up the training time for each epoch, but it requires much more training epochs than our proposed architecture.</p><p>More robust in hyper-parameter tuning. Our cascaded architecture is robust so that it can be easily trained without too much hyper-parameter tuning effort. By cascaded training, the domain shift (i.e., the discrepancy between two domain features, which always exists during the early training phase) in lower depths will not be passed to higher ones; while in end-to-end training this kind of error will accumulate as the depth increases.</p><p>In our experiment, we compared the training time and memory cost with other baselines. It proves our design choice for efficient training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Algorithm</head><p>We summarize our overall implementation framework for BGNN model in Algorithm 1 which is consistent to Figure <ref type="figure" target="#fig_1">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We design our experiments with the goals of (i) providing a rigorous comparison of the graph representation performance between our BGNN model and the state-or-art baselines, (ii) verifying the effectiveness of the cascaded archi-</p><formula xml:id="formula_15">Algorithm 1: BGNN algorithm Input: Graph G(U, V, E); input features {X u , X v } Output: Node representations Z u and Z v H 0 u ← X u ; H 0 v ← X v for k = 1, ...K do</formula><p>for e in epochs do Sampling batches (h  <ref type="table" target="#tab_0">1</ref> and appendix. Tencent-is a largescale real world social network represented by a bipartite graph. Nodes in set U are social network users and nodes in set V are social communities (E.g., some social network users who share the same interest in electrical products may join the same shopping community). Both users and communities are described by dense off-the-shelf feature vectors. The edge connection between two sets indicates that the user belongs to the community. Another type of dataset is the synthetic bipartite graphs dataset generated from citation network datasets: Cora, Citeseer and PubMed <ref type="bibr" target="#b18">(Sen et al., 2008)</ref>. Documents and citation links between them are treated as nodes and undirected edges, respectively. We summarize our synthetic method here: for Cora and Citeseer, we randomly divide the nodes into two sets and remove all the edge connections between nodes which belong to the same set. The node features in two sets are also chosen to be different. As for PubMed, we manu-ally select nodes with a low degree to construct a sparse bipartite graph. These four datasets cover graphs from small, medium to large-scale size, and from the long tail (Tencent, Cora, Citeseer) to balanced degree distribution (PubMed), which plays a comprehensive role in evaluating model effectiveness.</p><formula xml:id="formula_16">(k) u , h (k) v ) from (H (k) u , H (k) v ) for h (k) u , h (k) v as batches of input do h (k+1) u ← IDA (k) (h (k) u , IDMP (k) (h (k) v ) h (k) v→u ); h (k+1) v ← IDA (k) (h (k) v , IDMP (k) (h (k) u ) h (k) u→v ); end end Save(H (k+1) u , H (k+1) v ) for (k + 1)th depth training Release(IDMP (k) , IDA (k) , H k u , H k v ) end Z u ← H K u ; Z v ← H K v tecture, (iii)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>For our adversarial learning BGNN, we use hyperbolic function as the non-linear activation function in the graph convolution network. The dropout and L2 regularization are applied to each layer to prevent overfitting. During training, we use mini-batch to reduce the memory and computation cost for large-scale data set as discussed before. We found the optimal batch size for all four data set is near 500, and it only requires around 3 epochs on each data set to converge to the best result quickly. More details about parameter settings for each dataset are shown in the appendix.</p><p>Baselines We mainly compare our BGNN algorithm against four unsupervised node embedding baselines:</p><p>• Raw features: This indicates a naive classification model that learns from nodes' raw features, without using any graph structure information incorporated. • Node2Vec <ref type="bibr" target="#b4">(Grover and Leskovec, 2016)</ref>: This approach is an extension of Word2Vec <ref type="bibr" target="#b13">(Mikolov et al., 2013)</ref> on graph, which learns a feature representations by simulating a biased random walks on the graph. We run Node2Vec on the bipartite graph and then concatenate the node embeddings with their own features. • VGAE <ref type="bibr" target="#b9">(Kipf and Welling, 2016)</ref>: This method is based on variational auto-encoder, where GCN is used as an encoder and a simple inner product as a decoder to embed the nodes into low-dimensional feature space. • GraphSAGE-MEAN, GraphSAGE-GCN <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec, 2017;</ref><ref type="bibr" target="#b10">Kipf and Welling, 2017)</ref>: We implement two types of aggregator functions: GCN and MEAN aggregator. Node-wise sampling is used to address the scalability issue. • BGNN-MLP: We also implement the straight-forward MLP to do intra-domain alignment as one baseline. Due to the inconsistency of nodes feature dimensions in two bipartite sets, direct applying GCN is impractical. To make the comparison available, we reconstruct the bipartite graph into two subgraphs, where each of them only contains nodes from one set with their two-hops connection through the opposite set, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Through this conversion, GCN based methods can be implemented on two sets, each with the same feature dimensions respectively, but still contains the original connectivity information in bipartite graphs. For each baseline model, we follow the open-source implementation from the authors' original paper. In order to provide a fair comparison, we also tune the hyper-parameters for every baseline and report the best results among them.</p><p>Experiments are conducted on a GPU server with 8 Tesla V100 cards. We describe more details about our infrastructure in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>To get a faster training process, instead of training two sets simultaneously in one layer, we alternatively select one set in each layer to train the model. During each layer training, we wait until the loss functions converge to continue to the next layer step. The adversarial training loss function of BGNN is shown in Figure <ref type="figure" target="#fig_3">4</ref>. We only plot the training losses of layer one and two. Clearly, the generator and discriminator losses show that the output of IDMP is doing an intra-domain alignment in an adversarial way. We further evaluate our BGNN results on a classification downstream task. For binary classification on the Tencent dataset, we report F 1 scores. For other multi-classification tasks, we use both micro and micro-averaged F 1 scores.</p><p>Performance comparison From Table <ref type="table" target="#tab_2">2</ref> we can see BGNN achieves the best performance on bipartite graph representation learning. BGNN-Adv surpasses other methods on both large and small data sets, which suggests the effectiveness of BGNN to capture both inter-domain and intra-domain information. Although BGNN-MLP does not work better than BGNN-Adv, it still outperforms the baselines. Particularly on the Tencent large-scale bipartite graph, BGNN-Adv achieves a 29% gain over the raw feature baseline. In the PubMed dataset, due to the balanced degree distribution, even incorporate the graph structure information into the model can only improve a small fraction. However, BGNN still gets the best result among other baselines, which verifies that BGNN also has better compatibility to embed more information on the long-tail dataset and balanced dataset.   We also evaluate the performance with different number of layers in BGNN on four datasets. Figure <ref type="figure" target="#fig_4">5</ref> shows the results of F 1 score of downstream classification task along with the increase of number of layers. We observe the following phenomena:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Layer Numbers</head><p>• Particularly, one layer BGNN means that there is no cascaded architecture during learning; only one-time optimization is performed. Clearly, without cascaded architecture, the one layer BGNN has relatively lower performance on all datasets. Besides, one layer BGNN only embeds one-hop neighbor connection information in the final embeddings.</p><p>• BGNN can achieve better performance by increasing the number of layers. This benefit is due to the highly contrasting average degree in two sets of nodes, which cause a large number of two-hop connections for particular sparse connectivity set. However, one layer BGNN will completely ignore this relation. • With more than two layers, the improvement is not as significant as the first two layers. The reason is in the bipartite graph setting, nodes in two sets normally represent entirely different entities (e.g., user and community). So multi-hop connections (from the opposite set) of one node has less correlation between each other. As we see, the accuracy will sometimes slightly drop when the network becomes deeper. Perhaps the use of trained parameters from previous layers will solve this problem to a certain extent.</p><p>We remain this as our future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Towards Large-scale Bipartite Graphs</head><p>In order to compare scalability of BGNN against the baselines, we measure the training time and memory cost of algorithms running on Tencent large-scale dataset. We also add adaptive layer-wise sampling GCN (AS-GCN) into comparison, a method which efficiently solves the large-scale graph problem for GCN. Clearly, in Figure <ref type="figure" target="#fig_5">6</ref>, BGNN greatly outperforms others in terms of both space and time requirements. This final result is due to our experimental observations beforehand: (1) BGNN does not need to load the entire graph into memory; only one mini-batch is needed. However, all other methods first require to fill the graph into memory, which is showing as the unstable increase of memory cost at the training start; (2) BGNN does not require sampling procedure in each graph convolutional layer, which is an extra time-consuming procedure during training;</p><p>(3) The unique unsupervised learning loss in BGNN based on the adversarial learning between inter-domain message and intra-domain features does not need further computation process. For example, in GraphSAGE, the unsupervised loss is based on random walks where it will increase significantly with the graph size. More nodes and longer walk length are needed to maintain high performance, which requires even longer training time and larger memory cost. More discussion is covered in § 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we develop the BGNN model for bipartite graph node representation learning. BGNN is expressive, unsupervised and resource-economic. We proposed Interdomain Message Passing (IDMP) as the encoder and Intradomain Alignment (IDA) by adversarial learning to address the node feature inconsistency issue in bipartite graphs. We further design the cascaded architecture to capture the multihop relationship in local bipartite structure as well as to improve the scalability. The extensive experiments confirm the effectiveness and efficiency of our models.</p><p>This supplementary material provides source code for reproductivity, more details of the dataset, hyper-parameter settings, more experiment results, the infrastructure, and the future extension to large-scale bipartite graph system.</p><p>A Source Code for Reproductivity Source code. For experiment results reproductivity, we store this paper's source code at https://tinyurl.com/BGNN. Since we may refactor our code for further research, we maintain the original version of our code in this URL. We also provide the data that we use in this paper for running experiments.</p><p>Besides the BGNN model, we also provide baseline codes we use in our experiments. Each model's code is organized in an independent directory. In order to help reproduce our results efficiently, in the README.md file at the root directory, we organize a table of scripts for training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experimental Evaluations</head><p>In this section, we provide more information related to our paper, including detailed analysis of datasets and models implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Datasets</head><p>Data Preprocessing (Citation Networks). For citation network dataset, we process the Cora, CiteSeer and PubMed datasets similarly. We treat the original graph as an undirected graph. First, we divide the paper documents of each class into two equal size subsets. Then, we combine the first half of all classes into U group and the second half into V group. We remove some of the features of papers in the V group to introduce heterogeneity between U and V . Lastly, we only keep edges that connected a paper in U group and a paper in V group and remove all other edges to make the graph bipartite. All the nodes that are isolated are removed. Data Preprocessing (Social Networks). As for Tencent dataset, it is already a bipartite graph, with one set represents users, and another set represents the social communities that users joined in. As for data preprocessing, the format keeps the same as the citation networks datasets to simplify the data loading process.</p><p>Data Distribution Visualization. Distribution of our datasets is shown in Figure <ref type="figure">7</ref>. The four datasets own distinct degree distribution. All Tencent, Citeseer and Cora datasets have a long tail exponential distribution. But Tencent dataset is more imbalance than others, which contains more multihops connection information. On the other hand, PubMed does not have long-tail distribution, but every node has almost the same number of edge connections.</p><p>Insight from the dataset. Due to the large-scale Tencent dataset, directly applying graph convolutional networks is impractical and impossible. The neighborhood expansion along with layer depth induces large computation time and memory cost. Even fill the entire graph into the model requires a large memory cost. So in order to deal with this problem, we come up with a new learning way called cascaded architecture. Instead of the end-to-end training which optimizing the loss after multiply layers, we transfer the optimization procedure in between two layers. The output from the previous layer is the input to the next layer. By doing this, only one-hop neighbor are being calculated at each time but with multi-hops information embedded. The experiments prove the effectiveness of cascaded training architecture.</p><p>Furthermore, the average degree of nodes in users set is 1.6 and 11.0 in communities set. This sparse connection of users motivates us to design the cascaded architecture with deeper layers in BGNN. Since only implementing one side aggregation will significantly lose the structure information from multi-hops connections. For example, the average onehop edge for users is 1.6, but each user has around 1.6×11 ≈ 18 two-hops connections on average. With deeper BGNN layers, multi-hops information is able to be embedded into final representations. The experiments result also prove the effectiveness of multi-layers BGNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Model Implementation Details</head><p>Logistic regression. In order to evaluate our model output embedding performance, we use logistic regression to predict the nodes' label. We use the logistic SGDClassifier from scikit-learn Python package. We split the nodes into 80 percentage for training and the rest for testing (30% for validation).</p><p>Model Implementation. We use the code of baselines published by the author of the original paper. We summarize the baseline code we use in Table <ref type="table" target="#tab_3">3</ref>. We follow the parameter settings in their original papers and fine-tuned on our bipartite datasets. The Node2Vec is a high-performance version (C++), so its running time is comparable to ours. Since all the baselines are not designed for heterogeneous bipartite graph, in order to make a fair comparison with our models, we first transform the bipartite graph into a simple connected graph. We multiply the incidence matrix with its transpose to extract all two-hops connection. Since it is a bipartite graph, the two-hops connection of one set will only contain nodes in the exact same set. Through this simple transformation, the graph becomes to a single homogeneous graph, and all the baselines can achieve on it.</p><p>Hyper-Parameters. We use a grid-search to tune our model on every dataset to find the best hyperparameters. Here, we list all the final hyperparameters of BGNN for different datasets.</p><p>As for epochs, we first search in a wide range and find that with small epochs size will achieve better performance. This also proves the reason why our model requires less training time. The BGNN-MLP model contains two dense layers with rectified activation layer and dropout layer in between. The output of the decoder is aligned in the range [−1, 1] using hyperbolic tangent, which is the same distribution as the input features. As for BGNN-Adv model, the discriminator also contains two dense layers but with leaky rectified activation layer, which can avoid sparse gradient problem.    ). In step 2, the QUERY operation takes the sampled node vectors as input and queries their neighbor node vectors from the opposite set V (E.g, the queried neighbor vectors are h v(a) , h v(b) , h v(c) , and h v(f ) ). The inter-domain message passing is in step 3 where neighbor vectors are aggregated to h v→u . Then in step 4, the intradomain alignment taking sampled node vectors in U and their neighbor node vectors as input is trained with an adversarial loss. After iterating all mini-batches with multiple epochs, the learned H u is saved for (k +1)th depth cascaded training. This pipeline is consistent with Algorithm 1.</p><formula xml:id="formula_17">x 1 x 2 x Q … [ ] x 1 x 2 x Q … [ ] x 1 x 2 x Q … [ ] x 1 x 2 x Q … [ ]</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A Bipartite Graph in the E-commerce system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of cascaded training with traditional end-to-end training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a). The processes for set U and V are symmetric. Each step in the outmost loop proceeds as follows, where k represents the current layer andH (k) u , H (k) vare hidden representations in layer k. For every epoch, sampling is conducted on these hidden representations to get mini-batch as input. After several epochs of training, the embedding representations of depth k can be learned and saved for k+1 layer training. The kth layer model instance and unused memory are released. The finally representations can be extracted in the last layer K, which then can be used for the downstream graph analytic tasks. The time complexity per epoch for BGNN is fixed at O(|E|) (|E| denotes the number of edges), where there is no neighborhood expansion along with layer (depth) in traditional end-to-end training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Adversarial training loss of cascaded architecture. The x axis denotes iteration numbers in each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Influence of training depth (number of layers) on downstream classification task. The x axis denotes the number of BGNN layers and the y axis is the F1-score .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Memory cost and training time on Tencent data with related best parameters. The x axis denotes the wallclock time in second, whereas the y axis is the memory cost. The short blue line of BGNN and orange line of AS-GCN mean the training has finished, whereas the training time of GraphSAGE is too long to be shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>B. 3 Figure 7 :</head><label>37</label><figDesc>Figure 7: Node degree distributions on four datsets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 8: BGNN-MLP training loss on Pubmed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Cascaded Training Pipeline We depict a detailed diagram to illustrate our cascaded training process from the perspective of set U , shown in Figure 9. The step order is shown as a circle within a number. In step 1, we sample a mini-batch of node feature vectors from group U (E.g., h u(a) and h u(b)). In step 2, the QUERY operation takes the sampled node vectors as input and queries their neighbor node vectors from the opposite set V (E.g, the queried neighbor vectors are h v(a) , h v(b) , h v(c) , and h v(f ) ). The inter-domain message passing is in step 3 where neighbor vectors are aggregated to h v→u . Then in step 4, the intradomain alignment taking sampled node vectors in U and their neighbor node vectors as input is trained with an adversarial loss. After iterating all mini-batches with multiple epochs, the learned H u is saved for (k +1)th depth cascaded training. This pipeline is consistent with Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>evaluating the BGNN efficiency of space and time complexity on a large-scale dataset. Dataset statistics Dataset The statistics and distributions of our datasets are summarized in Table</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="4">Tencent Cora Citeseer PubMed</cell></row><row><cell>#Edges</cell><cell></cell><cell cols="2">991,734 1,802</cell><cell>1,000</cell><cell>18,782</cell></row><row><cell>#Nodes</cell><cell cols="2">U 619,030 V 90,044</cell><cell>734 877</cell><cell>613 510</cell><cell>13,424 3,435</cell></row><row><cell>#Features</cell><cell>U V</cell><cell>8 16</cell><cell>1,433 1,000</cell><cell>3,703 3,000</cell><cell>400 500</cell></row><row><cell>#Classes</cell><cell>U V</cell><cell>2 N/A</cell><cell>7 6</cell><cell>6 6</cell><cell>3 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Prediction results for the four datasets (F 1 scores). Results for BGNN unsupervised nodes embedding are shown.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Reference for baselines code Baseline</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for BGNN on four datasets The reason is that PubMed is a medium-size dataset with balanced degree distribution, so it is a great dataset to illustrate. The loss function converges after around 500 iterations. The detail parameter settings can be found in table 4.</figDesc><table><row><cell></cell><cell>Hyperparameters</cell><cell cols="2">Tencent Citeseer</cell><cell>Cora</cell><cell>PubMed</cell></row><row><cell>BGNN-Adv</cell><cell>batch size</cell><cell>600</cell><cell>400</cell><cell>400</cell><cell>700</cell></row><row><cell></cell><cell>epochs</cell><cell>2</cell><cell>4</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell>learning rate</cell><cell>0.0004</cell><cell cols="3">0.0004 0.0004 0.0004</cell></row><row><cell></cell><cell>weight decay</cell><cell>0.0005</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0005</cell></row><row><cell></cell><cell>dropout</cell><cell>0.4</cell><cell>0.35</cell><cell>0.35</cell><cell>0.35</cell></row><row><cell></cell><cell>encoder output dimensions</cell><cell>16</cell><cell>16</cell><cell>24</cell><cell>24</cell></row><row><cell>BGNN-MLP</cell><cell>batch size</cell><cell>500</cell><cell>64</cell><cell>128</cell><cell>128</cell></row><row><cell></cell><cell>epochs</cell><cell>3</cell><cell>3</cell><cell>5</cell><cell>3</cell></row><row><cell></cell><cell>learning rate</cell><cell>0.0003</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0001</cell></row><row><cell></cell><cell>weight decay</cell><cell>0.001</cell><cell cols="2">0.0005 0.0008</cell><cell>0.005</cell></row><row><cell></cell><cell>dropout</cell><cell>0.4</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell></cell><cell>encoder output dimensions</cell><cell>24</cell><cell>48</cell><cell>48</cell><cell>48</cell></row><row><cell></cell><cell>decoder hidden dimensions</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell></row><row><cell>in Figure 8.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code link Node2Vec (high performance version) https://github.com/snap-stanford/snap VGAE https://github.com/tkipf/gae GraphSage https://github.com/williamleif/GraphSAGE GCN https://github.com/williamleif/GraphSAGE AS-GCN https://github.com/huangwb/AS-GCN</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">meta-path2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015. 2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
	<note>Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<title level="m">Domain-adversarial training of neural networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BiNE: Bipartite network embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SI-GIR Conference on Research &amp; Development in Information Retrieval -SIGIR</title>
				<imprint>
			<publisher>ACM Press</publisher>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="715" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<idno>arXiv: 1406.2661</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
				<meeting>2005 IEEE International Joint Conference on Neural Networks<address><addrLine>Gori, M; Montreal, Que., Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2014. 2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
	<note>Generative Adversarial Networks</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;16</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;16<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<idno>arXiv: 1706.02216</idno>
	</analytic>
	<monogr>
		<title level="j">Inductive Representation Learning on Large Graphs</title>
		<imprint/>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Junction Tree Variational Autoencoder for Molecular Graph Generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<idno>arXiv: 1802.04364</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<idno>arXiv: 1611.07308</idno>
		<title level="m">Variational Graph Auto-Encoders</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Amazon.com recommendations: item-to-item collaborative filtering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarially Regularized Graph Autoencoder for Graph Embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.6652</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeepInf: Social Influence Prediction with Deep Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05560</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining -KDD</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining -KDD</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collective Classification in Network Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03578</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web -WWW</title>
				<meeting>the 24th International Conference on World Wide Web -WWW</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representation Learning for Scene Graph Completion via Jointly Structural and Visual Embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="949" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structural Deep Network Embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;16</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;16<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph R-CNN for Scene Graph Generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
				<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6410" to="6421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep variational network embedding in wasserstein space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
