<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hedging in games: Faster convergence of external and swap regrets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-08">8 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
							<email>xichen@cs.columbia.edu</email>
						</author>
						<author>
							<persName><forename type="first">Binghui</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hedging in games: Faster convergence of external and swap regrets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-08">8 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.04953v1[cs.GT]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the setting where players run the Hedge algorithm or its optimistic variant <ref type="bibr" target="#b26">[27]</ref> to play an n-action game repeatedly for T rounds.</p><p>• For two-player games, we show that the regret of optimistic Hedge decays at rate O(1/T 5/6 ), improving the previous bound of O(1/T 3/4 ) by [27]. • In contrast, we show that the convergence rate of vanilla Hedge is no better than O(1/ √ T ), addressing an open question posed in [27].</p><p>For general m-player games, we show that the swap regret of each player decays at O(m 1/2 (n log n/T ) 3/4 ) when they combine optimistic Hedge with the classical external-to-internal reduction of Blum and Mansour [6]. Via standard connections, our new (swap) regret bounds imply faster convergence to coarse correlated equilibria in two-player games and to correlated equilibria in multiplayer games.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online algorithms for regret minimization play an important role in many applications in machine learning where real-time sequential decision making is crucial <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>. A number of algorithms have been developed, including Hedge / Multiplicative Weights <ref type="bibr" target="#b1">[2]</ref>, Mirror Decent <ref type="bibr" target="#b18">[19]</ref>, Follow the Regularized / Perturbed Leader <ref type="bibr" target="#b19">[20]</ref>, and their power and limits against an adversarial environment have been well understood: The average (external) regret decays at a rate of O(1/ √ T ) after T rounds, which is known to be tight for any online algorithm.</p><p>What happens if players in a repeated game run one of these algorithms? Given that they are now running against similar algorithms over a fixed game, could the regret of each player decay significantly faster than 1/ √ T ? This was answered positively in a sequence of works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. Among these results, the one that is most relevant to ours is that of Syrgkanis, Agarwal, Luo and Schapire <ref type="bibr" target="#b26">[27]</ref>. They showed that if every player in a multiplayer game runs an algorithm that satisfies the RVU (Regret bounded by Variation in Utilities) property, then the regret of each player decays at O(1/T 3/4 ). Can this bound be further improved?</p><p>Besides regret minimization, understanding no-regret dynamics in games is motivated by connections with various equilibrium concepts <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref>. For example, if every player runs an algorithm with vanishing regret, then the empirical distribution must converge to a coarse correlated equilibrium <ref type="bibr" target="#b6">[7]</ref>. Nevertheless, to converge to a more preferred correlated equilibrium <ref type="bibr" target="#b2">[3]</ref>, a stronger notion of regrets called swap regrets (see Section 2) is required <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6]</ref>. The minimization of swap regrets under the adversarial setting was studied by Blum and Mansour <ref type="bibr" target="#b5">[6]</ref>. They gave a generic reduction from regret minimization algorithms which led to a tight O( n log n/T )bound for the average swap regret. A natural question is whether a speedup similar to that of <ref type="bibr" target="#b26">[27]</ref> is possible for swap regrets in the repeated game setting.</p><p>Our contributions: Faster convergence of swap regrets. We give the first algorithm that achieves an average swap regret that is significantly lower than O(1/ √ T ) under the repeated game setting. This algorithm, denoted by BM-Optimistic-Hedge, combines the external-to-internal reduction of <ref type="bibr" target="#b5">[6]</ref> with the optimistic Hedge algorithm <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref> as its regret minimization component. (Optimistic Hedge can be viewed as an instantiation of the optimistic Follow the Regularized Leader algorithm; see Section 2.) We show that if every player in a repeated game of m players and n actions runs BM-Optimistic-Hedge, then the average swap regret is at most O(m 1/2 (n log n/T ) 3/4 ); see Theorem 5.1 in Section 5. Via the relationship between correlated equilibria and swap regrets, our result implies faster convergence to a correlated equilibrium. When specialized to two-player games, the empirical distribution of players running BM-Optimistic-Hedge converges to an ǫ-correlated equilibrium after O(n log n/ǫ 4/3 ) rounds, improving the O(n log n/ǫ 2 ) bound of <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our main technical lemma behind Theorem 5.1 shows that strategies produced by the algorithm of <ref type="bibr" target="#b5">[6]</ref> with optimistic Hedge moves very slowly in ℓ 1 -norm under the adversarial setting (which in turn allows us to apply a stability argument similar to <ref type="bibr" target="#b26">[27]</ref>). This came as a surprise because a key component of the algorithm of <ref type="bibr" target="#b5">[6]</ref> each round is to compute the stationary distribution of a Markov chain, which is highly sensitive to small changes in the Markov chain. We overcome this difficulty by exploiting the fact that Hedge only incurs small multiplicative changes to the Markov chain, which allows us to bound the change in the stationary distribution using the classical Markov chain tree theorem. We further demonstrate the power of this technical ingredient by deriving another fast no-swap regret algorithm, based on a folklore algorithm in <ref type="bibr" target="#b6">[7]</ref> and optimistic predictions (see Appendix D). Both of these two algorithms enjoy the benefits of faster convergence when playing with each other, while remain robust against adversaries (see Corollary 5.4 in Appendix C).</p><p>Our contributions: Hedge in two-player games. In addition we consider regret minimization in a two-player game with n actions using either vanilla or optimistic Hedge. We show that optimistic Hedge can achieve an average regret of O(1/T 5/6 ), improving the bound O(1/T 3/4 ) by <ref type="bibr" target="#b26">[27]</ref> for two-player games; see Theorem 3.1 in Section 3. In contrast, we show that even under this game-theoretic setting, vanilla Hedge cannot asymptotically outperform the O(1/ √ T ) adversarial bound; see Theorem 4.1 in Section 4. This addresses an open question posed by <ref type="bibr" target="#b26">[27]</ref> concerning the convergence rate of vanilla Hedge in a repeated game.</p><p>The key step in our analysis of optimistic Hedge is to show that, even under the adversarial setting, the trajectory length of strategy movements (in their squared ℓ 1 -norm) can be bounded using that of cost vectors (in ℓ ∞ -norm); see Lemma 3.2. (Intuitively, it is unlikely for the strategy of optimistic Hedge to change significantly over time while the loss vector stays stable.) This allows us to build a strong relationship between the trajectory length of each player's strategy movements, and then use the RVU property of optimistic Hedge to bound their individual regrets.</p><p>Our lower bounds for vanilla Hedge use three very simple 2 × 2 games to handle different ranges of the learning rate η. For the most intriguing case when η is at least Ω(1/ √ n) and bounded from above by some constant, we study the zero-sum Matching Pennies game and use it to show that the overall regret of at least one player is Ω( √ T ). Our analysis is inspired by the result of <ref type="bibr" target="#b4">[5]</ref> which shows that the KL divergence of strategies played by Hedge in a two-player zero-sum game is strictly increasing. For Matching Pennies, we start with a quantitative bound on how fast the KL divergence grows in Lemma 4.3. This implies the existence of a window of length √ T during which the cost of one of the player grows by Ω(1) each round; the zero-sum structure of the game allows us to conclude that at least one of the players must have regret at least Ω( √ T ) at some point in this window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Initiated by Daskalakis, Deckelbaum and Kim <ref type="bibr" target="#b8">[9]</ref>, there has been a sequence of works that study noregret learning algorithms in games <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>. Daskalakis et. al. <ref type="bibr" target="#b8">[9]</ref> designed an algorithm by adapting Nesterov's accelerated saddle point algorithm to two-player zero-sum games, and showed that if both players run this algorithm then their average regrets decay at rate O(1/T ), which is optimal. Later Rakhlin and Sridharan <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> developed a simple and intuitive family of algorithms, i.e. optimistic Mirror Descent and optimistic Follow the Regularized Leader, that incorporate predictions into the strategy. They proved that if both players adopt the algorithm, then their average regrets also decay at rate O(1/T ) in zero sum games. Syrgkanis et. al. <ref type="bibr" target="#b26">[27]</ref> further strengthened this line of works by showing that in a general m-player game, if every player runs an algorithm that satisfies the RVU property then the average regret decays at rate O(1/T 3/4 ). Syrgkanis et. al. <ref type="bibr" target="#b26">[27]</ref> also considered the convergence of social welfare and proved an even faster rate of O(1/T ) in smooth games <ref type="bibr" target="#b24">[25]</ref>. Foster et. al. <ref type="bibr" target="#b13">[14]</ref> extended <ref type="bibr" target="#b26">[27]</ref> and showed that if one only aims for an approximately optimal social welfare, then the class of algorithms allowed can be much broader. Recently, Daskalakis and Panageas <ref type="bibr" target="#b10">[11]</ref> proved the last iteration convergence of optimistic Hedge in zero-sum game, i.e., instead of averaging over the trajectory, they showed that optimistic Hedge converges to a Nash equilibrium in a zero-sum game.</p><p>There is also a growing body of works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref> on the dynamics of no-regret learning over games in the last few years. Most of these works studied the dynamics of no-regret learning from a dynamical system point of view and provided qualitative intuition on the evolution of no-regret learning. Among them, <ref type="bibr" target="#b3">[4]</ref> is most relevant, in which Bailey and Piliouras proved an Ω( √ T ) lower bound on the convergence rate of online gradient descent <ref type="bibr" target="#b29">[30]</ref> for the 2 × 2 Matching Pennies game. However, we remark that their lower bound only works for online gradient descent and they need to fix the learning rate η to 1. Our lower bound for vanilla Hedge in two-player games holds for arbitrary learning rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>Notation. Given two positive integers n ≤ m, we use [n] to denote {1, . . . , n} and [n : m] to denote {n, . . . , m}. We use D KL (p q) to denote the KL divergence with natural logarithm.</p><p>Repeated games and regrets. Consider a game G played between m players, where each player i ∈ [m] has a strategy space S i with |S i | = n and a loss function</p><formula xml:id="formula_0">L i : S 1 × • • • × S m → [0, 1] such that L i (s) is the loss of player i for each pure strategy profile s = (s 1 , . . . , s n ) ∈ S 1 × • • • × S m .</formula><p>A mixed strategy for player i is a probability distribution x i over S i , where the jth action is played with probability x i (j). Given a mixed (or pure) strategy profile x = (x 1 , . . . , x m ) (or s = (s 1 , . . . , s m )), we write x −i (or s −i ) to denote the profile after removing x i (or s i , respectively).</p><p>We consider the scenario where the m players play G repeatedly for T rounds. At the beginning of each round t, t ∈ [T ], each player i picks a mixed strategy x t i and let x t = (x t 1 , . . . , x t m ) be the mixed strategy profile. We consider the full information setting where each player observes the expected loss of all her actions. Formally, player i observes a loss vector ℓ t i with</p><formula xml:id="formula_1">ℓ t i (j) = E s−i∼x t −i [L i (j, s −i )],</formula><p>and her expected loss is given by x t i , ℓ t i . At the end of round T , the regret of player i is</p><formula xml:id="formula_2">regret i T = t∈[T ] x t i , ℓ t i − min j∈[n] t∈[T ] ℓ t i (j),<label>(1)</label></formula><p>i.e., the maximum gain one could have obtained by switching to some fixed action. A stronger notion of regret, referred as swap regret, is defined as</p><formula xml:id="formula_3">swap-regret i T = t∈[T ] x t i , ℓ t i − min φ t∈[T ] j∈[n] x t i (j) • ℓ t i (φ(j)),<label>(2)</label></formula><p>where the minimum is over all n n (swap) functions φ : [n] → [n] that swap action j with φ(j). The swap regret equals the maximum gain one could have achieved by using a fixed swap function over its past mixed strategies.</p><p>Hedge. Consider the adversarial online model where a player has n actions and picks a distribution x t over them at the beginning of each round t. During round t the player receives a loss vector ℓ t and pays a loss of x t , ℓ t . The vanilla Hedge algorithm <ref type="bibr" target="#b15">[16]</ref> with learning rate η &gt; 0 starts by setting x 1 to be the uniform distribution and then keeps applying the following updating rule to obtain x t+1 from x t and the loss vector ℓ t at the end of round t: for each action j ∈ [n],</p><formula xml:id="formula_4">x t+1 (j) = x t (j) • exp(−η • ℓ t (j)) k∈[n] x t (k) • exp(−η • ℓ t (k))</formula><p>.</p><p>On the other hand, the optimistic Hedge algorithm can be obtained from the optimistic follow the regularized leader proposed by <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>, and have the following updating rule:</p><formula xml:id="formula_5">x t+1 (j) = x t (j) • exp(−η(2ℓ t (j) − ℓ t−1 (j)) k∈[n] x t (k) • exp(−η(2ℓ t (k) − ℓ t−1 (k)) ,<label>(3)</label></formula><p>with ℓ 0 = 0 being the all-zero vector. We have the following regret bound for optimistic Hedge.</p><p>Lemma 2.1 ( <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>). Under the adversarial setting, optimistic Hedge satisfies</p><formula xml:id="formula_6">regret T ≤ 2 log n η + η t∈[T ] ℓ t − ℓ t−1 2 ∞ − 1 4η t∈[T ] x t+1 − x t 2 1 .<label>(4)</label></formula><p>3 Optimistic Hedge in Two-Player Games</p><p>In this section we analyze the performance of the optimistic Hedge algorithm when it is used by two players to play a (general, not necessarily zero-sum) n × n game repeatedly.</p><p>Theorem 3.1. Suppose both players in a two-player game run optimistic Hedge for T rounds with learning rate η = (log n/T ) 1/6 . Then the individual regret of each player is O(T 1/6 log 5/6 n).</p><p>We assume without loss of generality that T ≥ log n; otherwise, the regret of each player is trivially at most T ≤ T 1/6 log 5/6 n. The following lemma is essential to our proof of Theorem 3.1. Consider the adversarial online setting where a player runs optimistic Hedge for T rounds. The lemma bounds the trajectory length of the strategy movement using that of cost vectors.</p><p>Lemma 3.2. Suppose that a player runs optimistic Hedge with learning rate η for T rounds. Let ℓ 0 , ℓ 1 , . . . , ℓ T be the cost vectors with ℓ 0 = 0 and x 1 , . . . , x T be the strategies played. Then</p><formula xml:id="formula_7">t∈[2:T ] x t − x t−1 2 1 ≤ O(log n) + O(η + η 2 ) t∈[T −1] ℓ t − ℓ t−1 ∞ .<label>(5)</label></formula><p>We delay the proof of Lemma 3.2 to Appendix A and use it to prove Theorem 3.1.</p><p>Proof of Theorem 3.1 assuming Lemma 3.2. Let G = (A, B) be the game, where A, B ∈ [0, 1] n×n denote the cost matrices of the first and second players, respectively. We use x t and y t to denote strategies played by the two players and use ℓ t x and ℓ t y to denote their cost vectors in the tth round. So we have ℓ t x = Ay t and ℓ t y = B T x t . Therefore, we have for each t ≥ 2:</p><formula xml:id="formula_8">ℓ t y − ℓ t−1 y ∞ = B T (x t − x t−1 ) ∞ ≤ x t − x t−1 1 and<label>(6)</label></formula><formula xml:id="formula_9">ℓ t x − ℓ t−1 x ∞ = A(y t − y t−1 ) ∞ ≤ y t − y t−1 1 .</formula><p>Without loss of generality it suffices to bound the regret of the second player. Set η = (log n/T ) 1/6 with T ≥ log n so that η ≤ 1. We have</p><formula xml:id="formula_10">regret y T ≤ 2 log n η + η t∈[T ] ℓ t y − ℓ t−1 y 2 ∞ − 1 4η t∈[T ] y t+1 − y t 2 1 Lemma 2.1 ≤ 2 log n η + η + η t∈[2:T ] x t − x t−1 2 1 − 1 4η t∈[2:T +1] ℓ t x − ℓ t−1 x 2 ∞ using (6) ≤ 2 log n η + η + η   O(log n) + O(η) t∈[T −1] ℓ t x − ℓ t−1 x ∞   − 1 4η t∈[T −1] ℓ t x − ℓ t−1 x 2 ∞ + 1 4η Lemma 3.2 = O log n η + t∈[T −1] O(η 2 ) • ℓ t x − ℓ t−1 x ∞ − 1 4η • ℓ t x − ℓ t−1 x 2 ∞ ≤ O log n η + T • O(η 5 ) = O T 1/6 log 5/6 n .</formula><p>This finishes the proof of the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Lower Bounds for Hedge in Two-Player Games</head><p>We prove lower bounds for regrets of players when they both run the vanilla Hedge algorithm. We show that even in games with two actions, vanilla Hedge cannot perform asymptotically better than its guaranteed regret bound of O( √ T ) under the adversarial setting.</p><p>Theorem 4.1. Suppose two players run the vanilla Hedge algorithm to play a two-action game with initial strategy (0.4, 0.6). Then for any sufficiently large T and any learning rate η &gt; 0, there is a game such that at least one player has regret Ω( √ T ) after T ′ rounds for some</p><formula xml:id="formula_11">T ′ ∈ [T : T + √ T ].</formula><p>Remark 4.2. Theorem 4.1 shows that even if players have a good estimation about the number of rounds to play (i.e., between T and T + √ T ), vanilla Hedge with any learning rate η(T ) &gt; 0 picked using T cannot promise to achieve a regret bound that is asymptotically lower than O( √ T ) for every round</p><formula xml:id="formula_12">T ′ ∈ [T : T + √ T ].</formula><p>We would like to point out that the use of (0.4, 0.6) as the initial strategy instead of the uniform distribution is not crucial but only to simplify the construction and analysis.</p><p>Let T be a sufficiently large integer. We will use three games G i = (A, B i ), i ∈ {1, 2, 3}, to handle three cases of the learning rate η, where</p><formula xml:id="formula_13">A = 1 −1 −1 1 , B 1 = −1 1 1 −1 , B 2 = 1 1 1 1 and B 3 = 1 −1 −1 1 .</formula><p>We use G 2 to handle the case when η ≤ 64/(c 0 √ T ) (see Appendix B.1) where c 0 ∈ (0, 1] is a constant introduced below in Lemma 4.3. We use G 3 to handle the case when η ≥ 3 (see Appendix B.2). The most intriguing case is when the learning rate η is between 64/(c 0 √ T ) and 3. For this case we use the Matching Pennies game G 1 = (A, B 1 ). Let x t and y t denote strategies played in round t by the first and second players, respectively. Let x ⋆ = y ⋆ = (0.5, 0.5). The proof for this case relies on the following lemma, which shows that the KL divergence between (x ⋆ , y ⋆ ) and (x T , y T ) after T rounds is at least Ω( √ T η)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4.3. Suppose players run vanilla Hedge for T rounds with</head><formula xml:id="formula_14">η : 16/ √ T ≤ η ≤ 3. Then D KL (x ⋆ x T ) + D KL (y ⋆ y T ) ≥ c 0 √ T η, for some constant c 0 ∈ (0, 1].</formula><p>We are now ready to prove Theorem 4.1 for the main case when 64/(c 0 √ T ) ≤ η ≤ 3.</p><p>Proof of Theorem 4.1 for the main case. For convenience we let x t = x t (1) (or y t = y t (1)) denote the probability of playing the first action in x t (or y t , respectively). We first describe the high level idea behind the proof. Since we know the KL divergence is at least c 0 √ T η at time T by Lemma 4.3, at least one of x T and y T is extremely close to either 0 or 1. Assume without loss of generality that this is the case for x T . As a result, the probability of the first player playing the first action will not change much for the next √ T rounds. Consequently, during the next √ T rounds, one of the players must keep losing and the other player will keep winning. This can be used to show that one of the two players must have regret at least Ω( √ T ) at some point T ′ between T and T + √ T .</p><p>To make this more formal, let ℓ t x (or ℓ t y ) denote the cost vector of the first (or the second) player at round t and define L t</p><p>x and L t y to be the total loss up to round t of the two players:</p><formula xml:id="formula_15">L t x = τ ∈[t] x τ , ℓ τ x and L t y = τ ∈[t]</formula><p>y τ , ℓ τ y .</p><p>Since G 1 = (A, B 1 ) is zero-sum, we have x τ , ℓ τ x + y τ , ℓ τ y = 0 and thus, L t x +L t y = 0. Moreover, noting that the sum of two rows of A is zero, the first player can always guarantee an overall loss of at most 0 when playing the best fixed action in hindsight. Therefore, regret x t ≥ L t x and similarly regret y t ≥ L t y . Combining this with L t x + L t y = 0, we have</p><formula xml:id="formula_16">max regret x t , regret y t ≥ |L t x | = |L t y |.</formula><p>To finish the proof, it suffices to show that</p><formula xml:id="formula_17">L T ′ x = L T ′ y ≥ Ω( √ T ), for some T ′ ∈ [T : T + √ T ].<label>(7)</label></formula><p>Let</p><formula xml:id="formula_18">L = c 0 √ T /8 ≤ √ T . We have from Lemma 4.3 that the KL divergence is at least c 0 √ T η (using η ≥ 64/(c 0 √ T ) &gt; 16/ √ T ).</formula><p>We assume without loss of generality that D KL (x ⋆ x T ) ≥ c 0 √ T η/2. We further assume without loss of generality that the second term is larger:</p><formula xml:id="formula_19">1 2 • log 1 2(1 − x T ) ≥ c 0 √ T η 4 .</formula><p>It follows that x T is very close to 1:</p><formula xml:id="formula_20">x T ≥ 1 − exp(−c 0 √ T η/2</formula><p>), and we use this to show that x T +τ remains close to 1 for all τ ∈ [L]. To see this is the case, we note that</p><formula xml:id="formula_21">x T +τ 1 − x T +τ ≥ exp(−2ητ ) • x T 1 − x T ≥ 1 2 • exp −2ηL + c 0 √ T η 2 = 1 2 • exp c 0 √ T η 4 ≥ 3,</formula><p>where we used η ≥ 64/(c 0 √ T ) in the last inequality. This implies</p><formula xml:id="formula_22">x T +τ ≥ 3/4 for all τ ∈ [L].</formula><p>Now we turn our attention to the second player. Given that x T +τ ≥ 3/4 for all τ ∈ [L], y T +τ keeps growing for all τ ∈ [L]. As a result there is an interval I ⊆ [L] such that (i) every y T +τ , τ ∈ I, lies between 1/4 and 3/4; (ii) every y T +τ before I is smaller than 1/4; and (iii) every y T +τ after I is larger than 3/4. Using a similar argument, we show that I cannot be too long. Letting ℓ and r be the left and right endpoints of I, we have</p><formula xml:id="formula_23">3 ≥ y r 1 − y r ≥ exp η(r − ℓ) 2 • y ℓ 1 − y ℓ ≥ exp η(r − ℓ) 2 • 1 3 .</formula><p>As a result, we have (r − ℓ) ≤ 6/η ≤ (3/32) • c 0 √ T and thus, either (i) or (ii) is of length at least Ω(L). We focus on the case when (ii) is long; the other case can be handled similarly.</p><p>Summarizing what we have so far, there is an interval J = [α : β] ⊆ [L] of length Ω(L) such that for every τ ∈ J, both x T +τ and y T +τ are at least 3/4. This implies that the total loss of the first player grows by Ω(1) each round and thus,</p><formula xml:id="formula_24">L T +β x − L T +α x ≥ Ω(L). Therefore, either |L T +α x | ≥ Ω(L) or |L T +β x | ≥ Ω(L).</formula><p>This finishes the proof of ( <ref type="formula" target="#formula_17">7</ref>) using L = Ω( √ T ) and the proof of the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Faster Convergence of Swap Regrets</head><p>Under the adversarial online model, Blum and Mansour <ref type="bibr" target="#b5">[6]</ref> gave a black-box reduction showing that any algorithm that achieve good regrets can be converted into an algorithm that achieves good swap regrets. In this section we show that if every player in a repeated game runs their algorithm with optimistic Hedge as its core, then the swap regret of each player can be bounded from above by O((n log n) 3/4 (mT ) 1/4 ), where m is the number of players and n is the number of actions.</p><p>We start with an overview on the reduction framework of <ref type="bibr" target="#b5">[6]</ref>, which we will refer to as the BM algorithm. Let S = [n] be the set of available actions. Given an algorithm ALG that achieves good regrets, the BM algorithm instantiates n copies ALG 1 , . . . , ALG n of ALG over S. At the beginning of each round t = 1, . . . , T , the BM algorithm receives a distribution q t i over S from ALG i for each i ∈ [n], and plays x t , which is the unique distribution over S that satisfies x t = x t Q t , where Q t is the n × n matrix with row vectors q t 1 , . . . , q t n . After receiving the loss vector ℓ t , the BM algorithm experiences a loss of x t , ℓ t and distributes x t (i) • ℓ t to ALG i as its loss vector in round t.</p><p>We are now ready to state our main theorem of this section: Theorem 5.1. Suppose that every player in a repeated game runs the BM algorithm with optimistic Hedge as ALG and sets the learning rate of the latter to be η = (n log n/(m 2 T )) 1/4 . Then the swap regret of each player is O((n log n) 3/4 • (m 2 T ) 1/4 ).</p><p>For convenience we refer to the BM algorithm with optimistic Hedge as BM-Optimistic-Hedge in the rest of the section. We first combine the analysis of <ref type="bibr" target="#b5">[6]</ref> for the BM algorithm and Lemma 3 to obtain the following bound for the swap regret of BM-Optimistic-Hedge under the adversarial setting, in terms of the total path length of cost vectors the player's mixed strategies: Lemma 5.2. Suppose that a player runs BM-Optimistic-Hedge with η &gt; 0 for T rounds. Then The proof can be found in Appendix C.1. For the repeated game setting, we have for each t ≥ 2,</p><formula xml:id="formula_25">swap-regret T ≤ 2n log n η + 2η T t=2 x t − x t−1 2 1 + T t=1 ℓ t − ℓ t−1 2 ∞ , where ℓ 0 = 0. Q = 1 − ǫ ǫ ǫ ′ 1 − ǫ ′ x = 1 k + 1 k k + 1 vs Q = 1 − ǫ ′ ǫ ′ ǫ 1 − ǫ x = k k + 1 1 k + 1</formula><formula xml:id="formula_26">ℓ t i − ℓ t−1 i ∞ ≤ x t −i − x t−1 −i 1 ≤ j =i x t j − x t−1 j 1</formula><p>where the last inequality used the fact that both x t −i and x t−1 −i are product distributions. Combining it with Lemma 5.2, we can bound the swap regret of each player i ∈ [m] in the game by</p><formula xml:id="formula_27">swap-regret i T ≤ 2n log n η + 2η + 2ηm j∈[m] T t=2 x t j − x t−1 j 2 1 .<label>(8)</label></formula><p>We prove the following main technical lemma in the rest of the section, which states that the mixed strategy x t produced by BM-Optimistic-Hedge under the adversarial setting moves very slowly (by at most O(η) in ℓ 1 -distance each round). Theorem 5.1 follows by combining Lemma 5.2 and 5.3. Lemma 5.3. Suppose that a player runs BM-Optimistic-Hedge with rate η : 0 &lt; η ≤ 1/6 under the adversarial setting. Then we have</p><formula xml:id="formula_28">x t − x t−1 1 ≤ O(η) for all t ≥ 2.</formula><p>Proof of Theorem 5.1 Assuming Lemma 5.3. Let η = (n log n) 1/4 (m 2 T ) −1/4 . For the special case when η &gt; 1/6, the swap regret of each player is trivially at most</p><formula xml:id="formula_29">T = O((n log n) 3/4 • (m 2 T ) 1/4 ).</formula><p>Assuming η ≤ 1/6, by Lemma 5.2 we have from (8) that</p><formula xml:id="formula_30">swap-regret i T ≤ 2n log n η + 2η + 2ηm 2 T • O(η 2 ) = O (n log n) 3/4 • (m 2 T ) 1/4 .</formula><p>This finishes the proof of the theorem.</p><p>The proof of Lemma 5.3 can be found in Appendix C.2. Here we give a high-level description of its proof. Given that BM-Optimistic-Hedge runs n copies of optimistic Hedge with rate η, we know that mixed strategies proposed by each ALG i move very slowly:</p><formula xml:id="formula_31">q t i − q t−1 i 1 ≤ O(η)</formula><p>. However, it is not clear whether this translates into a similar property for x t since the latter is obtained by solving x t = x t Q t . Equivalently, x t can be viewed as the stationary distribution of the Markov chain Q t composed by strategies of each individual expert ALG i , and its dependency on Q t is highly nonlinear. While there is a vast literature on the perturbation analysis of Markov chains, many results require additional assumptions on the underlying Markov chain (e.g. bounded eigenvalue gap) and are not well suited for our setting here. Indeed, it is easy to come up with examples showing that the stationary distrbution is extremely sensitive to small additive perturbations (see Figure <ref type="figure" target="#fig_0">1</ref>). As a result one cannot hope to prove Lemma 5.3 based on the property q t i − q t−1 i 1 ≤ O(η) only. We circumvent this difficulty by noting that optimistic Hedge only incurs small multiplicative perturbations on the Markov chain (see Claim C.5), i.e., each entry of Q t differs from the corresponding entry of Q t−1 by no more than a small multiplicative factor of the latter. We present in Lemma C.2 an analysis on stationary distributions of Markov chains under multiplicative perturbations, based on the classical Markov chain tree theorem, and then use it to prove Lemma 5.3.</p><p>We further prove that one can design a wrapper for BM-Optimistic-Hedge that is robust against adversarial opponents: Corollary 5.4. There is an algorithm BM-Optimistic-Hedge * with the following guarantee. If all players run BM-Optimistic-Hedge * , then the swap regret of each individual is O(n 3/4 (m 2 T ) 1/4 ); if the player is facing adversaries, then the swap regret is still at most O((nT</p><formula xml:id="formula_32">) 1/2 + n 3/4 (m 2 T ) 1/4 ).</formula><p>The proof is similar to Corollary 16 in <ref type="bibr" target="#b26">[27]</ref>; we present it in Appendix C.3 for completeness.</p><p>In the appendix we give two more extensions to our results on swap regrets.</p><p>1. In Appendix D, we show that incorporating optimistic Hedge into a folklore algorithm from <ref type="bibr" target="#b6">[7]</ref> can also achieve faster convergence of swap regrets, with a slightly worse dependence on n. Interestingly, our analysis of this algorithm also crucially relies on the perturbation analysis of stationary distributions of Markov chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>In Appendix E, we study the convergence to the approximately optimal social welfare (following the definition in <ref type="bibr" target="#b13">[14]</ref>) with no-swap regret algorithms, and prove that O(1/T ) holds for a wide range of no-swap regret algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this paper, we studied the convergence rate of regrets of the Hedge algorithm and its optimistic variant in two-player games. We obtained a strict separation between vanilla Hedge and optimistic Hedge, i.e., 1/ √ T vs. 1/T 5/6 . We also initiated the study on algorithms with faster convergence rates of swap regrets in general multiplayer games and obtained an algorithm with average regret O(m 1/2 (n log n/T ) 3/4 ) , improving over the classic result of Blum and Mansour <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our work led to several interesting future directions:</p><p>• Our faster convergence result for optimistic Hedge currently only works for two-player games. Can we extend it to multiplayer games? Second, what is the optimal convergence rate for optimistic Hedge and other no-regret algorithms? even for two-player games?</p><p>• Regarding swap regrets, it is easy to generalize the result in Section 5 to any algorithm that (1) satisfies the RVU property and (2) makes only multiplicative changes on strategies each iteration. These include optimistic Hedge and optimistic multiplicative weights. However, our current analysis does not apply to general optimistic Mirror Descent or Follow the Regularized Leader. Can we still prove faster convergence of swap regrets via the reduction of <ref type="bibr" target="#b5">[6]</ref> without requiring (2) on the regret minimization algorithm? or does there exist some natural gap between these algorithms and optimistic Hedge / multiplicative weights?</p><p>• For our result in Appendix E on the convergence to the approximately optimal social welfare, can this fast convergence result be extended to the (exact) optimal social welfare setting (follow the definition in <ref type="bibr" target="#b26">[27]</ref>)?</p><p>• Can we achieve similar convergence rates under partial information models? such as those considered in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>A Missing proof from Section 3</p><p>Proof of Lemma 3.2 For each t ∈ [2 : T ], we apply Pinsker's inequality to have</p><formula xml:id="formula_33">1 2 • x t − x t−1 2 1 ≤ D KL (x t−1 x t ) = i∈[n] x t−1 (i) • log x t−1 (i) x t (i) = i∈[n] x t−1 (i) • log   j∈[n] exp −η 2ℓ t−1 (j) − ℓ t−2 (j) • x t−1 (j)   + i∈[n] x t−1 (i) • η 2ℓ t−1 (i) − ℓ t−2 (i) = log   j∈[n] exp −η 2ℓ t−1 (j) − ℓ t−2 (j) • x t−1 (j)   + η x t−1 , 2ℓ t−1 − ℓ t−2 Φ t + η x t−1 , 2ℓ t−1 − ℓ t−2 ,<label>(9)</label></formula><p>where we recall ℓ 0 = 0. The third step follows from the updating rule of optimistic Hedge. Letting L t = i∈[t] ℓ i , next we use induction to prove the following claim for each k = 1, . . . , T :</p><formula xml:id="formula_34">t∈[k] Φ t = log   j∈[n] x 1 (j) • exp −ηL k−1 (j) − ηℓ k−1 (j)   .<label>(10)</label></formula><p>The base case holds trivially, as Φ 1 = 0. Suppose the above holds for k. Then for k + 1 we have</p><formula xml:id="formula_35">k+1 t=1 Φ t = k t=1 Φ t + Φ k+1 = log   j∈[n] x 1 (j) • exp −ηL k−1 (j) − ηℓ k−1 (j)   + log   i∈[n] exp −η 2ℓ k (i) − ℓ k−1 (i) • x k (i)   = log     i∈[n] exp −η 2ℓ k (i) − ℓ k−1 (i) • x k (i)   •   j∈[n] x 1 (j) • exp −ηL k−1 (j) − ηℓ k−1 (j)     = log   i∈[n] exp −η 2ℓ k (i) − ℓ k−1 (i) • x 1 (i) • exp −ηL k−1 (i) − ηℓ k−1 (i)   = log   i∈[n] x 1 (i) • exp −ηL k (i) − ηℓ k (i)   ,</formula><p>where the third step follows from</p><formula xml:id="formula_36">x k (i) = x 1 (i) • exp −ηL k−1 (i) − ηℓ k−1 (i) j∈[n] x 1 (j) • exp (−ηL k−1 (j) − ηℓ k−1 (j))</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we have (recall that</head><formula xml:id="formula_37">Φ 1 = 0) 1 2 ln 2 t∈[2:T ] x t − x t−1 2 1 ≤ t∈[2:T ] Φ t + η x t−1 , 2ℓ t−1 − ℓ t−2 = log   j∈[n] 1 n • exp −ηL T −1 (j) − ηℓ T −1 (j)   + t∈[2:T ] η x t−1 , 2ℓ t−1 − ℓ t−2 ≤ − min j∈[n] ηL T −1 (j) + ηℓ T −1 (j) + t∈[2:T ] η x t−1 , 2ℓ t−1 − ℓ t−2 ≤ − η min j∈[n] L T −1 (j) + η t∈[T −1] x t , ℓ t + η t∈[T −1] x t , ℓ t − ℓ t−1 ≤ η   2 log n η + η t∈[T −1] ℓ t − ℓ t−1 2 ∞   + η t∈[T −1] x t , ℓ t − ℓ t−1 ≤ 2 log n + η 2 t∈[T −1] ℓ t − ℓ t−1 2 ∞ + η t∈[T −1] ℓ t − ℓ t−1 ∞ ≤ 2 log n + (η + η 2 ) t∈[T −1] ℓ t − ℓ t−1 ∞ .</formula><p>The first step follows from Eq. ( <ref type="formula" target="#formula_33">9</ref>) and the second step follows from Eq. ( <ref type="formula" target="#formula_34">10</ref>). The fifth step follows from Lemma 2.1. This finishes the proof of the lemma. Proof. The loss of player 2 is invariant to the strategy of player 1. Thus her strategy stays at (0.4, 0.6). Hence, for any t ∈ [T ], the loss for player 1 is always ℓ = (−0.2, 0.2) and we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Missing proof from</head><formula xml:id="formula_38">x t (1) = 0.4 • exp(0.2ηt) 0.4 • exp(0.2ηt) + 0.6 • exp(−0.2ηt) and x t (2) = 0.6 • exp(−0.2ηt) 0.4 • exp(0.2ηt) + 0.6 • exp(−0.2ηt) .</formula><p>One can verify that when t ≤ 1/2η, we have x t (1) ≤ 0.5 ≤ x t (2). Therefore, the regret is</p><formula xml:id="formula_39">regret x T = t∈[T ] x t , ℓ − t∈[T ] ℓ(1) ≥ 1/2η t=1 x t , ℓ − 1/2η t=1 ℓ(1) ≥ 0 + 1 2η • 0.2 = Ω( √ T ).</formula><p>Thus we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Case when the learning rate is large</head><p>We next work on the case when η ≥ 3. Recall that we write x t = x t (1) and y t = y t (1).</p><p>Lemma B.2. Suppose both players run vanilla Hedge on game G 3 = (A, B 3 ) with learning rate η ≥ 3 Then the regret of the first player is at least Ω(T ) after T rounds.</p><p>Proof. Intuitively, (A, B 3 ) is a cooperation game, and it is beneficial for both players if they choose to cooperate on one single action (by playing either (1, 2) or (2, 1)). However, when the learning rate is too large, they actually mismatch in every iterations. Formally, we have</p><formula xml:id="formula_40">x t+1 = x t • exp(η(1 − 2y t )) x t • exp(η(1 − 2y t )) + (1 − x t ) • exp(η(2y t − 1)) = x t • exp(η(1 − 2x t )) x t • exp(η(1 − 2x t )) + (1 − x t ) • exp(η(2x t − 1))</formula><p>.</p><p>The second step follows from x t = y t for all t because A = B 3 in the game. Motivated by this, we define a sequence a 0 , a 1 , . . . where a 0 = x 0 = 0.4 and</p><formula xml:id="formula_41">a t+1 = (1 − a t ) • exp(η(2a t − 1)) a t • exp(η(1 − 2a t )) + (1 − a t ) • exp(η(2a t − 1))</formula><p>, for each t ≥ 0.</p><p>Then a t = x t if t is even and a t = 1 − x t when t is odd. Furthermore, by Claim B.3 below, we have η exp(−2η) ≤ a t ≤ 0.4 for all t when η ≥ 3. Hence, we have</p><formula xml:id="formula_42">regret x T ≥ t∈[T ] x t , ℓ t x = t∈[T ] (2x t − 1) 2 = t∈[T ] (2a t − 1) 2 ≥ Ω(T ).</formula><p>This finishes the proof of the lemma.</p><p>Claim B.3. When η ≥ 3, we have η exp(−2η) ≤ a t ≤ 0.4 for all t ≥ 0.</p><p>Proof. We prove by induction on t. The base case holds trivially for t = 0. Suppose the inequality holds up to t. Then for t + 1, we have</p><formula xml:id="formula_43">a t+1 1 − a t+1 = 1 − a t a t • exp η(4a t − 2) f (a t ).</formula><p>By simple calculation, we know that f (a t ) takes maximium at η exp(−2η) or 0.4. Thus,</p><formula xml:id="formula_44">a t+1 1 − a t+1 ≤ max f (0.4), f (η exp(−2η)) ≤ 2 3 ,</formula><p>which implies that a t+1 ≤ 0.4. The second step above follows from</p><formula xml:id="formula_45">f (0.4) = 3 2 • exp(−0.4η) ≤ 2 3 , using η ≥ 3 and f η exp(−2η) ≤ 1 η exp(2η) • exp 4η 2 exp(−2η) − 2η = 1 η • exp 4η 2 exp(−2η) ≤ 2 3 .</formula><p>Moreover, f (a t ) takes minimum at the smaller solution a of 4ηa(1 − a) = 1. Thus,</p><formula xml:id="formula_46">a t+1 1 − a t+1 ≥ 1 − a a • exp η(4a − 2) ≥ 4 3 • η exp(−2η),</formula><p>where the second step used exp(η(4a − 2)) ≥ exp(−2η), a ≤ 1/2η and a ≤ 1/3. This shows that a t+1 ≥ η exp(−2η) using η ≥ 3, and finishes the induction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Lemma 4.3</head><p>Note that the Matching Pennies game G 1 = (A, B 1 ) is zero-sum. It is known (see <ref type="bibr" target="#b4">[5]</ref>) that the KL divergence of vanilla Hedge in zero-sum games is strictly increasing. We give a careful analysis on its increment each round when playing G 1 . (Recall that x ⋆ = y ⋆ = (0.5, 0.5).) Lemma B.4. Suppose both players run vanilla Hedge with η ≤ 3 on G 1 . Then for each t ≥ 0,</p><formula xml:id="formula_47">D KL (x ⋆ x t+1 ) + D KL (y ⋆ y t+1 ) − D KL (x ⋆ x t ) + D KL (y ⋆ y t ) ≥ e −7 η 2 x t (1 − x t )(2y t − 1) 2 + e −7 η 2 y t (1 − y t )(2x t − 1) 2 .</formula><p>Proof. Focusing on the first player, we have</p><formula xml:id="formula_48">D KL (x ⋆ x t+1 ) − D KL (x ⋆ x t ) = i∈[2] x ⋆ (i) • log x ⋆ (i) x t+1 (i) − i∈[2] x ⋆ (i) • log x ⋆ (i) x t (i) = i∈[2]</formula><p>x ⋆ (i)</p><formula xml:id="formula_49">• log x t (i) x t+1 (i) = i∈[2]</formula><p>x ⋆ (i)</p><formula xml:id="formula_50">• ηℓ t (i) + i∈[2]</formula><p>x</p><formula xml:id="formula_51">⋆ (i) • log   j∈[2]</formula><p>x t (j) • exp(−ηℓ t (j))</p><formula xml:id="formula_52">  = log   j∈[2]</formula><p>x t (j) • exp(−ηℓ t (j))</p><formula xml:id="formula_53">  = log x t • exp(−η(2y t − 1)) + (1 − x t ) • exp(−η(1 − 2y t )) ≥ x t • (−η(2y t − 1)) + (1 − x t ) • (−η(1 − 2y t )) + 1 2e 6 x t (1 − x t ) e −η(2yt−1) − e −η(1−2yt) 2 ≥ η(2y t − 1)(1 − 2x t ) + e −7 η 2 x t (1 − x t )(2y t − 1) 2 . (<label>11</label></formula><formula xml:id="formula_54">)</formula><p>The third step follows from the updating rule of vanilla Hedge. The fourth step uses x ⋆ (1) = x ⋆ (2) = 0.5 and ℓ t (1) + ℓ t (2) = (2y t − 1) + (1 − 2y t ) = 0. The sixth step uses the fact that f (x) = − log x is e −6 -strongly convex on (0, e 3 ). Similarly, we can prove</p><formula xml:id="formula_55">D KL (y ⋆ y t+1 ) − D KL (y ⋆ y t ) ≥ η(2x t − 1)(2y t − 1) + e −7 η 2 y t (1 − y t )(2x t − 1) 2 . (<label>12</label></formula><formula xml:id="formula_56">)</formula><p>The lemma follows by combining ( <ref type="formula" target="#formula_53">11</ref>) and <ref type="bibr" target="#b11">(12)</ref>.</p><p>We are now ready to prove Lemma 4.3.</p><p>Proof of Lemma 4.3. We first prove that within O(1/η 2 ) steps, the KL divergence D KL (x ⋆ x t ) + D KL (y ⋆ y t ) becomes at least 20. The proof follows directly from Lemma B.4, as for any t with D KL (x ⋆ x t ) + D KL (y ⋆ y t ) ≤ 20, we have</p><formula xml:id="formula_57">D KL (x ⋆ x t+1 ) + D KL (y ⋆ y t+1 ) − D KL (x ⋆ x t ) + D KL (y ⋆ y t ) ≥ e −7 η 2 x t (1 − x t )(2y t − 1) 2 + e −7 η 2 y t (1 − y t )(2x t − 1) 2 ≥ Ω(η 2 ).<label>(13)</label></formula><p>The second step follows from the fact that both x t and y t are bounded away from 0 and 1 given the divergence at t is at most 20; it also used max{|2x t − 1|, |2y t − 1|} ≥ 0.2 given that the divergence is strictly increasing.</p><p>Let T 0 = O(1/η 2 ) be the first time when the divergence becomes at least 20. If T /2 ≤ T 0 , it follows from ( <ref type="formula" target="#formula_57">13</ref>) that the divergence at T is Ω(T η 2 ) = Ω( √ T η) using the assumption that η ≥ 16/ √ T . So we focus on the case T 0 ≤ T /2 and thus, T = T 0 + L with L ≥ T /2. We prove</p><formula xml:id="formula_58">Claim B.5. At round t = T 0 + τ 2 , the KL divergence has D KL (x ⋆ x t ) + D KL (y ⋆ y t ) ≥ 10 −10 τ η.</formula><p>Setting τ = T /2 so that T 0 + τ 2 ≤ T , we have</p><formula xml:id="formula_59">D KL (x ⋆ x T ) + D KL (y ⋆ y T ) ≥ Ω( √ T η),</formula><p>and this finishes the proof of the lemma.</p><p>Proof of Claim B.5. We proceed to use induction on τ . The cases with τ ≤ 16/η holds trivially as the KL divergence at T 0 is already at least 20. For the induction step, suppose the claim holds up to k for some k ≥ 64/η at time t 0 = T 0 + k 2 . We show that at time T 0 + (k + 1) 2 the KL divergence is at least 10 −10 (k + 1)η. Without loss of generality, we assume that x t0 , y t0 ≥ 0.5; the other three cases can be handled similarly. In this region, x t with t = t 0 + 1, . . . will keep decreasing and y t will keep increasing, until the moment when x t drops below 0.5.</p><p>Let t 2 denote the first round t 2 &gt; t 0 such that x t ≤ 0.5. We first show that it will take no more than k/2 rounds for x t to drop below 0.5: t 2 − t 0 ≤ k/2. To this end, we use t 1 to denote the first round t 1 ≥ t 0 such that y t ≥ 3/4 and note that t 1 ≤ t 2 (since otherwise at t = t 2 − 1, we have 1/2 ≤ y t ≤ 3/4 and 1/2 ≤ x t ≤ e 6 in order for x t to go below 1/2 with η ≤ 3 in the next round; this contradicts with the fact that the KL divergence is at least 20 after T 0 ).</p><p>We break the proof of t 2 − t 0 ≤ k/2 into two phases:</p><formula xml:id="formula_60">t 1 − t 0 ≤ k/4 and t 2 − t 1 ≤ k/4.</formula><p>Phase 1. First we prove that it takes no more than k/4 steps for y t to get larger than 3/4. To this end, we notice that for all t ∈ [t 0 : t 1 − 1], we have y t ≤ 3/4 and thus, x t ≥ 3/4 since the KL divergence is at least 20. During all these rounds the loss vector ℓ t y of the second player satisfies ℓ t y (1) ≤ −3/4 + 1/4 ≤ −0.5 and ℓ t y (2) ≥ 0.5. Thus we have (using 0.5 ≤ y t0 ≤ y t1−1 ≤ 3/4)</p><formula xml:id="formula_61">3 ≥ y t1−1 1 − y t1−1 ≥ exp η(t 1 − t 0 − 1) • y t0 1 − y t0 ≥ exp η(t 1 − t 0 − 1) . Thus t 1 − t 0 ≤ (2/η) + 1 ≤ k/4 using k ≥ 64/η and η ≤ 3.</formula><p>Phase 2. Next we prove that, starting from t 1 , it takes less than k/4 steps for x t to drop below 0.5.</p><p>Note that for each t ∈ [t 1 : t 2 − 1], the loss vector ℓ t x of the first player satisfies ℓ t x (1) ≥ 0.5 and ℓ t x (2) ≤ −0.5. Moreover, we assume without loss of generality that 1 − x t1 ≥ exp(−(k + 1)η/20); otherwise the KL divergence at t 1 is already bigger than 10 −10 (k + 1)η and we are done. Therefore,</p><formula xml:id="formula_62">1 ≤ x t2−1 1 − x t2−1 ≤ exp − η(t 2 − t 1 − 1) • x t1 1 − x t1 ≤ exp η(−(t 2 − t 1 − 1) + (k + 1)/20) Thus t 2 − t 1 ≥ 1 + (k + 1)/20 ≤ k/4 using k ≥ 64/η ≥ 64/3.</formula><p>Now we are at time t 2 and we examine the next R = 3/η ≤ k/2 rounds [t 2 : t 2 + R]; these are the rounds where we will gain a lot in the KL divergence. Given that x t2 just dropped below 1/2, we have x t2 ≥ 0.5 • exp(−2η) and thus, for every t ∈ [t 2 : t 2 + R],</p><formula xml:id="formula_63">x t ≥ x t2 • exp(−2η • R) ≥ 0.5 • e −12 .</formula><p>Consequently, we have</p><formula xml:id="formula_64">D KL (x ⋆ x t2+R ) + D KL (y ⋆ y t2+R ) − D KL (x ⋆ x t2 ) + D KL (y ⋆ y t2 ) ≥ t2+R−1 t=t2 e −7 η 2 x t (1 − x t )(2y t − 1) 2 + e −7 η 2 y t (1 − y t )(2x t − 1) 2 ≥ t2+R−1 t=t2 e −7 η 2 x t (1 − x t )(2y t − 1) 2 ≥ 3 η • e −7 η 2 • 1 4 e −12 • 1 4 ≥ 10 −10 η.</formula><p>So we conclude that after at most k/4 + k/4 + k/2 = k steps, the KL divergence increase at least 10 −10 η. Thus at time . By Lemma 2.1, every ALG j achieves low regret. Thus,</p><formula xml:id="formula_65">T 0 + k 2 + k ≤ T 0 + (k + 1) 2 ,</formula><formula xml:id="formula_66">t∈[T ] q t j , x t (j)ℓ t ≤ t∈[T ] x t (j) • ℓ t (φ(j)) + 2 log n η + η t∈[T ] x t (j)ℓ t − x t−1 (j)ℓ t−1 2 ∞ ,<label>(14)</label></formula><p>where we used x t = Q t x t , set ℓ 0 = 0 and x 0 = 1/n = x 1 . Consequently, we have</p><formula xml:id="formula_67">t∈[T ] x t , ℓ t = t∈[T ] x t Q t , ℓ t = t∈[T ] j∈[n] x t (j)q t j , ℓ t = j∈[n] t∈[T ] q t j , x t (j)ℓ t ≤ j∈[n]   t∈[T ] x t (j) • ℓ t (φ(j)) + 2 log n η + η t∈[T ] x t (j)ℓ t − x t−1 (j)ℓ t−1 2 ∞   = t∈[T ] j∈[n]</formula><p>x t (j)</p><formula xml:id="formula_68">• ℓ t (φ(j)) + 2n log n η + η t∈[T ] j∈[n] x t (j)ℓ t − x t−1 (j)ℓ t−1 2 ∞</formula><p>where the first inequality follows from <ref type="bibr" target="#b13">(14)</ref>. Furthermore, we have (using ℓ t ∞ ≤ 1 and</p><formula xml:id="formula_69">x t 1 = 1) j∈[n] x t (j)ℓ t − x t−1 (j)ℓ t−1 2 ∞ ≤ j∈[n] x t (j)ℓ t − x t−1 (j)ℓ t ∞ + x t−1 (j)ℓ t − x t−1 (j)ℓ t−1 ∞ 2 ≤ 2 j∈[n] x t (j)ℓ t − x t−1 (j)ℓ t 2 ∞ + 2 j∈[n] x t−1 (j)ℓ t − x t−1 (j)ℓ t−1 2 ∞ = 2 j∈[n] x t (j) − x t−1 (j) 2 ℓ t 2 ∞ + 2 j∈[n] (x t−1 (j)) 2 ℓ t − ℓ t−1 2 ∞ = 2 x t − x t−1 2 2 • ℓ t 2 ∞ + x t−1 2 2 • ℓ t − ℓ t−1 2 ∞ ≤ 2 x t − x t−1 2 1 + ℓ t − ℓ t−1 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∞</head><p>We can combine all these inequalities (and note that x 0 = x 1 ) to finish the proof of the lemma.</p><p>C.2 Proof of Lemma 5.3</p><p>We start the proof of Lemma 5.3 with the following definition.</p><formula xml:id="formula_70">Definition C.1. Given Markov chains Q, Q ′ ∈ R n×n , we say Q ′ is (η 1 , . . . , η n )-approximate to Q if (1 − η i )q ′ i,j ≤ q i,j ≤ (1 + η i )q ′ i,j for every i, j ∈ [n]</formula><p>, where we write Q = (q i,j ) and Q ′ = (q ′ i,j ).</p><p>We are ready to state our perturbation analysis on ergodic<ref type="foot" target="#foot_0">2</ref> Markov chains.</p><p>Lemma C.2. Given two ergodic Markov chains Q and Q ′ , where</p><formula xml:id="formula_71">Q ′ is (η 1 , . . . , η n )-approximate to Q, the stationary distribution p, p ′ of Q and Q ′ , respectively, satisfy p − p ′ 1 ≤ 8 n i=1 η i .</formula><p>The proof of Lemma C.2 relies on the classical Markov chain tree theorem (see <ref type="bibr" target="#b0">[1]</ref>). To state it we need the following definition.</p><p>Definition C.3. Suppose Q is an ergodic Markov chain and G = (V, E) with V = [n] is the weighted directed graph associated with Q. We say a subgraph T of G is a directed tree rooted at i ∈ [n] if (1) T does not contain any cycles and ( <ref type="formula" target="#formula_3">2</ref>) Node i has no outgoing edges, while every other node j ∈ [n] has exactly one outgoing edge. For each node i ∈ [n], we write T i to denote the set of all directed trees rooted at node i. We further define</p><formula xml:id="formula_72">Σ i = T ∈Ti (a,b)∈T q a,b and Σ = i∈[n] Σ i ,</formula><p>i.e., the weight of T is the product of its edge weights and Σ i is the sum of weights of trees in T i .</p><p>We can now formally state the Markov chain tree theorem.</p><p>Theorem C.4 (Markov chain tree theorem; see <ref type="bibr" target="#b0">[1]</ref>). Suppose Q is an erogidc Markov chain and p is its stationary distribution. Then we have p i = Σ i /Σ for every i ∈ [n].</p><p>We now use the Markov chain tree theorem to prove Lemma C.2.</p><p>Proof of Lemma C.2. Note that the lemma is trivial when n i=1 η i &gt; 1/4 so we assume without loss of generality that n i=1 η i ≤ 1/4. For any i ∈ [n], we have</p><formula xml:id="formula_73">Σ i = T ∈Ti (a,b)∈T q a,b ≤ T ∈Ti (a,b)∈T (1 + η a ) q a,b ≤ j∈[n] (1 + η j ) T ∈Ti (a,b)∈T q ′ a,b = j∈[n] (1 + η j ) • Σ ′ i ≤   1 + 2 j∈[n] η j   Σ ′ i .<label>(15)</label></formula><p>The third step holds because for any tree T ∈ T i , each node, other than node i, appears exactly once as a when calculating the weight of T . The last step follows from the fact that when</p><formula xml:id="formula_74">n i=1 η i ≤ 1/4, j∈[n] (1 + η j ) ≤ j∈[n] e ηj = e j∈[n] ηj ≤ 1 + 2 j∈[n] η j .</formula><p>Similarly, we have</p><formula xml:id="formula_75">Σ i ≥ T ∈Ti (a,b)∈T (1 − η a ) q a,b ≥ j∈[n] (1 − η j ) • Σ ′ i ≥   1 − 2 j∈[n] η j   Σ ′ i .<label>(16)</label></formula><p>The last inequality holds since, for n j=1 η j ≤ 1/2, we have</p><formula xml:id="formula_76">j∈[n] (1 − η j ) ≥ j∈[n] e −2ηj = exp   −2 j∈[n] η j   ≥ 1 − 2 j∈[n] η j . Since Σ = i Σ i , we have (1 − 2 i η i ) Σ ≤ Σ ≤ (1 + 2 i η i ) Σ. Applying Theorem C.4, p − p ′ 1 = i∈[n] |p i − p ′ i | = i∈[n] Σ i Σ − Σ i ′ Σ ′ ≤ i∈[n] Σ i Σ − Σ i Σ ′ + i∈[n] Σ i Σ ′ − Σ ′ i Σ ′ ≤ i∈[n] 2 n i=1 η i 1 − 2 n i=1 η i Σ i /Σ + i∈[n] 2 j∈[n] η j • Σ ′ i /Σ ′ ≤ 6 i∈[n] η i .</formula><p>This finishes the proof of the lemma.</p><p>Finally we prove Lemma 5.3:</p><p>Proof of Lemma 5.3. We start with the following claim, which states that entries of Q t and Q t−1 only differs by a small multiplicative factor.</p><p>Claim C.5. Suppose that the learning rate η ≤ 1/6 and let x 0 = 1/n = x 1 . Then for any t ≥ 2, Q t is a (η 1 , . . . , η n )-approximate to Q t−1 , where η j = 2ηx t−2 (j) + 4ηx t−1 (j) for each j ∈ [n].</p><p>Combing Claim C.5 and Lemma C.2, we have</p><formula xml:id="formula_77">x t − x t−1 1 ≤ 8 j∈[n] η j = 8 j∈[n] 2x t−2 (j) + 4x t−1 (j) η = 48η.</formula><p>This finishes the proof of Lemma 5.3.</p><p>Proof of Claim C.5. Let x 0 = 1/n = x 1 . By the updating rule of optimisitic Hedge, we have for any t ≥ 2, i, j ∈ [n] that</p><formula xml:id="formula_78">q t j (i) = exp(−η(2x t−1 (j)ℓ t−1 (i) − x t−2 (j)ℓ t−2 (i))) • q t−1 j (i) k∈[n] exp(−η(2x t−1 (j)ℓ t−1 (k) − x t−2 (j)ℓ t−2 (k))) • q t−1 j (k) ≤ exp(ηx t−2 (j)) • q t−1 j (i) k∈[n] exp(−2ηx t−1 (j)) • q t−1 j (k) = exp ηx t−2 (j) + 2ηx t−1 (j) • q t−1 j (i) ≤ (1 + 2ηx t−2 (j) + 4ηx t−1 (j)) • q t−1 j (i).</formula><p>The second step follows from ℓ t ∈ [0, 1] n and the last step follows from exp(a) ≤ 1 + 2a for a ≤ 1/2. The other side holds similarly:</p><formula xml:id="formula_79">q t j (i) = exp(−η(2x t−1 (j)ℓ t−1 (i) − x t−2 (j)ℓ t−2 (i))) • q t−1 j (i) k∈[n] exp(−η(2x t−1 (j)ℓ t−1 (k) − x t−2 (j)ℓ t−2 (k))) • q t−1 j (k) ≥ exp(−2ηx t−1 (j)) • q t−1 j (i) k∈[n] exp(ηx t−2 (j)) • q t−1 j (k) = exp − ηx t−2 (j) − 2ηx t−1 (j) • q t−1 j (i) ≥ (1 − ηx t−2 (j) − 2ηx t−1 (j)) • q t−1 j (i).</formula><p>Thus completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Corollary 5.4</head><p>The algorithm works as follow. We set η = (n log n) 1/4 m 1/2 T 1/4 and B r = 1 at initialization, for any player i ∈ [m] and τ = 1, . . . , T 1. Play x t i according to BM-Optimistic-Hedge, and receive  </p><formula xml:id="formula_80">ℓ t i . 2. If τ t=2 ℓ t i − ℓ t−1 i 2 ∞ + τ t=2 x t i − x t−1 i 2 1 ≥ B r . (a) Update B r+1 = 2B r , r ← r + 1, η r = min</formula><formula xml:id="formula_81">t i − x t−1 i 2 1 + Tr t=Tr−1+1 ℓ t i − ℓ t−1 i 2 ∞ .</formula><p>Then we have</p><formula xml:id="formula_82">swap-regret Tr−1+1:Tr ≤ 2n log n η r + 2η r   Tr t=Tr−1+1 x t i − x t−1 i 2 1 + Tr t=Tr−1+1 ℓ t i − ℓ t−1 i 2 ∞   ≤ 2(n log n) 3/4 • T 1/4 m 1/2 + 2 n log nB r + 2η r • I r ≤ 2(n log n) 3/4 • T 1/4 m 1/2 + 2 n log nB r + 2 2n log nI r ≤ 2(n log n) 3/4 • T 1/4 m 1/2 + 4 2n log nI r ≤ 2(n log n) 3/4 • T 1/4 m 1/2 + 4 2n log n • T t=2 x t i − x t−1 i 2 1 + T t=2 ℓ t i − ℓ t−1 i 2 ∞</formula><p>The first step follows from Lemma 5.2, the second step follows from the definition of I r and the fact</p><formula xml:id="formula_83">1 η r ≤ 1 η + B r n log n = m 1/2 T 1/4 (n log n) 1/4 + B r n log n</formula><p>The third step follows from η r ≤ n log n Br ≤ n log n Ir/2 , and the last step comes from</p><formula xml:id="formula_84">√ B r ≤ √ 2I r .</formula><p>Since the number of round is at most O(log T ), we have</p><formula xml:id="formula_85">swap-regret T ≤ log T   2(n log n) 3/4 T 1/4 m 1/2 + 4 2n log n • 2 T t=1 x t i − x t−1 i 2 1 + T t=1 ℓ t i − ℓ t−1 i 2 ∞  </formula><p>If all players adopt the algorithm, then we know their learning rate is no greater than η = (n log n) 1/4 m 1/2 T 1/4 , thus we know</p><formula xml:id="formula_86">x t i − x t−1 i 1 ≤ O(η) = O (n log n) 1/4 m 1/2 T 1/4</formula><p>(see Lemma 5.3) and ℓ</p><formula xml:id="formula_87">t i − ℓ t−1 i ∞ ≤ j =i x t j − x t−1 j 1 ≤ m • O(η) = O m 1/2 (n log n) 1/4 T 1/4</formula><p>. Thus the swap regret is at most</p><formula xml:id="formula_88">O (n log n) 3/4 m 1/2 T 1/4 log T .</formula><p>If the player is facing an adversary, then</p><formula xml:id="formula_89">x t i − x t−1 i 1 ≤ 2 and ℓ t i − ℓ t−1 i ∞ ≤ 1, thus we conclude its regret is at most O n log nT log T + (n log n) 3/4 m 1/2 T 1/4 log T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Another no swap regret algorithm</head><p>We prove the optimistic variant of a folklore algorithm, originally appeared in <ref type="bibr" target="#b6">[7]</ref>, could also achieve fast convergence of swap regret. Our perturbation analysis again plays a key role in the regret analysis.</p><p>Define Φ to be all swap functions that map [n] to [n]. We have |Φ| = n n . For any φ ∈ Φ, define the swap matrice S φ as: S φ i,j = 1 if φ(i) = j and S φ i,j = 0 otherwise. It is easy to see that S φ contains exactly one 1 each row.</p><p>[7] treats each swap matrice S φ as an expert, and run Hedge algorithm on all n n swap matrices. At time t, the output strategy p t is determined by these experts via solving a fix point problem <ref type="foot" target="#foot_1">3</ref> . The optimisitic variant of <ref type="bibr" target="#b6">[7]</ref> is shown in Algorithm 1. We first analysis the regret, Play p t and receive the loss vector l t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Update</p><formula xml:id="formula_90">q t+1 (φ) = x t (φ) exp(−η(2x t S φ ℓ t − x t−1 S φ ℓ t−1 )) φ∈Φ x t (φ) exp(−η(2x t S φ ℓ t − x t−1 S φ ℓ t−1 )) ∀φ ∈ Φ 4: Compute x t+1 = x t+1 Q (t+1)</formula><p>, where</p><formula xml:id="formula_91">Q (t+1) = φ∈Φ q t+1 (φ)S φ . 5: end for Lemma D.1. Algorithm 1 achieves regret swap-regret T ≤ n log n η + 2η T t=2 x t − x t−1 2 1 + 2η T t=2 ℓ t − ℓ t−1 2 ∞ .</formula><p>Proof. According to the updating rule, for any φ ∈ Φ, we have</p><formula xml:id="formula_92">swap-regret T = T t=2 x t , ℓ t − max φ∈Φ T t=2 x t S φ ℓ t = T t=2 x t Q (t) , ℓ t − max φ∈Φ T t=2 x t S φ ℓ t = T t=2 φ∈Φ x t (q t (φ)S φ )ℓ t − max φ∈Φ T t=2 x t S φ ℓ t = T t=2 φ∈Φ q t (φ) • x t S φ ℓ t − max φ∈Φ T t=2 x t S φ ℓ t ≤ n log n η + η T t=2 max φ∈Φ x t S φ ℓ t−1 − x t−1 S φ ℓ t−1 2 ≤ log n η + 2η T t=2 x t − x t−1 2 1 + 2η T t=2 ℓ t − ℓ t−1 2 ∞ .</formula><p>The fifth step follows the regret bound of optimistic Hedge and the last step follows from the fact that for any φ ∈ Φ,</p><formula xml:id="formula_93">x t S φ ℓ t − x t S φ ℓ t 2 = x t S φ ℓ t − x t−1 S φ ℓ t + x t−1 A φ ℓ t − x t−1 S φ ℓ t−1 2 ≤ 2 x t S φ ℓ t − x t−1 S φ ℓ t | 2 + 2|x t−1 S φ ℓ t − x t−1 S φ ℓ t−1 2 = 2 x t − x t−1 , S φ ℓ t + 2 x t−1 S φ , ℓ t − l t−1 ≤ 2 x t − x t−1 2 1 S φ ℓ t 2 ∞ + 2 x t−1 S φ 1 ℓ t − ℓ t−1 2 ∞ ≤2 x t − x t−1 2 1 + 2 ℓ t − ℓ t−1 2</formula><p>∞ . Thus completing the proof.</p><p>It remains to show that the environment is stable. Again, since x t is the stationary distribution of Q (t) , we only need some perturbation analysis on Q (t) . In particular, we have Lemma D.2. For any t, Q (t) is (6η, . . . , 6η) approximate to Q (t+1) . Proof. For any φ, we have q t+1 (φ) = q t (φ) exp(−η(2x t A φ ℓ t − x t−1 A φ ℓ t−1 )) φ∈Φ q t (φ) exp(−η(2x t A φ ℓ t − x t−1 A φ ℓ t−1 )) ≤ q t (φ) exp(η) φ∈Φ q t (φ) exp(−2η) ≤ (1 + 6η)q t (φ) Similarly, we have q t+1 (φ) = q t (φ) exp(−η(2x t A φ ℓ t − x t−1 A φ ℓ t−1 )) φ∈Φ q t (φ) exp(−η(2x t A φ ℓ t − x t−1 A φ ℓ t−1 )) ≥ q t (φ) exp(−2η) φ∈Φ q t (φ) exp(η) ≥ (1 − 6η)q t (φ) Thus, for any i, j ∈ [n], we have Q (t+1) i,j = φ∈Φ q t+1 (φ)S φ i,j ≤ (1 + 6η) φ∈Φ q t (φ)S φ i,j = (1 + 6η)Q (t) i,j and Q (t+1) i,j = φ∈Φ q t+1 (φ)S φ i,j ≥ (1 − 6η) φ∈Φ q t (φ)S φ i,j ≥ (1 − 6η)Q (t) i,j</p><p>Thus we conclude Q (t) is (6η, . . . , 6η) approximate to Q (t+1) .</p><p>Combining the above results, we have Theorem D.3. Suppose every player uses Algorithm 1 and choose η = O ( log n nm 2 T ) 1/4 , then each individual's swap regret is at most O m 1/2 n 5/4 (log n) where w t denotes the other player's strategy. Moreover, since Q (t−1) is (6η, . . . , 6η) approximates to Q (t) , we know </p><formula xml:id="formula_94">x t i − x t−1 i 1 ≤ 8 •</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Price of anarchy</head><p>In this section, we show that a large class of no swap regret algorithm satisfies the low approximate regret property (see Definition E.2). Thus when all players adopt such algorithm, they experience fast convergence to an approximately optimal social welfare in smooth games (see Definition E.1).</p><p>In particular, we show that the average social welfare converges to an approximately optimal welfare at rate O(1/T ). The proof in this section is straightforward, our aim is to point out that such fast convergence rate generally holds for no-swap regret algorithms. We first introduce the smooth game. Recall L(x) = i∈[m] L i (x) is the summation of each individual's loss under strategy profile x.</p><p>Definition E.1 (Smooth game). A cost minimization game is (λ, µ)-smooth if for all strategy profiles x and x ⋆ , i L i (x ⋆ i , x −i ) ≤ λ • L(x ⋆ ) + µ • L(x).</p><p>A wide range of games belongs to smooth game, including routing games, auctions, etc. We refer interested reader to <ref type="bibr" target="#b24">[25]</ref> for detailed coverage.</p><p>We next introduce the definition of low approximate regret.</p><p>Definition E.2 (Low approximate regret <ref type="bibr" target="#b13">[14]</ref>). A learning algorithm satisfies the low approximate regret property for given parameters (ǫ, A(n)) , if</p><formula xml:id="formula_95">(1 − ǫ) T t=1</formula><p>x t , ℓ t ≤ min i L(i) + A(n) ǫ .</p><p>Lemma E.3. The BM reduction transfers the low approximate regret property. In particular, if we reduce from a no external regret algorithm satisfying low approximate regret with(ǫ, A(n)), then the no swap regret algorithm satisfies low approximate regret with (ǫ, nA(n)).</p><p>Proof. For any fixed i, using the low approximate regret property, we know</p><formula xml:id="formula_96">(1 − ǫ) T t=1</formula><p>q t j , x t (j)ℓ t ≤ min i ′ T t=1</p><p>x t (j)ℓ t (i ′ ) + A(n) ǫ ≤ T t=1</p><p>x t (j)ℓ t (i) + A(n) ǫ .</p><p>Consequently, we have</p><formula xml:id="formula_97">(1 − ǫ) T t=1</formula><p>x t , ℓ t = (1 − ǫ)</p><formula xml:id="formula_98">T t=1 x t Q (t) , ℓ t = (1 − ǫ) T t=1 n j=1</formula><p>x t (j)q t j , ℓ t = (1 − ǫ) Thus concluding the proof.</p><p>A direct corollary of Lemma E.3 and Theorem 3 in <ref type="bibr" target="#b13">[14]</ref> is Theorem E.4. In a (λ, µ)-smooth game, if all players use no swap regret algorithm generated from BM reduction and a no external regret algorithm satisfying low approximate regret property with parameter ǫ and A(n) = log n, then we have</p><formula xml:id="formula_99">1 T T t=1 L(x t ) ≤ λ 1 − µ − ǫ • OPT + m T • 1 1 − µ − ǫ • n log n ǫ .</formula><p>where OPT denotes the optimal social welfare, i.e., min x L(x).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Let ǫ ′ = ǫ/k. Additive perturbations may change the stationary distribution dramatically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Section 4 B. 1</head><label>41</label><figDesc>Case when the learning rate is small We handle the case when η ≤ 64/(c 0 √ T ) = O(1/ √ T ) with the following lemma: Lemma B.1. Suppose both players run vanilla Hedge on game G 2 = (A, B 2 ) with learning rate η = O(1/ √ T ). Then the regret of the first player is at least Ω( √ T ) after T rounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>C Missing proof from Section 5 C. 1 2 Fix</head><label>512</label><figDesc>the KL divergence is at least 10 −10 kη + 10 −10 η = 10 −10 (k + 1)η. This finishes the induction and the proof of the claim. Proof of Lemma 5.any swap function φ : [n] → [n]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Start a new run of BM-Optimistic-Hedge with learning rate η r . For any round r, we use T r to denote its final iteration and I r = Tr t=Tr−1+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 1 :</head><label>1</label><figDesc>for t = 1, 2, . . . , do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>η 3 n 2 m 2 T ).Choosing η = O ( log n nm 2 T ) 1/4 , the regret is swap-regret T = O n 5/4 (log n) 3/4 T 1/4 m 1/2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3/4 T 1/4 . Proof. By Lemma D.1, for any palyer i ∈ [m], we have</figDesc><table><row><cell>swap-regret T ≤</cell><cell>n log n η</cell><cell>+ 2η</cell><cell>T t=2</cell><cell>x t i − x t−1 i</cell><cell>2 1 + 2η</cell><cell cols="2">T t=2</cell><cell cols="2">ℓ t i − ℓ t−1 i</cell><cell>2 ∞</cell></row><row><cell>≤</cell><cell>n log n η</cell><cell>+ 2η</cell><cell>T t=2</cell><cell cols="3">x t − x t−1 2 1 + 2mη</cell><cell cols="2">T t=2 j =i</cell><cell>x t j − x t−1 j</cell><cell>2 1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Note that Q t used in BM-Optimistic-Hedge is always ergodic.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">The algorithm is not efficient in general. However, we can turn it into an effiecient one by considering only n 2 swap matrices that are equal to indentical mapping except for one coordinate. The regret bound will only blow up by a √ n factor.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Supported by NSF IIS-1838154 and NSF CCF-1703925.</p><p>Preprint. Under review.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A proof of the markov chain tree theorem</title>
		<author>
			<persName><forename type="first">Venkat</forename><surname>Anantharam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pantelis</forename><surname>Tsoucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="192" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The multiplicative weights update method: a meta-algorithm and applications</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Theory of Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="164" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Subjectivity and correlation in randomized strategies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Aumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Economics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="67" to="96" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and furious learning in zero-sum games: vanishing regret with non-vanishing step sizes</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Piliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12977" to="12987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiplicative weights update in zero-sum games</title>
		<author>
			<persName><forename type="first">P</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><surname>Piliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM Conference on Economics and Computation</title>
				<meeting>the 2018 ACM Conference on Economics and Computation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="321" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From external to internal regret</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1307" to="1324" />
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Prediction, learning, and games</title>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Cesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vortices instead of equilibria in minmax optimization: Chaos and butterfly effects of online learning in zero-sum games</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Piliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="807" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Near-optimal no-regret algorithms for zero-sum games</title>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Deckelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms</title>
				<meeting>the twenty-second annual ACM-SIAM symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="235" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training gans with optimism</title>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasilis</forename><surname>Syrgkanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Last-iterate convergence: Zero-sum games and constrained min-max optimization</title>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Panageas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04252</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regret in the on-line decision problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><surname>Vohra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games and Economic Behavior</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="7" to="35" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName><surname>Vohra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games and Economic Behavior</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning in games: Robustness of fast convergence</title>
		<author>
			<persName><forename type="first">Dylan</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thodoris</forename><surname>Lykouris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4734" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Game theory, on-line prediction and boosting</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth annual conference on Computational learning theory</title>
				<meeting>the ninth annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="325" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. System Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">More efficient internal-regret-minimizing algorithms</title>
		<author>
			<persName><forename type="first">Amy</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Schudy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="239" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple adaptive procedure leading to correlated equilibrium</title>
		<author>
			<persName><forename type="first">Sergiu</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreu</forename><surname>Mas-Colell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1127" to="1150" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Introduction to online convex optimization</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Optimization</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="157" to="325" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient algorithms for online decision problems</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="307" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cycles in adversarial regularized learning</title>
		<author>
			<persName><forename type="first">Panayotis</forename><surname>Mertikopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Piliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
				<meeting>the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2703" to="2717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Algorithmic Game Theory</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Nisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Roughgarden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Tardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online learning with predictable sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="993" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimization, learning, and games with predictable sequences</title>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3066" to="3074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Intrinsic robustness of the price of anarchy</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Roughgarden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online learning and online convex optimization</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and trends in Machine Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="107" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast convergence of regularized learning in games</title>
		<author>
			<persName><forename type="first">Vasilis</forename><surname>Syrgkanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2989" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Acceleration through optimistic no-regret dynamics</title>
		<author>
			<persName><forename type="first">Jun-Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">D</forename><surname>Abernethy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3824" to="3834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">More adaptive algorithms for adversarial bandits</title>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference On Learning Theory</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1263" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online convex programming and generalized infinitesimal gradient ascent</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on machine learning (icml-03)</title>
				<meeting>the 20th international conference on machine learning (icml-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
