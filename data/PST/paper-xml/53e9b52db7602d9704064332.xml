<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classes of Kernels for Machine Learning: A Statistics Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Genton</surname></persName>
							<email>genton@stat.ncsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics North</orgName>
								<orgName type="institution">Carolina State University Raleigh</orgName>
								<address>
									<postCode>27695-8203</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics North</orgName>
								<orgName type="institution">Carolina State University Raleigh</orgName>
								<address>
									<postCode>27695-8203</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics North</orgName>
								<orgName type="institution">Carolina State University Raleigh</orgName>
								<address>
									<postCode>27695-8203</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Williamson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics North</orgName>
								<orgName type="institution">Carolina State University Raleigh</orgName>
								<address>
									<postCode>27695-8203</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classes of Kernels for Machine Learning: A Statistics Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Submitted 3/01; Published 12/01</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Anisotropic</term>
					<term>Compactly Supported</term>
					<term>Covariance</term>
					<term>Isotropic</term>
					<term>Locally Stationary</term>
					<term>Nonstationary</term>
					<term>Reducible</term>
					<term>Separable</term>
					<term>Stationary</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernelbased methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, the use of kernels in learning systems has received considerable attention. The main reason is that kernels allow to map the data into a high dimensional feature space in order to increase the computational power of linear machines (see for example <ref type="bibr" target="#b27">Vapnik, 1995</ref><ref type="bibr" target="#b28">, 1998</ref><ref type="bibr" target="#b8">, Cristianini and Shawe-Taylor, 2000)</ref>. Thus, it is a way of extending linear hypotheses to nonlinear ones, and this step can be performed implicitly. Support vector machines, kernel principal component analysis, kernel Gram-Schmidt, Bayes point machines, Gaussian processes, are just some of the algorithms that make crucial use of kernels for problems of classification, regression, density estimation, and clustering. In this paper, we present classes of kernels for machine learning from a statistics perspective. We discuss simple methods to design kernels in each of those classes and describe the algebra associated with kernels.</p><p>The kinds of kernel K we will be interested in are such that for all examples x and z in an input space X ⊂ R d :</p><formula xml:id="formula_0">K(x, z) = φ(x), φ(z) ,</formula><p>where φ is a nonlinear (or sometimes linear) map from the input space X to the feature space F, and •, • is an inner product. Note that kernels can be defined on more general input spaces X, see for instance <ref type="bibr" target="#b0">Aronszajn (1950)</ref>. In practice, the kernel K is usually defined directly, thus implicitly defining the map φ and the feature space F. It is therefore</p><formula xml:id="formula_1">i=1 l j=1 λ i λ j K(x i , x j ) ≥ 0. (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>Symmetric positive definite functions are called covariances in the statistics literature.</p><p>Hence kernels are essentially covariances, and we propose a statistics perspective on the design of kernels. It is simple to create new kernels from existing kernels because positive definite functions have a pleasant algebra, and we list some of their main properties below. First, if K 1 , K 2 are two kernels, and a 1 , a 2 are two positive real numbers, then:</p><formula xml:id="formula_3">K(x, z) = a 1 K 1 (x, z) + a 2 K 2 (x, z),<label>(2)</label></formula><p>is a kernel. This result implies that the family of kernels is a convex cone. The multiplication of two kernels K 1 and K 2 yields a kernel:</p><formula xml:id="formula_4">K(x, z) = K 1 (x, z)K 2 (x, z).<label>(3)</label></formula><p>Properties ( <ref type="formula" target="#formula_3">2</ref>) and (3) imply that any polynomial with positive coefficients, pol</p><formula xml:id="formula_5">+ (x) = { n i=1 α i x i |n ∈ N, α 1 , . . . , α n ∈ R + }, evaluated at a kernel K 1 , yields a kernel: K(x, z) = pol + (K 1 (x, z)).<label>(4)</label></formula><p>In particular, we have that:</p><formula xml:id="formula_6">K(x, z) = exp(K 1 (x, z)),<label>(5)</label></formula><p>is a kernel by taking the limit of the series expansion of the exponential function. Next, if g is a real-valued function on X, then</p><formula xml:id="formula_7">K(x, z) = g(x)g(z), (6) is a kernel. If ψ is an R p -valued function on X and K 3 is a kernel on R p × R p , then: K(x, z) = K 3 (ψ(x), ψ(z)),<label>(7)</label></formula><p>is also a kernel. Finally, if A is a positive definite matrix of size d × d, then:</p><formula xml:id="formula_8">K(x, z) = x T Az, (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>is a kernel. The results ( <ref type="formula" target="#formula_3">2</ref>)-( <ref type="formula" target="#formula_8">8</ref>) can easily be derived from (1), see also <ref type="bibr" target="#b8">Cristianini and Shawe-Taylor (2000)</ref>. The following property can be used to construct kernels and seems not to be known in the machine learning literature. Let h be a real-valued function on X, positive, with minimum at 0 (that is, h is a variance function). Then:</p><formula xml:id="formula_10">K(x, z) = 1 4 h(x + z) − h(x − z) ,<label>(9)</label></formula><p>is a kernel. The justification of (9) comes from the following identity for two random variables Y 1 and <ref type="formula" target="#formula_10">9</ref>), we obtain the kernel:</p><formula xml:id="formula_11">Y 2 : Covariance(Y 1 ,Y 2 )=[Variance(Y 1 + Y 2 )−Variance(Y 1 − Y 2 )]/4. For instance, consider the function h(x) = x T x. From (</formula><formula xml:id="formula_12">K(x, z) = 1 4 (x + z) T (x + z) − (x − z) T (x − z) = x T z.</formula><p>The remainder of the paper is set up as follows. In Section 2, 3, and 4, we discuss respectively the class of stationary, locally stationary, and nonstationary kernels. Of particular interest are the classes of compactly supported kernels and separable nonstationary kernels because they reduce the computational burden of kernel-based methods. For each class of kernels, we present their spectral representation and show how it can be used to design many new kernels. Section 5 addresses the reducibility of nonstationary kernels to stationarity or local stationarity, and we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Stationary Kernels</head><p>A stationary kernel is one which is translation invariant:</p><formula xml:id="formula_13">K(x, z) = K S (x − z),</formula><p>that is, it depends only on the lag vector separating the two examples x and z, but not on the examples themselves. Such a kernel is sometimes referred to as anisotropic stationary kernel, in order to emphasize the dependence on both the direction and the length of the lag vector. The assumption of stationarity has been extensively used in time series (see for example <ref type="bibr" target="#b2">Brockwell and Davis, 1991)</ref> and spatial statistics (see for example <ref type="bibr" target="#b6">Cressie, 1993)</ref> because it allows for inference on K based on all pairs of examples separated by the same lag vector. Many stationary kernels can be constructed from their spectral representation derived by <ref type="bibr" target="#b1">Bochner (1955)</ref>. He proved that a stationary kernel K S (x − z) is positive definite in R d if and only if it has the form:</p><formula xml:id="formula_14">K S (x − z) = R d cos ω T (x − z) F (dω), (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where F is a positive finite measure. The quantity F/K S (0) is called the spectral distribution function. Note that (10) is simply the Fourier transform of F . <ref type="bibr" target="#b7">Cressie and Huang (1999)</ref> and <ref type="bibr" target="#b12">Gneiting (2002b)</ref> use (10) to derive nonseparable space-time stationary kernels, see also <ref type="bibr" target="#b4">Christakos (2000)</ref> for illustrative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Genton</head><p>When a stationary kernel depends only on the norm of the lag vector between two examples, and not on the direction, then the kernel is said to be isotropic (or homogeneous), and is thus only a function of distance:</p><formula xml:id="formula_16">K(x, z) = K I ( x − z ).</formula><p>The spectral representation of isotropic stationary kernels has been derived from Bochner's theorem <ref type="bibr" target="#b1">(Bochner, 1955)</ref> by <ref type="bibr" target="#b29">Yaglom (1957)</ref>:</p><formula xml:id="formula_17">K I ( x − z ) = ∞ 0 Ω d ω x − z F (dω),<label>(11)</label></formula><p>where </p><formula xml:id="formula_18">Ω d (x) = 2 x (d−2)/2 Γ d 2 J (d−2)/2 (x</formula><formula xml:id="formula_19">(x) = cos(x), Ω 2 (x) = J 0 (x)</formula><p>, and Ω 3 (x) = sin(x)/x. Here again, by choosing a nondecreasing bounded function F (or its derivative f ), we can derive the corresponding kernel from (11). For instance in R 1 , with the spectral density f (ω) = (1 − cos(ω))/(πω 2 ), we derive the triangular kernel:</p><formula xml:id="formula_20">K I (x − z) = ∞ 0 cos(ω|x − z|) 1 − cos(ω) πω 2 dω = 1 2 (1 − |x − z|) + ,</formula><p>where (x) + = max(x, 0) (see Figure <ref type="figure" target="#fig_0">1</ref>). Note that an isotropic stationary kernel obtained with Ω d is positive definite in R d and in lower dimensions, but not necessarily in higher dimensions. For example, the kernel <ref type="bibr">Cressie (1993, p.84</ref>) for a counterexample. It is interesting to remark from ( <ref type="formula" target="#formula_17">11</ref>) that an isotropic stationary kernel has a lower bound <ref type="bibr" target="#b26">(Stein, 1999)</ref>:</p><formula xml:id="formula_21">K I (x − z) = (1 − |x − z|) + /2 is positive definite in R 1 but not in R 2 , see</formula><formula xml:id="formula_22">K I ( x − z )/K I (0) ≥ inf x≥0 Ω d (x),</formula><p>thus yielding:</p><formula xml:id="formula_23">K I ( x − z )/K I (0) ≥ −1 i nR 1 K I ( x − z )/K I (0) ≥ −0.403 in R 2 K I ( x − z )/K I (0) ≥ −0.218 in R 3 K I ( x − z )/K I (0) ≥ 0 i nR ∞ .</formula><p>The isotropic stationary kernels must fall off more quickly as the dimension d increases, as might be expected by examining the basis functions Ω d . Those in R ∞ have the greatest restrictions placed on them. Isotropic stationary kernels that are positive definite in R d form a nested family of subspaces. When d → ∞ the basis Ω d (x) goes to exp(−x 2 ). Schoenberg (1938) proved that if β d is the class of positive definite functions of the form given by <ref type="bibr" target="#b1">Bochner (1955)</ref>, then the classes for all d have the property:</p><formula xml:id="formula_24">β 1 ⊃ β 2 ⊃ • • • ⊃ β d ⊃ • • • ⊃ β ∞ ,</formula><p>so that as d is increased, the space of available functions is reduced. Only functions with the basis exp(−x 2 ) are contained in all the classes. The positive definite requirement imposes a smoothness condition on the basis as the dimension d is increased. Several criteria to check the positive definiteness of stationary kernels can be found in <ref type="bibr" target="#b3">Christakos (1984)</ref>. Further isotropic stationary kernels defined with non-Euclidean norms have recently been discussed by <ref type="bibr" target="#b5">Christakos and Papanicolaou (2000)</ref>.</p><p>From the spectral representation (11), we can construct many isotropic stationary kernels. Some of the most commonly used are depicted in Figure <ref type="figure" target="#fig_1">2</ref>. They are defined by the equations listed in Table <ref type="table">1</ref>, where θ &gt; 0 is a parameter. As an illustration, the exponential kernel (d) is obtained from the spectral representation (11) with the spectral density:</p><formula xml:id="formula_25">f (ω) = 1 π θ + πθω 2</formula><p>, whereas the Gaussian kernel (e) is obtained with the spectral density:</p><formula xml:id="formula_26">f (ω) = √ θ 2 √ π exp − θω 2 4 .</formula><p>Note also that the circular and spherical kernels have compact support. They have a linear behavior at the origin, which is also true for the exponential kernel. The rational quadratic, Gaussian, and wave kernels have a parabolic behavior at the origin. This indicates a different degree of smoothness. Finally, the Matérn kernel <ref type="bibr" target="#b17">(Matérn, 1960)</ref> has recently received considerable attention, because it allows to control the smoothness with a parameter ν. The Matérn kernel is defined by:</p><formula xml:id="formula_27">K I ( x − z )/K I (0) = 1 2 ν−1 Γ(ν) 2 √ ν x − z θ ν H ν 2 √ ν x − z θ , (<label>12</label></formula><formula xml:id="formula_28">)</formula><p>平滑度不同， matern核能控 制平滑度  where Γ is the Gamma function and H ν is the modified Bessel function of the second kind of order ν. Note that the Matérn kernel reduces to the exponential kernel for ν = 0.5 and</p><formula xml:id="formula_29">同向异性平稳核 Name of kernel K I ( x − z )/K I (0) (a) Circular positive definite in R 2 2 π arccos x−z θ − 2 π x−z θ 1 − x−z θ 2 if x − z &lt; θ zero otherwise (b) Spherical positive definite in R 3 1 − 3 2 x−z θ + 1 2 x−z θ 3 if x − z &lt; θ zero otherwise (c) Rational quadratic positive definite in R d 1 − x−z 2 x−z 2 +θ (d) Exponential positive definite in R d exp − x−z θ (e) Gaussian positive definite in R d exp − x−z 2 θ (f) Wave positive definite in R 3 θ x−z sin x−z θ</formula><p>Table <ref type="table">1</ref>: Some commonly used isotropic stationary kernels.</p><p>to the Gaussian kernel for ν → ∞. Therefore, the Matérn kernel includes a large class of kernels and will prove very useful for applications because of this flexibility.</p><p>Compactly supported kernels are kernels that vanish whenever the distance between two examples x and z is larger than a certain cut-off distance, often called the range. For instance, the spherical kernel (b) is a compactly supported kernel since K I ( x − z ) = 0 when x − z ≥ θ. This might prove a crucial advantage for certain applications dealing with massive data sets, because the corresponding Gram matrix G, whose ij-th element is G ij = K(x i , x j ), will be sparse. Then, linear systems involving the matrix G can be solved very efficiently using sparse linear algebra techniques, see for example <ref type="bibr" target="#b10">Gilbert et al. (1992)</ref>. As an illustrative example in R 2 , consider 1,000 examples, uniformly distributed in the unit square. Suppose that a spherical kernel (b) is used with a range of θ = 0.2. The corresponding Gram matrix contains 1,000,000 entries, of which only 109,740 are not equal to zero, and is represented in the left panel of Figure <ref type="figure" target="#fig_2">3</ref> (black dots represent nonzero entries). The entries of the Gram matrix can be reordered, for instance with a sparse reverse Cuthill-McKee algorithm (see <ref type="bibr" target="#b10">Gilbert et al., 1992)</ref>, in order to have the nonzero elements closer to the diagonal. The result is displayed in the right panel of Figure <ref type="figure" target="#fig_2">3</ref>. The reordered Gram matrix has now a bandwidth of only 252 instead of 1,000 for the initial matrix, and important computational savings can be obtained. Of course, if the spherical and the circular kernels would be the only compactly supported kernels available, this technique would be limited. Fortunately, large classes of compactly supported kernels can be constructed, see for example <ref type="bibr" target="#b11">Gneiting (2002a)</ref> and references therein. A compactly supported kernel of Matérn type can be obtained by multiplying the kernel (12) by the kernel:</p><formula xml:id="formula_30">max 1 − x − z θ ν , 0 ,</formula><p>where θ &gt; 0 and ν ≥ (d + 1)/2, in order to insure positive definiteness. This product is a kernel by the property (3). Beware that it is not possible to simply "cut-off" a kernel in order to obtain a compactly supported one, because the result will not be positive definite in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Locally Stationary Kernels</head><p>A simple departure from the stationary kernels discussed in the previous section is provided by locally stationary kernels <ref type="bibr" target="#b24">(Silverman, 1957</ref><ref type="bibr" target="#b25">(Silverman, , 1959))</ref>:</p><formula xml:id="formula_31">K(x, z) = K 1 x + z 2 K 2 (x − z),<label>(13)</label></formula><p>where K 1 is a nonnegative function and K 2 is a stationary kernel. Note that if K 1 is a positive constant, then (13) reduces to a stationary kernel. Thus, the class of locally stationary kernels has the desirable property of including stationary kernels as a special case. Because the product of K 1 and K 2 is defined only up to a multiplicative positive constant, we further impose that K 2 (0) = 1. The variable (x + z)/2 has been chosen because of its suggestive meaning of the average or centroid of the examples x and z. The variance is determined by:</p><formula xml:id="formula_32">K(x, x) = K 1 (x)K 2 (0) = K 1 (x), (<label>14</label></formula><formula xml:id="formula_33">)</formula><p>thus justifying the name of power schedule for K 1 (x), which describes the global structure.</p><p>On the other hand, K 2 (x−z) is invariant under shifts and thus describes the local structure.</p><p>It can be obtained by considering:</p><formula xml:id="formula_34">K(x/2, −x/2) = K 1 (0)K 2 (x). (<label>15</label></formula><formula xml:id="formula_35">)</formula><p>Equations ( <ref type="formula" target="#formula_32">14</ref>) and ( <ref type="formula" target="#formula_34">15</ref>) imply that the kernel K(x, z) defined by ( <ref type="formula" target="#formula_31">13</ref>) is completely determined by its values on the diagonal x = z and antidiagonal x = −z, for:</p><formula xml:id="formula_36">K(x, z) = K((x + z)/2, (x + z)/2)K((x − z)/2, −(x − z)/2) K(0, 0) . (<label>16</label></formula><formula xml:id="formula_37">)</formula><p>Thus, we see that K 1 is invariant with respect to shifts parallel to the antidiagonal, whereas K 2 is invariant with respect to shifts parallel to the diagonal. These properties allow to find moment estimators of both K 1 and K 2 from a single realization of data, although the kernel is not stationary. We already mentioned that stationary kernels are locally stationary. Another special class of locally stationary kernels is defined by kernels of the form:</p><formula xml:id="formula_38">K(x, z) = K 1 (x + z),<label>(17)</label></formula><p>the so-called exponentially convex kernels <ref type="bibr" target="#b14">(Loève, 1946</ref><ref type="bibr" target="#b15">(Loève, , 1948))</ref>. From ( <ref type="formula" target="#formula_36">16</ref>), we see immediately that K 1 (x + z) ≥ 0. Actually, as noted by Loève, any two-sided Laplace transform of a nonnegative function is an exponentially convex kernel. A large class of locally stationary kernels can therefore be constructed by multiplying an exponentially convex kernel by a stationary kernel, since the product of two kernels is a kernel by the property (3). However, the following example is a locally stationary kernel in R 1 which is not the product of two kernels:</p><formula xml:id="formula_39">exp − a(x 2 + z 2 ) = exp − 2a((x + z)/2) 2 exp − a(x − z) 2 /2 , a &gt; 0, (<label>18</label></formula><formula xml:id="formula_40">)</formula><p>since the first factor in the right side is a positive function without being a kernel, and the second factor is a kernel. Finally, with the positive definite Delta kernel δ(x − z), which is equal to 1 if x = z and 0 otherwise, the product:</p><formula xml:id="formula_41">K(x, z) = K 1 x + z 2 δ(x − z),</formula><p>is a locally stationary kernel, often called a locally stationary white noise.</p><p>The spectral representation of locally stationary kernels has remarkable properties. Indeed, it can be written as <ref type="bibr" target="#b24">(Silverman, 1957)</ref>:</p><formula xml:id="formula_42">K(x, z) = R d R d cos ω T 1 x − ω T 2 z f 1 ω 1 + ω 2 2 f 2 (ω 1 − ω 2 )dω 1 dω 2 ,</formula><p>i.e. the spectral density f 1</p><formula xml:id="formula_43">ω 1 +ω 2 2 f 2 (ω 1 − ω 2</formula><p>) is also a locally stationary kernel, and:</p><formula xml:id="formula_44">K 1 (u) = R d cos(ω T u)f 2 (ω)dω, K 2 (v) = R d cos(ω T v)f 1 (ω)dω,</formula><p>i.e. K 1 , f 2 and K 2 , f 1 are Fourier transform pairs. For instance, to the locally stationary kernel (18) corresponds the spectral density:</p><formula xml:id="formula_45">f 1 ω 1 + ω 2 2 f 2 (ω 1 − ω 2 ) = 1 4πa exp − 1 2a ((ω 1 + ω 2 )/2) 2 exp − 1 8a (ω 1 − ω 2 ) 2 /2 ,</formula><p>which is immediately seen to be locally stationary since, except for a positive factor, it is of the form ( <ref type="formula" target="#formula_39">18</ref>), with a replaced by 1/(4a). Thus, we can design many locally stationary kernels with the help of their spectral representation. In particular, we can obtain a very rich family of locally stationary kernels by multiplying a Matérn kernel ( <ref type="formula" target="#formula_27">12</ref>) by an exponentially convex kernel (17). The resulting product is still a kernel by the property (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Nonstationary Kernels</head><p>The most general class of kernels is the one of nonstationary kernels, which depend explicitly on the two examples x and z: K(x, z).</p><p>For example, the polynomial kernel of degree p:</p><formula xml:id="formula_46">K(x, z) = (x T z) p ,</formula><p>is a nonstationary kernel. The spectral representation of nonstationary kernels is very general. A nonstationary kernel K(x, z) is positive definite in R d if and only if it has the form <ref type="bibr" target="#b30">(Yaglom, 1987)</ref>:</p><formula xml:id="formula_47">K(x, z) = R d R d cos ω T 1 x − ω T 2 z F (dω 1 , dω 2 ),<label>(19)</label></formula><p>where F is a positive bounded symmetric measure. When the function F (ω 1 , ω 2 ) is concentrated on the diagonal ω 1 = ω 2 , then (19) reduces to the spectral representation (10) of stationary kernels. Here again, many nonstationary kernels can be constructed with ( <ref type="formula" target="#formula_47">19</ref>).</p><p>Of interest are nonstationary kernels obtained from ( <ref type="formula" target="#formula_47">19</ref>) with ω 1 = ω 2 but with a spectral density that is not integrable in a neighborhood around the origin. Such kernels are referred to as generalized kernels <ref type="bibr" target="#b18">(Matheron, 1973)</ref>. For instance, the Brownian motion generalized kernel corresponds to a spectral density f (ω) = 1/ ω 2 <ref type="bibr" target="#b16">(Mandelbrot and Van Ness, 1968)</ref>.</p><p>A particular family of nonstationary kernels is the one of separable nonstationary kernels:</p><formula xml:id="formula_48">K(x, z) = K 1 (x)K 2 (z),</formula><p>where K 1 and K 2 are stationary kernels evaluated at the examples x and z respectively. The resulting product is a kernel by the property (3) in Section 1. Separable nonstationary kernels possess the property that their Gram matrix G, whose ij-th element is G ij = K(x i , x j ), can be written as a tensor product (also called Kronecker product, see <ref type="bibr" target="#b13">Graham, 1981)</ref> of two vectors defined by K 1 and K 2 respectively. This is especially useful to reduce computational burden when dealing with massive data sets. For instance, consider a set of l examples x 1 , . . . , x l . The memory requirements fot the computation of the Gram matrix is then reduced from l 2 to 2l since it suffices to evaluate the vectors a = (K 1 (x 1 ), . . . , K 1 (x l )) T and b = (K 2 (x 1 ), . . . , K 2 (x l )) T . We then have G = ab T . Such a computational reduction can be of crucial importance for certain applications involving very large training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Reducible Kernels</head><p>In this section, we discuss the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity. The main idea is to find a new feature space where stationarity (see <ref type="bibr" target="#b22">Sampson and Guttorp, 1992)</ref> or local stationarity (see <ref type="bibr" target="#b9">Genton and Perrin, 2001</ref>) can be achieved. We say that a nonstationary kernel K(x, z) is stationary reducible if there exist a bijective deformation Φ such that:</p><formula xml:id="formula_49">K(x, z) = K * S (Φ(x) − Φ(z)),<label>(20)</label></formula><p>where K * S is a stationary kernel. For example in R 2 , the nonstationary kernel defined by:</p><formula xml:id="formula_50">K(x, z) = x + z − z − x 2 x z ,<label>(21)</label></formula><p>is stationary reducible with the deformation:</p><formula xml:id="formula_51">Φ(x 1 , x 2 ) = ln x 2 1 + x 2 2 , arctan(x 2 /x 1 ) T ,</formula><p>yielding the stationary kernel:</p><formula xml:id="formula_52">K * S (u 1 , u 2 ) = cosh(u 1 /2) − (cosh(u 1 /2) − cos(u 2 ))/2. (<label>22</label></formula><formula xml:id="formula_53">)</formula><p>Effectively, it is straightforward to check with some algebra that (22) evaluated at:</p><formula xml:id="formula_54">Φ(x) − Φ(z) = ln x z , arctan(x 2 /x 1 ) − arctan(z 2 /z 1 )</formula><p>T , yields the kernel (21). <ref type="bibr">Perrin and</ref><ref type="bibr">Senoussi (1999, 2000)</ref> characterize such deformations Φ. Specifically, if Φ and its inverse are differentiable in R d , and K(x, z) is continuously differentiable for x = y, then K satisfies (20) if and only if:</p><formula xml:id="formula_55">D x K(x, z)Q −1 Φ (x) + D z K(x, z)Q −1 Φ (z) = 0, x = y,<label>(23)</label></formula><p>where Q Φ is the Jacobian of Φ and D x denotes the partial derivatives operator with respect to x. It can easily be checked that the kernel (21) satisfies the above equation ( <ref type="formula" target="#formula_55">23</ref>). Unfortunately, not all nonstationary kernels can be reduced to stationarity through a deformation Φ. Consider for instance the kernel in R 1 :</p><formula xml:id="formula_56">K(x, z) = exp(2 − x 6 − z 6 ),<label>(24)</label></formula><p>which is positive definite as can be seen from ( <ref type="formula">6</ref>). It is obvious that K(x, z) does not satisfy Equation ( <ref type="formula" target="#formula_55">23</ref>) and thus is not stationary reducible. This is the motivation of <ref type="bibr" target="#b9">Genton and Perrin (2001)</ref> to extend the model (20) to locally stationary kernels. We say that a nonstationary kernel K is locally stationary reducible if there exists a bijective deformation Φ such that:</p><formula xml:id="formula_57">K(x, z) = K 1 Φ(x) + Φ(z) 2 K 2 Φ(x) − Φ(z) , (<label>25</label></formula><formula xml:id="formula_58">)</formula><p>where K 1 is a nonnegative function and K 2 is a stationary kernel. Note that if K 1 is a positive constant, then Equation ( <ref type="formula" target="#formula_57">25</ref>) reduces to the model ( <ref type="formula" target="#formula_49">20</ref>). <ref type="bibr" target="#b9">Genton and Perrin (2001)</ref> characterize such transformations Φ. For instance, the nonstationary kernel (24) can be reduced to a locally stationary kernel with the transformation:</p><formula xml:id="formula_59">Φ(x) = x 3 3 − 1 3 , (<label>26</label></formula><formula xml:id="formula_60">)</formula><p>yielding:</p><formula xml:id="formula_61">K 1 (u) = exp −18u 2 − 12u (27) K 2 (v) = exp − 9 2 v 2 . (<label>28</label></formula><formula xml:id="formula_62">)</formula><p>Here again, it can easily be checked from ( <ref type="formula">27</ref>), (28), and (26) that:</p><formula xml:id="formula_63">K 1 Φ(x) + Φ(z) 2 K 2 Φ(x) − Φ(z) = exp(2 − x 6 − z 6 ).</formula><p>Of course, it is possible to construct nonstationary kernels that are neither stationary reducible nor locally stationary reducible. Actually, the familiar class of polynomial kernels of degree p, K(x, z) = (x T z) p , cannot be reduced to stationarity or local stationarity with a bijective transformation Φ. Further research is needed to characterize such kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have described several classes of kernels that can be used for machine learning: stationary (anisotropic/isotropic/compactly supported), locally stationary, nonstationary and separable nonstationary kernels. Each class has its own particular properties and spectral representation. The latter allows for the design of many new kernels in each class. We have not addressed the question of which class is best suited for a given problem, but we hope that further research will emerge from this paper. It is indeed important to find adequate classes of kernels for classification, regression, density estimation, and clustering. Note that kernels from the classes presented in this paper can be combined indefinitely by using the properties (2)-( <ref type="formula" target="#formula_10">9</ref>). This should prove useful to researchers designing new kernels and algorithms for machine learning. In particular, the reducibility of nonstationary kernels to simpler kernels which are stationary or locally stationary suggests interesting applications. For instance, locally stationary kernels are in fact separable kernels in a new coordinate system defined by (x + z)/2 and x − z, and as already mentioned, provide computational advantages when dealing with massive data sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The spectral density f (ω) = (1 − cos(ω))/(πω 2 ) (left) and its corresponding isotropic stationary kernel K I (x − z) = (1 − |x − z|) + /2 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Some isotropic stationary kernels: (a) circular; (b) spherical; (c) rational quadratic; (d) exponential; (e) Gaussian; (f) wave.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The Gram matrix for 1,000 examples uniformly distributed in the unit square, based on a spherical kernel with range θ = 0.2: initial (left panel); after reordering (right panel).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>I would like to acknowledge support for this project from U.S. Army TACOM Research, Development and Engineering Center under the auspices of the U.S. Army Research Office Scientific Services Program administered by Battelle (Delivery Order 634, Contract No. DAAH04-96-C-0086, TCN 00-131). I would like to thank David Gorsich from U.S. Army TACOM, Olivier Perrin, as well as the Editors and two anonymous reviewers, for their comments that improved the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Theory of reproducing kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. American Mathematical Soc</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="337" to="404" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Harmonic Analysis and the Theory of Probability</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bochner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955">1955</date>
			<publisher>University of California Press</publisher>
			<pubPlace>Los Angeles, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Brockwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<title level="m">Time Series: Theory and Methods</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the problem of permissible covariance and variogram models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Christakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Water Resources Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="265" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Christakos</surname></persName>
		</author>
		<title level="m">Modern Spatiotemporal Geostatistics</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Norm-dependent covariance permissibility of weakly homogeneous spatial random fields and its consequences in spatial statistics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Christakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Papanicolaou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Environmental Research and Risk assessment</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="471" to="478" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Statistics for Spatial Data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cressie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classes of nonseparable, spatio-temporal stationary covariance functions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cressie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">448</biblScope>
			<biblScope unit="page" from="1330" to="1340" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines and other Kernel-based Learning Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On a time deformation reducing nonstationary stochastic processes to local stationarity</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Genton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Perrin</surname></persName>
		</author>
		<idno>NCSU</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse matrices in MATLAB: design and implementation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="333" to="356" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compactly supported correlation functions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gneiting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<date type="published" when="2002">2002a</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonseparable, stationary covariance functions for space-time data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gneiting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="2002">2002b</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Kronecker Products and Matrix Calculus: with Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Ellis Horwood Limited</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fonctions aléatoires à décomposition orthogonale exponentielle</title>
		<author>
			<persName><forename type="first">M</forename><surname>Loève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">La Revue Scientifique</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="159" to="162" />
			<date type="published" when="1946">1946</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fonctions aléatoires du second ordre</title>
		<author>
			<persName><forename type="first">M</forename><surname>Loève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processus Stochastiques et Mouvement Brownien</title>
				<meeting>essus Stochastiques et Mouvement Brownien<address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<publisher>Gauthier-Villars</publisher>
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fractional brownian motions, fractional noises and applications</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Mandelbrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Van Ness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="422" to="437" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spatial Variation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Matérn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The intrinsic random functions and their applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Matheron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Probab</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="439" to="468" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Functions of positive and negative type and their connection with the theory of integral equations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. Roy. Soc. London, A</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="page" from="415" to="446" />
			<date type="published" when="1909">1909</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reducing non-stationary stochastic processes to stationarity by a time deformation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Senoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Probability Letters</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="393" to="397" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducing non-stationary random fields to stationarity and isotropy using a space deformation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Senoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Probability Letters</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonparametric estimation of nonstationary spatial covariance structure</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guttorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">417</biblScope>
			<biblScope unit="page" from="108" to="119" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Metric spaces and completely monotone functions</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Schoenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="811" to="841" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Locally stationary random processes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions Information Theory</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="182" to="187" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A matching theorem for locally stationary random processes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="373" to="383" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Interpolation of Spatial Data: Some Theory for Kriging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Some classes of random fields in n-dimensional space, related to stationary random processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Yaglom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability and its Applications</title>
				<imprint>
			<date type="published" when="1957">1957</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="273" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Yaglom</surname></persName>
		</author>
		<title level="m">Correlation Theory of Stationary and Related Random Functions</title>
				<editor>
			<persName><forename type="first">I</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Series in Statistics</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
