<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fetching instruction streams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alex</forename><surname>Ramirez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Department</orgName>
								<orgName type="institution">Universitat Politecnica de Catalunya aramirez</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Oliverio</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
							<email>osantana@ac.upc.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Department</orgName>
								<orgName type="institution">Universitat Politecnica de Catalunya aramirez</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josep</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Department</orgName>
								<orgName type="institution">Universitat Politecnica de Catalunya aramirez</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mateo</forename><surname>Valero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Department</orgName>
								<orgName type="institution">Universitat Politecnica de Catalunya aramirez</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fetching instruction streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fetch performance is a very important factor because it effectively limits the overall processor performance. However, there is little performance advantage in increasing front-end performance beyond what the back-end can consume. For each processor design, the target is to build the best possible fetch engine for the required performance level. A fetch engine will be better if it provides better performance, but also if it takes fewer resources, requires less chip area, or consumes less power.</p><p>In this paper we propose a novel fetch architecture based on the execution of long streams of sequential instructions, taking maximum advantage of code layout optimizations. We describe our architecture in detail, and show that it requires less complexity and resources than other high performance fetch architectures like the trace cache, while providing a high fetch performance suitable for wide-issue superscalar processors.</p><p>Our results show that using our fetch architecture and code layout optimizations obtains 10% higher performance than the EV8 fetch architecture, and 4% higher than the FTB architecture using state-of-the-art branch predictors, while being only 1.5% slower than the trace cache. Even in the absence of code layout optimizations, fetching instruction streams is still 10% faster than the EV8, and only 4% slower than the trace cache.</p><p>Fetching instruction streams effectively exploits the special characteristics of layout optimized codes to provide a high fetch performance, close to that of a trace cache, but has a much lower cost and complexity, similar to that of a basic block architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fetch performance is a very important factor, because in a classic processor design, it is not possible to execute instructions faster than they can be fetched. In this sense, the fetch engine effectively limits the overall processor performance.</p><p>However, there is little performance advantage in in-creasing front-end performance beyond what the back-end can consume. For each processor design, the target is to build the best possible fetch engine for the required performance level. A fetch engine will be better if it provides higher performance, but also if it takes fewer resources, requires less chip area, or consumes less power.</p><p>The fetch engine has evolved from fetching a single instruction every few cycles, to one instruction per cycle, to a full basic block per cycle, to a full instruction trace per cycle. Each new architecture increases performance over the previous one, but also increases the cost and complexity.</p><p>The increase in complexity derives from the fact that the fetch engine has lost its relationship with the high-level programming constructs that it is fetching. While fetching basic blocks, the architecture is still aware of what it is fetching. When fetching instruction traces, or any other randomly constructed fetch structure, the architecture loses track of what the code is doing, and that increases complexity.</p><p>High fetch performance is always desirable, but to maintain complexity under control, it is necessary to design the fetch engine around the high-level programming constructs. This is what we have done with instruction streams.</p><p>An instruction stream is a sequential run of instructions, from the target of a taken branch, to the next taken branch (also called a dynamic block in the literature). A single instruction stream may contain multiple basic blocks, and multiple branches, as long as all the intermediate branches are not-taken.</p><p>As such, an instruction stream is fully identified by the starting instruction address and the stream length. Unlike traces, streams do not require information about the behavior of the branches contained in the stream, because it is implicit in the definition: all intermediate branches are not taken, and the terminating branch is always taken.</p><p>Furthermore, instruction streams are defined by the program (the branches, and the branch behavior) not by hardware limitations, and once again provide semantic information about the code behavior to the fetch engine Figure <ref type="figure" target="#fig_0">1</ref> shows an example control flow graph from which we will find the possible streams. The figure shows a loop containing an if-then-else structure. Our profile data shows that is the most frequently followed path through the loop. Using this information, we lay out the code so that the path goes through a not-taken branch, and falls-through from . Basic block is mapped somewhere else, and can only be reached through a taken branch at the end of basic block .</p><p>From the resulting code layout we may encounter four possible streams composed by basic blocks , , , . The first stream corresponds to the sequential path starting at basic block and going through the frequent path found by our profile. Basic block is the target of a taken branch, and the next taken branch is found at the end of basic block . The infrequent case follows the taken branch at the end of , goes through , and jumps back into basic block .</p><p>These are all the possible streams found as long as branch prediction is correct. In our architecture, instruction streams are not treated as atomic regions of code. That is, if a branch misprediction is detected halfway through a stream, the front-end engine is informed, fetch is redirected, and proceeds from the point of misprediction. We do not rollback execution to the beginning of the stream. Instructions up to the mispredicted branch are executed and graduated normally. The front-end recovers after the branch, and fetch is redirected towards the correct branch target.</p><p>If we account for branch mispredictions, there is a fifth possible stream which does not follow the definition above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stream</head><p>does not start at the target of a taken branch: it starts at the target of a branch misprediction. That is, if the branch at the end of is predicted taken, but it turns out to be not taken, execution would continue from basic block (we do not rollback to basic block ), but there is no full stream starting there.</p><p>To avoid this situation, we define a partial instruction stream as a stream which starts at the target of a branch misprediction, and goes on until the next taken branch. This allows the front-end engine to maintain the stream semantics even in the event of a branch misprediction. Table <ref type="table" target="#tab_0">1</ref> summarizes the reasons for designing a fetch engine around the concept of an instruction stream. Streams directly map to the structure of the high-level programming constructs, respecting loop bodies and hammock structures. Streams span multiple basic blocks and represent long sequences of instructions, specially in layout optimized codes, as we show in <ref type="bibr" target="#b27">[28]</ref>. Based on these facts, we will show that a fetch engine based on streams has a low implementation cost, results in a low complexity fetch architecture, and deliver high fetch performance comparable to that of a trace cache.</p><p>This paper describes in detail our implementation of the stream front-end architecture (a front-end valid for any back-end), which introduces a novel feature: the next stream predictor, which provides stream level sequencing (that is, it steps through the code one stream at a time), allowing the rest of the fetch engine to fetch multiple consecutive basic blocks in a single cycle without need of complex circuitry.</p><p>Our results show that indeed the stream fetch architecture provides for high performance, comparable to that of a trace cache, while maintaining a low implementation cost and complexity, similar to that of a basic block architecture.</p><p>In Section 2 we discuss previous related work, including state of the art fetch architectures like the FTB proposed by Reinman, Austin and Calder <ref type="bibr" target="#b29">[30]</ref> and the trace cache architecture as proposed by Rotenberg, Bennett and Smith in <ref type="bibr" target="#b31">[32]</ref>.</p><p>In Section 3 we describe our proposed stream fetch architecture in detail, showing how it improves on other architectures by exploiting the special characteristics of layout optimized applications without requiring additional complexity.</p><p>In Section 4 we provide simulation results comparing our stream architecture with the Alpha EV8, the FTB, and the trace cache architectures to back up our claims about the stream front-end architecture:</p><p>¯Fetching streams provides higher performance than the EV8 and FTB front-ends, and a performance close to that of a trace cache, maintaining a reduced cost and minimum complexity.</p><p>¯Even in the absence of code layout optimizations, the stream fetch architecture still provides performance improvements over the EV8 and FTB architectures.</p><p>Finally, in Section 5 we provide our conclusions for this work, and provide guidelines for future development of this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Superscalar processors require a fetch engine capable of providing more than one instruction per cycle in order to keep their functional units busy. However, fetching more instructions per cycle can not be accomplished by replicating the fetch engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The FTB front-end</head><p>In order to increase fetch width, Yeh and Patt introduced a novel fetch architecture based on dynamic branch prediction <ref type="bibr" target="#b38">[39]</ref>. Figure <ref type="figure" target="#fig_9">2</ref>  The fetch engine is divided in two sections: the dynamic branch prediction mechanism, composed of the BTB, the return address stack (RAS), and the two-level branch predictor (BP); and the instruction cache. The BTB provides information about the basic block being fetched, and the BP predicts the behavior of its terminating branch. All the provided data is combined to calculate the fetch address for the next basic block. The same information drives the instruction cache to provide a cache line, and select the valid instructions from it.</p><p>The original design by Yeh and Patt <ref type="bibr" target="#b38">[39]</ref> had a single table for the BTB and branch predictor. Calder and Grunwald <ref type="bibr" target="#b2">[3]</ref> showed that decoupling the BTB and branch predictor allowed an independent resource allocation which leads to better branch prediction accuracy. They also suggested that basic blocks should not be terminated at branches which have never been taken, that is, that only taken branches should introduce a basic block in the BTB.</p><p>Further development of this fetch architecture leads to a decoupling of the dynamic branch prediction mechanism and the instruction cache access, as proposed by Reinman, Austin, and Calder <ref type="bibr" target="#b29">[30]</ref>. The branch prediction mechanism is a fully autonomous engine, capable of following a speculative path without further assistance. Each cycle it generates the fetch address for the next cycle, and a fetch request which is stored in a fetch target queue (FTQ). The instruction cache is then driven by the requests stored in the FTQ.</p><p>Another important contribution of <ref type="bibr" target="#b29">[30]</ref> is the Fetch Target Buffer (FTB). It extends the BTB by allowing the storage of variable length fetch blocks. A fetch block is a sequence of instructions starting at a branch target, and ending at a strongly biased taken branch. This allows strongly biased not taken branches to be embedded within a fetch block, increasing the fetch width without increasing the cost, as such not taken branches can be easily predicted by simply ignoring them.</p><p>Code layout optimizations benefit this fetch architecture in two ways: reducing the number of instruction cache misses, and increasing the effective fetch width. Because branches are aligned towards their not taken direction, it is likely that a branch which exhibits a biased behavior will be biased towards not taken. If a branch is always not taken, it is effectively ignored by the FTB architecture, and it never terminates a fetch block, enlarging the size of the fetch unit of the architecture.</p><p>The stream fetch architecture takes this advantage one step further. The FTB ignores biased not taken branches, but the next stream predictor ignores all not taken branch instances. For example: if a branch is 100% not taken, it will be ignored by both the FTB and the next stream predictor. If a branch is only 80% not taken, it will be ignored by the FTB until it is taken for the first time. After it has been taken once, it enters the FTB tables, and always terminates the fetch block.</p><p>The FTB architecture does not store overlapping fetch blocks. If a taken branch is found half-way through a fetch block, the fetch block is split in two smaller parts. Meanwhile, the stream fetch architecture allows for overlapping fetch blocks (streams), choosing the appropriate one fore each instance. This allows the stream predictor to ignore the branch in all its not taken instances, which allows a larger basic block to be fetched 80% of the time for than particular branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The trace cache</head><p>Fetching one basic block per cycle effectively limits the fetch performance to 5-6 instructions per cycle on most integer benchmarks. In order to feed an 8 or 16 wide superscalar processor, fetching multiple basic blocks per cycle becomes necessary.</p><p>The trace cache <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> is one such high fetch width mechanism, recently implemented in the Pentium4 processor <ref type="bibr" target="#b13">[14]</ref> <ref type="foot" target="#foot_0">1</ref> . Figure <ref type="figure" target="#fig_1">3</ref> shows a block diagram of the trace cache mechanism as proposed by Rotenberg, Benett and Smith in <ref type="bibr" target="#b31">[32]</ref>. The trace cache captures the dynamic instruction stream, and fragments it in smaller segments called traces. These traces are then stored in a special purpose cache (the trace cache), expecting that the same code sequence will be reexecuted in the future. If such is the case, it can be provided from the trace cache in a single cycle without need of further processing regardless of taken branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Next</head><p>A trace is defined by heuristics which limit either the number of instructions in a trace, the number of branches, the number of indirect branches, and such. In order to fully identify an instruction trace, it is necessary to know the starting instruction of the trace, and the number and behavior of the branches contained. A trace may contain multiple basic blocks, and several branches, regardless of them being taken or not taken.</p><p>The next trace predictor <ref type="bibr" target="#b16">[17]</ref> provides trace level sequencing. That is, the fetch engine steps through the code at the trace level granularity. However, when a branch misprediction is encountered, or the predicted trace is not found in the trace cache, instructions must be provided by a secondary fetch engine. The secondary fetch engine is composed by an instruction cache and a BTB for dynamic branch prediction.</p><p>There has been much work done to improve the trace cache mechanism. Techniques such as path associativity, partial matching, and inactive instruction issue <ref type="bibr" target="#b8">[9]</ref> target an improvement in the trace cache hit rate. Branch promotion <ref type="bibr" target="#b20">[21]</ref> targets an increase in the average trace length, allowing traces to contain more branches. The possibility of using the trace buffers to do dynamic code optimization has also been explored <ref type="bibr" target="#b9">[10]</ref>.</p><p>building a trace cache is not feasible, we just propose an easier alternative with similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Other high-performance fetch architectures</head><p>Aside from the widely adopted trace cache, there are other high performance fetch mechanisms, with varying degrees of complexity and performance. The branch address cache <ref type="bibr" target="#b37">[38]</ref>, and the collapsing buffer <ref type="bibr" target="#b6">[7]</ref> represent earlier attempts at a fetch architecture capable of fetching multiple non-sequential basic blocks in a single cycle. Both require multiple read ports to the instruction cache, a complex branch predictor, and a complicated realignment network to join the fetched blocks before passing them to the decode stage.</p><p>Other multiple branch predictors like the multiple blockahead predictor <ref type="bibr" target="#b34">[35]</ref>, or the tree-like subgraph predictor <ref type="bibr" target="#b7">[8]</ref> can also be used to implement a fetch engine capable of providing multiple basic blocks per cycle, but would also require a high complexity alignment network. The trace cache solves this problem by moving the alignment network out of the critical path, and storing the already aligned instructions in a special purpose cache.</p><p>The next line and set predictor (NLS) architecture <ref type="bibr" target="#b4">[5]</ref>, implemented in the Alpha 21264 <ref type="bibr" target="#b11">[12]</ref>, also allows fetching of multiple basic blocks in a single cycle, as long as they reside sequentially in the same cache line.</p><p>The Alpha EV8 architecture <ref type="bibr" target="#b33">[34]</ref> uses an interleaved BTB and a multiple branch predictor to fetch instructions from multiple basic blocks up to the first encountered taken branch, much in the way the SEQ.3 engine described in <ref type="bibr" target="#b30">[31]</ref>.</p><p>The rePLay microarchitecture <ref type="bibr" target="#b21">[22]</ref> uses a front end derived from the trace cache, making extensive use of the branch promotion technique to build very long instruction traces, called frames, and then dynamically optimize them. Opposite to streams, frames are defined as atomic regions. That is, if a branch misprediction is detected halfway through a frame, execution is backed up to the beginning of the frame, and execution proceeds again fetching instructions from the instruction cache instead, one basic block at a time.</p><p>The stream architecture uses the static approach to solve the same problems. Both the trace cache and rePLay read the dynamic instruction stream and record segments of it for increased fetch performance or dynamic optimization, and to avoid the alignment network required by other mechanisms. We rely on the compiler to organize the code so that the code segments that would be constructed by the trace cache/rePLay are already mapped sequentially in memory. This allows the stream architecture to use the instruction cache as the only source for instructions, avoiding any redundant and/or complex mechanisms.</p><p>Finally, superblocks bear a non-casual resemblance to instruction streams, and they are also composed of multiple sequentially executed basic blocks, and are also organized by the compiler. IBM holds a later patent on a branch predictor designed to fetch (and prefetch) superblocks <ref type="bibr" target="#b19">[20]</ref>. While our mechanism pursues a similar goal (fetching long sequences of sequential instructions), we achieve it in a different way. A detailed comparison between both can be found in <ref type="bibr" target="#b32">[33]</ref>.</p><p>A performance comparison between the stream fetch architecture and all other high-performance architectures is beyond the scope of this work. Instead we have chosen to compare with the state of the art regarding fetch architectures for sequential basic blocks (the FTB and the EV8 architectures), and the most widely adopted mechanism for non-sequential basic blocks (the trace cache).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Code layout optimizations</head><p>Previous work has explored the use of code layout optimizations to increase fetch performance both in BTB architectures and trace cache architectures. Code layout optimizations were initially proposed to improve the performance of the instruction memory hierarchy (instruction cache, instruction TLB) by reducing the code footprint and minimizing conflict misses <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>, and to align branches to benefit the underlying fetch architecture and branch predictor <ref type="bibr" target="#b3">[4]</ref>. Our previous work presents a detailed analysis <ref type="bibr" target="#b24">[25]</ref> of the effects of these optimizations, concluding that the improvements on the instruction cache performance are due to an increase in the sequential execution of code, and a better packing of useful code to cache lines.</p><p>We also show that layout optimizations not only improve instruction memory performance, but also have an impact on the branch prediction accuracy <ref type="bibr" target="#b26">[27]</ref>, and the effective fetch width of the front-end <ref type="bibr" target="#b25">[26]</ref>. The layout optimizations align branches towards not-taken, which translates in longer chains of sequential instructions being executed.</p><p>The use of code layout optimizations usually has the availability of profile data as a requisite, and experience shows that it is infrequent for end-users to do so. In those cases in which profile data is unavailable, it is still possible to benefit from code layout optimizations either using heuristics to replace the profile data <ref type="bibr" target="#b1">[2]</ref>, or dynamic code optimizers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>While our stream fetch architecture is designed to exploit the special characteristics of optimized codes, it does not exclusively depends on them. However, the use of such heuristics or dynamic optimizations would provide an additional fetch performance increase due to the synergy of the layout optimizations and our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The stream fetch architecture</head><p>Our previous work with the Software Trace Cache <ref type="bibr" target="#b25">[26]</ref> shows that a sequential fetch engine obtains a fetch performance similar to that of a trace cache when using layout optimized codes. The sequential engine used in that work is the SEQ.3 unit described in <ref type="bibr" target="#b30">[31]</ref>. However, such engine proves very complex to implement, and is still limited to 3 consecutive basic blocks per cycle. Next, we describe a fetch engine designed to fetch sequential code, up to a whole instruction stream per cycle, that overcomes the 3 basic block limitation without requiring more complexity than the classic BTB front-end engine.</p><p>Figure <ref type="figure" target="#fig_2">4</ref> shows the block diagram for the proposed stream fetch engine. The next stream predictor provides the fetch engine with stream level sequencing. That is, given the current stream starting address, it provides the current stream length, and the next stream starting address. The predicted next stream address is used as the fetch address for the next cycle. The current stream address, and the current stream length are stored in the fetch target queue (FTQ), and represent a fetch request for a full instruction stream.</p><p>The instruction cache is driven by the fetch requests stored in the FTQ. The stream starting address is used to access the instruction cache, which provides one or more consecutive cache lines. If the cache lines provided contain the whole stream, the FTQ is advanced to the next request, if not, the fetch request is updated to reflect the remaining part of the stream to be fetched.</p><p>As described, the stream fetch engine can be run as a single pipeline stage, or can be divided in several shorter stages using the different intermediate structures (FTQ and cache line buffer) to separate them. In the following sections we describe the different fetch sub-stages in detail.</p><p>The novel aspect of this fetch architecture is the use of a specialized branch predictor (the stream predictor) which provides stream level sequencing. Each prediction contains information about a whole instruction stream, possibly containing multiple basic blocks.</p><p>The use of an FTQ is not novel, it was introduced in <ref type="bibr" target="#b29">[30]</ref>. It decouples the branch prediction from the memory access, and stores information about the instruction sequence as indicated by the branch predictor providing a certain degree of tolerance to the branch predictor latency, and storing a glimpse of the future control flow of the program. In our case, the usefulness of the FTQ increases, as each entry now contains information about a whole instruction stream instead of just a fetch block.</p><p>Our fetch architecture has a single source for instructions (the instruction cache), which uses very wide lines to provide a high fetch width. Fetching streams does not require any additional instruction storage not special purpose caches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Complexity issues</head><p>Across al the paper we claim that the stream fetch architecture has a lower cost and complexity than a trace cache architecture, but quantifying the cost of both architectures to allow an objective comparison is beyond the scope of this work.  However, it is clear that the trace cache requires two separate instruction paths (a primary path for traces, and a secondary path when the trace cache misses), requiring additional storage, and redundant branch prediction.</p><p>The stream fetch architecture does not require additional instruction storage (the trace cache), nor a trace construction engine (streams are recorded, not built dynamically), nor two separate branch predictors (a trace predictor, and a back-up predictor for trace construction).</p><p>The trace cache uses a BTB as a back-up predictor in case of trace predictor misses, which makes it necessary to keep the BTB updated at any point, requiring an update for each executed branch. The BTB is the backup predictor for the trace cache, and is used on a trace predictor miss, or when a misprediction is detected half-way through a trace.</p><p>In the event of a stream predictor miss, we resort to sequential fetching, which does not require any back-up predictor.</p><p>Our stream fetch architecture has a single instruction path (as opposed to two in the trace cache), a single branch predictor, a single storage cache, and is fairly straightforward to implement, without need of much control logic to coordinate the different components, as they all work decoupled from each other by the FTQ.</p><p>In any case, we do not wish to disregard other advantages of the trace cache mechanism, like its ability to store decoded instructions for processors like the Pentium4 <ref type="bibr" target="#b13">[14]</ref>, or enabling dynamic optimization of traces <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. Our instruction stream fetch architecture is intended as another alternative for a high-performance fetch architecture when complexity and cost issues are to be involved, but does not completely replace the trace cache in all its possible uses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stream prediction</head><p>The stream fetch engine we propose is driven by a branch predictor providing stream level sequencing. Such is the purpose of the next stream predictor. Given a stream starting address, the predictor provides a stream identifier and the next stream starting address. The stream identifier consists of the stream starting address and the stream length.</p><p>The next stream predictor serves both the purpose of branch direction predictor and target address predictor, replacing both the conditional branch predictor and the BTB/FTB of a conventional fetch engine. The branch directions are predicted implicitly: all intermediate branches from the start address of the stream are predicted not taken, and the stream terminating branch is predicted taken. The target addresses for all not taken branches are the next sequential instruction, and the target address of the terminating branch is the starting address for the next stream.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows our implementation of the next stream predictor (a cascaded next stream predictor). Each table entry contains information about one stream: start address, length (in instructions or bytes in a variable-length ISA), terminating branch type (for return stack management), the next stream address, and a 2-bit saturating counter used for the replacement policy.</p><p>The first table is indexed using the current fetch address only, and the second table is indexed using a hash of the current fetch address and the previous stream starting addresses (the previous fetch addresses). The hash function uses a DOLC scheme similar to what was used in multiscalar processors <ref type="bibr" target="#b15">[16]</ref>.   The predictor maintains two separate path history registers: a lookup register which is updated immediately with speculative information, and an update register which is updated at commit time when the stream has completed, using correct path information only. In the case of a misprediction, the contents of the non-speculative register is copied to the speculative register, restoring the correct history state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Address indexed table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path indexed table</head><p>To improve the prediction accuracy of return instructions, we use a Return Address Stack (RAS). The RAS is updated speculatively as guided by the branch type field, and a shadow copy of the top of the stack is kept with each branch instruction. When a misprediction is detected, the stack index and the top of the stack are restored to their correct values.</p><p>To obtain a prediction, the stream predictor is presented with the current fetch address (the address where the new stream starts). The predictor calculates the hash function, and obtains an index into the prediction tables. If both tables have a hit, we choose the data from the path correlated table. If only one hits, we use whichever data is provided. If both tables miss, we resort to sequential fetching until the predictor hits again, or a branch misprediction is detected.</p><p>The confidence counter is used to implement the replacement policy. When a new stream is completed, the prediction tables are checked. If the stream is already there (the table is being updated with the same length and target address already stored in that entry) the confidence counter is increased. If the new stream data and the data stored in the table do not match (either the stream length or target address differ), the counter is decreased. When the counter reaches zero, the old data is replaced by the new data (both length and target are replaced) and the counter is set to one.</p><p>The use of path correlation and the hysteresis counter for replacement is what allows the stream predictor to hold overlapping streams in the prediction tables. This allows the streams to be kept as long as possible, without having to cut them short to avoid overlapping of multiple fetch blocks.</p><p>A stream is introduced in both tables the first time it appears. In following appearances of the stream, its information is updated only in those tables where it has not yet been replaced. Streams which do not require patch correlation for accurate prediction will be replaced from the second table, but the first table will still be able to predict them.</p><p>A stream present only in the first table will be upgraded to the second table if it is mispredicted. Streams not requiring path correlation will thus never be upgraded to the second table, avoiding aliasing.</p><p>Each access to the predictor provides information about a whole instruction stream, possibly containing multiple basic blocks as long as they are connected by not taken branches. This is how the stream front end engine takes advantage of the characteristics of layout optimized codes: an average of 80% of all conditional branch instances are not taken, while only 60% of all branches are strongly biased to not taken. By stepping through the code passing through not taken branches, we are able to fetch much longer code sequences than if we had to stop at all branches to predict them individually. And by ignoring not taken branch instances, we avoid interference in the prediction tables, which allows us to increase prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fetch target queue</head><p>Following the proposal of Reinman, Austin and Calder <ref type="bibr" target="#b29">[30]</ref> we have decoupled the branch prediction stage from the instruction cache access stage. The stream predictor provides information about an instruction stream with each prediction, the predictions are stored in a Fetch Target Queue (FTQ), and are used to drive the instruction cache access.</p><p>The FTQ does not serve any actual performance improvement purpose, the branch prediction and instruction cache access could be done in parallel and avoid the use of an FTQ and the additional pipeline stage. We have used it because it allows the branch predictor to work at a different rate than the instruction cache. It is likely that a single FTQ entry will take multiple cycles to fetch from the instruction cache (long basic blocks, or streams containing multiple basic blocks). This allows the branch predictor to run ahead of the instruction cache, and to stall (and avoid power consumption) when the FTQ is full <ref type="foot" target="#foot_1">2</ref> .</p><p>The usefulness of the FTQ with our stream front-end is higher than that of a basic block architecture, because each FTQ entry contains information about more instructions, which allows the front-end to have a larger view of the future instruction stream.</p><p>The average stream contains over 16 instructions, which means that the average fetch request stored in the FTQ is too large to be fetched in a single cycle. Instead of dividing a large fetch request into several smaller ones, we have implemented a fetch request update mechanism as shown in Figure <ref type="figure" target="#fig_5">6</ref>.</p><p>Start @ Length effective fetch width Each fetch request contains the stream starting address and the stream length. The starting address is used to access the instruction cache and fetch one or more consecutive cache lines. Depending on how many cache lines were fetched, and the cache line width, a number of instructions belonging to the stream will be fetched.</p><p>The fetch request is updated using the actual number of instructions obtained from the instruction cache. The stream starting address is advanced, and the stream length is reduced appropriately. If the stream length reaches zero, then the fetch request has been satisfied, and the FTQ is advanced to the next request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Instruction cache</head><p>Instruction streams are composed of sequential instructions. The easiest way to fetch an instruction stream is to read multiple consecutive cache lines from the instruction cache until the whole stream has been fetched. However, fetching a large number of cache lines in a single cycle is not always feasible, nor cost-effective.</p><p>The simplest fetch mechanism would be to fetch a single cache line per cycle. In this scenario we must face the problem of instruction misalignment, as shown in Figure <ref type="figure" target="#fig_7">7</ref>.  A fetch request consisting of 3 consecutive instructions should be fetched in a single cycle by a 4-wide fetch engine, but such is not always the case. It is possible that the 3 instruction stream is split across two separate cache lines, which means that it will take two cycles to fetch if we fetch a single cache line per cycle.</p><p>The use of long instruction cache lines alleviates this problem. A longer cache line reduces the possibilities of the instruction stream crossing the cache line boundary.</p><p>Also, previous work shows that layout optimized codes benefit from long cache lines more than unoptimized codes due to a denser packing of useful instructions to cache lines. These results show that even a very long 128-byte line (32 instructions) is usually fully used before being replaced <ref type="bibr" target="#b24">[25]</ref>.</p><p>Considering both benefits together (the reduced stream misalignment, and the instruction cache miss rate benefits), we have adopted a simple instruction cache design which reads a single line per cycle, but we use a very long line size.</p><p>An alternate solution would be to fetch two consecutive cache lines from a multi-banked instruction cache, so that we can always guarantee a full width of instructions, as done in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. Our solution requires a wider read port to the instruction cache, while this solution requires an interchange network which increases complexity. For the remaining of this paper we will use the wide cache line approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Performance evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Simulation setup</head><p>The results in this paper were generated using trace driven simulation of a superscalar processor. Our simulator uses a static basic block dictionary to allow simulating the effect of wrong path execution. This model includes the simulation of wrong speculative predictor history updates, as well as the possible interference and prefetching effects on the instruction cache.</p><p>We compare our stream fetch architecture with three other state-of-the-art fetch architectures: the FTB architecture <ref type="bibr" target="#b29">[30]</ref> using a perceptron branch predictor <ref type="bibr" target="#b17">[18]</ref>, the Alpha EV8 architecture using a 2bcgskew predictor <ref type="bibr" target="#b33">[34]</ref>, and the trace cache architecture using a trace predictor <ref type="bibr" target="#b31">[32]</ref> and selective trace storage <ref type="bibr" target="#b28">[29]</ref>.</p><p>Table <ref type="table" target="#tab_5">2</ref> shows the values used in the processor simulation. Most of the setups correspond to the fetch engine, which was simulated in greater detail.</p><p>In all cases in which the branch predictor requires two separate mechanisms (branch predictor and target address predictor) we have simulated multiple resource allocations maintaining a total approximate budget of 45KB, and include here only the one which provided better performance.</p><p>The trace cache processor uses a 32KB trace cache (instruction storage only), and a 32KB instruction cache, both 2-way set associative. We tested multiple combinations of instruction and trace cache sizes (big trace cache with small instruction cache and vice versa), and multiple secondary fetch paths (multiple block instruction cache with a multiple branch predictor, single block instruction cache with a classic branch predictor), and selected the one which provided better performance. For the trace cache, we use only the trace packing and selective trace storage optimizations <ref type="foot" target="#foot_2">3</ref> .</p><p>The results shown are the harmonic average of all SPEC 2000 integer benchmarks. We feed our simulator with traces of 300 million instructions collected using the ref  input set. To find the most representative instruction segment we have analyzed the distribution of basic blocks as described in <ref type="bibr" target="#b35">[36]</ref>.</p><p>Previous has shown that code layout optimizations have a very important effect on all aspects of fetch performance. For this reason we use two separate sets of executables: the baseline set, and a layout optimized set. The layout optimized codes were generated with the spike tool shipped with Compaq Tru64 Unix 5.1 <ref type="bibr" target="#b5">[6]</ref>. The profile data was obtained using pixie and the train input set. As mentioned above, we use the ref input set to obtain simulation results. It is important to note that we used both optimized and unoptimized codes with all fetch architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Processor performance</head><p>In Section 3 we have described the stream fetch architecture: a low cost, low complexity fetch engine designed to exploit the characteristics of optimized code layouts. As we already mentioned in the introduction, a fetch engine is better than others if it has a lower cost, requires fewer resources, or has a lower energy consumption. However, raw performance is still a very important metric when designing a high performance processor. ferent fetch architectures varying the pipeline width, using both baseline and optimized code layouts.</p><p>The results for the 2-wide processor show that indeed there is very little performance advantage in increasing fetch performance beyond what the back-end can consume. In this narrow pipeline all setups obtain a similar performance, with the streams and the trace cache being only 2% better than the EV8 architecture.</p><p>Given this circumstance, the best option would be the one which obtains that performance at the minimum cost and complexity, or consumes the less power. Our fetch architecture has a slightly higher performance, and does not introduce additional costs and complexity, which make it the more suitable choice.</p><p>The 4-wide pipeline already shows significant performance differences, as the fetch width and branch prediction accuracy become more important. The results show that the use of code layout optimizations is beneficial to all architectures, but it is the stream fetch architecture which benefits most from them.</p><p>Using optimized codes, the stream fetch architecture obtains a 5% speedup compared to the EV8 architecture, and a 3% speedup agains the FTB, while being only 2% slower than the trace cache architecture.</p><p>Even in the absence of layout optimized codes the stream fetch architecture still obtains a 4% speedup against the EV8 architecture, and a 1% speedup against the FTB. That is, the streams architecture using baseline codes obtains the same performance as the FTB architecture using optimized applications. Also, it is worth noting that the combination of optimized applications and the stream fetch architecture obtains better performance than the trace cache using unoptimized codes (this result is also visible in the 2-wide pipeline). The results in Table <ref type="table" target="#tab_6">3</ref> will show that this is due to the higher prediction accuracy of the stream predictor in an environment where the fetch width is still not the most relevant factor. The 8-wide pipeline shows the larger performance differences, because in this wide pipeline setup, the fetch width becomes a more relevant aspect, as fetching instructions from multiple basic blocks per cycle becomes imperative.</p><p>When using unoptimized codes, both the FTB and stream fetch architectures provide an intermediate performance between the EV8 and the trace cache architectures (10% faster than EV8, and only 4-5% slower than the trace cache).</p><p>However, the use of code layout optimizations uncovers the potential performance of instruction streams. The stream fetch architecture is designed to exploit optimized codes, and does so successfully, increasing performance by a full 3%, while the FTB and the trace cache improve less than 1%.</p><p>When using code layout optimizations, the stream fetch architecture obtains a 4% speedup over the FTB, and is only 1.5% slower than the trace cache, while requiring a lower cost and complexity.</p><p>Figure <ref type="figure">9</ref> shows individual benchmark results for the 8wide processor in the presence of code layout optimizations. The results show that the stream fetch architecture obtains the best performance in 5 benchmarks, (176, 186, 254, 255, and 256) and is at least second best is all but one case (175) where it is third best. Comparing only streams vs. traces performance, the trace cache is better in 6 codes, and the streams are better in 5 codes. </p><note type="other">IPC EV8+2bcgskew FTB+perceptron Streams Tcache+Tpred Figure 9</note><p>. Individual IPC results for the 8-wide processor using optimized codes.  The results show that the main advantage of the trace cache over the other fetch architectures is the increased fetch width, obtained fetching instructions from multiple non-sequential basic blocks in a single cycle. This fetch width increase means that the trace cache provides 25-30% more instructions per cycle than the EV8 or FTB architectures. However, the stream fetch architecture does not lag behind, providing only 11-15% fewer instructions per cycle than the trace cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fetch performance</head><p>However, an increased fetch width is not the only advantage of the stream architecture. Our results also show that instruction streams also provide for increased prediction accuracy, with a misprediction rate 18% lower than that of the perceptron or trace predictors, and 36-40% lower than the 2bcgskew miss rate.</p><p>Instruction streams seem to be inherently easier to predict. The same seems to hold true for fetch blocks (FTB units) and traces (trace predictor units). As the prediction unit size increases, fewer predictions are required, which means that there is less pressure and interference in the pre-diction tables. This would point the advantage to traces and streams, but traces are built using hardware-derived heuristics, and result in a somewhat random splitting of code. Meanwhile, streams are defined by the natural control flow of the program, retaining their relationship to high-level programming constructs, which makes them easier to predict.</p><p>The combination of increased fetch width and improved branch prediction accuracy explain the IPC improvements obtained fetching instruction streams. The fact that the trace cache provides only 11-15% more instructions per fetch than the stream architecture, and that the trace predictor has a misprediction rate 17% higher than the stream predictor show why the stream architecture comes so close to the trace cache performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper presents the instruction stream fetch architecture, our first step towards an architecture which ties the internal processing units to the high-level programming language structures.</p><p>Instruction streams are sequences of sequential instructions between two consecutive taken branches. As such, streams contain instructions from multiple basic blocks, but still maintain their relationship with high-level structures such as loop bodies. Their large size and their high-level information make them suitable for a low complexity, but high performance fetch engine.</p><p>Our fetch architecture replaces the conventional branch predictor with a next stream predictor, which steps through the code at a larger granularity (that of instruction streams), providing higher prediction accuracy and larger fetch blocks. Because streams are sequentially stored in memory, we use the instruction cache as our only source of instructions, avoiding redundant mechanisms and additional complexity.</p><p>Our results show that for narrow issue processors, the stream architecture offers higher performance than other state-of-the-art architectures, and a lower complexity than a trace cache. On wide issue processors, the use of code layout optimizations and our fetch architecture obtains 10% higher performance than the EV8 fetch architecture, and 4% higher than the FTB architecture using state-of-the-art branch predictors, while being only 1.5% slower than the trace cache.</p><p>When not using optimized applications, fetching instruction streams is still 10% faster than the EV8 fetch architecture, and only 4% slower than fetching traces, and always at a lower cost and complexity.</p><p>If performance is the only relevant factor, then the trace cache is still probably the best option. If implementation cost and complexity are also taken into account, we believe that our stream fetch architecture provides a better alternative.</p><p>Future lines of research include improving the accuracy of the next stream predictor, devising better ways to explode the characteristics of optimized codes, and further development of the instruction stream architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example instruction streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The trace cache fetch architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The proposed stream fetch engine. The instruction stream becomes the basic fetch entity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Cascaded implementation of the next stream predictor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Fetch target queue update mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The instruction misalignment problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 Figure 8 .</head><label>88</label><figDesc>Figure 8. IPC performance for multiple pipeline widths using base and optimized codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>H m e a n g z i p v p r g c c c r a f t y p a r s e r e o n p e r l b m k g a p v o r t e x b z i p 2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . A comparison of fetch engines in terms of their relation to high-level code, cost, and performance.</head><label>1</label><figDesc></figDesc><table><row><cell>Fetch unit</cell><cell>high</cell><cell>size</cell><cell>cost</cell><cell>complex.</cell><cell>perform.</cell></row><row><cell></cell><cell>level</cell><cell>(inst.)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Basic block</cell><cell>yes</cell><cell>5-6</cell><cell>low</cell><cell>avg.</cell><cell>low</cell></row><row><cell>Trace cache</cell><cell>no</cell><cell>14</cell><cell>high</cell><cell>high</cell><cell>high</cell></row><row><cell>rePLay</cell><cell>no</cell><cell>60-100</cell><cell>high</cell><cell>high</cell><cell>high</cell></row><row><cell>Streams</cell><cell>yes</cell><cell>20+</cell><cell>low</cell><cell>low</cell><cell>high</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Figure 2. The BTB fetch architecture uses dy- namic branch prediction to fetch a full basic block per cycle.</head><label></label><figDesc>shows a block diagram of this fetch architecture.</figDesc><table><row><cell>RAS Branch Pred. BTB</cell><cell>FTQ</cell><cell>Instruction Cache</cell><cell>Rotate &amp; Select</cell></row><row><cell cols="2">Fetch request generation</cell><cell>Instruction cache access</cell><cell>Instruction fetch</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 . Processor setup simulated.</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>shows other performance metrics which determine the fetch engine performance: the actual fetch width on the 8-wide processor, and the branch misprediction rate. The instruction cache miss rate is very low in all setups, and hardly contributes to the IPC performance difference.</figDesc><table><row><cell></cell><cell></cell><cell>base</cell><cell cols="2">optimized</cell></row><row><cell></cell><cell>Mispred.</cell><cell>Fetch IPC</cell><cell>Mispred.</cell><cell>Fetch IPC</cell></row><row><cell>EV8+2bcgskew</cell><cell>3.9%</cell><cell>5.3</cell><cell>3.9%</cell><cell>5.5</cell></row><row><cell>FTB+perceptron</cell><cell>3.0%</cell><cell>5.5</cell><cell>2.8%</cell><cell>5.5</cell></row><row><cell>Streams</cell><cell>2.5%</cell><cell>5.8</cell><cell>2.3%</cell><cell>6.0</cell></row><row><cell>Traces</cell><cell>2.8%</cell><cell>6.8</cell><cell>2.8%</cell><cell>6.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 . Branch misprediction rate and Fetch IPC for the 8-wide processor.</head><label>3</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">This shows that regardless of its high cost and complexity, the trace cache mechanism is indeed implementable. Our work does not suggest that</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">This is not the case of the trace cache, where each trace prediction corresponds to a single trace cache access. In the trace cache setup, if the fetched trace can feed the processor for more than one cycle, both the predictor and the trace cache stall at the same time.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Our results show that in the context of code layout optimizations, the partial matching optimization actually causes a drop in trace cache performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Ministry of Education and Science of Spain under contract TIC-2001-0995-C02-01, Generalitat de Catalunya under grant 2001FI-00724-APTIND, CEPBA, and an Intel Fellowship. Thanks go to Ayose Falcon for all his inputs, and his work on the simulation tool. The authors also want to thank the anonymous reviewers for their constructive inputs, and specially Brad Calder for his help in writing the final version of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamo: A transparent dynamic optimization system</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duesterwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN Conf. on Programming Language Design and Implementation</title>
				<meeting>ACM SIGPLAN Conf. on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Branch prediction for free</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN Conf. on Programming Language Design and Implementation</title>
				<meeting>ACM SIGPLAN Conf. on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
			<biblScope unit="page" from="300" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast &amp; accurate instruction fetch and branch prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 21st Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reducing branch costs via branch alignment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Intl. Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 6th Intl. Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1994-10">Oct. 1994</date>
			<biblScope unit="page" from="242" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Next cache line and set prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22th Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 22th Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spike: an optimizer for alpha/nt executables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997-08">Aug. 1997</date>
			<publisher>USENIX</publisher>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimization of instruction fetch mechanism for high issue rates</title>
		<author>
			<persName><forename type="first">T</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Patell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22th Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 22th Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="333" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Control flow prediction with treelike subgraphs for superscalar processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual ACM/IEEE Intl. Symposium on Microarchitecture</title>
				<meeting>the 28th Annual ACM/IEEE Intl. Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1995-11">Nov. 1995</date>
			<biblScope unit="page" from="258" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Alternative fetch and issue techniques from the trace cache mechanism. Proceedings of the 30th</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Friendly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual ACM/IEEE Intl. Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Putting the fill unit to work: Dynamic optimization for trace cache microprocessors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Friendly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual ACM/IEEE Intl. Symposium on Microarchitecture</title>
				<meeting>the 31st Annual ACM/IEEE Intl. Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Procedure placement using temporal ordering information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual ACM/IEEE Intl. Symposium on Microarchitecture</title>
				<meeting>the 30th Annual ACM/IEEE Intl. Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997-12">Dec. 1997</date>
			<biblScope unit="page" from="303" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Digital 21264 sets new standard</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gwennap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-10">Oct. 1996</date>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient procedure mapping using cache line coloring</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIG-PLAN Conf. on Programming Language Design and Implementation</title>
				<meeting>ACM SIG-PLAN Conf. on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The microarchitecture of the pentium 4 processor</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roussel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Achieving high instruction cache performance with an optimizing compiler</title>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 16th Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
			<biblScope unit="page" from="242" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Control flow speculation in multiscalar processors</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sharms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Intl. Conference on High Performance Computer Architecture</title>
				<meeting>the 3rd Intl. Conference on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997-02">Feb. 1997</date>
			<biblScope unit="page" from="218" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Path-based next trace prediction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual ACM/IEEE Intl. Symposium on Microarchitecture</title>
				<meeting>the 30th Annual ACM/IEEE Intl. Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic branch prediction with perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Intl. Conference on High Performance Computer Architecture</title>
				<meeting>the 7th Intl. Conference on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A hardware-driven profiling scheme for identifying program hot spots to support runtime optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Merten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Trick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gyllenhaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mei Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 26th Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="136" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Method and apparatus for prefetching superblocks in a computer processing system</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="962" to="B963" />
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving trace cache effectiveness with branch promotion and trace packing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 25th Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="262" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Increasing the size of atomic instruction blocks using control flow assertions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Crum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM/IEEE Intl. Symposium on Microarchitecture</title>
				<meeting>the 33rd Annual ACM/IEEE Intl. Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2000-12">Dec. 2000</date>
			<biblScope unit="page" from="303" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dynamic flow instruction cache memory organized around trace segments independent of virtual address line</title>
		<author>
			<persName><forename type="first">A</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995-01">Jan. 1995</date>
			<pubPlace>U.S</pubPlace>
		</imprint>
	</monogr>
	<note>Patent Number 5.381.533</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Profile guided code positioning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pettis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN Conf. on Programming Language Design and Implementation</title>
				<meeting>ACM SIGPLAN Conf. on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="1990-06">June 1990</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Code layout optimizations for transaction processing workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lawney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 28th Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-07">July 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Software trace cache</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Intl. Conference on Supercomputing</title>
				<meeting>the 13th Intl. Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The effect of code reordering on branch prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intl. Conference on Parallel Architectures and Compilation Techniques</title>
				<meeting>the Intl. Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A stream processor fron-end</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCCA Newsletter</title>
		<imprint>
			<biblScope unit="page" from="10" to="13" />
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Trace cache redundancy: Red &amp; blue traces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Intl. Conference on High Performance Computer Architecture</title>
				<meeting>the 6th Intl. Conference on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A scalable frontend architecture for fast instruction delivery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 26th Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="234" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Trace cache: a low latency approach to high bandwidth instruction fetching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual ACM/IEEE Intl. Symposium on Microarchitecture</title>
				<meeting>the 29th Annual ACM/IEEE Intl. Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1996-12">Dec. 1996</date>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A trace cache microarchitecture and evaluation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, Special Issue on Cache Memory</title>
		<imprint>
			<date type="published" when="1999-02">Feb. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Differences between the next stream predictor and the apparatus for prefetching superblocks described in us patent 6,304,962 b1</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<idno>UPC-DAC-2002- 18</idno>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
		<respStmt>
			<orgName>Universitat Politecnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Design tradeoffs for the alpha ev8 conditional branch predictor. Proceedings of the 29th Annual Intl</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazeides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multipleblock ahead branch predictors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sainrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Intl. Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 7th Intl. Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1996-10">Oct. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Basic block distribution analysis to find periodic behavior and simulation points in applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intl. Conference on Parallel Architectures and Compilation Techniques</title>
				<meeting>the Intl. Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimizing instruction cache performance for operating system intensive workloads</title>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daigle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Intl. Conference on High Performance Computer Architecture</title>
				<meeting>the 1st Intl. Conference on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995-01">Jan. 1995</date>
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Increasing the instruction fetch rate via multiple branch prediction and a branch address cache</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Intl. Conference on Supercomputing</title>
				<meeting>the 7th Intl. Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1993-07">July 1993</date>
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comprehensive instruction fetch mechanism for a processor supporting speculative execution</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual ACM/IEEE Intl. Symposium on Microarchitecture</title>
				<meeting>the 25th Annual ACM/IEEE Intl. Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1992-12">Dec. 1992</date>
			<biblScope unit="page" from="129" to="139" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
