<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Simple Siamese Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-11-20">20 Nov 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
						</author>
						<title level="a" type="main">Exploring Simple Siamese Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-11-20">20 Nov 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2011.10566v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently there has been steady progress in un-/selfsupervised representation learning, with encouraging results on multiple visual tasks (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref>). Despite various original motivations, these methods generally involve certain forms of Siamese networks <ref type="bibr" target="#b3">[4]</ref>. Siamese networks are weight-sharing neural networks applied on two or more inputs. They are natural tools for comparing (including but not limited to "contrasting") entities. Recent methods define the inputs as two augmentations of one image, and maximize the similarity subject to different conditions.</p><p>An undesired trivial solution to Siamese networks is all outputs "collapsing" to a constant. There have been several general strategies for preventing Siamese networks from collapsing. Contrastive learning <ref type="bibr" target="#b15">[16]</ref>, e.g., instantiated in SimCLR <ref type="bibr" target="#b7">[8]</ref>, repulses different images (negative pairs) while attracting the same image's two views (positive pairs). The negative pairs preclude constant outputs from the solution space. Clustering <ref type="bibr" target="#b4">[5]</ref> is another way of avoiding constant output, and SwAV <ref type="bibr" target="#b6">[7]</ref> incorporates online clustering into Siamese networks. Beyond contrastive learning and Two augmented views of one image are processed by the same encoder network f (a backbone plus a projection MLP). Then a prediction MLP h is applied on one side, and a stop-gradient operation is applied on the other side. The model maximizes the similarity between both sides. It uses neither negative pairs nor a momentum encoder.</p><p>clustering, BYOL <ref type="bibr" target="#b14">[15]</ref> relies only on positive pairs but it does not collapse in case a momentum encoder is used.</p><p>In this paper, we report that simple Siamese networks can work surprisingly well with none of the above strategies for preventing collapsing. Our model directly maximizes the similarity of one image's two views, using neither negative pairs nor a momentum encoder. It works with typical batch sizes and does not rely on large-batch training. We illustrate this "SimSiam" method in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>Thanks to the conceptual simplicity, SimSiam can serve as a hub that relates several existing methods. In a nutshell, our method can be thought of as "BYOL without the momentum encoder". Unlike BYOL but like SimCLR and SwAV, our method directly shares the weights between the two branches, so it can also be thought of as "SimCLR without negative pairs", and "SwAV without online clustering". Interestingly, SimSiam is related to each method by removing one of its core components. Even so, SimSiam does not cause collapsing and can perform competitively.</p><p>We empirically show that collapsing solutions do exist, but a stop-gradient operation (Figure <ref type="figure" target="#fig_1">1</ref>) is critical to prevent such solutions. The importance of stop-gradient suggests that there should be a different underlying optimization problem that is being solved. We hypothesize that there are implicitly two sets of variables, and SimSiam behaves like alternating between optimizing each set. We provide proof-of-concept experiments to verify this hypothesis.</p><p>Our simple baseline suggests that the Siamese architectures can be an essential reason for the common success of the related methods. Siamese networks can naturally introduce inductive biases for modeling invariance, as by definition "invariance" means that two observations of the same concept should produce the same outputs. Analogous to convolutions <ref type="bibr" target="#b24">[25]</ref>, which is a successful inductive bias via weight-sharing for modeling translation-invariance, the weight-sharing Siamese networks can model invariance w.r.t. more complicated transformations (e.g., augmentations). We hope our exploration will motivate people to rethink the fundamental roles of Siamese architectures for unsupervised representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Siamese networks. Siamese networks <ref type="bibr" target="#b3">[4]</ref> are general models for comparing entities. Their applications include signature <ref type="bibr" target="#b3">[4]</ref> and face <ref type="bibr" target="#b33">[34]</ref> verification, tracking <ref type="bibr" target="#b2">[3]</ref>, one-shot learning <ref type="bibr" target="#b22">[23]</ref>, and others. In conventional use cases, the inputs to Siamese networks are from different images, and the comparability is determined by supervision.</p><p>Contrastive learning. The core idea of contrastive learning <ref type="bibr" target="#b15">[16]</ref> is to attract the positive sample pairs and repulse the negative sample pairs. This methodology has been recently popularized for un-/self-supervised representation learning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Simple and effective instantiations of contrastive learning have been developed using Siamese networks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In practice, contrastive learning methods benefit from a large number of negative samples <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8]</ref>. These samples can be maintained in a memory bank <ref type="bibr" target="#b35">[36]</ref>. In a Siamese network, MoCo <ref type="bibr" target="#b16">[17]</ref> maintains a queue of negative samples and turns one branch into a momentum encoder to improve consistency of the queue. SimCLR <ref type="bibr" target="#b7">[8]</ref> directly uses negative samples coexisting in the current batch, and it requires a large batch size to work well.</p><p>Clustering. Another category of methods for unsupervised representation learning are based on clustering <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>They alternate between clustering the representations and learning to predict the cluster assignment. SwAV <ref type="bibr" target="#b6">[7]</ref> incorporates clustering into a Siamese network, by computing the assignment from one view and predicting it from another view. SwAV performs online clustering under a balanced partition constraint for each batch, which is solved by the Sinkhorn-Knopp transform <ref type="bibr" target="#b9">[10]</ref>.</p><p>While clustering-based methods do not define negative exemplars, the cluster centers can play as negative prototypes. Like contrastive learning, clustering-based methods require either a memory bank <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1]</ref>, large batches <ref type="bibr" target="#b6">[7]</ref>, or a queue <ref type="bibr" target="#b6">[7]</ref> to provide enough samples for clustering. BYOL. BYOL <ref type="bibr" target="#b14">[15]</ref> directly predicts the output of one view from another view. It is a Siamese network in which one branch is a momentum encoder. <ref type="foot" target="#foot_0">1</ref> It is hypothesized in <ref type="bibr" target="#b14">[15]</ref> that the momentum encoder is important for BYOL to avoid collapsing, and it reports failure results if removing the momentum encoder (0.3% accuracy, Table <ref type="table" target="#tab_8">5</ref> in <ref type="bibr" target="#b14">[15]</ref>). <ref type="foot" target="#foot_1">2</ref> Our empirical study challenges the necessity of the momentum encoder for preventing collapsing. We discover that the stop-gradient operation is critical. This discovery can be obscured with the usage of a momentum encoder, which is always accompanied with stop-gradient (as it is not updated by its parameters' gradients). While the moving-average behavior may improve accuracy with an appropriate momentum coefficient, our experiments show that it is not directly related to preventing collapsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our architecture (Figure <ref type="figure" target="#fig_1">1</ref>) takes as input two randomly augmented views x 1 and x 2 from an image x. The two views are processed by an encoder network f consisting of a backbone (e.g., ResNet <ref type="bibr" target="#b18">[19]</ref>) and a projection MLP head <ref type="bibr" target="#b7">[8]</ref>. The encoder f shares weights between the two views. A prediction MLP head <ref type="bibr" target="#b14">[15]</ref>, denoted as h, transforms the output of one view and matches it to the other view. Denoting the two output vectors as p 1 h(f (x 1 )) and z 2 f (x 2 ), we minimize their negative cosine similarity:</p><formula xml:id="formula_0">D(p 1 , z 2 ) = − p 1 p 1 2 • z 2 z 2 2 ,<label>(1)</label></formula><p>where • 2 is 2 -norm. This is equivalent to the mean squared error of 2 -normalized vectors <ref type="bibr" target="#b14">[15]</ref>, up to a scale   <ref type="bibr" target="#b35">[36]</ref> as a monitor of progress. Table <ref type="table">:</ref> ImageNet linear evaluation ("w/ stop-grad" is mean±std over 5 trials).</p><p>of 2. Following <ref type="bibr" target="#b14">[15]</ref>, we define a symmetrized loss as:</p><formula xml:id="formula_1">L = 1 2 D(p 1 , z 2 ) + 1 2 D(p 2 , z 1 ).<label>(2)</label></formula><p>This is defined for each image, and the total loss is averaged over all images. Its minimum possible value is −1.</p><p>An important component for our method to work is a stop-gradient (stopgrad) operation (Figure <ref type="figure" target="#fig_1">1</ref>). We implement it by modifying (1) as:</p><formula xml:id="formula_2">D(p 1 , stopgrad(z 2 )).<label>(3)</label></formula><p>This means that z 2 is treated as a constant in this term. Similarly, the form in ( <ref type="formula" target="#formula_1">2</ref>) is implemented as:</p><formula xml:id="formula_3">L= 1 2 D(p 1 , stopgrad(z 2 ))+ 1 2 D(p 2 , stopgrad(z 1 )).<label>(4</label></formula><p>) Here the encoder on x 2 receives no gradient from z 2 in the first term, but it receives gradients from p 2 in the second term (and vice versa for x 1 ).</p><p>The pseudo-code of SimSiam is in Algorithm 1.</p><p>Baseline settings. Unless specified, our explorations use the following settings for unsupervised pre-training:</p><p>• Optimizer. We use SGD for pre-training. Our method does not require a large-batch optimizer such as LARS <ref type="bibr" target="#b37">[38]</ref> (unlike <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref>). We use a learning rate of lr×BatchSize/256 (linear scaling <ref type="bibr" target="#b13">[14]</ref>), with a base lr = 0.05. The learning rate has a cosine decay schedule <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8]</ref>. The weight decay is 0.0001 and the SGD momentum is 0.9.</p><p>The batch size is 512 by default, which is friendly to typical 8-GPU implementations. Other batch sizes also work well (Sec. 4.3). We use batch normalization (BN) <ref type="bibr" target="#b21">[22]</ref> synchronized across devices, following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Study</head><p>In this section we empirically study the SimSiam behaviors. We pay special attention to what may contribute to the model's non-collapsing solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Stop-gradient</head><p>Figure <ref type="figure">2</ref> presents a comparison on "with vs. without stop-gradient". The architectures and all hyper-parameters are kept unchanged, and stop-gradient is the only difference.</p><p>Figure <ref type="figure">2</ref> (left) shows the training loss. Without stopgradient, the optimizer quickly finds a degenerated solution and reaches the minimum possible loss of −1. To show that the degeneration is caused by collapsing, we study the standard deviation (std) of the 2 -normalized output z/ z 2 . If the outputs collapse to a constant vector, their std over all samples should be zero for each channel. This can be observed from the red curve in Figure <ref type="figure">2</ref> (middle).</p><p>As a comparison, if the output z has a zero-mean isotropic Gaussian distribution, we can show that the std of z/ z 2 is 1 √ d . 3 The blue curve in Figure <ref type="figure">2</ref> (middle) shows 3 Here is an informal derivation: denote z/ z 2 as z , that is,</p><formula xml:id="formula_4">z i = z i /( d j=1 z 2 j ) 1 2</formula><p>for the i-th channel. If z j is subject to an i.i.d Gaussian distribution: z j ∼ N (0, 1), ∀j, then z i ≈ z i /d that with stop-gradient, the std value is near 1 √ d . This indicates that the outputs do not collapse, and they are scattered on the unit hypersphere.</p><p>Figure <ref type="figure">2</ref> (right) plots the validation accuracy of a knearest-neighbor (kNN) classifier <ref type="bibr" target="#b35">[36]</ref>. This kNN classifier can serve as a monitor of the progress. With stop-gradient, the kNN monitor shows a steadily improving accuracy.</p><p>The linear evaluation result is in the table in Figure <ref type="figure">2</ref>. SimSiam achieves a nontrivial accuracy of 67.7%. This result is reasonably stable as shown by the std of 5 trials. Solely removing stop-gradient, the accuracy becomes 0.1%, which is the chance-level guess in ImageNet.</p><p>Discussion. Our experiments show that there exist collapsing solutions. The collapse can be observed by the minimum possible loss and the constant outputs. <ref type="foot" target="#foot_3">4</ref> The existence of the collapsing solutions implies that it is insufficient for our method to prevent collapsing solely by the architecture designs (e.g., predictor, BN, 2 -norm). In our comparison, all these architecture designs are kept unchanged, but they do not prevent collapsing if stop-gradient is removed.</p><p>The introduction of stop-gradient implies that there should be another optimization problem that is being solved underlying. We propose a hypothesis in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Predictor</head><p>In Table <ref type="table" target="#tab_2">1</ref> we study the predictor MLP's effect. The model does not work if removing h (Table <ref type="table" target="#tab_2">1a</ref>), i.e., h is the identity mapping. Actually, this observation can be expected if the symmetric loss (4) is used. Now the loss is 1  2</p><formula xml:id="formula_5">D(z 1 , stopgrad(z 2 )) + 1 2 D(z 2 , stopgrad(z 1 )</formula><p>). Its gradient has the same direction as the gradient of D(z 1 , z 2 ), with the magnitude scaled by 1/2. In this case, using stopgradient is equivalent to removing stop-gradient and scaling the loss by 1/2. Collapsing is observed (Table <ref type="table" target="#tab_2">1a</ref>).</p><p>We note that this derivation on the gradient direction is valid only for the symmetrized loss. <ref type="bibr">But</ref>  collapsing. The training does not converge, and the loss remains high. The predictor h should be trained to adapt to the representations.</p><p>We also find that h with a constant lr (without decay) can work well and produce even better results than the baseline (Table <ref type="table" target="#tab_2">1c</ref>). A possible explanation is that h should adapt to the latest representations, so it is not necessary to force it converge (by reducing lr) before the representations are sufficiently trained. In many variants of our model, we have observed that h with a constant lr provides slightly better results. We use this form in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Batch Size</head><p>Table <ref type="table" target="#tab_3">2</ref> reports the results with a batch size from 64 to 4096. When the batch size changes, we use the same linear scaling rule (lr×BatchSize/256) <ref type="bibr" target="#b13">[14]</ref> with base lr = 0.05. We use 10 epochs of warm-up <ref type="bibr" target="#b13">[14]</ref> for batch sizes ≥ 1024. Note that we keep using the same SGD optimizer (rather than LARS <ref type="bibr" target="#b37">[38]</ref>) for all batch sizes studied.</p><p>Our method works reasonably well over this wide range of batch sizes. Even a batch size of 128 or 64 performs decently, with a drop of 0.8% or 2.0% in accuracy. The results are similarly good when the batch size is from 256 to 2048, and the differences are at the level of random variations.</p><p>This behavior of SimSiam is noticeably different from SimCLR <ref type="bibr" target="#b7">[8]</ref> and SwAV <ref type="bibr" target="#b6">[7]</ref>. All three methods are Siamese networks with direct weight-sharing, but SimCLR and SwAV both require a large batch (e.g., 4096) to work well.</p><p>We also note that the standard SGD optimizer does not work well when the batch is too large (even in supervised learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref>), and our result is lower with a 4096 batch. We expect a specialized optimizer (e.g., LARS <ref type="bibr" target="#b37">[38]</ref>) will help in this case. However, our results show that a specialized optimizer is not necessary for preventing collapsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Batch Normalization</head><p>Table <ref type="table" target="#tab_4">3</ref> compares the configurations of BN on the MLP heads. In Table <ref type="table" target="#tab_4">3a</ref> we remove all BN layers in the MLP heads (10-epoch warmup <ref type="bibr" target="#b13">[14]</ref> is used specifically for this entry). This variant does not cause collapse, although the accuracy is low (34.6%). The low accuracy is likely because of optimization difficulty. Adding BN to the hidden layers (Table <ref type="table" target="#tab_4">3b</ref>) increases accuracy to 67.4%.</p><p>Further adding BN to the output of the projection MLP (i.e., the output of f ) boosts accuracy to 68.1% (Table <ref type="table" target="#tab_4">3c</ref>), which is our default configuration. In this entry, we also find that the learnable affine transformation (scale and offset <ref type="bibr" target="#b21">[22]</ref>) in f 's output BN is not necessary, and disabling it leads to a comparable accuracy of 68.2%.</p><p>Adding BN to the output of the prediction MLP h does not work well (Table <ref type="table" target="#tab_4">3d</ref>). We find that this is not about collapsing. The training is unstable and the loss oscillates.</p><p>In summary, we observe that BN is helpful for optimization when used appropriately, which is similar to BN's behavior in other supervised learning scenarios. But we have seen no evidence that BN helps to prevent collapsing: actually, the comparison in Sec. 4.1 (Figure <ref type="figure">2</ref>) has exactly the same BN configuration for both entries, but the model collapses if stop-gradient is not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Similarity Function</head><p>Besides the cosine similarity function <ref type="bibr" target="#b0">(1)</ref>, our method also works with cross-entropy similarity. We modify D as:</p><formula xml:id="formula_6">D(p 1 , z 2 ) = −softmax(z 2 )• log softmax(p 1 ).</formula><p>Here the softmax function is along the channel dimension. The output of softmax can be thought of as the probabilities of belonging to each of d pseudo-categories.</p><p>We simply replace the cosine similarity with the crossentropy similarity, and symmetrize it using (4). All hyperparameters and architectures are unchanged, though they may be suboptimal for this variant. Here is the comparison: The cross-entropy variant can converge to a reasonable result without collapsing. This suggests that the collapsing prevention behavior is not just about the cosine similarity. This variant helps to set up a connection to SwAV <ref type="bibr" target="#b6">[7]</ref>, which we discuss in Sec. 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Symmetrization</head><p>Thus far our experiments have been based on the symmetrized loss (4). We observe that SimSiam's behavior of preventing collapsing does not depend on symmetrization. We compare with the asymmetric variant (3) as follows: The asymmetric variant achieves reasonable results. Symmetrization is helpful for boosting accuracy, but it is not related to collapse prevention. Symmetrization makes one more prediction for each image, and we may roughly compensate for this by sampling two pairs for each image in the asymmetric version ("2×"). It makes the gap smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Summary</head><p>We have empirically shown that in a variety of settings, SimSiam can produce meaningful results without collapsing. The optimizer (batch size), batch normalization, similarity function, and symmetrization may affect accuracy, but we have seen no evidence that they are related to collapse prevention. It is mainly the stop-gradient operation that plays an essential role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Hypothesis</head><p>We discuss a hypothesis on what is implicitly optimized by SimSiam, with proof-of-concept experiments provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Formulation</head><p>Our hypothesis is that SimSiam is an implementation of an Expectation-Maximization (EM) like algorithm. It implicitly involves two sets of variables, and solves two underlying sub-problems. The presence of stop-gradient is the consequence of introducing the extra set of variables.</p><p>We consider a loss function of the following form:</p><formula xml:id="formula_7">L(θ, η) = E x,T F θ (T (x)) − η x 2 2 .<label>(5)</label></formula><p>F is a network parameterized by θ. T is the augmentation.</p><p>x is an image. The expectation E[•] is over the distribution of images and augmentations. For the ease of analysis, here we use the mean squared error • 2 2 , which is equivalent to the cosine similarity if the vectors are 2 -normalized. We do not consider the predictor yet and will discuss it later.</p><p>In (5), we have introduced another set of variables which we denote as η. The size of η is proportional to the number of images. Intuitively, η x is the representation of the image x, and the subscript x means using the image index to access a sub-vector of η. η is not necessarily the output of a network; it is the argument of an optimization problem.</p><p>With this formulation, we consider solving:</p><formula xml:id="formula_8">min θ,η L(θ, η).<label>(6)</label></formula><p>Here the problem is w.r.t. both θ and η. This formulation is analogous to k-means clustering <ref type="bibr" target="#b27">[28]</ref>. The variable θ is analogous to the clustering centers: it is the learnable parameters of an encoder. The variable η x is analogous to the assignment vector of the sample x (a one-hot vector in kmeans): it is the representation of x. Also analogous to k-means, the problem in (6) can be solved by an alternating algorithm, fixing one set of variables and solving for the other set. Formally, we can alternate between solving these two subproblems:</p><formula xml:id="formula_9">θ t ← arg min θ L(θ, η t−1 )<label>(7)</label></formula><formula xml:id="formula_10">η t ← arg min η L(θ t , η)<label>(8)</label></formula><p>Here t is the index of alternation and "←" means assigning.</p><p>Solving for θ. One can use SGD to solve the sub-problem <ref type="bibr" target="#b6">(7)</ref>. The stop-gradient operation is a natural consequence, because the gradient does not back-propagate to η t−1 which is a constant in this subproblem.</p><p>Solving for η. The sub-problem ( <ref type="formula" target="#formula_10">8</ref>) can be solved independently for each η x . Now the problem is to minimize:</p><formula xml:id="formula_11">E T F θ t (T (x)) − η x 2 2</formula><p>for each image x, noting that the expectation is over the distribution of augmentation T . Due to the mean squared error, <ref type="foot" target="#foot_4">5</ref> it is easy to solve it by:</p><formula xml:id="formula_12">η t x ← E T F θ t (T (x)) .<label>(9)</label></formula><p>This indicates that η x is assigned with the average representation of x over the distribution of augmentation.</p><p>One-step alternation. SimSiam can be approximated by one-step alternation between ( <ref type="formula" target="#formula_9">7</ref>) and ( <ref type="formula" target="#formula_10">8</ref>). First, we approximate ( <ref type="formula" target="#formula_12">9</ref>) by sampling the augmentation only once, denoted as T , and ignoring E T [•]:</p><formula xml:id="formula_13">η t x ← F θ t (T (x)).<label>(10)</label></formula><p>Inserting it into the sub-problem (7), we have:</p><formula xml:id="formula_14">θ t+1 ← arg min θ E x,T F θ (T (x)) − F θ t (T (x)) 2 2 . (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>Now θ t is a constant in this sub-problem, and T implies another view due to its random nature. This formulation exhibits the Siamese architecture. Second, if we implement <ref type="bibr" target="#b10">(11)</ref> by reducing the loss with one SGD step, then we can approach the SimSiam algorithm: a Siamese network naturally with stop-gradient applied.</p><p>Predictor. Our above analysis does not involve the predictor h. We further assume that h is helpful in our method because of the approximation due to <ref type="bibr" target="#b9">(10)</ref>. By definition, the predictor h is expected to minimize:</p><formula xml:id="formula_16">E z h(z 1 ) − z 2<label>2</label></formula><p>2 . The optimal solution to h should satisfy:</p><formula xml:id="formula_17">h(z 1 ) = E z [z 2 ] = E T f (T (x))</formula><p>for any image x. This term is similar to the one in <ref type="bibr" target="#b8">(9)</ref>. In our approximation in <ref type="bibr" target="#b9">(10)</ref>, the expectation E T [•] is ignored. The usage of h may fill this gap. In practice, it would be unrealistic to actually compute the expectation E T . But it may be possible for a neural network (e.g., the preditor h) to learn to predict the expectation, while the sampling of T is implicitly distributed across multiple epochs. Symmetrization. Our hypothesis does not involve symmetrization. Symmetrization is like denser sampling T in <ref type="bibr" target="#b10">(11)</ref>. Actually, the SGD optimizer computes the empirical expectation of E x,T [•] by sampling a batch of images and one pair of augmentations (T 1 , T 2 ). In principle, the empirical expectation should be more precise with denser sampling. Symmetrization supplies an extra pair (T 2 , T 1 ). This explains that symmetrization is not necessary for our method to work, yet it is able to improve accuracy, as we have observed in Sec. 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Proof of concept</head><p>We design a series of proof-of-concept experiments that stem from our hypothesis. They are methods different with SimSiam, and they are designed to verify our hypothesis.</p><p>Multi-step alternation. We have hypothesized that the SimSiam algorithm is like alternating between ( <ref type="formula" target="#formula_9">7</ref>) and ( <ref type="formula" target="#formula_10">8</ref>), with an interval of one step of SGD update. Under this hypothesis, it is likely for our formulation to work if the interval has multiple steps of SGD.</p><p>In this variant, we treat t in ( <ref type="formula" target="#formula_9">7</ref>) and ( <ref type="formula" target="#formula_10">8</ref>) as the index of an outer loop; and the sub-problem in ( <ref type="formula" target="#formula_9">7</ref>) is updated by an inner loop of k SGD steps. In each alternation, we pre-compute the η x required for all k SGD steps using <ref type="bibr" target="#b9">(10)</ref> and cache them in memory. Then we perform k SGD steps to update θ. We use the same architecture and hyperparameters as SimSiam. The comparison is as follows: Here, "1-step" is equivalent to SimSiam, and "1-epoch" denotes the k steps required for one epoch. All multi-step variants work well. The 10-/100-step variants even achieve better results than SimSiam, though at the cost of extra precomputation. This experiment suggests that the alternating optimization is a valid formulation, and SimSiam is a special case of it.</p><p>Expectation over augmentations. The usage of the predictor h is presumably because the expectation E T [•] in ( <ref type="formula" target="#formula_12">9</ref>) is ignored. We consider another way to approximate this expectation, in which we find h is not needed.</p><p>In this variant, we do not update η x directly by the assignment <ref type="bibr" target="#b9">(10)</ref>; instead, we maintain a moving-average:</p><formula xml:id="formula_18">η t x ← m * η t−1 x + (1 − m) * F θ t (T (x))</formula><p>, where m is a momentum coefficient (0.8 here). This computation is similar to maintaining the memory bank as in <ref type="bibr" target="#b35">[36]</ref>. This movingaverage provides an approximated expectation of multiple views. This variant has 55.0% accuracy without the predictor h. As a comparison, it fails completely if we remove h but do not maintain the moving average (as shown in Table 1a). This proof-of-concept experiment supports that the usage of predictor h is related to approximating  <ref type="bibr" target="#b12">[13]</ref>. All VOC results are the average over 5 trials. Bold entries are within 0.5 below the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Discussion</head><p>Our hypothesis is about what the optimization problem can be. It does not explain why collapsing is prevented. We point out that SimSiam and its variants' non-collapsing behavior still remains as an empirical observation.</p><p>Here we briefly discuss our understanding on this open question. The alternating optimization provides a different trajectory, and the trajectory depends on the initialization. It is unlikely that the initialized η, which is the output of a randomly initialized network, would be a constant. Starting from this initialization, it may be difficult for the alternating optimizer to approach a constant η x for all x, because the method does not compute the gradients w.r.t. η jointly for all x. The optimizer seeks another trajectory (Figure <ref type="figure">2 left</ref>), in which the outputs are scattered (Figure <ref type="figure">2</ref> middle).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Result Comparisons</head><p>ImageNet. We compare with the state-of-the-art frameworks in Table <ref type="table" target="#tab_7">4</ref> on ImageNet linear evaluation. For fair comparisons, all competitors are based on our reproduction, and "+" denotes improved reproduction vs. the original papers (see supplement). For each individual method, we follow the hyper-parameter and augmentation recipes in its original paper. <ref type="foot" target="#foot_5">6</ref> All entries are based on a standard ResNet-50, with two 224×224 views used during pre-training.</p><p>Table <ref type="table" target="#tab_7">4</ref> shows the results and the main properties of the methods. SimSiam is trained with a batch size of 256, using neither negative samples nor a momentum encoder. Despite it simplicity, SimSiam achieves competitive results. It has the highest accuracy among all methods under 100-epoch pre-training, though its gain of training longer is smaller. It has better results than SimCLR in all cases.</p><p>Transfer Learning. In Table <ref type="table" target="#tab_8">5</ref> we compare the representation quality by transferring them to other tasks, including VOC <ref type="bibr" target="#b11">[12]</ref> object detection and COCO <ref type="bibr" target="#b25">[26]</ref> object detection and instance segmentation. We fine-tune the pretrained models end-to-end in the target datasets. We use the public codebase from MoCo <ref type="bibr" target="#b16">[17]</ref> for all entries, and search the fine-tuning learning rate for each individual method. All methods are based on 200-epoch pre-training in ImageNet using our reproduction.</p><p>Table <ref type="table" target="#tab_8">5</ref> shows that SimSiam's representations are transferable beyond the ImageNet task. It is competitive among these leading methods. The "base" SimSiam in Table <ref type="table" target="#tab_8">5</ref> uses the baseline pre-training recipe as in our ImageNet experiments. We find that another recipe of lr = 0.5 and wd = 1e-5 (with similar ImageNet accuracy) can produce better results in all tasks (Table <ref type="table" target="#tab_8">5</ref>, "SimSiam, optimal").</p><p>We emphasize that all these methods are highly successful for transfer learning-in Table <ref type="table" target="#tab_8">5</ref>, they can surpass or be on par with the ImageNet supervised pre-training counterparts in all tasks. Despite many design differences, a common structure of these methods is the Siamese network. This comparison suggests that the Siamese structure is a core factor for their general success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Methodology Comparisons</head><p>Beyond accuracy, we also compare the methodologies of these Siamese architectures. Our method plays as a hub to connect these methods. Figure <ref type="figure">3</ref> abstracts these methods. The "encoder" subsumes all layers that can be shared between both branches (e.g., backbone, projection MLP <ref type="bibr" target="#b7">[8]</ref>, prototypes <ref type="bibr" target="#b6">[7]</ref>). The components in red are those missing in SimSiam. We discuss the relations next.</p><p>Relation to SimCLR <ref type="bibr" target="#b7">[8]</ref>. SimCLR relies on negative samples ("dissimilarity") to prevent collapsing. SimSiam can be thought of as "SimCLR without negatives".</p><p>To have a more thorough comparison, we append the prediction MLP h and stop-gradient to SimCLR. <ref type="foot" target="#foot_6">7</ref> Here is the ablation on our SimCLR reproduction: Relation to SwAV <ref type="bibr" target="#b6">[7]</ref>. SimSiam is conceptually analogous to "SwAV without online clustering". We build up this connection by recasting a few components in SwAV. (i) The shared prototype layer in SwAV can be absorbed into the Siamese encoder. (ii) The prototypes were weight-normalized outside of gradient propagation in <ref type="bibr" target="#b6">[7]</ref>; we instead implement by full gradient computation <ref type="bibr" target="#b32">[33]</ref>. <ref type="foot" target="#foot_7">8</ref>(iii) The similarity function in SwAV is cross-entropy. With these abstractions, a highly simplified SwAV illustration is shown in Figure <ref type="figure">3</ref>.</p><p>SwAV applies the Sinkhorn-Knopp (SK) transform [10] on the target branch (which is also symmetrized <ref type="bibr" target="#b6">[7]</ref>). The SK transform is derived from online clustering <ref type="bibr" target="#b6">[7]</ref>: it is the outcome of clustering the current batch subject to a balanced partition constraint. The balanced partition can avoid collapsing. Our method does not involve this transform.</p><p>We study the effect of the prediction MLP h and stopgradient on SwAV. Note that SwAV applies stop-gradient on the SK transform, so we ablate by removing it. Here is the comparison on our SwAV reproduction: an alternating formulation <ref type="bibr" target="#b6">[7]</ref>. This may explain why stopgradient should not be removed from SwAV.</p><p>Relation to BYOL <ref type="bibr" target="#b14">[15]</ref>. Our method can be thought of as "BYOL without the momentum encoder", subject to many implementation differences. The momentum encoder may be beneficial for accuracy (Table <ref type="table" target="#tab_7">4</ref>), but it is not necessary for preventing collapsing. Given our hypothesis in Sec. 5, the η sub-problem (8) can be solved by other optimizers, e.g., a gradient-based one. This may lead to a temporally smoother update on η. Although not directly related, the momentum encoder also produces a smoother version of η. We believe that other optimizers for solving (8) are also plausible, which can be a future research problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have explored Siamese networks with simple designs. The competitiveness of our minimalist method suggests that the Siamese shape of the recent methods can be a core reason for their effectiveness. Siamese networks are natural and effective tools for modeling invariance, which is a focus of representation learning. We hope our study will attract the community's attention to the fundamental role of Siamese networks in representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Unsupervised pre-training. Our implementation follows the practice of existing works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Data augmentation. We describe data augmentation using the PyTorch <ref type="bibr" target="#b30">[31]</ref>   <ref type="bibr" target="#b7">[8]</ref>, we initialize the scale parameters as 0 <ref type="bibr" target="#b13">[14]</ref> in the last BN layer for every residual block.</p><p>Weight decay. We use a weight decay of 0.0001 for all parameter layers, including the BN scales and biases, in the SGD optimizer. This is in contrast to the implementation of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref> that excludes BN scales and biases from weight decay in their LARS optimizer.</p><p>Linear evaluation. Given the pre-trained network, we train a supervised linear classifier on frozen features, which are from ResNet's global average pooling layer (pool 5 ). The linear classifier training uses base lr = 0.02 with a cosine decay schedule for 90 epochs, weight decay = 0, momentum= 0.9, batch size= 4096 with a LARS optimizer <ref type="bibr" target="#b37">[38]</ref>. We have also tried the SGD optimizer following <ref type="bibr" target="#b16">[17]</ref> with base lr = 30.0, weight decay = 0, momentum = 0.9, and batch size= 256, which gives ∼1% lower accuracy. After training the linear classifier, we evaluate it on the center 224×224 crop in the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Ablations on ImageNet</head><p>The following table reports the SimSiam results vs. the output dimension d: It benefits from a larger d and gets saturated at d = 2048. This is unlike existing methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref> whose accuracy is saturated when d is 256 or 512. In this table, the prediction MLP's hidden layer dimension is always 1/4 of the output dimension. We find that this bottleneck structure is more robust. If we set the hidden dimension to be equal to the output dimension, the training can be less stable or fail in some variants of our exploration. We hypothesize that this bottleneck structure, which behaves like an auto-encoder, can force the predictor to digest the information. We recommend to use this bottleneck structure for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Reproducing Related Methods</head><p>Our comparison in Table <ref type="table" target="#tab_7">4</ref> is based on our reproduction of the related methods. We re-implement the related methods as faithfully as possible following each individual paper. In addition, we are able to improve SimCLR, MoCo v2, and SwAV by small and straightforward modifications: specifically, we use 3 layers in the projection MLP in SimCLR and SwAV (vs. originally 2), and use symmetrized loss for MoCo v2 (vs. originally asymmetric). Table <ref type="table">C</ref>.1 compares our reproduction of these methods with the original papers' results (if available). Our reproduction has better results for SimCLR, MoCo v2, and SwAV (denoted as "+" in Table <ref type="table" target="#tab_7">4</ref>), and has at least comparable results for BYOL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CIFAR Experiments</head><p>We have observed similar behaviors of SimSiam in the CIFAR-10 dataset <ref type="bibr" target="#b23">[24]</ref>. The implementation is similar to that in ImageNet. We use SGD with base lr = 0.03 and a cosine decay schedule for 800 epochs, weight decay = 0.0005, momentum = 0.9, and batch size = 512. The input image size is 32×32. We do not use blur augmentation. The backbone is the CIFAR variant of ResNet-18 <ref type="bibr" target="#b18">[19]</ref>, followed by a 2-layer projection MLP. The outputs are 2048-d.</p><p>Figure <ref type="figure">D</ref>.1 shows the kNN classification accuracy (left) and the linear evaluation (right). Similar to the ImageNet observations, SimSiam achieves a reasonable result and does not collapse. We compare with SimCLR <ref type="bibr" target="#b7">[8]</ref> trained with the same setting. Interestingly, the training curves are similar between SimSiam and SimCLR. SimSiam is slightly better by 0.7% under this setting. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 1 .</head><label>21</label><figDesc>Figure 1. SimSiam architecture. Two augmented views of one image are processed by the same encoder network f (a backbone plus a projection MLP). Then a prediction MLP h is applied on one side, and a stop-gradient operation is applied on the other side. The model maximizes the similarity between both sides. It uses neither negative pairs nor a momentum encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>SimSiam Pseudocode, PyTorch-like # f: backbone + projection mlp # h: prediction mlp for x in loader: # load a minibatch x with n samples x1, x2 = aug(x), aug(x) # random augmentation z1, z2 = f(x1), f(x2) # projections, n-by-d p1, p2 = h(z1), h(z2) # predictions, n-by-d L = D(p1, z2)/2 + D(p2, z1)/2 # loss L.backward() # back-propagate update(f, h) # SGD update def D(p, z): # negative cosine similarity z = z.detach() # stop gradient p = normalize(p, dim=1) # l2-normalize z = normalize(z, dim=1) # l2-normalize return -(p * z).sum(dim=1).mean()</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure D. 1 .</head><label>1</label><figDesc>Figure D.1. CIFAR-10 experiments. Left: validation accuracy of kNN classification as a monitor during pre-training. Right: linear evaluation accuracy. The backbone is ResNet-18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Effect of prediction MLP (ImageNet linear evaluation accuracy with 100-epoch pre-training). In all these variants, we use the same schedule for the encoder f (lr with cosine decay).</figDesc><table><row><cell>1 2 .</cell></row></table><note>1 2 and std[z i ] ≈ 1/d</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>we have observed that the asymmetric variant (3) also fails if removing h, while it can work if h is kept (Sec. 4.6). These experiments suggest that h is helpful for our model.If h is fixed as random initialization, our model does not work either (Table1b). However, this failure is not about Effect of batch sizes (ImageNet linear evaluation accuracy with 100-epoch pre-training).</figDesc><table><row><cell>batch size</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell cols="2">1024 2048 4096</cell></row><row><cell>acc. (%)</cell><cell cols="4">66.1 67.3 68.1 68.1</cell><cell>68.0</cell><cell>67.9</cell><cell>64.0</cell></row><row><cell></cell><cell></cell><cell cols="5">proj. MLP's BN pred. MLP's BN</cell></row><row><cell>case</cell><cell></cell><cell cols="5">hidden output hidden output acc. (%)</cell></row><row><cell>(a) none</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>34.6</cell></row><row><cell cols="2">(b) hidden-only</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>67.4</cell></row><row><cell>(c) default</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>68.1</cell></row><row><cell>(d) all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>unstable</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Effect of batch normalization on MLP heads (Ima-geNet linear evaluation accuracy with 100-epoch pre-training).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>E T [•]. Comparisons on ImageNet linear classification. All are based on ResNet-50 pre-trained with two 224×224 views. Evaluation is on a single crop. All competitors are from our reproduction, and "+" denotes improved reproduction vs. original papers (see supplement).</figDesc><table><row><cell>method</cell><cell></cell><cell></cell><cell>batch size</cell><cell>negative pairs</cell><cell cols="2">momentum encoder</cell><cell cols="5">100 ep 200 ep 400 ep 800 ep</cell><cell></cell></row><row><cell cols="3">SimCLR (repro.+)</cell><cell>4096</cell><cell></cell><cell></cell><cell></cell><cell>66.5</cell><cell>68.3</cell><cell>69.8</cell><cell>70.4</cell><cell></cell><cell></cell></row><row><cell cols="3">MoCo v2 (repro.+)</cell><cell>256</cell><cell></cell><cell></cell><cell></cell><cell>67.4</cell><cell>69.9</cell><cell>71.0</cell><cell>72.2</cell><cell></cell><cell></cell></row><row><cell cols="2">BYOL (repro.)</cell><cell></cell><cell>4096</cell><cell></cell><cell></cell><cell></cell><cell>66.5</cell><cell>70.6</cell><cell>73.2</cell><cell>74.3</cell><cell></cell><cell></cell></row><row><cell cols="2">SwAV (repro.+)</cell><cell></cell><cell>4096</cell><cell></cell><cell></cell><cell></cell><cell>66.5</cell><cell>69.1</cell><cell>70.7</cell><cell>71.8</cell><cell></cell><cell></cell></row><row><cell cols="2">SimSiam</cell><cell></cell><cell>256</cell><cell></cell><cell></cell><cell></cell><cell>68.1</cell><cell>70.0</cell><cell>70.8</cell><cell>71.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">VOC 07 detection</cell><cell cols="3">VOC 07+12 detection</cell><cell cols="3">COCO detection</cell><cell cols="3">COCO instance seg.</cell></row><row><cell>pre-train</cell><cell>AP 50</cell><cell>AP</cell><cell>AP 75</cell><cell>AP 50</cell><cell>AP</cell><cell>AP 75</cell><cell>AP 50</cell><cell>AP</cell><cell cols="2">AP 75 AP mask 50</cell><cell cols="2">AP mask AP mask 75</cell></row><row><cell>scratch</cell><cell>35.9</cell><cell>16.8</cell><cell>13.0</cell><cell>60.2</cell><cell>33.8</cell><cell>33.1</cell><cell>44.0</cell><cell>26.4</cell><cell>27.8</cell><cell>46.9</cell><cell>29.3</cell><cell>30.8</cell></row><row><cell cols="2">ImageNet supervised 74.4</cell><cell>42.4</cell><cell>42.7</cell><cell>81.3</cell><cell>53.5</cell><cell>58.8</cell><cell>58.2</cell><cell>38.2</cell><cell>41.2</cell><cell>54.7</cell><cell>33.3</cell><cell>35.2</cell></row><row><cell>SimCLR (repro.+)</cell><cell>75.9</cell><cell>46.8</cell><cell>50.1</cell><cell>81.8</cell><cell>55.5</cell><cell>61.4</cell><cell>57.7</cell><cell>37.9</cell><cell>40.9</cell><cell>54.6</cell><cell>33.3</cell><cell>35.3</cell></row><row><cell>MoCo v2 (repro.+)</cell><cell>77.1</cell><cell>48.5</cell><cell>52.5</cell><cell>82.3</cell><cell>57.0</cell><cell>63.3</cell><cell>58.8</cell><cell>39.2</cell><cell>42.5</cell><cell>55.5</cell><cell>34.3</cell><cell>36.6</cell></row><row><cell>BYOL (repro.)</cell><cell>77.1</cell><cell>47.0</cell><cell>49.9</cell><cell>81.4</cell><cell>55.3</cell><cell>61.1</cell><cell>57.8</cell><cell>37.9</cell><cell>40.9</cell><cell>54.3</cell><cell>33.2</cell><cell>35.0</cell></row><row><cell>SwAV (repro.+)</cell><cell>75.5</cell><cell>46.5</cell><cell>49.6</cell><cell>81.5</cell><cell>55.4</cell><cell>61.4</cell><cell>57.6</cell><cell>37.6</cell><cell>40.3</cell><cell>54.2</cell><cell>33.1</cell><cell>35.1</cell></row><row><cell>SimSiam, base</cell><cell>75.5</cell><cell>47.0</cell><cell>50.2</cell><cell>82.0</cell><cell>56.4</cell><cell>62.8</cell><cell>57.5</cell><cell>37.9</cell><cell>40.9</cell><cell>54.2</cell><cell>33.2</cell><cell>35.2</cell></row><row><cell>SimSiam, optimal</cell><cell>77.3</cell><cell>48.5</cell><cell>52.5</cell><cell>82.4</cell><cell>57.0</cell><cell>63.7</cell><cell>59.3</cell><cell>39.2</cell><cell>42.1</cell><cell>56.0</cell><cell>34.4</cell><cell>36.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Transfer Learning. All unsupervised methods are based on 200-epoch pre-training in ImageNet. VOC 07 detection: Faster R-CNN [32] fine-tuned in VOC 2007 trainval, evaluated in VOC 2007 test; VOC 07+12 detection: Faster R-CNN fine-tuned in VOC 2007 trainval + 2012 train, evaluated in VOC 2007 test; COCO detection and COCO instance segmentation: Mask R-CNN [18] (1× schedule) fine-tuned in COCO 2017 train, evaluated in COCO 2017 val. All Faster/Mask R-CNN models are with the C4-backbone</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Neither the stop-gradient nor the extra predictor is necessary or helpful for SimCLR. As we have analyzed in Sec. 5, the introduction of the stop-gradient and extra predictor is presumably a consequence of another underlying optimization problem. It is different from the contrastive learning problem, so these extra components may not be helpful.</figDesc><table><row><cell>SimCLR</cell><cell>w/ predictor</cell><cell>w/ pred. &amp; stop-grad</cell></row><row><cell>66.5</cell><cell>66.4</cell><cell>66.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Comparison on Siamese architectures. The encoder includes all layers that can be shared between both branches. The dash lines indicate the gradient propagation flow. In BYOL, SwAV, and SimSiam, the lack of a dash line implies stop-gradient, and their symmetrization is not illustrated for simplicity. The components in red are those missing in SimSiam.</figDesc><table><row><cell></cell><cell></cell><cell>grad</cell><cell>similarity &amp; dissimilarity</cell><cell>grad</cell><cell>grad</cell><cell>similarity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>predictor</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>moving</cell></row><row><cell></cell><cell></cell><cell>encoder</cell><cell></cell><cell>encoder</cell><cell>encoder</cell><cell>average</cell><cell>encoder momentum</cell></row><row><cell></cell><cell></cell><cell></cell><cell>image</cell><cell></cell><cell>image</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SimCLR</cell><cell></cell><cell>BYOL</cell></row><row><cell></cell><cell></cell><cell>grad</cell><cell>similarity</cell><cell></cell><cell>grad</cell><cell>similarity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sinkhorn-Knopp</cell><cell>predictor</cell></row><row><cell></cell><cell></cell><cell>encoder</cell><cell></cell><cell>encoder</cell><cell>encoder</cell><cell>encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell>image</cell><cell></cell><cell>image</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell><cell>SimSiam</cell></row><row><cell></cell><cell></cell><cell>Figure 3.</cell><cell></cell><cell></cell></row><row><cell>SwAV</cell><cell>w/ predictor</cell><cell>remove stop-grad</cell><cell></cell><cell></cell></row><row><cell>66.5</cell><cell>65.2</cell><cell>NaN</cell><cell></cell><cell></cell></row><row><cell cols="3">Adding the predictor does not help either. Removing stop-</cell><cell></cell><cell></cell></row><row><cell cols="3">gradient (so the model is trained end-to-end) leads to diver-</cell><cell></cell><cell></cell></row><row><cell cols="3">gence. As a clustering-based method, SwAV is inherently</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table C.1. Our reproduction vs. original papers' results. All are based on ResNet-50 pre-trained with two 224×224 crops.</figDesc><table><row><cell>SimCLR</cell><cell>MoCo v2</cell><cell>BYOL</cell><cell></cell><cell>SwAV</cell></row><row><cell cols="4">epoch 200 800 1000 200 800 300 800 1000</cell><cell>400</cell></row><row><cell cols="2">origin 66.6 68.3 69.3 67.5 71.1 72.5</cell><cell cols="3">-74.3 70.1</cell></row><row><cell>repro. 68.3 70.4</cell><cell cols="2">-69.9 72.2 72.4 74.3</cell><cell cols="2">-70.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">MoCo<ref type="bibr" target="#b16">[17]</ref> and BYOL<ref type="bibr" target="#b14">[15]</ref> do not directly share the weights between the two branches, though in theory the momentum encoder should converge to the same status as the trainable encoder. We view these models as Siamese networks with "indirect" weight-sharing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><ref type="bibr" target="#b1">2</ref> In BYOL's arXiv v3 update, it reports 66.9% accuracy</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="300" xml:id="foot_2">with 300-epoch pre-training when removing the momentum encoder and increasing the predictor's learning rate by 10×. Our work was done concurrently with this arXiv update. Our work studies this topic from different perspectives, with better results achieved.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We note that a chance-level accuracy (0.1%) is not sufficient to indicate collapsing. A model with a diverging loss, which is another pattern of failure, may also exhibit a chance-level accuracy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">If we use the cosine similarity, we can approximately solve it by 2normalizing F 's output and ηx.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">In our BYOL reproduction, the 100, 200(400), 800-epoch recipes follow the 100, 300, 1000-epoch recipes in<ref type="bibr" target="#b14">[15]</ref>: lr is {0.45, 0.3, 0.2}, wd is {1e-6, 1e-6, 1.5e-6}, and momentum coefficient is {0.99, 0.99, 0.996}.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">We append the extra predictor to one branch and stop-gradient to the other branch, and symmetrize this by swapping.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">This modification produces similar results as original SwAV, but it can enable end-to-end propagation in our ablation.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Markus Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<title level="m">Learning representations by maximizing mutual information across views</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional Siamese networks for object tracking</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Signature verification using a &quot;Siamese&quot; time delay neural network</title>
		<author>
			<persName><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A simple framework for contrastive learning of visual representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes (VOC) Challenge. IJCV</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Piotr Dollár, and Kaiming He. Detectron</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733v1</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272v2</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Microsoft</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Selfsupervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01991</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
