<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Theoretical Analysis of Recursive Identification Methods*t</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">T</forename><surname>Soderstrom</surname></persName>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
						</author>
						<author>
							<persName><forename type="first">I</forename><surname>Gustavsson ~-</surname></persName>
						</author>
						<title level="a" type="main">A Theoretical Analysis of Recursive Identification Methods*t</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A022D11268378FD589AD526F79304FF2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A unified analysis of five common recursive identification methods provides general conditions for convergence, and simulation studies indicate greatest accuracy for the recursive maximum likelihood method, at least for long samples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>IN MANY applications it is highly desirable to identify a process recursively as the process develops. For example it might be of interest to proceed with the experiment until a specified parameter accuracy is obtained. Another situation requiring the model to be updated is when the process or noise characteristics are gradually changing. Recursive estimation is also necessary in many adaptive control systems and for monitoring the process for diagnostic purposes.</p><p>Many different ways of obtaining recursive algorithms have been proposed. In [11 some early references on recursive identification methods are given. The properties of most presented methods have been illustrated mainly by simulations, see e.g. <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. There are now various comparisons available but only few results on unification and convergence have been presented.</p><p>The aim of the present paper is to give a unified description and convergence analysis of a number of *Received <ref type="bibr">August 2, 1976</ref>; revised <ref type="bibr">June 10, 1977</ref>; revised <ref type="bibr">November 14, 1977.</ref> The original version of this paper was not presented at any IFAC meeting. It was recommended for publication in revised form by associate editor R. Isermann.</p><p>[This work was partly supported by the Swedish Board for Technical Development under Contract 74-3476.</p><p>~Department of Automatic Control and Systems Analysis, Institute of Technology, Uppsala University, S-751 21 Uppsala, Sweden.</p><p>§Department of Electrical Engineering, Link6ping University, S-581 83 Link6ping, Sweden. ¶Department of Automatic Control, Lund Institute of Technology, S-220 07 Lund, Sweden. 231</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>The five different recursive identification methods to be analysed here have great similarities as far as their analysis is concerned. Most of them are very well known and discussed in several papers, but there could be some confusion concerning the names of the methods. The names used in this paper are given below, along with some variants.</p><p>--RLS--the recursive least squares method --RIV--the recursive instrumental variables method (also called the bootstrap estimator)</p><p>--RGLS--tbe recursive generalized least squares method --RELS--the recursive extended least squares method (also called the extended matrix method, the approximate maximum likelihood method, Panuska's method, or RML 1)</p><p>--RML--the recursive maximum likelihood method.</p><p>The RLS method is well-known. It is treated e.g. in <ref type="bibr" target="#b0">[1]</ref>. Some early work on the RIV method has been done by Mayne <ref type="bibr" target="#b6">[7]</ref>, Wong and Polak <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr">Young[91.</ref> References on early applications of the RIV method to econometrics can be found in [61. The RGLS method was proposed by Hastings-James and Sage <ref type="bibr" target="#b9">[10]</ref> based on the off-line GLS method[l 1]. The RELS method was suggested by Panuska <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">131 and Young[9]</ref>, while presentation of the RML method can be found in the works by Furht[141 and S6derstr6m[151. Several references dealing with the methods can be found in <ref type="bibr" target="#b15">[16]</ref>. It will be shown later that these different algorithms have essentially the same structure.</p><p>Before going into the details of the different methods, some general conventions and assumptions will be made.</p><p>Identification means that a model is fitted to measured data ull ) ..... u <ref type="bibr">(N)</ref> of the input signal and y(l ) ..... y <ref type="bibr">(N)</ref> of the output signal. The model is here assumed to have the following general structure ~4(q-~)y(t)=B(q -~)u(t)+lgI(q-~ )c(t) <ref type="bibr">(2.11</ref> where E(t) denotes the residual and ,4(q ~) and /~(q-1) are polynomials in the backward shift operator q-1 (i.e. q-ly(tI=y4t-1 )), A(q-l)=l +filq-I +. • • +c~,,/l .... B(q-~)=/~q -~ +...+b,,~q-"~ gaussian, but this is not essential for the convergence analysis. The polynomials A(q-~ ) and Blq~-~ ) are given as A(q-l)=l+alq ~ +...+U,oq -~,, B(q-1)=hla -l +...+hn~q -bb.</p><p>For RLS it is assumed that</p><formula xml:id="formula_0">H(q-11= 1 (2.6)</formula><p>for RGLS that</p><p>• H(q-lj=l/C(q-1)=l/(1 +clq and for RELS and RML that H(q-1)=C(q-1)= l +clq (2.8)</p><p>The filter [~(q-1) is of different form for the different methods. For RLS /4(q-~ )= 1.</p><p>(</p><p>For RIV /4 is not specified at all. For formal reasons we will use (2.2) for RIV and allow ~:(t) to be arbitrary.</p><p>For RGLS we have i2i (q -11= l/C(q-~ )= 1/(1 +(,q and for RELS and RML</p><formula xml:id="formula_2">-1 "4-...-Jf'(~'.cq -"c ) (2.3)</formula><p>~(q-l)=(~(q-~)= 1 +?lq-1 +... +(,fl-,c.</p><p>(2.4) Some of the methods, e.g. RELS, cf. <ref type="bibr" target="#b16">[17]</ref>, and RML can be applied also to other model structures. In order to restrict the analysis only the structure given above will be analysed.</p><p>It would be straightforward to include several input signals or a larger time delay in the model. No principal difficulties occur but these generalizations are not treated in order to keep the notations simple.</p><p>There are no principal difficulties to carry out the forthcoming analysis of convergence for a general description of the actual process or system. Since we are interested in particular in analysis of consistency properties we shall assume that for each method the system has an appropriate form fitting the model structure used. Thus it will be generally assumed that the system is given by A (q-1 )y(t) = B(q-1 )u (t) + H(q -1 )e(t) <ref type="bibr">(2.5)</ref> where e (t) is white noise. For certain interpretations like the ML method it is also assumed that e(t) is Note that for most variants it is not necessary to specify H(q-1) for RIV.</p><p>Note also that it is implicitly assumed that the true order of the system is known before the recursive identification method is applied.</p><p>It is generally assumed that the description of the system is such that the pair of polynomials A(z), B(z), (z being an arbitrary complex variable, replacing q-l) has no common factor. For RELS and RML it is also assumed that the pair A(z), C(z) has no common factor. This implies in particular that the process is controllable. It will also be generally assumed that the system is asymptotically stable. For open loop operation this means that the A(z) polynomial has all its zeros outside the unit circle. It is also assumed that the C(z)-polynomial has all zeros outside the unit circle. This is only a minor restriction, cf. the spectral factorization theorem <ref type="bibr" target="#b17">[18]</ref>. This assumption is made, since for RELS and RML, the polynomials ~'(z) will be restricted to have all zeros outside the unit circle, i.e. the noise process is invertible, in order to assure finite variance of the residuals.</p><p>The input signal can be determined in several ways. In this paper most results will be valid for an arbitrary, persistently exciting (cf. <ref type="bibr" target="#b18">[19]</ref>) input, e.g. filtered white noise, which covers many common cases. In particular most of the analysis covers closed loop operation where the regulator is time variable and determined by the present model of the process (i.e. an adaptive control system).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">REVIEW OF THE METHODS</head><p>The five methods under study can all be described by an algorithm of the following form</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O(t + l )=O(t)+K(t + l )e(t + l )</head><p>(3.1) K(t+l)= P(t)z(t + 1) 2(t+ 1)+ q)(t + 1)rP(t)z(t+ is (an estimate of) the one step prediction error. The parameters of the system (2.5) will be collected into a vector 0o of 'true parameter values'. The choice of the variables ~0 (t), z (t)and 2(t) will be discussed later. If 20 = 1 and2(0)= 1,then(3.4)gives2(t)= 1 forallt.Now the methods under discussion will be obtained as special cases with appropriate interpretations of the quantities in (3.1)-(3.3). input signal according to "" B(q-~)u(t) <ref type="bibr">(3.11)</ref> with .~(q-X)= 1 +alq -1 +...+d,,q-"a B(q-~ )= ~lq-1 +... +/~,bq-,b being relatively prime cf. <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>• In particular the choice A(q-1)=A(q-1), B(q-1)=B(q -1) will work for all persistently exciting input signals• Then x(t) becomes equal to the deterministic part of the output signal• This choice is also optimal in some sense, cf. <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b21">[22]</ref>. However, this choice is of course not possible since the true parameter values are required. Instead it is often suggested, see e.g. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref> to 'bootstrap' between estimation of 0o and of B(q-1 )/A(q-1 )u(t) by taking</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The RLS method</head><p>This method is obtained from (3.1)-(3.3) with the choices (3.7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>tp(t ) = [--y(t--1)...--y(t -n~)u(t--1 )... u(t--nb )] r</head><p>The prediction error e(t + 1 ) is computed as e(t+l)=y(t+l)-~o(t+l)rO(t).</p><p>(3.8)</p><formula xml:id="formula_3">Yc(t ) = z(t )r O(t ) (3.12)</formula><p>and replacing x(t-i), i=1 ... n~ in (3.10)with ~(t -i).</p><p>Some simple modifications of (3•12) have been suggested, i.e. to replace 0(t) in (3.12) with 0(t -~) or with a low pass filtered estimate• These modifications will, however, have no influence on the convergence properties discussed in this paper• The choice ,4(q-1)=1, B(q-l)=q -"b is sometimes used. Then z(t) will just contain lagged values of the input. After a reordering of the elements we then have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The RIV method</head><p>This method is obtained when the choices (3•5), (3.7), <ref type="bibr">(3.8)</ref> are made and the so-called instrumental variables z(t) are chosen so that As mentioned earlier the noise dynamics is not estimated in the RIV method. For this reason it has been suggested by Young, see e.g. <ref type="bibr" target="#b5">[6]</ref>, to combine the RIV and the RELS methods (to obtain RIV-AML in Young's terminology). Then RIV is used as a first step to estimate ,~(q-1) and /)(q-i). From these estimates the residuals are obtained and these are used as input data to the RELS method. All this means that the convergence properties of the combined method can be easily obtained from the properties of the single RIV and RELS method• More recently <ref type="bibr" target="#b21">[22]</ref>, Young has suggested another scheme to refine the RIV method. In this case the estimation of noise dynamics is more interweaved with the estimation of/](q-x) and /~(q-1). This method is related but not equivalent to the RML method. We will not carry out the detailed analysis of it here but refer to the work in <ref type="bibr" target="#b24">[25]</ref>.</p><formula xml:id="formula_4">• 1 N (i) hm~-~ z(t)q)(t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The RGLS method</head><p>Inspired by Clarke's off-line algorithm <ref type="bibr" target="#b10">[11]</ref>, the recursive RGLS method was suggested by Hastings-James-Sage <ref type="bibr" target="#b9">[10]</ref>. It consists of two RLS estimators combined via filtering. One estimator is used for estimation of the ,4-and /)-coefficients while the other is used to estimate (~(q-~ ). We refer to <ref type="bibr" target="#b9">[10]</ref> for details. Similar algorithms have later been suggested e.g. in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The RELS method</head><p>For this case we take </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.e.f z(t)=~o(t)= [ -y(t-1)...-y(t-na) u(t-1)... u(t -nb )e(t -1)...e(t-n~ )] r e(t+ 1) =y(t + 1)-tp(t + 1 )TO(t).</head><p>(3.14) <ref type="bibr">(3.15)</ref> This algorithm is often used to model time series i.e. used without b,-parameters and sometimes also processes without a~-parameters. In <ref type="bibr" target="#b16">[17]</ref> it is used for the model structure given by (q ) t f~(q-')y(t) = B(q-')u(t)+ a-w-~-~_,, e(). <ref type="bibr">(3.16)</ref> utq ' ~ be solved from t=0 for every new measurement. This will require too much computation, and some suitable approximation has to be made. One way is to take</p><formula xml:id="formula_5">~(t)=y(t)-[-y(t-1)...-y(t-n,) u(t-1 )... u(t-nb )e(t -1 )... ~:(t -nc)]O(t -1 ). i3.20)</formula><p>This equation is iterated only once for every measurement.</p><p>During the recursion the polynomial (~'(z) may be unstable which leads to large values of e(t) and ¢p(t).</p><p>This may cause the estimate to diverge. A simple way to overcome this difficulty is to reduce the correction term K(t + 1 )e(t + 1 ) in (3.1) such that the (~(z) polynomial formed from 0(t + 1) is stable. In such a way stability in the computations of e(t) and ~o(t) are guaranteed. This modification may also be advantageous when the RELS method is used. For computation of (3.18) consider 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YV(t)-C(q-1)Y(t).</head><p>We then have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The RML method</head><p>The off-line maximum likelihood method was developed by Astr6m and Bohlin <ref type="bibr" target="#b18">[19]</ref>, for models like (2.1), <ref type="bibr">(2.4)</ref>. The off-line ML method corresponds to minimizing the function 21N/32(t) where e(t) is defined by (2.1). A recursive (approximative) version of this minimization is described in <ref type="bibr" target="#b14">[15]</ref>. Similar algorithms are reported in [14-1, <ref type="bibr" target="#b37">[38]</ref>. RELS and the method recently suggested by Young <ref type="bibr" target="#b21">[22]</ref> can be interpreted as approximations of the RML method.</p><p>The RML method is described by the equations (3.1)- <ref type="bibr">(3.3)</ref> where O=[a,...a.. 6,...b., e,...e.f (3.17)</p><formula xml:id="formula_6">1 1 z(t)=tp(t)= ~,(q_ 1)y(t--1)... ~(q_l)Y(t-n~) 1 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~(q-i u(t--1)'" ~(q-l)u(t-nb) ~(q_l)e(t-1)...~(q_l)e(t-nc)</head><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3.18)</head><p>The prediction error e(t) is computed from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C(q-~)e(t)=,4(q-~ )y(t)-B(q-t)u(t). (3.19)</head><p>To obtain the ~xact prediction error. <ref type="bibr">(3.19)</ref> </p><formula xml:id="formula_7">has to n¢ )~(t)=y(t)-~ ~,yV(t-i). (3.21) i=1</formula><p>This equation can be used for a simple approximate generation of tp(t). When if(t) is computed, the estimate 0(t -1 ) is used in <ref type="bibr">(3.21)</ref>. Approximative ways of computing e(tt and tp(t) are further discussed in <ref type="bibr" target="#b14">[ 15]</ref>.</p><p>A comparison reveals that there are great similarities between the RELS method and the RML method. The difference is that for RML filtered data are used in the ¢p(t) vector. This use of the filter 1/C(q -1) originates from the desired relation tp(t)r=de(t)/dO, which turns out to be most important for the convergence properties, cf. Sections 4 and 5.</p><p>It should be noted that it is easy to apply the RML method also for other model structures. The basic algorithm then remains unchanged but some relevant approximations for computation of e(t) and tp(t)r= -de(t)/dO must be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modifications</head><p>It should be noted that in (3.1)-(3.3) the matrix P(t) is used only as an intermediary step to compute the gain matrix K(t). Substantial computational savings could, for higher dimensional cases, be obtained by updating K(t) directly. That this is possible is proved in <ref type="bibr" target="#b22">[23]</ref>.</p><p>In order to start up the algorithms initial values are needed. If no a priori knowledge is available a common choice is 0(0)= 0, P(0)= ~-1 with ct large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The choice of initial values has no influence asymptotically but the transient behaviour can change significantly.</head><p>There are several recursive identification methods based on stochastic approximation, see e.g. <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. The methods treated above can be simplified to yield algorithms of this type essentially by substituting the matrix P(t) by a scalar p(t), e.g. c/t or 1/tr P(t)-~. This means that the general description of the algorithms is transformed into the single equation O(t+ 1)=O(t)+p(t+ 1)z(t+ 1)s(t + 1). <ref type="bibr">(3.22)</ref> Such a simplification will of course reduce the computation time considerably.</p><p>Algorithms of the type <ref type="bibr">(3.22)</ref> are not considered explicitly in this paper. The uniqueness and convergence properties of such simplified algorithms are analogous to the results for the original algorithms for the methods treated here.</p><p>Several ways for increasing the convergence rate have been suggested in the literature. A common choice is to include a time varying weighting factor 2(t) and let it tend to 1 exponentially. This can be written as (3.4):</p><formula xml:id="formula_8">2(t + 1 ) = 202(0 + (1 -2 o).</formula><p>The number 20 is chosen close to 1, e.g. as 0.99, and so is the initial value 2(0). The time dependent weighting factor 2(t) will cause the influence of the first estimates 0(i), 0(2) on later ones to decrease. On the other hand, since 2(t) tends to 1 the algorithm will 'remember' more and more data and the estimates may converge.</p><p>In many applications it is important to track time-dependent parameters. One way to do so is to use the algoiithm (3.1)-(3.4) with a constant ;t less than 1. Such a choice of the 'forgetting factor" 2 will give a real time algorithm. Then the parameter estimates cannot converge with probability one. However, if the parameters are slowly varying and 2 is chosen close to 1 the forthcoming convergence analysis will describe the behaviour of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TOOLS FOR THE ANALYSIS</head><p>When evaluating recursive identification algorithms, several questions can be raised, like --Do the estimates converge? --If the estimates converge, what is then the limit?</p><p>Is it unique? --What about convergence rate and the accuracy of the estimates?</p><p>For many identification methods it is quite difficult to study these questions analytically. One reason is that the algorithm (3.1)-(3.4) is a nonlinear time varying stochastic difference equation.</p><p>Moreover, in general e(t) in (3.1) depends implicitly on all previous estimates 0(k), k&lt;t. This makes analytical treatment fairly difficult.</p><p>The most common way of studying and comparing different methods no doubt is simulation, which indeed is a natural and valuable tool. However, it is often difficult to be conclusive about properties studied only by simulation. Several items influence the results, such as the realization of the random processes, the choice of examples etc.</p><p>We shall here present analytical results, based on a general technique for the analysis of recursive, stochastic algorithms, see <ref type="bibr" target="#b27">[28]</ref>. For details of this technique, as well as for verification of its applicability to the algorithm (3.1)-(3.4), we refer to <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>.</p><p>The analysis is based on the following ordinary differential equation (ODE), associated with the algorithm (3.1)--(3.4)  We shall now proceed to illustrate how these results can be applied to the analysis of the recursive identification methods under discussion. Only some simple cases will be treated here. A more Complete summary of the results that can be obtained in this way is given in the next section.</p><formula xml:id="formula_9">dO(~) -R-1 (z)f (0(~)) d~ (4.1) dR(z) --=G(O(z))-R(z</formula><p>We thus have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d ," g(t:O)lo= -d~ J (O)lo=Oo=E=-U:Oo)~</head><p>Oo"</p><p>For RLS, RIV and RML (cf. <ref type="bibr" target="#b18">[19]</ref>) we further have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Possible convergence points</head><p>The set of convergence points can be characterized using the results (4.3), <ref type="bibr">(4.4)</ref>. We may start by examining if the true parameter vector 0o belongs to this set. Since</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>j (Oo ) = E~.(t;Oo)g(t;Oo)</head><p>and for all methods except RIV g(t;0o) = e(t) it is easy to see that for these methods ? ~--b g(t;0) = -Oft;O) r and thus</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d f(o)lo=Oo = -E2(t;Oo)O(t;Oo) T = -G(Oo). dO</head><p>Therefore for RLS, RIV and RML dy</p><formula xml:id="formula_10">G(Oo)-'-~-~(O)lo=oo = -I (4.7) f(0o)=0.</formula><p>For the RIV method we have ~(t;0o) =H(q-l)e(t). It is therefore required that the process operates in open loop in order to assure that f(0o)=0. However, for the other methods (RLS, RGLS, RELS and RML) the result holds also for closed loop operations of various kinds. Moreover  Some caution has to be taken in the interpretation of (d/d0)g(t ;0) for adaptive systems. Let the regulator be written as u(t)= -F(O(t );q-1)y(t ). (The case when the regulator depends explicitly on also older estimates is easily obtained by generalization.) Then the stationary residual can be written symbolically as g(t;O, F(O)). It obviously depends on 0 both explicitly and implicitly via the regulator. Thus we have, symbolically,</p><formula xml:id="formula_11">dg(t;O) =-~ g(t;O,F(O)) dF(0) +~-~gtt;O,F(O)) ~-~ . (4.6)</formula><p>Consider now evaluation at 0 = 0 0. Then ~.(t;0o, F(Oo)) = e(t) is independent ofF(0). Thus the second term in (4.6) can be dropped. Moreover, it follows that the first term of (4.5) is zero for RLS, RELS and RML since e(t) is independent of the past. This term is also zero for RIV if the process operates in open loop.</p><p>which clearly is stable.</p><p>It can also be investigated whether there are other points that satisfy (4.3) and (4.4) for the different methods. The results are summarized in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convergence</head><p>To prove convergence it is required to show global stability of the differential equation (4.1). To do that we try the function</p><formula xml:id="formula_12">V(O, R ) = ½Eg(t ;0) 2 (4.8)</formula><p>as a Lyapunov function for (4.1). Along solutions of (4.4), the time derivative is As remarked above, for RLS and RML, with non-adaptive feedback.</p><formula xml:id="formula_13">d d---0 g(t;0) = -O(t;O) r = -~(t;O) r which gives I2(0, R)= --f(O)r R -if(O).</formula><p>If 0o is the only value that satisfies f(0)=0, then consequently V guarantees the global stability of (4.1) for RLS and RML.</p><p>We therefore find that the estimates converge with probability one to 0o for RLS and RML under the assumption that 0o is the only solution off(0) =0. This clearly is the case for RLS. In general V may have several local minima for RML, and our results guarantee that O(t) converges w.p.1 to one such local minimum of V. This is the best one can hope for, since even for off-line ML only convergence to a local minimum of V can be guaranteed for the numerical minimization routine.</p><p>When investigating convergence for the RGLS method we first note that the associated differential equation will consist of two coupled equations. Simple calculations will show that V(O1,02, R 1, R2 ) = ½Ee(t)2 can be used as a Lyapunov function. Along the solutions of (4.1) its time derivative is negative, and zero only in stationary points of the ODEs. Thus convergence to 0o is achieved if 0o is a unique solution of the ODE similarly to the result for the RML method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Numerical solution of the differential equation</head><p>Since the trajectories of (4.1) may be interpreted as the asymptotic paths of the algorithms (3.1)-(3.4), numerical solution of the differential equation can be used as a valuable complement to simulation of the algorithm. Some examples of this will be given in Section 6.</p><p>Numerical solution of (4.1) reveals the asymptotic behaviour of the estimates. It is often difficult to decide from a simulation whether it is a case of very slow convergence or lack of convergence, or if a certain feature is something inherent or just random influence.</p><p>• In addition the trajectories may yield insight into the global stability properties of(4.1), which may be difficult to study analytically, e.g. for the RELS method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONVERGENCE PROPERTIES</head><p>The results when applying the analysis sketched in Section 4 to the different methods are summarized below. It is generally assumed that the system is given by (2.5) and the model structure by (2.1). The interpretation of H(q -1 ) and/~(q-l) for the different methods is given in Section 2. For technical reasons only points for which G(O) is nonsingular are considered.</p><p>When comparing the methods one essential difference is that they are connected to different model structures.</p><p>Below we shall consider the different methods in the following respects --(i) Is 0o a unique solution of f(0)=0? --(ii) Is 0o a unique solution of {f(0)=0, and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G(O)-1.('(0) stable} --(iii) Does the estimate O(t) converge to 0o?</head><p>The reasons for separating points (i) and (ii) are twofold. The equation f(0)=0 determines simultaneously the uniqueness properties of the cor-responding non-recursive methods. For the RLS and RIV method the analysis is trivial. Basic references for our analysis off(0)=0 is <ref type="bibr" target="#b30">[31]</ref> for the RGLS method, <ref type="bibr" target="#b15">[ 16]</ref> for the RELS method and <ref type="bibr" target="#b15">[16]</ref> as well as <ref type="bibr" target="#b31">[32]</ref> for the RML method. Tile other reason for the separation is that the uniqueness properties of the solution off(0)=0 are crucial in the proofs of convergence.</p><p>It should be stressed that we perform the analysis under the following three general assumptions, cf. Section 2.</p><p>--The system is identifiable in the sense that it can be found in the considered model structure.</p><p>--The order of the model polynomials coincides with the respective polynomials in the system description (i.e. the order of the model is equal to the order of the true system).</p><p>--Only points for which the matrix G(O) is nonsingular are examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The RLS method</head><p>Assume that G(O) is nonsingular for all 0. This is the case e.g. for open loop operation with a persistently exciting input or for closed loop operation with a constant regulator of high order or a suitably timevarying regulator. The system may operate in closed loop. We have (i) The solution 0=0 o off(0)=0 is unique.</p><p>(ii) G(Oo)-lf'(Oo) is stable.</p><p>(iii) The estimate O(t) converges to 0 o w.p.1.</p><p>As can be seen from the following calculations the results depend heavily on the nonsingularity of G(O). Also note that t~(t; 0) is independent of 0 if the ~)stem is not controlled in an adaptive way and</p><formula xml:id="formula_14">0 =f(0) = EYp(t;O)g(t ;0) = E~p(t ;O)[y(t )-7p(t ;0)r0]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= E~ (t;O)[e(t) + (o (t;O)r[O o -0}]</head><p>=6(0)(00-0).</p><p>(5.1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The RIV method</head><p>When analysing the RIV method it is essential to assume open loop operation. Assume further that G(O) is nonsingular for all 0. Then we have (i) The solution 0 =0o is a unique solution of J(0) =0.</p><p>(ii) The matrix G(Oo)-~f'(Oo) is stable.</p><p>(iii) Assume that the instrumental variables are generated according to (3.10), <ref type="bibr">(3.11)</ref>. Then (3.9) is almost always fulfilled and when this is the case O(t) converges to 0o. w.p.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part (i) is proved by a calculation similar to (5.1) namely) 0 =f(0) =E~,(t;O)g{t;O) = E-{t;O)[y(t)-(o(t;O)rO] = E,~(t; O)[H (q -~ )e(t) + Cp (t; O)r(Oo -0)]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=E£(t;O)" H(q-t )e(t)+ G(O)(Oo-O).</head><p>(5.2)</p><p>In order to guarantee that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ES{t;O) . H (q-~ )e(t )=0</head><p>it is effectively necessary to require either that the process operates in open loop or that H(q-1) = 1.</p><p>Otherwise some elements of the vector z(t) will generally be correlated with the noise H(q-~)e(t). Part (iii) follows since for this kind of instrumental variables the RIV algorithm is equivalent to the off-line IV method. The latter gives consistent estimates. Note that it is an open question whether the RIV method with z(t) generated according to (3.10), (3.12) will give convergence or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The RGLS method</head><p>(i) Assume that the process operates in open loop. If the signal-to-noise ratio is very large then 0 =0o is a unique solution off~(0)=0. On the other hand, if this ratio is small then there are in most cases also other solutions.</p><p>(ii) The linear system obtained by linearization of (4.1) is asymptotically stable if linearization is performed around 0o. This is true also for closed loop operation. There are cases when also linearization around other solutions off~(0)=0 gives an asymptotically stable system.</p><p>(iii) Assume that the feedback is not adaptive and that f~(0)=0(i= 1,2) has a unique solution 0=0o. Then the estimate O(t) converges to 0o w.p.1.</p><p>Part (i)is proved in <ref type="bibr" target="#b30">[31]</ref> and part (ii)in <ref type="bibr" target="#b15">[16]</ref>. The proof of part (iii) follows the ideas of <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The RELS method</head><p>(i) The equation f(0)=0 has a unique solution 0 = 0o if any of the following conditions is satisfied (a) The system is an ARMA process, i.e. given by G(Oo)-lJ'(Oo) are -1 (multiplicity no) and The proofs of the assertions can be found as follows: 0)(a) in <ref type="bibr" target="#b15">[16]</ref>, (i)(b) in <ref type="bibr" target="#b28">[29]</ref>, (ii)(a) in <ref type="bibr" target="#b24">[25]</ref>, (ii)(b) in <ref type="bibr" target="#b32">[33]</ref> and (iii) in <ref type="bibr" target="#b28">[29]</ref>. Note that the condition in (iii) is in particular fulfilled for all first order polynomials C(z).</p><p>The method proposed by Young <ref type="bibr" target="#b21">[22]</ref>, has convergence properties similar to the RELS method. It has e.g. been shown, <ref type="bibr" target="#b24">[25]</ref> that for ARMA processes the eigenvalues of G(Oo)f'(Oo) are where {/ai} are the roots of C(z)=0, and -1/C(2[-1). i=1 ..... na where {2i} are the roots of A(z)=0. Thus also this method does not give consistent estimates for all ARMA processes. Note,however, that Youngproposestousethemethod only in conjunction with the RIV method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.5</head><p>The RML method (i) The equation J(0)=0 has a unique solution 0 = 0 if the system is an ARMA process.</p><p>(ii) The matrix G(Oo)J'(Oo) is always asymptotically stable.</p><p>(iii) Assume that the feedback is not adaptive and that 0o is a unique solution off(0)=0. Then O(t) converges w.p.1 to 0o. In fact O(t) always converges to a local maximum of the log likelihood function.</p><p>Part (i) is proved in <ref type="bibr" target="#b31">[32]</ref>, part (ii) and (iii) in <ref type="bibr" target="#b15">[16]</ref>. Note that the results in points (ii) and (iii) are valid also for general model structures.</p><p>There are also available some more specialized results on the uniqueness properties of the RELS and the RML methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A(q-l )y(t)=C(q-1)e(t).</head><p>(b) The C-polynomial in (2.5), (2.8) is positive real, i.e.</p><p>Re C(ei')&gt; 0, -n_&lt;to_&lt;n.</p><p>(ii) (a) For an ARMA process the eigenvalues of 6. NUMERICAL ILLUSTRATIONS Simulation of identification methods is no doubt the most common basis for comparisons. At recent IFAC meetings test cases for identification methods have been used and discussed, see <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. We shall in this section present some simulations of the methods under discussion as well as some trajectories of the associated differential equations. The material is intended as an illustration of the analytical results of the previous sections. It can, however, also be used for direct comparisons, but then it should be kept in mind that simulation studies seldom give a fair basis for this. One reason is that the methods behave differently for different systems. A very important restriction for the current results is that there are different noise models associated with the various methods. This must be kept in mind when studying the tables. The RELS and RML methods are favoured by the choice of system. It should be noted that the theoretical analysis in Section 5 can be carried out for an arbitrary choice of system. However, the results presented in Section 5 are derived under the assumption that the structure of the system fits precisely the model structure used.</p><p>In all the examples the input signal was chosen as a PRBS, which can be considered as a good approximation of white noise. The following systems have been used in the examples:</p><p>.of,: y(t)-O.8y(t-1 )= 1.0u(t-1)+e(t) + 0.7e(t -1 ) (</p><p>-0.8y(t -1 ) = 1.0u(t -1 ) + 1 + 0.7q-1 ~s: y(t)-1.5y(t-1)+0.7y(t-2)= 1.Ou(t-1) + 0.5u(t -2) + e(t) -e(t -1 ) + 0.2e(t -2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e(t)</head><p>(</p><p>The white noise e(t) was obtained from a random number generator producing gaussian N(0, 1 ) variables. In the ODEs the input signal was chosen as white noise for all the cases. Objections can be raised against the choice of only low order systems for the simulations. However, the orders are sufficient for illustrating the theoretical convergence results and the relations between the estimation algorithms and associated ODEs. Also for the comparisons of the different methods simulation of the systems above will indicate clear differences in obtainable accuracy for the considered methods.</p><p>It may be appropriate to give some details of how the simulations have been performed. The initial values were always chosen as 0(0)=0, P(0)= 100. If nothing else is said the forgetting factors ;t have been identical to 1 (i.e.)40)= 1, 2o = 1). All methods are performed with truly on-line algorithms thus processing the data once and without iterations, The Cram&amp;-Rao lower bounds were calculated as in <ref type="bibr" target="#b35">[36]</ref>.</p><p>The signal to noise ratio is the quotient between the variance of the deterministic output B(q-1)/A(q-l )u(t) and the variance of the stochastic output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H(q-1 )/A(q-~ )e(t ).</head><p>Several simulation studies have been performed, in addition to those described explicitly in the example. For the RLS and RGLS methods we have also studied systems of the same structure as the models used. For the RLS method in a general case a better fit can be expected if a high order model is used. Therefore the RLS method was applied also with a fifth order model.</p><p>The RI.V method is used in its 'basic' version and without combination with the RELS method. The effect of the delay z described in Section 3 was also studied. The results obtained show only minor variations with r. The figures below refer to the case z---2.</p><p>The RGLS method was applied without filtering during the first 50 samples. This means that then an ordinary RLS algorithm was used. Our experience is that this choice will improve the transient behaviour considerably. For the RGLS method it was also studied how the number n c of ('-parameters influenced the accuracy. In the examples it was found that a large n, did not give improved accuracy of the/] and/)-parameters. In the examples below the choice na = nb = nc has been made. Example 6.1. A first order example. Consider the system (6.1). The amplitude of the input signal was chosen so that the signal to noise ratio is 1. The system was identified with all the methods described in Section 2. For all the methods a first order model structure was used. According to the theory we expect the estimates for the RLS and RGLS to be biased.</p><p>In Fig. <ref type="figure">1</ref> we show the results of the simulation together with computed solutions of the corresponding differential equations.</p><p>The curves in Fig. <ref type="figure">1</ref> confirm that the solution of the ODE describes the asymptotic behaviour of the recursive algorithms. In particular we see that RLS and RGLS and no other method give biased estimates. Note that it is difficult to be conclusive in this respect by just regarding the simulations. In general the estimates t~ and ~ are close to their final values after 100 samples. The ? estimate converges considerably slower. The authors' experience is that all the methods converge slower for higher order models and that 6i estimates in general converge slower than fii,/;i estimates.</p><p>Example 6.21 Several convergence points for the RGLS method. Consider the system (6.2) identified with the RGLS method in a first order model structure (no = n~ = nc = 1 ). The signal-to-noise ratio is assumed to be 1. A phase plane diagram for the ODE was computed. It is given in Fig. <ref type="figure">2</ref>. It clearly illustrates that there are two possible convergence points, one of which is 0o. This is in accordance with subsection 5.3. Example 6.3. Accuracy for a.first order example.</p><p>Consider the system ,9°~, (6.1). In order to investigate the accuracy, the different identification methods were all applied to 10 different realizations. The mean and the standard deviation of the parameter estimates were then computed from the obtained result. The input signal was chosen so that the signal-to-noise ratio is 1. The result is given in Fig. <ref type="figure" target="#fig_9">3</ref>. It can be seen that RLS and RGLS produce biased estimates. Further the estimates obtained with RML seem to approximately achieve the Cramer-Rao lower bound for a large number of samples.</p><p>Example 6.4. Accuracy for a second order example. We now repeat the computation made in Example 6.3 but for the second order system i6.3).</p><p>The results obtained are given in Fig. <ref type="figure" target="#fig_2">4</ref>. Here we have a signal to noise ratio of 10. Again we see how RLS and RGLS produce biased estimates and also that RML seems to give asymptotically efficient estimates.</p><p>Example 6.51 Accuracy tests. Scalar measures. The two systems 5/1, (6.1) and 5/3 , (6.3) were considered for further experiments. Identification was performed for ten realizations with a signal-tonoise ratio of 1 and to ten realizations with this ratio equal to 10. For all models three different scalar measures of the accuracy were computed. These are discussed further in <ref type="bibr" target="#b36">[37]</ref>. Consider therefore V 1 (0)= Ee(t) 2 (6.4) which describes how good prediction of future data can be expected using the obtained model. It is easy to see that V~ (0) &gt; 1, where equality is obtained FIG. <ref type="figure">1</ref>. Simulations and solutions of the ordinary differential equations for the first order system 6e~, (6.1). The true values are given by dashed or thin lines.</p><p>For the RIV method two versions were tried. On the second row the bootstrap choice (3.12) is shown, while the third corresponds to lagged inputs, (3.13). For the RELS and RML methods, 2(0)=0.95, 20 =0.99.</p><p>only for a perfect model (i.e. when the estimates and the true values are equal). It is in fact possible to show that V l (0).~ 1 +½(0 -0o)Tp -1(0-0o)</p><p>where P is the Cram6r-Rao lower bound on the variance of the estimates. The second scalar measure is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I([~(q-')</head><formula xml:id="formula_17">B(q-')'~ . -]2 V2(O)=E -~-z , ) -~-zi~luU) J • (6.5)</formula><p>The measure was evaluated with u(t) as white noise.</p><p>This function expresses the deviation between the impulse responses of the model and the system. Clearly I/2 (0) &gt; 0. Equality is at hand precisely when A and B-parameters of the model coincide with the true ones.</p><p>The third measure will be denoted by V3(0). Assume that a minimum variance strategy, cf. <ref type="bibr" target="#b17">[18]</ref>, is constructed from the model obtained from the identification and that this regulator is applied to the true system. Then V3 (0) will denote the variance of the output signal of the closed loop system. Due to parameter deviations between the system and the model we have I/3 (0)&gt; 1. Equality is obtained if the model coincides with the system. However, also other models will give the (true) minimum variance strategy and hence equality.</p><p>The averages of the quantities Vl(0), 1/2(0) and I"3(0) over ten different runs are given in Tables <ref type="table" target="#tab_7">1</ref> and<ref type="table" target="#tab_8">2</ref>. Thenotation '*' means that for at least one realization the closed loop system became unstable. ~.</p><p>-065 I.</p><p>- The notation'*' stands for the case when the measures have not been evaluated. The RLS method was tried also with a fifth order model. The figures for RLS when the criteria V1 and V3 were used are put in parentheses.</p><p>The reason is that due to the model structu re used there is no chance to give as good values as produced by RELS and RML. However, a comparison of the first and fifth order models are of interest. Several conclusions can be drawn from the figures in the tables. The increase of the model order for the RLS method gives a considerably better fit as expected. For the second order system Sea sometimes instability of the closed loop system was obtained when 1/3 was computed which explains some of the omitted figures in Table <ref type="table" target="#tab_8">2</ref>. It should be stressed, however, that we have used the RIV method quite straightforwardly. Then the first estimates often become bad and deteriorate the following ones. One common way to improve the RIV method is to use the RLS method for the first 50 samples, say, and then switch over to the RIV algorithm. Independently of which system we choose and which criterion we use we find in our cases that the RML method gives the best accuracy. It should be noted that the investigation here is based on rather long  samples. For short or very short samples some of the other methods may be more advantageous due to a faster convergence rate.   Comparison between the different methods: RLS, RGLS, RIV (z(t) according to (3.12)) RELS (2(0)=95, io=0.99), RML (1(0) --0.95, 1o=0.99) and the Cram6r-Rao lower bound (in order from left to right). The length of the lines is twice the standard deviation.</p><p>The system S~ 3 is given by (6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>Recursive identification methods can be analysed with theoretical analysis and with simulation. In this paper both ways have been used for examination of the recursive least squares method, the recursive instrumental variable method, the recursive generalized least squares method, the recursive extended least squares method and the recursive maximum likelihood method.</p><p>By the theoretical analysis it has here been possible to establish convergence under weak conditions for several of the considered methods. It is also possible to use the theory for construction of counter-examples to general convergence of other methods.</p><p>Simulations have been used partly for illustrating the results, partly for investigation of the accuracy of the models obtained with the different methods. It is difficult to draw too general conclusions from simulations of a few systems. However, the chosen cases indicate that the recursive maximum likelihood method is the more accurate one, at least for long samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>dl ... d,fl • ../~,~]r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>0=[,~,...,~. a bl.../;.~ ~,..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 )</head><label>4</label><figDesc>has all eigenvalues in the left half plane --global asymptotic stability of a stationary solution (0", G(0*)) of (4.1) implies that O(t)-,O* to.p.1 as t~oo for the algorithm --the trajectories of (4.1) can be interpreted as the asymptotic paths of the algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>~ J (O)=E d-~ Z(t;O)~(t;O)+ E~(t;O)-d-~ g(t:O).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1</head><label></label><figDesc>i=1 ..... na C(;.? l ) where 21 are the roots of A(z)=0. (Thus there are ARMA processes making this matrix unstable.) (b) Similarly there exist also systems with input such that matrix G(Oo)-~f'(Oo) is unstable. (iii) If the transfer function (1/C(z)) -½ is positive real(i.e, if Re -C(ei~) ~ &gt;0; -~&lt;~o_&lt;rr) then O(t) converges w.p.1 to a solution off(0)=0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>FIG. 3 .</head><label>3</label><figDesc>FIG.3. Comparison between the different methods: RLS, RGLS, RIV {z(t) according to (3.12)) RELS (2(0) = 0.95, 20 = 0.99), RML (2(0)=0.95, 2o=0.99) and the Cramer-Rao lower bound (in order from left to right). The length of the lines is twice the standard deviation. The system ,~ is given by 16.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>FIG. 4. Comparison between the different methods: RLS, RGLS, RIV (z(t) according to (3.12)) RELS (2(0)=95, io=0.99), RML (1(0) --0.95, 1o=0.99) and the Cram6r-Rao lower bound (in order from left to right). The length of the lines is twice the standard deviation.The system S~ 3 is given by (6.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>z(t)= [ -x(t -1 )...-x(t -n=)u(t -1</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">r=R, R a non-singular</cell></row><row><cell>N~ot~</cell><cell>I=1</cell></row><row><cell>matrix</cell><cell></cell><cell>(3.9a)</cell></row><row><cell cols="2">. (ii) hm ~ ~ z(t)H(q-l)e(t)=O. 1 N</cell><cell>(3.9b)</cell></row><row><cell>N~oc</cell><cell>t=l</cell></row><row><cell cols="2">One possibility is to take</cell></row></table><note><p>)... u(t</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>-nb )]r</head><label></label><figDesc></figDesc><table><row><cell>z(t)=[u(t-1)...u(t-n~-nb)] r.</cell><cell>(3.13)</cell></row><row><cell>(3.10)</cell><cell></cell></row><row><cell>where the signal x(t) is obtained by filtering the</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 1 .</head><label>1</label><figDesc>COMPARISON OF THE DIFFERENT METHODS*</figDesc><table><row><cell></cell><cell></cell><cell>Criterion</cell><cell></cell><cell></cell><cell>Criterion</cell><cell></cell></row><row><cell></cell><cell>v,</cell><cell>v~</cell><cell>vs</cell><cell>v,</cell><cell>v~</cell><cell>v3</cell></row><row><cell>Signal-to-noise ratio</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RLS</cell><cell>(1.445)</cell><cell>0.145</cell><cell>(1.420)</cell><cell>(1.477)</cell><cell>0.0047</cell><cell>(1.475)</cell></row><row><cell>RLS (5th order model)</cell><cell>(1.018)</cell><cell>0.021</cell><cell>(1.018)</cell><cell>(1.017)</cell><cell>0.0025</cell><cell>(1.017)</cell></row><row><cell>RIV</cell><cell>--</cell><cell>0.012</cell><cell>--</cell><cell>--</cell><cell>0.0011</cell><cell>-</cell></row><row><cell>RGLS</cell><cell>--</cell><cell>0.015</cell><cell>--</cell><cell>--</cell><cell>0.0056</cell><cell>-</cell></row><row><cell>RELS</cell><cell>1.008</cell><cell>0.0093</cell><cell>1.006</cell><cell>1.005</cell><cell>0.0017</cell><cell>1.004</cell></row><row><cell>RML</cell><cell>1.002</cell><cell>0.0064</cell><cell>1.001</cell><cell>1.002</cell><cell>0.0010</cell><cell>1.001</cell></row></table><note><p>*The system is Y/I, (6.1). The figures shown are averages of ten runs of 2000 samples each.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 2 .</head><label>2</label><figDesc>COMPARISON OF THE DIFFERENT METHODS T The system is Y'3, (6.3). The figures are averages over ten runs of 2000 samples each.</figDesc><table><row><cell></cell><cell></cell><cell>Criterion</cell><cell></cell><cell></cell><cell>Criterion</cell><cell></cell></row><row><cell></cell><cell>v,</cell><cell>v~</cell><cell>v3</cell><cell>~G</cell><cell>v2</cell><cell>v3</cell></row><row><cell>Signal-to-noise ratio</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RLS</cell><cell>(2.647)</cell><cell>5.017</cell><cell>(*)</cell><cell>(1.752)</cell><cell>2.061</cell><cell>*</cell></row><row><cell>RLS (5th order model)</cell><cell>(1.211)</cell><cell>0.560</cell><cell>(*)</cell><cell>(1.067)</cell><cell>0.050</cell><cell>(1.066)</cell></row><row><cell>RIV</cell><cell>--</cell><cell>0.420</cell><cell>--</cell><cell>--</cell><cell>0.031</cell><cell>--</cell></row><row><cell>RGLS</cell><cell>--</cell><cell>3.378</cell><cell>--</cell><cell>--</cell><cell>0.329</cell><cell>--</cell></row><row><cell>RELS</cell><cell>1.099</cell><cell>0.312</cell><cell>*</cell><cell>1.018</cell><cell>0.037</cell><cell>1.009</cell></row><row><cell>RML</cell><cell>1.033</cell><cell>0.087</cell><cell>*</cell><cell>1.006</cell><cell>0.010</cell><cell>1.004</cell></row><row><cell>~'</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">System identification--a survey</title>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eykhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="123" to="162" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparison of six on-line identification and parameter estimation methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Isermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knepo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sieberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="81" to="103" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of parameter estimation algorithms for discrete systems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Engng Sci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="539" to="547" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SARIDIS: Comparison of six on-line identification algorithms</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date>19741</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Critical evaluation of on-line identification methods</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Sznha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEE</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="1153" to="1158" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recursive approaches to time series analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Inst. Math. Applic</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="209" to="224" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A method for estimating discrete time transfer functions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Mayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Computer Control, Second UKAC Control Cont&apos;ention. The University of Bristol</title>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identification of linear discrete time system using the instrumental variable method</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Polak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. AC</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="707" to="718" />
			<date>19671</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The use of linear regression and related procedures for the identification of dynamic processes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th IEEE Symposium on Adaptive Processes</title>
		<meeting>7th IEEE Symposium on Adaptive esses</meeting>
		<imprint>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Recursive generalized least squares procedure for on-line identification of process parameters, lEE Proc</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hasttngs-James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Sage</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="2057" to="2062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generalized least squares estimation of parameters of a dynamic model. Preprints 1st IFAC Symposium on ldent!]~cation in Automatic Control Systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<pubPlace>Prague</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PANUSKA: A stochastic approximation method for identification of linear systems using adaptive filtering</title>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. JACC</title>
		<meeting>JACC</meeting>
		<imprint>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PANUSKA: An adaptive recursive least squares identification algorithm</title>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Syrup. on Adaptive Processes, Decision and Control</title>
		<meeting>IEEE Syrup. on Adaptive esses, Decision and Control</meeting>
		<imprint>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FURHT: New estimator for the identification of dynamic processes</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBK Report. Institut Boris Kidri</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="1973">1973</date>
			<pubPlace>Belgrade, Yugoslavia</pubPlace>
		</imprint>
	</monogr>
	<note>Vin~a</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">M: An on-line algorithm for approximate maximum likelihood identification of linear dynamic systems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">O[)</forename><surname>Erstr(</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. of Automatic Control. Lund Inst. of Tech</title>
		<imprint>
			<biblScope unit="volume">7308</biblScope>
			<date type="published" when="1973">1973</date>
			<pubPlace>Lund, Sweden</pubPlace>
		</imprint>
	</monogr>
	<note>Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparative study of recursive identification methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jderstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gustavsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. of Automatic Control. Lund Inst. of Tech</title>
		<imprint>
			<biblScope unit="volume">7427</biblScope>
			<date type="published" when="1974">1974</date>
			<pubPlace>Lund, Sweden</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
	<note>t</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the estimation of transfer function parameters of process and noise dynamics using a single stage estimator</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Talmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J W</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Boom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IFAC Symposium on Identification and System Parameter Egtimation</title>
		<meeting>3rd IFAC Symposium on Identification and System Parameter Egtimation<address><addrLine>The Hague/Delft</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1973">{1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename></persName>
		</author>
		<title level="m">kSTROM: Introduction to Stochastic Control Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Numerical identification of linear dynamic systems from normal operating records. IFAC Symposium on Se!/-Adaptive Systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bohlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Also in Theory of Se(f-Adaptive Control Systems</title>
		<meeting><address><addrLine>Teddington, England; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Strongly consistent estimation by the introduction of strong instrumental variables</title>
		<author>
			<persName><forename type="first">B</forename><surname>Finigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. AC</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="825" to="830" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SODERSTR6M: Convergence of identification methods based on the instrumental variable approach</title>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="685" to="688" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Some observations on instrumental variable methods of time-series analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comro</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="593" to="612" />
			<date>19761</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast calculations of gain matrices for recursive estimation schemes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Falconer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A bootstrap method for the statistical estimation of model parameters</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">738</biblScope>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive prediction and recursive estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept.ofAutomaticControl, Lund lnst.ofTech</title>
		<imprint>
			<date type="published" when="1977">1977</date>
			<pubPlace>Lund, Sweden</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Report 1013</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The use of stochastic approximation to solve the system identification problem</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sagrtson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. AC</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="563" to="567" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A generalized pseudoinverse algorithm for unbiased parameter estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1103" to="1109" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Analysis of recursive stochastic algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. AC</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="551" to="575" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On positive real functions and the convergence of some recursive schemes</title>
		<author>
			<persName><forename type="first">U</forename><surname>Ljung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. AC</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="539" to="551" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Theory and applications of self-tuning regulators</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>~strom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Borisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wittenmark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">476</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DERSTROM: Convergence properties of the generalized least squares identification method</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="617" to="626" />
			<date type="published" when="1974">6. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">~STR6M and T. S6DERSTROM: Uniqueness of the maximum likelihood estimates of the parameters of an ARMA model</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. AC</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="769" to="773" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Counterexamples to general convergence of a commonly used identification method</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Soderstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gustavsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. AC</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="643" to="652" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Test-cases for evaluation of different identification methods using simulated processes (test case A)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Isermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IFAC Symposium on hlentfieation</title>
		<meeting>3rd IFAC Symposium on hlentfieation</meeting>
		<imprint>
			<publisher>The Hague</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tesbcase C for comparison of different identification and parameter estimation methods using simulated processes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lsermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blessing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th IFAC Congress</title>
		<meeting>6th IFAC Congress<address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On the achievable accuracy in identification problems. Preprints. 1st I F A C Symposium on Identification in Automatic Control Systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>~strom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<pubPlace>Prague</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the accuracy of identification and the design of identification experiments</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gustavsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. of Automatic Control, Lund Inst. of Tech</title>
		<imprint>
			<biblScope unit="volume">7428</biblScope>
			<date type="published" when="1974">1974</date>
			<pubPlace>Sweden</pubPlace>
		</imprint>
	</monogr>
	<note>Lurid</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A recursive(on-line)maximum likelihood identification method</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gertlep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csbanfasz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. AC</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date>19741</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
