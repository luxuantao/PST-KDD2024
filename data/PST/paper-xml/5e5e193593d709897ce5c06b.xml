<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Resource Cross-Lingual Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-22">22 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">M</forename><surname>Saiful</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<email>srjoty@</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Salesforce Research Asia</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prathyusha</forename><surname>Jwalapuram</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Resource Cross-Lingual Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-22">22 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1911.09812v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, neural methods have achieved state-of-the-art (SOTA) results in Named Entity Recognition (NER) tasks for many languages without the need for manually crafted features. However, these models still require manually annotated training data, which is not available for many languages. In this paper, we propose an unsupervised cross-lingual NER model that can transfer NER knowledge from one language to another in a completely unsupervised way without relying on any bilingual dictionary or parallel data. Our model achieves this through word-level adversarial learning and augmented fine-tuning with parameter sharing and feature augmentation. Experiments on five different languages demonstrate the effectiveness of our approach, outperforming existing models by a good margin and setting a new SOTA for each language pair.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Named-entity recognition (NER) is a tagging task that seeks to locate and classify named entities in a text into predefined semantic types such as person, organization, location, etc. It has been a challenging problem mainly because there is not enough labeled data for most languages to learn the specific patterns for words that are part of a named entity. It is also harder to generalize from a small dataset since there can be a wide and often unconstrained variation in what constitutes names. Traditional methods relied on carefully designed orthographic features and language or domain-specific knowledge sources like gazetteers.</p><p>With the ongoing neural tsunami, most recent approaches use deep neural networks to circumvent the expensive steps of designing informative features and constructing knowledge sources <ref type="bibr" target="#b7">(Lample et al. 2016;</ref><ref type="bibr" target="#b7">Ma and Hovy 2016;</ref><ref type="bibr" target="#b12">Strubell et al. 2017;</ref><ref type="bibr" target="#b10">Peters et al. 2017;</ref><ref type="bibr" target="#b1">Akbik, Blythe, and Vollgraf 2018;</ref><ref type="bibr" target="#b4">Devlin et al. 2018)</ref>. However, crucial to their success is the availability of large amounts of labeled training data. Unfortunately, building large labeled datasets for each new language of interest is expensive and time-consuming and we need fairly educated manpower to do the annotation.</p><p>As many languages lack suitable corpora annotated with named entities, there have been efforts to design models for cross-lingual transfer learning. This offers an attractive solution that allows us to leverage annotated data from a source language (e.g., English) to recognize named entities in a target language (e.g., German). One possible way to build such a cross-lingual NER system is to encode knowledge about the target language as constraints to regularize the training, which has been tried before for part-of-speech (POS) tagging <ref type="bibr" target="#b4">(Ganchev et al. 2010</ref>). However, this would require extensive knowledge of the target language.</p><p>Another way is to perform cross-language projection. Most projection-based methods use a parallel sentence-aligned bilingual corpus, or a bi-text. For example, <ref type="bibr" target="#b17">Yarowsky et al. (2001)</ref> use an English NER tagger on the English side of a bi-text, then project its token-level predictions to the target side, and finally train a NER tagger on them. <ref type="bibr" target="#b14">Wang and Manning (2014)</ref> project model expectations and use them as constraints rather than directly projecting labels, to better transfer information and uncertainty across languages. Joint learning of NER tags and cross-lingual word alignments has also been proposed <ref type="bibr" target="#b15">(Wang, Che, and Manning 2013)</ref>. Overall, all of these methods require a bi-text with NER tags on one side, which is not typical for low-resource languages. Sentence-aligned parallel corpora are often not available for low-resource languages, and building such corpora could be even more expensive than building the NER dataset.</p><p>It is only recently that researchers have proposed crosslingual NER models for low-resource languages. <ref type="bibr" target="#b7">Lin et al. (2018)</ref> propose a multi-lingual multi-task architecture to develop supervised NER models with minimal amount of labeled data in the target language. <ref type="bibr" target="#b16">Xie et al. (2018)</ref> propose an unsupervised transfer model by projecting source language tags into the target language through word-to-word translation using the unsupervised word translation model of <ref type="bibr" target="#b4">Conneau et al. (2017)</ref>. However, this approach has several key limitations. First, for each target language, they need to translate from source to target and learn a brand new NER model. For this, they have to pre-compute a translation dictionary based on nearest neighbour search over the vocabulary items, which is often computationally expensive (e.g., fasttext-enwiki has âˆ¼2M items). This makes it difficult to scale time-and memory-wise. Furthermore, this often requires (as they do) a target language labeled development set to select the best model. Therefore, although the translation process is unsupervised, their NER model is not purely unsupervised.1 Also, the training of the target language NER model is done without any knowledge about the source.</p><p>Comprehensible Output (CO) theory (Swain and Lapkin 1995) of Second-Language Acquisition (SLA) states "learning takes place when a learner encounters a gap in his or her linguistic knowledge of the second language. By noticing this gap, the learner becomes aware of it and may be able to modify his output so that he learns something new about the language". In other words, in SLA, the first language plays an important role in learning the second language.</p><p>In this paper, we propose an unsupervised (or zeroresource) cross-lingual neural NER model, which allows one to train a model for a target language, using labeled data from a source language. Inspired by the CO theory of SLA, we propose to learn the second language task under the supervision of the first language as opposed to completely forgetting about the first language. Thus, rather than doing word-or phrase-based translation <ref type="bibr" target="#b16">(Xie et al. 2018;</ref><ref type="bibr" target="#b8">Mayhew et al. 2017)</ref>, we choose to learn a base NER model on the source language first, and then tune the base model further in the presence of both languages to maximize the objective.</p><p>Our framework has two encoders -one for the source language and the other for the target. Our source model is based on a bidirectional LSTM-CRF <ref type="bibr" target="#b7">(Lample et al. 2016</ref>), which we transfer to a target model in two steps. We first project the mono-lingual word embeddings to a common space through word-level adversarial training. The word-level mapping yields initial cross-lingual links between two languages but does not take any NER information into account. Transferring task information in the cross-lingual setup is specifically challenging because languages vary in the word order. To tackle this, we propose an augmented fine-tuning method with parameter sharing and feature augmentation, and jointly train the target model in supervision of the source model. In summary, we make the following key contributions:</p><p>â€¢ We propose a novel unsupervised cross-lingual NER model, assuming no labels in target language, no parallel bi-texts, no cross-lingual dictionaries, and no comparable corpora. To the best of our knowledge, we are the first to show true unsupervised results (validation by sourcelanguage) for zero-shot cross-lingual NER.</p><p>â€¢ Our approach is inspired by the CO theory of how humans acquire a second language, which enables easy transfer to a new language. Our approach only requires the tuning of the pre-trained source model on the (unlabeled) target data.</p><p>â€¢ We systematically analyze the effect of different components of the model and their contributions for transferring the NER knowledge from one language to another.</p><p>â€¢ We report sizable improvements over state-of-the-art cross-lingual NER methods on five language pairs encompassing languages from different families (2.43 for Span-1We use 'unsupervised' to refer to cross-lingual models that do not use any NER labels in the target language. ish, 2.21 for Dutch, 6.14 for German, 7.1 for Arabic, 5.73 for Finnish). Our method also outperforms the models that use cross-lingual and multilingual external resources.</p><p>â€¢ We have released our code for research purposes.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition</head><p>Our objective is to transfer NER knowledge from a source language (e.g., English) to a target language (e.g., German) in an unsupervised way. While doing so, we also wish to provide the landscape of the probable solutions and analyze different solution stages and the importance of different components of the neural model. We make the following assumptions.</p><p>â€¢ We have access to mono-lingual corpora for both source and target languages to create pretrained word embeddings such as fasttext <ref type="bibr" target="#b5">(Grave et al. 2018</ref>). â€¢ For training, we assume that we have NER labels only for the source language dataset. â€¢ We consider two validation scenarios for model selection:</p><p>(i) we have access to a labeled target language validation set, and (ii) only source language validation set is available.</p><p>Learning cross-lingual models involves two fundamental steps: (i) learn a mapping between the source and the target language, and (ii) retrain the mapped resources to maximize the task objective. These two steps can be done separately or jointly. For example, <ref type="bibr" target="#b16">(Xie et al. 2018</ref>) first translate the source sequences to target word-by-word (step i), then they learn a target language NER model using the translated texts and projected NER tags (step ii). However, as mentioned before, this approach has several key limitations. Besides, training over the (translated) source sequence makes the sequence encoder more dependent on the source language order, which could introduce noise for the target language.</p><p>In contrast, we propose to perform mapping and task transfer jointly. Our model comprises two encoders -one for the source language and the other for the target. We first train a base NER model on the source language, and use it to jointly train the target model through adversarial learning and augmented fine-tuning. This way, the model is able to learn from both source and target sequences. In the following, we first describe our base model, then we present our novel unsupervised cross-lingual transfer approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Source (Base) Model</head><p>Our source (base) model has the same architecture as Lample et al. ( <ref type="formula">2016</ref>), as shown in Figure <ref type="figure" target="#fig_0">1</ref> (the left portion). Given an input sentence s = (w 1 , . . . , w m ) of length m, we first encode each token w k with a character-level bi-LSTM (Hochreiter and Schmidhuber 1997), which gives a token representation w ch k by sequentially combining the current input character representation with the previous hidden state in both directions. The character bi-LSTM (shown at the bottom in the box) captures orthographic properties (e.g., capitalization, prefix, suffix) of a token. For each token w k , we also have a word embedding w wr k that we fetch from a pretrained word embedding matrix. The pretrained word vectors capture distributional semantics of the words. We concatenate the character-level representation of a word with its word embedding to get the combined representation</p><formula xml:id="formula_0">x k = [w ch k ; w wr k ]. Let X = (x 1 , . . . , x m</formula><p>) denote the representation of the words in the sentence that we get from the character bi-LSTM and embedding lookup. X is then fed into another word-level bi-LSTM, which is also processed recurrently to obtain contextualised representations of the words.</p><p>The word-level bi-LSTM captures contextual information by propagating information through hidden layers, and can be used directly as a feature for NER classification. However, its modeling strength is limited compared to structured models that use global inference to model consistency in the output, especially in tasks having strong dependencies between output labels such as NER. Therefore, instead of classifying words independently with a Softmax layer, we model them jointly with a CRF layer <ref type="bibr" target="#b7">(Lafferty et al. 2001)</ref>.</p><p>For an input-output sequence pair (X, y), we define the joint probability distribution as follows.</p><formula xml:id="formula_1">p(y|X) = 1 Z(Î¸ s ) m i=1 Ïˆ n (y i |u i , V ) node factor m i=0 Ïˆ e (y i,i+1 | A) edge factor (1) where (u 1 , â€¢ â€¢ â€¢ , u m ) are the LSTM encoded contextualized word vectors, and Ïˆ n (y i = j |u i , V ) = exp(V T j u i )</formula><p>is the node-level score with V being the weight matrix, Ïˆ e is the transition matrix parameterized by A, and Z(.) is the normalization constant to ensure a valid probability distribution, and Î¸ s denotes all the parameters of the (source) model. The cross entropy loss for the (X, y) sequence pair is:</p><formula xml:id="formula_2">L s (Î¸ s ) = âˆ’ m i=1 log Ïˆ n (y i |u i , V ) âˆ’ m i=0 log A i,i+1 +log Z (2)</formula><p>We use Viterbi decoding to infer the most probable tag sequence for an input sequence, y * = arg max y p(y|X, Î¸ s ).</p><p>Following Lample et al. ( <ref type="formula">2016</ref>), we use a point-wise dense layer to transform the word representations before passing them to the CRF layer. As described later, the dense layer works as a common encoder in our cross-lingual model through which the two encoders share task information and common language properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Cross-Lingual Model</head><p>Our main goal is to learn a mapping of NER distributions between source and target languages. Neural approaches to NER depend heavily on fixed or contextualized pretrained embeddings <ref type="bibr" target="#b10">(Peters et al. 2018;</ref><ref type="bibr" target="#b4">Devlin et al. 2018;</ref><ref type="bibr" target="#b1">Akbik, Blythe, and Vollgraf 2018)</ref>. However, when we learn the embeddings for two different languages separately, their distribution spaces are very different even for closely related languages <ref type="bibr" target="#b12">(SÃ¸gaard, Ruder, and VuliÄ‡ 2018)</ref>. For example, Figure <ref type="figure">3a</ref> shows the t-SNE plot for NER tagged monolingual embeddings for English and Spanish. We see that the distributions are very different. Mapping these two distributions is indeed a very challenging task, especially in the unsupervised setup where no parallel data or dictionary is given. The challenge is further compounded by the requirement that the mappings should also reflect NER information; the effective modeling of NER requires the consideration of sequential dependencies, which generally vary between two languages under consideration.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the overall architecture of our cross-lingual NER model. We add three new components to the base model described in the previous section: (i) a separate encoder for the target language with shared character embeddings (box on the right) followed by a target-specific dense layer, (ii) wordlevel adversarial mappers that can map word embeddings from one language to another (shown in the middle of the two boxes), and (iii) an augmented fine-tuning method with parameter sharing and feature augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Encoder with Shared Character Embedding</head><p>Our target encoder parameterized by Î¸ t has the same architecture as the source encoder -a character-level bi-LSTM followed by a word-level bi-LSTM. Having a separate encoder as opposed to a shared one allows us to explicitly model specific characteristics (e.g., morphology, word order) of the respective languages. However, this also adds an additional challenge on how to effectively share the NER knowledge between the two encoders.</p><p>To promote knowledge sharing through cross-lingual mapping, we share the character embeddings of the two languages by defining a common embedding matrix. If two languages share alphabets or words, these common features can be used as a prior to learn the mapping.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word-level Adversarial Mapping</head><p>Sharing of character embeddings works only for languages that share alphabets. Even for languages sharing alphabets, it can only provide an initial mapping that is often not good 3We also tried subword units with BPE. However, given that the datasets are small, it did not give any additional gain.</p><p>enough to learn cross-lingual mappings. To learn the wordlevel mapping in an unsupervised way, we adopt the adversarial approach of <ref type="bibr" target="#b4">Conneau et al. (2017)</ref>.</p><p>Let X = {x 1 , . . . , x n } and Y = { y 1 , . . . , y m } be two sets consisting of n and m word embeddings of d-dimensions for a source and a target language, respectively. We assume that X and Y are trained independently from monolingual corpora. Our aim is to learn a mapping f (y) in an unsupervised way (i.e., no bi-lingual dictionary is given) such that for every y i , f (y) corresponds to its translation in X . Let W tâ†’s denote the linear mapping weight from target to source, and Î¸ D denote the parameters of a discriminator D (a binary classifier). We define the discriminator and adversary losses as follows.</p><formula xml:id="formula_3">L D (Î¸ D |W tâ†’s ) = âˆ’ 1 m m j=1 log P Î¸ D (src = 0|W tâ†’s y j ) âˆ’ 1 n n i=1 log P Î¸ D (src = 1|x i ) (3) L adv (W tâ†’s |Î¸ D ) = âˆ’ 1 m m i=1 log P Î¸ D (src = 1|W tâ†’s y j ) âˆ’ 1 n n i=1 log P Î¸ D (src = 0|x i ) (4)</formula><p>where P Î¸ D (src| z) is the probability according to D to distinguish whether z is coming from the source (src = 1) or from the target-to-source mapping (src = 0). The mapper W tâ†’s is trained jointly to fool the discriminator D.</p><p>Adversarial training gives an initial word-level mapping, which is often not good enough. A refinement step follows, to enrich the initial mapping by considering the global properties of the embedding spaces. Following <ref type="bibr" target="#b4">Conneau et al. (2017)</ref>, we use refinement with the Procrustes solution, where we first induce a seed dictionary using the learned mapper from our adversarial training. In order to find the nearest source word (x) of a target word (y) in the common space, we use the Cross-domain Similarity Local Scaling (CSLS). With the seed dictionary, we apply the following Procrustes solution to improve the initial mappings, W tâ†’s .</p><formula xml:id="formula_4">W tâ†’s = VU T , where UÎ£V T = SVD(X T Y )<label>(5)</label></formula><p>We perform this fine-tuning iteratively: induce a new dictionary using CSLS on the newly learned mapping, then use the dictionary in the Procrustes solution to improve the mapping. The mapper for source to target W sâ†’t can be similarly trained to map the source embedddings to the target space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmented Fine-tuning</head><p>The word-level adversarial training gives a mapping of the words independently. However, NER is a sequence labeling task, and the word order varies from one language to another. Besides, the word-level cross-lingual mapping process does not consider any task information (NER tags); it is simply a word translation model. As a result, the mappings may still lack alignments based on the NER tags. This can be seen in Figure <ref type="figure">3b</ref>, where the words are mapped to their translations but not clustered according to their NER tags. To learn target language ordering information in the target encoder and simultaneously transfer the NER knowledge from the source model, we propose a novel augmented finetuning method, which works in three steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(i) Source model pretraining through weight sharing.</head><p>We first train an NER model on the source where we have supervision. Our goal is to use this source model to generate pseudo NER labels for the target language sentences in the second step. Therefore, we train the model on the mapped representation of the source words. Formally, we optimize:</p><formula xml:id="formula_5">P i=1 L i s (Î¸ s |W sâ†’t )<label>(6)</label></formula><p>where L i s is the CRF classification loss in Equation 2 with P being the number of training samples in the source.</p><p>The word order in the target language generally differs from the source. To make the model more effective on target sentences, we promote order invariant features in the source encoder by binding the parameters of the forward and backward layers of the character bi-LSTM and word bi-LSTM. Later in our experiments we show its effectiveness. Sharing also reduces the number of parameters and helps to achieve better generalization across languages <ref type="bibr">(Lample et al. 2018</ref>). We will refer to this pretrained model as the mapped source model or simply source model parameterized by Î¸ s .</p><p>(ii) Generating pseudo target labels. Since our source model is already trained in a cross-lingual space, it can directly be applied to infer the NER tags for the target sentences. As shown in Figure <ref type="figure">3b</ref>, the word-level mapping provides good initial alignments that can be used to produce pseudo training samples in the target language to bootstrap training.</p><p>However, since the source model initially does not have any knowledge about the target language word order, it may generate noisy labels as the length of the target sentence increases. For example, Figure <ref type="figure" target="#fig_1">2</ref> shows the ratio of correctly tagged target words for different sentence lengths in different language pairs. We notice that the noise ratio is less for shorter sentences and it increases upto a point as the length increases. To effectively train our models with the pseudo target labels, we adopt a stochastic selection method based on sentence length. In particular, we randomly select a length (iii) Joint training with feature augmentation. We train our target NER model jointly with the source model with feature augmentation. For each batch from the source, we optimize our source model as before (Equation <ref type="formula" target="#formula_5">6</ref>). For each target batch with pseudo labels, we jointly train the source and the target model, and the features from the source encoder are augmented with the features from the target encoder (see Figure <ref type="figure" target="#fig_0">1</ref>). The overall loss function of our model is:</p><formula xml:id="formula_6">L(Î¸ s , Î¸ t ) = P i=1 L i s (Î¸ s |W sâ†’t ) source batch + Q j=1 L j t (Î¸ s ) target batch + Q j=1 L j t (Î¸ t ) target batch (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where Q is the number of target samples considered for training. This joint training with augmented features ensures that the target model does not overfit on the (potentially) noisy target samples. In a way, the source model guides the target one. Algorithm 1 provides the pseudocode of our training method. Fig. <ref type="figure">3c</ref> shows a sample output distribution of our common encoder. We can see that the representations are now well clustered based on the NER tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>Dataset We experiment with five different target languages -Spanish, Dutch, German, Arabic and Finnish. The source language is always English, for which we have sentences </p><formula xml:id="formula_8">t iii) Update Î¸ t on CRF loss L t (Î¸ t ) for b t until n_steps; Sample a length-threshold l from U (min, max) Create a target dataset D l T = {x j , Å·j } Q l j=1 using Î¸ s until convergence;</formula><p>refactored a few tags. For Arabic, we use AQMAR Arabic Wikipedia Named Entity Corpus <ref type="bibr" target="#b8">(Mohit et al. 2012)</ref>.5 The corpus contains 28 annotated Wikipedia articles. We randomly take 20% of the sentences from each article to create development and test sets.6 The NER data is tagged in the IOB1 format. Following the standard practice, we convert it to IOB2 to facilitate evaluation. We train and validate our model in the IOBES format, which is more expressive, for all languages except Arabic. Table <ref type="table" target="#tab_0">1</ref> presents some basic statistics of the datasets used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Models</head><p>We experiment with different baselines and variants of our model as described below.</p><p>â€¢ Source-Mono: We train an NER model on the source language with source word embeddings and apply it to the target language with target embeddings, which can be pre-trained or randomly initialized. This model does not use any cross-lingual information. â€¢ Cross-Word: We project source and target word embeddings to a common space using the unsupervised mapper (W sâ†’t or W tâ†’s ). This model uses word-level crosslingual information learned from adversarial training and the Procrustes-CSLS refinement procedure. â€¢ Cross-Shared: This model is the same as Cross-Word, but the weights of the forward and backward LSTM cells are shared to encourage order invariance in the model. â€¢ Cross-Augmented: This is our full cross-lingual model trained with source labels and target pseudo-labels generated by the pretrained model and the model itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Settings</head><p>We only use sentences with a maximum length of 250 words for training on the source language data. We use FastText embeddings <ref type="bibr" target="#b5">(Grave et al. 2018)</ref>, which are trained on Common Crawl and Wikipedia, and SGD with a gradient clipping of 5.0 to train the model. We found that the learning rate was crucial for training, and used a decaying rate to scale it down after every epoch. In particular, the learning rate was set to max( lr 0 1+decay * epoch , 0.0001). The initial learning rate of lr 0 = 0.1 and decay = 0.01 worked well with a dropout rate of 0.5. We trained the model for 30 epochs while using a batch size of 16, and evaluated the model after every 150 batches. The sizes of the character embeddings and char-LSTM hidden states were set to 25. Our word LSTM's hidden size was set to 100. The details of the hyperparameters are given in our Github repository.7 We conducted all the experiments in Table <ref type="table" target="#tab_2">3 and Table 4</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Results</head><p>In Table <ref type="table" target="#tab_1">2</ref>, we show the effect of different embeddings on the NER task. We observe that character embeddings contribute very little towards learning the monolingual NER task. Though the monolingual model performs better with GloVe embeddings <ref type="bibr" target="#b10">(Pennington et al. 2014)</ref>, adversarial training performs better with FastText <ref type="bibr" target="#b2">(Bojanowski et al. 2017</ref>), so we use FastText embeddings for all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source-Mono</head><p>In Table <ref type="table" target="#tab_2">3</ref>, we show how the source base models perform when they are directly applied to the target language. We can see that the model only learns when character embeddings (shared) are used. Random word embeddings provide better results than monolingual word embeddings.</p><p>Cross-Shared model, but a gain of +7.1 F1 using the Cross-Augmented method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>Char embeddings Contrary to the monolingual case, we find that pretrained source character embeddings make a significant contribution towards transferring NER knowledge in the cross-lingual task, if the two languages have similar morphological features (en-es, en-nl, en-fi). For Arabic (does not share characters English), the character embeddings only seem to work as noise. However, in case of German, there is a similar noise effect despite the shared characters. Presumably, this is because of the differences in the capitalisation patterns, since German capitalises all nouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding distribution</head><p>In the cross lingual model, the baseline results improve significantly. 3a and 3b show the distributions of the pairs of monolingual and cross-lingual embeddings. As the two languages do not share (Fig <ref type="figure">3a</ref>) any space in their distribution, it is impossible for the model to learn anything. Monolingual embeddings also hamper training; random embeddings increase the transfer score (Table <ref type="table" target="#tab_2">3</ref>), but the model performs poorly with random embeddings for monolingual training (Table <ref type="table" target="#tab_1">2</ref>). However, the result improves in 3. This suggests that we need to search for a better common space for both languages; thus, we perform crosslingual projection by adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared LSTM cell</head><p>In order to get better sequence invariance, we experimented with shared weights in forward and backward LSTM cells. This comes from the idea of learning less to transfer more. For Spanish and Dutch, this leads to significant improvements in results along with a 47% reduction in parameters. For German and Finnish there is no significant difference, but the number of parameters are reduced by 54% and 47%. However, for Arabic, there is a drop in the results, probably because of significant word-order differences with the source language (English).</p><p>Effect of Sentence Length One of our main assumptions is that pseudo-labels can reduce the entropy of the model <ref type="bibr" target="#b4">(Grandvalet and Bengio 2004)</ref>. Sentence length is a good feature for finding better pseudo-labels. However, this comes with a cost. To study the effect of sentence length while training the Cross-Augmented model, we perform experiments with sentences of lengths varying from 30 to 150. Figure <ref type="figure" target="#fig_1">2</ref> shows that as the sentence length increases, the ratio of correctly tagged sentences reduces. But if we only train the model on short sentences, the model will overfit on the short sentences of the target language data. Our main model addresses this issue by adding a teacher model and randomly sampling sentence lengths from a uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source vs. Target NER distribution</head><p>We report the results of our model tuned on both target and source development data. We see that the model tuned on target development data performs better than the model tuned on source dev data. The results of the source dev data tuned model should be considered as the results under a purely unsupervised setting. These results highlight the differences between the source and target NER distributions. Tuning on the target dev data therefore plays a significant role in the results obtained in cross-lingual NER research thus far. We also tried tuning the model with target test data. Here also we observe a gap between the results. To report stable results, the standard practice should be to report the results of multiple experiments with their standard deviations. Until now, to our knowledge, the only other paper to follow this has been <ref type="bibr" target="#b16">Xie et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Lample et al. ( <ref type="formula">2016</ref>) proposed an LSTM-CRF model for NER, which passes a hierarchical bi-LSTM encoding to a CRF layer to encourage global consistency of the NER tags. This model achieved impressive results for EN, NL, DE and ES despite not using any explicit feature engineering or manual gazetteers. We extend this base model to a cross-lingual named entity recognizer for a target language using annotated data for a source language and only monolingual, unannotated data for the target. <ref type="bibr" target="#b8">Mayhew et al. (2017)</ref> use a dictionary and co-occurrence probabilities to generate word and phrase based translations of the source data into a target data and then transfer the labels; although the translation quality is poor, the words/phrases and most of the relevant context is preserved, and they are able to achieve good results using a combination of orthographic and Wikifier <ref type="bibr" target="#b14">(Tsai et al. 2016)</ref> features. <ref type="bibr" target="#b9">Ni et al. (2017)</ref> use weak supervision for cross-lingual NER where they do annotation projection to get target labels and project word embeddings from the target language to the source language. Finally, <ref type="bibr" target="#b17">Yang et al. (2017)</ref> used a hierarchical recurrent network for semi-supervised cross-language transfer learning, where the source and the target language share the same character embeddings. <ref type="bibr" target="#b16">Xie et al. (2018)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed model for unsupervised Crosslingual Named Entity Recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sentence length vs. correctly tagged target words.</figDesc><graphic url="image-1.png" coords="4,352.35,54.00,172.80,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: t-SNE plot of NER tagged embeddings of two languages with 1000 samples: (a) Mono-lingual embeddings (fasttext), (b) Cross-lingual embeddings after word-level adversarial training, (c) Embeddings from our common encoder.</figDesc><graphic url="image-2.png" coords="5,54.00,54.00,172.80,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>tagged with NER classes. The data for English is from the CoNLL-2003 shared task for NER (Sang and Meulder 2003), while the data for Spanish and Dutch is from the CoNLL-2002 shared task for NER (Sang 2002). We collected the Finnish NER dataset from (Ruokolainen et al. 2019)4 and 4Available from https://github.com/mpsilfve/finer-data Algorithm 1: Augmented fine-tuning for x-lingual NER Input : Data D S = {x i , y i } P i=1 , D T = {x j } Q j=1 , Monolingual Embeddings E s and E t . // Word-level adversarial mapping 1. repeat repeat i) Sample batches b s âˆ¼ E s and b t âˆ¼ E t ii) Update Î¸ D on disc. loss L D (Î¸ D |W ) for b s and b t until n_disc_steps; Sample batches b s âˆ¼ E s and b t âˆ¼ E t Update W on adv. loss L adv (W |Î¸ D ) for b s and b t until w_steps; // Source model pre-training 2. repeat i) Sample a batch of sentences b s âˆ¼ D S ii) Update Î¸ s on CRF classification loss L s (Î¸ s ) for b s until n_steps; // Augmented fine-tuning 3. Sample a length-threshold l from U (min, max) 4. Use Î¸ s to infer on D T to create a dataset D l T = {x j , Å·j } Q l j=1 5. repeat repeat i) Sample a batch of sentences b s âˆ¼ D S and b t âˆ¼ D l T ii) Update Î¸ s on CRF loss L s (Î¸ s ) for b s and b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>are the first to propose a neural-based model for cross-lingual NER using the (Lample et al. 2016) model, with the addition of a self-attention layer on top of word representation, and validate the model based on target side development dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Training, Test and Development splits for different datasets. We exclude document start tags (DOCSTART).</figDesc><table><row><cell cols="4">Language Train Dev. Test</cell></row><row><cell>English</cell><cell cols="2">14041 3250</cell><cell></cell></row><row><cell>Spanish</cell><cell cols="3">8323 1915 1517</cell></row><row><cell>Dutch</cell><cell cols="3">15519 2821 5076</cell></row><row><cell>German</cell><cell cols="3">12152 2867 3005</cell></row><row><cell>Arabic</cell><cell>2166</cell><cell>267</cell><cell>254</cell></row><row><cell>Finnish</cell><cell cols="3">13497 986 3512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Monolingual NER results in the supervised setting.</figDesc><table><row><cell>five (5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results for monolingual models applied to target language NER task. 'x' means the model fails to learn anything. F * 1 and F 1 scores are calculated by tuning on the development datasets of the target and source, respectively.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">2https://github.com/ntunlp/Zero-Shot-Cross-Lingual-NER</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1">8See https://github.com/ntunlp/Zero-Shot-Cross-Lingual-NER for detailed results.gives us 1.73 and 0.71 absolute F1 score increments for the English to Spanish and Dutch language pairs respectively, over the Cross-Word (with/without character) model (Table4). This already achieves a SOTA result by an absolute F1 score of +0.89 for the English-Spanish language pair.Our Cross-Augmented model, (Tables4-5) that performs adaptation from source to target language, achieves SOTA performance for all language pairs. It improves over the previous SOTA by 2.43, 2.21, 6.14 and 5.73 F1 for the English to Spanish, Dutch, German and Finnish language pairs respectively, even outperforming multi-lingual models. Our model also outperforms the models that use cross-lingual resources for all languages -including German, which has not been the case in previous works. We also show the effectiveness of our model by reporting results on a "proxy" low-resource9 dataset (Arabic), where there is no improvement using</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_2">the 9In the Wikipedia dump as of September 2019, Arabic/English size ratio is 891/16384 (in MB)=.0543 ( 5.5% of en)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Jiateng Xie, Guillaume Lample and Emma Strubell for sharing their code and embeddings, and for their helpful replies on Github issues and e-mail. Also thanks to Tasnim Mohiuddin for a useful discussion on the hyperparameters of the Word Translation model.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-lingual Results</head><p>Word-level Mapping For all language pairs except En-Ar and En-Fi, projecting word embeddings from the source to the target language achieves the best results. For En-Ar, we could not get reasonable results for source-to-target projection, which is an issue, as discussed by <ref type="bibr" target="#b6">Hoshen and Wolf (2018)</ref>.8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Results</head><p>From Table <ref type="table">4</ref> we can see that the Cross-Word model with character LSTM performs significantly better than the monolingual model (Source-Mono, Table <ref type="table">2</ref>) for all languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Main Results</head><p>The Cross-Shared model, in which the weights of the forward and backward LSTM cells are shared,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Work</head><p>In this paper, we contribute a detailed definition of the problem of cross-lingual NER, thus providing a structure to the research to come hereafter. We also propose a new method for cross-lingual NER that generalizes well by weight-sharing and iteratively adapting to the target language domain, achieving SOTA in the process across languages from different language families. In future work, we want to explore pre-trained language models for cross-lingual NER transfer.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">F â™  1 (tuned on tgt-dev) F â™  1 -max F â™¦ (tuned on src-dev) F â™¦ 1 -max F â™£ 1 (tuned on tgt-test) F â™£ 1 -max # of params Cross-Word en â†’ es</title>
		<author>
			<persName><forename type="first">Model</forename><surname>Emb</surname></persName>
		</author>
		<author>
			<persName><surname>Prj</surname></persName>
		</author>
		<idno>68.63 1.49 70.62 64.79 1.68 67.42 68.90 1.10 70.62 342906 (âˆ¼13â†“) .69 50.13 209856 (âˆ¼47â†“</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">Blythe</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vollgraf ; Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Zero-resource multilingual model transfer: Learning what to share</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno>CoRR abs/1810</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><surname>Conneau</surname></persName>
		</author>
		<idno>CoRR abs/1810.04805. [Ganchev et al. 2010</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS, NIPS&apos;04</title>
				<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2017. 2017. 2018. 2018. 2010. 2001-2049. 2004. 2004</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
	<note>Semi-supervised learning by entropy minimization</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="1997">2018. 2018. 1997. 1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An iterative closest point method for unsupervised word translation</title>
		<author>
			<persName><forename type="first">Wolf</forename><forename type="middle">;</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno>CoRR abs/1801.06126</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">Mccallum</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pereira ; Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno>CoRR abs/1603.01360</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001">2001. 2001. 2016. 2018. 2018. Ma and Hovy 2016. 2016</date>
			<biblScope unit="page" from="799" to="809" />
		</imprint>
	</monogr>
	<note>ACL. End-toend sequence labeling via bi-directional lstm-cnns-crf. CoRR abs/1603.01354</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recall-oriented learning of named entities in arabic wikipedia</title>
		<author>
			<persName><forename type="first">Tsai</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roth</forename><forename type="middle">;</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL, EACL &apos;12</title>
				<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012">2017. 2017. 2012</date>
			<biblScope unit="page" from="162" to="173" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection</title>
		<author>
			<persName><forename type="first">Dinu</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><forename type="middle">;</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename></persName>
		</author>
		<idno>CoRR abs/1707.02483</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2013">2017. 2017. 2013</date>
		</imprint>
	</monogr>
	<note>Learning multilingual named entity recognition from wikipedia</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName><forename type="first">Socher</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Manning ; Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR abs/1705.00108</idno>
	</analytic>
	<monogr>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2014">2014. 2014. 2017. 2018</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>NAACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohn ; Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ruokolainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kauppinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Silfverberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>LindÃ£Ä¾n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T K</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Meulder</surname></persName>
		</author>
		<idno>CoRR abs/1902.00193</idno>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
				<imprint>
			<date type="published" when="2003">2019. 2019. 2019. 2003. 2003</date>
		</imprint>
	</monogr>
	<note>A finnish news corpus for named entity recognition. Language Resources and Evaluation. Sang 2002] Sang, E. T. K. 2002. Introduction to the conll-2002 shared task: Language-independent named entity recognition. CoRR cs.CL/0209010</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and accurate entity recognition with iterated dilated convolutions</title>
		<author>
			<persName><forename type="first">Ruder</forename><surname>SÃ¸gaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>VuliÄ‡ ; SÃ¸gaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>VuliÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Problems in Output and the Cognitive Processes They Generate: A Step Towards Second Language Learning</title>
				<imprint>
			<date type="published" when="1995">2018. 2018. 2017. 1995</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="371" to="391" />
		</imprint>
	</monogr>
	<note>EMNLP, EMNLP &apos;17</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-lingual word clusters for direct transfer of linguistic structure</title>
		<author>
			<persName><forename type="first">Mcdonald</forename><surname>TÃ¤ckstrÃ¶m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Uszkoreit ; TÃ¤ckstrÃ¶m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL, NAACL HLT &apos;12</title>
				<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="477" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-lingual projected expectation regularization for weakly supervised learning</title>
		<author>
			<persName><forename type="first">Mayhew</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roth</forename><forename type="middle">;</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
				<imprint>
			<date type="published" when="2014">2016. 2016. 2014. 2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="55" to="66" />
		</imprint>
	</monogr>
	<note>Cross-lingual named entity recognition via wikification</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint word alignment and bilingual named entity recognition using dual decomposition</title>
		<author>
			<persName><forename type="first">Che</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manning ; Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1073" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural cross-lingual named entity recognition with minimal resources</title>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
		<idno>CoRR abs/1808.09861</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inducing multilingual text analysis tools via robust projection across aligned corpora</title>
		<author>
			<persName><forename type="first">Salakhutdinov</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cohen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wicentowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT, HLT &apos;01</title>
				<editor>
			<persName><forename type="first">Iclr</forename><surname>Iclr</surname></persName>
		</editor>
		<editor>
			<persName><surname>'17. [yarowsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wicentowski</forename><surname>Ngai</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001">2017. 2001. 2001</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Transfer learning for sequence tagging with hierarchical recurrent networks</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
