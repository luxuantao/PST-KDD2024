<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NETWORK AUGMENTATION FOR TINY DEEP LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-24">24 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>76.8 77.9 77.6 78.4 76.5 Baseline Mixup AutoAugment Dropblock NetAug TinyMobileNetV2 52.4 51.7 51.0 48.7 53.7</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NETWORK AUGMENTATION FOR TINY DEEP LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-24">24 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.08890v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Network Augmentation (NetAug), a new training method for improving the performance of tiny neural networks. Existing regularization techniques (e.g., data augmentation, dropout) have shown much success on large neural networks by adding noise to overcome over-fitting. However, we found these techniques hurt the performance of tiny neural networks. We argue that training tiny models are different from large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model. At test time, only the tiny model is used for inference, incurring zero inference overhead. We demonstrate the effectiveness of NetAug on image classification and object detection. NetAug consistently improves the performance of tiny models, achieving up to 2.2% accuracy improvement on Im-ageNet. On object detection, achieving the same level of performance, NetAug requires 41% fewer MACs on Pascal VOC and 38% fewer MACs on COCO than the baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Tiny IoT devices are witnessing rapid growth, reaching 75.44 billion by 2025 (iot). Deploying deep neural networks directly on these tiny edge devices without the need for a connection to the cloud brings better privacy and lowers the cost. However, tiny edge devices are highly resource-constrained compared to cloud devices (e.g., GPU). For example, a microcontroller unit (e.g., STM32F746) typically only has 320KB of memory <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>, which is 50,000x smaller than the memory of a GPU. Given such strict constraints, neural networks must be extremely small to run efficiently on these tiny edge devices. Thus, improving the performance of tiny neural networks (e.g., MCUNet <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>) has become a fundamental challenge for tiny deep learning.</p><p>Conventional approaches to improve the performance of deep neural networks rely on regularization techniques to alleviate over-fitting, including data augmentation methods (e.g., <ref type="bibr">AutoAugment (Cubuk et al., 2019)</ref>, <ref type="bibr" target="#b60">Mixup (Zhang et al., 2018a)</ref>), dropout methods (e.g., Dropout <ref type="bibr" target="#b42">(Srivastava et al., 2014)</ref>, DropBlock <ref type="bibr" target="#b15">(Ghiasi et al., 2018)</ref>), and so on. Unfortunately, this common approach does not apply to tiny neural networks. Figure <ref type="figure">1</ref> (left) shows the ImageNet <ref type="bibr" target="#b11">(Deng et al., 2009)</ref> accuracy of state-ofthe-art regularization techniques on ResNet50 <ref type="bibr" target="#b19">(He et al., 2016)</ref> and MobileNetV2-Tiny <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>. These regularization techniques significantly improve the ImageNet accuracy of ResNet50, but unfortunately, they hurt the ImageNet accuracy for MobileNetV2-Tiny, which is 174x smaller. We argue that training tiny neural networks is fundamentally different from training large neural networks. Rather than augmenting the dataset, we should augment the network. Large neural networks tend to over-fit the training data, and data augmentation techniques can alleviate the over-fitting issue. However, tiny neural networks tend to under-fit the training data due to limited capacity (174x smaller); applying regularization techniques to tiny neural networks will worsen the under-fitting issue and degrade the performance. Extensive experiments on ImageNet (ImageNet, ImageNet-21k-P) and five fine-grained image classification datasets (Food101, Flowers102, Cars, Cub200, and Pets) show that NetAug is much more effective than regularization techniques for tiny neural networks. Applying NetAug to MobileNetV2-Tiny improves the ImageNet accuracy by 1.6% while adding only 16.7% training cost overhead and zero inference overhead. On object detection datasets, NetAug improves the AP50 of YoloV3 <ref type="bibr" target="#b35">(Redmon &amp; Farhadi, 2018)</ref> with Mbv3 w0.35 as the backbone by 3.36% on Pascal VOC and by 1.8% on COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Knowledge Distillation. Knowledge distillation (KD) <ref type="bibr" target="#b21">(Hinton et al., 2015;</ref><ref type="bibr" target="#b14">Furlanello et al., 2018;</ref><ref type="bibr" target="#b49">Yuan et al., 2020;</ref><ref type="bibr" target="#b2">Beyer et al., 2021;</ref><ref type="bibr" target="#b41">Shen et al., 2021;</ref><ref type="bibr" target="#b50">Yun et al., 2021</ref>) is proposed to transfer the "dark knowledge" learned in a large teacher model to a small student model. It trains the student model to match the teacher model's output logits <ref type="bibr" target="#b21">(Hinton et al., 2015)</ref> or intermediate activations <ref type="bibr" target="#b37">(Romero et al., 2015;</ref><ref type="bibr" target="#b51">Zagoruyko &amp; Komodakis, 2017)</ref> for better performances. Apart from being used alone, KD can be combined with other methods to improve the performance, such as <ref type="bibr" target="#b56">(Zhou et al., 2020)</ref> that combines layer-wise KD and network pruning.</p><p>Unlike KD, our method aims to improve the performances of neural networks from a different perspective, i.e., tackling the under-fitting issue of tiny neural networks. Technically, our method does not require the target model to mimic a teacher model. Instead, we train the target model to work as a sub-model of a set of larger models, built by augmenting the width of the target model, to get extra training supervision. Since the underlying mechanism of our method is fundamentally different from KD's. Our method is complementary to the use of KD and can be combined to boost performance (Table <ref type="table" target="#tab_1">2</ref>).</p><p>Regularization Methods. Regularization methods typically can be categorized into data augmentation families and dropout families. Data augmentation families add noise to the dataset by applying specially-designed transformations on the input, such as Cutout (DeVries &amp; Taylor, 2017) and Mixup <ref type="bibr" target="#b52">(Zhang et al., 2018a)</ref>. Additionally, AutoML has been employed to search for a combination of transformations for data augmentation, such as AutoAugment <ref type="bibr" target="#b8">(Cubuk et al., 2019)</ref> and RandAugment <ref type="bibr">(Cubuk et al., 2020)</ref>.</p><p>Instead of injecting noise into the dataset, dropout families add noise to the network to overcome overfitting. A typical example is Dropout <ref type="bibr" target="#b42">(Srivastava et al., 2014)</ref> that randomly drops connections of the neural network. Inspired by Dropout, many follow-up extensions propose structured forms of dropout for better performance, such as StochasticDepth <ref type="bibr" target="#b23">(Huang et al., 2016)</ref>, SpatialDropout <ref type="bibr" target="#b44">(Tompson et al., 2015)</ref>, and DropBlock <ref type="bibr" target="#b15">(Ghiasi et al., 2018)</ref>. In addition, some regularization techniques combine dropout with other methods to improve the performance, such as Self-distillation <ref type="bibr" target="#b53">(Zhang et al., 2019a)</ref> that combines knowledge distillation and depth dropping, and GradAug <ref type="bibr" target="#b47">(Yang et al., 2020)</ref> that combines data augmentation and channel dropping.</p><p>Unlike these regularization methods, our method targets improving the performance of tiny neural networks that suffer from under-fitting by augmenting the width of the neural network instead of shrinking it via random dropping. It is a reversed form of dropout. Our experiments show that NetAug is more effective than regularization methods on tiny neural networks (Table <ref type="table" target="#tab_2">3</ref>).</p><p>Tiny Deep Learning. Improving the inference efficiency of neural networks is very important in tiny deep learning. One commonly used approach is to compress existing neural networks by pruning <ref type="bibr" target="#b17">(Han et al., 2015;</ref><ref type="bibr" target="#b20">He et al., 2017;</ref><ref type="bibr" target="#b30">Liu et al., 2017)</ref> and quantization <ref type="bibr" target="#b18">(Han et al., 2016;</ref><ref type="bibr" target="#b57">Zhu et al., 2017;</ref><ref type="bibr" target="#b34">Rastegari et al., 2016)</ref>. Another widely adopted approach is to design efficient neural network architectures <ref type="bibr" target="#b24">(Iandola et al., 2016;</ref><ref type="bibr" target="#b40">Sandler et al., 2018;</ref><ref type="bibr" target="#b54">Zhang et al., 2018b)</ref>. In addition to manually designed compression strategies and neural network architectures, AutoML techniques recently gain popularity in tiny deep learning, including auto model compression <ref type="bibr" target="#b4">(Cai et al., 2019a;</ref><ref type="bibr" target="#b48">Yu &amp; Huang, 2019)</ref> and auto neural network architecture design <ref type="bibr" target="#b43">(Tan et al., 2019;</ref><ref type="bibr" target="#b5">Cai et al., 2019b;</ref><ref type="bibr" target="#b46">Wu et al., 2019)</ref>. Unlike these techniques, our method focuses on improving the accuracy of tiny neural networks without changing the model architecture . Combining these techniques with our method leads to better performances in our experiments (Table <ref type="table" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NETWORK AUGMENTATION</head><p>In this section, we first describe the formulation of NetAug. Then we introduce practical implementations. Lastly, we discuss the overhead of NetAug during training (16.7%) and test (zero).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FORMULATION</head><p>We denote the weights of the tiny neural network as W t and the loss function as L. During training, W t is optimized to minimize L with gradient updates:</p><formula xml:id="formula_0">W n+1 t = W n t − η ∂L(W n t ) ∂W n t</formula><p>, where η is the learning rate, and we assume using standard stochastic gradient descent for simplicity. Since the capacity of the tiny neural network is limited, it is more likely to get stuck in local minimums than large neural networks, leading to worse training and test performances.</p><p>We aim to tackle this challenge by introducing additional supervision to assist the training of the tiny neural network. Contrary to dropout methods that encourage subsets of the neural network to produce predictions, NetAug encourages the tiny neural network to work as a sub-model of a set of larger models constructed by augmenting the width of the tiny model (Figure <ref type="figure">2 left</ref>). The augmented loss function L aug is:</p><formula xml:id="formula_1">L aug = L(W t ) base supervision + α 1 L([W t , W 1 ]) + • • • + α i L([W t , W i ]) + • • • auxiliary supervision, working as a sub-model of augmented models ,<label>(1)</label></formula><p>where [W t , W i ] represents an augmented model that contains the tiny neural network W t and new weights W i . α i is the scaling hyper-parameter for combining loss from different augmented models. This weight-sharing strategy is also used in one-shot neural architecture search (NAS) <ref type="bibr" target="#b16">(Guo et al., 2020;</ref><ref type="bibr" target="#b6">Cai et al., 2020a)</ref> and multi-task learning <ref type="bibr" target="#b38">(Ruder, 2017)</ref>. Our objective and training process are completely different from theirs: i) one-shot NAS trains a weight-sharing super-net that supports all possible sub-networks. Its goal is to provide efficient performance estimation in NAS. In contrast, NetAug focuses on improving the performance of a tiny neural network by utilizing auxiliary supervision from augmented models. In addition, NetAug can be applied to NAS-designed neural networks for better performances (Table <ref type="table" target="#tab_0">1</ref>). ii) Multi-task learning aims to transfer knowledge across different tasks via weight sharing. In contrast, NetAug transmits auxiliary supervision on a single task, from augmented models to the tiny model.</p><p>Specifically, we construct the largest augmented model by augmenting the width (Figure <ref type="figure">2</ref> right), which incurs smaller training time overhead on GPUs than augmenting the depth <ref type="bibr" target="#b33">(Radosavovic et al., 2020)</ref>. For example, assume the width of a convolution operation is w, we augment its width by an augmentation factor r. Then the width of the largest augmented convolution operation is r × w. For simplicity, we use a single hyper-parameter to control the augmentation factor for all operators in the network.</p><p>After building the largest augmented model, we construct other augmented models by selecting a subset of channels from the largest augmented model. We use a hyper-parameter s, named diversity factor, to control the number of augmented model configurations. We set the augmented widths to be linearly spaced between w and r × w. For instance, with r = 3 and s = 2, the possible widths are <ref type="bibr">[w, 2w, 3w]</ref>. Different layers can use different augmentation ratios. In this way, we get diverse augmented models from the largest augmented model, each containing the target neural network.</p><p>Training Process. As shown in Eq. 1, getting supervision from one augmented network requires an additional forward and backward process. It is computationally expensive to involve all augmented networks in one training step. To address this challenge, we only sample one augmented network at each step. The tiny neural network is updated by merging the base supervision (i.e., ∂L(W n t ) ∂W n t ) and the auxiliary supervision from this sampled augmented network (Figure <ref type="figure">2</ref> left):</p><formula xml:id="formula_2">W n+1 t = W n t − η( ∂L(W n t ) ∂W n t + α ∂L([W n t , W n i ]) ∂W n t ),<label>(2)</label></formula><p>where [W n t , W n i ] represents the sampled augmented network at this training step. For simplicity, we use the same scaling hyper-parameter α (α = 1.0 in our experiments) for all augmented networks. In addition, W i is also updated via gradient descent in this training step. It is possible to sample more augmented networks in one training step. However, in our experiments, we found it not only increases the training cost but also hurts the performance. Thus, we only sample one augmented network in each training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TRAINING AND INFERENCE OVERHEAD</head><p>NetAug is only applied at the training time. At inference time, we only keep the tiny neural network. Therefore, the inference overhead of NetAug is zero. In addition, as NetAug does not change the network architecture, it does not require special support from the software system or hardware, making it easier to deploy in practice.</p><p>Regarding the training overhead, applying NetAug adds an extra forward and backward process in each training step, which seems to double the training cost. However, in our experiments, the training time is only 16.7% longer (245 GPU hours v.s. 210 GPU hours, shown in Table <ref type="table" target="#tab_2">3</ref>). It is because the total training cost of a tiny neural network is dominated by data loading and communication cost, not the forward and backward computation, since the model is very small. Therefore, the overall training time overhead of NetAug is only 16.7%. Apart from the training cost, applying NetAug will increase the peak training memory footprint. Since we focus on training tiny neural networks whose peak training memory footprint is much smaller than large neural networks', in practice, the slightly increased training memory footprint can still fit in GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SETUP</head><p>Datasets. We conducted experiments on seven image classification datasets, including ImageNet <ref type="bibr" target="#b11">(Deng et al., 2009)</ref>, ImageNet-21K-P (winter21 version) <ref type="bibr" target="#b36">(Ridnik et al., 2021)</ref>, Food101 <ref type="bibr" target="#b3">(Bossard et al., 2014)</ref>, Flowers102 <ref type="bibr" target="#b31">(Nilsback &amp; Zisserman, 2008)</ref>, Cars <ref type="bibr" target="#b26">(Krause et al., 2013)</ref>, Cub200 <ref type="bibr" target="#b45">(Wah et al., 2011), and</ref><ref type="bibr">Pets (Parkhi et al., 2012)</ref>. In addition to image classification, we also evaluated our method on Pascal VOC object detection <ref type="bibr" target="#b13">(Everingham et al., 2010)</ref> and COCO object detection <ref type="bibr" target="#b29">(Lin et al., 2014)</ref>  <ref type="foot" target="#foot_0">1</ref> .</p><p>Training Details. For ImageNet experiments, we train models with batch size 2048 using 16 GPUs. We use the SGD optimizer with Nesterov momentum 0.9 and weight decay 4e-5. By default, the models are trained for 150 epochs on ImageNet and 20 epochs on ImageNet-21K-P, except stated explicitly. The initial learning rate is 0.4 and gradually decreases to 0 following the cosine schedule. Label smoothing is used with a factor of 0.1 on ImageNet.</p><p>For experiments on fine-grained image classification datasets (Food101, Flowers102, Cars, Cub200, and Pets), we train models with batch size 256 using 4 GPUs. We use ImageNet-pretrained weights to initialize the models and finetune the models for 50 epochs.</p><p>For Pascal VOC object detection, we train models for 200 epochs with batch size 64 using 8 GPUs.</p><p>The training set consists of Pascal VOC 2007 trainval set and Pascal VOC 2012 trainval set, while Pascal VOC 2007 test set is used for testing. For COCO object detection, we train models for 120 epochs with batch size 128 using 16 GPUs. COCO2017 train is used for training while COCO2017 val is used for testing.</p><p>We use the YoloV3 <ref type="bibr" target="#b35">(Redmon &amp; Farhadi, 2018)</ref> detection framework and replace the backbone with tiny neural networks. We also replace normal convolution operations with depthwise convolution operations in the head of YoloV3. ImageNet-pretrained weights are used to initialize the backbone while the detection head is initialized randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS ON IMAGENET</head><p>Main Results. We apply NetAug to commonly used tiny neural network architectures in TinyML <ref type="bibr" target="#b28">(Lin et al., 2020;</ref><ref type="bibr" target="#b39">Saha et al., 2020;</ref><ref type="bibr" target="#b1">Banbury et al., 2021)</ref>, including MobileNetV2-Tiny <ref type="bibr">(Lin et</ref>     <ref type="bibr" target="#b5">(Cai et al., 2019b)</ref>, MCUNet <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>, MobileNetV3 <ref type="bibr" target="#b22">(Howard et al., 2019)</ref>, and MobileNetV2 <ref type="bibr" target="#b40">(Sandler et al., 2018)</ref>. As shown in Table <ref type="table" target="#tab_0">1</ref>, NetAug provides consistent accuracy improvements over the baselines on different neural architectures. Specifically, for ProxylessNAS-Mobile, MCUNet and MobileNetV3, whose architectures are optimized using NAS, NetAug still provides significant accuracy improvements (1.7% for ProxylessNAS-Mobile, 1.2% for MCUNet, and 2.2% for MobileNetV3). In addition, we find NetAug tends to provide higher accuracy improvement on smaller neural networks (+0.9% on MobileNetV2 w1.0 → +1.5% on MobileNetV2 w0.35). We conjecture that smaller neural networks have lower capacity, thus suffer more from the under-fitting issue and benefits more from NetAug. Unsurprisingly, NetAug hurts the accuracy of non-tiny neural network (ResNet50), which already has enough model capacity on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Training Epochs</head><p>Figure <ref type="figure" target="#fig_3">3</ref> summarizes the results of MobileNetV2-Tiny on ImageNet and ImageNet-21K-P under different numbers of training epochs. NetAug provides consistent accuracy improvements over the baseline under all settings. In addition, to achieve similar accuracy, NetAug requires much fewer training epochs than the baseline (75% fewer epochs on ImageNet, 83% fewer epochs on ImageNet-21K-P), which can save the training cost and reduce the CO 2 emissions.</p><p>Comparison with KD. We compare the ImageNet performances of NetAug and knowledge distillation (KD) on MobileNetV2-Tiny <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>, MobileNetV2, MobileNetV3, and ProxylessNAS. All models use the same teacher model (Assemble-ResNet50 <ref type="bibr" target="#b27">(Lee et al., 2020))</ref> when training with KD.</p><p>The results are summarized in Table <ref type="table" target="#tab_1">2</ref>. Compared with KD, NetAug provides slightly higher ImageNet accuracy improvements: +0.5% on MobileNetV2-Tiny, +0.8% on MobileNetV2 (w0.35, r160), +0.5% on <ref type="bibr">MobileNetV3 (w0.35,</ref><ref type="bibr">r160),</ref><ref type="bibr">and +0.4% on ProxylessNAS (w0.35,</ref><ref type="bibr">r160)</ref>. In addition, we find NetAug's improvement is orthogonal to KD's. Comparison with Regularization Methods. Regularization techniques hurt the performance of tiny neural network (Table <ref type="table" target="#tab_2">3</ref>), even when the regularization strength is very weak (e.g., dropout with keep probability 0.9). This is due to tiny networks has very limited model capacity. When adding stronger regularization (e.g., RandAugment, Dropblock), the accuracy loss gets larger (up to 3.7% accuracy loss). Additionally, we notice that Mixup, Dropblock, and RandAugment provide hyper-parameters to adjust the strength of regularization. We further studied these methods under different regularization strengths. The results are reported in the appendix (Table <ref type="table">6</ref>) due to the space limit. Similar to observations in Table <ref type="table" target="#tab_2">3</ref>, we consistently find that: i) adding these regularization methods hurts the accuracy of the tiny neural network. ii) Increasing the regularization strength leads to a higher accuracy loss.</p><p>Based on these results, we conjecture that tiny neural networks suffer from the under-fitting issue rather than the over-fitting issue. Applying regularization techniques designed to overcome overfitting will exacerbate the under-fitting issue of tiny neural networks, thereby leading to accuracy loss. In contrast, applying NetAug improves the ImageNet accuracy of MobileNetV2-Tiny by 1.3%, with zero inference overhead and only 16.7% training cost overhead. NetAug is more effective than regularization techniques in tiny deep learning.</p><p>Discussion. NetAug improves the accuracy of tiny neural networks by alleviating under-fitting. However, for larger neural networks that do not suffer from under-fitting, applying NetAug may, on the contrary, exacerbate over-fitting, leading to degraded validation accuracy (as shown in Table <ref type="table" target="#tab_4">1,  ResNet50</ref>). We verify the idea by plotting the training and validation curves of a tiny network (MobileNetV2-Tiny) and a large network (ResNet50) in Figure <ref type="figure">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RESULTS ON TRANSFER LEARNING</head><p>Models pre-trained on ImageNet are usually used for initialization in downstream tasks such as fine-grained image classification <ref type="bibr" target="#b10">(Cui et al., 2018;</ref><ref type="bibr" target="#b25">Kornblith et al., 2019;</ref><ref type="bibr" target="#b7">Cai et al., 2020b)</ref> and object detection <ref type="bibr" target="#b13">(Everingham et al., 2010;</ref><ref type="bibr" target="#b29">Lin et al., 2014)</ref>. In this subsection, we study whether NetAug can benefit these downstream tasks using five fine-grained image classification datasets and two object detection datasets. The input image size is 160 for fine-grained image classification and 416 for object detection.</p><p>The transfer learning results of MobileNetV2 w0.35 and MobileNetV3 w0.35 with different pretrained weights are summarized in Table <ref type="table">4</ref>. We find that a higher accuracy on the pre-training dataset (i.e., ImageNet in our case) does not always lead to higher performances on downstream tasks. For example, though adding KD improves the ImageNet accuracy of MobileNetV2 w0.35 and MobileNetV3 w0.35, using weights pre-trained with KD hurts the performances on two finegrained classification datasets (Cub200 and Pets) and all object detection datasets (Pascal VOC and COCO). Similarly, training models for more epochs significantly improves the ImageNet accuracy of MobileNetV2 w0.35 but hurts the performances on three fine-grained classification datasets. Pascal VOC (AP50) COCO (AP50)</p><p>1 improves the ImageNet accuracy of the models but also improves the quality of learned representation.</p><p>In addition to the normal transfer learning setting, we also test the effectiveness of NetAug under the tiny transfer learning <ref type="bibr" target="#b7">(Cai et al., 2020b)</ref> setting where the pre-trained weights are frozen while only updating the biases and additional lite residual modules to reduce the training memory footprint. As shown in Table <ref type="table" target="#tab_4">5</ref>, NetAug can also benefit tiny transfer learning, providing consistently accuracy improvements over the baseline.</p><p>Apart from improving the performance, NetAug can also be applied for better inference efficiency. Figure <ref type="figure" target="#fig_5">5</ref> demonstrates the results of YoloV3+MobileNetV2 w0.35 and YoloV3+MobileNetV3 w0.35 under different input resolutions. Achieving similar AP50, NetAug requires a smaller input resolution than the baseline, leading to 41% MACs reduction on Pascal VOC and 38% MACs reduction on COCO. Meanwhile, a smaller input resolution also reduces the inference memory footprint <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>, which is also critical for running tiny deep learning models on memory-constrained devices.</p><p>Additionally, we experiment with detection mixup <ref type="bibr" target="#b55">(Zhang et al., 2019b)</ref> on Pascal VOC. Similar to ImageNet experiments, adding detection mixup provides worse mAP than the baseline, especially on YoloV3+MobileNetV2 w0.35. It shows that the under-fitting issue of tiny neural networks also exists beyond image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose Network Augmentation (NetAug) for improving the performance of tiny neural networks, which suffer from limited model capacity. Unlike regularization methods that aim to address the over-fitting issue for large neural networks, NetAug tackles the under-fitting problem of tiny neural networks. This is achieved by putting the target tiny neural network into larger neural networks to get auxiliary supervision during training. Extensive experiments on image classification and object detection consistently demonstrate the effectiveness of NetAug on tiny neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 Figure 1 :</head><label>11</label><figDesc>Figure 1: Left: ResNet50 (large neural network) benefits from regularization techniques, while MobileNetV2-Tiny (tiny neural network) losses accuracy by these regularizations. Right: Large neural networks suffer from over-fitting, thus require regularization such as data augmentation and dropout. In contrast, tiny neural networks tend to under-fit the dataset, thus requires more capacity during training. NetAug augments the network (reverse dropout) during training to provide more supervision for tiny neural networks. Contrary to regularization techniques, it improves the accuracy of tiny neural networks and as expected, hurts the accuracy of non-tiny neural networks.</figDesc><graphic url="image-13.png" coords="2,303.36,138.68,124.14,124.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Figure 2 :</head><label>12</label><figDesc>Figure 2: Left: We augment a tiny network by putting it into larger neural networks. They share the weights. The tiny neural network is supervised to produce useful representations for larger neural networks beyond functioning independently. At each training step, we sample one augmented network to provide auxiliary supervision that is added to the base supervision. At test time, only the tiny network is used for inference, which has zero overhead. Right: NetAug is implemented by augmenting the width multiplier and expand ratio of the tiny network.</figDesc><graphic url="image-36.png" coords="4,118.44,126.30,140.41,140.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NetAug outperforms the baseline under different numbers of training epochs on ImageNet and ImageNet-21K-P for MobileNetV2-Tiny. With similar accuracy, NetAug requires 75% fewer training epochs on ImageNet and 83% fewer training epochs on ImageNet-21K-P.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>MACs</head><label></label><figDesc>, with and without applying NetAug. Applying NetAug improves the training and validation accuracy of MobileNetV2-Tiny, demonstrating that NetAug effectively reduces the under-fitting issue. For ResNet50, NetAug improves the training accuracy while lowers the validation accuracy, showing signs of overfitting. Published as a conference paper at ICLR 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: On Pascal VOC and COCO, models pre-trained with NetAug achieve a better performanceefficiency trade-off. Similar to ImageNet classification, adding detection mixup<ref type="bibr" target="#b55">(Zhang et al., 2019b)</ref> that is effective for large neural networks causes performance drop for tiny neural networks.</figDesc><graphic url="image-86.png" coords="9,326.52,221.87,66.57,52.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>NetAug consistently improves the ImageNet accuracy for popular tiny neural networks. The smaller the model, the larger the improvement. 'w' represents the width multiplier and 'r' represents the input image size.</figDesc><table><row><cell>al.,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Combining NetAug and KD can further boost the ImageNet accuracy of tiny neural networks: 2.7% on MobileNetV2-Tiny, 2.9% Comparison with KD<ref type="bibr" target="#b21">(Hinton et al., 2015)</ref> on ImageNet. NetAug is orthogonal to KD. Combining NetAug with KD further boosts the ImageNet accuracy of tiny neural networks.</figDesc><table><row><cell>Model</cell><cell>Baseline</cell><cell>KD</cell><cell>NetAug</cell><cell cols="2">NetAug + KD</cell></row><row><cell>MobileNetV2-Tiny</cell><cell cols="5">51.7% 52.8% (+1.1%) 53.3% (+1.6%) 54.4% (+2.7%)</cell></row><row><cell cols="6">MobileNetV2 w0.35, r160 56.3% 57.0% (+0.7%) 57.8% (+1.5%) 59.2% (+2.9%)</cell></row><row><cell cols="6">MobileNetV3 w0.35, r160 58.1% 59.8% (+1.7%) 60.3% (+2.2%) 61.5% (+3.4%)</cell></row><row><cell cols="6">ProxylessNAS w0.35, r160 59.1% 60.4% (+1.3%) 60.8% (+1.7%) 61.5% (+2.4%)</cell></row><row><cell cols="2">Model (MobileNetV2-Tiny)</cell><cell>#Epochs</cell><cell cols="3">Training Cost (GPU Hours) Top1 Acc ∆Acc ImageNet</cell></row><row><cell>Baseline</cell><cell></cell><cell>150</cell><cell>210</cell><cell>51.7%</cell><cell>-</cell></row><row><cell cols="2">Dropout (kp=0.9) (Srivastava et al., 2014)</cell><cell>150</cell><cell>210</cell><cell cols="2">51.0% -0.7%</cell></row><row><cell cols="2">Dropout (kp=0.8) (Srivastava et al., 2014)</cell><cell>150</cell><cell>210</cell><cell cols="2">50.3% -1.4%</cell></row><row><cell>Baseline</cell><cell></cell><cell>300</cell><cell>420</cell><cell>52.4%</cell><cell>-</cell></row><row><cell cols="2">Mixup (Zhang et al., 2018a)</cell><cell>300</cell><cell>420</cell><cell cols="2">51.7% -0.7%</cell></row><row><cell cols="2">AutoAugment (Cubuk et al., 2019)</cell><cell>300</cell><cell>440</cell><cell cols="2">51.0% -1.4%</cell></row><row><cell cols="2">RandAugment (Cubuk et al., 2020)</cell><cell>300</cell><cell>440</cell><cell cols="2">49.6% -2.8%</cell></row><row><cell cols="2">DropBlock (Ghiasi et al., 2018)</cell><cell>300</cell><cell>420</cell><cell cols="2">48.7% -3.7%</cell></row><row><cell>NetAug</cell><cell></cell><cell>300</cell><cell>490</cell><cell cols="2">53.7% +1.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Regularization techniques hurt the accuracy for MobileNetV2-Tiny, while NetAug provides 1.3% top1 accuracy improvement with only 16.7% training cost overhead.</figDesc><table><row><cell>on MobileNetV2 (w0.35, r160), 3.4% on MobileNetV3 (w0.35, r160), and 2.4% on ProxylessNAS</cell></row><row><cell>(w0.35, r160).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>NetAug also benefits the tiny transfer learning<ref type="bibr" target="#b7">(Cai et al., 2020b)</ref> setting where pre-trained weights are frozen to reduce training memory footprint.</figDesc><table><row><cell cols="8">Published as a conference paper at ICLR 2022</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">MobileNetV2-COCO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MobileNetV3-COCO</cell></row><row><cell>NetAug</cell><cell></cell><cell>#5 Acc Loss</cell><cell>Baseline</cell><cell cols="2">#5 Acc Loss</cell><cell></cell><cell>NetAug</cell><cell></cell><cell>#5 Acc Loss</cell><cell>Baseline</cell><cell>#5 Acc Loss</cell></row><row><cell>Untitled 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Untitled 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Untitled 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Untitled 2</cell><cell></cell><cell>110.1</cell><cell></cell><cell>23.92</cell><cell>110.1</cell><cell>21.7</cell></row><row><cell>Untitled 3</cell><cell>153.0</cell><cell>22.89</cell><cell></cell><cell>153.0</cell><cell>22.42</cell><cell>Untitled 3</cell><cell></cell><cell>139.2</cell><cell></cell><cell>25.93</cell><cell>139.2</cell><cell>24.2</cell></row><row><cell>Untitled 4</cell><cell>188.9</cell><cell>24.26</cell><cell></cell><cell>188.9</cell><cell>23.53</cell><cell>Untitled 4</cell><cell></cell><cell>171.9</cell><cell></cell><cell>27.68</cell><cell>171.9</cell><cell>25.9</cell></row><row><cell></cell><cell>228.6 272.0 319.2</cell><cell cols="2">24.7 25.3 Method 25.4</cell><cell>228.6 272.0 319.2</cell><cell>24.1 24.6 24.7</cell><cell cols="5">247.4 Fine-grained Classificaition: Top1 (%) 29.1 247.4 27.36 290.3 30.2 290.3 28.4 Food101 Flowers102 Cars Cub200 Pets 336.7 30.7 336.7 29.2</cell></row><row><cell></cell><cell>370.3</cell><cell>25.9</cell><cell></cell><cell>370.3</cell><cell>24.9</cell><cell></cell><cell></cell><cell>386.4</cell><cell></cell><cell>31.3</cell><cell>386.4</cell><cell>29.7</cell></row><row><cell></cell><cell></cell><cell>MbV2</cell><cell></cell><cell cols="4">Baseline (150) 67.33</cell><cell cols="2">89.04 439.6</cell><cell>58.28 57.75 78.88 31.6 439.6 29.8</cell></row><row><cell></cell><cell></cell><cell cols="2">w0.35 r160</cell><cell cols="2">NetAug</cell><cell cols="2">68.67</cell><cell cols="2">90.40</cell><cell>60.02 58.39 79.37</cell></row><row><cell></cell><cell></cell><cell>MbV3</cell><cell></cell><cell cols="4">Baseline (150) 67.53</cell><cell cols="2">89.19</cell><cell>51.18 57.99 80.32</cell></row><row><cell></cell><cell></cell><cell cols="2">w0.35 r160</cell><cell cols="2">NetAug</cell><cell cols="2">69.74</cell><cell cols="2">90.73</cell><cell>56.21 58.94 82.04</cell></row><row><cell>64</cell><cell cols="2">YoloV3+MbV2</cell><cell></cell><cell>67</cell><cell cols="2">YoloV3+MbV3</cell><cell cols="2">26</cell><cell cols="2">YoloV3+MbV2</cell><cell>35</cell><cell>YoloV3+MbV3</cell></row><row><cell>60 62</cell><cell></cell><cell cols="2">-41% MACs</cell><cell></cell><cell></cell><cell></cell><cell cols="2">25</cell><cell cols="2">-38% MACs</cell><cell>30</cell><cell>+ 1.8% AP50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell cols="2">24</cell><cell></cell></row><row><cell>54 56 58</cell><cell></cell><cell cols="2">NetAug Baseline Mixup</cell><cell>53</cell><cell>+ 3.36% mAP</cell><cell>NetAug Baseline Mixup</cell><cell cols="2">23 22</cell><cell></cell><cell>NetAug Baseline</cell><cell>25 20</cell><cell>NetAug Baseline</cell></row><row><cell>90</cell><cell>150</cell><cell>210</cell><cell>270</cell><cell>60</cell><cell>120</cell><cell>180</cell><cell>240</cell><cell cols="3">150 200 250 300 350 400</cell><cell>100 170 240 310 380 450</cell></row><row><cell></cell><cell cols="2">MACs (M)</cell><cell></cell><cell></cell><cell cols="2">MACs (M)</cell><cell></cell><cell></cell><cell cols="2">MACs (M)</cell><cell>MACs (M)</cell></row></table><note>Compared to KD and training for more epochs, we find models pre-trained with NetAug achieve clearly better transfer learning performances in most cases, though their ImageNet performances are similar. It shows that encouraging the tiny models to work as a sub-model of larger models not only</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code and pre-trained weights: https://github.com/mit-han-lab/tinyml</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank National Science Foundation, MIT-IBM Watson AI Lab, Hyundai, Ford, Intel and Amazon for supporting this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NetAug</head><p>53.7% +1.3%</p><p>Table <ref type="table">6</ref>: Ablation study on regularization methods. All models are trained for 300 epochs on ImageNet.</p><p>For DropBlock, we adopted the implementation from https://github.com/miguelvr/ dropblock. Additionally, block size=7 is not applicable on MobileNetV2-Tiny, because the feature map size at the last stage of MobileNetV2-Tiny is 5. For RandAugment, we adopted the implementation from https://github.com/rwightman/pytorch-image-models.</p><p>As shown in Table <ref type="table">6</ref>, increasing the regularization strength results in a higher accuracy loss for the tiny neural network. Removing regularization provides a better result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ABLATION STUDY ON TRAINING SETTINGS</head><p>In addition to NetAug, there are several simple techniques that potentially can alleviate the underfitting issue of tiny neural networks, including using a smaller weight decay, using weaker data augmentation, and using a smaller batch size. However, as shown in </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Internet of things (IoT) connected devices installed base worldwide from</title>
		<ptr target="https://www.statista.com/statistics/471264/iot-number-of-connected-devices-worldwide/" />
		<imprint>
			<date type="published" when="2015">2015 to 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Colby</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuteng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dibakar</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Whatmough</surname></persName>
		</author>
		<title level="m">Micronets: Neural network architectures for deploying tinyml applications on commodity microcontrollers. Proceedings of Machine Learning and Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Knowledge distillation: A good teacher is patient and consistent</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amélie</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larisa</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05237</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automl for architecting efficient and specialized neural networks</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<publisher>IEEE</publisher>
			<pubPlace>Micro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1812.00332.pdf.3,6" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1908.09791.pdf.4" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tinytl: Reduce memory, not parameters for efficient on-device learning</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropblock: a regularization method for convolutional networks</title>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018. 1, 3, 7, 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Song</forename><surname>Forrest N Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
				<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Jungkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeryun</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tae</forename><surname>Kwan Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geonmo</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiho</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06268</idno>
		<title level="m">Compounding the performance improvements of assembled techniques in a convolutional neural network</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mcunet: Tiny deep learning on iot devices</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2005">2020. 1, 5</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Omkar M Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet-21k pretraining for the masses</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Zkj_VcZ6ol.5" />
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rnnpool: Efficient non-linear pooling for ram constrained inference</title>
		<author>
			<persName><forename type="first">Oindrila</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Vardhan Simhadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11921</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Is label smoothing truly incompatible with knowledge distillation: An empirical study</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gradaug: A new regularization method for deep neural networks</title>
		<author>
			<persName><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Autoslim: Towards one-shot architecture search for channel numbers</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11728</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2340" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb.1,3" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Bag of freebies for training object detection neural networks</title>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04103</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Go wide, then narrow: Efficient training of deep thin networks</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11546" to="11555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Trained ternary quantization</title>
		<author>
			<persName><forename type="first">Chenzhuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m">A ABLATION STUDY ON REGULARIZATION METHODS Model</title>
				<meeting><address><addrLine>MobileNetV2-Tiny</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno>2% -0.2%</idno>
	</analytic>
	<monogr>
		<title level="j">ImageNet Top1 Acc ∆Acc Baseline</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note>α=0.1</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">;</forename><surname>Mixup</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno>7% -0.7%</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="page">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ghiasi</forename><surname>Dropblock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>kp=0.95, block size=5. 8% DropBlock (kp=0.9, block size=5</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Randaugment</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M=9) (</forename><surname>N=1</surname></persName>
		</author>
		<author>
			<persName><surname>Cubuk</surname></persName>
		</author>
		<idno>5% -0.9%</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Randaugment</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M=9) (</forename><surname>N=2</surname></persName>
		</author>
		<author>
			<persName><surname>Cubuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
