<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NODE FEATURE EXTRACTION BY SELF-SUPERVISED MULTI-SCALE NEIGHBORHOOD PREDICTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-29">29 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
							<email>ichien3@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
							<email>jiongz@amazon.com</email>
						</author>
						<author>
							<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
							<email>chohsieh@cs.ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
							<email>milenkov@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Search</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NODE FEATURE EXTRACTION BY SELF-SUPERVISED MULTI-SCALE NEIGHBORHOOD PREDICTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-29">29 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.00064v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning on graphs has attracted significant attention in the learning community due to numerous real-world applications. In particular, graph neural networks (GNNs), which take numerical node features and graph structure as inputs, have been shown to achieve state-of-the-art performance on various graph-related learning tasks. Recent works exploring the correlation between numerical node features and graph structure via self-supervised learning have paved the way for further performance improvements of GNNs. However, methods used for extracting numerical node features from raw data are still graph-agnostic within standard GNN pipelines. This practice is sub-optimal as it prevents one from fully utilizing potential correlations between graph topology and node attributes. To mitigate this issue, we propose a new self-supervised learning framework, Graph Information Aided Node feature exTraction (GIANT). GIANT makes use of the eXtreme Multi-label Classification (XMC) formalism, which is crucial for fine-tuning the language model based on graph information, and scales to large datasets. We also provide a theoretical analysis that justifies the use of XMC over link prediction and motivates integrating XR-Transformers, a powerful method for solving XMC problems, into the GIANT framework. We demonstrate the superior performance of GIANT over the standard GNN pipeline on Open Graph Benchmark datasets: For example, we improve the accuracy of the top-ranked method GAMLP from 68.25% to 69.67%, SGC from 63.29% to 66.10% and MLP from 47.24% to 61.10% on the ogbn-papers100M dataset by leveraging GIANT. Our code can be found at: https://github.com/amzn/pecos/tree/ mainline/examples/giant-xrt * This work was done during Eli Chien's internship at Amazon Search, USA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The ubiquity of graph-structured data and its importance in solving various real-world problems such as node and graph classification have made graph-centered machine learning an important research area <ref type="bibr" target="#b37">(Lü &amp; Zhou, 2011;</ref><ref type="bibr" target="#b46">Shervashidze et al., 2011;</ref><ref type="bibr" target="#b67">Zhu, 2005)</ref>. Graph neural networks (GNNs) offer state-of-the-art performance on many graph learning tasks and have by now become a standard methodology in the field <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b12">Hamilton et al., 2017;</ref><ref type="bibr" target="#b48">Velickovic et al., 2018;</ref><ref type="bibr" target="#b5">Chien et al., 2020)</ref>. In most such studies, GNNs take graphs with numerical node attributes as inputs and train them with task-specific labels. Note that only the language model (i.e., the Transformers) in GIANT can use the correlation between graph topology and raw text. Nevertheless, GIANT can work with other types of input data formats, such as images and audio; the study of these models is deferred to future work. Right: Illustration of the connection between our neighborhood prediction and the XMC problem. We use graph information to self-supervise fine-tuning of the text encoder Φ (i.e., the Transformer) in our neighborhood prediction problem. The resulting fine-tuned text encoder is then used to generate numerical node features X GIANT for use in downstream tasks (see also Figure <ref type="figure">2</ref> and the description in Section 3).</p><p>Recent research has shown that self-supervised learning (SSL) leads to performance improvements in many applications, including graph learning, natural language processing and computer vision. Several SSL approaches have also been successfully used with GNNs <ref type="bibr" target="#b16">(Hu et al., 2020b;</ref><ref type="bibr" target="#b57">You et al., 2018;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b17">Hu et al., 2020c;</ref><ref type="bibr" target="#b49">Velickovic et al., 2019;</ref><ref type="bibr" target="#b23">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b6">Deng et al., 2020)</ref>. The common idea behind these works is to explore the correlated information provided by the numerical node features and graph topology, which can lead to improved node representations and GNN initialization. However, one critical yet neglected issue in the current graph learning literature is how to actually obtain the numerical node features from raw data such as text, images and audio signals. As an example, when dealing with raw text features, the standard approach is to apply graph-agnostic methods such as bag-of-words, word2vec <ref type="bibr" target="#b39">(Mikolov et al., 2013)</ref> or pre-trained BERT <ref type="bibr" target="#b8">(Devlin et al., 2018</ref>) (As a further example, raw texts of product descriptions are used to construct node features via the bag-of-words model for benchmarking GNNs on the ogbn-products dataset <ref type="bibr" target="#b15">(Hu et al., 2020a;</ref><ref type="bibr" target="#b4">Chiang et al., 2019)</ref>). The pre-trained BERT language model, as well as convolutional neural networks (CNNs) <ref type="bibr" target="#b11">(Goyal et al., 2019;</ref><ref type="bibr" target="#b26">Kolesnikov et al., 2019)</ref>, produce numerical features that can significantly improve the performance of various downstream learners <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>. Still, none of these works leverage graph information for actual self-supervision. Clearly, using graphagnostic methods to extract numerical features is sub-optimal, as correlations between the graph topology and raw features are ignored.</p><p>Motivated by the recent success of SSL approaches for GNNs, we propose GIANT, an SSL framework that resolves the aforementioned issue of graph-agnostic feature extraction in the standard GNN learning pipeline. Our framework takes raw node attributes and generates numerical node features with graph-structured self-supervision. To integrate the graph topology information into language models such as BERT, we also propose a novel SSL task termed neighborhood prediction, which works for both homophilous and heterophilous graphs, and establish connections between neighborhood prediction and the eXtreme Multi-label Classification (XMC) problem <ref type="bibr" target="#b45">(Shen et al., 2020;</ref><ref type="bibr" target="#b60">Yu et al., 2020;</ref><ref type="bibr" target="#b4">Chang et al., 2020b)</ref>. Roughly speaking, the neighborhood of each node can be encoded using binary multi-labels (indicating whether a node is a neighbor or not) and the BERT model is fine-tuned by successively improving the predicted neighborhoods. This approach allows us to not only leverage the advanced solvers for the XMC problem and address the issue of graph-agnostic feature extraction, but also to perform a theoretical study of the XMC problem and determine its importance in the context of graph-guided SSL.</p><p>Throughout the work, we focus on raw texts as these are the most common data used for largescale graph benchmarking. Examples include titles/abstracts in citation networks and product descriptions in co-purchase networks. To solve our proposed self-supervised XMC task, we adopt the state-of-the-art XR-Transformer method <ref type="bibr" target="#b64">(Zhang et al., 2021a)</ref>. By using the encoder from the XR-Transformer pre-trained with GIANT, we obtain informative numerical node features which consistently boost the performance of GNNs on downstream tasks.</p><p>Notably, GIANT significantly improves state-of-the-art methods for node classification tasks described on the Open Graph Benchmark (OGB) <ref type="bibr" target="#b15">(Hu et al., 2020a)</ref> leaderboard on three large-scale graph datasets, with absolute improvements in accuracy roughly 1.5% for the first-ranked methods, 3% for standard GNNs and 14% for multilayer perceptron (MLP). GIANT coupled with XR-Transformer is also highly scalable and can be combined with other downstream learning methods.</p><p>Our contributions may be summarized as follows.</p><p>1. We identify the issue of graph-agnostic feature extraction in standard GNN pipelines and propose a new GIANT self-supervised framework as a solution to the problem.</p><p>2. We introduce a new approach to numerical feature extraction supervised by graph information based on the idea of neighborhood prediction. The gist of the approach is to use neighborhood prediction within a language model such as BERT to guide the process of fine-tuning the features. Unlike link-prediction, neighborhood prediction resolves problems associated with heterophilic graphs.</p><p>3. We establish pertinent connections between neighborhood prediction and the XMC problem by noting that neighborhoods of individual nodes can be encoded by binary vectors which may be interpreted as multi-labels. This allows for performing neighborhood prediction via XR-Transformers, especially designed to solve XMC problems at scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>We demonstrate through extensive experiments that GIANT consistently improves the performance of tested GNNs on downstream tasks by large margins. We also report new state-of-the-art results on the OGB leaderboard, including absolute improvements in accuracy roughly 1.5% compared to the top-ranked method, 3% for standard GNNs and 14% for multilayer perceptron (MLP).</p><p>More precisely, we improve the accuracy of the top-ranked method GAMLP <ref type="bibr" target="#b65">(Zhang et al., 2021b)</ref> from 68.25% to 69.67%, SGC <ref type="bibr" target="#b51">(Wu et al., 2019)</ref> from 63.29% to 66.10% and MLP from 47.24% to 61.10% on the ogbn-papers100M dataset.</p><p>5. We present a new theoretical analysis that verifies the benefits of key components in XR-Transformers on our neighborhood prediction task. This analysis also further improves our understanding of XR-Transformers and the XMC problem.</p><p>Due to the space limitation, all proofs are deferred to the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>General notation. Throughout the paper, we use bold capital letters such as A to denote matrices. We use A i for the i-th row of the matrix and A ij for its entry in row i and column j. We reserve bold lowercase letters such as a for vectors. The symbol I denotes the identity matrix while 1 denotes the all-ones vector. We use o(•), O(•), ω(•), Θ(•) in the standard manner.</p><p>SSL in GNNs. SSL is a topic of substantial interest due to its potential for improving the performance of GNNs on various tasks. Exploiting the correlation between node features and the graph structure is known to lead to better node representations or GNN initialization <ref type="bibr" target="#b16">(Hu et al., 2020b;</ref><ref type="bibr" target="#b57">You et al., 2018;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b17">Hu et al., 2020c)</ref>. Several methods have been proposed for improving node representations, including (variational) graph autoencoders <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016)</ref>, Deep Graph Infomax <ref type="bibr" target="#b49">(Velickovic et al., 2019)</ref> and GraphZoom <ref type="bibr" target="#b6">(Deng et al., 2020)</ref>. For more information, the interested reader is referred to a survey of SSL GNNs <ref type="bibr" target="#b52">(Xie et al., 2021)</ref>. While these methods can be used as SSL modules in GNNs (Figure <ref type="figure" target="#fig_0">1</ref>), it is clear that they do not solve the described graph agnostics issue in the standard GNN pipeline. Furthermore, as the above described SSL GNNs modules and other pre-processing and post-processing methods for GNNs such as C&amp;S <ref type="bibr" target="#b19">(Huang et al., 2021)</ref> and FLAG <ref type="bibr" target="#b27">(Kong et al., 2020)</ref> in general improve graph learners, it is worth pointing out that they can be naturally be integrated into the GIANT framework. This topic is left as a future work.</p><p>The XMC problem, PECOS and XR-Transformer. The XMC problem can be succinctly formulated as follows: We are given a training set {T i , y i } n i=1 , where T i ∈ D is the ith input text instance and y i ∈ {0, 1} L is the target multi-label from an extremely large collection of labels. The goal is to learn a function f : D × [L] → R, where f (T, l) captures the relevance between the input text T and the label l. The XMC problem is of importance in many real-world applications <ref type="bibr" target="#b20">(Jiang et al., 2021;</ref><ref type="bibr" target="#b56">Ye et al., 2020)</ref>: For example, in E-commerce dynamic search advertising, XMC arises when trying to find a "good" mapping from items to bid queries on the market <ref type="bibr" target="#b43">(Prabhu et al., 2018;</ref><ref type="bibr" target="#b42">Prabhu &amp; Varma, 2014)</ref>. In open-domain question answering, XMC problems arise when trying to map questions to "evidence" passages containing the answers <ref type="bibr" target="#b3">(Chang et al., 2020a;</ref><ref type="bibr" target="#b29">Lee et al., 2019)</ref>. Many methods for the XMC problem leverage hierarchical clustering approaches for labels <ref type="bibr" target="#b43">(Prabhu et al., 2018;</ref><ref type="bibr" target="#b58">You et al., 2019)</ref>. This organizational structure allows one to handle potentially enormous numbers of labels, such as used by PECOS <ref type="bibr" target="#b60">(Yu et al., 2020)</ref>. The key is to take advantage of the correlations among labels within the hierarchical clustering. In our approach, we observe that the multi-labels correspond to neighborhoods of nodes in the given graph. These have to be predicted using the textual information in order to best match the a priori given graph topology. This is achieved this by using the state-of-the-art XR-Transformer <ref type="bibr" target="#b64">(Zhang et al., 2021a)</ref> method for solving the XMC problem. The high-level idea is to first cluster the output labels, and then learn the instance-to-cluster "matchers" (please refer to Figure <ref type="figure">2</ref>). Note that many other methods have used PECOS (including XR-Transformers) for solving large-scale real-world learning problems <ref type="bibr" target="#b9">(Etter et al., 2021;</ref><ref type="bibr" target="#b35">Liu et al., 2021;</ref><ref type="bibr" target="#b4">Chang et al., 2020b;</ref><ref type="bibr" target="#b0">Baharav et al., 2021;</ref><ref type="bibr" target="#b4">Chang et al., 2021;</ref><ref type="bibr" target="#b53">Yadav et al., 2021;</ref><ref type="bibr" target="#b44">Sen et al., 2021)</ref>, but not in the context of self-supervised numerical feature extraction as done in our work.</p><p>GNNs with raw text data. It is conceptually possible to jointly train BERT and GNNs in an end-to-end fashion, which could potentially resolve the issue of being graph agnostic in the standard pipeline. However, the excessive model complexity of BERT makes such a combination practically prohibitive due to GPU memory limitations. Furthermore, it is nontrivial to train this combination of methods with arbitrary mini-batch sizes <ref type="bibr" target="#b4">(Chiang et al., 2019;</ref><ref type="bibr" target="#b61">Zeng et al., 2020)</ref>. In contrast, the XR-Transformer architecture naturally supports mini-batch training and scales well <ref type="bibr" target="#b20">(Jiang et al., 2021)</ref>. Hence, our GIANT method uses XR-Transformers instead of combinations of BERT and GNNs. To the best of our knowledge, we are aware of only one prior work that uses raw text inputs for node classification problem <ref type="bibr">(Zhang et al., 2020)</ref>, but it still follows the standard pipeline described in Figure <ref type="figure" target="#fig_0">1</ref>. Some other works apply GNNs on texts and for document classification, where the actual graphs are constructed based on the raw text. This is clearly not the focus of this work <ref type="bibr" target="#b55">(Yao et al., 2019;</ref><ref type="bibr" target="#b18">Huang et al., 2019;</ref><ref type="bibr">Zhang &amp; Zhang, 2020;</ref><ref type="bibr" target="#b34">Liu et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>Our goal is to resolve the issue of graph-agnostic numerical feature extraction for standard GNN learning pipelines. Although our interest lies in raw text data, as already pointed out, the proposed methodology can be easily extended to account for other types of raw data and corresponding feature extraction methods.</p><p>To this end, consider a large-scale graph G with node set V = {1, 2, . . . , n} and adjacency matrix A ∈ {0, 1} n×n . Each node i is associated with some raw text, which we denote by T i . The language model is treated as an encoder Φ that maps the raw text T i to numerical node feature X i ∈ R d . Key to our SSL approach is the task of neighborhood prediction, which aims to determine the neighborhood A i from T i . The neighborhood vector A i can be viewed as a target multi-label y i for node i, where we have L = n labels. Hence, neighborhood prediction represents an instance of the XMC problem, which we solve by leveraging XR-Transformers. The trained encoder in a XR-Transformer generates informative numerical node features, which can then be used further in downstream tasks, the SSL GNNs module and for GNN pre-training.</p><p>Detailed description regarding the use of XR-Transformers for Neighborhood Prediction. The most straightforward instance of the XMC problem is the one-versus-all (OVA) model, which can be formalized as</p><formula xml:id="formula_0">f (T, l) = w T l Φ(T ); l ∈ [L]</formula><p>, where W = [w 1 , . . . , w L ] ∈ R d×L are weight vectors and Φ : D → R d is the encoder that maps T to a d-dimensional feature vector. OVA can be a deterministic model such as bag-of-words, the Term Frequency-Inverse Document Frequency (TFIDF) model or some other model with learnable parameters, such as XLNet <ref type="bibr" target="#b54">(Yang et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b36">(Liu et al., 2019b)</ref>. We choose to work with pre-trained BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>. Also, one can change Φ according to the type of input data format (i.e., CNNs for images). Despite their simple formulation, it is known <ref type="bibr" target="#b4">Chang et al. (2020b)</ref> that fine-tuning transformer models directly on large output spaces can be prohibitively complex. For neighborhood prediction, L = n, • Find the most relevant clusters recursively (from coarse-to-fine).</p><p>• Results from higher level will be used in lower level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-based matching</head><p>Figure <ref type="figure">2</ref>: Illustration of the use of XR-Transformers.</p><p>Step 1: Perform semantic hierarchical clustering of target labels (neighborhoods) to build a tree.</p><p>Step 2: At each intermediate (internal node) level of the tree, fine-tune the Transformers for the XMC sub-problem that maps raw text of nodes to label clusters. Note that the results of higher levels are used to guide the Transformers at lower levels and hence improve their performance. The resulting Transformers are used as encoders that generate numerical node features from raw texts. Note that we can change the encoder (e.g., Transformer) to address other raw data formats such as images or audio signals.</p><p>and the graphs encountered may have millions of nodes. Hence, we need a more scalable approach to training Transformers.</p><p>As part of an XR-Transformer, one builds a hierarchical label clustering tree based on the label features Z ∈ R L×d ; Z is based on Positive Instance Feature Aggregation (PIFA):</p><formula xml:id="formula_1">Z l = v l v l , where v l = i:y i,l =1 Ψ(T i ), ∀l ∈ [L].<label>(1)</label></formula><p>Note that for neighborhood prediction, the above expression represents exactly one step of a graph convolution with node features Ψ(T i ), followed by a norm normalization; here, Ψ(•) denotes some text vectorizer such as bag-of-words or TFIDF. In the next step, XR-Transformer uses balanced kmeans to recursively partition label sets and generate the hierarchical label cluster tree in a top-down fashion. This step corresponds to Step 1 in Figure <ref type="figure">2</ref>. Note that at each intermediate level, it learns a matcher to find the most relevant clusters, as illustrated in Step 2 of Figure <ref type="figure">2</ref>. By leveraging the label hierarchy defined by the cluster tree, the XR-Transformer can train the model on multi-resolution objectives. Multi-resolution learning has been used in many different contexts, including computer vision <ref type="bibr" target="#b28">(Lai et al., 2017;</ref><ref type="bibr" target="#b21">Karras et al., 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b40">Pedersoli et al., 2015)</ref>, meta-learning <ref type="bibr" target="#b33">(Liu et al., 2019a</ref>), but has only recently been applied to the XMC problem as part of PECOS and XR-Transformers. For neighborhood prediction, multi-resolution amounts to generating a hierarchy of coarse-to-fine views of neighborhoods. The only line of work in self-supervised graph learning that somewhat resembles this approach is GraphZoom <ref type="bibr" target="#b6">(Deng et al., 2020)</ref>, in so far that it applies SSL on coarsened graphs. Nevertheless, the way in which we perform coarsening is substantially different; furthermore, GraphZoom still falls into the standard GNN pipeline category depicted in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL ANALYSIS</head><p>We also provide theoretical evidence in support of using each component of our proposed learning framework. First, we show that self-supervised neighborhood prediction is better suited to the task at hand than standard link prediction. More specifically, we show that the standard design criteria in self-supervised link prediction tasks are biased towards graph homophily assumptions <ref type="bibr" target="#b38">(McPherson et al., 2001;</ref><ref type="bibr" target="#b25">Klicpera et al., 2018)</ref>. In contrast, our self-supervised neighborhood prediction model works for both homophilic and heterophilic graphs. This universality property is crucial for the robustness of graph learning methods, especially in relationship to GNNs <ref type="bibr" target="#b5">(Chien et al., 2020)</ref>. Second, we demonstrate the benefits of using PIFA embeddings and clustering in XR-Transformers for graph-guided numerical feature extraction. Our analysis is based on the contextual stochastic block model (cSBM) <ref type="bibr" target="#b7">(Deshpande et al., 2018)</ref>, which was also used in <ref type="bibr" target="#b5">Chien et al. (2020)</ref> for testing the GPR-GNN framework and in <ref type="bibr" target="#b2">Baranwal et al. (2021)</ref> for establishing the utility of graph convolutions for node classification.</p><p>Link versus neighborhood prediction. One standard SSL task on graphs is link prediction, which aims to find an entry in the adjacency matrix according to</p><formula xml:id="formula_2">P (A ij = 1) ∝ Similarity (Φ(T i ), Φ(T j )) .<label>(2)</label></formula><p>Here, the function Similarity(x, y) is a measure of similarity of two vectors, x and y. The most frequently used choice for the function is the inner product of two input vectors followed by a sigmoid function. However, this type of design implicitly relies on the homophily assumption:  A simple example that shows how SSL link prediction may fail is presented in Figure <ref type="figure" target="#fig_3">3</ref>. Nodes of the same color share the same features (these are for simplicity represented as numerical values).</p><formula xml:id="formula_3">Nodes</formula><p>Clearly, no matter what encoder Φ we have, the similarity of node features for nodes of the same color is the highest. However, there is no edge between nodes of the same color, hence the standard methodology of link prediction based on homophily assumption fails to work for this simple heterophilous graph. In order to fix this issue, we use a different modeling assumption, stated below.</p><p>Assumption 4.1. Nodes with similar node features have similar "structural roles" in the graph.</p><p>In our study, we equate "structure" with the 1-hop neighborhood of a node (i.e., the row of the adjacency matrix indexed by the underlying node).</p><p>The above assumption is in alignment with our XMC problem assumptions, where nodes with a small perturbation in their raw text should be mapped to a similar multi-label. Our assumption is more general then the standard homophily assumption; it is also clear that there exists a perfect mapping from node features to their neighborhoods for the example in Figure <ref type="figure" target="#fig_3">3</ref>. Hence, neighborhood prediction appears to be a more suitable SSL approach than SSL link prediction for graph-guided feature extraction.</p><p>Analysis of key components in XR-Transformers. In the original XR-Transformer work <ref type="bibr" target="#b64">(Zhang et al., 2021a)</ref>, the authors argued that one needs to perform clustering of the multi-label space in order to resolve scarce training instances in XMC. They also empirically showed that directly finetuning language models on extremely large output spaces is prohibitive. Furthermore, they empirically established that constructing clusters based on PIFA embedding with TFIDF features gives the best performance. However, no theoretical evidence was given in support of this approach to solving the XMC problem. We next leverage recent advances in graph learning to analytically characterize the benefits of using XR-Transformers. Description of the cSBM. Using our Assumption 4.1, we analyze the case where the graph and node features are generated according to a cSBM <ref type="bibr" target="#b7">(Deshpande et al., 2018)</ref> (see Figure <ref type="figure" target="#fig_4">4</ref>). For simplicity, we use the most straightforward two-cluster cSBM. Let {y i } n i=1 ∈ {0, 1} be the labels of nodes in a graph. We denote the size of class j ∈ {0, 1} by C j = |{i : y i = j, ∀i ∈ [n]}|. We also assume that the classes are balanced, i.e.,</p><formula xml:id="formula_4">C 0 = C 1 = n 2 . The node features {X i } n i=1 ∈ R d are independent d-dimensional Gaussian random vectors, such that X i ∼ N ( r √ d 1, σ 2 d I) if y i = 0 and X i ∼ N (− r √ d 1, σ 2 d I) if y i = 1.</formula><p>The adjacency matrix of the cSBM is denoted by A, and is clearly symmetric. All edges are drawn according to independent Bernoulli random variables, so that A ij ∼ Ber(p) if y i = y j and A ij ∼ Ber(q) if y i = y j . Our analysis is inspired by <ref type="bibr" target="#b2">Baranwal et al. (2021)</ref> and <ref type="bibr" target="#b18">Li et al. (2019)</ref>, albeit their definitions of graph convolutions and random walks differ from those in PIFA. For our subsequent analysis, we also make use of the following standard assumption and define the notion of effect size.</p><p>Assumption 4.2. p, q = ω( log n n ). |p−q| p+q = Θ(1). d = o(n). 0 &lt; r, σ = Θ(1).</p><p>Note that <ref type="bibr" target="#b2">Baranwal et al. (2021)</ref> also poses constraints on p, q, |p−q| p+q and d. In contrast, we do not require p − q &gt; 0 to hold <ref type="bibr" target="#b2">(Baranwal et al., 2021;</ref><ref type="bibr" target="#b18">Li et al., 2019</ref>) so that we can address graph structures that are either homophilic or heterophilic. Due to the difference between PIFA and  <ref type="table" target="#tab_1">papers100M 111,059,956 1,615,685,872</ref> 29.1 78/8/14 Accuracy standard graph convolution, we require p, q to be larger compared to the corresponding values used in <ref type="bibr" target="#b2">Baranwal et al. (2021)</ref>. Definition 4.3. For cSBM, the effect size of the two centroids of the node features X of the two different classes is defined as</p><formula xml:id="formula_5">EX i − EX j E X i − EX i 2 + E X j − EX j 2</formula><p>, where y i = y j .</p><p>(3)</p><p>In the standard definition of effect size, the mean difference is divided by the standard deviation of a class, as the latter is assumed to be the same for both classes. We use the sum of both standard deviations to prevent any ambiguity in our definition. Note that for the case of isotropic Gaussian distributions, the larger the effect size the larger the separation of the two classes.</p><p>Theoretical results. We are now ready to state our main theoretical result, which asserts that the effect size of centroids for PIFA embeddings Z is asymptotically larger than that obtained from the original node features. Our Theorem 4.4 provides strong evidence that using PIFA in XR-Transformers offers improved clustering results and consequently, better feature quality. Theorem 4.4. For the cSBM and under Assumption 4.2, the effect size of the two centroids of the node features X of the two different classes is r σ = Θ(1). Moreover, the effect size of the two centroids of the PIFA embedding Z of the two different classes, conditioned on an event of probability at least 1 − O( 1 n c ) for some constant c &gt; 0, is ω(1).</p><p>We see that although two nodes i, j from the same class have the same neighborhood vectors in expectation, EA i = EA j , their Hamming distance can be large in practice. This finding is formally characterized in Proposition 4.5. Proposition 4.5. For the cSBM and under Assumption 4.2, the Hamming distance between A i and A j with y i = y j is ω( √ n log n) with probability at least 1 − O( 1 n c ), for some c &gt; 0.</p><p>Hence, directly using neighborhood vectors for self-supervision is not advisable. Our result also agrees with findings from the XMC literature <ref type="bibr" target="#b4">(Chang et al., 2020b)</ref>. It is also intuitively clear that averaging neighborhood vectors from the same class can reduce the variance, which is approximately performed by clustering based on node representations (in our case, via a PIFA embedding). This result establishes the importance of clustering within the XR-Transformer approach and for the SSL neighborhood prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Evaluation Datasets. We consider node classification as our downstream task and evaluate GI-ANT on three large-scale OGB datasets <ref type="bibr" target="#b15">(Hu et al., 2020a)</ref> with available raw text: ogbn-arxiv, ogbn-products, and ogbn-papers100M. The parameters of these datasets are given in Table <ref type="table" target="#tab_0">1</ref> and detailed descriptions are available in the Appendix D.1. Following the OGB benchmarking protocol, we report the average test accuracy and the corresponding standard deviation by repeating 3 runs of each downstream GNN model.</p><p>Evaluation Protocol. We refer to our actual implementation as GIANT-XRT since the multi-scale neighborhood prediction task in the proposed GIANT framework is solved by an XR-Transformer.</p><p>In the pre-training stage, GIANT-XRT learns a raw text encoder by optimizing the self-supervised neighborhood prediction objective, and generates a fine-tuned node embedding for later stages. For the node classification downstream tasks, we input the node embeddings from GIANT-XRT into several different GNN models. One is the multi-layer perceptron (MLP), which does not use graph information. Two other methods are GraphSAGE <ref type="bibr" target="#b12">(Hamilton et al., 2017)</ref>, which we applied to ogbn-arxiv, and GraphSAINT <ref type="bibr" target="#b61">(Zeng et al., 2020)</ref>, which we used for ogbn-products as it allows for mini-batch training. Due to scalability issues, we used Simple Graph Convolution (SGC) <ref type="bibr" target="#b51">(Wu et al., 2019)</ref> for ogbn-papers100M. We also tested the state-of-the-art GNN for each dataset.</p><p>At the time we conducted the main experiments (07/01/2021), the top-ranked model for ogbn-arxiv was RevGAT<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b30">(Li et al., 2021)</ref> and the top-ranked model for ogbn-products was SAGN<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b47">(Sun &amp; Wu, 2021)</ref>. When we conducted the experiment on ogbn-papers100M (09/10/2021), the topranked model for ogbn-papers100M was GAMLP<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b65">(Zhang et al., 2021b</ref>) (Since then, the highest reported accuracy was improved by 0.05% for ogbn-arxiv and 0.31% for ogbn-products; both of these improvements fall short compared to those offered by GIANT). For all evaluations, we use publicly available implementations of the GNNs. For RevGAT, we report the performance of the model with and without self knowledge distillation; the former setting is henceforth referred to as +SelfKD. For SAGN, we report results with the self-label-enhanced (SLE) feature, and denote them by SAGN+SLE. For GAMLP, we report results with and without Reliable Label Utilization (RLU); the former is denoted as GAMLP+RLU.</p><p>SSL GNN Competing Methods. We compare GIANT-XRT to methods that rely on graphagnostic feature inputs and use node embeddings generated by various SSL GNNs modules. The graph-agnostic features are either default features available from the OGB datasets (denoted by OGB-feat) or obtained from plain BERT embeddings (without fine-tuning) generated from raw text (denoted by BERT ). For OGB-feat combined with downstream GNN methods, we report the results from the OGB leaderboard (and denote them by † ). For the SSL GNNs modules, we test three frequently-used methods: (Variantional) Graph AutoEncoders <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016</ref>) (denoted by (V)GAE); Deep Graph Infomax <ref type="bibr" target="#b49">(Velickovic et al., 2019)</ref> (denoted by DGI); and GraphZoom <ref type="bibr" target="#b6">(Deng et al., 2020)</ref> (denoted by GZ). The hyper-parameters of SSL GNNs modules are given in the Appendix D.2. For all reported results, we use X plain , X SSLGNN and X GIANT (c.f. Figure <ref type="figure" target="#fig_0">1</ref>) to denote which framework the method belongs to. Note that X GIANT refers to our approach. The implementation details and hyper-parameters of GIANT-XRT can be founded in the Appendix D.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MAIN RESULTS</head><p>The results for the ogbn-arxiv and ogbn-products datasets are listed in Table <ref type="table" target="#tab_1">2</ref>. Our GIANT-XRT approach gives the best results for both datasets and all downstream models. It improves the accuracy of the top-ranked OGB leaderboard models by a large margin: 1.86% on ogbn-arxiv and 1.19% on ogbn-products. Using graph-agnostic BERT embeddings does not necessarily lead to good results (see the first two rows in Table <ref type="table" target="#tab_1">2</ref>). This shows that the improvement of our method is not merely due to the use of a more powerful language model, and establishes the need for self-supervision governed by graph information. Another observation is that among possible combinations involving a standard GNN pipeline with a SSL module, BERT+(V)GAE offers the best performance. This can be attributed to exploiting the correlation between numerical node features and the graph structure, albeit in a two-stage approach within the standard GNN pipeline. The important finding is that using node features generated by GIANT-XRT leads to consistent and significant improvements in the accuracy of all tested methods, when compared to the standard GNN pipeline: In particular, on ogbn-arxiv, the improvement equals 17.58% for MLP and 3.1% for GraphSAGE; on ogbn-products, the improvement equals 18.76% for MLP and 5.32% for GraphSAINT.</p><p>Another important observation is that GIANT-XRT is highly scalable, which can be clearly observed on the example of the ogbn-papers100M dataset, for which the results are shown in Table <ref type="table" target="#tab_2">3</ref>.</p><p>In particular, GIANT-XRT improves the accuracy of the top-ranked model, GAMLP-RLU, by a margin of 1.42%. Furthermore, GIANT-XRT again consistently improves all tested downstream methods on the ogbn-papers100M dataset. As a final remark, we surprisingly find that combining MLP with GIANT-XRT greatly improves the performance of the former learner on all datasets. It becomes just slightly worse then GIANT-XRT+GNNs and can even outperform the GraphSAGE and GraphSAINT methods with default OGB features on ogbn-arxiv and ogbn-products datasets. This is yet another positive property of GIANT, since MLPs are low-complexity and more easily implementable than other GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ABLATION STUDY</head><p>We also conduct an ablation study of the GIANT framework to determine the relevance of each module involved. The first step is to consider alternatives to the proposed multi-scale neighborhood prediction task: In this case, we fine-tune BERT with a SSL link prediction approach, which we for simplicity refer to as BERT+LP. In addition, we examine how the PIFA embedding affects the performance of GIANT-XRT and how more informative node features (TFIDF) can improve the clustering steps. First, recall that in GIANT-XRT, we use TFIDF features from raw text to construct PIFA embeddings. We subsequently use the term "NO TFIDF" to indicate that we replaced the TFIDF feature matrix by an identity matrix, which contain no raw text information. The term "TFIDF+NO PIFA" is used to refer to the setting where only raw text information (node attributes) is used to perform hierarchical clustering. Similarly, "NO TFIDF+PIFA" indicates that we only use normalized neighborhood vectors (graph structure) to construct the hierarchical clustering. If both node attributes and graph structure are ignore, the result is a random clustering. Nevertheless, we keep the same sizes of clusters at each level in the hierarchical clustering.</p><p>The results of the ablation study are listed under rows indexed by X GIANT in Table <ref type="table" target="#tab_1">2</ref> for ogbn-arxiv and ogbn-products datasets. They once again confirm that GIANT-XRT consistently outperforms other tested methods. For BERT+LP, we find that it has better performance on ogbn-arixv compared to that of the standard GNN pipeline but a worse performance on ogbn-products. This shows that using link prediction to fine-tune BERT in a self-supervised manner is not robust in general, and further strengthens the case for using neighborhood instead of link prediction. With respect to the ablation study of GIANT-XRT, we see that NO TFIDF+NO PIFA indeed gives the worst results. Using node attributes (TFIDF features) or graph information (PIFA) to construct the hierarchical clustering in GIANT-XRT leads to performance improvements that can be seen from Table <ref type="table" target="#tab_1">2</ref>. Nevertheless, combining both as done in GIANT-XRT gives the best results. Moreover, one can observe that using PIFA always gives better results compared to the case when PIFA is not used. It aligns with our theoretical analysis, which shows that PIFA embeddings lead to better hierarchical clusterings.</p><p>By the i.i.d assumption, we have</p><formula xml:id="formula_6">min t&gt;0 exp(−ta)E exp(tS n ) = min t&gt;0 exp(−ta)(E exp(tX 1 )) n .<label>(33)</label></formula><p>Note that the moment generating function (MGF) of a zero-mean, σ standard deviation Gaussian random variable is exp( σ 2 t 2 2 ). Hence we have</p><formula xml:id="formula_7">min t&gt;0 exp(−ta)(E exp(tX 1 )) n = min t&gt;0 exp( 1 2 nσ 2 t 2 − ta).<label>(34)</label></formula><p>By minimizing the upper bound with respect to t, we can choose t = a nσ 2 . Plug in this choice of t we have</p><formula xml:id="formula_8">P (S n ≥ a) ≤ exp( −a 2 2nσ 2 ).<label>(35)</label></formula><p>Finally, by choosing a = cσ √ n log n for some c &gt; 0, applying the same bound for the other side and the union bound, we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENTAL DETAIL D.1 DATASETS</head><p>In this work, we choose node classification as our downstream task to focus. We conduct experiments on three large-scale datasets, ogbn-arxiv, ogbn-products and ogbn-papers100M as these are the only three datasets with raw text available in OGB. Ogbn-arxiv and ogbn-papers100M <ref type="bibr" target="#b50">(Wang et al., 2020;</ref><ref type="bibr" target="#b15">Hu et al., 2020a)</ref> are citation networks where each node represents a paper. The corresponding input raw text consists of titles and abstracts and the node labels are the primary categories of the papers. <ref type="bibr">Ogbn-products (Chiang et al., 2019;</ref><ref type="bibr" target="#b15">Hu et al., 2020a</ref>) is an Amazon co-purchase network where each node represents a product. The corresponding input raw text consists of titles and descriptions of products. The node labels are categories of products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 HYPER-PARAMETERS OF SSL GNN MODULES</head><p>The implementation of (V)GAE and DGI are taken from Pytorch Geometric Library <ref type="bibr" target="#b10">(Fey &amp; Lenssen, 2019)</ref>. Note that due to the GPU memory constraint, we adopt GraphSAINT <ref type="bibr" target="#b61">(Zeng et al., 2020)</ref> to (V)GAE and DGI for ogbn-products. GraphZoom is directly taken from the official repository<ref type="foot" target="#foot_3">4</ref> . For all downstream GNNs in the experiment, we average the results over three independent runs. The only exception is OGB-feat + downstream GNNs, where we directly take the results from OGB leaderboard. Note that we also try to repeat the experiment of OGB-feat + downstream GNNs, where the accuracy is similar to the one reported on the leaderboard. To prevent confusion we decide to take the results from OGB leaderboard for comparison. For the BERT model used throughout the paper, we use "bert-base-uncased" downloaded from HuggingFace<ref type="foot" target="#foot_4">5</ref> . For the methods used in the SSL GNNs module, we try our best to follow the default setting. We slightly optimize some hyperparameters (such as learning rate, max epochs...etc) to ensure the training process converge. To ensure the fair comparison, we fix the output dimension for all SSL GNNs as 768 which is the same as bert-base-uncased and XR-Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 HYPER-PARAMETERS OF GIANT-XRT AND BERT+LP</head><p>Pre-training of GIANT-XRT. In Table <ref type="table">4</ref>, we outline the pre-training hyper-parameter of GIANT-XRT for all three OGB benchmark datasets. We mostly follow the convention of XR-Transformer <ref type="bibr" target="#b64">(Zhang et al., 2021a)</ref> to set the hyper-parameters. For ogbn-arxiv and ogbn-products datasets, we use the full graph adjacency matrix as the XMC instance-to-label matrix Y ∈ {0, 1} n×n , where n is number of nodes in the graph. For ogbn-papers100M, we sub-sample 50M (out of 111M) most important nodes based on page rank scores of the bipartite graph <ref type="bibr" target="#b13">(He et al., 2016)</ref>. The resulting XMC instance-to-label matrix Y has 50.0M of rows, 49.9M of columns, and 2.5B of edges. The PIFA node embedding for hierarchical clustering is constructed by aggregating its neighborhood nodes' TF-IDF features. Specifically, the PIFA node embedding is a 4.2M highdimensional sparse vector, consisting of 1M of word unigram, 3M of word bigram, and 200K of character triagram. Finally, for ogbn-arxiv and ogbn-products, we use T F N +M AN negative sampling to pre-train XR-Transformer where the model-aware negatives (MAN) are selected from top 20 model's predictions. Because of the extreme scale of ogbn-papers100M, we consider T F N only to avoid excessive CPU memory consumption on the GPU machine.</p><p>Table <ref type="table">4</ref>: Hyper-parameters of GIANT-XRT. HLT defines the structures of the hierarchical label trees. lr max is the maximum learning rate in pre-training. n step is the number of optimization steps for each layer of HLT, respectively. B is total number of batch size when using 8 Nvidia V100 GPUs. N S is the negative sampling strategy in XR-Transformer. D is the Dth layer of HLT where we take the Transformer encoder to generate node embeddings as input for downstream GNN models. Pre-training on BERT+LP. To verify the effectiveness of multi-scale neighborhood prediction loss, we consider learning a Siamese BERT encoder with the alternative Link Prediction loss for pretraining, hence the name BERT+LP. We implement BERT+LP baseline with the triplet loss <ref type="bibr" target="#b1">(Balntas et al., 2016)</ref> as we empirically observed it has better performance than other loss functions for the link prediction task. We sample one positive pair and one negative pair for each node as training samples for each epoch, and train the model until the loss is converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 HYPER-PARAMETERS OF DOWNSTREAM METHODS</head><p>For the downstream models, we optimize the learning rate within {0.01, 0.001} for all models. For MLP, GraphSAGE and GraphSAINT, we optimize the number of layers within {1, 3}. For RevGAT, we keep the hyperparameter choice the same as default. For SAGN, we also optimize the number of layers within {1, 3}. For GAMLP, we directly adopt the setting from the official implementation. Note that all hyperparameter tuning applies for all pre-trained node features (X plain , X SSLGNN and X GIANT ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 COMPUTATIONAL ENVIRONMENT</head><p>All experiments are conducted on the AWS p3dn.24xlarge instance, consisting of 96 Intel Xeon CPUs with 768 GB of RAM and 8 Nvidia V100 GPUs with 32 GB of memory each.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Left: Illustration of the standard GNN pipeline and our GIANT framework. Note that only the language model (i.e., the Transformers) in GIANT can use the correlation between graph topology and raw text. Nevertheless, GIANT can work with other types of input data formats, such as images and audio; the study of these models is deferred to future work. Right: Illustration of the connection between our neighborhood prediction and the XMC problem. We use graph information to self-supervise fine-tuning of the text encoder Φ (i.e., the Transformer) in our neighborhood prediction problem. The resulting fine-tuned text encoder is then used to generate numerical node features X GIANT for use in downstream tasks (see also Figure2and the description in Section 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Machine-learned Matching.   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>with similar node representations are more likely to have links. It has been shown in Pei et al. (2020); Chien et al. (2020); Zhu et al. (2020); Lim et al. (2021) that there are real-world graph datasets that violate the homophily assumption and on which many GNN architectures fail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A counter-example for standard link prediction methodology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of a cSBM: Node features are independent Gaussian random vectors while edges are modeled as independent Bernoulli random variables.</figDesc><graphic url="image-9.png" coords="6,373.97,463.88,129.38,114.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>-2048-8192-32768 6 × 10 −5 10,000 256 T F N +M AN 2 ogbn-papers100M 128-2048-32768 6 × 10 −5 100,000 512 T F N 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of the OGB benchmark datasets<ref type="bibr" target="#b15">(Hu et al., 2020a)</ref>.</figDesc><table><row><cell></cell><cell>#Nodes</cell><cell cols="3">#Edges Avg. Node Degree Split ratio (%)</cell><cell>Metric</cell></row><row><cell>ogbn-arxiv</cell><cell>169,343</cell><cell>1,166,243</cell><cell>13.7</cell><cell>54/18/28</cell><cell>Accuracy</cell></row><row><cell>ogbn-products</cell><cell>2,449,029</cell><cell>61,859,140</cell><cell>50.5</cell><cell>8/2/90</cell><cell>Accuracy</cell></row><row><cell>ogbn-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results for the obgn-arxiv and ogbn-products datasets. Mean accuracy (%) ± one standard deviation. Boldfaced numbers indicate the best performances of downstream models, while underlined numbers indicate the best performance of models with a standard GNN pipeline for downstream models using X plain and X SSLGNN . Methods under X GIANT (GIANT framework) are part of the ablation study.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell></cell><cell cols="2">ogbn-arxiv</cell><cell></cell><cell>ogbn-products</cell></row><row><cell></cell><cell></cell><cell>MLP</cell><cell>GraphSAGE</cell><cell cols="2">RevGAT RevGAT+SelfKD</cell><cell>MLP</cell><cell>GraphSAINT SAGN+SLE</cell></row><row><cell>Xplain</cell><cell>OGB-feat  † BERT</cell><cell cols="3">55.50 ± 0.23 71.49 ± 0.27 74.02 ± 0.18 62.91 ± 0.60 70.97 ± 0.33 73.59 ± 0.10</cell><cell>74.26 ± 0.17 73.55 ± 0.41</cell><cell>61.06 ± 0.08 79.08 ± 0.24 84.28 ± 0.14 60.90 ± 1.09 79.55 ± 0.85 83.11 ± 0.18</cell></row><row><cell></cell><cell>OGB-feat+GZ</cell><cell cols="3">70.95 ± 0.38 71.41 ± 0.09 72.42 ± 0.16</cell><cell>72.50 ± 0.08</cell><cell>74.19 ± 0.55 78.38 ± 0.21 79.78 ± 0.11</cell></row><row><cell></cell><cell>BERT +GZ</cell><cell cols="3">70.46 ± 0.21 71.24 ± 0.19 72.33 ± 0.06</cell><cell>72.30 ± 0.20</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell></cell><cell>OGB-feat+DGI</cell><cell cols="3">56.02 ± 0.16 71.72 ± 0.26 73.48 ± 0.14</cell><cell>73.90 ± 0.26</cell><cell>70.54 ± 0.13 79.26 ± 0.16 81.59 ± 0.14</cell></row><row><cell>XSSLGNN</cell><cell>BERT +DGI OGB-feat+GAE</cell><cell cols="3">59.42 ± 0.38 72.15 ± 0.06 73.24 ± 0.25 56.47 ± 0.08 72.00 ± 0.27 73.70 ± 0.28</cell><cell>73.60 ± 0.21 74.06 ± 0.10</cell><cell>73.62 ± 0.23 81.29 ± 0.41 82.90 ± 0.21 74.81 ± 0.22 78.23 ± 0.10 82.85 ± 0.11</cell></row><row><cell></cell><cell>BERT +GAE</cell><cell cols="3">62.11 ± 0.32 72.72 ± 0.17 74.26 ± 0.20</cell><cell>74.48 ± 0.15</cell><cell>78.42 ± 0.14 82.74 ± 0.16 84.42 ± 0.04</cell></row><row><cell></cell><cell>OGB-feat+VGAE</cell><cell cols="3">56.70 ± 0.20 72.04 ± 0.29 73.59 ± 0.17</cell><cell>73.95 ± 0.09</cell><cell>74.66 ± 0.10 78.65 ± 0.20 83.06 ± 0.06</cell></row><row><cell></cell><cell>BERT +VGAE</cell><cell cols="3">62.48 ± 0.14 72.92 ± 0.02 74.21 ± 0.01</cell><cell>74.44 ± 0.09</cell><cell>78.81 ± 0.25 82.80 ± 0.11 84.40 ± 0.09</cell></row><row><cell></cell><cell>BERT+LP</cell><cell cols="3">67.33 ± 0.54 66.61 ± 2.86 75.50 ± 0.11</cell><cell>75.75 ± 0.04</cell><cell>73.83 ± 0.06 81.66 ± 0.08 82.33 ± 0.16</cell></row><row><cell>XGIANT</cell><cell cols="4">NO TFIDF+ NO PIFA 69.33 ± 0.19 73.41 ± 0.34 74.95 ± 0.07 NO TFIDF+PIFA 72.74 ± 0.17 74.43 ± 0.20 75.88 ± 0.05</cell><cell>75.16 ± 0.06 76.06 ± 0.02</cell><cell>74.16 ± 0.22 80.70 ± 0.51 81.63 ± 0.28 78.91 ± 0.28 81.54 ± 0.14 82.22 ± 0.15</cell></row><row><cell></cell><cell>TFIDF+NO PIFA</cell><cell cols="3">71.74 ± 0.15 74.09 ± 0.33 75.56 ± 0.09</cell><cell>75.85 ± 0.05</cell><cell>79.37 ± 0.15 83.83 ± 0.14 85.01 ± 0.10</cell></row><row><cell></cell><cell>GIANT-XRT</cell><cell cols="3">73.08 ± 0.06 74.59 ± 0.28 75.96 ± 0.09</cell><cell>76.12 ± 0.16</cell><cell>79.82 ± 0.07 84.40 ± 0.17 85.47 ± 0.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results for the obgn-papers100M dataset. Mean accuracy (%) ± one standard deviation. Boldfaced values indicate the best performance amongst the tested downstream models.</figDesc><table><row><cell></cell><cell>ogbn-papers100M</cell><cell>MLP</cell><cell>SGC</cell><cell>GAMLP GAMLP+RLU</cell></row><row><cell>Xplain</cell><cell>OGB-feat  † BERT</cell><cell cols="3">47.24 ± 0.31 63.29 ± 0.19 67.71 ± 0.20 68.25 ± 0.19 47.24 ± 0.39 61.69 ± 0.29 66.25 ± 0.05 67.15 ± 0.07</cell></row><row><cell>XGIANT</cell><cell>GIANT-XRT</cell><cell cols="3">61.10 ± 0.19 66.10 ± 0.13 69.16 ± 0.08 69.67 ± 0.04</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/lightaime/deep_gcns_torch/tree/master/examples/ogb_ eff/ogbn_arxiv_dgl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/skepsun/SAGN_with_SLE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/PKU-DAIR/GAMLP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/cornell-zhang/GraphZoom</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://huggingface.co/bert-base-uncased</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We introduced a novel self-supervised learning framework for graph-guided numerical node feature extraction from raw data, and evaluated it within multiple GNN pipelines. Our method, termed GIANT, for the first time successfully resolved the issue of graph-agnostic numerical feature extraction. We also described a new SSL task, neighborhood prediction, established a connection between the task and the XMC problem, and solved it using XR-Transformers. We also examined the theoretical properties of GIANT in order to evaluate the advantages of neighborhood prediction over standard link prediction, and to assess the benefits of using XR-Transformers. Our extensive numerical experiments, which showed that GIANT consistently improves state-of-the-art GNN models, were supplemented with an ablation study that aligns with our theoretical analysis.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>A PROOF OF <ref type="bibr">THEOREM 4.4</ref> Throughout the proof, we use µ = r d 1 to denote the mean of node feature from class 0 and ν = −r d 1 for class 1. We choose to keep this notation to demonstrate that our setting on mean can be easily generalized. The choice of the high probability events will be clear in the proof.</p><p>The proof for the effect size of centroid for node feature X is quite straightforward from the Definition 4.3. By directly plugging in the mean and standard deviation, we have</p><p>The last equality is due to our assumption that both r, σ &gt; 0 are both constants.</p><p>To prove the effect size of centroid for PIFA embedding Z, we need to first introduce some standard concentration results for sum of Bernoulli and Gaussian random variables. Lemma A.1 (Hoeffiding's inequality <ref type="bibr" target="#b14">(Hoeffding, 1994)</ref>).</p><p>Let S n = n i=1 X n , where X i are i.i.d. Bernoulli random variable with parameter p. Then for any t &gt; 0, we have</p><p>, where X i are i.i.d. Gaussian random variable with zero mean and standard deviation σ. Then for some constant c &gt; 0, we have</p><p>Now we are ready to prove our claim. Recall that the definition of PIFA embedding Z is as follows:</p><p>We first focus on analyzing the vector v i . We denote N iy = |{j :</p><p>Without loss of generality, we assume y i = 0. The conditional expectation of it is as follows:</p><p>Next, by leveraging Lemma A.1, we have</p><p>By choosing t = c 1 √ n log n for some constant c 1 &gt; 1, we know that with probability at least</p><p>Finally, by our Assumption 4.2, we know that nq = ω( √ n log n). Thus, we arrive to the conclusion that with probability at least</p><p>. Following the same analysis, we can prove that with probability at least 1−O( 1 n c 1 ), N i0 = np 2 ±o(1). The only difference is that we are dealing with n 2 − 1 random variables in this case as there's no self-loop. Nevertheless,</p><p>) so the result is the same. Note that we need to apply union bound over 2n error events (∀i ∈ [n] and 2 cases for N i0 and N i1 respectively). Together we know that the error probability is upper bounded by O( 1 n c 2 ) for some new constant c 2 = c 1 − 1 &gt; 0. Hence, we characterize the mean of v i on a high probability event B 1 .</p><p>Next we need to analyze its norm. By direct analysis and condition on A, we have</p><p>where G jk are i.i.d. Gaussian random variables with zero mean, σ standard deviation and d = stands for equal in distribution. Then by Lemma A.2 we know that with probability at least</p><p>This is because condition on A, we are summing over N i Gaussian random variables. Recall that condition on our high probability event B 1 ,</p><p>Thus, we know that for some c 2 &gt; 0, with probability at least</p><p>Again, we need to apply union bound over nd error events, which result in the error probability</p><p>We denote the corresponding high probability event to be B 2 . Note that the same analysis can be applied to the case y i = 1, where the result for the norm is the same and the result for v i would be just swapping p and q. Combine all the current result, we know that with probability at least 1 − O( 1 n c 2 ) for some c 2 &gt; 0, the PIFA embedding Z i equals to the following</p><p>Hence, the centroid distance would be</p><p>Now we turn to the standard deviation part. Specifically, we will characterize the following quantity (again, recall that we assume y i = 0 w.l.o.g.).</p><p>Recall that the latter part is the centroid for nodes with label 0. Hence, by characterize this quantity we can understand the deviation of PIFA embedding around its centroid. From the analysis above, we know that given A, we have</p><p>For the terms v i , N i0 , N i1 and n j=1 A ij Z j , we already derive their concentration results above. Plug in those results, the first term becomes</p><p>The second term becomes</p><p>where the last equality is from our Assumption 4.2 that r, σ are constants. Together we show that the deviation of nodes from their centroid is of scale o(1). The similar result holds for the case y i = 1. Together we have shown that the standard deviation of Z i is o(1) on the high probability event B 1 ∩ B 2 . Hence, the effect size for the PIFA embedding is ω(1) with probability at least 1 − O( 1 n c 2 ) for some constant c 2 &gt; 0, which implies that PIFA embedding gives a better clustered node representation. Thus, it is preferable to use PIFA embedding and we complete the proof.</p><p>B PROOF OF <ref type="bibr">PROPOSITION 4.5</ref> Note that under the setting of cSBM and the Assumption 4.2, the Hamming distance of A i , A j for y i = y j is a Poisson-Binomial random variable. More precisely, note that</p><p>where they are all independent. Hence, we have</p><p>where Hamming(A i , A j ) denotes the Hamming distance of A i , A j and Bin(a, b) stands for the Binomial random variable with a trials and the success probability is b. By leveraging the Lemma A.1, we know that for a random variable X ∼ Bin( n 2 , 2q(1 − q)), we have</p><p>Note that the function q(1 − q) is monotonic increasing for q ∈ [0, 1 2 ] and has maximum at q = 1 2 . Combine with Assumption 4.2 we know that nq(1 − q) = ω( √ n log n). Hence, by choosing t = (32)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enabling efficiency-precision trade-offs for label trees in extreme classification</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">L</forename><surname>Tavor Z Baharav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kedarnath</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Kolluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors with triplets and shallow convolutional neural networks</title>
		<author>
			<persName><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph convolution for semisupervised classification: Improved linear separability and out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Aseem</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimon</forename><surname>Fountoulakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aukosh</forename><surname>Jagannath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pre-training tasks for embedding-based large-scale retrieval</title>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choon-Hui</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kedarnath</forename><surname>Kolluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Shandilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vyacheslav</forename><surname>Ievgrafov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Japinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2020b. 2021. 2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
	<note>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</title>
		<author>
			<persName><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contextual stochastic block models</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elchanan</forename><surname>Mossel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Accelerating inference for sparse extreme multi-label ranking trees</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Philip A Etter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02697</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking selfsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6391" to="6400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Birank: Towards ranking on bipartite graphs</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName><forename type="first">Wassily</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The collected works of Wassily Hoeffding</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="409" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJlWWJSFDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020c</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Text level graph neural network for text classification</title>
		<author>
			<persName><forename type="first">Lianzhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Benson</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=8E1-f3VhX1o" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">LightXML: Transformer with dynamic negative sampling for high-performance extreme multilabel text classification</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leilei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1920" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">FLAG: Adversarial data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mucong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Laplacian mid networks for fast and accurate super-resolution</title>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing generalized pagerank methods for seedexpansion community detection</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11710" to="11721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01404</idno>
		<title level="m">New benchmarks for learning on nonhomophilous graphs</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-Supervised generalisation with meta auxiliary learning</title>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/92262bf907af914b95a0fc33c3f33bf6-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tensor graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xien</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxin</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8409" to="8416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Label disentanglement in partition-based extreme multilabel classification</title>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12751</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Link prediction in complex networks: A survey</title>
		<author>
			<persName><forename type="first">Linyuan</forename><surname>Lü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: statistical mechanics and its applications</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A coarse-to-fine approach for fast deformable object detection</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Roca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1844" to="1853" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">FastXML: A fast, accurate and stable tree-classifier for extreme multi-label learning</title>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rahul Agrawal, and Manik Varma. Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising</title>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrutendra</forename><surname>Harsola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="993" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Top-k extreme contextual bandits with arm hierarchy</title>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Kidambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/sen21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="9422" to="9433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Extreme multi-label classification from aggregated labels</title>
		<author>
			<persName><forename type="first">Yanyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8752" to="8762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">77</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Scalable and adaptive graph neural networks with self-labelenhanced training</title>
		<author>
			<persName><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoshi</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09376</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Self-supervised learning of graph neural networks: A unified review</title>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10757</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Session-aware query auto-completion using extreme multi-label ranking</title>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arya</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Mazumdar</surname></persName>
		</author>
		<author>
			<persName><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 27th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
				<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pretrained generalized autoregressive model with adaptive probabilistic label clusters for extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10809" to="10819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">GraphRNN: Generating realistic graphs with deep auto-regressive models</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5708" to="5717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">At-tentionXML: Label tree-based attention-aware deep model for high-performance extreme multilabel text classification</title>
		<author>
			<persName><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5820" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">PECOS: Prediction for enormous and correlated output spaces</title>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05878</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph-SAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJe8pkHFwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Text graph transformer for document classification</title>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Graph-Bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fast multi-resolution transformer fine-tuning for extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Graph attention multi-layer perceptron</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeang</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10097</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
