<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured output regression for detection with partial truncation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<email>vedaldi@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structured output regression for detection with partial truncation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F52D5018D1B841E0F1475DA6E3E6DEF9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a structured output model for object category detection that explicitly accounts for alignment, multiple aspects and partial truncation in both training and inference. The model is formulated as large margin learning with latent variables and slack rescaling, and both training and inference are computationally efficient. We make the following contributions: (i) we note that extending the Structured Output Regression formulation of Blaschko and Lampert [1] to include a bias term significantly improves performance; (ii) that alignment (to account for small rotations and anisotropic scalings) can be included as a latent variable and efficiently determined and implemented; (iii) that the latent variable extends to multiple aspects (e.g. left facing, right facing, front) with the same formulation; and (iv), most significantly for performance, that truncated and truncated instances can be included in both training and inference with an explicit truncation mask. We demonstrate the method by training and testing on the PASCAL VOC 2007 data set -training includes the truncated examples, and in testing object instances are detected at multiple scales, alignments, and with significant truncations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been a steady increase in the performance of object category detection as measured by the annual PASCAL VOC challenges <ref type="bibr" target="#b2">[3]</ref>. The training data provided for these challenges specifies if an object is truncated -when the provided axis aligned bounding box does not cover the full extent of the object. The principal cause of truncation is that the object partially lies outside the image area. Most participants simple disregard the truncated training instances and learn from the non-truncated ones. This is a waste of training material, but more seriously many truncated instances are missed in testing, significantly reducing the recall and hence decreasing overall recognition performance.</p><p>In this paper we develop a model (Fig. <ref type="figure" target="#fig_0">1</ref>) which explicitly accounts for truncation in both training and testing, and demonstrate that this leads to a significant performance boost. The model is specified as a joint kernel and learnt using an extension of the structural SVM with latent variables framework of <ref type="bibr" target="#b12">[13]</ref>. We use this approach as it allows us to address a second deficiency of the provided supervision -that the annotation is limited to axis aligned bounding boxes, even though the objects may be in plane rotated so that the box is a loose bound. The latent variables allow us to specify a pose transformation for each instances so that we achieve a spatial correspondence between all instances with the same aspect. We show the benefits of this for both the learnt model and testing performance.</p><p>Our model is complementary to that of Felzenszwalb et al. <ref type="bibr" target="#b3">[4]</ref> who propose a latent SVM framework, where the latent variables specify sub-part locations. The parts give their model some tolerance to in plane rotation and foreshortening (though an axis aligned rectangle is still used for a first stage as a "root filter") but they do not address the problem of truncation. Like them we base our implementation on the efficient and successful HOG descriptor of Dalal and Triggs <ref type="bibr" target="#b1">[2]</ref>.</p><p>Previous authors have also considered occlusion (of which truncation is a special case). Williams et al. <ref type="bibr" target="#b10">[11]</ref> used pixel wise binary latent variables to specify the occlusion and an Ising prior for spatial coherence. Inference involved marginalizing out the latent variables using a mean field approximation. There was no learning of the model from occluded data. For faces with partial occlusion, the resulting classifier showed an improvement over a classifier which did not model occlusion. Others have explicitly included occlusion at the model learning stage, such as the Constellation model of Fergus et al. <ref type="bibr" target="#b4">[5]</ref> and the Layout Consistent Random Field model of Winn et al. <ref type="bibr" target="#b11">[12]</ref>. There are numerous papers on detecting faces with various degrees of partial occlusion from glasses, or synthetic truncations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Our contribution is to define an appropriate joint kernel and loss function to be used in the context of structured output prediction. We then learn a structured regressor, mapping an image to a list of objects with their pose (or bounding box), while at the same time handling explicitly truncation and multiple aspects. Our choice of kernel is inspired by the restriction kernel of <ref type="bibr" target="#b0">[1]</ref>; however, our kernel performs both restriction and alignment to a template, supports multiple templates to handle different aspects and truncations, and adds a bias term which significantly improves performance.</p><p>We refine pose beyond translation and scaling with an additional transformation selected from a finite set of possible perturbations covering aspect ratio change and small in plane rotations. Instead of explicitly transforming the image with each element of this set (which would be prohibitively expensive) we introduce a novel approximation based on decomposing the HOG descriptor into small blocks and quickly rearranging those. To handle occlusions we selectively switch between an object descriptor and an occlusion descriptor. To identify which portions of the template are occluded we use a field of binary variables. These could be treated as latent variables; however, since we consider here only occlusions caused by the image boundaries, we can infer them deterministically from the position of the object relative to the image boundaries. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates various detection examples including truncation, multiple instances and aspects, and in-plane rotation.</p><p>In training we improve the ground-truth pose parameters, since the bounding boxes and aspect associations provided in PASCAL VOC are quite coarse indicators of the object pose. For each instance we add a latent variable which encodes a pose adjustment. Such variables are then treated as part of learning using the technique presented in <ref type="bibr" target="#b12">[13]</ref>. However, while there the authors use the CCCP algorithm to treat the case of margin rescaling, here we show that a similar algorithm applies to the case of slack rescaling. The resulting optimization alternates between optimizing the model parameters given the latent variables (a convex problem solved by a cutting plane algorithm) and optimizing the latent variable given the model (akin to inference).</p><p>The overall method is computationally efficient both in training and testing, achieves very good performances, and it is able to learn and recognise truncated objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our purpose is to learn a function y = f (x), x ∈ X , y ∈ Y which, given an image x, returns the poses y of the objects portrayed in the image. We use the structured prediction learning framework of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>, which considers along with the input and output variables x and y, an auxiliary latent variable h ∈ H as well (we use h to specify a refinement to the ground-truth pose parameters). The function f is then defined as f (x; w) = ŷx (w) where</p><formula xml:id="formula_0">(ŷ x (w), ĥx (w)) = argmax (y,h)∈Y×H F (x, y, h; w), F (x, y, h; w) = w, Ψ(x, y, h) ,<label>(1)</label></formula><p>and Ψ(x, y, h) ∈ R d is a joint feature map. This maximization estimates both y and h from the data x and corresponds to performing inference. Given training data (x 1 , y 1 ), . . . , (x N , y N ), the parameters w are learned by minimizing the regularized empirical risk</p><formula xml:id="formula_1">R(w) = 1 2 w 2 + C N N i=1</formula><p>∆(y i , ŷi (w), ĥi (w)), where ŷi (w) = ŷxi (w), ĥi (w) = ĥxi (w).</p><p>(2) Here ∆(y i , y, h) ≥ 0 is an appropriate loss function that encodes the cost of an incorrect prediction.</p><p>In this section we develop the model Ψ(x, y, h), or equivalently the joint kernel function K(x, y, h, x , y , h ) = Ψ(x, y, h), Ψ(x , y , h ) , in a number of stages. We first define the kernel for the case of a single unoccluded instance in an image including scale and perturbing transformations, then generalise this to include truncations and aspects; and finally to multiple instances. An appropriate loss function ∆(y i , y, h) is subsequently defined which takes into account the pose of the object specified by the latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Restriction and alignment kernel</head><p>Denote by R a rectangular region of the image x, and by x| R the image cropped to that rectangle. A restriction kernel <ref type="bibr" target="#b0">[1]</ref> is the kernel</p><formula xml:id="formula_2">K((x, R), (x , R )) = K image (x| R , x | R )</formula><p>where K image is an appropriate kernel between images. The goal is that the joint kernel should be large when the two regions have similar appearance.</p><p>Our kernel is similar, but captures both the idea of restriction and alignment. Let R 0 be a reference rectangle and denote by R(p) = g p R 0 the rectangle obtained from R 0 by a geometric transformation g p : R 2 → R 2 . We call p the pose of the rectangle R(p). Let x be an image defined on the reference rectangle R 0 and let H(x) ∈ R d be a descriptor (e.g. SIFT, HOG, GIST <ref type="bibr" target="#b1">[2]</ref>) computed from the image appearance. Then a natural definition of the restriction and alignment kernel is</p><formula xml:id="formula_3">K((x, p), (x , p )) = K descr (H(x; p), H(x ; p ))<label>(3)</label></formula><p>where K descr is an appropriate kernel for descriptors, and H(x; p) is the descriptor computed on the transformed patch x as H(x; p) = H(g -1 p x). Implementation with HOG descriptors. Our choice of the HOG descriptor puts some limits on the space of poses p that can be efficiently explored. To see this, it is necessary to describe how HOG descriptors are computed.</p><p>The computation starts by decomposing the image x into cells of d × d pixels (here d = 8). The descriptor of a cell is the nine dimensional histogram of the orientation of the image gradient inside the cell (appropriately weighed and normalized as in <ref type="bibr" target="#b1">[2]</ref>). We obtain the HOG descriptor of a rectangle of w × h cells by stacking the enclosed cell descriptors (this is a 9 × w × h vector). Thus, given the cell histograms, we can immediately obtain the HOG descriptors H(x, y) for all the cellaligned translations (x, y) of the dw × dh pixels rectangle. To span rectangles of different scales this construction is simply repeated on the rescaled image g -1 s x, where g s (z) = γ s z is a rescaling, γ &gt; 0, and s is a discrete scale parameter.</p><p>To further refine pose beyond scale and translation, here we consider an additional perturbation g t , indexed by a pose parameter t, selected in a set of transformations g 1 , . . . , g T (in the experiments we use 16 transformations, obtained from all combinations of rotations of ±5 and ±10 degrees and scaling along x of 95%, 90%, 80% and 70%). Those could be achieved in the same manner as scaling by transforming the image g -1 t x for each one, but this would be very expensive (we would need to recompute the cell descriptors every time). Instead, we approximate such transformations by rearranging the cells of the template (Fig. <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_1">2</ref>). First, we partition the w × h cells of the template into blocks of m × m cells (e.g. m = 4). Then we transform the center of each block according to g t and we translate the block to the new center (approximated to units of cells). Notice that we could pick m = 1 (i.e. move each cell of the template independently), but we prefer to use blocks as this accelerates inference (see Sect. 4).</p><p>Hence, pose is for us a tuple (x, y, s, t) representing translation, scale, and additional perturbation. Since HOG descriptors are designed to be compared with a linear kernel, we define</p><formula xml:id="formula_4">K((x, p), (x , p )) = Ψ(x, p), Ψ(x , p ) , Ψ(x, p) = H(x; p).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modeling truncations</head><p>If part of the object is occluded (either by clutter or by the image boundaries), some of the descriptor cells will be either unpredictable or undefined. We explicitly deal with occlusion at the granularity of the HOG cells by adding a field of w × h binary indicator variables v ∈ {0, 1} wh . Here v j = 1 means that the j-th cell of the HOG descriptor H(x, p) should be considered to be visible, and v j = 0 that it is occluded. We thus define a variant of (4) by considering the feature map</p><formula xml:id="formula_5">Ψ(x, p, v) = (v ⊗ 1 9 ) H(x, p) ((1 wh -v) ⊗ 1 9 ) H(x, p)<label>(5)</label></formula><p>where 1 d is a d-dimensional vector of all ones, ⊗ denotes the Kroneker product, and the Hadamard (component wise) product. To understand this expression, recall that H is the stacking of w × h 9dimensional histograms, so for instance (v ⊗ 1 9 ) H(x, p) preserves the visible cells and nulls the others. Eq. ( <ref type="formula" target="#formula_5">5</ref>) is effectively defining a template for the object and one for the occlusions.</p><p>Notice that v are in general latent variables and should be estimated as such. However here we note that the most severe and frequent occlusions are caused by the image boundaries (finite field of view). In this case, which we explore in the experiments, we can write v = v(p) as a function of the pose p, and remove the explicit dependence on v in Ψ. Moreover the truncated HOG cells are undefined and can be assigned a nominal common value. So here we work with a simplified kernel, in which occlusions are represented by a scalar proportional to the number of truncated cells:</p><formula xml:id="formula_6">Ψ(x, p) = (v(p) ⊗ 1 9 ) H(x, p) wh -|v(p)|<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Modeling aspects</head><p>A template model works well as long as pose captures accurately enough the transformations resulting from changes in the viewing conditions. In our model, the pose p, combined with the robustness of the HOG descriptor, can absorb a fair amount of viewpoint induced deformation. It cannot, however, capture the 3D structure of a physical object. Therefore, extreme changes of viewpoint require switching between different templates. To this end, we augment pose with an aspect indicator a (so that pose is the tuple p = (x, y, s, t, a)), which indicates which template to use.</p><p>Note that now the concept of pose has been generalized to include a parameter, a, which, differently from the others, does not specify a geometric transformation. Nevertheless, pose specifies how the model should be aligned to the image, i.e. by (i) choosing the template that corresponds to the aspect a, (ii) translating and scaling such template according to (x, y, s), and (iii) applying to it the additional perturbation g t . In testing, all such parameters are estimated as part of inference.</p><p>In training, they are initialized from the ground truth data annotations (bounding boxes and aspect labels), and are then refined by estimating the latent variables (Sect. 2.4).</p><p>We assign each aspect to a different "slot" of the feature vector Ψ(x, p). Then we null all but the one of the slots, as indicated by a:</p><formula xml:id="formula_7">Ψ(x, p) =    δ a=1 Ψ 1 (x; p) . . . δ a=A Ψ A (x; p)   <label>(7)</label></formula><p>where Ψ a (x; p) is a feature vector in the form of <ref type="bibr" target="#b5">(6)</ref>. In this way, we compare different templates for different aspects, as indicated by a.</p><p>The model can be extended to capture symmetries of the aspects (resulting from symmetries of the objects). For instance, a left view of a bicycle can be obtained by mirroring a right view, so that the same template can be used for both aspects by defining</p><formula xml:id="formula_8">Ψ(x; p) = δ a=left Ψ left (x; p) + δ a=right flip Ψ right (x; p),<label>(8)</label></formula><p>where flip is the operator that "flips" the descriptor (this can be defined in general by computing the descriptor of the mirrored image, but for HOG it reduces to rearranging the descriptor components).</p><p>The problem remains of assigning aspects to the training data. In the Pascal VOC data, objects are labeled with one of five aspects: front, left, right, back, undefined. However, such assignments may not be optimal for use in a particular algorithm. Fortunately, our method is able to automatically reassign aspects as part of the estimation of the hidden variables (Sect. 2.4 and Fig. <ref type="figure" target="#fig_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Latent variables</head><p>The PASCAL VOC bounding boxes yield only a coarse estimate of the ground truth pose parameters (e.g. they do not contain any information on the object rotation) and the aspect assignments may also be suboptimal (see previous section). Therefore, we introduce latent variables h = (δp) that encode an adjustment to the ground-truth pose parameters y = (p). In practice, the adjustment δp is a small variation of translation x, y, scale s, and perturbation t, and can switch the aspect a all together.</p><p>We modify the feature maps to account for the adjustment in the obvious way. For instance <ref type="bibr" target="#b5">(6)</ref> becomes</p><formula xml:id="formula_9">Ψ(x, p, δp) = (v(p + δp) ⊗ 1 9 ) H(x, p + δp) wh -|v(p + δp)|<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Variable number of objects: loss function, bias, training</head><p>So far, we have defined the feature map Ψ(x, y) = Ψ(x; p) for the case in which the label y = (p) contains exactly one object, but an image may contain no or multiple object instances (denoted respectively y = and y = (p 1 , . . . , p n )). We define the loss function between a ground truth label y i and the estimated output y as</p><formula xml:id="formula_10">∆(y i , y) =    0 if y i = y = , 1 -overl(B(p), B(p )) if y i = (p) and y = (p ), 1</formula><p>if y i = and y = , or y i = and y = , <ref type="bibr" target="#b9">(10)</ref> where B is the ground truth bounding box, and B is the prediction (the smallest axis aligned bounding box that contains the warped template g p R 0 ). The overlap score between B and B is given by overl</p><formula xml:id="formula_11">(B, B ) = |B ∩ B |/|B ∪ B |.</formula><p>Note that the ground truth poses are defined so that B(p l ) matches the PASCAL provided bounding boxes <ref type="bibr" target="#b0">[1]</ref> (or the manually extended ones for the truncated ones).</p><p>The hypothesis y = (no object) receives score F (x, ; w) = 0 by defining Ψ(x, ) = 0 as in <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this way, the hypothesis y = (p) is preferred to y = only if F (x, p; w) &gt; F (x, ; w) = 0, which implicitly sets the detection threshold to zero. However, there is no reason to assume that this threshold should be appropriate (in Fig. <ref type="figure" target="#fig_1">2</ref> we show that it is not). To learn an arbitrary threshold, it suffices to append to the feature vector Ψ(x, p) a large constant κ bias , so that the score of the hypothesis y = (p) becomes F (x, (p); w) = w, Ψ(x, p) + κ bias w bias . Note that, since the constant is large, the weight w bias remains small and has negligible effect on the SVM regularization term.</p><p>Finally, an image may contain more than one instance of the object. The model can be extended to this case by setting F (x, y; w) = L l=1 F (x, p l ; w) + R(y), where R(y) encodes a "repulsive" force that prevents multiple overlapping detections of the same object. Performing inference with such a model becomes however combinatorial and in general very difficult. Fortunately, in training the problem can be avoided entirely. If an image contains multiple instances, the image is added to the training set multiple times, each time "activating" one of the instances, and "deactivating" the others. Here "deactivating" an instance simply means removing it from the detector search space. Formally, let p 0 be the pose of the active instance and p 1 , . . . , p N the poses of the inactive ones. A pose p is removed from the search space if, and only if, max i overl(B(p), B(p i )) ≥ max{overl(B(p), B(p 0 )), 0.2}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Optimisation</head><p>Minimising the regularised risk R(w) as defined by Eq. ( <ref type="formula">2</ref>) is difficult as the loss depends on w through ŷi (w) and ĥi (w) (see Eq. ( <ref type="formula" target="#formula_0">1</ref>)). It is however possible to optimise an upper bound (derived below) given by</p><formula xml:id="formula_12">1 2 w 2 + C N N i=1 max (y,h)∈Y×H ∆(y i , y, h) [1 + w, Ψ(x i , y, h) -w, Ψ(x i , y i , h * i (w)) ] .<label>(11)</label></formula><p>Here h * i (w) = argmax h∈H w, Ψ(x i , y i , h) completes the label (y i , h * i (w)) of the sample x i (of which only the observed part y i is known from the ground truth).</p><p>Alternation optimization. Eq. ( <ref type="formula" target="#formula_12">11</ref>) is not a convex energy function due to the dependency of h * i (w) on w. Similarly to <ref type="bibr" target="#b12">[13]</ref>, however, it is possible to find a local minimum by alternating optimizing w and estimating h * i . To do this, the CCCP algorithm proposed in <ref type="bibr" target="#b12">[13]</ref> for the case of margin rescaling, must be extended to the slack rescaling formulation used here.</p><p>Starting from an estimate w t of the solution, define h * it = h i (w t ), so that, for any w, w, Ψ(x i , y i ,</p><formula xml:id="formula_13">h * i (w)) = max h w, Ψ(x i , y i , h ) ≥ w, Ψ(x i , y i , h * it )</formula><p>and the equality holds for w = w t . Hence the energy ( <ref type="formula" target="#formula_12">11</ref>) is bounded by</p><formula xml:id="formula_14">1 2 w 2 + C N N i=1 max (y,h)∈Y×H ∆(y i , y, h) [1 + w, Ψ(x i , y, h) -w, Ψ(x i , y i , h * it ) ]<label>(12)</label></formula><p>and the bound is strict for w = w t . Optimising <ref type="bibr" target="#b11">(12)</ref> will therefore result in an improvement of the energy <ref type="bibr" target="#b10">(11)</ref> as well. The latter can be carried out with the cutting-plane technique of <ref type="bibr" target="#b8">[9]</ref>.</p><p>Derivation of the bound <ref type="bibr" target="#b10">(11)</ref>. The derivation involves a sequence of bounds, starting from</p><formula xml:id="formula_15">∆(yi, ŷi(w), ĥi(w)) ≤ ∆(yi, ŷi(w), ĥi(w)) h 1 + w, Ψ(xi, ŷi(w), ĥi(w)) -w, Ψ(xi, yi, h * i (w)) i<label>(13)</label></formula><p>This bound holds because, by construction, the quantity in the square brackets is not smaller than one, as can be verified by substituting the definitions of ŷi (w), ĥi (w) and h * i (w). We further upper bound the loss by</p><formula xml:id="formula_16">∆(yi, ŷi(w), ĥi(w)) ≤ ∆(yi, y, h) [1 + w, Ψ(xi, y, h) -w, Ψ(xi, yi, h * i (w)) ] ˛y=ŷ i (w),h= ĥi (w) ≤ max (y,h)∈Y×H ∆(yi, y, h) [1 + w, Ψ(xi, y, h) -w, Ψ(xi, yi, h * i (w)) ]<label>(14)</label></formula><p>Substituting this bound into (2) yields <ref type="bibr" target="#b10">(11)</ref>. Note that ŷi (w) and ĥi (w) are defined as the maximiser of w, Ψ(x i , y, h) alone (see Eq. 1), while the energy maximised in (14) depends on the loss ∆(y i , y, h) as well. The algorithm improves their estimate as part of inference of the latent variables h. Notice that not only translation, scale, and small jitters are re-estimated, but also the aspect subclass can be updated. In the example, an instance originally labeled as misc (a) is reassigned to the left aspect (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Data. As training data we use the PASCAL VOC annotations. Each object instance is labeled with a bounding box and a categorical aspect variable (left, right, front, back, undefined). From the bounding box we estimate translation and scale of the object, and we use aspect to select one of multiple HOG templates. Symmetric aspects (e.g. left and right) are mapped to the same HOG template as suggested in Sect. 2.3.</p><p>While our model is capable of handling correctly truncations, truncated bounding boxes provide a poor estimate of the pose of the object pose which prevents using such objects for training. While we could simply avoid training with truncated boxes (or generate artificially truncated examples whose pose would be known), we prefer exploiting all the available training data. To do this, we manually augment all truncated PASCAL VOC annotations with an additional "physical" bounding box. The purpose is to provide a better initial guess for the object pose, which is then refined by optimizing over the latent variables.</p><p>Training and testing speed. Performing inference with the model requires evaluating w, Ψ(x, p) for all possible poses p. This means matching a HOG template O(W HT A) times, where W × H is the dimension of the image in cells, T the number of perturbations (Sect. 2.1), and A the number of aspects (Sect. 2.3). <ref type="foot" target="#foot_0">1</ref> For a given scale and aspect, matching the template for all locations reduces to convolution. Moreover, by breaking the template into blocks (Fig. <ref type="figure" target="#fig_1">2</ref>) and pre-computing the convolution with each of those, we can quickly compute perturbations of the template. All in all, detection requires roughly 30 seconds per image with the full model and four aspects. The cutting plane algorithm used to minimize <ref type="bibr" target="#b11">(12)</ref> requires at each iteration solving problems similar to inference. This can be easily parallelized, greatly improving training speed. To detect additional objects at test time we run inference multiple times, but excluding all detections that overlap by more than 20% with any previously detected object. Benefit of various model components. Fig. <ref type="figure" target="#fig_1">2</ref> shows how the model improves by the successive introduction of the various features of the model. The example is carried on the VOC left-right facing bicycle, but similar effects were observed for other categories. The baseline model uses only the HOG template without bias, truncations, nor pose refinement (Sect. 2.1). The two most significant improvements are (a) the ability of detecting truncated instances (+22% AP, Fig. <ref type="figure" target="#fig_2">3</ref>) and (b) the addition of the bias (+11% AP). Training with the truncated instances, adding the number of occluded HOG cells as a feature component, and adding jitters beyond translation and scaling all yield an improvement of about +2-3% AP.</p><p>Full model. The model was trained to detect the class bicycle in the PASCAL VOC 2007 data, using five templates, initialized from the PASCAL labeling left, right, front/rear, other. Initially, the pose refinimenent h is null and the alternation optimization algorithm is iterated five times to estimate the model w and refinement h. The detector is then tested on all the test data, enabling multiple detections per image, and computing average-precision as specified by <ref type="bibr" target="#b2">[3]</ref>. The AP score was 47%. By comparison, the state of the art for this category <ref type="bibr" target="#b7">[8]</ref> achieves 56%. The experiment was repeated for the class horse, obtaining a score of 40%. By comparison, the state of the art on this category, our MKL sliding window classifier <ref type="bibr" target="#b9">[10]</ref>, achieves 51%. Note that the proposed method uses only HOG, while the others use a combination of at least two features. However <ref type="bibr" target="#b3">[4]</ref>, using only HOG but a flexible part model, also achieves superior results. Further experiments are needed to evaluate the combined benefits of truncation/occlusion handling (proposed here), with multiple features <ref type="bibr" target="#b9">[10]</ref> and flexible parts <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We have shown how structured output regression with latent variables provides an integrated and effective solution to many problems in object detection: truncations, pose variability, multiple objects, and multiple aspects can all be dealt in a consistent framework.</p><p>While we have shown that truncated examples can be used for training, we had to manually extend the PASCAL VOC annotations for these cases to include rough "physical" bounding boxes (as a hint for the initial pose parameters). We plan to further extend the approach to infer pose for truncated examples in a fully automatic fashion (weak supervision).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>RIGHTFigure 1 :</head><label>1</label><figDesc>Figure 1: Model overview. Detection examples on the VOC images for the bicycle class demonstrate that the model can handle severe truncations (a-b), multiple objects (c), multiple aspects (d), and pose variations (small in-plane rotations) (e). Truncations caused by the image boundary, (a) &amp; (b), are a significant problem for template based detectors, since the template can then only partially align with the image. Small in-plane rotations and anisotropic rescalings of the template are approximated efficiently by rearranging sub-blocks of the HOG template (white boxes in (e)).</figDesc><graphic coords="2,396.00,35.92,108.06,163.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effect of different model components. The left panel evaluates the effect of different components of the model on the task of learning a detector for the left-right facing PASCAL VOC 2007 bicycles. In order of increasing AP (see legend): baseline model (see text); bias term (Sect. 2.5); detecting trunctated instances, training on truncated instances, and counting the truncated cells as a feature (Sect.: 2.2); with searching over small translation, scaling, rotation, skew (Sect. 2.1). Right panel: (a) Original VOC specified bounding box and aspect; (b) alignment and aspect after pose inference -in addition to translation and scale, our templates are searched over a set of small perturbations. This is implemented efficiently by breaking the template into blocks (dashed boxes) and rearranging those. Note that blocks can partially overlap to capture foreshortening. The ground truth pose parameters are approximate because they are obtained from bounding boxes (a).The algorithm improves their estimate as part of inference of the latent variables h. Notice that not only translation, scale, and small jitters are re-estimated, but also the aspect subclass can be updated. In the example, an instance originally labeled as misc (a) is reassigned to the left aspect (b).</figDesc><graphic coords="7,281.46,83.57,103.28,137.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top row. Examples of detected bicycles. The dashed boxes are bicycles that were detected with or without truncation support, while the solid ones were detectable only when truncations were considered explicitly. Bottom row. Some cases of correct detections despite extreme truncation for the horse class.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that we do not multiply by the number S of scales as at each successive scale W and H are reduced geometrically.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We are grateful for discussions with Matthew Blaschko. Funding was provided by the EU under ERC grant VisRec no. 228180; the RAEng, Microsoft, and ONR MURI N00014-07-1-0182.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to localize objects with structured output regression</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2008/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grishick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scaleinvariant learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust face detection under partial occlusion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast object detection with occlusions</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Fuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="page" from="402" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative structure learning of hierarchical representations for object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schnitzspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple kernels for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The variational ising classifier (VIC) algorithm for coherently contaminated data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Layout Consistent Random Field for Recognizing and Segmenting Partially Occluded Objects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning structural SVMs with latent variables</title>
		<author>
			<persName><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
