<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable and distributed methods for entity matching, consolidation and disambiguation over linked data corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-11-18">18 November 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
							<email>aidan.hogan@deri.org</email>
							<affiliation key="aff0">
								<orgName type="department">Digital Enterprise Research Institute</orgName>
								<orgName type="institution">National University of Ireland</orgName>
								<address>
									<settlement>Galway</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Zimmermann</surname></persName>
							<email>antoine.zimmermann@insa-lyon.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR5205</orgName>
								<orgName type="institution">INSA-Lyon, LIRIS</orgName>
								<address>
									<postCode>F-69621</postCode>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jürgen</forename><surname>Umbrich</surname></persName>
							<email>juergen.umbrich@deri.org</email>
							<affiliation key="aff0">
								<orgName type="department">Digital Enterprise Research Institute</orgName>
								<orgName type="institution">National University of Ireland</orgName>
								<address>
									<settlement>Galway</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Axel</forename><surname>Polleres</surname></persName>
							<email>axel.polleres@siemens.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Siemens AG Österreich</orgName>
								<address>
									<addrLine>Siemensstrasse 90</addrLine>
									<postCode>1210</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
							<email>stefan.decker@deri.org</email>
							<affiliation key="aff0">
								<orgName type="department">Digital Enterprise Research Institute</orgName>
								<orgName type="institution">National University of Ireland</orgName>
								<address>
									<settlement>Galway</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable and distributed methods for entity matching, consolidation and disambiguation over linked data corpora</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-11-18">18 November 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">AAF21FA8EED408CA562552DBC7BDD19E</idno>
					<idno type="DOI">10.1016/j.websem.2011.11.002</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Entity consolidation Web data Linked Data RDF</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With respect to large-scale, static, Linked Data corpora, in this paper we discuss scalable and distributed methods for entity consolidation (aka. smushing, entity resolution, object consolidation, etc.) to locate and process names that signify the same entity. We investigate (i) a baseline approach, which uses explicit owl: sameAs relations to perform consolidation; (ii) extended entity consolidation which additionally uses a subset of OWL 2 RL/RDF rules to derive novel owl:sameAs relations through the semantics of inverse-functional properties, functional-properties and (max-)cardinality restrictions with value one; (iii) deriving weighted concurrence measures between entities in the corpus based on shared inlinks/outlinks and attribute values using statistical analyses; (iv) disambiguating (initially) consolidated entities based on inconsistency detection using OWL 2 RL/RDF rules. Our methods are based upon distributed sorts and scans of the corpus, where we deliberately avoid the requirement for indexing all data. Throughout, we offer evaluation over a diverse Linked Data corpus consisting of 1.118 billion quadruples derived from a domain-agnostic, open crawl of 3.985 million RDF/XML Web documents, demonstrating the feasibility of our methods at that scale, and giving insights into the quality of the results for real-world data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over a decade since the dawn of the Semantic Web, RDF publishing has begun to find some traction through adoption of Linked Data best practices as follows:</p><p>(i) use URIs as names for things (and not just documents); (ii) make those URIs dereferenceable via HTTP; (iii) return useful and relevant RDF content upon lookup of those URIs; (iv) include links to other datasets.</p><p>The Linked Open Data project has advocated the goal of providing dereferenceable machine readable data in a common format (RDF), with emphasis on the re-use of URIs and interlinkage between remote datasets-in so doing, the project has overseen exports from corporate entities (e.g., the BBC, BestBuy, Freebase), governmental bodies (e.g., the UK Government, the US government), existing structured datasets (e.g., DBPedia), social networking sites (e.g., flickr, Twitter, livejournal), academic communities (e.g., DBLP, UniProt), as well as esoteric exports (e.g., Linked Open Numbers, Poképédia). This burgeoning web of structured data has succinctly been dubbed the ''Web of Data''.</p><p>Considering the merge of these structured exports, at a conservative estimate there now exists somewhere in the order of thirty billion RDF triples published on the Web as Linked Data. 1 However, in this respect, size is not everything <ref type="bibr" target="#b72">[73]</ref>. In particular, although the situation is improving, individual datasets are still not wellinterlinked (cf. <ref type="bibr" target="#b71">[72]</ref>)-without sufficient linkage, the ideal of a ''Web of Data'' quickly disintegrates into the current reality of ''Archipelagos of Datasets''.</p><p>There have been numerous works that have looked at bridging the archipelagos. Some works aim at aligning a small number of related datasets (e.g., <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b48">49]</ref>), thus focusing more on theoretical considerations than scalability, usually combining symbolic (e.g., reasoning with consistency checking) methods and similarity measures. Some authors have looked at inter-linkage of domain specific RDF datasets at various degrees of scale (e.g., <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b45">46]</ref>). Further research has also looked at exploiting shared terminological data-as well as explicitly asserted links-to better integrate Linked Data collected from thousands or millions of sources (e.g., <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>); the work presented herein falls most closely into this category. One approach has tackled the problem from the publishers side, detailing a system for manually specifying some (possibly heuristic) criteria for creating links between two datasets <ref type="bibr" target="#b71">[72]</ref>. We leave further detailed related work to Section 9.</p><p>In this paper, we look at methods to provide better linkage between resources, in particular focusing on finding equivalent entities in the data. Our notion of an entity is a representation of something being described by the data; e.g., a person, a place, a musician, a protein, etc. We say that two entities are equivalent if they are coreferent; e.g., refer to the same person, place, etc. 2 Given a collection of datasets that speak about the same referents using different identifiers, we wish to identify these coreferences and somehow merge the knowledge contribution provided by the distinct parties. We call this merge consolidation.</p><p>In particular, our work is inspired by the requirements of the Semantic Web Search Engine project <ref type="bibr" target="#b31">[32]</ref>, within which we aim to offer search and browsing over large, static, Linked Data corpora crawled from the Web. 3 The core operation of SWSE is to take user keyword queries as input, and to generate a ranked list of matching entities as results. After the core components of a crawler, index and user-interface, we saw a clear need for a component that consolidates-by means of identifying and canonicalising equivalent identifiers-the indexed corpus: there was an observable lack of URIs such that coreferent blank-nodes were prevalent <ref type="bibr" target="#b29">[30]</ref> even within the same dataset, and thus we observed many duplicate results referring to the same thing, leading to poor integration of data from our source documents.</p><p>To take a brief example, consider a simple example query: ''WHO DOES TIM BERNERS-LEE KNOW?''. Knowing that Tim uses the URI timblfoaf:i to refer to himself in his personal FOAF profile document, and again knowing that the property foaf:knows relates people to their (reciprocated) acquaintances, we can formulate this request as the SPARQL query <ref type="bibr" target="#b52">[53]</ref> as follows:</p><p>SELECT ?person WHERE { timblfoaf:i foaf:knows ?person. } However, other publishers use different URIs to identify Tim, where to get more complete answers across these naming schemes, the SPARQL query must use disjunctive UNION clauses for each known URI; here we give an example using a sample of identifiers extracted from a real Linked Data corpus <ref type="bibr">(</ref> We see disparate URIs not only across data publishers, but also within the same namespace. Clearly, the expanded query quickly becomes extremely cumbersome.</p><p>In this paper, we look at bespoke methods for identifying and processing coreference in a manner such that the resultant corpus can be consumed as if more complete agreement on URIs was present; in other words, using standard query-answering techniques, we want the enhanced corpus to return the same answers for the original simple query as for the latter expanded query.</p><p>Our core requirements for the consolidation component are as follows:</p><p>-the component must give high precision of consolidated results; -the underlying algorithm(s) must be scalable; -the approach must be fully automatic; -the methods must be domain agnostic;</p><p>where a component with poor precision will lead to garbled final results merging unrelated entities, where scalability is required to apply the process over our corpora typically in the order of a billion statements (and which we feasibly hope to expand in future), where the scale of the corpora under analysis precludes any manual intervention, and where-for the purposes of research-the methods should not give preferential treatment to any domain or vocabulary of data (other than core RDF(S)/OWL terms). Alongside these primary requirements, we also identify the following secondary criteria:</p><p>-the analysis should demonstrate high recall; -the underlying algorithm(s) should be efficient;</p><p>where the consolidation component should identify as many (correct) equivalences as possible, and where the algorithm should be applicable in reasonable time. Clearly the secondary requirements are also important, but they are superceded by those given earlier, where a certain trade-off exists: we prefer a system that 2 Herein, we avoid philosophical discussion on the notion of identity; for interesting discussion thereon, see <ref type="bibr" target="#b25">[26]</ref>. 3 By static, we mean that the system does not cater for updates; this omission allows for optimisations throughout the system. Instead, we aim at a cyclical indexing paradigm, where new indexes are bulk-loaded in the background on separate machines.</p><p>gives a high percentage of correct results and leads to a clean consolidated corpus over an approach that gives a higher percentage of consolidated results but leads to a partially garbled corpus; similarly, we prefer a system that can handle more data (is more scalable), but may possibly have a lower throughput (is less efficient). <ref type="foot" target="#foot_0">4</ref>Thus, herein we revisit methods for scalable, precise, automatic and domain-agnostic entity consolidation over large, static Linked Data corpora. In order to make our methods scalable, we avoid dynamic on-disk index structures and instead opt for algorithms that rely on sequential on-disk reads/writes of compressed flat files, using operations such as scans, external sorts, merge-joins, and only light-weight or non-critical in-memory indices. In order to make our methods efficient, we demonstrate distributed implementations of our methods over a cluster of shared-nothing commodity hardware, where our algorithms attempt to maximise the portion of time spent in embarrassingly parallel execution-i.e., parallel, independent computation without need for inter-machine coordination. In order to make our methods domain-agnostic and itfully-automatic, we exploit the generic formal semantics of the data described in RDF(S)/OWL and also, generic statistics derivable from the corpus. In order to achieve high recall, we incorporate additional OWL features to find novel coreference through reasoning. Aiming at high precision, we introduce methods that again exploit the semantics and also the statistics of the data, but to conversely disambiguate entities: to defeat equivalences found in the previous step that are unlikely to be true according to some criteria.</p><p>As such, extending upon various reasoning and consolidation techniques described in previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, we now give a self-contained treatment of our results in this area. In addition, we distribute the execution of all methods over a cluster of commodity hardware, we provide novel, detailed performance and quality evaluation over a large, real-world Linked Data corpus of one billion statements, and we also examine exploratory techniques for interlinking similar entities based on statistical analyses, as well as methods for disambiguating and repairing incorrect coreferences.</p><p>In summary, in this paper we:</p><p>-provide some necessary preliminaries and describe our distributed architecture (Section 2); -characterise the 1 billion quadruple Linked Data corpus that will be used for later evaluation of our methods, particular focusing on the (re-)use of data-level identifiers in the corpus (Section 3); -describe and evaluate our distributed base-line approach for consolidation, which leverages explicit owl:sameAs relations (Section 4); -describe and evaluate a distributed approach that extends consolidation to consider a richer OWL semantics for consolidating (Section 5); -present a distributed algorithm for determining weighted concurrence between entities using statistical analysis of predicates in the corpus (Section 6); -present a distributed approach to disambiguate entities-i.e., detect likely erroneous consolidation-combining the semantics and statistics derivable from the corpus (Section 7); -provide critical discussion (Section 8), render related work (Section 9) and conclude with discussion (Section 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>In this section, we provide some necessary preliminaries relating to (i) RDF: the structured format used in our corpora (Section 2.1); (ii) RDFS/OWL, which provides formal semantics to RDF data, including the semantics of equality (Section 2.2); (iii) rules, which are used to interpret the semantics of RDF and apply reasoning (Sections 2.3, 2.4); (iv) authoritative reasoning, which critically examines the source of certain types of Linked Data to help ensure robustness of reasoning (Section 2.5); (v) and OWL 2 RL/RDF rules: a standard rule-based profile for supporting OWL semantics, a subset of which we use to deduce equality (Section 2.6). Throughout, we attempt to preserve notation and terminology as prevalent in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">RDF</head><p>We briefly give some necessary notation relating to RDF constants and RDF triples; see <ref type="bibr" target="#b27">[28]</ref>.</p><p>RDF constant: Given the set of URI references U, the set of blank nodes B, and the set of literals L, the set of RDF constants is denoted by C = U [ B [ L. As opposed to the RDF-mandated existential semantics for blank-nodes, we interpret blank-nodes as ground Skolem constants; note also that we rewrite blank-node labels to ensure uniqueness per document as per RDF merging <ref type="bibr" target="#b27">[28]</ref>.</p><p>RDF triple: A triple t: = (s, p, o) 2 G: = (U [ B) Â U Â C is called an RDF triple, where s is called subject, p predicate, and o object. We call a finite set of triples G &amp; G a graph. This notion of a triple restricts the positions in which certain terms may appear. We use the phrase generalised triple where such restrictions are relaxed, and where literals and blank-nodes are allowed to appear in the subject/predicate positions.</p><p>RDF triple in context/quadruple: Given c 2 U, let http(c) denote the possibly empty graph G c given by retrieving the URI c through HTTP (directly returning 200 Okay). An ordered pair (t, c) with an RDF triple t = (s, p, o), c 2U and t 2http(c) is called a triple in context c. We may also refer to (s, p, o, c) as an RDF quadruple or quad q with context c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">RDFS, OWL and owl:sameAs</head><p>Conceptually, RDF data is composed of assertional data (i.e., instance data) and terminological data (i.e., schema data). Assertional data define relationships between individuals, provide literalvalued attributes of those individuals, and declare individuals as members of classes. Thereafter, terminological data describe those classes, relationships (object properties), and attributes (datatype properties) and declaratively assigns them a semantics. <ref type="foot" target="#foot_1">5</ref> With well-defined descriptions of classes and properties, reasoning can then allow for deriving new knowledge, including over assertional data.</p><p>On the Semantic Web, the RDFS <ref type="bibr" target="#b10">[11]</ref> and OWL <ref type="bibr" target="#b41">[42]</ref> standards are prominently used for making statements with well-defined meaning and enabling such reasoning. Although primarily concerned with describing terminological knowledge-such as subsumption or equivalence between classes and properties, etc.-RDFS and OWL also contain features that operate on an assertional level. The most interesting such feature for our purposes is owl:sameAs: a core OWL property that allows for defining equivalences between individuals (as well as classes and properties). Two individuals related by means of owl:sameAs are interpreted as referring to the same entity; i.e., they are coreferent.</p><p>Interestingly, OWL also contains other features (on a terminological level) that allow for inferring new owl:sameAs relations. Such features include:</p><p>-inverse-functional properties, which act as ''key properties'' for uniquely identifying subjects; e.g., an :isbn datatype-property whose values uniquely identifies books and other documents, or the object-property :biologicalMotherOf, which uniquely identifies the biological mother of a particular child; -functional properties, which work in the inverse direction and uniquely identify objects; e.g., the property :hasBiological Mother; -cardinality and max-cardinality restrictions, which, when given a value of 1, uniquely identify subjects of a given class; e.g., the value for the property :spouse uniquely identifies a member of the class :Monogamist.</p><p>Thus, we see that RDFS and OWL contain various features that allow for inferring and supporting coreference between individuals. In order to support the semantics of such features, we apply inference rules, introduced next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Inference rules</head><p>Herein, we formalise the notion of an inference rule as applicable over RDF graphs <ref type="bibr" target="#b32">[33]</ref>. We begin by defining the preliminary concept of a triple pattern, of which rules are composed, and which may contain variables in any position.</p><p>Triple pattern, basic graph pattern: A triple pattern is a generalised triple where variables from the set V are allowed; i.e.:</p><formula xml:id="formula_0">t v : = (s v , p v , o v ) 2G V , G V : = (C [ V) Â (C [ V) Â (C [ V).</formula><p>We call a set (to be read as conjunction) of triple patterns G V &amp; G V a basic graph pattern. We denote the set of variables in graph pattern G V by VðG V Þ.</p><p>Intuitively, variables appearing in triple patterns can be bound by any RDF term in C. Such a mapping from variables to constants is called a variable binding.</p><p>Variable bindings: Let X be the set of variable binding V [ C ? V [ C that map every constant c 2 C to itself and every variable v 2 V to an element of the set C [ V. A triple t is a binding of a triple pattern t v : = (s v , p v , o v ) iff there exists l 2 X, such that t = l(t v ) = (l(s v ), l(p v ), l(o v )). A graph G is a binding of a graph pattern G V iff there exists a mapping l 2 X such that</p><formula xml:id="formula_1">S t v 2G V lðt v Þ ¼ G; we use the shorthand lðG V Þ ¼ G. We use XðG V ; GÞ :¼ fl j lðG V Þ # G; lðvÞ ¼ v if v R VðG V Þg</formula><p>to denote the set of variable binding mappings for graph pattern G V in graph G that map variables outside G V to themselves.</p><p>We can now give the notion of an inference rule, which is comprised of a pair of basic graph patterns that form an ''IF ) THEN'' logical structure.</p><p>Inference rule: We define an inference rule (or often just rule) as a pair r :¼ ðAnte r ; Con r Þ, where the antecedent (or body) Ante r &amp; G V and the consequent (or head) Con r &amp; G V are basic graph patterns such that all variables in the consequent appear in the antecedent: VðCon r Þ # VðAnte r Þ. We write inference rules as Ante r ) Con r .</p><p>Finally, rules allow for applying inference through rule applications, where the rule body is used to generate variable bindings against a given RDF graph, and where those bindings are applied on the head to generate new triples that graph entails.</p><p>Rule application and standard closure: A rule application is the immediate consequences T r ðGÞ :¼ S l2XðAnter;GÞ ðlðCon r Þ n lðAnte r ÞÞ of a rule r on a graph G; accordingly, for a ruleset R,</p><formula xml:id="formula_2">T R ðGÞ :¼ S r2R T r ðGÞ. Now, let G iþ1 :¼ G i [ T R ðG i Þ and G 0 :¼ G; the exhaustive application of the T R operator on a graph G is then the least fixpoint (the smallest value for n) such that G n ¼ T R ðG n Þ.</formula><p>We call G n the closure of G wrt. ruleset R, denoted as Cl R ðGÞ, or succinctly G where the ruleset is obvious.</p><p>The above closure takes a graph and a ruleset and recursively applies the rules over the union of the original graph and the inferences until a fixpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">T-split rules</head><p>In <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>, we formalised an optimisation based on a separation of terminological knowledge from assertional data when applying rules. Similar optimisations have also been used by other authors to enable large-scale rule-based inferencing <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b42">43]</ref>. This optimisation is based on the premise that, in Linked Data (and various other scenarios), terminological data speaking about classes and properties is much smaller than assertional data speaking about individuals. Previous observations indicate that for a Linked Data corpus in the order of a billion triples, such terminological data comprises of about 0.1% of the total data volume. Additionally, terminological data is very frequently accessed during reasoning. Hence, seperating and storing the terminological data in memory allows for more efficient execution of rules, albeit at the cost of possible incompleteness <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>We now reintroduce some of the concepts relating to separating terminological information <ref type="bibr" target="#b32">[33]</ref>, which we will use later when applying rule-based reasoning to support the semantics of OWL equality. We begin by formalising some related concepts that help define our notion of terminological information.</p><p>Meta-class: We consider a meta-class as a class whose members are themselves (always) classes or properties. Herein, we restrict our notion of meta-classes to the set defined in RDF(S) and OWL specifications, where examples include rdf:Property, rdfs:Class, owl:DatatypeProperty, owl:FunctionalProperty, etc. Note that, e.g., owl:Thing and rdfs:Resource are not meta-classes since not all of their members are classes or properties.</p><p>Meta-property: A meta-property is one that has a meta-class as its domain; again, we restrict our notion of meta-properties to the set defined in RDF(S) and OWL specifications, where examples include rdfs:domain, rdfs:range, rdfs:subClassOf, owl:hasKey, owl:inverseOf, owl:oneOf, owl:onProperty, owl:unionOf, etc. Note that rdf:type, owl:sameAs, rdfs:label, e.g., do not have a meta-class as domain and so are not considered as meta-properties.</p><p>Terminological triple: We define the set of terminological triples T &amp; G as the union of: (i) triples with rdf:type as predicate and a meta-class as object; (ii) triples with a meta-property as predicate; (iii) triples forming a valid RDF list whose head is the object of a meta-property (e.g., a list used for owl:unionOf, etc.).</p><p>The above concepts cover what we mean by terminological data in the context of RDFS and OWL. Next, we define the notion of triple patterns that distinguish between these different categories of data.</p><p>Terminological/assertional pattern: We refer to a terminologicaltriple/-graph pattern as one whose instance can only be a terminological triple or, resp., a set thereof. An assertional pattern is any pattern that is not terminological.</p><p>Given the above notions of terminological data/patterns, we can now define the notion of a T-split inference rule that distinguishes terminological from assertional information.</p><p>T-split inference rule Given a rule r :¼ ðAnte r ; Con r ), we define a T-split rule r s as the triple (Ante These T-ground rules encode the terminological OWL knowledge given by the previous two triples, and can be applied directly over the assertional data, e.g., describing specific books with ISBN numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Authoritative T-split rules</head><p>Caution must be exercised when applying reasoning over arbitrary data collected from the Web; e.g., Linked Data. In previous works, we have encountered various problems when naïvely performing rule-based reasoning over arbitrary Linked Data <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>. In particular, third-party claims made about popular classes and properties must be critically analysed to ensure their trustworthiness. As a single example of the type of claims that can cause issues with reasoning, we found one document that defines nine properties as the domain of rdf:type;<ref type="foot" target="#foot_4">6</ref> thereafter, the semantics of rdfs:domain mandate that everything which is a member of any class (i.e., almost every known resource) can be inferred to be a ''member'' of each of the nine properties. We found that naïve reasoning lead to $200Â more inferences than would be expected when considering the definitions of classes and properties as defined in their ''namespace documents''. Thus, in the general case, performing rule-based materialisation with respect to OWL semantics over arbitrary Linked Data requires some critical analysis of the source of data, as per our authoritative reasoning algorithm. Please see <ref type="bibr" target="#b8">[9]</ref> for a detailed analysis of the explosion in inferences that occurs when authoritative analysis is not applied.</p><p>Such observations prompted us to investigate more robust forms of reasoning. Along these lines, when reasoning over Linked Data we apply an algorithm called authoritative reasoning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, which critically examines the source of terminological triples and conservatively rejects those that cannot be definitively trusted: i.e., those that redefine classes and/or properties outside of the namespace document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dereferencing and authority:</head><p>We first give the function http:U ? 2 G that maps a URI to an RDF graph it returns upon a HTTP lookup that returns the response code 200 Okay; in the case of failure, the function maps to an empty RDF graph. Next, we define the function redir:U ? U that follows a (finite) sequence of HTTP redirects given upon lookup of a URI, or that returns the URI itself in the absence of a redirect or in the case of failure; this function first strips the fragment identifier (given after the '#' character) from the original URI if present. Then we give the dereferencing function deref:httpredir as the mapping from a URI (a Web location) to an RDF graph it may provide by means of a given HTTP lookup that follows redirects. Finally, we can define the authority function-that maps the URI of a Web document to the set of RDF terms it is authoritative for-as follows:</p><formula xml:id="formula_3">auth : U ! 2 C u # fc 2 U j redirðcÞ ¼ ug [ BðhttpðsÞÞ;</formula><p>where B(G) denotes the set of blank-nodes appearing in a graph G. In other words, a Web document is authoritative for URIs that dereference to it and the blank nodes it contains.</p><p>We note that making RDF vocabulary URIs dereferenceable is encouraged in various best-practices <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b3">4]</ref>. To enforce authoritative reasoning, when applying rules with terminological and assertional patterns in the body we require that terminological triples are served by a document authoritative for terms that are bound by specific variable positions in the rule.</p><p>Authoritative T-split rule application: Given a T-split rule and a rule application involving a mapping l as before, for the rule application to be authoritative there must additionally exist a l(v</p><formula xml:id="formula_4">) such that v 2 VðAnte T Þ \ VðAnte G Þ, l(v) 2 auth(u), lðAnte T Þ # httpðuÞ.</formula><p>We call the set of variables that appear in both the terminological and assertional segment of the rule body (i.e., VðAnte T Þ \ V ðAnte G Þ) the set of authoritative variables for the rule. Where a rule does not have terminological or assertional patterns, we consider any rule-application as authoritative. The notation of an authoritative T-ground rule follows likewise.</p><p>Example Let R prp-ifp denote the same rule as used in the previous example, and let the following two triples be given by the dereferenced document of v1:isbn, but not v2:mbox-i.e., a document authoritative for the former term but not the latter.</p><p>(v1:isbn, a, owl:InverseFunctionalProperty) (v2:mbox, a, owl:InverseFunctionalProperty)</p><p>The only authoritative variable appearing in both the terminological and assertional patterns of the body of R prp-ifp is ?p, bound by v1:isbn and v2:mbox above. Since the document serving the two triples is authoritative for v1:isbn, the first triple is considered authoritative, and the following T-ground rule will be generated:</p><p>(?x1, v1:isbn, ?y), (?x2, v1:isbn, ?y) )(?x1, owl:sameAs, ?x2) However, since the document is not authoritative for v2:mbox, the second T-ground rule will be considered non-authoritative and discarded:</p><p>(?x1, v2:mbox, ?y), (?x2, v2:mbox, ?y) )(?x1, owl:sameAs, ?x2) where third-party documents re-define remote terms in a way that affects inferencing over instance data, we filter such nonauthoritative definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">OWL 2 RL/RDF rules</head><p>Inference rules can be used to (partially) support the semantics of RDFS and OWL. Along these lines, the OWL 2 RL/RDF <ref type="bibr" target="#b23">[24]</ref> ruleset is a partial-axiomatisation of the OWL 2 RDF-Based Semantics, which is applicable for arbitrary RDF graphs and constitutes an extension of the RDF Semantics <ref type="bibr" target="#b27">[28]</ref>; in other words, the ruleset supports a standardised profile of reasoning that partially covers the complete OWL semantics. Interestingly for our scenario, this profile includes inference rules that support the semantics and inference of owl:sameAs as discussed.</p><p>First, in Table <ref type="table" target="#tab_3">1</ref>, we provide the set of OWL 2 RL/RDF rules that support the (positive) semantics of owl: sameAs, axiomatising the symmetry (rule eq-sym) and transitivity (rule eq-trans) of the relation. To take an example, rule eq-sym is as follows:</p><p>(?x, owl:sameAs, ?y) )(?y, owl:sameAs, ?x)</p><p>where if we know that (:a, owl:sameAs, :b), the rule will infer the symmetric relation (:b, owl:sameAs, :a). Rule eq-trans operates analogously; both together allow for computing the transitive, symmetric closure of the equivalence relation. Furthermore, Table <ref type="table" target="#tab_3">1</ref> also contains the OWL 2 RL/RDF rules that support the semantics of replacement (rules eq-rep-*), whereby data that holds for one entity must also hold for equivalent entities.</p><p>Note that we (optionally, and in the case of later evaluation) choose not to support: (i) the reflexive semantics of owl:sameAs, since reflexive owl:sameAs statements will not lead to any consolidation or other non-reflexive equality relations and will produce a large bulk of materialisations; (ii) equality on predicates or values for rdf:type, where we do not want possibly imprecise owl:sameAs data to affect terms in these positions.</p><p>Given that the semantics of equality is quadratic with respect to the A-Box, we apply a partial-materialisation approach that gives our notion of consolidation: instead of materialising (i.e., explicitly writing down) all possible inferences given by the semantics of replacement, we instead choose one canonical identifier to represent the set of equivalent terms, thus effectively compressing the data. We have used this approach in previous works <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>, and it has also appeared in related works in the literature <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b5">6]</ref>, as a common-sense optimisation for handling data-level equality. To take an example, in later evaluation (cf. Table <ref type="table" target="#tab_11">10</ref>) we find a valid equivalence class (set of equivalent entities) with 33,052 members; materialising all pairwise non-reflexive owl:sameAs statements would infer more than 1 billion owl:sameAs relations (33,052 2 À 33,052) = 1,092,401,652; further assuming that each entity appeared in on average, e.g., two quadruples, we would infer an additional $2 billion of massively duplicated data. By choosing a single canonical identifier, we would instead only materialise $100 thousand statements.</p><p>Note that although we only perform partial materialisation, we do not change the semantics of equality: our methods are sound with respect to OWL semantics. In addition, alongside the partially materialised data, we provide a set of consolidated owl: sameAs relations (containing all of the identifiers in each equivalence class) that can be used to ''backward-chain'' the full inferences possible through replacement (as required). Thus, we do not consider the canonical identifier as somehow 'definitive' or superceding the other identifiers, but merely consider it as representing the equivalence class. <ref type="foot" target="#foot_5">7</ref>Finally, herein we do not consider consolidation of literals; one may consider useful applications, e.g., for canonicalising datatype literals, but such discussion is out of the current scope.</p><p>As previously mentioned, OWL 2 RL/RDF also contains rules that use terminological knowledge (alongside assertional knowledge) to directly infer owl: sameAs relations. We enumerate these rules in Table <ref type="table" target="#tab_4">2</ref>; note that we italicise the labels of rules supporting features new to OWL 2, and that we list authoritative variables in bold. <ref type="foot" target="#foot_6">8</ref>Applying only these OWL 2 RL/RDF rules may miss inference of some owl:sameAs statements. For example, consider the example RDF data given in Turtle syntax <ref type="bibr" target="#b2">[3]</ref>  The example uses properties from the prominent FOAF vocabulary, which is used for publishing personal profiles as RDF. Here, we additionally need OWL 2 RL/RDF rules prp-inv and prp-spo1handling standard owl:inverseOf and rdfs:subPropertyOf inferencing, respectively-to infer the owl:sameAs relation entailed by the data.</p><p>Along these lines, we support an extended set of OWL 2 RL/RDF rules that contain precisely one assertional pattern and for which we have demonstrated a scalable implementation called SAOR, designed for Linked Data reasoning <ref type="bibr" target="#b32">[33]</ref>. These rules are listed in Table <ref type="table">3</ref>, and as per the previous example, can generate additional inferences that indirectly lead to the derivation of new owl: sameAs data. We will use these rules for entity consolidation in Section 5.</p><p>Lastly, as we discuss later (particularly in Section 7) Linked Data is inherently noisy and prone to mistakes. Although our methods are sound (i.e., correct) with respect to formal OWL semantics, such noise in our input data may lead to unintended consequences when applying reasoning which we wish to minimise and/or repair. Relatedly, OWL contains features that allow for detecting formal contradictions-called inconsistencies-in RDF data. An example of an inconsistency would be where something is a member of two disjoint classes, such as foaf:Person and foaf:Organization; formally, the intersection of such disjoint classes should be empty and they should not share members. Along these lines, OWL 2 RL/RDF contains rules (with the special symbol false in the head) that allow for detecting inconsistency in an RDF graph. We use a subset of these consistency-checking OWL 2 RL/RDF rules later in Section 7 to try to automatically detect and repair unintended owl:sameAs inferences; the supported subset is listed in Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Distribution architecture</head><p>To help meet our scalability requirements, our methods are implemented on a shared-nothing distributed architecture <ref type="bibr" target="#b63">[64]</ref> over a cluster of commodity hardware. The distributed framework consists of a master machine that orchestrates the given tasks, and several slave machines that perform parts of the task in parallel.</p><p>The master machine can instigate the following distributed operations:</p><p>-Scatter: partition on-disk data using some local split function, and send each chunk to individual slave machines for subsequent processing;</p><p>-Run: request the parallel execution of a task by the slave machines-such a task either involves processing of some data local to the slave machine, or the coordinate method (described later) for reorganising the data under analysis; -Gather: gathers chunks of output data from the slave swarm and performs some local merge function over the data; -Flood: broadcast global knowledge required by all slave machines for a future task.  The master machine provides input data to the slave swarm, provides the control logic required by the distributed task (commencing tasks, coordinating timing, ending tasks), gathers and locally perform tasks on global knowledge that the slave machines would otherwise have to replicate in parallel, and transmits globally required knowledge.</p><p>The slave machines, as well as performing tasks in parallel, can perform the following distributed operation (at the behest of the master machine):</p><p>-Coordinate: local data on each slave machine is partitioned according to some split function, with the chunks sent to individual machines in parallel; each slave machine also gathers the incoming chunks in parallel using some merge function.</p><p>The above operation allows slave machines to reorganise (split/send/gather) intermediary amongst themselves; the coordinate operation could be replaced by a pair of gather/scatter operations performed by the master machine, but we wish to avoid the channelling of all intermediary data through one machine.</p><p>Note that herein, we assume that the input corpus is evenly distributed and split across the slave machines, and that the slave machines have roughly even specifications: that is, we do not consider any special form of load balancing, but instead aim to have uniform machines processing comparable datachunks.</p><p>We note that there is the practical issue of the master machine being idle waiting for the slaves, and, more critically, the potentially large cluster of slave machines waiting idle for the master machine. One could overcome idle times with mature task-scheduling (e.g., interleaving jobs) and load-balancing. From an algorithmic point of view, removing the central coordination on the master machine may enable better distributability. One possibility would be to allow the slave machines to duplicate the aggregation of global knowledge in parallel: although this would free up the master machine and would probably take roughly the same time, duplicating computation wastes resources that could otherwise be exploited by, e.g., interleaving jobs. A second possibility would be to avoid the requirement for global knowledge and to coordinate upon the larger corpus (e.g., a coordinate function hashing on the subject and object of the data, or perhaps an adaptation of the SpeedDate routing strategy <ref type="bibr" target="#b50">[51]</ref>). Such decisions are heavily influenced by the scale of the task to perform, the percentage of knowledge that is globally required, how the input data are distributed, how the output data should be distributed, and the nature of the cluster over which it should be performed and the task-scheduling possible. The distributed implementation of our tasks are designed to exploit a relatively small percentage of global knowledge which is cheap to coordinate, and we choose to avoid-insofar as reasonable-duplicating computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.">Experimental setup</head><p>Our entire code-base is implemented on top of standard Java libraries. We instantiate the distributed architecture using Java RMI libraries, and using the lightweight open-source Java RMIIO package 9 for streaming data for the network.</p><p>All of our evaluation is based on nine machines connected by Gigabit ethernet, 10 each with uniform specifications, viz., 2.2 GHz Opteron x86-64, 4 GB main memory, 160 GB SATA hard-disks, running Java 1.6.0_12 on Debian 5.0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental corpus</head><p>Later in this paper, we discuss the performance and results of applying our methods over a corpus of 1.118 billion quadruples derived from an open-domain RDF/XML crawl of 3.985 million web documents in mid-May 2010 (detailed in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29]</ref>). The crawl was conducted in a breadth-first manner, extracting URIs from all positions of the RDF data. Individual URI queues were assigned to different pay-level-domains (aka. PLDs: domains that require payment, e.g., deri.ie, data.gov.uk), where we enforced a politeness policy of accessing a maximum of two URIs per PLD per second. URIs with the highest inlink count per each PLD queue were polled first.</p><p>With regards the resulting corpus, of the 1.118 billion quads, 1.106 billion are unique, and 947 million are unique triples. The data contain 23 thousand unique predicates and 105 thousand unique class terms (terms in the object position of an rdf:type triple). In terms of diversity, the corpus consists of RDF from 783 pay-level-domains <ref type="bibr" target="#b31">[32]</ref>.</p><p>Note that further details about the parameters of the crawl and statistics about the corpus are available in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29]</ref>. Now we discuss the usage of terms in a data-level position, viz., terms in the subject position or object position of non-rdf:type triples. 11 Since we do not consider the consolidation of literals or schema-level concepts, we focus on characterising blank-node and URI re-use in such data-level positions, thus rendering a picture of the structure of the raw data.</p><p>We found 286.3 million unique terms, of which, 165.4 million (57.8%) were blank-nodes, 92.1 million (32.2%) were URIs, and 28.9 million (10%) were literals. With respect to literals, each had on average 9.473 data-level occurrences (by definition, all in the object position).</p><p>With respect to blank-nodes, each had on average 5.233 datalevel occurrences. Each occurred on average 0.995 times in the object position of a non-rdf:type triple, with 3.1 million (1.87%) not occurring in the object position; conversely, each occurred on average 4.239 times in the subject position of a triple, with 69 thousand (0.04%) not occurring in the subject position. Thus, we summarise that almost all blank-nodes appear in both the subject position and object position, but occur most prevalently in the former. Importantly, note that in our input, blank-nodes cannot be re-used across sources.</p><p>With respect to URIs, each had on average 9.41 data-level occurrences (1.8Â the average for blank-nodes), with 4.399 average appearances in the subject position and 5.01 appearances in the object position-19.85 million (21.55%) did not appear in an object position, whilst 57.91 million (62.88%) did not appear in a subject position.</p><p>With respect to re-use across sources, each URI had a data-level occurrence in, on average, 4.7 documents, and 1.008 PLDs-56.2 million (61.02%) of URIs appeared in only one document, and 91.3 million (99.13%) only appeared in one PLD. Also, re-use of URIs across documents was heavily weighted in favour of use in the object position: URIs appeared in the subject position in, on average, 1.061 documents and 0.346 PLDs; for the object position of 9 http://openhms.sourceforge.net/rmiio/. 10 We observe, e.g., a max FTP transfer rate of 38MB/s between machines. 11 Please see <ref type="bibr">[32,</ref> Appendix A] for further statistics relating to this corpus. non-rdf:type triples, URIs occurred in, on average, 3.996 documents and 0.727 PLDs.</p><p>The URI with the most data-level occurrences (1.66 million) was http://identi.ca/; the URI with the most re-use across documents (appearing in 179.3 thousand documents) was http://creativecommons.org/licenses/by/3.0/; the URI with the most re-use across PLDs (appearing in 80 different domains) was http://www.ldodds.com/foaf/foaf-a-matic. Although some URIs do enjoy widespread re-use across different documents and domains, in Figs. <ref type="figure">1</ref> and<ref type="figure">2</ref> we give the distribution of re-use of URIs across documents and across PLDs, where a power-law relationship is roughly evident-again, the majority of URIs only appear in one document (61%) or in one PLD (99%).</p><p>From this analysis, we can conclude that with respect to datalevel terms in our corpus:</p><p>-blank-nodes, which by their very nature cannot be re-used across documents, are 1.8Â more prevalent than URIs; -despite a smaller number of unique URIs, each one is used in (probably coincidentally) 1.8Â more triples; -unlike blank-nodes, URIs commonly only appear in either a subject position or an object position;</p><p>-each URI is re-used on average in 4.7 documents, but usually only within the same domain-most external re-use is in the object position of a triple; -99% of URIs appear in only one PLD.</p><p>We can conclude that within our corpus-itself a general crawl for RDF/XML on the Web-we find that there is only sparse reuse of data-level terms across sources, and particularly across domains.</p><p>Finally, for the purposes of demonstrating performance across varying numbers of machines, we extract a smaller corpus, comprising of 100 million quadruples, from the full evaluation corpus. We extract the sub-corpus from the head of the raw corpus; since the data are ordered by access time, polling statements from the head roughly emulates a smaller crawl of data, which should ensure, e.g., that all well-linked vocabularies are contained therein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Base-line consolidation</head><p>We now present the ''base-line'' algorithm for consolidation that consumes asserted owl:sameAs relations in the data. Linked Data best-practices encourage the provision of owl:sameAs links between exporters that coin different URIs for the same entities: ''It is common practice to use the owl:sameAs property for stating that another data source also provides information about a specific non-information resource.'' <ref type="bibr" target="#b6">[7]</ref> We would thus expect there to be a significant amount of explicit owl:sameAs data present in our corpus-provided directly by the Linked Data publishers themselves-that can be directly used for consolidation.</p><p>To perform consolidation over such data, the first step is to extract all explicit owl:sameAs data from the corpus. We must then compute the transitive and symmetric closure of the equivalence relation and build equivalence classes: sets of coreferent identifiers. Note that the set of equivalence classes forms a partition of coreferent identifiers where, due to the transitive and symmetric semantics of owl:sameAs, each identifier can only appear in one such class. Also note that we do not need to consider singleton equivalence classes that contain only one identifier: consolidation need not perform any action if an identifier is found only to be coreferent with itself. Once the closure of asserted owl:sameAs data has been computed, we then need to build an index that maps identifiers to the equivalence class in which it is contained. This index enables lookups of coreferent identifiers. Next, in the consolidated data, we would like to collapse the data mentioning identifiers in each equivalence class to instead use a single consistent, canonical identifier; for each equivalence class, we must thus choose a canonical identifier. Finally, we can scan the entire corpus, and using the equivalence class index, rewrite the original identifiers to their canonincal form. As an optional step, we can also add links between each canonical identifier and its coreferent forms using an artifical owl:sameAs relation in the output, thus persisting the original identifiers.</p><p>The distributed algorithm presented in this section is based on an in-memory equivalence class closure and index, and is the current method of consolidation employed by the SWSE system, described previously in <ref type="bibr" target="#b31">[32]</ref>. Herein, we briefly reintroduce the approach from <ref type="bibr" target="#b31">[32]</ref>, where we also add new performance evaluation over varying numbers of machines in the distributed setup, present more discussion and analysis of the use of owl:sameAs in our Linked Data corpus, and manually evaluate the precision of the approach for an extended sample of one thousand coreferent pairs. In particular, this section serves as a baseline for comparison against the extended consolidation approach that uses richer reasoning features, explored later in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">High-level approach</head><p>Based on the previous discussion, the approach is straightforward:</p><p>(i) scan the corpus and separate out all asserted owl:sameAs relations from the main body of the corpus; (ii) load these relations into an in-memory index that encodes the transitive and symmetric semantics of owl:sameAs; (iii) for each equivalence class in the index, choose a canonical term; (iv) scan the corpus again, canonicalising any term in the subject position or object position of an rdf:type triple.</p><p>Thus, we need only index a small subset of the corpusowl:sameAs statements-and can apply consolidation by means of two scans.</p><p>The non-trivial aspects of the algorithm are given by the equality closure and index. To perform the in-memory transitive, symmetric closure, we use a traditional union-find algorithm <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b39">40]</ref> for computing equivalence partitions, where (i) equivalent elements are stored in common sets such that each element is only contained in one set; (ii) a map provides a find function for looking up which set an element belongs to; and (iii) when new equivalent elements are found, the sets they belong to are unioned. The process is based on an in-memory map index, and is detailed in Algorithm 1, where:</p><p>(i) the eqc labels refer to equivalence classes, and s and o refer to RDF subject and object terms; (ii) the function map.get refers to an identifier lookup on the inmemory index that should return the intermediary equivalence class associated with that identifier (i.e., find); (iii) the function map.put associates an identifier with a new equivalence class.</p><p>The output of the algorithm is an in-memory map from identifiers to their respective equivalence class.</p><p>Next, we must choose a canonical term for each equivalence class: we prefer URIs over blank-nodes, thereafter choosing a term with the lowest alphabetical ordering; a canonical term is thus associated to each equivalent set. Once the equivalence index has been finalised, we re-scan the corpus and canonicalise the data using the in-memory index to service lookups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Distributed approach</head><p>Again, distribution of the approach is fairly intuitive, as follows:</p><p>(i) Run: scan the distributed corpus (split over the slave machines) in parallel to extract owl:sameAs relations; (ii) Gather: gather all owl:sameAs relations onto the master machine, and build the in-memory equality index; (iii) Flood/run: send the equality index (in its entirety) to each slave machine, and apply the consolidation scan in parallel.</p><p>As we will see in the next section, the most expensive methods-involving the two scans of the main corpus-can be conducted in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1.</head><p>Building equivalence map <ref type="bibr" target="#b31">[32]</ref> Require: SAMEAS DATA: SA 1: map {} 2: for (s,owl:sameAs,o) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance evaluation</head><p>We applied the distributed base-line consolidation over our corpus with the aformentioned procedure and setup. The entire consolidation process took 63.3 min, with the bulk of time taken as follows: the first scan extracting owl:sameAs statements took 12.5 min, with an average idle time for the servers of 11 s (1.4%)-i.e., on average, the slave machines spent 1.4% of the time idly waiting for peers to finish. Transferring, aggregating and loading the owl:sameAs statements on the master machine took 8.4 min. The second scan rewriting the data according to the canonical identifiers took in total 42.3 min, with an average idle time of 64.7 s (2.5%) for each machine at the end of the round. The slower time for the second round is attributable to the extra overhead of re-writing the GZip-compressed data to disk, as opposed to just reading.</p><p>In the rightmost column of Table <ref type="table" target="#tab_7">5</ref> (full-8), we give a breakdown of the timing for the tasks over the full corpus using eight slave machines and one master machine. Independent of the number of slaves, we note that the master machine required 8.5 min for coordinating globally-required owl:sameAs knowledge, and that the rest of the task time is spent in embarrassingly parallel execution (amenable to reduction by increasing the number of machines). For our setup, the slave machines were kept busy for, on average, 84.6% of the total task time; of the idle time, 87% was spent waiting for the master to coordinate the owl:sameAs data, and 13% was spent waiting for peers to finish their task due to sub-optimal load balancing. The master machine spent 86.6% of the task idle waiting for the slaves to finish.</p><p>In addition, Table <ref type="table" target="#tab_7">5</ref> also gives performance results for one master and varying numbers of slave machines (1, 2, 4, 8) over the 100 million quadruple corpus. Note that in the table, we use the symbol $ to indicate a negligible value (0 &lt; $ &lt; 0.05). We see that as the number of slave machines increases, so too does the percentage of time taken to aggregate global knowledge on the master machine (the absolute time stays roughly stable). This indicates that for a high number of machines, the aggregation of global knowledge will eventually become a bottleneck, and an alternative distribution strategy may be preferable, subject to further investigation. Overall, however, we see that the total execution times roughly halve when the number of slave machines are doubled: we observe a 0.528 Â , 0.536Â and 0.561Â reduction in total task execution time moving from 1 slave machine to 2, 2 to 4, and 4 to 8, respectively (here, a value of 0.5Â would indicate linear scaling out).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results evaluation</head><p>We extracted 11.93 million raw owl:sameAs quadruples from the full corpus. Of these, however, there were only 3.77 million unique triples. After closure, the data formed 2.16 million equivalence classes mentioning 5.75 million terms (6.24% of URIs)-an average of 2.65 elements per equivalence class. Of the 5.75 million terms, only 4156 were blank-nodes. Fig. <ref type="figure">3</ref> presents the distribution of sizes of the equivalence classes, where the largest equivalence class contains 8481 equivalent entities and 1.6 million (74.1%) equivalence classes contain the minimum two equivalent identifiers. Fig. <ref type="figure">4</ref> shows a similar distribution, but for the number of PLDs extracted from URIs in each equivalence class. Interestingly, the majority (57.1%) of equivalence classes contained identifiers from more than one PLD, where the most diverse equivalence class contained URIs from 32 PLDs.</p><p>Table <ref type="table" target="#tab_8">6</ref> shows the canonical URIs for the largest five equivalence classes; we manually inspected the results and show whether or not the results were verified as correct/incorrect. Indeed, results for class 1 and 2 were deemed incorrect due to over-use of owl:sameAs for linking drug-related entities in the DailyMed and LinkedCT exporters. Results 3 and 5 were verified as correct consolidation of prominent Semantic Web related authors, resp.: Dieter Fensel and Rudi Studer-authors are given many duplicate URIs by the RKBExplorer coreference index. <ref type="foot" target="#foot_7">12</ref>Result 4 contained URIs from various sites generally referring to the United States, mostly from DBPedia and LastFM. With respect to the DBPedia URIs, these (i) were equivalent but for capitilisation variations or stop-words, (ii) were variations of abbreviations or valid synonyms, (iii) were different language versions (e.g., dbpedia: E ´tats_Unis), (iv) were nicknames (e.g., dbpedia:Yankee_land), (v) were related but not equivalent (e.g., dbpedia:American_ Civilization), (vi) were just noise (e.g., dbpedia:LOL_Dean).</p><p>Besides the largest equivalence classes, which we have seen are prone to errors perhaps due to the snowballing effect of the transitive and symmetric closure, we also manually evaluated a random sample of results. For the sampling, we take the closed  equivalence classes extracted from the full corpus, pick an identifier (possibly non-canonical) at random and then randomly pick another coreferent identifier from its equivalence class. We then retrieve the data associated for both identifiers and manually inspect them to see if they are, in fact, equivalent. For each pair, we then select one of four options: SAME, DIFFERENT, UNCLEAR, TRIVIALLY SAME. We used the UNCLEAR option sparingly and only where there was not enough information to make an informed decision, or for difficult subjective choices; note that where there was not enough information in our corpus, we tried to dereference more information to minimise use of UNCLEAR; in terms of difficult, subjective choices, one example pair we marked unclear was dbpedia:Folkcore and dbpedia:Experimental_folk. We used the TRIVIALLY SAME option to indicate that an equivalence is purely ''syntactic'', where one identifier has no information other than being the target of a owl:sameAs link; although the identifiers are still coreferent, we distinguish this case since the equivalence is not so ''meaningful''. For the 1000 pairs, we choose option TRIVIALLY SAME 661 times (66.1%), SAME 301 times (30.1%), DIFFERENT 28 times (2.8%) and UN-CLEAR 10 times (1%). In summary, our manual verification of the baseline results puts the precision at $97.2%, albeit with many syntatic equivalences found. Of those found to be incorrect, many were closely-related DBpedia resources that were linked to by the same resource on the Freebase exporter; an example of this was dbpedia:Rock_Hudson and dbpedia:Rock_Hudson_filmography having owl:sameAs links from the same Freebase concept fb:rock_hudson.</p><p>Moving on, in Table <ref type="table">7</ref> we give the most frequently co-occurring PLD-pairs in our equivalence classes, where datasets resident on these domains are ''heavily'' interlinked with owl: sameAs relations. We italicise the indexes of inter-domain links that were observed to often be purely syntactic.</p><p>With respect to consolidation, identifiers in 78.6 million subject positions (7% of subject positions) and 23.2 million non-rdf: type-object positions (2.6%) were rewritten, giving a total of 101.9 million positions rewritten (5.1% of total rewritable positions). The average number of documents mentioning each URI rose slightly from 4.691 to 4.719 (a 0.6% increase) due to consolidation, and the average number of PLDs also rose slightly from 1.005 to 1.007 (a 0.2% increase).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Extended reasoning consolidation</head><p>We now look at extending the baseline approach to include more expressive reasoning capabilities, in particular using OWL 2 RL/RDF rules (introduced previously in Section 2.6) to infer novel owl:sameAs relations that can then be used for consolidation alongside the explicit relations asserted by publishers.</p><p>As described in Section 2.2, OWL provides various featuresincluding functional properties, inverse-functional properties, and certain cardinality restrictions-that allow for inferring novel owl:sameAs data. Such features could ease the burden on publishers of providing explicit owl:sameAs mappings to coreferent identifiers in remote naming schemes. For example, inverse-functional properties can be used in conjunction with legacy identification schemes-such as ISBNs for books, EAN Á UCC-13 or MPN for products, MAC addresses for network-enabled devices, etc.-to bootstrap identity on the Web of Data within certain domains; such identification values can be encoded as simple datatype strings, thus bypassing the requirement for bespoke agreement or mappings between URIs. Also, ''information resources'' with indigenous URIs can be used for indirectly identifying related resources to which they have a functional mapping, where examples include personal email-addresses, personal homepages, WIKIPEDIA articles, etc.</p><p>Although Linked Data literature has not explicitly endorsed or encouraged such usage, prominent grass-roots efforts publishing RDF on the Web rely (or have relied) on inverse-functional properties for maintaining consistent identity. For example, in the Friend Of A Friend (FOAF) community, a technique called smushing was proposed to leverage such properties for identity, serving as an early precursor to methods described herein. 13  Versus the baseline consolidation approach, we must first apply some reasoning techniques to infer novel owl:sameAs relations. Once we have the inferred owl:sameAs data materialised and merged with the asserted owl:sameAs relations, we can then apply the same consolidation approach as per the baseline: (i) apply the transitive symmetric closure and generate the equivalence classes, (ii) pick canonical identifiers for each class and (iii) rewrite the corpus. However, in contrast to the baseline, we expect the extended volume of owl:sameAs data to make in-memory storage infeasible. 14 Thus, in this section, we avoid the need to store equivalence information in-memory, where we instead investigate batch-processing methods for applying an on-disk closure of the equivalence relations, and likewise on-disk methods for rewriting data.</p><p>Early versions of the local batch-processing <ref type="bibr" target="#b30">[31]</ref> techniques and some of the distributed reasoning techniques <ref type="bibr" target="#b32">[33]</ref> are borrowed from previous works. Herein, we reformulate and combine these techniques into a complete solution for applying enhanced consolidation in a distributed, scalable manner over large-scale Linked a In fact, these were within the same PLD. 13 cf. http://wiki.foaf-project.org/w/Smushing; retr. 2011/01/22. 14 Further note that since all machines must have all owl:sameAs information, adding more machines does not increase the capacity for handling more such relations: the scalability of the baseline approach is limited by the machine with the least in-memory capacity.</p><p>Data corpora. We provide new evaluation over our 1.118 billion quadruple evaluation corpus, presenting new performance results over the full corpus and for a varying number of slave machines. We also analyse, in depth, the results of applying the techniques over our evaluation corpus, analysing the applicability of our methods in such scenarios. We contrast the results given by the baseline and extended consolidation approaches, and again manually evaluate the results of the extended consolidation approach for 1000 pairs found to be coreferent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">High-level approach</head><p>In Table <ref type="table" target="#tab_4">2</ref>, we provide the pertinent rules for inferring new owl: sameAs relations from the data. However, after analysis of the data, we observed that no documents used the owl:maxQuali-fiedCardinality construct required for the cls-maxqc* rules, and that only one document defined one owl:hasKey axiom <ref type="foot" target="#foot_8">15</ref>involving properties with less than five occurrences in the datahence, we leave implementation of these rules for future work and note that these new OWL 2 constructs have probably not yet had time to find proper traction on the Web. Thus, on top of inferencing involving explicit owl:sameAs, we are left with rule prp-fp, which supports the semantics of properties typed owl:FunctionalProperty; and rule prp-ifp, which supports the semantics of properties typed owl:InverseFunctionalProperty; and rule cls-maxc1, which supports the semantics of classes with a specified cardinality of 1 for some defined property (a class restricted version of the functional-property inferencing). <ref type="foot" target="#foot_9">16</ref>Thus, we look at using OWL 2 RL/RDF rules prp-fp, prp-ifp and cls-maxc2 for inferring new owl: sameAs relations between individuals-we also support an additional rule that gives an exact cardinality version of cls-maxc2. <ref type="foot" target="#foot_10">17</ref> Herein, we refer to these rules as consolidation rules.</p><p>We also investigate pre-applying more general OWL 2 RL/RDF reasoning over the corpus to derive more complete results, where the ruleset is available in Table <ref type="table">3</ref> and is restricted to OWL 2 RL/RDF rules with one assertional pattern <ref type="bibr" target="#b32">[33]</ref>; unlike the consolidation rules, these general rules do not require the computation of assertional joins, and thus are amenable to execution by means of a single-scan of the corpus. We have demonstrated this profile of reasoning to have good competency with respect to the features of RDFS and OWL used in Linked Data <ref type="bibr" target="#b28">[29]</ref>. These general rules may indirectly lead to the inference of additional owl:sameAs relations, particularly when combined with the consolidation rules of Table <ref type="table" target="#tab_4">2</ref>.</p><p>Once we have used reasoning to derive novel owl:sameAs data, we then apply an on-disk batch processing technique to close the equivalence relation and to ultimately consolidate the corpus.</p><p>Thus, our high-level approach is as follows:</p><p>(i) extract relevant terminological data from the corpus;</p><p>(ii) bind the terminological patterns in the rules from this data, thus creating a larger set of general rules with only one assertional pattern and identifying assertional patterns that are useful for consolidation; (iii) apply general-rules over the corpus, and buffer any input/ inferred statements relevant for consolidation to a new file; (iv) derive the closure of owl:sameAs statements from the consolidation-relevant dataset;</p><p>(v) apply consolidation over the main corpus with respect to the closed owl:sameAs data.</p><p>In Step (i), we extract terminological data-required for application of our rules-from the main corpus. As discussed in Section 2.5, to help ensure robustness, we apply authoritative reasoning; in other words, at this stage we discard any third-party terminological statements that affect inferencing over instance data for remote classes and properties.</p><p>In Step (ii), we then compile the authoritative terminological statements into the rules to generate a set of T-ground (purely assertional) rules, which can then be applied over the entirety of the corpus <ref type="bibr" target="#b32">[33]</ref>. As mentioned in Section 2.6, the T-ground general rules only contain one assertional pattern, and thus are amenable to execution by a single scan; e.g., given the statement:</p><p>:homepage rdfs:subPropertyOf :isPrimaryTopicOf we would generate a T-ground rule: ?x :homepage ?y )?x :isPrimaryTopicOf ?y which does not require the computation of joins. We also bind the terminological patterns of the consolidation rules in Table <ref type="table" target="#tab_4">2</ref>; however, these rules cannot be performed by means of a single scan over the data since they contain multiple assertional patterns in the body. Thus, we instead extract triple patterns, such as:</p><formula xml:id="formula_5">?x : isPrimaryTopicOf ?y.</formula><p>which may indicate data useful for consolidation (note that here, :isPrimaryTopicOf is assumed to be inverse-functional).</p><p>In Step (iii), we then apply the T-ground general rules over the corpus with a single scan. Any input or inferred statements matching a consolidation-relevant pattern-including any owl:sameAs statements found-are buffered to a file for later join computation.</p><p>Subsequently, in Step (iv), we must now compute the on-disk canonicalised closure of the owl:sameAs statements. In particular, we mainly employ the following three on-disk primitives:</p><p>(i) sequential scans of flat files containing line-delimited tuples;<ref type="foot" target="#foot_11">18</ref> (ii) external-sorts where batches of statements are sorted in memory, the sorted batches written to disk, and the sorted batches merged to the final output; and (iii) merge-joins where multiple sets of data are sorted according to their required join position, and subsequently scanned in an interleaving manner that aligns on the join position and where an in-memory join is applied for each individual join element.</p><p>Using these primitives to compute the owl:sameAs closure minimises the amount of main memory required, where we have presented similar approaches in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>First, assertional memberships of functional properties, inversefunctional properties and cardinality restrictions (both properties and classes) are written to separate on-disk files. For functionalproperty and cardinality reasoning, a consistent join variable for the assertional patterns is given by the subject position; for inverse-functional-property reasoning, a join variable is given by the object position. <ref type="foot" target="#foot_12">19</ref> Thus, we can sort the former sets of data according to subject and perform a merge-join by means of a linear scan thereafter; the same procedure applies to the latter set, sorting and merge-joining on the object position. Applying merge-join scans, we produce new owl:sameAs statements.</p><p>Both the originally asserted and newly inferred owl:sameAs relations are similarly written to an on-disk file, over which we now wish to perform the canonicalised symmetric/transitive closure. We apply a similar method again, leveraging external sorts and merge-joins to perform the computation (herein, we sketch and point the interested reader to <ref type="bibr" target="#b30">[31,</ref><ref type="bibr">Section 4.6]</ref>). In the following, we use &gt; and &lt; to denote lexical ordering, SA as a shortcut for owl:sameAs, a, b, c, etc., to denote members of U [ B such that a &lt; b &lt; c, and define URIs to be lexically lower than blank nodes. The process is as follows:</p><p>(i) we only materialise symmetric equality relations that involve a (possibly intermediary) canonical term chosen by a lexical ordering: given b SA a, we materialise a SA b; given a SA b SA c, we materialise the relations a SA b, a SA c, and their inverses, but do not materialise b SA c or its inverse; (ii) transitivity is supported by iterative merge-join scans:</p><p>-in the scan, if we find c SA a (sorted by object) and c SA d (sorted naturally), we infer a SA d and drop the noncanonical c SA d (and d SA c); -at the end of the scan: newly inferred triples are marked and merge-joined into the main equality data; any triples echoing an earlier inference are ignored; dropped non-canonical statements are removed;</p><p>-the process is then iterative: in the next scan, if we find d SA a and d SA e, we infer a SA e and e SA a; -inferences will only occur if they involve a statement added in the previous iteration, ensuring that inference steps are not re-computed and that the computation will terminate; (iii) the above iterations stop when a fixpoint is reached and nothing new is inferred; (iv) the process of reaching a fixpoint is accelerated using available main-memory to store a cache of partial equality chains.</p><p>The above steps follow well-known methods for transtive closure computation, modified to support the symmetry of owl:sameAs and to use adaptive canonicalisation in order to avoid quadratic output. With respect to the last item, we use Algorithm 1 to derive ''batches'' of in-memory equivalences, and when in-memory capacity is achieved, we write these batches to disk and proceed with on-disk computation: this is particularly useful for computing the small number of long equality chains, which would otherwise require sorts and merge-joins over all of the canonical owl:sameAs data currently derived, and where the number of iterations would otherwise be the length of the longest chain. The result of this process is a set of canonicalised equality relations representing the symmetric/ transitive closure.</p><p>Next, we briefly describe the process of canonicalising data with respect to this on-disk equality closure, where we again use external-sorts and merge-joins. First, we prune the owl:sameAs index to only maintain relations s 1 SA s 2 such that s 1 &gt; s 2 ; thus, given s 1 SA s 2 , we know that s 2 is the canonical identifier, and s 1 is to be rewritten. We then sort the data according to the position that we wish to rewrite, and perform a merge-join over both sets of data, buffering the canonicalised data to an output file. If we want to rewrite multiple positions of a file of tuples (e.g., subject and object), we must rewrite one position, sort the results by the second position, and then rewrite the second position. 20  Finally, note that in the derivation of owl:sameAs from the consolidation rules prp-fp, prp-ifp, cax-maxc2, the overall process may be iterative. For example, consider: dblp:Axel foaf:isPrimaryTopicOf &lt;http://polleres.net/&gt; axel:me foaf:isPrimaryTopicOf &lt;http://axel.deri.ie/&gt; &lt;http://polleres.net/&gt; owl:sameAs &lt;http://axel.deri.ie/&gt; from which the conclusion that dblp:Axel is the same as axel:me holds. We see that new owl:sameAs relations (either asserted or derived from the consolidation rules) may in turn ''align'' terms in the join position of the consolidation rules, leading to new equivalences. Thus, for deriving the final owl:sameAs, we require a higher-level iterative process as follows:</p><p>(i) initially apply the consolidation rules, and append the results to a file alongside the asserted owl: sameAs statements found; (ii) apply the initial closure of the owl:sameAs data; (iii) then, iteratively until no new owl:sameAs inferences are found:</p><p>-rewrite the join positions of the on-disk files containing the data for each consolidation rule according to the current owl:sameAs data; -derive new owl:sameAs inferences possible through the previous rewriting for each consolidation rule; -re-derive the closure of the owl:sameAs data including the new inferences.</p><p>The final closed file of owl:sameAs data can then be re-used to rewrite the main corpus in two sorts and merge-join scans over subject and object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Distributed approach</head><p>The distributed approach follows quite naturally from the previous discussion. As before, we assume that the input data are evenly pre-distributed over the slave machines (in any arbitrary ordering), where we can then apply the following process:</p><p>(i) Run: scan the distributed corpus (split over the slave machines) in parallel to extract relevant terminological knowledge; (ii) Gather: gather terminological data onto the master machine and thereafter bind the terminological patterns of the general/consolidation rules; (iii) Flood: flood the rules for reasoning and the consolidationrelevant patterns to all slave machines; (iv) Run: apply reasoning and extract consolidation-relevant statements from the input and inferred data; (v) Gather: gather all consolidation statements onto the master machine, then in parallel: 20 One could instead build an on-disk map for equivalence classes and pivot elements and follow a consolidation method similar to the previous section over the unordered data: however, we would expect such an on-disk index to have a low cache hit-rate given the nature of the data, which would lead to many (slow) disk seeks. An alternative approach might be to hash-partition the corpus and equality index over different machines: however, this would require a non-trivial minimum amount of memory on the cluster.</p><p>-Local: compute the closure of the consolidation rules and the owl:sameAs data on the master machine; -Run: each slave machine sorts its fragment of the main corpus according to natural order (s, p, o, c); (vi) Flood: send the closed owl:sameAs data to the slave machines once the distributed sort has been completed; (vii) Run: each slave machine then rewrites the subjects of their segment of the corpus, subsequently sorts the rewritten data by object, and then rewrites the objects (of non-rdf:type triples) with respect to the closed owl:sameAs data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance evaluation</head><p>Applying the above process to our 1.118 billion quadruple corpus took 12.34 h. Extracting the terminological data took 1.14 h with an average idle time of 19 min (27.7%) (one machine took $18 min longer than the rest due to processing a large ontology containing $2 million quadruples <ref type="bibr" target="#b31">[32]</ref>). Merging and aggregating the terminological data took roughly $1 min. Applying the reasoning and extracting the consolidation relevant statements took 2.34 h, with an average idle time of 2.5 min (1.8%). Aggregating and merging the consolidation relevant statements took 29.9 min. Thereafter, locally computing the closure of the consolidation rules and the equality data took 3.52 h, with the computation requiring two iterations overall (the minimum possible-the second iteration did not produce any new results). Concurrent to the previous step, the parallel sort of remote data by natural order took 2.33 h with an average idle time of 6 min (4.3%). Subsequent parallel consolidation of the data took 4.8 h with 10 min (3.5%) average idle time-of this, $19% of the time was spent consolidating the pre-sorted subjects, $60% of the time was spent sorting the rewritten data by object, and $21% of the time was spent consolidating the objects of the data.</p><p>As before, Table <ref type="table" target="#tab_9">8</ref> summarises the timing of the task. Focusing on the performance for the entire corpus, the master machine took 4.06 h to coordinate global knowledge, constituting the lower bound on time possible for the task to execute with respect to increasing machines in our setup-in future it may be worthwhile to investigate distributed strategies for computing the owl:sameAs closure (which takes 28.5% of the total computation time), but for the moment we mitigate the cost by concurrently running a sort on the slave machines, thus keeping the slaves busy for 63.4% of the time taken for this local aggregation step. The slave machines were, on average, busy for 80.9% of the total task time; of the idle time, 73.3% was spent waiting for the master machine to aggregate the consolidation relevant data and to finish the closure of owl: sameAs data, and the balance (26.7%) was spent waiting for peers to finish (mostly during the extraction of terminological data).</p><p>With regards performance evaluation over the 100 million quadruple corpus for a varying number of slave machines, perhaps most notably, the average idle time of the slave machines increases quite rapidly as the number of machines increases. In particular, by 4 machines, the slaves can perform the distributed sort-by-subject task faster than the master machine can perform the local owl: sameAs closure. The significant workload of the master machine will eventually become a bottleneck as the number of slave machines increases further. This is also noticeable in terms of overall task time: the respective time savings as the number of slave machines doubles (moving from 1 slave machine to 8) are 0.507 Â , 0.554Â and 0.638Â, respectively.</p><p>Briefly, we also ran the consolidation without the general reasoning rules (Table <ref type="table">3</ref>) motivated earlier. With respect to performance, the main variations were given by (i) the extraction of consolidation relevant statements-this time directly extracted from explicit statements as opposed to explicit and inferred statements-which took 15.4 min (11% of the time taken including the general reasoning) with an average idle time of less than one minute (6% average idle time); (ii) local aggregation of the consolidation relevant statements took 17 min (56.9% of the time taken previously); (iii) local closure of the owl:sameAs data took 3.18 h (90.4% of the time taken previously). The total time saved equated to 2.8h (22.7%), where 33.3 min were saved from coordination on the master machine, and 2.25 h were saved from parallel execution on the slave machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results evaluation</head><p>In this section, we present the results of the consolidation, which included the general reasoning step in the extraction of the consolidation-relevant statements. In fact, we found that the only major variation between the two approaches was in the amount of consolidation-relevant statements collected (discussed presently), where the changes in the total amounts of coreferent identifiers, equivalence classes, and canonicalised terms were in fact negligible (&lt;0.1%). Thus, for our corpus, extracting only asserted consolidation-relevant statements offered a very close approximation of the extended reasoning approach. 21    Extracting the terminological data, we found authoritative declarations of 434 functional properties, 57 inverse-functional properties, and 109 cardinality restrictions with a value of 1. We discarded some third-party, non-authoritative declarations of inverse-functional or functional properties, many of which were simply ''echoing'' the authoritative semantics of those properties; e.g., many people copy the FOAF vocabulary definitions into their local documents. We also found some other documents on the bio2rdf.org domain that define a number of popular third-party properties as being functional, including dc:title, dc:identifier, foaf:name, etc. 22 However, these particular axioms would only affect consolidation for literals; thus, we can say that if applying only the consolidation rules, also considering non-authoritative definitions would not affect the owl:sameAs inferences for our corpus. Again, non-authoritative reasoning over general rules for a corpus such as ours quickly becomes infeasible <ref type="bibr" target="#b8">[9]</ref>.</p><p>We (again) gathered 3.77 million owl:sameAs triples, as well as 52.93 million memberships of inverse-functional properties, 11.09 million memberships of functional properties, and 2.56 million cardinality-relevant triples. Of these, respectively, 22.14 million (41.8%), 1.17 million (10.6%) and 533 thousand (20.8%) were asserted-however, in the resulting closed owl:sameAs data derived with and without the extra reasoned triples, we detected a variation of less than 12 thousand terms (0.08%), where only 129 were URIs, and where other variations in statistics were less than 0.1% (e.g., there were 67 less equivalence classes when the reasoned triples were included).</p><p>From previous experiments for older RDF Web data <ref type="bibr" target="#b29">[30]</ref>, we were aware of certain values for inverse-functional properties and functional properties that are erroneously published by exporters, and which cause massive incorrect consolidation. We again blacklist statements featuring such values from our consolidation processing, where we give the top 10 such values encountered for our corpus in Table <ref type="table" target="#tab_10">9</ref>-this blacklist is the result of trial and error, manually inspecting large equivalence classes and the most common values for (inverse-)functional properties. Empty literals are commonly exported (with and without language tags) as values for inverse-functional-properties (particularly FOAF ''chat-ID properties''). The literal "08445a31a78661b5c746fef-f39a9db6e4e2cc5cf"is the SHA-1 hash of the string 'mailto: ', commonly assigned as a foaf:mbox_sha1sum value to users who do not specify their email in some input form. The remaining URIs are mainly user-specified values for foaf:homepage, or values automatically assigned for users that do not specify such. 23  During the computation of the owl:sameAs closure, we found zero inferences through cardinality rules, 106.8 thousand raw owl: sameAs inferences through functional-property reasoning, and 8.7 million raw owl:sameAs inferences through inverse-func-tional-property reasoning. The final canonicalised, closed, and nonsymmetric owl:sameAs index (such that s 1 SA s 2 , s 1 &gt; s 2 , and s 2 is a canonical identifier) contained 12.03 million statements.</p><p>From this data, we generated 2.82 million equivalence classes (an increase of 1.31Â from baseline consolidation) mentioning a total of 14.86 million terms (an increase of 2.58Â from baseline-5.77% of all URIs and blank-nodes), of which 9.03 million were blank-nodes (an increase of 2173Â from baseline-5.46% of all blank-nodes) and 5.83 million were URIs (an increase of 1.014Â from baseline-6.33% of all URIs). Thus, we see a large expansion in the amount of blank-nodes consolidated, but only minimal expansion in the set of URIs referenced in the equivalence classes. With respect to the canonical identifiers, 641 thousand (22.7%) were blank-nodes and 2.18 million (77.3%) were URIs.</p><p>Fig. <ref type="figure">5</ref> contrasts the equivalence class sizes for the baseline approach (seen previously in Fig. <ref type="figure">3</ref>), and for the extended reasoning approach. Overall, there is an observable increase in equivalence class sizes, where we see the average equivalence class size grow to 5.26 entities (1.98Â baseline), the largest equivalence class size grow to 33,052 (3.9 Â baseline) and the percentage of equivalence classes with the minimum size 2 drop to 63.1% (from 74.1% in baseline).</p><p>In Table <ref type="table" target="#tab_11">10</ref>, we update the five largest equivalence classes. Result 2 carries over from the baseline consolidation. The rest of the results are largely intra-PLD equivalences, where the entity is described using thousands of blank-nodes, with a consistent (inverse-)functional property value attached. Result 1 refers to a meta-user-labelled Team Vox-commonly appearing in user-FOAF exports on the Vox blogging platform. 24 Result 3 refers to a person identified using blank-nodes (and once by URI) in thousands of RDF documents resident on the same server. Result 4 refers to the Image Bioinformatics Research Group in the University of Oxford-labelled IBRG-where again it is identified in thousands of documents using different blank-nodes, but a consistent foaf:homepage. Result 5 is similar to result 1, but for a Japanese version of the Vox user.</p><p>We performed an analogous manual inspection of one thousand pairs as per the sampling used previously in Section 4.4 for the baseline consolidation results. This time, we selected SAME 823Â (82.3%), TRIVIALLY SAME 145Â (14.5%), DIFFERENT 23Â (2.3%) and UNCLEAR 9Â (0.9%). This gives an observed precision for the sample of $97.7% (vs. $97.2% for baseline). After applying our blacklisting, the extended consolidation results are, in fact, slightly more precise (on average) than the baseline equivalences: this is due in part  <ref type="figure">5</ref>. Distribution of the number of identifiers per equivalence classes for baseline consolidation and extended reasoning consolidation (log/log). 22 cf. http://bio2rdf.org/ns/bio2rdf:Topic; offline as of 2011/06/20. 23 Our full blacklist contains forty-one such values, and can be found at http:// aidanhogan.com/swse/blacklist.txt. 24 This site shut down on 2010/09/30.</p><p>to a high number of blank-nodes being consolidated correctly from very uniform exporters of FOAF data that ''dilute'' the incorrect results. <ref type="foot" target="#foot_13">25</ref>Fig. <ref type="figure" target="#fig_2">6</ref> presents a similar analysis to Fig. <ref type="figure">5</ref>, this time looking at identifiers on a PLD-level granularity. Interestingly, the difference between the two approaches is not so pronounced, initially indicating that many of the additional equivalences found through the consolidation rules are ''intra-PLD''. In the baseline consolidation approach, we determined that 57% of equivalence classes were inter-PLD (contain identifiers from more than one PLD), with the plurality of equivalence classes containing identifiers from precisely two PLDs (951thousand, 44.1%); this indicates that explicit owl:sameAs relations are commonly asserted between PLDs. In the extended consolidation approach (which of course subsumes the above results), we determined that the percentage of inter-PLD equivalence classes dropped to 43.6%, with the majority of equivalence classes containing identifiers from only one PLD (1.59 million, 56.4%). The entity with the most diverse identifiers (the observable outlier on the x-axis in Fig. <ref type="figure" target="#fig_2">6</ref>) was the person ''Dan Brickley''-one of the founders and leading contributors of the FOAF project-with 138 identifiers (67 URIs and 71 blanknodes) minted in 47 PLDs; various other prominent community members and some country identifiers also featured high on the list.</p><p>In Table <ref type="table" target="#tab_12">11</ref>, we compare the consolidation of the top five ranked identifiers in the SWSE system (see <ref type="bibr" target="#b31">[32]</ref>). The results refer, respectively, to (i) the (co-)founder of the Web ''Tim Berners-Lee''; (ii) ''Dan Brickley'' as aforementioned; (iii) a meta-user for the micro-blogging platform StatusNet, which exports RDF; (iv) the ''FOAF-a-matic'' FOAF profile generator (linked from many diverse domains hosting FOAF profiles it created); and (v) ''Evan Prodromou'', founder of the identi.ca/StatusNet micro-blogging service and platform. We see a significant increase in equivalent identifiers found for these results; however, we also noted that after reasoning consolidation, Dan Brickley was conflated with a second person. <ref type="foot" target="#foot_14">26</ref>Note that the most frequently co-occurring PLDs in our equivalence classes remained unchanged from Table <ref type="table">7</ref>.</p><p>During the rewrite of the main corpus, terms in 151.77 million subject positions (13.58% of all subjects) and 32.16 million object positions (3.53% of non-rdf:type objects) were rewritten, giving a total of 183.93 million positions rewritten (1.8Â the baseline consolidation approach). In Fig. <ref type="figure" target="#fig_3">7</ref>, we compare the re-use of terms across PLDs before consolidation, after baseline consolidation, and after the extended reasoning consolidation. Again, although there is an increase in re-use of identifiers across PLDs, we note that:    (i) the vast majority of identifiers (about 99%) still only appear in one PLD; (ii) the difference between the baseline and extended reasoning approach is not so pronounced. The most widely referenced consolidated entity-in terms of unique PLDs-was ''Evan Prodromou'' as aformentioned, referenced with six equivalent URIs in 101 distinct PLDs.</p><p>In summary, we conclude that applying the consolidation rules directly (without more general reasoning) is currently a good approximation for Linked Data, and that in comparison to the baseline consolidation over explicit owl:sameAs, (i) the additional consolidation rules generate a large bulk of intra-PLD equivalences for blank-nodes; <ref type="foot" target="#foot_15">27</ref> (ii) relatedly, there is only a minor expansion (1.014Â) in the number of URIs involved in the consolidation; (iii) with blacklisting, the overall precision remains roughly stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Statistical concurrence analysis</head><p>Having looked extensively at the subject of consolidating Linked Data entities, in this section, we now introduce methods for deriving a weighted concurrence score between entities in the Linked Data corpus: we define entity concurrence as the sharing of outlinks, inlinks and attribute values, denoting a specific form of similarity. Conceptually, concurrence generates scores between two entities based on their shared inlinks, outlinks, or attribute values. More ''discriminating'' shared characteristics lend a higher concurrence score; for example, two entities based in the same village are more concurrent than two entities based in the same country. How discriminating a certain characteristic is is determined through statistical selectivity analysis. The more characteristics two entities share, (typically) the higher their concurrence score will be.</p><p>We use these concurrence measures to materialise new links between related entities, thus increasing the interconnectedness of the underlying corpus. We also leverage these concurrence measures in Section 7 for disambiguating entities, where, after identifying that an equivalence class is likely erroneous and causing inconsistency, we use concurrence scores to realign the most similar entities.</p><p>In fact, we initially investigated concurrence as a means of increasing the recall of consolidation for Linked Data; the core premise of this investigation was that very similar entities-with higher concurrence scores-are likely to be coreferent. Along these lines, the methods described herein are based on preliminary works <ref type="bibr" target="#b33">[34]</ref>, where we:</p><p>-investigated domain-agnostic statistical methods for performing consolidation and identifying equivalent entities; -formulated an initial small-scale (5.6 million triples) evaluation corpus for the statistical consolidation using reasoning consolidation as a best-effort ''gold-standard''.</p><p>Our evaluation gave mixed results where we found some correlation between the reasoning consolidation and the statistical methods, but we also found that our methods gave incorrect results at high degrees of confidence for entities that were clearly not equivalent, but shared many links and attribute values in common. This highlights a crucial fallacy in our speculative approach: in almost all cases, even the highest degree of similarity/concurrence does not necessarily indicate equivalence or co-reference (Section 4.4 cf. <ref type="bibr" target="#b26">[27]</ref>). Similar philosophical issues arise with respect to handling transitivity for the weighted ''equivalences'' derived <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>. However, deriving weighted concurrence measures has applications other than approximative consolidation: in particular, we can materialise named relationships between entities that share a lot in common, thus increasing the level of inter-linkage between entities in the corpus. Again, as we will see later, we can leverage the concurrence metrics to repair equivalence classes found to be erroneous during the disambiguation step of Section 7. Thus, we present a modified version of the statistical analysis presented in <ref type="bibr" target="#b33">[34]</ref>, describe a (novel) scalable and distributed implementation thereof, and finally evaluate the approach with respect to finding highlyconcurring entities in our 1 billion triple Linked Data corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">High-level approach</head><p>Our statistical concurrence analysis inherits similar primary requirements to that imposed for consolidation: the approach should be scalable, fully automatic, and domain agnostic to be applicable in our scenario. Similarly, with respect to secondary criteria, the approach should be efficient to compute, should give high precision, and should give high recall. Compared to consolidation, high precision is not as critical for our statistical use-case: in SWSE, we aim to use concurrence measures as a means of suggesting additional navigation steps for users browsing the entities-if the suggestion is uninteresting, it can be ignored, whereas incorrect consolidation will often lead to conspicuously garbled results, aggregating data on multiple disparate entities.</p><p>Thus, our requirements (particularly for scale) preclude the possibility of complex analyses or any form of pair-wise comparison, etc. Instead, we aim to design lightweight methods implementable by means of distributed sorts and scans over the corpus. Our methods are designed around the following intuitions and assumptions: (i) the concurrence of entities is measured as a function of their shared pairs, be they predicate-subject (loosely, inlinks), or predicate-object pairs (loosely, outlinks or attribute values); (ii) the concurrence measure should give a higher weight to exclusive shared-pairs-pairs that are typically shared by few entities, for edges (predicates) that typically have a low in-degree/out-degree; (iii) with the possible exception of correlated pairs, each additional shared pair should increase the concurrence of the entities-a shared pair cannot reduce the measured concurrence of the sharing entities; (iv) strongly exclusive property-pairs should be more influential than a large set of weakly exclusive pairs; (v) correlation may exist between shared pairs-e.g., two entities may share an inlink and an inverse-outlink to the same node (e.g., foaf:depiction, foaf:depicts), or may share a large number of shared pairs for a given property (e.g., two entities co-authoring one paper are more likely to co-author subsequent papers)-where we wish to dampen the cumulative effect of correlation in the concurrence analysis; (vi) the relative value of the concurrence measure is important; the absolute value is unimportant.</p><p>In fact, the concurrence analysis follows a similar principle to that for consolidation, where instead of considering discrete functional and inverse-functional properties as given by the semantics of the data, we attempt to identify properties that are quasi-functional, quasi-inverse-functional, or what we more generally term exclusive: we determine the degree to which the values of properties (here abstracting directionality) are unique to an entity or set of entities. The concurrence between two entities then becomes an aggregation of the weights for the property-value pairs they share in common. Consider the following running-example: dblp:AliceB10 foaf:maker ex:Alice dblp:AliceB10 foaf:maker ex:Bob ex:Alice foaf:gender ''female'' ex:Alice foaf:workplaceHomepage &lt;http:// wonderland.com&gt; ex:Bob foaf:gender ''male'' ex:Bob foaf:workplaceHomepage &lt;http:// wonderland.com&gt; ex:Claire foaf:gender ''female'' ex:Claire foaf:workplaceHomepage &lt;http:// wonderland.com&gt;</p><p>where we want to determine the level of (relative) concurrence between three colleagues: ex:Alice, ex:Bob and ex:Claire: i.e., how much do they coincide/concur with respect to exclusive shared pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">Quantifying concurrence</head><p>First, we want to characterise the uniqueness of properties; thus, we analyse their observed cardinality and inverse-cardinality as found in the corpus (in contrast to their defined cardinality as possibly given by the formal semantics): Definition 1 (Observed cardinality). Let G be an RDF graph, p be a property used as a predicate in G and s be a subject in G. The observed cardinality henceforth in this section, simply cardinality) of p wrt s in G, denoted Card G (p, s), is the cardinality of the set {o 2 Cj(s, p, o) 2 G}.</p><p>Definition 2 (Observed inverse-cardinality). Let G and p be as before, and let o be an object in G. The observed inverse-cardinality (or henceforth in this section, simply inverse-cardinality) of p wrt o in G, denoted ICard G (p, o), is the cardinality of the set {s 2 U [ Bj(s, p, o) 2 G}. Thus, loosely, the observed cardinality of a property-subject pair is the number of unique objects it appears within the graph (or unique triples it appears in); letting G ex denote our example graph, then, e.g., Card Gex (foaf:maker,dblp:AliceB10) = 2. We see this value as a good indicator of how exclusive (or selective) a given property-subject pair is, where sets of entities appearing in the object position of low-cardinality pairs are considered to concur more than those appearing with high-cardinality pairs. The observed inverse-cardinality of a property-object pair is the number of unique subjects it appears with in the graph-e.g., ICard Gex (foaf: gender,"female") = 2. Both directions are considered analogous for deriving concurrence scores-note however that we do not consider concurrence for literals (i.e., we do not derive concurrence for literals that share a given predicate-subject pair; we do of course consider concurrence for subjects with the same literal value for a given predicate).</p><p>To avoid unnecessary duplication, we henceforth focus on describing only the inverse-cardinality statistics of a property, where the analogous metrics for plain-cardinality can be derived by switching subject and object (that is, switching directionality)-we choose the inverse direction as perhaps being more intuitive, indicating concurrence of entities in the subject position based on the predicate-object pairs they share.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3 (Average inverse-cardinality).</head><p>Let G be an RDF graph, and p be a property used as a predicate in G. The average inversecardinality (AIC) of p with respect to G, written AIC G (p), is the average of the non-zero inverse-cardinalities of p in the graph G. Formally: AIC G ðpÞ ¼ j fðs; oÞ j ðs; p; oÞ 2 Gg j j fo j 9s : ðs; p; oÞ 2 Gg j :</p><p>The average cardinality AC G (p) of a property p is defined analogously as for AIC G (p). Note that the (inverse-)cardinality value of any term appearing as a predicate in the graph is necessarily greater-than or equal-to one: the numerator is by definition greaterthan or equal-to the denominator. Taking an example, AIC Gex (foaf: gender)=1.5, which can be viewed equivalently as the average non-zero cardinalities of foaf:gender (1 for "male" and 2 for "female"), or the number of triples with predicate foaf: gender divided by the number of unique values appearing in the object position of such triples ( <ref type="formula">3</ref>2 ). We call a property p for which we observe AIC G (p) % 1, a quasiinverse-functional property with respect to the graph G, and analogously properties for which we observe AC G (p) % 1 as quasi-functional properties. We see the values of such properties-in their respective directions-as being very exceptional: very rarely shared by entities. Thus, we would expect a property such as foaf: gender to have a high AIC G (p) since there are only two object-values ("male", "female") shared by a large number of entities, whereas we would expect a property such as foaf:workplace-Homepage to have a lower AIC G (p) since there are arbitrarily many values to be shared amongst the entities; given such an observation, we then surmise that a shared foaf:gender value represents a weaker ''indicator'' of concurrence than a shared value for foaf:workplaceHomepage.</p><p>Given that we deal with incomplete information under the Open World Assumption underlying RDF(S)/OWL, we also wish to weigh the average (inverse) cardinality values for properties with a low number of observations towards a global meanconsider a fictional property ex:maritalStatus for which we only encounter a few predicate-usages in a given graph, and consider two entities given the value "married": given sparse inverse-cardinality observations, we may naïvely over-estimate the significance of this property-object pair as an indicator for concurrence. Thus, we use a credibility formula as follows to weight properties with few observations towards a global mean, as follows.</p><p>Definition 4 (Adjusted average inverse-cardinality). Let p be a property appearing as a predicate in the graph G. The adjusted average inverse-cardinality (AAIC) of p with respect to G, written AAIC G (p), is then</p><formula xml:id="formula_6">AAIC G ðpÞ ¼ AIC G ðpÞÂ j G p ! j þAIC G Â j G ! j j G p ! j þ j G ! j<label>ð1Þ</label></formula><p>where jG p ! j is the number of distinct objects that appear in a triple with p as a predicate (the denominator of Definition 3), AIC G is the average inverse-cardinality for all predicate-object pairs (formally, AIC G ¼ jGj jfðp;oÞj9s:ðs;p;oÞ2Ggj ), and jG ? j is the average number of distinct objects for all predicates in the graph (formally, j G ! j ¼ jfðp;oÞj9s:ðs;p;oÞ2Ggj jfpj9s;9o:ðs;p;oÞ2Ggj Þ:</p><p>Again, the adjusted average cardinality AAC G (p) of a property p is defined analogously as for AAIC G (p). Some reduction of Eq. ( <ref type="formula" target="#formula_6">1</ref>) is possible if one considers that AIC G ðpÞ Â jG p ! j ¼ jfðs; oÞjðs; p; oÞ 2 Ggj also denotes the number of triples for which p appears as a predicate in graph G, and that AIC G Â j G !j ¼ jGj jfpj9s;9o:ðs;p;oÞ2Ggj also denotes the average number of triples per predicate. We maintain Eq.</p><p>(1) in the given unreduced form as it more clearly corresponds to the structure of a standard credibility formula: the reading (AIC G (p)) is dampened towards a mean (AIC G ) by a factor determined by the size of the sample used to derive the reading (jG p ! j) relative to the average sample size (jG ! j). Now, we move towards combining these metrics to determine the concurrence of entities who share a given non-empty set of property-value pairs. To do so, we combine the adjusted average (inverse) cardinality values, which apply generically to properties, and the (inverse) cardinality values, which apply to a given property-value pair. For example, take the property foaf:workplace-Homepage: entities that share a value referential to a large company-e.g., http://google.com/-should not gain as much concurrence as entities that share a value referential to a smaller company-e.g., http://deri.ie/. Conversely, consider a fictional property ex:citizenOf, which relates a citizen to its country, and for which we find many observations in our corpus, returning a high AAIC value, and consider that only two entities share the value ex:Vanuatu for this property: given that our data are incomplete, we can use the high AAIC value of ex:citizenOf to determine that the property is usually not exclusive, and that it is generally not a good indicator of concurrence. 28  We start by assigning a coefficient to each pair (p, o) and each pair (p, s) that occur in the dataset, where the coefficient is an indicator of how exclusive that pair is: Again, please note that these coefficients fall into the interval ]0, 1] since the denominator, by definition, is necessarily greater than one.</p><p>To take an example, let p wh = foaf:workplaceHomepage and say that we compute AAIC G (p wh )=7 from a large number of obser-vations, indicating that each workplace homepage in the graph G is linked to by, on average, seven employees. Further, let o g = http://google.com/ and assume that o g occurs 2000 times as a value for p wh : ICard G (p wh ,o g ) = 2000; now, IC G ðp wh ; o g Þ ¼ Finally, we require some means of aggregating the coefficients of the set of pairs that two entities share to derive the final concurrence measure.</p><p>Definition 6 (Aggregated concurrence score). Let Z = (z 1 , . . . z n ) be a tuple such that for each i = 1, . . . ,n, z i 2]0,1]. The aggregated concurrence value ACS n is computed iteratively: starting with ACS 0 = 0, then for each k = 1 . . . n, ACS k = z k + ACS kÀ1 À z k ⁄ACS kÀ1 .</p><p>The computation of the ACS value is the same process as determining the probability of two independent events occurring-P(A _ B) = P(A) + P(B) À P(A⁄B)-which is by definition commutative and associative, and thus computation is independent of the order of the elements in the tuple. It may be more accurate to view the coefficients as fuzzy values, and the aggregation function as a disjunctive combination in some extensions of fuzzy logic <ref type="bibr" target="#b74">[75]</ref>.</p><p>However, the underlying coefficients may not be derived from strictly independent phenomena: there may indeed be correlation between the property-value pairs that two entities share. To illustrate, we reintroduce a relevant example from <ref type="bibr" target="#b33">[34]</ref> shown in Fig. <ref type="figure">8</ref>, where we see two researchers that have co-authored many papers together, have the same affiliation, and are based in the same country.</p><p>This example illustrates three categories of concurrence correlation:</p><p>(i) same-value correlation where two entities may be linked to the same value by multiple predicates in either direction (e.g., foaf: made, dc:creator, swrc:author, foaf:maker); (ii) intra-property correlation where two entities that share a given property-value pair are likely to share further values for the same property (e.g., co-authors sharing one value for foaf:made are more likely to share further values); (iii) inter-property correlation where two entities sharing a given property-value pair are likely to share further distinct but related property-value pairs (e.g., having the same value for swrc:affiliation and foaf:based_near).</p><p>Ideally, we would like to reflect such correlation in the computation of the concurrence between the two entities.</p><p>Regarding same-value correlation, for a value with multiple edges shared between two entities, we choose the shared predicate Example of same-value, inter-property and intra-property correlation, where the two entities under comparison are highlighted in the dashed box, and where the labels of inward-edges (with respect to the principal entities) are italicised and underlined (from <ref type="bibr" target="#b33">[34]</ref>). 28 Here, we try to distinguish between property-value pairs that are exclusive in reality (i.e., on the level of what's signified) and those that are exclusive in the given graph. Admittedly, one could think of counter-examples where not including the general statistics of the property may yield a better indication of weighted concurrence, particularly for generic properties that can be applied in many contexts; for example, consider the exclusive predicate-object pair (skos: subject, category: Koenigsegg_vehicles) given for a non-exclusive property.</p><p>edge with the lowest AA[I]C value and disregard the other edges: i.e., we only consider the most exclusive property used by both entities to link to the given value and prune the other edges.</p><p>Regarding intra-property correlation, we apply a lower-level aggregation for each predicate in the set of shared predicate-value pairs. Instead of aggregating a single tuple of coefficients, we generate a bag of tuples Z ¼ fZ p 1 ; . . . ; Z p n g, where each element Z p i represents the tuple of (non-pruned) coefficients generated for the predicate p i . <ref type="foot" target="#foot_16">29</ref> We then aggregate this bag as follows:</p><p>where AA[I]C is either AAC or AAIC, dependant on the directionality of the predicate-value pair observed. Thus, the total contribution possible through a given predicate (e.g., foaf:made) has an upper-bound set as its AA[I]C value, where each successive shared value for that predicate (e.g., each successive co-authored paper) contributes positively (but increasingly less) to the overall concurrence measure.</p><p>Detecting and counteracting inter-property correlation is perhaps more difficult, and we leave this as an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">Implementing entity-concurrence analysis</head><p>We aim to implement the above methods using sorts and scans, and we wish to avoid any form of complex indexing, or pair-wise comparison. First, we wish to extract the statistics relating to the (inverse-)cardinalities of the predicates in the data. Given that there are 23 thousand unique predicates found in the input corpus, we assume that we can fit the list of predicates and their associated statistics in memory-if such were not the case, one could consider an on-disk map, where we would expect a high cache hit-rate based on the distribution of property occurrences in the data (cf. <ref type="bibr" target="#b31">[32]</ref>).</p><p>Moving forward, we can calculate the necessary predicate-level statistics by first sorting the data according to natural order (s, p, o, c), and then scanning the data, computing the cardinality (number of distinct objects) for each (s, p) pair, and maintaining the average cardinality for each p found. For inverse-cardinality scores, we apply the same process, sorting instead by (o, p, s, c) order, counting the number of distinct subjects for each (p, o) pair, and maintaining the average inverse-cardinality scores for each p. After each scan, the statistics of the properties are adjusted according to the credibility formula in Eq. ( <ref type="formula">4</ref>).</p><p>We then apply a second scan of the sorted corpus; first we scan the data sorted in natural order, and for each (s,p) pair, for each set of unique objects O ps found thereon, and for each pair in</p><formula xml:id="formula_7">fðo i ; o j Þ 2 U [ B Â U [ B j o i ; o j 2 O ps ; o i &lt; o j g</formula><p>where &lt; denotes lexicographical order, we output the following sextuple to an on-disk file: . We apply the same process for the other direction: for each (p, o) pair, for each set of unique subjects S po , and for each pair in</p><formula xml:id="formula_8">fðs i ; s j Þ 2 U [ B Â U [ B j s i ; s j 2 S po ; s i &lt; s j g</formula><p>we output analogous sextuples of the form: ðs i ; s j ; ICðp; oÞ; p; o; þÞ We call the sets O ps and their analogues S po concurrence classes, denoting sets of entities that share the given predicate-subject/ predicate-object pair, respectively. Here, note that the ' + ' and ' À ' elements simply mark and track the directionality from which the tuple was generated, required for the final aggregation of the co-efficient scores. Similarly, we do not immediately materialise the symmetric concurrence scores, where we instead do so at the end so as to forego duplication of intermediary processing.</p><p>Once generated, we can sort the two files of tuples by their natural order, and perform a merge-join on the first two elementsgeneralising the directional o i /s i to simply e i , each (e i , e j ) pair denotes two entities that share some predicate-value pairs in common, where we can scan the sorted data and aggregate the final concurrence measure for each (e i , e j ) pair using the information encoded in the respective tuples. We can thus generate (trivially sorted) tuples of the form (e i , e j , s), where s denotes the final aggregated concurrence score computed for the two entities; optionally, we can also write the symmetric concurrence tuples (e j , e i , s), which can be sorted separately as required.</p><p>Note that the number of tuples generated is quadratic with respect to the size of the respective concurrence class, which becomes a major impediment for scalability given the presence of large such sets-for example, consider a corpus containing 1 million persons sharing the value "female" for the property foaf: gender, where we would have to generate 10 6Â2 À10 6 2 % 500 billion non-reflexive, non-symmetric concurrence tuples. However, we can leverage the fact that such sets can only invoke a minor influence on the final concurrence of their elements, given that the magnitude of the set-e.g., jS po j-is a factor in the denominator of the computed C(p, o) score, such that Cðp; oÞ / 1 jSopj . Thus, in practice, we implement a maximum-size threshold for the S po and O ps concurrence classes: this threshold is selected based on a practical upper limit for raw similarity tuples to be generated, where the appropriate maximum class size can trivially be determined alongside the derivation of the predicate statistics. For the purpose of evaluation, we choose to keep the number of raw tuples generated at around $1 billion, and so set the maximum concurrence class size at 38-we will see more in Section 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Distributed implementation</head><p>Given the previous discussion, our distributed implementation is fairly straight-forward as follows:</p><p>(i) Coordinate: the slave machines split their segment of the corpus according to a modulo-hash function on the subject position of the data, sort the segments, and send the split segments to the peer determined by the hash-function; the slaves simultaneously gather incoming sorted segments, and subsequently perform a merge-sort of the segments; (ii) Coordinate: the slave machines apply the same operation, this time hashing on object-triples with rdf: type as predicate are not included in the object-hashing; subsequently the slaves merge-sort the segments ordered by object; (iii) Run: the slave machines then extract predicate-level statistics, and statistics relating to the concurrence-class-size distribution, which are used to decide upon the class size threshold; (iv) Gather/flood/run: the master machine gathers and aggregates the high-level statistics generated by the slave machines in the previous step and sends a copy of the global statistics back to each machine; the slaves subsequently generate the raw concurrence-encoding sextuples as described before from a scan of the data in both orders; (v) Coordinate: the slave machines coordinate the locally generated sextuples according to the first element (join position) as before; (vi) Run: the slave machines aggregate the sextuples coordinated in the previous step, and produce the final non-symmetric concurrence tuples;</p><p>(vii) Run: the slave machines produce the symmetric version of the concurrence tuples, and coordinate and sort on the first element.</p><p>Here, we make heavy use of the coordinate function to align data according to the join position required for the subsequent processing step-in particular, aligning the raw data by subject and object, and then the concurrence tuples analogously.</p><p>Note that we do not hash on the object position of rdf: type triples: our raw corpus contains 206.8 million such triples, and given the distribution of class memberships, we assume that hashing these values will lead to uneven distribution of data, and subsequently uneven load balancing-e.g., 79.2% of all class memberships are for foaf:Person, hashing on which would send 163.7 million triples to one machine, which alone is greater than the average number of triples we would expect per machine (139.8 million). In any case, given that our corpus contains 105 thousand unique values for rdf: type, we would expect the average-inverse-cardinality to be approximately 1970-even for classes with two members, the potential effect on concurrence is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Performance evaluation</head><p>We apply our concurrence analysis over the consolidated corpus derived in Section 5. The total time taken was 13.9 h. Sorting, splitting and scattering the data according to subject on the slave machines took 3.06 h, with an average idle time of 7.7 min (4.2%). Subsequently, merge-sorting the sorted segments took 1.13 h, with an average idle time of 5.4 min (8%). Analogously sorting, splitting and scattering the non-rdf:type statements by object took 2.93 h, with an average idle time of 11.8 min (6.7%). Merge sorting the data by object took 0.99 h, with an average idle time of 3.8 min (6.3%). Extracting the predicate statistics and threshold information from data sorted in both orders took 29 min, with an average idle time of 0.6 min (2.1%). Generating the raw, unsorted similarity tuples took 69.8 min with an average idle time of 2.1 min (3%). Sorting and coordinating the raw similarity tuples across the machines took 180.1 min, with an average idle time of 14.1 min (7.8%). Aggregating the final similarity took 67.8 min, with an average idle time of 1.2 min (1.8%).</p><p>Table <ref type="table" target="#tab_15">12</ref> presents a breakdown of the timing of the task. First, with regards application over the full corpus, although this task requires some aggregation of global-knowledge by the master machine, the volume of data involved is minimal: a total of 2.1 minutes is spent on the master machine performing various minor tasks (initialisation, remote calls, logging, aggregation and broadcast of statistics). Thus, 99.7% of the task is performed in parallel on the slave machine. Although there is less time spent waiting for the master machine compared to the previous two tasks, deriving the concurrence measures involves three expensive sort/coordinate/merge-sort operations to redistribute and sort the data over the slave swarm. The slave machines were idle for, on average, 5.8% of the total task time; most of this idle time (99.6%) was spent waiting for peers. The master machine was idle for almost the entire task, with 99.7% waiting for the slave machines to finish their tasks-again, interleaving a job for another task would have practical benefits.</p><p>Second, with regards the timing of tasks when varying the number of slave machines for 100 million quadruples, we see that the percentage of time spent idle on the slave machines increases from 0.4% (1) to 9% <ref type="bibr" target="#b7">(8)</ref>. However, for each incremental doubling of the number of slave machines, the total overall task times are reduced by 0.509Â , 0.525Â and 0.517Â, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Results evaluation</head><p>With respect to data distribution, after hashing on subject we observed an average absolute deviation (average distance from the mean) of 176 thousand triples across the slave machines, representing an average 0.13% deviation from the mean: near-optimal data distribution. After hashing on the object of non-rdf:type triples, we observed an average absolute deviation of 1.29 million triples across the machines, representing an average 1.1% deviation from the mean; in particular, we note that one machine was assigned 3.7 million triples above the mean (an additional 3.3% above the mean). Although not optimal, the percentage of data deviation given by hashing on object is still within the natural variation in run-times we have seen for the slave machines during most parallel tasks.</p><p>In Fig. <ref type="figure" target="#fig_6">9a</ref> and<ref type="figure">b</ref>, we illustrate the effect of including increasingly large concurrence classes on the number of raw concurrence tuples generated. For the predicate-object pairs, we observe a powerlaw(-esque) relationship between the size of the concurrence class and the number of such classes observed. Second, we observe that the number of concurrences generated for each increasing class size initially remains fairly static-i.e., larger class sizes give quadratically more concurrences, but occur polynomially less often-until the point where the largest classes that generally only have one occurrence is reached, and the number of concurrences begins to increase quadratically. Also shown is the cumulative count of concurrence tuples generated for increasing class sizes, where we initially see a power-law(-esque) correlation, which subsequently begins to flatten as the larger concurrence classes become more sparse (although more massive).</p><p>For the predicate-subject pairs, the same roughly holds true, although we see fewer of the very largest concurrence classes: the largest concurrence class given by a predicate-subject pair was 79 thousand, versus 1.9 million for the largest predicate-object pair, respectively, given by the pairs (kwa:map, macs:man-ual_rameau_lcsh) and (opiumfield:rating, ""). Also, we observe some ''noise'' where for milestone concurrence class sizes (esp., at 50, 100, 1000, 2000) we observe an unusual amount of classes. For example, there were 72 thousand concurrence classes of precisely size 1000 (versus 88 concurrence classes at size 996)-the 1000 limit was due to a FOAF exporter from the hi5.com domain, which seemingly enforces that limit on the total ''friends count'' of users, translating into many users with precisely 1000 values for foaf:knows. 30 Also for example, there were 5.5 thousand classes of size 2000 (versus 6 classes of size 1999)-almost all of these were due to an exporter from the bio2rdf.org domain, which puts this limit on values for the b2r:linkedToFrom property. 31 We also encountered unusually large numbers of classes approximating these milestones, such as 73 at 2001. Such phenomena explain the staggered ''spikes''and ''discontinuities'' in Fig. <ref type="figure" target="#fig_6">9b</ref>, which can be observed to correlate with such milestone values (in fact, similar but less noticeable spikes are also present in Fig. <ref type="figure" target="#fig_6">9a</ref>).</p><p>These figures allow us to choose a threshold of concurrenceclass size given an upper bound on raw concurrence tuples to generate. For the purposes of evaluation, we choose to keep the number of materialised concurrence tuples at around 1 billion, which limits our maximum concurrence class size to 38 (from which we produce 1.024 billion tuples: 721 million through shared (p, o) pairs and 303 million through (p, s) pairs).</p><p>With respect to the statistics of predicates, for the predicatesubject pairs, each predicate had an average of 25,229 unique objects for 37,953 total triples, giving an average cardinality of $1.5. We give the five predicates observed to have the lowest ad-justed average cardinality in Table <ref type="table" target="#tab_16">13</ref>. These predicates are judged by the algorithm to be the most selective for identifying their subject with a given object value (i.e., quasi-inverse-functional); note that the bottom two predicates will not generate any concurrence scores since they are perfectly unique to a given object (i.e., the number of triples equals the number of objects such that no two entities can share a value for this predicate). For the predicate-object pairs, there was an average of 11,572 subjects for 20,532 triples, giving an average inverse-cardinality of $2.64; We analogously give the five predicates observed to have the lowest adjusted average inverse cardinality in Table <ref type="table" target="#tab_5">14</ref>. These predicates are judged to be the most selective for identifying their object with a given subject value (i.e., quasi-functional); however, four of these predicates will not generate any concurrences since they are perfectly unique to a given subject (i.e., those where the number of triples equals the number of subjects).</p><p>Aggregation produced a final total of 636.9 million weighted concurrence pairs, with a mean concurrence weight of $0.0159. Of these pairs, 19.5 million involved a pair of identifiers from different PLDs (3.1%), whereas 617.4 million involved identifiers from the same PLD; however, the average concurrence value for an intra-PLD pair was 0.446, versus 0.002 for inter-PLD pairs-although 2 ); a cumulative count of tuples generated for increasing class sizes ( c sp i ¼ P j6i sp j ); and our cut-off (s i = 38) that we have chosen to keep the total number of tuples at $ 1 billion (log/log) 30 cf. http://api.hi5.com/rest/profile/foaf/100614697. 31 cf. http://bio2rdf.org/mesh:D000123Q000235. fewer intra-PLD concurrences are found, they typically have higher concurrences. 32  In Table <ref type="table" target="#tab_17">15</ref>, we give the labels of top five most concurrent entities, including the number of pairs they share-the concurrence score for each of these pairs was &gt;0.9999999. We note that they are all locations, where particularly on WIKIPEDIA (and thus filtering through to DBpedia), properties with location values are typically duplicated (e.g., dbp:deathPlace, dbp:birthPlace, dbp:headquarters-properties that are quasi-functional); for example, New York City and New York State are both the dbp:death-Place of dbpedia:Isacc_Asimov, etc.</p><p>In Table <ref type="table" target="#tab_18">16</ref>, we give a description of the concurrent entities found for the top-five ranked entities-for brevity, again we show entity labels. In particular, we note that a large amount of concurrent entities are identified for the highly-ranked persons. With respect to the strongest concurrences: (i) Tim and his former student Lalana share twelve primarily academic links, coauthoring six papers; (ii) Dan and Libby, co-founders of the FOAF project, share 87 links, primarily 73 foaf:knows relations to and from the same people, as well as a co-authored paper, occupying the same professional positions, etc. 33 ; (iii) update.status.net and socialnetwork.ro share a single foaf:accountServiceHomepage link from a common user; (iv) similarly, the FOAF-a-matic and foaf.me services share a single mvcb:generatorAgent inlink; (v) finally, Evan and Stav share 69 foaf:knows inlinks and outlinks exported from the identi.ca service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Entity disambiguation and repair</head><p>We have already seen that-even by only exploiting the formal logical consequences of the data through reasoning-consolidation may already be imprecise due to various forms of noise inherent in Linked Data. In this section, we look at trying to detect erroneous coreferences as produced by the extended consolidation approach introduced in Section 5. Ideally, we would like to automatically detect, revise, and repair such cases to improve the effective precision of the consolidation process. As discussed in Section 2.6, OWL 2 RL/ RDF contains rules for automatically detecting inconsistencies in RDF data, representing formal contradictions according to OWL semantics. Where such contradictions are created due to consolidation, we believe this to be a good indicator of erroneous consolidation.</p><p>Once erroneous equivalences have been detected, we would like to subsequently diagnose and repair the coreferences involved. One option would be to completely disband the entire equivalence class; however, there may often be only one problematic identifier that, e.g., causes inconsistency in a large equivalence class-breaking up all equivalences would be a coarse solution. Instead, herein we propose a more fine-grained method for repairing equivalence classes that incrementally rebuilds coreferences in the set in a manner that preserves consistency and that is based on the original evidences for the equivalences found, as well as concurrence scores (discussed in the previous section) to indicate how similar the original entities are. Once the erroneous equivalence classes have been repaired, we can then revise the consolidated corpus to reflect the changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">High-level approach</head><p>The high-level approach is to see if the consolidation of any entities conducted in Section 5 lead to any novel inconsistencies, and subsequently recant the equivalences involved; thus, it is important to note that our aim is not to repair inconsistencies in the data, but instead to repair incorrect consolidation symptomised by inconsistency. 34 In this section, we (i) describe what forms of inconsistency we detect and how we detect them; (ii) characterise how inconsistencies can be caused by consolidation using examples from our corpus where possible; (iii) discuss the repair of equivalence classes that have been determined to cause inconsistency.</p><p>First, in order to track which data are consolidated and which not, in the previous consolidation step we output sextuples of the form: ðs; p; o; c; s 0 ; o 0 Þ where s, p, o, c, denote the consolidated quadruple containing canonical identifiers in the subject/object position as appropriate, and s 0 and o 0 denote the input identifiers prior to consolidation. 35  To detect inconsistencies in the consolidated corpus, we use the OWL 2 RL/RDF rules with the false consequent <ref type="bibr" target="#b23">[24]</ref> as listed in Table <ref type="table" target="#tab_5">4</ref>. A quick check of our corpus revealed that one document provides eight owl:AsymmetricProperty and 10 owl:Irre-flexiveProperty axioms, 36 and one directory gives 9 owl:All-DisjointClasses axioms, and where we no other OWL 2 axioms relevant to the rules in Table <ref type="table" target="#tab_5">4</ref>.</p><p>We also consider an additional rule, whose are indirectly by the OWL 2 RL/RDF rules (through prp-fp, dtdiff and eq-diff1), but which we must support directly since we do not consider consolidation of literals: ?p a owl:FunctionalProperty. ?x ?p ?l 1 , ?l 2 . ?l 1 owl:differentFrom ?l 2 . )false Philadelphia Pennsylvania 217 32 Note that we apply this analysis over the consolidated data, and thus this is an approximative reading for the purposes of illustration: we extract the PLDs from canonical identifiers, which are choosen based on arbitrary lexical ordering.  33 Notably, Leigh Dodds (creator of the FOAF-a-matic service) is linked by the property quaffing: drankBeerWith to both. 34 We instead refer the interested reader to <ref type="bibr" target="#b8">[9]</ref> for some previous works on the topic of general inconsistency repair for Linked Data. 35 We use syntactic shortcuts in our file to denote when s = s 0 and/or o = o 0 .</p><p>Maintaining the additional rewrite information during the consolidation process is trivial, where the output of consolidating subjects gives quintuples (s, p, o, c, s 0 ), which are then sorted and consolidated by o to produce the given sextuples. 36 http://models.okkam.org/ENS-core-vocabulary#country_of_residence. 37 http://ontologydesignpatterns.org/cp/owl/fsdas/.</p><p>where we underline the terminological pattern. To illustrate, we take an example from our corpus:</p><p># Terminological [http://dbpedia.org/data3/ length.rdf] dpo:length rdf:type owl:FunctionalProperty # Assertional [http://dbpedia.org/data/ Fiat_Nuova_500.xml] dbpedia:Fiat_Nuova_500 0 dpo:length "3.546" ^^xsd:double # Assertional [http://dbpedia.org/data/ Fiat_500.xml] dbpedia:Fiat_Nuova 0 dpo:length "2.97" ^^xsd:double where we use the prime symbol [ 0 ] to denote identifiers considered coreferent by consolidation. Here we see two very closely related models of cars consolidated in the previous step, but we now identify that they have two different values for dpo:length-a functional-property-and thus the consolidation raises an inconsistency.</p><p>Note that we do not expect the owl:differentFrom assertion to be materialised, but instead intend a rather more relaxed semantics based on a heurisitic comparison: given two (distinct) literal bindings for ?l 1 and ?l 2 , we flag an inconsistency iff (i) the data values of the two bindings are not equal (standard OWL semantics); and (ii) their lower-case string value (minus language-tags and datatypes) are lexically unequal. In particular, the relaxation is inspired by the definition of the FOAF (datatype) functional properties foaf:age, foaf:gender, and foaf:birthday, where the range of these properties is rather loosely defined: a generic range of rdfs:Literal is formally defined for these properties, with informal recommendations to use male/female as gender values, and MM-DD syntax for birthdays, but not giving recommendations for datatype or language-tags. The latter relaxation means that we would not flag an inconsistency in the following data:</p><p># Terminological [http://xmlns.com/foaf/spec/index.rdf] foaf:Person owl:disjointWith foaf:Document # Assertional [fictional] ex:Ted foaf:age 25 ex:Ted foaf:age ''25'' ex:Ted foaf:gender ''male'' ex:Ted foaf:gender ''Male''@en ex:Ted foaf:birthday ''25-05'' ^^xsd:gMonthDay ex:Ted foaf:birthday ''25-05'' With respect to these consistency checking rules, we consider the terminological data to be sound. <ref type="foot" target="#foot_17">38</ref> We again only consider terminological axioms that are authoritatively served by their source; for example, the following statement: sioc: User owl:disjointWith foaf: Person.</p><p>would have to be served by a document that either sioc:User or foaf:Person dereferences to (either the FOAF or SIOC vocabulary since the axiom applies over a combination of FOAF and SIOC assertional data).</p><p>Given a grounding for such an inconsistency-detection rule, we wish to analyse the constants bound by variables in join positions to determine whether or not the contradiction is caused by consolidation; we are thus only interested in join variables that appear at least once in a consolidatable position (thus, we do not support dt-not-type) and where the join variable is ''intra-assertional'' (exists twice in the assertional patterns). Other forms of inconsistency must otherwise be present in the raw data.</p><p>Further note that owl:sameAs patterns-particularly in rule eq-diff1-are implicit in the consolidated data; e.g., consider:</p><p># Assertional [http://www.wikier.org/foaf.rdf] wikier:wikier 0 owl:differentFrom eswc2006p:sergio-fernandez'.</p><p>where an inconsistency is implicitly given by the owl:sameAs relation that holds between the consolidated identifiers wikier:wikier' and eswc2006p:sergio-fernandez'. In this example, there are two Semantic Web researchers, respectively named ''Sergio Fernández'' <ref type="foot" target="#foot_18">39</ref> and ''Sergio Fernández Anzuola'' <ref type="foot" target="#foot_19">40</ref> , who both participated in the ESWC 2006 conference, and who were subsequently conflated in the ''DogFood'' export. <ref type="foot" target="#foot_20">41</ref> The former Sergio subsequently added a counter-claim in his FOAF file, asserting the above owl:dif-ferentFrom statement. Other inconsistencies do not involve explicit owl:sameAs patterns, a subset of which may require ''positive'' reasoning to be detected; e.g.:</p><p># Terminological [http://xmlns.com/foaf/spec] foaf:Person owl:disjointWith foaf:Organization foaf:knows rdfs:domain foaf:Person # Assertional [http://identi.ca/w3c/foaf] identica:48404 0 foaf:knows identica:45563 # Assertional [inferred by prp-dom] identica:48404 0 a foaf:Person # Assertional [http://www.data.semanticweb.org/ organization/w3c/rdf] semweborg:w3c 0 a foaf:Organization where the two entities are initially consolidated due to sharing the value http://www.w3.org/ for the inverse-functional property foaf:homepage; the W3C is stated to be a foaf:Organization in one document, and is inferred to be a person from its identi.ca profile through rule prp-dom; finally, the W3C is a member of two disjoint classes, forming an inconsistency detectable by rule caxdw. <ref type="foot" target="#foot_21">42</ref>Once the inconsistencies caused by consolidation have been identified, we need to perform a repair of the equivalence class involved. In order to resolve inconsistencies, we make three simplifying assumptions:</p><p>(i) the steps involved in the consolidation can be rederived with knowledge of direct inlinks and outlinks of the consolidated entity, or reasoned knowledge derived from there; (ii) inconsistencies are caused by pairs of consolidated identifiers; (iii) we repair individual equivalence classes and do not consider the case where repairing one such class may indirectly repair another.</p><p>With respect to the first item, our current implementation will be performing a repair of the equivalence class based on knowledge of direct inlinks and outlinks, available through a simple merge-join as used in the previous section; this thus precludes repair of consolidation found through rule cls-maxqc2, which also requires knowledge about the class memberships of the outlinked node. With respect to the second item, we say that inconsistencies are caused by pairs of identifiers-what we term incompatible identifiers-such that we do not consider inconsistencies caused with respect to a single identifier (inconsistencies not caused by consolidation) and do not consider the case where the alignment of more than two identifiers are required to cause a single inconsistency (not possible in our rules) where such a case would again lead to a disjunction of repair strategies. With respect to the third item, it is possible to resolve a set of inconsistent equivalence classes by repairing one; for example, consider rules with multiple ''intra-assertional'' join-variables (prp-irp, prp-asyp) that can have explanations involving multiple consolidated identifiers as follows:</p><p># Terminological [fictional] :made owl:propertyDisjointWith :maker # Assertional [fictional] ex:AZ 00 :maker ex:entcons 0 dblp:Antoine_Zimmermann 00 :made dblp:HoganZUPD15 0 where both equivalences together constitute an inconsistency. Repairing one equivalence class would repair the inconsistency detected for both: we give no special treatment to such a case, and resolve each equivalence class independently. In any case, we find no such incidences in our corpus: these inconsistencies require (i) axioms new in OWL 2 (rules prp-irp, prp-asyp, prppdw and prp-adp); (ii) alignment of two consolidated sets of identifiers in the subject/object positions. Note that such cases can also occur given the recursive nature of our consolidation, whereby consolidating one set of identifiers may lead to alignments in the join positions of the consolidation rules in the next iteration; however, we did not encounter such recursion during the consolidation phase for our data.</p><p>The high-level approach to repairing inconsistent consolidation is as follows:  <ref type="figure">d,</ref><ref type="figure">c</ref>) where d is the number of sets of input triples in the corpus that allow to directly derive the given equivalence relation by means of a direct owl:sameAs assertion (in either direction), or a shared inverse-functional object, or functional subject-loosely, the independent evidences for the relation given by the input graph, excluding transitive owl:sameAs semantics; c is the concurrence score derivable between the unconsolidated entities and is used to resolve ties (we would expect many strongly connected equivalence graphs where, e.g., the entire equivalence class is given by a single shared value for a given inverse-functional property, and we thus require the additional granularity of concurrence for repairing the data in a non-trivial manner). We define a total lexicographical order over these pairs.</p><p>Given an equivalence class Eq &amp; U [ B that we perceive to cause a novel inconsistency-i.e., an inconsistency derivable by the alignment of incompatible identifiers-we first derive a collection of sets C = {C 1 , . . . ,C n }, C &amp; 2 U[B , such that each C i 2 C, C i # Eq denotes an unordered pair of incompatible identifiers.</p><p>We then apply a straightforward, greedy consistent clustering of the equivalence class, loosely following the notion of a minimal cutting (see, e.g., <ref type="bibr" target="#b62">[63]</ref>). For Eq, we create an initial set of singleton sets Eq 0 , each containing an individual identifier in the equivalence class. Now let U(E i , E j ) denote the aggregated weight of the edge considering the merge of the nodes of E i and the nodes of E j in the graph: the pair (d,c) such that d denotes the unique evidences for equivalence relations between all nodes in E i and all nodes in E j and such that c denotes the concurrence score considering the merge of entities in E i and E j -intuitively, the same weight as before, but applied as if the identifiers in E i and E j were consolidated in the graph. We can apply the following clustering:</p><p>-for each pair of sets E i , E j 2 Eq n such that 9 = {a,b} 2 C:a 2 Eq i ,b 2</p><p>Eq j (i.e., consistently mergeable subsets) identify the weights of U(E i , E j ) and order the pairings;</p><p>-in descending order with respect to the above weights, merge E i , E j pairs-such that neither E i or E j have already been merged in this iteration-producing E n+1 at iteration's end; -iterate over n until fixpoint. Thus, we determine the pairs of incompatible identifiers that must necessarily be in different repaired equivalence classes, deconstruct the equivalence class, and then begin reconstructing the repaired equivalence class by iteratively merging the most strongly linked intermediary equivalence classes that will not contain incompatible identifers. 43   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Implementing disambiguation</head><p>The implementation of the above disambiguation process can be viewed on two levels: the macro level, which identifies and collates the information about individual equivalence classes and their respectively consolidated inlinks/outlinks, and the micro level, which repairs individual equivalence classes.</p><p>On the macro level, the task assumes input data sorted by both subject (s, p, o, c, s 0 ,o 0 ) and object (o, p, s, c, o 0 , s 0 ), again such that s,o represent canonical identifiers and s 0 , o 0 represent the original identifiers as before. Note that we also require the asserted owl:sameAs relations encoded likewise. Given that all the required information about the equivalence classes (their inlinks, outlinks, derivable equivalences and original identifiers) are gathered under the canonical identifiers, we can apply a straight-forward merge-join on s-o over the sorted stream of data and batch consolidated segments of data.</p><p>On a micro level, we buffer each individual consolidated segment into an in-memory index; currently, these segments fit in 43 We note the possibility of a dual correspondence between our ''bottom-up'' approach to repair and the ''top-down'' minimal hitting set techniques introduced by Reiter <ref type="bibr" target="#b54">[55]</ref>. memory, where for the largest equivalence classes we note that inlinks/outlinks are commonly duplicated-if this were not the case, one could consider using an on-disk index, which should be feasible given that only small batches of the corpus are under analysis at each given time. We assume access to the relevant terminological knowledge required for reasoning, and the predicate-level statistics derived during from the concurrence analysis. We apply scan-reasoning and inconsistency detection over each batch, and for efficiency, skip over batches that are not symptomised by incompatible identifiers.</p><p>For equivalence classes containing incompatible identifiers, we first determine the full set of such pairs through application of the inconsistency detection rules: usually, each detection gives a single pair, where we ignore pairs containing the same identifier (i.e., detections that would equally apply over the unconsolidated data). We check the pairs for a trivial solution: if all identifiers in the equivalence class appear in some pair, we check whether the graph formed by the pairs is strongly connected, in which case, the equivalence class must necessarily be completely disbanded.</p><p>For non-trivial repairs, we extract the explicit owl:sameAs relations (which we view as directionless) and reinfer owl:sameAs relations from the consolidation rules, encoding the subsequent graph. We label edges in the graph with a set of hashes denoting the input triples required for their derivation, such that the cardinality of the hashset corresponds to the primary edge weight. We subsequently use a priority-queue to order the edge-weights, and only materialise concurrence scores in the case of a tie. Nodes in the equivalence graph are merged by combining unique edges and merging the hashsets for overlapping edges. Using these operations, we can apply the aformentioned process to derive the final repaired equivalence classes.</p><p>In the final step, we encode the repaired equivalence classes in memory, and perform a final scan of the corpus (in natural sorted order), revising identifiers according to their repaired canonical term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Distributed implementation</head><p>Distribution of the task becomes straight-forward, assuming that the slave machines have knowledge of terminological data, predicate-level statistics, and already have the consolidation encoding sextuples sorted and coordinated by hash on s and o. Note that all of these data are present on the slave machines from previous tasks; for the concurrence analysis, we in fact maintain sextuples during the data preparation phase (although not required by the analysis).</p><p>Thus, we are left with two steps:</p><p>-Run: each slave machine performs the above process on its segment of the corpus, applying a merge-join over the data sorted by (s, p, o, c, s 0 , o 0 ) and (o, p, s, c, o 0 , s 0 ) to derive batches of consolidated data, which are subsequently analysed, diagnosed, and a repair derived in memory; -Gather/run: the master machine gathers all repair information from all slave machines, and floods the merged repairs to the slave machines; the slave machines subsequently perform the final repair of the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Performance evaluation</head><p>We ran the inconsistency detection and disambiguation over the corpora produced by the extended consolidation approach. For the full corpus, the total time taken was 2.35 h. The inconsistency extraction and equivalence class repair analysis took 1.2 h, with a significant average idle time of 7.5 min (9.3%): in particular, certain large batches of consolidated data took significant amounts of time to process, particularly to reason over. The additional expense is due to the relaxation of duplicate detection: we cannot consider duplicates on a triple level, but must consider uniqueness based on the entire sextuple to derive the information required for repair; we must apply many duplicate inferencing steps. On the other hand, we can skip certain reasoning paths that cannot lead to inconsistency. Repairing the corpus took 0.98 h, with an average idle time of 2.7 min.</p><p>In Table <ref type="table" target="#tab_19">17</ref>, we again give a detailed breakdown of the timings for the task. With regards the full corpus, note that the aggregation of the repair information took a negligible amount of time, and where only a total of one minute is spent on the slave machine. Most notably, load-balancing is somewhat of an issue, causing slave machines to be idle for, on average, 7.2% of the total task time, mostly waiting for peers. This percentage-and the associated load-balancing issues-would likely be aggrevated further by more machines or a higher scale of data.</p><p>With regards the timings of tasks when varying the number of slave machines, as the number of slave machines doubles, the total execution times decrease by factors of 0.534 Â , 0.599Â and 0.649Â, respectively. Unlike for previous tasks where the aggregation of global knowledge on the master machine poses a bottleneck, this time load-balancing for the inconsistency detection and repair selection task sees overall performance start to converge when increasing machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Results evaluation</head><p>It seems that our discussion of inconsistency repair has been somewhat academic: from the total of 2.82 million consolidated In terms of repairing these 280 equivalence classes, 29 (10.4%) had to be completely disbanded since all identifiers were pairwise incompatible. A total of 905 partitions were created during the repair, with each of the 280 classes being broken up into an average of 3.23 repaired, consistent partitions. Fig. <ref type="figure" target="#fig_8">10</ref> gives the distribution of the number of repaired partitions created for each equivalence class; 230 classes were broken into the minimum of two partitions, and one class was broken into 96 partitions. Fig. <ref type="figure" target="#fig_9">11</ref> gives the distribution of the number of identfiers in each repaired partition; each partition contained an average of 4.4 equivalent identifiers, with 577 identifiers in singleton partitions, and one partition containing 182 identifiers.</p><p>Finally, although the repaired partitions no longer cause inconsistency, this does not necessarily mean that they are correct. From the raw equivalence classes identified to cause inconsistency, we applied the same sampling technique as before: we extracted 503 identifiers and for each, randomly sampled an identifier originally thought to be equivalent. We then manually checked whether or not we considered the identifiers to refer to the same entity or not. In the (blind) manual evaluation, we selected option UNCLEAR 19 times, option SAME 232 times (of which, 49 were TRIVIALLY SAME) and option DIFFERENT 249 times. 44 We checked our manually annotated results against the partitions produced by our repair to see if they corresponded. The results are enumerated in Table <ref type="table" target="#tab_4">20</ref>. Of the 481 (clear) manually-inspected pairs, 361 (72.1%) remained equivalent after the repair, and 123 (25.4%) were separated. Of the 223 pairs manually annotated as SAME, 205 (91.9%) also remained equivalent in the repair, whereas 18 (18.1%) were deemed different; here we see that the repair has a 91.9% recall when reesatablishing equivalence for resources that are the same. Of the 261 pairs verified as DIFFERENT, 105 (40.2%) were also deemed different in the repair, whereas 156 (59.7%) were deemed to be equivalent.</p><p>Overall, for identifying which resources were the same and should be re-aligned during the repair, the precision was 0.568, with a recall of 0.919, and F 1 -measure of 0.702. Conversely, the precision for identifying which resources were different and should be kept separate during the repair, the precision was 0.854, with a recall of 0.402, and F 1 -measure of 0.546.</p><p>Interpreting and inspecting the results, we found that the    44 We were particularly careful to distinguish information resources-such as WIKIPEDIA articles-and non-information resources as being DIFFERENT.</p><p>inconsistency-based repair process works well for correctly fixing equivalence classes with one or two ''bad apples''. However, for equivalence classes which contain many broken resources, we found that the repair process tends towards re-aligning too many identifiers: although the output partitions are consistent, they are not correct. For example, we found that 30 of the pairs which were annotated as different but were re-consolidated by the repair were from the opera.com domain, where users had specified various nonsense values which were exported as values for inversefunctional chat-ID properties in FOAF (and were missed by our black-list). This lead to groups of users (the largest containing 205 users) being initially identified as being co-referent through a web of sharing different such values. In these groups, inconsistency was caused by different values for gender (a functional property), and so they were passed into the repair process. However, the repair process simply split the groups into male and female partitions, which although now consistent, were still incorrect. This illustrates a core weakness of the proposed repair process: consistency does not imply correctness. In summary, for an inconsistency-based detection and repair process to work well over Linked Data, vocabulary publishers would need to provide more, sensible axioms which indicate when instance data are inconsistent. These can then be used to detect more examples of incorrect equivalence (amongst other use-cases <ref type="bibr" target="#b8">[9]</ref>). To help repair such cases, in particular, the widespread use and adoption of datatype-properties which are both inverse-functional and functional (i.e., one-to-one properties such as isbn) would greatly help to automatically identify not only which resources are the same, but to identify which resources are different in a granular manner. 45 Functional properties (e.g., gender) or disjoint classes (e.g., Person, Document) which have wide catchments are useful to detect inconsistencies in the first place, but are not ideal for performing granular repairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Critical discussion</head><p>In this section, we provide critical discussion of our approach, following the dimensions of the requirements listed at the outset.</p><p>With respect to scale, on a high level, our primary means of organising the bulk of the corpus is external-sorts, characterised by the linearithmic time complexity O(n⁄log(n)); external-sorts do not have a critical main-memory requirement, and are efficiently distributable. Our primary means of accessing the data is via linear scans. With respect to the individual tasks:</p><p>-our current baseline consolidation approach relies on an inmemory owl:sameAs index: however we demonstrate an ondisk variant in the extended consolidation approach; -the extended consolidation currently loads terminological data that is required by all machines into memory: if necessary, we claim that an on-disk terminological index would offer good performance given the distribution of class and property memberships, where we believe that a high cache-hit rate would be enjoyed; -for the entity concurrence analysis, the predicate level statistics required by all machines is small in volume-for the moment, we do not see this as a serious factor in scaling-up; -for the inconsistency detection, we identify the same potential issues with respect to terminological data; also, given large equivalence classes with a high number of inlinks and outlinks, we would encounter main-memory problems, where we believe that an on-disk index could be applied assuming a reasonable upper limit on batch sizes.</p><p>With respect to efficiency:</p><p>-the on-disk aggregation of owl:sameAs data for the extended consolidation has proven to be a bottleneck-for efficient processing at higher levels of scale, distribution of this task would be a priority, which should be feasible given that again, the primitive operations involved are external sorts and scans, with non-critical in-memory indices to accelerate reaching the fixpoint; -although we typically observe terminological data to constitute a small percentage of Linked Data corpora (0.1% in our corpus; cf. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>) at higher scales, aggregating the terminological data for all machines may become a bottleneck, and distributed approaches to perform such would need to be investigated; similarly, as we have seen, large terminological documents can cause load-balancing issues; 46 -for the concurrence analysis and inconsistency detection, data are distributed according to a modulo-hash function on the subject and object position, where we do not hash on the objects of rdf:type triples; although we demonstrated even data distribution by this approach for our current corpus, this may not hold in the general case; -as we have already seen for our corpus and machine count, the complexity of repairing consolidated batches may become an issue given large equivalence class sizes; -there is some notable idle time for our machines, where the total cost of running the pipeline could be reduced by interleaving jobs.</p><p>With the exception of our manually derived blacklist for values of (inverse-)functional-properties, the methods presented herein have been entirely domain-agnostic and fully automatic.</p><p>One major open issue is the question of precision and recall. Given the nature of the tasks-particularly the scale and diversity of the datasets-we believe that deriving an appropriate gold standard is currently infeasible:</p><p>-the scale of the corpus precludes manual or semi-automatic processes; -any automatic process for deriving the gold standard would make redundant the approach to test; -results derived from application of the methods on subsets of manually verified data would not be equatable to the results derived from the whole corpus; -even assuming a manual approach were feasible, oftentimes there is no objective criteria for determining what precisely signifies what-the publisher's original intent is often ambiguous.</p><p>Thus, we prefer symbolic approaches to consolidation and disambiguation that are predicated on the formal semantics of the data, where we can appeal to the fact that incorrect consolidation is due to erroneous data, not an erroneous approach. Without a formal means of sufficiently evaluating the results, we employ statistical methods for applications where precision is not a primary requirement. We also use formal inconsistency as an indicator of imprecise consolidation, although we have shown that this method does not currently yield many detections. In general, we believe that for the corpora we target, such research can only find it's real 45 Of course, in OWL (2) DL, datatype-properties cannot be inverse-functional, but Linked Data vocabularies often break this restriction <ref type="bibr" target="#b28">[29]</ref>. 46 We reduce terminological statements on a document-by-document basis according to unaligned blank-node positions: for example, we prune RDF collections identified by blank-nodes that do not join with, e.g., an owl:unionOf axiom. litmus test when integrated into a system with a critical user-base.</p><p>Finally, we have only briefly discussed issues relating to web-tolerance: e.g., spamming or conflicting data. With respect to such consideration, we currently (i) derive and use a blacklist for common void values; (ii) consider authority for terminological data <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>; and (iii) try to detect erroneous consolidation through consistency verification. One might question an approach which trusts all equivalences asserted or derived from the data. Along these lines, we track the original pre-consolidation identifiers (in the form of sextuples), which can be used to revert erroneous consolidation. In fact, similar considerations can be applied more generally to the re-use of identifiers across sources: giving special consideration to the consolidation of third party data about an entity is somewhat fallacious without also considering the third party contribution of data using a consistent identifier. In both cases, we track the context of (consolidated) statements, which at least can be used to verify or post-process sources. <ref type="foot" target="#foot_22">47</ref> Currently, the corpus we use probably does not exhibit any significant deliberate spamming, but rather indeliberate noise. We leave more mature means of handling spamming for future work (as required).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">Related fields</head><p>Work relating to entity consolidation has been researched in the area of databases for a number of years, aiming to identify and process co-referent signifiers, with works under the titles of record linkage, record or data fusion, merge-purge, instance fusion, and duplicate identification, and (ironically) a plethora of variations thereupon; see <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>, etc., and surveys at <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b7">8]</ref>. Unlike our approach, which leverages the declarative semantics of the data in order to be domain agnostic, such systems usually operate given closed schemas-similarly, they typically focus on stringsimilarity measures and statistical analysis. Haas et al. note that ''in relational systems where the data model does not provide primitives for making same-as assertions &lt;. . .&gt; there is a value-based notion of identity'' <ref type="bibr" target="#b24">[25]</ref>. However, we note that some works have focused on leveraging semantics for such tasks in relation databases; e.g., Fan et al. <ref type="bibr" target="#b20">[21]</ref> leverage domain knowledge to match entities, where interestingly they state ''real life data is typically dirty. . . &lt;thus&gt; it is often necessary to hinge on the semantics of the data''.</p><p>Some other works-more related to Information Retrieval and Natural Language Processing-focus on extracting coreferent entity names from unstructured text, tying in with aligning the results of Named Entity Recognition where for example, Singh et al. <ref type="bibr" target="#b59">[60]</ref> present an approach to identify coreferences from a corpus of 3 million natural language ''mentions'' of persons, where they build compound ''entities'' out of the individual mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">Instance matching</head><p>With respect to RDF, one area of research also goes by the name instance matching: for example, in 2009, the Ontology Alignment Evaluation Initiative <ref type="foot" target="#foot_23">48</ref> introduced a new test track on instance matching <ref type="foot" target="#foot_24">49</ref> . We refer the interested reader to the results of OAEI 2010 <ref type="bibr" target="#b19">[20]</ref> for further information, where we now discuss some recent instance matching systems. It is worth noting that in contrast to our scenario, many instance matching systems take as input a small number of instance sets-i.e., consistently named datasets-across which similarities and coreferences are computed. Thus, the instance matching systems mentioned in this section are not specifically tailored for processing Linked Data (and most do not present experiments along these lines).</p><p>The LN2R <ref type="bibr" target="#b55">[56]</ref> system incorporates two methods for performing instance matching: (i) L2R applies deductive techniques where rules are used to model the semantics of the respective ontology-particularly functional properties, inverse-functional properties and disjointness constraints-which are then used to infer consistent coreference matches; (ii) N2R is an inductive, numeric approach which uses the results of L2R to perform unsupervised matching, where a system of linear-equations representing similarities is seeded using text-based analysis and then used to compute instance similarity by iterative resolution. Scalability is not directly addressed.</p><p>The MOMA <ref type="bibr" target="#b67">[68]</ref> engine matches instances from two distinct datasets; to help ensure high precision, only members of the same class are matched (which can be determined, perhaps, by an ontology-matching phase). A suite of matching tools is provided by MOMA, along with the ability to pipeline matchers, and to select a variety of operators for merging, composing and selecting (thresholding) results. Along these lines, the system is designed to facilitate a human expert in instance-matching, focusing on a different scenario to ours presented herein.</p><p>The RiMoM <ref type="bibr" target="#b40">[41]</ref> engine is designed for ontology matching, implementing a wide range of strategies which can be composed and combined in a graphical user interface; strategies can also be selected by the engine itself based on the nature of the input. Matching results from different strategies can be composed using linear interpolation. Instance matching techniques mainly rely on text-based analysis of resources using Vector Space Models and IR-style measures of similarity and relevance. Large scale instance matching is enabled by an inverted-index over text terms, similar in principle to candidate reduction techniques discussed later in this section. They currently do not support symbolic techniques for matching (although they do allow disambiguation of instances from different classes).</p><p>Nikolov et al. <ref type="bibr" target="#b47">[48]</ref> present the KnoFuss architecture for aligning data on an assertional level; they identify a three phase process involving coreferencing (finding equivalent individuals), conflict detection (finding inconsistencies caused by the integration), and inconsistency resolution. For the coreferencing, the authors introduce and discuss approaches incorporating string similarity measures and class-based machine learning techniques. Although the high-level process is similar to our own, the authors do not address scalability concerns.</p><p>In motivating the design of their RDF-AI system, Scharffe et al. <ref type="bibr" target="#b57">[58]</ref> identify four steps in aligning datasets: align, interlink, fuse and post-process. The align process identifies equivalences between entities in the two datasets, the interlink process materialises owl: sameAs relations between the two datasets, the aligning step merges the two datasets (on both a terminological and assertional level, possibly using domain-specific rules), and the post-processing phase subsequently checks the consistency of the output data. Although parts of this conceptual process echoes our own, the RDF-AI system itself differs greatly from our work. First, the authors focus on the task of identifying coreference across two distinct datasets, whereas we aim to identify coreference in a large ''bag of instances''. Second, the authors do not emphasise scalability, where the RDF datasets are loaded into memory and processed using the popular Jena framework; similarly, the system proposes performing using pair-wise comparison for, e.g., using string matching techniques (which we do not support). Third, although inconsistency detection is mentioned in the conceptual process, the authors do not discuss implementation details for the RDF-AI system itself.</p><p>Noessner et al. <ref type="bibr" target="#b48">[49]</ref> present an approach for aligning two A-Boxes described using the same T-Box; in particular they leverage similarity measures introduced by Stuckenschmidt <ref type="bibr" target="#b64">[65]</ref>, and define an optimisation problem to identify the alignment that generates the highest weighted similarity between the two A-Boxes under analysis: they use Integer Linear Programming to generate the optimal alignment, encoding linear constraints to enforce valid (i.e., consistency preserving), one-to-one, functional mappings. Although they give performance results, they do not directly address scalability. Their method for comparing entities is similar in practice to ours: they measure the ''overlapping knowledge'' between two entities, counting how many assertions are true about both. The goal is to match entities such that: the resulting consolidation is consistent; the measure of overlap is maximal.</p><p>Like us, Castano et al. <ref type="bibr" target="#b11">[12]</ref> approach instance matching from two distinct perspectives: (i) determine coreferent identifiers; (ii) detect similar individuals based on the data they share. Much of their work is similar in principle to ours: in particular, they use reasoning for identifying equivalences and use a statistical approach for identifying properties ''with high identification power''. They do not consider use of inconsistency detection for disambiguating entities, and perhaps more critically, only evaluate with respect to a dataset containing $15 thousand entities.</p><p>Cudré-Mauroux et al. <ref type="bibr" target="#b16">[17]</ref> present the idMesh system, which leverages user-defined associations and probabalistic methods to derive entity-level relationships, including resolution of conflicts; they also delineate entities based on ''temporal discrimination'', whereby coreferent entities may predate or postdate one another, capturing a description thereof at a particular point in time. The id-Mesh system itself is designed over a peer-to-peer network with centralised coordination. However, evaluation is over synthetic data, where they only demonstrate a maximum scale involving 8000 entities and 24,000 links, over 400 machines: the evaluation of performance focuses on network traffic and message exchange as opposed to time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3.">Domain-specific consolidation</head><p>Various authors have looked at applying consolidation over domain-specific RDF corpora: e.g., Sleeman and Finin look at using machine learning techniques to consolidate FOAF personal profile information <ref type="bibr" target="#b60">[61]</ref>; Shi et al. similarly look at FOAF-specific alignment techniques <ref type="bibr" target="#b58">[59]</ref> using inverse-functional properties and fuzzy string matching; Jentzsch et al. examine alignment of published drug data <ref type="bibr" target="#b37">[38]</ref>; 50 Raimond et al. look at interlinking RDF from the music-domain <ref type="bibr" target="#b53">[54]</ref>; Monaghan and O' Sullivan apply consolidation to photo annotations expressed in RDF <ref type="bibr" target="#b45">[46]</ref>.</p><p>Salvadores et al. <ref type="bibr" target="#b56">[57]</ref> present the LinksB2N system, which aims to perform scalable integration of RDF data, particularly focusing on evaluation over corpora from the marketing domain; however, their methods are not specific to this domain. They do not leverage the semantics of the data for performing consolidation, instead using similarity measures based on the idea that ''the unique combination of RDF predicates associated with RDF resources is what defines their existence as unique'' <ref type="bibr" target="#b56">[57]</ref>. This is a similar intuition to that behind our concurrence analysis, but we again question the validity of such an assumption for consolidation, particularly given incomplete data and the Open World Assumption underlying RDF(S)/OWL-we view an RDF resource as a description of something signified, and would wish to avoid conflating unique signifiers, even if they match precisely with respect to their description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.">Large-scale/Web-data consolidation</head><p>Like us, various authors have investigated consolidation techniques tailored to the challenges-esp. scalability and noise-of resolving coreference in heterogeneous RDF Web data.</p><p>On a larger scale and following a similar approach to us, Hu et al. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref> have recently investigated a consolidation systemcalled ObjectCoRef-for application over $ 500 million triples of Web data. They too define a baseline consolidation (which they call the kernel) built by applying reasoning on owl:sameAs, owl:InverseFunctionalProperty, owl:FunctionalProperty and owl:cardinality. No other OWL constructs or reasoning is used when building the kernel. Then, starting from the coreferents in the kernel, they use a learning technique to find the most discriminative properties. Further, they reuse the notion of average (inverse) cardinality <ref type="bibr" target="#b33">[34]</ref> to find frequent property combination pairs. A small scale experiment is conducted that shows good results. At large scale, although they manage to build the kernel for the $500 million triples dataset, they only apply and evaluate their statistical method on a very small subset, confessing that the approach would not be feasible for the complete set.</p><p>The Sindice system has historically used inverse-functional properties to find equivalent identifiers, also investigating some bespoke ''schema-level'' reasoning to identify a wider range of such properties <ref type="bibr" target="#b49">[50]</ref>. However, Sindice no longer uses such OWL features for determining coreference of entities. 51 The related Sig.ma search system <ref type="bibr" target="#b68">[69]</ref> internally uses IR-style string-based measures (e.g., TF-IDF scores) to mashup results in their engine; compared to our system, they do not prioritise precision, where mashups are designed to have a high recall of related data, and to be disambiguated manually by the user using the interface. This can be verified anecdotally by searching for various ambiguous terms in their public interface. 52  Bishop et al. <ref type="bibr" target="#b5">[6]</ref> apply reasoning over 1.2 billion Linked Data triples using the BigOWLIM reasoner; however, this dataset is manually selected as a merge of a number of smaller, known datasets as opposed to an arbitrary corpus. They discuss well-known optimisations similar to our canonicalisation as a necessary means of avoiding the quadratic nature of traditional replacement semantics for owl:sameAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5.">Large-scale/distributed consolidation</head><p>With respect to distributed consolidation, Urbani et al. <ref type="bibr" target="#b69">[70]</ref> propose the WebPie system, which uses MapReduce to perform pD* reasoning <ref type="bibr" target="#b66">[67]</ref> using a cluster of commodity hardware similar to ourselves. The pD* ruleset contains rules for handling owl: sameAs replacement, inverse-functional properties and functional-properties, but not for cardinalities (which, in any case we demonstrated to be ineffective over out corpus). The authors also discuss a canonicalisation approach for handling equivalent identifiers. They demonstrate their methods over 100 billion triples of synthetic LUBM data over 64 machines; however, they do not present evaluation over Linked Data, do not perform any form of similarity or concurrence measures, do not consider inconsistency detection (not given by the pD* ruleset, or by their corpora) and generally have a somewhat different focus: scalable distributed rule-based materialisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6.">Candidate reduction</head><p>For performing large-scale consolidation, one approach is to group together sets of candidates within which there are likely 50 In fact, we believe that this work generates the incorrect results observable in Table <ref type="table" target="#tab_8">6</ref>; cf. http://groups.google.com/group/pedantic-web/browse_thread/thread/ ad740f7052cc3a2d. 51 From personal communication with the Sindice team. 52 http://sig.ma/search?q=paris. to be coreferent identifiers. Such candidate reduction then bypasses the need for complete pair-wise comparison and can enable the use of more expensive matching methods in closed regions. This is particularly relevant for text-based analyses which we have not tackled in this paper. However, the principle of candidate reduction generalises to any form of matching that cannot be performed in a complete pair-wise manner.</p><p>Song and Heflin <ref type="bibr" target="#b61">[62]</ref> investigate scalable methods to prune the candidate-space for the instance matching task, focusing on scenarios such as our own. First, discriminating properties-i.e., those for which a typical value is shared by few instances-are used to group initial candidate sets. Second, the authors propose building textual descriptions of instances from surrounding text and using an IR-style inverted index to query and group lexically similar instances. Evaluation demonstrates feasibility for matching up-to one million instances, although the authors claim that the approach can scale further.</p><p>Ioannou et al. <ref type="bibr" target="#b36">[37]</ref> also propose a system, called RDFSim, to cluster similar candidates based on associated textual information. Virtual prose documents are constructed for resources from surrounding text, which are then encoded in a special index which using Locality Sensitive Hashing (LSH). The core idea of LSH is to hash similar virtual documents into regions of space, where distance measures can be used to identify similar resources. A set of hash functions can be employed for these purposes, and ''close'' documents are then subject to other similarity measures; the RDF-Sim system currently uses the Jaccard coefficient to compute textual similarities for these purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.7.">Naming/linking resources on the Web</head><p>A number of works have investigated the fundamentals of resource naming and linking on the Web, proposing schemes or methods to bootstrap agreement between remote publishers on common resource URIs, or to enable publishers to better link their contribution with other external datasets.</p><p>With respect to URI naming on the Web, Bouquet et al. <ref type="bibr" target="#b9">[10]</ref> propose OKKAM: a centralised naming architecture for minting URI signifiers for the Web. The OKKAM system thus supports entity lookups for publishers, where a search providing a description of the entity in question returns a set of possible candidates. Publishers can then re-use the URI for the candidate which most closely matches their intention in their local data, with the intuition that other publishers have done the same; thus, all users of the system will have their naming schemes aligned. However, we see such a centralised '''naming authority'' as going against the ad-hoc, decentralised, scale-free nature of the Web. More concretely, for example, having one URI per resource goes against current Linked Data principles which encourage use of dereferenceable URIs: one URI can only dereference to one document, so which document should that be, and who should decide?</p><p>Online systems RKBExplorer <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>, 53 &lt;sameAs&gt; 54 and Object-Coref <ref type="bibr" target="#b14">[15]</ref> 55 offer on-demand querying for owl:sameAs relations found for a given input URI, which they internally compute and store; the former focus on publishing owl:sameAs relations for authors and papers in the area of scientific publishing, with the latter two systems offering more general owl:sameAs relationships between Linked Data identifiers. In fact, many of the owl:sameAs relations we consume are published as Linked Data by the RKBExplorer system.</p><p>Volz et al. <ref type="bibr" target="#b71">[72]</ref> present the Silk framework for creating and maintaining inter-linkage between domain-specific RDF datasets; in particular, this framework provides publishers with a means of discovering and creating owl:sameAs links between data sources using domain-specific rules and parameters. Thereafter, publishers can integrate discovered links into their exports, enabling better linkage of the data and subsequent consolidation by data consumers: this framework goes hand-in-hand with our approach, producing the owl:sameAs relations which we consume.</p><p>Popitsch and Haslhofer present discussion on the problem of broken links in Linked Data, identifying structurally broken links (the Web of Data's version of a ''deadlink'') and semantically broken links, where the original meaning of an identifier changes after a link has been remotely asserted <ref type="bibr" target="#b51">[52]</ref>. The authors subsequently present the DSNotify system, which monitors dynamicity over a given subset of Linked Data and can detect and act upon changese.g., to notify another agent or correct broken links-and can also be used to indirectly link the dynamic target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.8.">Analyses of owl:sameAs</head><p>Recent papers have analysed and discussed the use of owl:sam-eAs on the Web. These papers have lead to debate on whether owl:sameAs is appropriate for modelling coreference across the Web.</p><p>Halpin et al. <ref type="bibr" target="#b25">[26]</ref> look at the semantics and current usage of owl: sameAs in Linked Data, discussing issues relating to identity, and providing four categories of owl: sameAs usage to relate entities that are closely related, but for which the semantics of owl: sameAs-particularly substitution-does not quite hold. The authors also take and manually inspect 500 owl:sameAs links sampled (using logarithmic weights for each domain) from Web data. Their experiments suggest that although the majority of owl:sameAs relations are considered correct, many were still incorrect, and disagreement between the judges indicates that the quality of specific owl:sameAs links can be subjective <ref type="bibr" target="#b25">[26]</ref>. In fact, their results are much more pessimistic than ours with regards the quality of owl:sameAs on the Web: whereas we established a precision figure of 97.2% for explicit owl:sameAs through manual inspection, Halpin et al. put the figure at 51% (±21%). Even taking the most optimistic figure of 72% from Halpin's paper, there is a wide gap to our result. Possible reasons for the discrepancy include variations in sampling techniques, as well as different criteria for manual judging. 56  Ding et al. <ref type="bibr" target="#b17">[18]</ref> also provide quantitative analysis of owl:sam-eAs usage in the BTC-2010 dataset; some of these results correspond with analogous measures we have presented in Section 4.4, but for a different (albeit similar) sampled corpus. They found that URIs with at least one coreferent identifier had an average of 2.4 identifiers (this corresponds closely with our measurement of 2.65 identifiers per equivalence-class for baseline consolidation). The average path length of owl:sameAs was 1.07, indicating that few transitive links are given. The largest equivalence class found was 5 thousand (versus 8481 in our case). They also discuss the landscape of owl:sameAs linkage between different publishers of Linked Data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>In this paper, we have provided comprehensive discussion on scalable and distributed methods for consolidating, matching, and disambiguating entities present in a large static Linked Data corpus. Throughout, we have focused on the scalability and practicalities of applying our methods over real, arbitrary Linked Data in a domain agnostic and (almost entirely) automatic fashion. We 53 http://www.rkbexplorer.com/sameAs/. 54 http://sameas.org/. 55 http://ws.nju.edu.cn/objectcoref/. 56 Again, our inspection results are available at http://swse.deri.org/entity/.</p><p>have shown how to use explicit owl:sameAs relations in the data to perform consolidation, and subsequently expanded this approach, leveraging the declarative formal semantics of the corpus to materialise additional owl:sameAs relations. We also presented a scalable approach to identify weighted concurrence relations for entities that share many inlinks, outlinks, and attribute values; we note that many of those entities demonstrating the highest concurrence were not coreferent. Next, we presented an approach using inconsistencies to disambiguate entities and subsequently repair equivalence classes: we found that this approach currently derives few diagnoses, where the granularity of inconsistencies within Linked Data is not sufficient for accurately pinpointing all incorrect consolidation. Finally, we tempered our contribution with critical discussion, particularly focusing on scalability and efficiency concerns.</p><p>We believe that the area of research touched upon in this paper-particularly as applied to large scale Linked Data corpora-is of particular significance given the rapid growth in popularity of Linked Data publishing. As the scale and diversity of the Web of Data expands, scalable and precise data integration technique will become of vital importance, particularly for data warehousing applications; we see the work presented herein as a significant step in the right direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Distribution of URIs and the number of documents they appear in (in a dataposition).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig.3. Distribution of sizes of equivalence classes on log/log scale (from<ref type="bibr" target="#b31">[32]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Distribution of the number of PLDs per equivalence class for baseline consolidation and extended reasoning consolidation (log/log).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Distribution of number of PLDs the terms are referenced by, for the raw, baseline consolidated, and reasoning consolidated data (log/log).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig.8. Example of same-value, inter-property and intra-property correlation, where the two entities under comparison are highlighted in the dashed box, and where the labels of inward-edges (with respect to the principal entities) are italicised and underlined (from<ref type="bibr" target="#b33">[34]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ðo i ; o j ; Cðp; sÞ; p; s; ÀÞ where Cðp; sÞ ¼ 1 jOpsjÂAACðpÞ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Breakdown of potential concurrence classes given with respect to predicate-object pairs and predicate-subject pairs, respectively, where for each class size on the xaxis (s i ) we show the number of classes (c i ); the raw non-reflexive, non-symmetric concurrence tuples generated (sp i ¼ c i Â s 2 i þs i</figDesc><graphic coords="23,105.32,97.55,158.84,109.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(i) rederive and build a non-transitive, symmetric graph of equivalences between the identifiers in the equivalence class, based on the inlinks and outlinks of the consolidated entity; (ii) discover identifiers that together cause inconsistency and must be separated, generating a new seed equivalence class for each, and breaking the direct links between them; (iii) assign the remaining identifiers into one of the seed equivalence classes based on: (a) minimum distance in the non-transitive equivalence class; (b) if tied, use a concurrence score. Given the simplifying assumptions, we can formalise the problem thus: we denote the graph of non-transitive equivalences for a given equivalence class as a weighted graph G = (V,E,x) such that V &amp; B [ U is the set of vertices, E &amp; B [ U Â B [ U is the set of edges, and x : E # N Â R is a weighting function for the edges. Our edge weights are pairs (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Distribution of the number of partitions the inconsistent equivalence classes are broken into (log/log).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Distribution of the number of identifiers per repaired partition (log/log).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>assertional (i.e., any) knowledge. Such rules enable an optimisation whereby terminological patterns are pre-bound to generate a new, larger set of purely assertional rules called Tground rules. We illustrate this with an example.Example: Let R prp-ifp denote the following rule</figDesc><table><row><cell>(?p, a, owl: InverseFunctionalProperty),</cell></row><row><cell>(?x1, ?p, ?y),</cell></row><row><cell>(?x2, ?p, ?y)</cell></row><row><cell>)(?x1, owl: sameAs, ?x2)</cell></row><row><cell>When writing T-split rules, we denote terminological patterns</cell></row><row><cell>in the body by underlining. Also, we use 'a' as a convenient short-</cell></row><row><cell>cut for rdf:type, which indicates class-membership. Now take the</cell></row><row><cell>terminological triples:</cell></row><row><cell>(:isbn, a, owl:InverseFunctionalProperty)</cell></row><row><cell>(:mbox, a, owl:InverseFunctionalProperty)</cell></row><row><cell>From these two triples, we can generate two T-ground rules as</cell></row><row><cell>follows:</cell></row><row><cell>(?x1, :isbn, ?y),</cell></row><row><cell>(?x2, :isbn, ?y)</cell></row><row><cell>)(?x1, owl: sameAs, ?x2)</cell></row><row><cell>(?x1, :mbox, ?y),</cell></row><row><cell>(?x2, :mbox, ?y)</cell></row><row><cell>)(?x1, owl: sameAs, ?x2)</cell></row></table><note><p>T r s ; Ante G r s ; Con) where Ante T r s is the set of terminological patterns in Ante r , and Ante G r s :¼ Ante r n Ante T r s . The body of T-split rules are divided into a set of patterns that apply over terminological knowledge, and a set of patterns that apply over</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>as follows:</figDesc><table><row><cell># From the FOAF Vocabulary:</cell></row><row><cell>foaf:homepage rdfs:subPropertyOf</cell></row><row><cell>foaf:isPrimaryTopicOf .</cell></row><row><cell>foaf:isPrimaryTopicOf owl:inverseOf</cell></row><row><cell>foaf:primaryTopic .</cell></row><row><cell>foaf:isPrimaryTopicOf a</cell></row><row><cell>owl:InverseFunctionalProperty .</cell></row><row><cell># From Example Document A:</cell></row></table><note><p>exA:axel foaf:homepage &lt;http://polleres.net/&gt; . # From Example Document B: &lt;http://polleres.net/&gt; foaf:primaryTopic exB:apolleres . # Inferred through prp-spo1: exA:axel foaf:isPrimaryTopicOf &lt;http://polleres.net/&gt; . # Inferred through prp-inv: exB:apolleres foaf:isPrimaryTopicOf &lt;http:// polleres.net/&gt; . # Subsequently, inferred through prp-ifp: exA:axel owl:sameAs exB:apolleres . exB:apolleres owl:sameAs exA:axel .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Rules that support the positive semantics of owl: sameAs.</figDesc><table><row><cell>OWL2RL</cell><cell>Antecedent</cell><cell>Consequent</cell></row><row><cell></cell><cell>Assertional</cell><cell></cell></row><row><cell>eq-sym</cell><cell>?x owl:sameAs ?y .</cell><cell>?y owl:sameAs ?x .</cell></row><row><cell>eq-trans</cell><cell>?x owl:sameAs ?y. ?y owl:sameAs ?z .</cell><cell>?x owl:sameAs ?z .</cell></row><row><cell>eq-rep-s</cell><cell>?s owl:sameAs ?s 0 . ?s ?p ?o .</cell><cell>?s 0 ?p ?o .</cell></row><row><cell>eq-rep-o</cell><cell>?o owl:sameAs ?o 0 . ?s ?p ?o .</cell><cell>?s ?p . ?o 0 .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>OWL 2 RL/RDF rules that directly produce owl: sameAs relations; we denote authoritative variables with bold and italicise the labels of rules requiring new OWL 2 constructs.</figDesc><table><row><cell>OWL2RL</cell><cell>Antecedent</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">OWL 2 RL/RDF rules used to detect inconsistency that we currently support; we</cell></row><row><cell cols="2">denote authoritative variables with bold.</cell><cell></cell></row><row><cell>OWL2RL</cell><cell>Antecedent</cell><cell></cell></row><row><cell></cell><cell>Terminological</cell><cell>Assertional</cell></row><row><cell>eq-diff1</cell><cell>-</cell><cell>?x owl:sameAs ?y</cell></row><row><cell></cell><cell></cell><cell>?x owl:differentFrom ?y</cell></row><row><cell>prp-irp</cell><cell>?p a owl:IrreflexiveProperty</cell><cell>?x ?p ?x</cell></row><row><cell>prp-asyp</cell><cell>?p a owl:AsymmetricProperty</cell><cell>?x ?p it ?y. ?y ?p ?x</cell></row><row><cell>prp-pdw</cell><cell>?p 1 owl:propertyDisjointWith ?p 2</cell><cell>?x ?p 1 ?y; ?p 2 ?y</cell></row><row><cell>prp-adp</cell><cell>?x a owl:AllDisjointProperties</cell><cell>?u ?p i ?y; ?p j ?y (i -j)</cell></row><row><cell></cell><cell>?x owl:members (?p 1 . . . ?p n )</cell><cell></cell></row><row><cell>cls-com</cell><cell>?c 1 owl:complementOf ?c 2</cell><cell>?x a ?c 1 , ?c 2</cell></row><row><cell>cls-maxc1</cell><cell>?x owl:maxCardinality 0</cell><cell>?u a ?x; ?p ?y</cell></row><row><cell></cell><cell>?x owl:onProperty ?p</cell><cell></cell></row><row><cell>cls-maxqc2</cell><cell>?x owl:maxQualifiedCardinality 0</cell><cell>?u a ?x; ?p ?y</cell></row><row><cell></cell><cell>?x owl:onProperty ?p</cell><cell></cell></row><row><cell></cell><cell>?x owl:onClass owl:Thing</cell><cell></cell></row><row><cell>cax-dw</cell><cell>?c 1 owl:disjointWith ?c 2</cell><cell>?x a ?c 1 ,?c 2 .</cell></row><row><cell>cax-adc</cell><cell>?x a owl:AllDisjointClasses</cell><cell>?z a ?c i , ?c j (i -j)</cell></row><row><cell></cell><cell>?x owl:members (?c 1 . . . ?c n )</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>2 SA ^s -o do 3: eqc s map.get(s) 4: eqc o map.get(o) 5: if eqc s = ; ^eqc o = ; then</figDesc><table><row><cell>6:</cell><cell>eqc s[o</cell><cell>{s,o}</cell></row><row><cell>7:</cell><cell cols="2">map.put(s,eqc s[o )</cell></row><row><cell>8:</cell><cell cols="2">map.put(o,eqc s[o )</cell></row><row><cell cols="3">9: else if eqc s = ; then</cell></row><row><cell>10:</cell><cell cols="2">add s to eqc o</cell></row><row><cell>11:</cell><cell cols="2">map.put(s,eqc o )</cell></row><row><cell cols="3">12: else if eqc o = ; then</cell></row><row><cell>13:</cell><cell cols="2">add o to eqc s</cell></row><row><cell>14:</cell><cell cols="2">map.put(o,eqc s )</cell></row><row><cell cols="3">15: else if eqc s -eqc o then</cell></row><row><cell>16:</cell><cell cols="2">if jeqc s j &gt; jeqc o j</cell></row><row><cell>17:</cell><cell cols="2">add all eqc o into eqc s</cell></row><row><cell>18:</cell><cell cols="2">for e o 2 eqc o do</cell></row><row><cell>19:</cell><cell cols="2">map.put(e o ,eqc s )</cell></row><row><cell>20:</cell><cell cols="2">end for</cell></row><row><cell>21:</cell><cell>else</cell><cell></cell></row><row><cell>22:</cell><cell cols="2">add all eqc s into eqc o</cell></row><row><cell>23:</cell><cell cols="2">for e s 2 eqc s do</cell></row><row><cell>24:</cell><cell cols="2">map.put(e s ,eqc o )</cell></row><row><cell>25:</cell><cell cols="2">end for</cell></row><row><cell>26:</cell><cell>end if</cell><cell></cell></row><row><cell cols="2">27: end if</cell><cell></cell></row><row><cell cols="2">28: end for</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Breakdown of timing of distributed baseline consolidation.</figDesc><table><row><cell>Category</cell><cell>1</cell><cell></cell><cell>2</cell><cell></cell><cell>4</cell><cell></cell><cell>8</cell><cell></cell><cell>Full-8</cell><cell></cell></row><row><cell></cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell><cell>min</cell><cell>%</cell><cell>Min</cell><cell>%</cell></row><row><cell>Master</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total execution time</cell><cell>42.2</cell><cell>100.0</cell><cell>22.3</cell><cell>100.0</cell><cell>11.9</cell><cell>100.0</cell><cell>6.7</cell><cell>100.0</cell><cell>63.3</cell><cell>100.0</cell></row><row><cell>Executing</cell><cell>0.5</cell><cell>1.1</cell><cell>0.5</cell><cell>2.2</cell><cell>0.5</cell><cell>4.0</cell><cell>0.5</cell><cell>7.5</cell><cell>8.5</cell><cell>13.4</cell></row><row><cell>Aggregating owl: sameAs</cell><cell>0.4</cell><cell>1.1</cell><cell>0.5</cell><cell>2.2</cell><cell>0.5</cell><cell>4.0</cell><cell>0.5</cell><cell>7.5</cell><cell>8.4</cell><cell>13.3</cell></row><row><cell>Miscellaneous</cell><cell>$</cell><cell>0.1</cell><cell>$</cell><cell>0.1</cell><cell>$</cell><cell>0.1</cell><cell>$</cell><cell>0.4</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Idle (waiting for slaves)</cell><cell>41.8</cell><cell>98.9</cell><cell>21.8</cell><cell>97.7</cell><cell>11.4</cell><cell>95.8</cell><cell>6.2</cell><cell>92.1</cell><cell>54.8</cell><cell>86.6</cell></row><row><cell>Slaves</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg. Executing (total)</cell><cell>41.8</cell><cell>98.9</cell><cell>21.8</cell><cell>97.7</cell><cell>11.4</cell><cell>95.8</cell><cell>6.2</cell><cell>92.1</cell><cell>53.5</cell><cell>84.6</cell></row><row><cell>Extract owl: sameAs</cell><cell>9.0</cell><cell>21.4</cell><cell>4.5</cell><cell>20.2</cell><cell>2.2</cell><cell>18.4</cell><cell>1.1</cell><cell>16.8</cell><cell>12.3</cell><cell>19.5</cell></row><row><cell>Consolidate</cell><cell>32.7</cell><cell>77.5</cell><cell>17.1</cell><cell>76.9</cell><cell>8.8</cell><cell>73.5</cell><cell>4.7</cell><cell>70.5</cell><cell>41.2</cell><cell>65.1</cell></row><row><cell>Avg. Idle</cell><cell>0.5</cell><cell>1.1</cell><cell>0.7</cell><cell>2.9</cell><cell>1.0</cell><cell>8.2</cell><cell>0.8</cell><cell>12.7</cell><cell>9.8</cell><cell>15.4</cell></row><row><cell>Waiting for peers</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.7</cell><cell>0.5</cell><cell>4.0</cell><cell>0.3</cell><cell>4.7</cell><cell>1.3</cell><cell>2.0</cell></row><row><cell>Waiting for master</cell><cell>0.5</cell><cell>1.1</cell><cell>0.5</cell><cell>2.3</cell><cell>0.5</cell><cell>4.2</cell><cell>0.5</cell><cell>7.9</cell><cell>8.5</cell><cell>13.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>Largest five equivalence classes (from<ref type="bibr" target="#b31">[32]</ref>).</figDesc><table><row><cell>#</cell><cell>Canonical term (lexically first in equivalence class)</cell><cell>Size</cell><cell>Correct?</cell></row><row><cell>1</cell><cell>http://bio2rdf.org/dailymed_drugs:1000</cell><cell>8481</cell><cell></cell></row><row><cell>2</cell><cell>http://bio2rdf.org/dailymed_drugs:1042</cell><cell>800</cell><cell></cell></row><row><cell>3</cell><cell>http://acm.rkbexplorer.com/id/person-53292-22877d02973d0d01e8f29c7113776e7e</cell><cell>443</cell><cell>U</cell></row><row><cell>4</cell><cell>http://agame2teach.com/#ddb61cae0e083f705f65944cc3bb3968ce3f3ab59-ge_1</cell><cell>353</cell><cell>U/</cell></row><row><cell>5</cell><cell>http://www.acm.rkbexplorer.com/id/person-236166-1b4ef5fdf4a5216256064c45a8923bc9</cell><cell>316</cell><cell>U</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>Breakdown of timing of distributed extended consolidation w/reasoning where identifies the master/slave tasks that run concurrently.</figDesc><table><row><cell>Category</cell><cell>1</cell><cell></cell><cell>2</cell><cell></cell><cell>4</cell><cell></cell><cell>8</cell><cell></cell><cell>Full-8</cell><cell></cell></row><row><cell></cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell></row><row><cell>Master</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total execution time</cell><cell>454.3</cell><cell>100.0</cell><cell>230.2</cell><cell>100.0</cell><cell>127.5</cell><cell>100.0</cell><cell>81.3</cell><cell>100.0</cell><cell>740.4</cell><cell>100.0</cell></row><row><cell>Executing</cell><cell>29.9</cell><cell>6.6</cell><cell>30.3</cell><cell>13.1</cell><cell>32.5</cell><cell>25.5</cell><cell>30.1</cell><cell>37.0</cell><cell>243.6</cell><cell>32.9</cell></row><row><cell>Aggregate Consolidation Relevant Data</cell><cell>4.3</cell><cell>0.9</cell><cell>4.5</cell><cell>2.0</cell><cell>4.7</cell><cell>3.7</cell><cell>4.6</cell><cell>5.7</cell><cell>29.9</cell><cell>4.0</cell></row><row><cell>Closing owl:sameAs</cell><cell>24.4</cell><cell>5.4</cell><cell>24.5</cell><cell>10.6</cell><cell>25.4</cell><cell>19.9</cell><cell>24.4</cell><cell>30.0</cell><cell>211.2</cell><cell>28.5</cell></row><row><cell>Miscellaneous</cell><cell>1.2</cell><cell>0.3</cell><cell>1.3</cell><cell>0.5</cell><cell>2.4</cell><cell>1.9</cell><cell>1.1</cell><cell>1.3</cell><cell>2.5</cell><cell>0.3</cell></row><row><cell>Idle (waiting for slaves)</cell><cell>424.5</cell><cell>93.4</cell><cell>199.9</cell><cell>86.9</cell><cell>95.0</cell><cell>74.5</cell><cell>51.2</cell><cell>63.0</cell><cell>496.8</cell><cell>67.1</cell></row><row><cell>Slave</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg. Executing (total)</cell><cell>448.9</cell><cell>98.8</cell><cell>221.2</cell><cell>96.1</cell><cell>111.6</cell><cell>87.5</cell><cell>56.5</cell><cell>69.4</cell><cell>599.1</cell><cell>80.9</cell></row><row><cell>Extract Terminology</cell><cell>44.4</cell><cell>9.8</cell><cell>22.7</cell><cell>9.9</cell><cell>11.8</cell><cell>9.2</cell><cell>6.9</cell><cell>8.4</cell><cell>49.4</cell><cell>6.7</cell></row><row><cell>Extract Consolidation Relevant Data</cell><cell>95.5</cell><cell>21.0</cell><cell>48.3</cell><cell>21.0</cell><cell>24.3</cell><cell>19.1</cell><cell>13.2</cell><cell>16.2</cell><cell>137.9</cell><cell>18.6</cell></row><row><cell>Initial Sort (by subject)</cell><cell>98.0</cell><cell>21.6</cell><cell>48.3</cell><cell>21.0</cell><cell>23.7</cell><cell>18.6</cell><cell>11.6</cell><cell>14.2</cell><cell>133.8</cell><cell>18.1</cell></row><row><cell>Consolidation</cell><cell>211.0</cell><cell>46.4</cell><cell>101.9</cell><cell>44.3</cell><cell>51.9</cell><cell>40.7</cell><cell>24.9</cell><cell>30.6</cell><cell>278.0</cell><cell>37.5</cell></row><row><cell>Avg. Idle</cell><cell>5.5</cell><cell>1.2</cell><cell>8.9</cell><cell>3.9</cell><cell>37.9</cell><cell>29.7</cell><cell>23.6</cell><cell>29.0</cell><cell>141.3</cell><cell>19.1</cell></row><row><cell>Waiting for peers</cell><cell>0.0</cell><cell>0.0</cell><cell>3.2</cell><cell>1.4</cell><cell>7.1</cell><cell>5.6</cell><cell>6.3</cell><cell>7.7</cell><cell>37.5</cell><cell>5.1</cell></row><row><cell>Waiting for master</cell><cell>5.5</cell><cell>1.2</cell><cell>5.8</cell><cell>2.5</cell><cell>30.8</cell><cell>24.1</cell><cell>17.3</cell><cell>21.2</cell><cell>103.8</cell><cell>14.0</cell></row></table><note><p><p>21 </p>At least in terms of pure quantity. However, we do not give an indication of the quality or importance of those few equivalences we miss with this approximation, which may well be application specific.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc>Top ten most frequently occurring blacklisted values.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1e+007</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>baseline consolidation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>reasoning consolidation</cell></row><row><cell>#</cell><cell>Blacklisted term</cell><cell>Occurrences</cell><cell></cell><cell>1e+006</cell><cell></cell></row><row><cell>1</cell><cell>Empty literals</cell><cell>584,735</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 3 4 5 6 7 8</cell><cell>&lt;http://null&gt; &lt;http://www.vox.com/gone/&gt; ''08445a31a78661b5c746feff39a9db6e4e2cc5cf'' &lt;http://www.facebook.com&gt; &lt;http://facebook.com&gt; &lt;http://www.google.com&gt; &lt;http://www.facebook.com/&gt;</cell><cell>414,088 150,402 58,338 6988 5462 2234 1254</cell><cell>number of classes</cell><cell>100000 1000 10000</cell><cell></cell></row><row><cell>9</cell><cell>&lt;http://google.com&gt;</cell><cell>1108</cell><cell></cell><cell>100</cell><cell></cell></row><row><cell>10</cell><cell>&lt;http://null.com&gt;</cell><cell>542</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>1000</cell><cell>10000</cell><cell>100000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>equivalence class size</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Fig.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc>Largest five equivalence classes after extended consolidation.</figDesc><table><row><cell>#</cell><cell>Canonical term (lexically lowest in equivalence class)</cell><cell>Size</cell><cell>Correct?</cell></row><row><cell>1</cell><cell>bnode37@http://a12iggymom.vox.com/profile/foaf.rdf</cell><cell>33,052</cell><cell>U</cell></row><row><cell>2</cell><cell>http://bio2rdf.org/dailymed_drugs:1000</cell><cell>8481</cell><cell></cell></row><row><cell>3</cell><cell>http://ajft.org/rdf/foaf.rdf#_me</cell><cell>8140</cell><cell>U</cell></row><row><cell>4</cell><cell>bnode4@http://174.129.12.140:8080/tcm/data/association/100</cell><cell>4395</cell><cell>U</cell></row><row><cell>5</cell><cell>bnode1@http://aaa977.vox.com/profile/foaf.rdf</cell><cell>1977</cell><cell>U</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11</head><label>11</label><figDesc>Number of equivalent identifiers found for the top-five ranked entities in SWSE with respect to baseline consolidation (BL#) and reasoning consolidation (R#).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1e+009</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>baseline consolidated</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1e+008</cell><cell>raw data reasoning consolidated</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1e+007</cell></row><row><cell></cell><cell></cell><cell></cell><cell>number of terms</cell><cell>1000 10000 100000 1e+006</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>10</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>number of PLDs mentioning blank-node/URI term</cell></row><row><cell>#</cell><cell>Canonical Identifier</cell><cell>BL#</cell><cell>R#</cell></row><row><cell>1</cell><cell>&lt;http://dblp.l3s.de/.../Tim_Berners-Lee&gt;</cell><cell>26</cell><cell>50</cell></row><row><cell>2</cell><cell>&lt;genid:danbri&gt;</cell><cell>10</cell><cell>138</cell></row><row><cell>3</cell><cell>&lt;http://update.status.net/&gt;</cell><cell>0</cell><cell>0</cell></row><row><cell>4</cell><cell>&lt;http://www.ldodds.com/foaf/foaf-a-matic&gt;</cell><cell>0</cell><cell>0</cell></row><row><cell>5</cell><cell>&lt;http://update.status.net/user/1#acct&gt;</cell><cell>0</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Definition 5 (Concurrence coefficients). The concurrence-coefficient of a predicate-subject pair (p, s) with respect to a graph G is given as: C G ðp; sÞ ¼ 1 Card G ðp; sÞ Â AAC G ðpÞ and the concurrence-coefficient of a predicate-object pair (p, o) with respect to a graph G is analogously given as: IC G ðp; oÞ ¼ 1 ICard G ðp; oÞ Â AAIC G ðpÞ :</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>1</head><label></label><figDesc>2000Â7 ¼ 0:00007. Also, let o d = http://deri.ie/ such that ICard G (p wh ,o d )=100; now, IC G ðp wh ; o d Þ ¼ 110Â7 % 0:00143. Here, sharing DERI as a workplace will indicate a higher level of concurrence than analogously sharing Google.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12</head><label>12</label><figDesc>Breakdown of timing of distributed concurrence analysis.</figDesc><table><row><cell>Category</cell><cell>1</cell><cell></cell><cell>2</cell><cell></cell><cell>4</cell><cell></cell><cell>8</cell><cell></cell><cell>Full-8</cell><cell></cell></row><row><cell></cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell></row><row><cell>Master</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total execution time</cell><cell>516.2</cell><cell>100.0</cell><cell>262.7</cell><cell>100.0</cell><cell>137.8</cell><cell>100.0</cell><cell>73.0</cell><cell>100.0</cell><cell>835.4</cell><cell>100.0</cell></row><row><cell>Executing</cell><cell>2.2</cell><cell>0.4</cell><cell>2.3</cell><cell>0.9</cell><cell>3.1</cell><cell>2.3</cell><cell>3.4</cell><cell>4.6</cell><cell>2.1</cell><cell>0.3</cell></row><row><cell>Miscellaneous</cell><cell>2.2</cell><cell>0.4</cell><cell>2.3</cell><cell>0.9</cell><cell>3.1</cell><cell>2.3</cell><cell>3.4</cell><cell>4.6</cell><cell>2.1</cell><cell>0.3</cell></row><row><cell>Idle (waiting for slaves)</cell><cell>514.0</cell><cell>99.6</cell><cell>260.4</cell><cell>99.1</cell><cell>134.6</cell><cell>97.7</cell><cell>69.6</cell><cell>95.4</cell><cell>833.3</cell><cell>99.7</cell></row><row><cell>Slave</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg. executing (total exc. idle)</cell><cell>514.0</cell><cell>99.6</cell><cell>257.8</cell><cell>98.1</cell><cell>131.1</cell><cell>95.1</cell><cell>66.4</cell><cell>91.0</cell><cell>786.6</cell><cell>94.2</cell></row><row><cell>Split/sort/scatter (subject)</cell><cell>119.6</cell><cell>23.2</cell><cell>59.3</cell><cell>22.6</cell><cell>29.9</cell><cell>21.7</cell><cell>14.9</cell><cell>20.4</cell><cell>175.9</cell><cell>21.1</cell></row><row><cell>Merge-sort (subject)</cell><cell>42.1</cell><cell>8.2</cell><cell>20.7</cell><cell>7.9</cell><cell>11.2</cell><cell>8.1</cell><cell>5.7</cell><cell>7.9</cell><cell>62.4</cell><cell>7.5</cell></row><row><cell>Split/sort/scatter (object)</cell><cell>115.4</cell><cell>22.4</cell><cell>56.5</cell><cell>21.5</cell><cell>28.8</cell><cell>20.9</cell><cell>14.3</cell><cell>19.6</cell><cell>164.0</cell><cell>19.6</cell></row><row><cell>Merge-sort (object)</cell><cell>37.2</cell><cell>7.2</cell><cell>18.7</cell><cell>7.1</cell><cell>9.6</cell><cell>7.0</cell><cell>5.1</cell><cell>7.0</cell><cell>55.6</cell><cell>6.6</cell></row><row><cell>Extract high-level statistics</cell><cell>20.8</cell><cell>4.0</cell><cell>10.1</cell><cell>3.8</cell><cell>5.1</cell><cell>3.7</cell><cell>2.7</cell><cell>3.7</cell><cell>28.4</cell><cell>3.3</cell></row><row><cell>Generate raw concurrence tuples</cell><cell>45.4</cell><cell>8.8</cell><cell>23.3</cell><cell>8.9</cell><cell>11.6</cell><cell>8.4</cell><cell>5.9</cell><cell>8.0</cell><cell>67.7</cell><cell>8.1</cell></row><row><cell>Cooordinate/sort concurrence tuples</cell><cell>97.6</cell><cell>18.9</cell><cell>50.1</cell><cell>19.1</cell><cell>25.0</cell><cell>18.1</cell><cell>12.5</cell><cell>17.2</cell><cell>166.0</cell><cell>19.9</cell></row><row><cell>Merge-sort/aggregate similarity</cell><cell>36.0</cell><cell>7.0</cell><cell>19.2</cell><cell>7.3</cell><cell>10.0</cell><cell>7.2</cell><cell>5.3</cell><cell>7.2</cell><cell>66.6</cell><cell>8.0</cell></row><row><cell>Avg. idle</cell><cell>2.2</cell><cell>0.4</cell><cell>4.9</cell><cell>1.9</cell><cell>6.7</cell><cell>4.9</cell><cell>6.5</cell><cell>9.0</cell><cell>48.8</cell><cell>5.8</cell></row><row><cell>Waiting for peers</cell><cell>0.0</cell><cell>0.0</cell><cell>2.6</cell><cell>1.0</cell><cell>3.6</cell><cell>2.6</cell><cell>3.2</cell><cell>4.3</cell><cell>46.7</cell><cell>5.6</cell></row><row><cell>Waiting for master</cell><cell>2.2</cell><cell>0.4</cell><cell>2.3</cell><cell>0.9</cell><cell>3.1</cell><cell>2.3</cell><cell>3.4</cell><cell>4.6</cell><cell>2.1</cell><cell>0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13</head><label>13</label><figDesc>Top five predicates with respect to lowest adjusted average cardinality (AAC).</figDesc><table><row><cell>#</cell><cell>Predicate</cell><cell>Objects</cell><cell></cell><cell>Triples</cell><cell>AAC</cell></row><row><cell>1</cell><cell>foaf:nick</cell><cell cols="2">150,433,035</cell><cell>150,437,864</cell><cell>1.000</cell></row><row><cell>2</cell><cell>lldpubmed:journal</cell><cell cols="2">6,790,426</cell><cell>6,795,285</cell><cell>1.003</cell></row><row><cell>3</cell><cell>rdf:value</cell><cell cols="2">2,802,998</cell><cell>2,803,021</cell><cell>1.005</cell></row><row><cell>4</cell><cell>eurostat:geo</cell><cell cols="2">2,642,034</cell><cell>2,642,034</cell><cell>1.005</cell></row><row><cell>5</cell><cell>eurostat:time</cell><cell cols="2">2,641,532</cell><cell>2,641,532</cell><cell>1.005</cell></row><row><cell>Table 14</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Top five predicates with respect to lowest adjusted average inverse-cardinality</cell></row><row><cell>(AAIC).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>#</cell><cell>Predicate</cell><cell></cell><cell>Subjects</cell><cell>Triples</cell><cell>AAIC</cell></row><row><cell>1</cell><cell>lldpubmed:meshHeading</cell><cell></cell><cell>2,121,384</cell><cell>2,121,384</cell><cell>1.009</cell></row><row><cell>2</cell><cell cols="2">opiumfield:recommendation</cell><cell>1,920,992</cell><cell>1,920,992</cell><cell>1.010</cell></row><row><cell>3</cell><cell>fb:type.object.key</cell><cell></cell><cell>1,108,187</cell><cell>1,108,187</cell><cell>1.017</cell></row><row><cell>4</cell><cell>foaf:page</cell><cell></cell><cell>1,702,704</cell><cell>1,712,970</cell><cell>1.017</cell></row><row><cell>5</cell><cell>skipinions:hasFeature</cell><cell></cell><cell>1,010,604</cell><cell>1,010,604</cell><cell>1.019</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15</head><label>15</label><figDesc>Top five concurrent entities and the number of pairs they share.</figDesc><table><row><cell>#</cell><cell>Entity label 1</cell><cell>Entity label 2</cell><cell>Concur</cell></row><row><cell>1</cell><cell>New York City</cell><cell>New York State</cell><cell>791</cell></row><row><cell>2</cell><cell>London</cell><cell>England</cell><cell>894</cell></row><row><cell>3</cell><cell>Tokyo</cell><cell>Japan</cell><cell>900</cell></row><row><cell>4</cell><cell>Toronto</cell><cell>Ontario</cell><cell>418</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16</head><label>16</label><figDesc>Breakdown of concurrences for top five ranked entities in SWSE, ordered by rank, with, respectively, entity label, number of concurrent entities found, the label of the concurrent entity with the largest degree, and finally the degree value.</figDesc><table><row><cell>#</cell><cell>Ranked entity</cell><cell>#Con.</cell><cell>''Closest'' entity</cell><cell>Val.</cell></row><row><cell>1</cell><cell>Tim Berners-Lee</cell><cell>908</cell><cell>Lalana Kagal</cell><cell>0.83</cell></row><row><cell>2</cell><cell>Dan Brickley</cell><cell>2552</cell><cell>Libby Miller</cell><cell>0.94</cell></row><row><cell>3</cell><cell>update.status.net</cell><cell>11</cell><cell>socialnetwork.ro</cell><cell>0.45</cell></row><row><cell>4</cell><cell>FOAF-a-matic</cell><cell>21</cell><cell>foaf.me</cell><cell>0.23</cell></row><row><cell>5</cell><cell>Evan Prodromou</cell><cell>3367</cell><cell>Stav Prodromou</cell><cell>0.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17</head><label>17</label><figDesc>were detected through distinct literal values for inverse-functional properties, and one was detected through owl:differentFrom assertions. We list the top five functionalproperties given non-distinct literal values in Table18and the top five disjoint classes in Table19; note that some inconsistent equivalent classes had multiple detections, where, e.g., the class foaf:Person is a subclass of foaf:Agent and thus an identical detection is given for each. Notably, almost all of the detections involve FOAF classes or properties. (Further, we note that between the time of the crawl and the time of writing, the FOAF vocabulary has removed disjointness constraints between the foaf:Document and foaf:Person/foaf:Agent classes.)</figDesc><table><row><cell cols="2">Breakdown of timing of distributed disambiguation and repair</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell>1</cell><cell></cell><cell>2</cell><cell></cell><cell>4</cell><cell></cell><cell>8</cell><cell></cell><cell>Full-8</cell><cell></cell></row><row><cell></cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell><cell>min</cell><cell>%</cell><cell>Min</cell><cell>%</cell><cell>Min</cell><cell>%</cell></row><row><cell>Master</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total execution time</cell><cell>114.7</cell><cell>100.0</cell><cell>61.3</cell><cell>100.0</cell><cell>36.7</cell><cell>100.0</cell><cell>23.8</cell><cell>100.0</cell><cell>141.1</cell><cell>100.0</cell></row><row><cell>Executing</cell><cell>$</cell><cell>$</cell><cell>$</cell><cell>$</cell><cell>$</cell><cell>0.1</cell><cell>$</cell><cell>0.1</cell><cell>$</cell><cell>$</cell></row><row><cell>Miscellaneous</cell><cell>$</cell><cell>$</cell><cell>$</cell><cell>$</cell><cell>$</cell><cell>0.1</cell><cell>$</cell><cell>0.1</cell><cell>$</cell><cell>$</cell></row><row><cell>Idle (waiting for slaves)</cell><cell>114.7</cell><cell>100.0</cell><cell>61.2</cell><cell>100.0</cell><cell>36.6</cell><cell>99.9</cell><cell>23.8</cell><cell>99.9</cell><cell>141.1</cell><cell>100.0</cell></row><row><cell>Slave</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg. Executing (total exc. idle)</cell><cell>114.7</cell><cell>100.0</cell><cell>59.0</cell><cell>96.3</cell><cell>33.6</cell><cell>91.5</cell><cell>22.3</cell><cell>93.7</cell><cell>130.9</cell><cell>92.8</cell></row><row><cell>Identify inconsistencies and repairs</cell><cell>74.7</cell><cell>65.1</cell><cell>38.5</cell><cell>62.8</cell><cell>23.2</cell><cell>63.4</cell><cell>17.2</cell><cell>72.2</cell><cell>72.4</cell><cell>51.3</cell></row><row><cell>Repair Corpus</cell><cell>40.0</cell><cell>34.8</cell><cell>20.5</cell><cell>33.5</cell><cell>10.3</cell><cell>28.1</cell><cell>5.1</cell><cell>21.5</cell><cell>58.5</cell><cell>41.5</cell></row><row><cell>Avg. Idle</cell><cell>$</cell><cell>$</cell><cell>2.3</cell><cell>3.7</cell><cell>3.1</cell><cell>8.5</cell><cell>1.5</cell><cell>6.3</cell><cell>10.2</cell><cell>7.2</cell></row><row><cell>Waiting for peers</cell><cell>0.0</cell><cell>0.0</cell><cell>2.2</cell><cell>3.7</cell><cell>3.1</cell><cell>8.4</cell><cell>1.5</cell><cell>6.2</cell><cell>10.1</cell><cell>7.2</cell></row><row><cell>Waiting for master</cell><cell>$</cell><cell>$</cell><cell>$</cell><cell>$</cell><cell>$</cell><cell>0.1</cell><cell>$</cell><cell>0.1</cell><cell>$</cell><cell>$</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>Of course, entity consolidation has many practical applications outside of our referential use-case SWSE, and is useful in many generic query-answering secnarioswe see these requirements as being somewhat fundamental to a consolidation component, to varying degrees.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p><ref type="bibr" target="#b4">5</ref> Terminological and assertional data are not by any means necessarily disjoint.Furthermore, this distinction is not always required, but is useful for our purposes herein.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>A. Hogan et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 76-110</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>A.Hogan  et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 76-110</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>viz. http://www.eiao.net/rdf/1.0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>We may optionally consider non-canonical blank-node identifiers as redundant and discard them.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>As discussed later, our current implementation requires at least one variable to appear in all assertional patterns in the body, and so we do not support prp-key and cls-maxqc3; however, these two features are not commonly used in Linked Data vocabularies<ref type="bibr" target="#b28">[29]</ref>. A. Hogan et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 76-110</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_7"><p>For example, see the coreference results given by http://www.rkbexplorer.com/ s a m e A s / ? u r i = h t t p : / / w w w . a c m . r k b e x p l o r e r . c o m / i d / p e r s o n -5 3 2 9 2 -22877d02973d0d01e8f29c7113776e7e, which at the time of writing correspond to 436 out of the 443 equivalent URIs found for Dieter Fensel.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_8"><p>http://huemer.lstadler.net/role/rh.rdf.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_9"><p>Note that we have presented non-distributed, batch-processing execution of these rules at a smaller scale (147 million quadruples) in<ref type="bibr" target="#b30">[31]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_10"><p>Exact cardinalities are disallowed in OWL 2 RL due to their effect on the formal proposition of completeness underlying the profile, but such considerations are moot in our scenario.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_11"><p>These files are G-Zip compressed flat files of N-Triple-like syntax encoding arbitrary length tuples of RDF constants.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_12"><p>Although a predicate-position join is also available, we prefer object-position joins that provide smaller batches of data for the in-memory join.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_13"><p>We make these and later results available for review at http://swse.deri.org/ entity/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_14"><p>Domenico Gendarmi with three URIs-one document assigns one of Dan's foaf:mbox_sha1sum values (for danbri@w3.org) to Domenico: http://foafbuilder.qdos.com/people/myriamleggieri.wordpress.com/foaf.rdf.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_15"><p>We believe this to be due to FOAF publishing practices whereby a given exporter uses consistent inverse-functional property values instead of URIs to uniquely identify entities across local documents. A. Hogan et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 76-110</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_16"><p>For brevity, we omit the graph subscript.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="38" xml:id="foot_17"><p>In any case, we always source terminological data from the raw unconsolidated corpus.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="39" xml:id="foot_18"><p>h t t p : / / w w w . i n f o r m a t i k . u n i -t r i e r . d e / l e y / d b / i n d i c e s / a -t r e e / f / Fern=aacute=ndez:Sergio.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="40" xml:id="foot_19"><p>h t t p : / / w w w . i n f o r m a t i k . u n i -t r i e r . d e / l e y / d b / i n d i c e s / a -t r e e / a / Anzuola:Sergio_Fern=aacute=ndez.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="41" xml:id="foot_20"><p>http://www.data.semanticweb.org/dumps/conferences/eswc-2006-complete.rdf.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="42" xml:id="foot_21"><p>Note that this also could be viewed as a counter-example for using inconsistencies to recant consolidation, where arguably the two entities are coreferent from a practical perspective, even if ''incompatible'' from a symbolic perspective.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="47" xml:id="foot_22"><p>Although it must be said, we currently do not track the steps used to derive the equivalences involved in consolidation, which would be expensive to materialise and maintain.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="48" xml:id="foot_23"><p>OAEI. http://oaei.ontologymatching.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="49" xml:id="foot_24"><p>Instance data matching. http://www.instancematching.org/. A. Hogan et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 76-110</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Andreas Harth for involvement in early works. We also thank the anonymous reviewers and the editors for their time and valuable comments. The work presented in this paper has been funded in part by Science Foundation Ireland under Grant No. SFI/ 08/CE/I1380 (Lion-2), and by an IRCSET postgraduate scholarship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Prefixes</head><p>Table A1 lists the prefixes used throughout the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic similarity of ontology instances tailored on the application context</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Albertoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">De</forename><surname>Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of OTM 2006, Part I</title>
		<meeting>of OTM 2006, Part I</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4275</biblScope>
			<biblScope unit="page" from="1020" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Asymmetric and context-dependent semantic similarity among ontology instances</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Albertoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">De</forename><surname>Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Data Sem</title>
		<imprint>
			<biblScope unit="volume">4900</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Beckett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Berners-Lee</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TeamSubmission/turtle/" />
		<title level="m">Turtle -Terse RDF Triple Language, W3C Team Submission</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Design issues for the World Wide Web, World Wide Web Consortium</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Berners-Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linked</forename><surname>Data</surname></persName>
		</author>
		<ptr target="&lt;http://www.w3.org/DesignIssues/LinkedData.html&gt;" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interactive schema translation with instance-level mappings</title>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Melnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VLDB 2005</title>
		<meeting>of VLDB 2005</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1283" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Factforge: a fast track to the web of data</title>
		<author>
			<persName><forename type="first">Barry</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atanas</forename><surname>Kiryakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damyan</forename><surname>Ognyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Peikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zdravko</forename><surname>Tashev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Velkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heath</surname></persName>
		</author>
		<ptr target="&lt;http://linkeddata.org/docs/how-to-publish&gt;" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>How to Publish Linked Data on the Web, linkeddata.org Tutorial</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jens</forename><surname>Bleiholder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Naumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data fusion, ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust and scalable linked data reasoning incorporating provenance and trust annotations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Piero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Bonatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Polleres</surname></persName>
		</author>
		<author>
			<persName><surname>Sauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Web Sem</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="201" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prefix URI &apos;&apos;T-Box prefixes</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Bouquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Stoermer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Mancioppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Giacomuzzi</surname></persName>
		</author>
		<ptr target="http://www.mpii.de/yago/resource/ProceedingsofSWAP2006" />
	</analytic>
	<monogr>
		<title level="m">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
	<note>A-Box prefixes. the Third Italian Semantic Web Workshop</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Brickley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Guha</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/rdf-schema/" />
		<title level="m">RDF Vocabulary Description Language 1.0: RDF Schema, W3C Recommendation</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Instance Matching for Ontology Population</title>
		<author>
			<persName><forename type="first">Silvana</forename><surname>Castano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Montanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Lorusso</surname></persName>
		</author>
		<editor>Salvatore Gaglio, Ignazio Infantino, Domenico Saccá</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>SEBD</publisher>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting relationships for object consolidation</title>
		<author>
			<persName><forename type="first">Zhaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><forename type="middle">V</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IQIS &apos;05: Proceedings of the Second international workshop on Information quality in information systems</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching Semantic Web objects based on class hierarchies</title>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Linked Data on the Web Workshop</title>
		<meeting>Linked Data on the Web Workshop</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Searching linked objects with falcons: approach, implementation and evaluation</title>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Sem. Web Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="49" to="70" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On (un)suitable fuzzy relations to model approximate equality</title>
		<author>
			<persName><forename type="first">Martine</forename><surname>De Cock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><forename type="middle">E</forename><surname>Kerre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="153" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hermann de Meer, idMesh: graph-based disambiguation of linked data</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Cudré-Mauroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parisa</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Aberer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SameAs networks and beyond: analyzing deployment status and implications of owl: sameAs in linked data</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Shinavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenning</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Semantic Web Conference on the Semantic Web -Volume Part I, ISWC&apos;10</title>
		<meeting>the Ninth International Semantic Web Conference on the Semantic Web -Volume Part I, ISWC&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="145" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Duplicate record detection: a survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Verykios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Euzenat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Scharffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Shvaiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Sváb-Zamazal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vojtech</forename><surname>Svátek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cássia Trojahn, Results of the ontology alignment evaluation initiative 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<author>
			<persName><forename type="first">Wenfei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xibei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reasoning about record matching rules</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="407" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Managing co-reference on the Semantic Web</title>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afraz</forename><surname>Jaffri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Millard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LDOW</title>
		<meeting>of LDOW</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">com: A knowledge driven infrastructure for linked data providers</title>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Millard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afraz</forename><surname>Jaffri</surname></persName>
		</author>
		<author>
			<persName><surname>Rkbexplorer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC Demo Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="797" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<ptr target="http://www.w3.org/TR/owl2-profiles/" />
		<title level="m">OWL 2 Web Ontology Language: Profiles, W3C Working Draft</title>
		<editor>
			<persName><forename type="first">Cuenca</forename><surname>Bernardo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Boris</forename><surname>Grau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhe</forename><surname>Motik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Achille</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carsten</forename><surname>Fokoue</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lutz</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Schema and data: a holistic approach to mapping, resolution and fusion in information integration</title>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hentschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Kossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renée</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ER</title>
		<imprint>
			<biblScope unit="page" from="27" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">When owl:sameAs Isn&apos;t the Same: An Analysis of Identity in Linked Data</title>
		<author>
			<persName><forename type="first">Harry</forename><surname>Halpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">P</forename><surname>Mccusker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">S</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="305" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">When owl:sameAs isn&apos;t the Same: An Analysis of Identity Links on the Semantic Web</title>
		<author>
			<persName><forename type="first">Harry</forename><surname>Halpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linked Data on the Web WWW2010 Workshop (LDOW2010)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><surname>Semantics</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/rdf-mt/" />
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>W3C Recommendation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<ptr target="&lt;http://aidanhogan.com/docs/thesis/&gt;" />
		<title level="m">Exploiting RDFS and OWL for Integrating Heterogeneous, Large-Scale, Linked Data Corpora</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Digital Enterprise Research Institute, National University of Ireland, Galway</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Performing Object Consolidation on the Semantic Web Data Graph</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Harth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First I3 Workshop: Identity, Identifiers, Identification Workshop</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable Authoritative OWL Reasoning for the Web</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Harth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Polleres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Semantic Web Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="90" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Searching and browsing linked data with SWSE: the Semantic Web search engine</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Harth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Umbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Kinsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Polleres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Web Sem</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="401" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SAOR: template rule optimisations for distributed reasoning over 1 billion linked data triples</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Polleres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Some entities are more equal than others: statistical methods to consolidate Linked Data</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Polleres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Umbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Workshop on New Forms of Reasoning for the Semantic Web: Scalable and Dynamic</title>
		<imprint>
			<date type="published" when="2010">NeFoRS2010</date>
			<biblScope unit="page">2010</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A self-training approach for resolving object coreference on the Semantic Web</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bootstrapping object coreferencing on the Semantic Web</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhi</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="663" to="675" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient semantic-aware detection of near duplicate resources</title>
		<author>
			<persName><forename type="first">Ekaterini</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Odysseas</forename><surname>Papapetrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Skoutas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ESWC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="136" to="150" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Linking Open Drug Data</title>
		<author>
			<persName><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oktie</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kei-Hoi</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Samwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Semantic Systems (I-SEMANTICS&apos;09)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Should fuzzy equality and similarity satisfy transitivityComments</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Klawonn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Cock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kerre</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="180" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Optimizing enterprise-scale OWL 2 RL reasoning in a relational database system</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Kolovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Eadon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rimom: A dynamic multistrategy ontology alignment framework</title>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1218" to="1232" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">OWL Web Ontology Language Overview, W3C Recommendation</title>
		<author>
			<persName><forename type="first">Deborah</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Van Harmelen</surname></persName>
		</author>
		<ptr target="&lt;http://www.w3.org/TR/owl-features/&gt;" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DLEJena: A practical forward-chaining OWL 2 RL reasoner combining Jena and Pellet</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Meditskos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Bassiliades</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Web Sem</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="94" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploiting secondary sources for automatic object consolidation</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Michalowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snehal</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of 2003 KDD Workshop on Data Cleaning, Record Linkage, and Object Consolidation</title>
		<meeting>eeding of 2003 KDD Workshop on Data Cleaning, Record Linkage, and Object Consolidation</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Best Practice Recipes for Publishing RDF Vocabularies</title>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Miles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Swick</surname></persName>
		</author>
		<ptr target="&lt;http://www.w3.org/TR/swbp-vocab-pub/&gt;" />
		<editor>Berrueta and Phipps</editor>
		<imprint>
			<date type="published" when="2006-03">March 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Leveraging ontologies, context and social networks to automate photo annotation</title>
		<author>
			<persName><forename type="first">Fergal</forename><surname>Monaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Sullivan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>SAMT</publisher>
			<biblScope unit="page" from="252" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatic Linkage of Vital Records: Computers can be used to extract &apos;&apos;follow-up&apos;&apos; statistics of families from files of routine records</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Axford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="954" to="959" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Integration of Semantically Annotated Data by the KnoFuss Architecture</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><forename type="middle">S</forename><surname>Uren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Motta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">N De</forename><surname>Roeck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>EKAW</publisher>
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Leveraging terminological structure for object reconciliation</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Noessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ESWC</publisher>
			<biblScope unit="page" from="334" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">com: a document-oriented lookup index for open linked data</title>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Delbru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Stenzhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Tummarello</surname></persName>
		</author>
		<author>
			<persName><surname>Sindice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Metadata Semant. Ontol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="52" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Annette ten Teije, Frank van Harmelen, Marvin: distributed reasoning over large-scale Semantic Web data</title>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Kotoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Anadiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Siebes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Web Sem</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">DSNotify: handling broken links in the web of data</title>
		<author>
			<persName><forename type="first">Niko</forename><surname>Popitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Haslhofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="761" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">SPARQL Query Language for RDF</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Prud'hommeaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Seaborne</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/rdf-sparql-query/" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>W3C Recommendation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Interlinking Music-Related Data on the Web</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Raimond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">B</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="52" to="63" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A Theory of Diagnosis from First Principles</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="95" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Combining a logical and a numerical method for data reconciliation</title>
		<author>
			<persName><forename type="first">Fatiha</forename><surname>Saı ¨s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Pernelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Christine</forename><surname>Rousset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Data Sem</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="66" to="94" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">LinksB2N: automatic data integration for the Semantic Web</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Salvadores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Correndo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bene</forename><surname>Rodriguez-Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Gibbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Darlington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><forename type="middle">R</forename><surname>Shadbolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OTM Conferences</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1121" to="1138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">RDF-AI: an architecture for RDF datasets matching, fusion and interlink</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scharffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2009 Workshop on Identity, Reference, and Knowledge Representation</title>
		<imprint>
			<publisher>IR-KR</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Smushing RDF instances: are Alice and Bob the same open source developer?</title>
		<author>
			<persName><forename type="first">Lian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Berrueta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Polo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvino</forename><surname>Fernández</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>PICKME Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Distantly labeling data for large scale cross-document coreference</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>abs/1005.4298</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning co-reference relations for FOAF instances</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Sleeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Poster and Demo Session at ISWC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Automatically generating data linkages using a domain-independent candidate selection approach</title>
		<author>
			<persName><forename type="first">Dezhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Heflin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A simple min-cut algorithm</title>
		<author>
			<persName><forename type="first">Mechthild</forename><surname>Stoer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="585" to="591" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The Case for Shared Nothing</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Database Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="9" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A semantic similarity measure for ontology-based information</title>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FQAS &apos;09: Proceedings of the Eighth International Conference on Flexible Query Answering Systems</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="406" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A class of algorithms which require nonlinear time to maintain disjoint sets</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Endre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarjan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="127" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Completeness, decidability and complexity of entailment for RDF Schema and a semantic extension involving the OWL vocabulary</title>
		<author>
			<persName><forename type="first">J</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><surname>Horst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Web Sem</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="79" to="115" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Moma -a mapping-based object matching system</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Thor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erhard</forename><surname>Rahm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CIDR</publisher>
			<biblScope unit="page" from="247" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Tummarello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Danielczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</author>
		<title level="m">Sig.ma: live views on the Web of data</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note>Semantic Web Challenge</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">OWL reasoning with WebPIE: calculating the closure of 100 billion triples</title>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Urbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Kotoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Maassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Van Harmelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henri</forename><forename type="middle">E</forename><surname>Bal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ESWC</publisher>
			<biblScope unit="page" from="213" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Urbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Kotoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Van Harmelen</surname></persName>
		</author>
		<title level="m">Scalable distributed reasoning using mapreduce</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="634" to="649" />
		</imprint>
	</monogr>
	<note>International Semantic Web Conference</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Discovering and maintaining links on the Web of data</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Volz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Gaedke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="650" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Leveraging non-lexical knowledge for the linked open data Web</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandec ˇíc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uta</forename><surname>Lösch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of April Fool&apos;s day Transactions (RAFT)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="18" to="27" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Parallel materialization of the finite RDFS closure for hundreds of millions of triples</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Hendler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="682" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">J</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Klir</surname></persName>
		</author>
		<author>
			<persName><surname>Yuan</surname></persName>
		</author>
		<title level="m">Fuzzy Sets, Fuzzy Logic, Fuzzy Systems</title>
		<imprint>
			<publisher>World Scientific Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
