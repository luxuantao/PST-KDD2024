<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPARSITY MAY CRY: LET US FAIL (CURRENT) SPARSE NEURAL NETWORKS TOGETHER!</title>
				<funder ref="#_ACxbvTG">
					<orgName type="full">NSF AI Institute for Foundations of Machine Learning</orgName>
				</funder>
				<funder ref="#_mSFbdjc #_4jRXhkS">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-03">3 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
							<email>shiwei.liu@austin.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
							<email>tianlong.chen@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
							<email>zhenyu.zhang@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuxi</forename><surname>Chen</surname></persName>
							<email>xxchen@utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Tianjin</forename><surname>Huang</surname></persName>
							<email>t.huang@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ajay</forename><surname>Jaiswal</surname></persName>
							<email>ajayjaiswal@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SPARSITY MAY CRY: LET US FAIL (CURRENT) SPARSE NEURAL NETWORKS TOGETHER!</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-03">3 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.02141v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparse Neural Networks (SNNs) have received voluminous attention predominantly due to growing computational and memory footprints of consistently exploding parameter count in large-scale models. Similar to their dense counterparts, recent SNNs generalize just as well and are equipped with numerous favorable benefits (e.g., low complexity, high scalability, and robustness), sometimes even better than the original dense networks. As research effort is focused on developing increasingly sophisticated sparse algorithms, it is startling that a comprehensive benchmark to evaluate the effectiveness of these algorithms has been highly overlooked. In absence of a carefully crafted evaluation benchmark, most if not all, sparse algorithms are evaluated against fairly simple and naive tasks (eg. CIFAR-10/100, ImageNet, GLUE, etc.), which can potentially camouflage many advantages as well unexpected predicaments of SNNs. In pursuit of a more general evaluation and unveiling the true potential of sparse algorithms, we introduce "Sparsity May Cry" Benchmark (SMC-Bench), a collection of carefully-curated 4 diverse tasks with 10 datasets, that accounts for capturing a wide range of domain-specific and sophisticated knowledge. Our systemic evaluation of the most representative sparse algorithms reveals an important obscured observation: the state-of-the-art magnitude-and/or gradient-based sparse algorithms seemingly fail to perform on SMC-Bench when applied out-of-the-box, sometimes at significantly trivial sparsity as low as 5%. The observations seek the immediate attention of the sparsity research community to reconsider the highly proclaimed benefits of SNNs. We further conduct a thorough investigation into the reasons for the failure of common SNNs. Our analysis points out that such failure is intimately related to the "lazy regime" of large model training, which hints us with stronger pruning recipes that alleviate the failure on SMC-Bench (though still more or less suffering). By incorporating these well-thought and diverse tasks, SMC-Bench is designed to favor and encourage the development of more scalable and generalizable sparse algorithms. We open-source SMC-Bench to assist researchers in building next-generation sparse algorithms that scale and generalize: https://github.com/VITA-Group/SMC-Bench.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Sparse Neural Networks (SNNs) are no stranger to the deep learning community <ref type="bibr">(Liu &amp; Wang, 2023)</ref>, but recently they have received stupendous attention in the era of transformers (eg. BERT, GPT, ViT, CLIP, etc.), when the parameter count is frequently measured in billions rather than millions. Due to the consistent efforts of sparsity researchers, SNNs have ushered enormous breakthroughs and can generalize just as well as original dense networks, and it is feasible to procure them after training <ref type="bibr" target="#b16">(Frankle &amp; Carbin, 2019;</ref><ref type="bibr" target="#b67">Sanh et al., 2020;</ref><ref type="bibr" target="#b4">Chen et al., 2020;</ref><ref type="bibr" target="#b17">Frankle et al., 2020)</ref>, during training <ref type="bibr" target="#b93">(Zhu &amp; Gupta, 2017;</ref><ref type="bibr" target="#b20">Gale et al., 2019;</ref><ref type="bibr">Liu et al., 2021b)</ref>, and even before training <ref type="bibr" target="#b53">(Mocanu et al., 2018;</ref><ref type="bibr" target="#b37">Lee et al., 2019;</ref><ref type="bibr" target="#b44">Liu et al., 2022)</ref> their dense counterparts using pruning. Apart from well-known efficiency benefits, surprisingly, SNNs also enjoy auxiliary benefits such as adversarial robustness <ref type="bibr" target="#b21">(Guo et al., 2018;</ref><ref type="bibr" target="#b60">?zdenizci &amp; Legenstein, 2021;</ref><ref type="bibr" target="#b5">Chen et al., 2022)</ref>, out-of-distribution Issues with current evaluation paradigm: Firstly, the vast majority of current work on SNNs is narrowly evaluated, i.e., only targeting a single or a few tasks (usually on image classification and sometimes on language understanding) on which SNNs have already proven their proficiency <ref type="bibr" target="#b20">(Gale et al., 2019;</ref><ref type="bibr" target="#b16">Frankle &amp; Carbin, 2019)</ref>. Surprisingly, 79 papers out of our carefully selected 100 papers on SNNs, evaluate sparse models merely on a single task, where 72 out of them evaluate image classification. Secondly, people are obsessed with evaluating SNNs on well-understood datasets, including but not limited to <ref type="bibr">MNIST (LeCun, 1998</ref>) (26 papers), CIFAR-10/100 <ref type="bibr" target="#b30">(Krizhevsky et al., 2009)</ref> (59 and 37 papers, respectively), ImageNet <ref type="bibr" target="#b10">(Deng et al., 2009</ref>) (62 papers), and GLUE <ref type="bibr" target="#b78">(Wang et al., 2018</ref>) (9 papers), where deep neural networks have already exceeded the human-equivalent performance (refer to Appendix D for more details). For instance, even though ImageNet has been considered a rather challenging task over years, very high accuracy (&gt;90%) has been reported many times <ref type="bibr" target="#b85">(Yu et al., 2022;</ref><ref type="bibr" target="#b82">Wortsman et al., 2022;</ref><ref type="bibr" target="#b88">Zhai et al., 2022)</ref>. Such relatively restricted evaluations with "nearly saturated performance" limit the scope of sparse neural networks and are potentially ill-suited to identify new and unexpected capabilities of SNNs.</p><p>Addressing the aforementioned limitations of current SNN evaluation protocols is a pressing need for the community. To this end, we assemble a large-scale, fairly arduous, and diverse benchmark for sparse neural networks -"Sparsity May Cry" Benchmark (or briefly SMC-Bench). Specifically, we consider a broad set of tasks including complex reasoning, multilingual translation, and protein prediction, whose content spans multiple domains. Those tasks require a vast amount of commonsense knowledge, solid mathematical and scientific backgrounds to solve even for humans. Note that none of the datasets in SMC-Bench was created from scratch for the benchmark, we rely on pre-existing datasets as they have been agreed by researchers as challenging, interesting, and of high practical value. We rigorously measure the performance of state-of-the-art (SOTA) pruning and sparse training approaches (in their most common, basic settings) on SMC-Bench, to understand the potential of SNNs to scale and generalize. Our key observations and contributions can be unfolded as:</p><p>? We present "Sparsity May Cry" Benchmark, to re-define the evaluation protocols for sparse neural networks and facilitate a comprehensive assessment of SOTA sparse algorithms.</p><p>The premise of SMC-bench is to develop a suite of large-scale, challenging, realistic, and diverse tasks and datasets that can empower the rigorous advancements in the community.</p><p>? SMC-Bench unveils a critical and startling observation -all of the SOTA sparse algorithms seem to fail on SMC-Bench "out-of-the-box", sometimes at significantly trivial sparsity e.g., 5%. Note that the failure does not appear specific to one sparsification approach but unanimously across all approaches we evaluated. This observation alarmingly demands the attention of the sparsity community to reconsider the highly proclaimed benefits of SNNs.</p><p>? We conduct extensive experiments across representative SNNs produced by various SOTA pruning and sparse training approaches on SMC-Bench, and we summarize our findings: Model prunability is intimately related to task difficulty: models trained on difficult tasks suffer more from pruning compared to easier tasks; The success of before-training sparsification (sparse training or pruning at initialization) is hard to generalize in more complex scenarios; Iterative magnitude pruning (IMP) does not necessarily generalize better than one-shot pruning (OMP) or during-training pruning; Despite performance difference, different magnitude-based pruning approaches lead to extremely similar layerwise sprasities.</p><p>? We further carry out a comprehensive investigation into the potential causes of SNN failures on SMC-Bench. Our analysis suggests that the failure of the existing sparse algorithms might be due to the "lazy regime" dynamics emerging in sufficiently overparameterized models <ref type="bibr" target="#b7">(Chizat et al., 2019;</ref><ref type="bibr" target="#b50">Malladi et al., 2022)</ref>. Inspired by this finding, we hypothesize and confirm that the second-order pruning approaches, i.e., oBERT <ref type="bibr" target="#b31">(Kurtic et al., 2022)</ref>, are more reliable pruning approaches for SMC-Bench, which yield relatively more promising performance on SMC-Bench in Appendix C.  <ref type="formula">2017</ref>)). Second-order sparsification usually achieves higher performance than the other two but is also more expensive due to the full Hessian calculation. Fortunately, many approaches have been proposed to efficiently approximate Hessian <ref type="bibr" target="#b87">(Zeng &amp; Urtasun, 2018;</ref><ref type="bibr" target="#b79">Wang et al., 2019;</ref><ref type="bibr" target="#b70">Singh &amp; Alistarh, 2020)</ref>. The Lottery Ticket Hypothesis (LTH) adopts iterative magnitude pruning (IMP) on fully trained networks and finds a subnetwork at initialization that can be re-trained in isolation to match the original dense networks. <ref type="bibr" target="#b64">Renda et al. (2020)</ref> further found that instead of re-training with the initial weights, re-training with the final weights achieves better performance. With the rise of large language models (LLMs), newer post-training pruning methods have emerged which aim to improve the affordability of these models <ref type="bibr" target="#b67">(Sanh et al., 2020;</ref><ref type="bibr" target="#b4">Chen et al., 2020;</ref><ref type="bibr" target="#b86">Zafrir et al., 2021;</ref><ref type="bibr" target="#b31">Kurtic et al., 2022;</ref><ref type="bibr" target="#b83">Xu et al., 2021;</ref><ref type="bibr" target="#b33">Lagunas et al., 2021;</ref><ref type="bibr" target="#b91">Zhang et al., 2022;</ref><ref type="bibr" target="#b19">Frantar et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>During-Training. During-training sparsification <ref type="bibr" target="#b15">(Finnoff et al., 1993</ref>) is a cheaper option, compared to sparsify a fully converged model. Approaches of during-training sparsification usually train a dense network for some time and then gradually sparsify the network with some schedules and end up with a sparse model with target sparsities. <ref type="bibr" target="#b93">Zhu &amp; Gupta (2017)</ref>; <ref type="bibr" target="#b20">Gale et al. (2019)</ref>; <ref type="bibr" target="#b39">Lin et al. (2020)</ref>; <ref type="bibr">Liu et al. (2021b)</ref> are highlight approaches that gradually prune networks during training and meanwhile allow the pruned weights to be reactivated in case of inaccurate pruning. Another direction of during-training sparsification is adding sparsifying penalties such as (grouped) L 0 and L 1 norm to the loss function, which will punish the unimportant weights to zero, leading to sparse weights <ref type="bibr" target="#b48">(Louizos et al., 2018;</ref><ref type="bibr" target="#b49">Luo &amp; Wu, 2020;</ref><ref type="bibr" target="#b68">Savarese et al., 2020)</ref>.</p><p>Prior-Training. Recently, foundation models <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b8">Chowdhery et al., 2022;</ref><ref type="bibr" target="#b62">Ramesh et al., 2022)</ref> have demonstrated promising quantitative improvement and new qualitative capabilities with increasing scale <ref type="bibr">(Zhang et al., 2020b)</ref>. Along with the scaling of model size and data size, the training resources of these foundation models also get outrageous. To accelerate training, we need to sparsify models before training. LTH unveils the possibility to find SNNs at initialization that can match their dense counterparts, even though it uses post-training pruning to find them. At the same time, sparse training <ref type="bibr" target="#b53">(Mocanu et al., 2018;</ref><ref type="bibr" target="#b56">Mostafa &amp; Wang, 2019;</ref><ref type="bibr" target="#b11">Dettmers &amp; Zettlemoyer, 2019;</ref><ref type="bibr" target="#b14">Evci et al., 2020;</ref><ref type="bibr">Liu et al., 2021c;</ref><ref type="bibr" target="#b69">Schwarz et al., 2021)</ref> was proposed that can train a randomlyinitialized sparse neural network from scratch while dynamically optimizing the sparse connectivity with promising performance. Instead of randomly initializing sparse networks, one iteration <ref type="bibr" target="#b37">(Lee et al., 2019;</ref><ref type="bibr" target="#b80">Wang et al., 2020)</ref> or a few iterations <ref type="bibr" target="#b75">(Tanaka et al., 2020;</ref><ref type="bibr" target="#b9">de Jorge et al., 2021)</ref> of training can be utilized to guide the search for sparse networks before training. <ref type="bibr" target="#b20">Gale et al. (2019)</ref> rigorously evaluated variational dropout <ref type="bibr" target="#b54">(Molchanov et al., 2017)</ref>, l 0 regularizaion <ref type="bibr" target="#b48">(Louizos et al., 2018)</ref>, and GMP <ref type="bibr" target="#b93">(Zhu &amp; Gupta, 2017)</ref> on two large-scale tasks. They demonstrated that the appealing performance advantages of variational dropout and l 0 regularization cannot generalize to large-scale tasks whereas simple magnitude pruning performs surprisingly well. <ref type="bibr" target="#b47">Liu et al. (2018)</ref> examined two pipelines: training from scratch and fine-tuning, concluding that fine-tuning a pruned model only gives comparable or worse performance than training from scratch. <ref type="bibr" target="#b2">Blalock et al. (2020)</ref> provided a comprehensive literature review on SNNs and found that pruning papers rarely make direct and controlled comparisons. <ref type="bibr" target="#b18">Frankle et al. (2021)</ref> assessed the efficacy of various pruning at initialization approaches and attribute their inferior performance to their insensitivity to weight shuffling and re-initialization. <ref type="bibr" target="#b44">Liu et al. (2022)</ref> re-evaluated the performance of various random pruning before training and found that sparsely training a randomly pruned network from scratch can surprisingly match the performance of its dense equivalent. These papers shed light on the behavior of SNNs and discover important research problems for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BENCHMARKING IN SPARSE NEURAL NETWORKS</head><p>3 SMC-BENCH SMC-Bench is crafted for evaluating if all proclaimed benefits of SNNs can "scale and generalize". It consists of 4 diverse and difficult tasks, including commonsense reasoning, arithmetic reasoning, multilingual translation, and protein prediction, with 10 datasets collected from prior work and opensource GitHub repositories. To investigate if there is a strong correlation between model prunability and task difficulty, we choose multiple datasets with different degrees of difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">COMMONSENSE REASONING</head><p>Commonsense reasoning task asks commonsense questions about the world involving complex semantics that often require rich common sense and background knowledge. We consider three commonly used datasets for commonsense reasoning. (1) RACE <ref type="bibr" target="#b34">(Lai et al., 2017)</ref> contains near 28,000 passages and 100,000 questions collected from the English exams for Chinese students in middle (RACE-M) and high school (RACE-H). ( <ref type="formula">2</ref>) WinoGrande <ref type="bibr" target="#b66">(Sakaguchi et al., 2021)</ref> is a modified version of the Winograd Schema Challenge (WSC) benchmark <ref type="bibr" target="#b38">(Levesque et al., 2012)</ref> with enhanced scale and hardness, containing 44k problems.</p><p>(3) Commonsense QA (CSQA) <ref type="bibr" target="#b74">(Talmor et al., 2018</ref>) is a challenging dataset containing 12,247 multiple-choice questions where one source concept and three target concepts are first extracted from ConceptNet <ref type="bibr" target="#b72">(Speer et al., 2017)</ref> based on which the Crowd-works are asked to author multiple-choice questions with two additional distractors.</p><p>In general, CSQA is harder than WinoGrande and RACE, with ceiling human performance of 89%, 94%, and 95%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ARITHMETIC REASONING</head><p>Arithmetic reasoning poses a question of a math problem and the model is asked to generate a mathematical equation that can be evaluated to get the answer. We consider the following three math word problem datasets: (1) the widely used MAWPS benchmark (Koncel-Kedziorski et al., 2016) composed of 2,373 problems; (2) the arithmetic subset of ASDiv (Miao et al., 2021) -ASDiv-A with 1,218 math problems;</p><p>(3) the more challenging SVAMP <ref type="bibr" target="#b61">(Patel et al., 2021)</ref> dataset which is created by applying complex types of variations to the samples from ASDiv-A. The task difficulty monotonically increases from MAWPS to ASDiv-A, and to SVAMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PROTEIN THERMOSTABILITY PREDICTION</head><p>Maintaining a stable 3D structure is an essential pre-condition for protein to function correctly in biological phenomena. Numerous efforts are devoted to modeling and predicting protein's stability against pH, salinity, and temperature. We consider the tasks of protein thermostability prediction on two representative datasets: (1) HotProtein <ref type="bibr" target="#b6">(Chen et al., 2023)</ref> is recently proposed as a large-scale, standardized protein benchmark with organism-level temperature annotations, which contains 182K protein sequences and 3K folded structure from 230 different species. Three dataset variants, HP-S, HP-S 2 C5, and HP-S 2 C2, are adopted to examine sequence-and structure-based methods, respectively. HP-S has {6, 390, 3, 4946, 30, 333, 79, 087, 31, 549} protein sequences from five categories of {Cryophilic, Psychrophilic, Mesophilic, Thermophilic, Hyperthermophilic}; HP-S 2 C5 has both sequences and structures for <ref type="bibr">{73, 387, 195, 196, 189}</ref> proteins from the same five classes ordered from Cryophilic to Hyperthermophilic; HP-S 2 C2 has both sequences and structures for {1, 026, 939} proteins from {"hot"</p><formula xml:id="formula_0">(? 45 ? C), "cold" (&lt; 45 ? C)} two classes.</formula><p>(2) Meltome Atlas <ref type="bibr" target="#b26">(Jarzab et al., 2020)</ref> is another challenging test bed for protein's thermostability. It has {7, 902, 15, 833, 10, 518} protein sequences from three of the five aforementioned classes, from Mesophilic to Hyperthermophilic. All samples are annotated with their melting temperature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MULTILINGUAL TRANSLATION</head><p>Multilingual translation processes multiple languages using a single language model and requires the model to have the ability to perform translation across languages. We follow <ref type="bibr" target="#b46">Liu et al. (2020)</ref>; <ref type="bibr" target="#b76">Tang et al. (2020)</ref> and choose 10 English-centric language pairs (Fr, Cs, De, Gu, Ja, My, Ro, Ru, Vi, Zh ? En) from an open source parallel corpus -OPUS (OPU, 2020). We follow <ref type="bibr" target="#b1">Arivazhagan et al. (2019)</ref> and use pivot data through English to create 3 Many-to-Many multilingual translation fine-tuning settings including 2-to-2 (Fr, Cs), 5-to-5 (Fr, Cs, De, Gi, Ja), and 10-to-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION ON SMC-BENCH</head><p>Models. Despite we are aware that performing few-shot prompting on large-scale pre-training language models with billions of parameters is able to solve these tasks <ref type="bibr" target="#b81">(Wei et al., 2022;</ref><ref type="bibr" target="#b73">Srivastava et al., 2022)</ref>, we choose to focus on fine-tuning or training with pre-trained mid-scale models with millions of parameters, to improve the accessibility of our Benchmark. Specifically, we choose to finetune the popular RoBERTa <ref type="bibr" target="#b45">(Liu et al., 2019)</ref> for commonsense reasoning; to fine-tune mBART <ref type="bibr" target="#b46">(Liu et al., 2020)</ref> for multilingual translation; to train GTS (Xie &amp; Sun, 2019) and Graph2Tree <ref type="bibr">(Zhang et al., 2020a</ref>) with RoBERTa's pre-trained embedding for arithmetic reasoning; to fine-tune Transformerbased <ref type="bibr" target="#b77">(Vaswani et al., 2017)</ref> for protein property prediction. See Appendix A for full details.</p><p>Sparse Neural Networks. We select the most representative magnitude-and/or gradient-based approaches where the prune operation is performed before, during, or after training. Formally, given a dense network ? l ? R d l with a dimension of d l in each layer l ? {1, . . . , L}, pruning generates binary masks m l ? {0, 1} d l yielding sparse neural networks with sparse weights ? l m l . The sparsity level is the fraction of the weights that are zero-valued, calculated as s = 1l m l l d l . Following a mainstream convention in many sparse training papers <ref type="bibr" target="#b16">(Frankle &amp; Carbin, 2019;</ref><ref type="bibr" target="#b20">Gale et al., 2019;</ref><ref type="bibr" target="#b14">Evci et al., 2020;</ref><ref type="bibr" target="#b37">Lee et al., 2019;</ref><ref type="bibr">Liu et al., 2021c)</ref>, we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding <ref type="bibr" target="#b64">(Renda et al., 2020)</ref> and Knowledge Distillation <ref type="bibr" target="#b24">(Hinton et al., 2015)</ref> in our main evaluations, even if we observe that they help to alleviate accuracy drops as in Appendix C.</p><p>? Lottery Ticket Hypothesis (LTH) <ref type="bibr" target="#b16">(Frankle &amp; Carbin, 2019</ref>) is a strong post-training pruning baseline that iteratively adopts magnitude pruning after training to produce binary masks and re-train together with weights from step t. We set t = 0 in this paper, since rewinding to early training does not notably improve the performance of Transformer models (e.g., BERT) for downstream tasks <ref type="bibr" target="#b4">(Chen et al., 2020)</ref>. The pruning rate of each IMP is set as 20%.</p><p>? Magnitude After Training is a strong baseline for prune after training, which has demonstrated strong results in various regimes. After training or fine-tuning models on the specific task, we prune the model with one-shot magnitude pruning and re-train it with the full learning rate schedule from the beginning, dubbed "OMP (After)" in our experiments.</p><p>? Random After Training <ref type="bibr" target="#b52">(Mittal et al., 2019)</ref> is the most naive baseline for post-training pruning. It uniformly samples a random score s l ? Uniform(0, 1) for each weight and prunes the weights with the lowest scores. After pruning, we also re-train with the entire training schedule.</p><p>? Gradual Magnitude Pruning (GMP) <ref type="bibr" target="#b93">(Zhu &amp; Gupta, 2017;</ref><ref type="bibr" target="#b20">Gale et al., 2019)</ref> gradually sparsifies networks during training according to a pre-defined sparsification schedule with sorting-based weight thresholding. The starting and the ending iteration of the gradual sparsification process are set as 10% and 80% of the entire training iterations. The frequency of sparsification steps is tuned among 500, 1000, and 4000, depending on the specific tasks. While we are aware of the advanced gradual pruning methods -movement pruning <ref type="bibr" target="#b67">(Sanh et al., 2020)</ref>, it usually exceeds GMP only at high sparsities (e.g., &gt;90%), which is interesting but not within the scope of this paper.</p><p>? Magnitude Before Training <ref type="bibr" target="#b18">(Frankle et al., 2021)</ref> simply removes weights with the lowest magnitude at initialization. Since we inherit weights from pre-trained models, the initial weights actually refer to the weights that are learned on the pre-trained tasks. We abbreviate this approach to "OMP (Before)" as we use one-shot magnitude pruning.</p><p>? Random Before Training <ref type="bibr" target="#b44">(Liu et al., 2022)</ref> is the most naive baseline for prior-training pruning. We randomly sample scores for each weight and removes the weights with the lowest scores. Different from Random After Training, the pruning operation is performed before fine-tuning.</p><p>? SNIP <ref type="bibr" target="#b37">(Lee et al., 2019</ref>) is a prior-training pruning technique that globally removes weights with the lowest connection sensitivity score |g w|. SNIP is a strong baseline that consistently performs well among various prior-training approaches <ref type="bibr" target="#b18">(Frankle et al., 2021)</ref>.</p><p>? Rigging the Lottery (RigL) <ref type="bibr" target="#b14">(Evci et al., 2020</ref>) is a leading sparse training method that updates the topology of sparse neural networks during training via a prune-and-grow scheme. To evaluate its effectiveness on downstream fine-tuning, we combine RigL with the other three prior-training methods. The update interval of RigL is set the same as the ones used for updating sparsity in GMP, following <ref type="bibr">Liu et al. (2021b)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">COMMONSENSE REASONING</head><p>Implementation Details. We follow the training settings of sequence modeling toolkit Fairseq <ref type="bibr" target="#b59">(Ott et al., 2019)</ref> and fine-tune the pre-trained RoBERTa on our datasets with a standard cross-entropy loss. Specifically for each question, we also construct five inputs, one for each of the five candidate answer choices. Each input is constructed by concatenating the question and candidate answer together. We then encode each input and pass the resulting "[CLS]" representations through a classifier to predict the correct answer. All models are trained with the Adam (Kingma &amp; Ba, 2014) optimizer with a learning rate of 1 ? 10 -5 using an A100 GPU. For CSQA, we train the model for 3000 steps with a linear warmup of 150 steps and a batch size of 16. Results and Analyses. The results of various sparse neural networks are demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>. We summarize our main observations below:</p><p>1 All sparse algorithms seemingly fail to find matching SNNs, even at trivial sparsities such as 36%. While several methods maintain the dense performance at 20% sparsity, their performance starts to drop significantly after that, and will undergo a catastrophic failure as the sparsity continues increasing. It is difficult even for the top-performing LTH to maintain the matching performance after the 2 rd IMP iteration. This is in stark contrast with the behavior of SNNs on the image classification task, where LTH can gracefully preserve the matching performance even at very extreme sparsities (&gt;95% on CIFAR-10/100 <ref type="bibr" target="#b84">(Yin et al., 2022)</ref> and &gt;80% on ImageNet <ref type="bibr" target="#b64">(Renda et al., 2020)</ref>).</p><p>2 The quality of SNNs on harder tasks suffers more from sparsity. Models trained on the hardest task, CSQA, undergo a larger accuracy loss at the same sparsity than the other two datasets. For instance, all the SNNs on CSQA suffer from a catastrophic accuracy drop (up to 74%) and become no better than the random prediction at 59% sparsity. Meanwhile, when trained on WinoGrande and RACE at 59% sparsity, two sparse algorithms (LTH and GMP) can maintain relatively good performance with a smaller performance loss (i.e., 3% ? 10%).</p><p>3 Post-training pruning consistently outperforms prior-training pruning. LTH achieves the best performance across datasets, GMP performs well, and OMP (After) follows behind. However, priortraining pruning achieves worse performance. OMP (Before) performs closely behind OMP (After), whereas SNIP performs no better than the naive random pruning. After digging deeper into the case of SNIP, we find SNIP aggressively prunes the embedding layers to more than 99% sparsity even with a mild overall sparsity of 20%. Surprisingly, the leading dynamic sparsity approach RigL does not bring significant gains to these prior-training approaches, and sometimes even hurts the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ARITHMETIC REASONING</head><p>Implementation Details. We follow SVAMP <ref type="bibr" target="#b61">(Patel et al., 2021)</ref> and choose the two top-performing tree-based models for arithmetic reasoning: GTS (Xie &amp; Sun, 2019) and Graph2Tree <ref type="bibr">(Zhang et al., 2020a)</ref>. Graph2Tree in general performs slightly better than GTS. GTS adopts LSTM to encode input sequences and a tree-based Decoder to generate questions. Graph2Tree uses a graph transformer to learn the latent quantity representations from data, and a tree structure decoder to generate a solution expression tree. We follow exactly the training settings of <ref type="bibr" target="#b61">Patel et al. (2021)</ref>. The embedding weights are inherited from THE pre-trained RoBERTa. All models are trained with Adam for 50 epochs. On MAWPS and ASDiv-A, models are trained with the training data and then evaluated on 5-fold cross-validation based on pre-assigned splits. For SVAMP, we train the models on a combination of MAWPS and ASDiv-A and test them on SVAMP, following <ref type="bibr" target="#b61">Patel et al. (2021)</ref>.</p><p>Results and Analyses. The performance on arithmetic reasoning is reported in Figure <ref type="figure" target="#fig_1">2</ref>. The overall performance trend is very similar to the commonsense reasoning: SNNs can only match the dense performance when the sparsity level is lower than 48.8% with the exception of Graph2Tree on the relatively simple MAWPS dataset whose failing sparsity is 59%; SNNs are prone to sacrifice more performance on harder datasets (i.e., ASDiv-A and SVAMP) than the easier MAWPS dataset; prior-training methods perform no differently than random pruning. Moreover, we want to highlight that LTH surprisingly reaches lower accuracy than OMP and GMP at high sparsity levels, indicating that iterative magnitude pruning may not necessarily generalize better than on more complex tasks. Moreover, Magnitude Before Training (OMG (Before)) consistently causes severe layer collapse in the non-embedding layers, leading to zero accuracies. Since including the results of OMG (Before) will significantly dilute the performance difference of different sparsification methods, we choose to report it in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PROTEIN THERMAL STABILITY PREDICTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">SEQUENCE-BASED MODELS</head><p>Implementation Details. We examine two classic sequence-based approaches in protein property prediction, i.e., TAPE <ref type="bibr" target="#b63">(Rao et al., 2019)</ref> and ESM-1B <ref type="bibr">(Rives et al., 2021)</ref>. For TAPE, we fine-tune it from the official pre-training <ref type="bibr" target="#b63">(Rao et al., 2019)</ref> for 4 epochs with an initial learning rate of 1 ? 10 -4 and a linear decay scheduler together with 100 warm-up steps. As for ESM-1B <ref type="bibr">(Rives et al., 2021)</ref>, starting from the official pre-trained checkpoint, we fine-tune the backbone with a learning rate of 1 ? 10 -6 and the linear classification head on top of the backbone with a learning rate of 2 ? 10 -2 . The learning rate schedulers used for both backbone and linear head are OneCycle (Smith &amp; Topin, 2019) decay schedulers. The training batch size is 2 for Meltome Atlas and 3 for HotProtein (HP-S). Classification accuracy on test sets is collected to measure the model performance.</p><p>Results and Analyses. In this section, we examine diverse sparse neural networks of sequencebased models (i.e., transformers) on protein thermostability prediction benchmarks. ESM-1B <ref type="bibr">(Rives et al., 2021)</ref>, a SOTA approach in protein property modeling, is evaluated on both HotProtein (HP-S) and Meltome Atlas datasets. TAPE <ref type="bibr" target="#b63">(Rao et al., 2019</ref>) is a classic transformer-based model adopted on HotProtein (HP-S). Extensive results of both static and dynamic sparsifications are collected in Figure <ref type="figure">3</ref>. We observe that: For ESM-1B, all extracted sparse neural networks incur significant performance degradation whenever the sparsity level is larger than 20%. Note that here we only sparsify the fully connected layers in multi-head self-attentions &amp; feed-forward networks of each </p><formula xml:id="formula_1">Q, K, V, O, FFN 92.19 Q, K, V 92.55 Q, K 93.62 Q, V 93.62 K, V 92.55</formula><p>Furthermore, we conduct a more fine-grained pruning schedule to investigate the tolerance of ESM-1B against sparsification. In detail, we prune 5% weights with OMP (after) on different modules in ESM-1B and present the obtained accuracies in Table <ref type="table">1</ref>. Q, K, V, O, and FFN represent the fully connected layer in the query, key, value, &amp; output of the self-attention module and feed-forward networks, respectively. The results show that whatever modules we select, 5% sparsity damages the ESM-1B performance of protein thermostability prediction on HP-S 2 C2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">STRUCTURE-BASED MODELS</head><p>Implementation Details. We further consider a representative structure-based algorithm for thermostability prediction, i.e., ESM-IF1 <ref type="bibr">(Hsu et al., 2022)</ref>. Specifically, for ESM-IF1, we train the backbone and its linear head with learning rates of 1 ? 10 -4 and 2 ? 10 -2 , respectively. A batch size of 4 is used for both models on HotProtein (HP-S 2 C5). Classification testing accuracy is reported to reflect the model performance.</p><p>Results and Analyses. In this section, we study structure-based models and their sparse variants on HotProtein (HP-S 2 C5). ESM-IF1 <ref type="bibr">(Hsu et al., 2022)</ref>, a recent SOTA approach, is chosen for benchmarking. It takes the 3D structure of proteins as input and predicts its thermal stable temperature. As shown in Figure <ref type="figure">3</ref>, ESM-IF1 produces inferior sparse neural networks with all pruning mechanisms of both static and dynamic, where OMP (after) and GMP (During) present relatively better accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">MULTILINGUAL TRANSLATION</head><p>Implementation Details. We choose the official multilingual model mBART<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b46">(Liu et al., 2020)</ref>, which was originally pre-trained on 25 languages using masked language modeling (MLM), following the fine-tuning setting of <ref type="bibr" target="#b76">Tang et al. (2020)</ref>. We first choose 10 languages from the language pools used for MLM pre-training; create three sub-groups containing 2, 5, 10 languages; and fine-tune mBART on each sub-group, referring to 2-to-2, 5-to-5, and 10-to-10 multilingual translation finetuning, respectively. During inference, we report the averaged BLEU <ref type="bibr" target="#b76">(Tang et al., 2020;</ref><ref type="bibr" target="#b46">Liu et al., 2020)</ref> scores of bi-directional translation across 10 languages to measure the translation performance. Hence, the task difficulty monotonically decreases from 2-to-2 to 5-to-5, and to 10-to-10 fine-tuning as more languages are involved during training. The default fine-tuning configurations in Tang et al.</p><p>(2020) are adopted for 40K iterations with an Adam optimizer and a learning rate of 1 ? 10 -6 .</p><p>Results and Analyses. Intuitively, fewer languages involved during fine-tuning leads to a more difficult translation for all languages. As demonstrated in Figure <ref type="figure" target="#fig_2">4</ref>, several consistent conclusions can be drawn: Besides OMP (After) and LTH (After), all other produced sparse subnetworks perform worse than the dense baseline when the sparsity is larger than 20%. The BLEU scores of OMP (After) and LTH (After) also decline and fail to match at ? 20%, ? 48.8%, ? 59% sparsity levels for 2-to-2, 5-to-5, and 10-to-10 fine-tuning, respectively. Magnitude-based sparsifications like OMP, LTH, and GMP are comparably robust across all three translation settings, while other pruning methods have negligible advantages compared to random pruning. While the overall tendency of SNNs is quite consistent across different tasks, the prunability of mBART increases as more languages are involved during fine-tuning. It seems that multilingual translation has already been a challenging task for pruning, and involving more languages in inference causes extra obstacles. This is why in the hardest scenario of fine-tuning on 2-to-2 and evaluating with 10 languages, all sparse subnetworks suffer from substantial performance degradation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">WHY SNNS FAIL ON SMC-BENCH</head><p>We conduct a thorough investigation into the reasons why most SNNs struggle to perform on SMC-Bench. Our analysis identifies two possible causes for the failure: (1) the "lazy regime" in LLMs, and (2) the specific model components that are pruned. Based on these findings, we discover a set of stronger pruning recipes that alleviates (though still more or less suffering from) the failure on SMC-Bench, by breaking down the performance of the state-of-the-art BERT-pruning framework -oBERT <ref type="bibr" target="#b31">(Kurtic et al., 2022)</ref> on SMC-Bench (note that most models evaluated in this paper are also <ref type="bibr">BERT-based)</ref>. Due to the limited space, we present our full investigation in Appendix C, and briefly present our sanity check of layer collapse below.</p><p>Does layer collapse occur unexpectedly on SMC-Bench? Layer collapse is the most common cause that blocks the information flow (signal propagation) of sparse neural networks, leading to a catastrophic performance drop <ref type="bibr" target="#b75">(Tanaka et al., 2020)</ref>. We plot the layerwise sparsity ratios of various sparse models in Appendix C.1. We do not observe severe layer collapse across methods except for SNIP which specifically removes nearly entire embedding layers. However, we do observe an unexpected phenomenon: layerwise sparsities of different magnitude-based pruning approaches (i.e., IMP, OMP, and GMP) are extremely similar, all overlapped on one line, despite the significant performance gap among them (up to 42.3%); small differences only start to appear until reaching very deep layers (e.g., classification heads) (see Appendix C.1 for more details). This abnormal behavior is highly correlated with the "lazy regime" <ref type="bibr" target="#b58">(Neyshabur et al., 2020;</ref><ref type="bibr" target="#b50">Malladi et al., 2022)</ref> where the model stays in the same basin during fine-tuning with fairly small weight changes, and hence all magnitudebased pruning approaches, before, during, and after fine-tuning, tend to collapse to the same solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Given the enormous breakthroughs and the fruitful results that sparse neural networks have achieved in numerous fields, it is necessary to rethink the sufficiency of current evaluation protocols and introduce more difficult and diverse benchmarks to explore the limitation of sparse neural networks.</p><p>In this paper, we assemble a large-scale, challenging, and more diverse benchmark, SMC-Bench. Through our thorough evaluation across various leading sparsifications, we confirm that SMC-Bench notably challenges the capability of most magnitude-or/and gradient-based sparse algorithms. We further dig deeper into the behavior of SNNs, and observe several surprising phenomena that are absent in the current evaluation. Our analysis points out that such failure is intimately related to the "lazy regime", which leads us to a suite of strong pruning recipes that alleviates (yet still more or less suffering from) the failure on SMC-Bench. Our subsequent effort will focus on exploring stronger sparse training algorithms that can scale and generalize on SMC-Bench, and meanwhile will consider the training costs of different sparse algorithms for a more holistic picture of their efficiency benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RESULTS OF ARITHMETIC REASONING WITH MAGNITUDE BEFORE TRAINING</head><p>In this appendix, we report the performance of SNNs on arithmetic reasoning including Magnitude before Training (OMP (Before)). We can clearly observe that the accuracy of magnitude pruning before training dramatically falls from 80% to nearly 0% when sparsity is larger than 36%. After checking the corresponding layerwise sparsity, we find that OMP (Before) completely removes all the weights from non-embedding and non-encoder layers, leading to severe layer collapse.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C AN INVESTIGATION OF WHY SNNS FAIL ON SMC-BENCH</head><p>In this section, we conduct a full investigation, attempting to open the box for the potential causes of SNN failures on SMC-Bench. Our analysis reveals two possible causes: (1) the "lazy regime" in finetuning LLMs, and (2) the model components to prune. Due to the "lazy regime" phenomenon <ref type="bibr" target="#b7">(Chizat et al., 2019;</ref><ref type="bibr" target="#b50">Malladi et al., 2022)</ref>, commonly used pruning techniques that rely on magnitude and gradient can be very uninformative. Therefore, we turn to the latest strong second-order pruning framework -oBERT <ref type="bibr" target="#b31">(Kurtic et al., 2022)</ref>, which utilizes inverse-Hessian approximations to guide pruning decisions. We choose RoBERTa.large on CSQA to conduct this investigation. The roadmap of our full investigation is presented below.</p><p>? Layerwise sparsity ratios of various magnitude-based pruning methods are strikingly similar, suggesting that "lazy regime" may occur during fine-tuning. In this regime, the most common pruning criteria such as magnitude and gradient can be rather unreliable.</p><p>? Second-order pruning approaches like oBERT provide more faithful signals than magnitudes and gradients for LLM pruning, and hence achieve significantly higher accuracy at high sparsities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 LAYERWISE SPARSITY RATIOS ON SMC-BENCH</head><p>To check if severe layer collapse occurs on SMC-Bench, we plot the per-layer sparsity ratios discovered by various sparsification approaches at three sparsity levels: 36%, 64%, and 83%. Layers are ordered from input to output on the X-axis. We respectively report the layerwise sparsity of commonsense reasoning with RoBERTa on CSQA and RACE in Figure <ref type="figure">6</ref> and 7, and arithmetic reasoning with GTS on SVAMP in Figure <ref type="figure" target="#fig_4">8</ref>. We summarize our main findings here.</p><p>Layerwise sparsities of magnitude-based pruning approaches are extremely similar. IMP, OMP, and GMP that rely on weight magnitude for pruning share an extremely similar set of layerwise sparsities. Especially, sparsity values of magnitude pruning on commonsense reasoning are completely identical, all overlapped on blue lines, except for the tiny difference in classification heads. This phenomenon indicates that weights of RoBERTa excluding classifiers remain rather stable during commonsense reasoning fine-tuning so that all the magnitude pruning variants (both before and after) discover the same sparsity pattern. The sparsity difference of arithmetic reasoning is more distinguishable than commonsense reasoning. Still, sparsities in the encoder (pre-trained RoBERTa) of IMP, GMP, and OMP (After) largely overlap. Until reaching the later layers, the sparsity ratios of different approaches start to be distinct. SNIP tends to prune all the weights in embedding layers aggressively. Even at the mild 36% sparsity, SNIP prunes weights of embedding layers to 99.7% sparsity, which may explain why SNIP struggles on SMC-Bench. OMP (Before) suffers from layer collapse on arithmetic reasoning. We empirically find that OMP (Before) leads to completely empty deep layers when sparsity is larger than 36%, indicating the limitation of only considering pruning with the magnitude before fine-tuning or re-training.</p><p>The near-identical layerwise sparsity ratios across various magnitude-based methods remind us of the "lazy training" regime <ref type="bibr" target="#b7">(Chizat et al., 2019;</ref><ref type="bibr" target="#b50">Malladi et al., 2022)</ref> which was revealed to occur during the fine-tuning of LLMs. Under this regime, weight changes during fine-tuning are negligible, hence non-informative and more "noisy". Consequently, various magnitude-based pruning approaches, regardless of their timing, all tend to converge to the same solution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 EVALUATION OF OBERT ON SMC-BENCH</head><p>Based on our conjecture that magtitudes/gradients become unreliable for pruning LLMs, we hypothesize that the second-order pruning approaches with approximated Hessian matrix would be more accurate options. To verify our conjecture, we turn to the latest stronger second-order pruning framework, oBERT <ref type="bibr" target="#b31">(Kurtic et al., 2022)</ref>. Specifically, we follow <ref type="bibr" target="#b31">Kurtic et al. (2022)</ref> and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) <ref type="bibr" target="#b64">(Renda et al., 2020)</ref> and Knowledge Distillation (KD) <ref type="bibr" target="#b24">(Hinton et al., 2015)</ref> during each pruning iteration; and keep the embeddings and classification heads dense.</p><p>Figure <ref type="figure" target="#fig_5">9</ref> support our hypothesis and demonstrates that oBERT notably outperforms the zero-order and first-order sparsification approaches, and substantially improves the accuracy to a competitive level. More importantly, oBERT produces a completely different layerwise sparsity pattern from magnitudebased pruning approaches, which is consistent with the patterns that are commonly observed in sparse computer vision models: deeper layers tend to have higher sparsities than lower layers <ref type="bibr" target="#b14">(Evci et al., 2020;</ref><ref type="bibr" target="#b32">Kusupati et al., 2020;</ref><ref type="bibr" target="#b75">Tanaka et al., 2020;</ref><ref type="bibr">Liu et al., 2021b)</ref>. So far, our investigation has discovered that the pruning recipe used in <ref type="bibr" target="#b31">(Kurtic et al., 2022)</ref> can remarkably improve the LLM pruning performance on SMC-Bench. Nevertheless, such a well-tuned pruning recipe is both time-and resource-intensive (9? more fine-tuning time, besides Hessian matrix</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Commonsense reasoning performance of various sparse RoBERTa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Arithmetic reasoning performance of various sparse GTS and Graph2Tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Multilingual performance of various sparse mBART. All models are tested on 10-to-10 multilingual translation and the averaged BLEU are reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Arithmetic reasoning performance of various sparse GTS and Graph2Tree on MAWPS, ASDiv-A, and SVAMP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Layerwise sparsity of GTS on SVAMP at sparsity levels ? [36%, 67%, 83%].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: A roadmap of accuracy recovering via a suite of stronger pruning recipes with RoBERTa-Large on CSQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2.1 ADVANCES IN SPARSE NEURAL NETWORKSPost-Training. SNNs refer to a neural network where a certain portion of its components (e.g., weights, neurons, filters, and attention heads) have exactly zero values. The initial purpose of SNNs is retrospectively to accelerate model at inference time (a.k.a.</figDesc><table><row><cell>, post-training sparsification; Mozer &amp;</cell></row><row><cell>Smolensky (1989); LeCun et al. (1990)). Thanks to the over-parameterization property of deep neural</cell></row><row><cell>networks, we can dramatically prune deep neural networks to smaller sizes with marginal loss of</cell></row><row><cell>performance. Post-training sparsification has been well studied and results in various mature criteria</cell></row><row><cell>that can be generally categorized into zero-order methods (magnitude-based; Han et al. (2015)),</cell></row><row><cell>first-order methods (gradient-based; Molchanov et al. (2016); Sanh et al. (2020); Jiang et al. (2021)),</cell></row><row><cell>and second-order methods (Hessian-based; LeCun et al. (1990); Hassibi &amp; Stork (1992); Dong et al.</cell></row><row><cell>(</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The dropout rate is set as 0.1. This gives us a test accuracy of 77.3% with dense RoBERTa. For RACE, we train each model for 3 epochs with a batch size of 16. This gives us 86.6% and 82.0% dense accuracy on RACE (H) and RACE (M), matching the ones reported in Fairseq (86.5% and 81.3%). Models on WinoGrande are trained for 23, 750 steps with 2, 735 warmup steps and 32 batch size, reaching a 76.3% accuracy.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Figure 3: Protein prediction performance of various sparse models.transformer layer, and leave all other modules unpruned. Even under this loose condition, ESM-1B still fails after pruning on both HP-S and Meltome Atlas, which indicates the low parameter redundancy in ESM-1B for modeling protein thermal stability. In general, static and dynamic pruning algorithms achieve similar performance with ESM-1B. SNIP (Before) and SNIP + RIGL (Before) deliver relatively better accuracies, especially for the high sparsity (? 48.80%) subnetworks on HP-S. As for the worse backbone TAPE compared with ESM-1B, magnitude-based prunings like LTH (After), OMP (After), and GMP (During) show satisfactory results before 59% sparsity.</figDesc><table><row><cell>Accuracy [%]</cell><cell>50 52 54 56 58 60</cell><cell>ESM-1F1 on HP-S 2 C5</cell><cell>Accuracy [%]</cell><cell>45 50 55 60 65 70</cell><cell cols="2">ESM-1B on HP-S</cell><cell>30 35 40 45 50 55 60 65</cell><cell>Tape on HP-S</cell><cell cols="2">38 40 42 44 46 48 50</cell><cell>ESM-1B on Atlas</cell></row><row><cell></cell><cell cols="3">0 .2 0 .3 6 0 .4 8 8 0 .5 9 0 .6 7 2 0 .7 3 8 0 .7 9 1 0 .8 3 2 5 0 .8 6 6 Sparsity</cell><cell cols="3">0 .2 0 .3 6 0 .4 8 8 0 .5 9 0 .6 7 2 0 .7 3 8 0 .7 9 1 0 .8 3 2 5 0 .8 6 6 Sparsity</cell><cell cols="3">0 .2 0 .3 6 0 .4 8 8 0 .5 9 0 .6 7 2 0 .7 3 8 0 .7 9 1 0 .8 3 2 5 0 .8 6 6 Sparsity</cell><cell>0 .2 0 .3 6 0 .4 8 8 0 .5 9 0 .6 7 2 0 .7 3 8 0 .7 9 1 0 .8 3 2 5 0 .8 6 6 Sparsity</cell></row><row><cell></cell><cell></cell><cell>Dense model SNIP (Before)</cell><cell cols="3">LTH (After) SNIP+RIGL (Before)</cell><cell>OMP (After) Random (After)</cell><cell cols="2">Random+RIGL (Before) GMP (During)</cell><cell>OMP (Before)</cell><cell>OMP+RIGL (Before)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Table 1: OMP (after) pruning 5%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">weights of ESM-1B on different</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">modules with HP-S 2 C2.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pruned Modules Accuracy (?)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">None (Dense)</cell><cell>94.68</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/facebookresearch/fairseq</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>We thank <rs type="person">Dan Alistarh</rs> and <rs type="person">Eldar Kurtic</rs> for the extremely helpful discussions about the implementation details of oBERT as well as our benchmark's claims; and <rs type="person">Zhangheng Li</rs> for helping run extra experiments with oBERT. <rs type="person">S. Liu</rs> and <rs type="person">Z. Wang</rs> are in part supported by the <rs type="funder">NSF AI Institute for Foundations of Machine Learning</rs> (IFML). Part of this work used the Dutch national e-infrastructure with the support of the SURF Cooperative using grant no. <rs type="grantNumber">NWO2021.060</rs>, <rs type="grantNumber">EINF-2694</rs> and <rs type="grantNumber">EINF-2943/L1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ACxbvTG">
					<idno type="grant-number">NWO2021.060</idno>
				</org>
				<org type="funding" xml:id="_mSFbdjc">
					<idno type="grant-number">EINF-2694</idno>
				</org>
				<org type="funding" xml:id="_4jRXhkS">
					<idno type="grant-number">EINF-2943/L1</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Layerwise Sparsity GTS on SVAMP, Overall Sparity=67% embeddings.word_embeddings.weight embeddings.position_embeddings.weight embeddings.token_type_embeddings.weight encoder.layer.0.attention.self.query.weight encoder.layer.0.attention.self.key.weight encoder.layer.0.attention.self.value.weight encoder.layer.0.attention.output.dense.weight encoder.layer.0.intermediate.dense.weight encoder.layer.0.output.dense.weight encoder.layer.1.attention.self.query.weight encoder.layer.1.attention.self.key.weight encoder.layer.1.attention.self.value.weight encoder.layer.1.attention.output.dense.weight encoder.layer.1.intermediate.dense.weight encoder.layer.1.output.dense.weight encoder.layer.2.attention.self.query.weight encoder.layer.2.attention.self.key.weight encoder.layer.2.attention.self.value.weight encoder.layer.2.attention.output.dense.weight encoder.layer.2.intermediate.dense.weight encoder.layer.2.output.dense.weight encoder.layer.3.attention.self.query.weight encoder.layer.3.attention.self.key.weight encoder.layer.3.attention.self.value.weight encoder.layer.3.attention.output.dense.weight encoder.layer.3.intermediate.dense.weight encoder.layer.3.output.dense.weight encoder.layer.4.attention.self.query.weight encoder.layer.4.attention.self.key.weight encoder.layer.4.attention.self.value.weight encoder.layer.4.attention.output.dense.weight encoder.layer.4.intermediate.dense.weight encoder.layer.4.output.dense.weight encoder.layer.5.attention.self.query.weight encoder.layer.5.attention.self.key.weight encoder.layer.5.attention.self.value.weight encoder.layer.5.attention.output.dense.weight encoder.layer.5.intermediate.dense.weight encoder.layer.5.output.dense.weight encoder.layer.6.attention.self.query.weight encoder.layer.6.attention.self.key.weight encoder.layer.6.attention.self.value.weight encoder.layer.6.attention.output.dense.weight encoder.layer.6.intermediate.dense.weight encoder.layer.6.output.dense.weight encoder.layer.7.attention.self.query.weight encoder.layer.7.attention.self.key.weight encoder.layer.7.attention.self.value.weight encoder.layer.7.attention.output.dense.weight encoder.layer.7.intermediate.dense.weight encoder.layer.7.output.dense.weight encoder.layer.8.attention.self.query.weight encoder.layer.8.attention.self.key.weight encoder.layer.8.attention.self.value.weight encoder.layer.8.attention.output.dense.weight encoder.layer.8.intermediate.dense.weight encoder.layer.8.output.dense.weight encoder.layer.9.attention.self.query.weight encoder.layer.9.attention.self.key.weight encoder.layer.9.attention.self.value.weight encoder.layer.9.attention.output.dense.weight encoder.layer.9.intermediate.dense.weight encoder.layer.9.output.dense.weight encoder.layer.10.attention.self.query.weight encoder.layer.10.attention.self.key.weight encoder.layer.10.attention.self.value.weight encoder.layer.10.attention.output.dense.weight encoder.layer.10.intermediate.dense.weight encoder.layer.10.output.dense.weight encoder.layer.11.attention.self.query.weight encoder.layer.11.attention.self.key.weight encoder.layer.11.attention.self.value.weight encoder.layer.11.attention.output.dense.weight encoder.layer.11.intermediate.dense.weight encoder.layer.11.output.dense.weight rnn.weight_ih_l0 rnn.weight_hh_l0 rnn.weight_ih_l0_reverse rnn.weight_hh_l0_reverse rnn.weight_ih_l1 rnn.weight_hh_l1 rnn.weight_ih_l1_reverse rnn.weight_hh_l1_reverse</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SUMMARY OF TASKS, MODELS, DATASETS, AND TRAINING</head><p>We summarize the combinations of models and configurations that we used to evaluate SNNs on SMC-Bench. We strictly follow the training configurations reported in the original source to replicate the results of each task. The hyperparameters and configurations used for each model in this paper are shared below.</p><p>A.1 COMMONSENSE REASONING   approximation); and even so, the strongest SNNs still fall short of their dense counterpart by around 10% accuracy at sparsities between 60% -80%, in contrast to "normal" SNNs that easily match their dense models on CIFAR, ImageNet, or GLUE. Therefore, the main claim of our paper still holds, that is, SMC-Bench indeed provides a new benchmark that is way more challenging for SOTA sparse algorithms, than existing testbeds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D SUMMARY OF EVALUATION TASKS AND DATASETS IN 100 PAPERS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://opus.nlpl.eu/opus-100.php" />
		<title level="m">Open source parallel corpus of opus</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05019</idno>
		<title level="m">Massively multilingual neural machine translation in the wild: Findings and challenges</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">What is the state of neural network pruning? Proceedings of machine learning and systems</title>
		<author>
			<persName><forename type="first">Davis</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><forename type="middle">Gonzalez</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="129" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis for pre-trained bert networks</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15834" to="15846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparsity winning twice: Better robust generalization from more efficient training</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Balachandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hotprotein: A novel framework for protein thermostability prediction and editing</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesus</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuxi</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Tyler Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ellington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><surname>Klivans</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YDJRFWBMNby" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On lazy training in differentiable programming</title>
		<author>
			<persName><forename type="first">Lenaic</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Progressive skeletonization: Trimming more fat from a network at initialization</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Pau De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amartya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harkirat</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=9GsFOUyUPi" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04840</idno>
		<title level="m">Sparse networks from scratch: Faster training without losing performance</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A winning hand: Compressing deep networks can improve out-of-distribution robustness</title>
		<author>
			<persName><forename type="first">James</forename><surname>Diffenderfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Bartoldson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Chaganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jize</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="664" to="676" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to prune deep neural networks via layer-wise optimal brain surgeon</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinno</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rigging the lottery: Making all tickets winners</title>
		<author>
			<persName><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Samuel Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2943" to="2952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving model selection by nonconvergent methods</title>
		<author>
			<persName><forename type="first">William</forename><surname>Finnoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferdinand</forename><surname>Hergert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><forename type="middle">Georg</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJl-b3RcF7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The early phase of neural network training</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hkl1iRNFwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pruning neural networks at initialization: Why are we missing the mark?</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Ig-VyQc-MLK" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">M-fac: Efficient matrix-free approximations of second-order information</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eldar</forename><surname>Kurtic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14873" to="14886" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The state of sparsity in deep neural networks</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09574</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparse dnns with improved adversarial robustness</title>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName><forename type="first">Babak</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning inverse folding from millions of predicted structures</title>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<idno type="DOI">10.1101/2022.04.10.487779</idno>
		<ptr target="https://www.biorxiv.org/content/early/2022/04/10/2022.04.10.487779" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meltome atlas-thermal proteome stability across the tree of life</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Jarzab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Kurzawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Moerch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Zecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><surname>Leijten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Musiol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Maschberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Stoehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="495" to="503" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoye</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikai</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07431</idno>
		<title level="m">Towards more effective and economic sparsely-activated model</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mawps: A math word problem repository</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
		<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1152" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The optimal bert surgeon: Scalable and accurate second-order pruning for large language models</title>
		<author>
			<persName><forename type="first">Eldar</forename><surname>Kurtic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Kurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Fineran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Goin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07259</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Soft threshold weight reparameterization for learnable sparsity</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Vivek Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><surname>Farhadi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5544" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Ella</forename><surname>Franc ?ois Lagunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Charlaix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04838</idno>
		<title level="m">Block pruning for faster transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">The mnist database of handwritten digits</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY</title>
		<author>
			<persName><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1VZqjAcYX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth international conference on the principles of knowledge representation and reasoning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic model pruning with feedback</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Dmitriev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJem8lSFwB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.02596</idno>
		<title level="m">Ten lessons we have learned in the new&quot; sparseland&quot;: A short handbook for sparse neural network researchers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Atashgahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghada</forename><surname>Sokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Decebal</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mocanu</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14568</idno>
		<title level="m">Deep ensembling with no overhead for either training or testing: The all-round blessings of dynamic sparsity</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparse training via boosting pruning plasticity with neuroregeneration</title>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Atashgahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanyu</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Decebal</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mocanu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPs)</title>
		<imprint/>
	</monogr>
	<note>2021b</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Do we actually need dense over-parameterization? in-time over-parameterization in sparse training</title>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6989" to="7000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training</title>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=VBZJ_3tz-t" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05270</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning sparse neural networks through l 0 regularization</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference</title>
		<author>
			<persName><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107461</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A kernel-based view of language model fine-tuning</title>
		<author>
			<persName><forename type="first">Sadhika</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.05643</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><surname>Shen-Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Chun</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keh-Yih</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15772</idno>
		<title level="m">A diverse corpus for evaluating and developing english math word problem solvers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Studying the plasticity in deep convolutional neural networks using random pruning</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweta</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="203" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science</title>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Decebal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phuong</forename><forename type="middle">H</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName><surname>Liotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2383</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Variational dropout sparsifies deep neural networks</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2498" to="2507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization</title>
		<author>
			<persName><forename type="first">Hesham</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4646" to="4655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Using relevance to reduce network size automatically</title>
		<author>
			<persName><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">What is being transferred in transfer learning? Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Hanie</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="512" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<publisher>Demonstrations</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Training adversarially robust sparse networks via bayesian connectivity sampling</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>?zdenizci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Legenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8314" to="8324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Arkil</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Bhattamishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07191</idno>
		<title level="m">Are nlp models really able to solve simple math word problems? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Hierarchical textconditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Evaluating protein transfer learning with tape</title>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Comparing rewinding and fine-tuning in neural network pruning</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1gSj0NKvB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Movement pruning: Adaptive sparsity by fine-tuning</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/eae15" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20378" to="20389" />
		</imprint>
	</monogr>
	<note>aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Winning the lottery with continuous sparsification</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11380" to="11390" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Powerpropagation: A sparsity inducing weight reparameterisation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28889" to="28903" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Woodfisher: Efficient second-order approximation for neural network compression</title>
		<author>
			<persName><forename type="first">Pal</forename><surname>Sidak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Alistarh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18098" to="18109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Topin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>International Society for Optics and Photonics</publisher>
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">1100612</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Adam R Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri?</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Garriga-Alonso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04615</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00937</idno>
		<title level="m">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pruning neural networks without any data by iteratively conserving synaptic flow</title>
		<author>
			<persName><forename type="first">Hidenori</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Daniel Lk Yamins</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05467</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Multilingual translation with extensible multilingual pretraining and finetuning</title>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00401</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Eigendamage: Structured pruning in the kronecker-factored eigenbasis</title>
		<author>
			<persName><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6566" to="6575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Picking winning tickets before training by preserving gradient flow</title>
		<author>
			<persName><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkgsACVKPH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2022. 2019</date>
			<biblScope unit="page" from="5299" to="5305" />
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Rethinking network pruning-under the pre-train and fine-tune paradigm</title>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">Eh</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08682</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlado</forename><surname>Menkovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.10842</idno>
		<title level="m">Lottery pools: Winning more by interpolating tickets without increasing training or inference cost</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Legg</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01917</idno>
		<title level="m">Coca: Contrastive captioners are image-text foundation models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Prune once for all: Sparse pre-trained language models</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Zafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Larey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haihao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Wasserblat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05754</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Mlprune: Multi-layer pruning for automated neural network compression</title>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Scaling vision transformers</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12104" to="12113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Can subnetwork structure be the key to out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12356" to="12367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Graphto-tree learning for solving math word problems</title>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Platon: Pruning large transformer models with upper confidence bound of weight importance</title>
		<author>
			<persName><forename type="first">Qingru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simiao</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Bukharin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="26809" to="26823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Yian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haau-Sing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04946</idno>
		<title level="m">When do you need billions of words of pretraining data?</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01878</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">fc1.weight L.5.fc2.weight L.6.self_attn.k_proj.weight L.6.self_attn.v_proj.weight L.6.self_attn.q_proj.weight L.6.self_attn.out_proj.weight L.6.fc1.weight L.6.fc2.weight L.7.self_attn.k_proj.weight L.7.self_attn.v_proj.weight L.7.self_attn.q_proj.weight L.7.self_attn.out_proj.weight L.7.fc1.weight L.7.fc2.weight L.8.self_attn</title>
		<author>
			<orgName type="collaboration">L.0.self_attn.k_proj.weight L.0.self_attn.v_proj.weight embed_positions.weight L.0.self_attn.out_proj.weight L.0.fc1.weight L.0.fc2.weight L.1.self_attn.k_proj.weight L.1.self_attn.v_proj.weight L.1.self_attn.q_proj.weight L.1.self_attn.out_proj.weight L.1.fc1.weight L.1.fc2.weight L.2.self_attn ; L.4.self_attn.v_proj.weight L.4.self_attn.q_proj.weight L.4.self_attn.out_proj.weight L.4.fc1.weight L.4.fc2.weight L.5.self_attn.k_proj.weight L.5.self_attn.v_proj.weight L.5.self_attn.q_proj.weight L.5.self_attn.out_proj.weight L.5. ; 2.weight L.10.self_attn.k_proj.weight L.10.self_attn.v_proj.weight L.10.self_attn.q_proj.weight L.10.self_attn.out_proj.weight L.10.fc1.weight L.10.fc2.weight L.11.self_attn.k_proj.weight L.11.self_attn.v_proj.weight L.11.self_attn.q_proj.weight L.11.self_attn.out_proj.weight L.11.fc1.weight L.11.fc2.weight L.12.self_attn.k_proj.weight L.12.self_attn.v_proj.weight L.12.self_attn.q_proj.weight L.12.self_attn.out_proj.weight L.12.fc1.weight L.12.fc2.weight L.13.self_attn.k_proj.weight L.13.self_attn.v_proj.weight L.13.self_attn.q_proj.weight L.13.self_attn.out_proj.weight L.13.fc1.weight L.13.fc2.weight L.14.self_attn.k_proj.weight L.14.self_attn.v_proj.weight L.14.self_attn.q_proj.weight L.14.self_attn.out_proj.weight L.14.fc1.weight L.14.fc2.weight L.15.self_attn.k_proj.weight L.15.self_attn.v_proj.weight L.15.self_attn.q_proj.weight L.15.self_attn.out_proj.weight L.15.fc1.weight L.15.fc2.weight L.16.self_attn.k_proj.weight L.16.self_attn.v_proj.weight L.16.self_attn.q_proj.weight L.16.self_attn.out_proj.weight L.16.fc1.weight L.16.fc2.weight L.17.self_attn.k_proj.weight L.17.self_attn.v_proj.weight L.17.self_attn.q_proj.weight L.17.self_attn.out_proj.weight L.17.fc1.weight L.17.fc2.weight L.18.self_attn.k_proj.weight L.18.self_attn.v_proj.weight L.18.self_attn.q_proj.weight L.18.self_attn.out_proj.weight L.18.fc1.weight L.18.fc2.weight L.19.self_attn.k_proj.weight L.19.self_attn.v_proj.weight L.19.self_attn.q_proj.weight L.19.self_attn.out_proj.weight L.19.fc1.weight L.19.fc2.weight L.20.self_attn.k_proj.weight L.20.self_attn.v_proj.weight L.20.self_attn.q_proj.weight L.20.self_attn.out_proj.weight L.20.fc1.weight L.20.fc2.weight L.21.self_attn.k_proj.weight L.21.self_attn.v_proj.weight L.21.self_attn.q_proj.weight L.21.self_attn.out_proj.weight L.21.fc1</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="m">weight L.9.self_attn.k_proj.weight L.9.self_attn.v_proj.weight L.9.self_attn.q_proj.weight L.9.self_attn.out_proj.weight L.9.fc1.weight L.9.fc</title>
		<title level="s">k_proj.weight L.2.self_attn.v_proj.weight L.2.self_attn.q_proj.weight L.2.self_attn.out_proj.weight L.2.fc1.weight L.2.fc2.weight L.3.self_attn.k_proj.weight L.3.self_attn.v_proj.weight L.3.self_attn.q_proj.weight L.3.self_attn.out_proj.weight L.3.fc1.weight L.3.fc2.weight L.4.self_attn.k_proj.weight</title>
		<imprint/>
	</monogr>
	<note>k_proj.weight L.8.self_attn.v_proj.weight L.8.self_attn.q_proj.weight L.8.self_attn.out_proj.weight L.8.fc1.weight L.8.fc2.. weight L.21.fc2.weight L.22.self_attn.k_proj.weight L.22.self_attn.v_proj.weight L.22.self_attn.q_proj.weight L.22.self_attn.out_proj.weight L.22.fc1.weight L.22.fc2.weight L.23.self_attn.k_proj.weight L.23.self_attn.v_proj.weight L.23.self_attn.q_proj.weight L.23.self_attn.out_proj.weight L.23.fc1.weight L.23.fc2.weight</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
