<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature selection for high-dimensional class-imbalanced data sets using Support Vector Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-07-30">30 July 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sebastián</forename><surname>Maldonado</surname></persName>
							<email>smaldonado@uandes.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de los Andes</orgName>
								<address>
									<postCode>12455</postCode>
									<settlement>Mons. Álvaro del Portillo</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Las Condes</orgName>
								<address>
									<settlement>Santiago</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Weber</surname></persName>
							<email>rweber@dii.uchile.cl</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="institution">Universidad de Chile</orgName>
								<address>
									<postCode>701</postCode>
									<settlement>República, Santiago</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fazel</forename><surname>Famili</surname></persName>
							<email>fazel.famili@nrc-cnrc.gc.ca</email>
							<affiliation key="aff3">
								<orgName type="institution">National Research Council of Canada</orgName>
								<address>
									<settlement>Ottawa</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature selection for high-dimensional class-imbalanced data sets using Support Vector Machines</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-07-30">30 July 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">A0ED7AA02E63343F6C48BFD0B6AB5DCF</idno>
					<idno type="DOI">10.1016/j.ins.2014.07.015</idno>
					<note type="submission">Received 31 May 2013 Received in revised form 27 May 2014 Accepted 6 July 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Feature selection Imbalanced data set Dimensionality reduction Support Vector Machine Data mining</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature selection and classification of imbalanced data sets are two of the most interesting machine learning challenges, attracting a growing attention from both, industry and academia. Feature selection addresses the dimensionality reduction problem by determining a subset of available features to build a good model for classification or prediction, while the class-imbalance problem arises when the class distribution is too skewed. Both issues have been independently studied in the literature, and a plethora of methods to address high dimensionality as well as class-imbalance has been proposed. The aim of this work is to simultaneously explore both issues, proposing a family of methods that select those attributes that are relevant for the identification of the target class in binary classification. We propose a backward elimination approach based on successive holdout steps, whose contribution measure is based on a balanced loss function obtained on an independent subset. Our experiments are based on six highly imbalanced microarray data sets, comparing our methods with well-known feature selection techniques, and obtaining a better prediction with consistently fewer relevant features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Feature selection is an important topic in data mining, especially in high-dimensional applications. A low-dimensional representation of the data reduces the risk of overfitting, which is higher in this kind of data sets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>, improving the model's generalization ability. Feature selection is a combinatorial problem in the number of original features <ref type="bibr" target="#b13">[14]</ref>, and finding the optimal subset of variables is considered NP-hard. 1  Feature selection can be very helpful when facing imbalanced data sets <ref type="bibr" target="#b7">[8]</ref>. In the context of classification, this problem occurs when there are many more examples from some classes than from others. In this paper we focus on binary classification, and we refer to the class imbalance problem when one of the two classes (the negative class) is significantly larger in terms of instances than the other, the positive class. Furthermore, we assume that misclassifying an instance from the positive class (also called target class) is more expensive than misclassifying an instances from the negative class <ref type="bibr" target="#b15">[16]</ref>. This problem is especially challenging when both classes have a high degree of overlap as has been pointed out e.g. by Prati et al. <ref type="bibr" target="#b24">[25]</ref> and Qu et al. <ref type="bibr" target="#b25">[26]</ref>. It is prevalent in many applications, including fraud/churn detection <ref type="bibr" target="#b18">[19]</ref>, text categorization <ref type="bibr" target="#b42">[43]</ref>, medical diagnosis <ref type="bibr" target="#b34">[35]</ref>, detection of software defects <ref type="bibr" target="#b23">[24]</ref>, and many others.</p><p>Technically speaking, any data set with an unequal distribution between the two classes, can be considered imbalanced. However, class ratios of 5:1 (majority class:minority class) or higher have often been considered in experiments as imbalanced data sets ( <ref type="bibr" target="#b15">[16]</ref>).</p><p>Support Vector Machines (SVMs) <ref type="bibr" target="#b35">[36]</ref> are effective classifiers that provide several advantages such as adequate generalization of new objects and a representation that depends on few parameters. Additionally, it can be shown that the respective objective function (see Section 3.1) is convex which assures that the method does not get stuck in local minima of the particular model <ref type="bibr" target="#b35">[36]</ref>. However, SVMs do not determine the features' importance within the respective model <ref type="bibr" target="#b19">[20]</ref>. In the present paper, we introduce a new feature selection approach for binary classification using SVM which is especially suited for imbalanced data sets.</p><p>Section 2 provides an overview on the class-imbalance problem. In Section 3 we briefly introduce SVM for classification as well as for feature selection lying the basis for the posterior developments. Section 4 introduces the proposed family of methods for feature selection based on SVM. Experimental results using imbalanced real-world data sets are presented in Section 5. Section 6 summarizes this paper, provides its main conclusions, and hints at future developments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The class imbalance problem</head><p>Several solutions to handle the problem of classifying imbalanced data sets have been proposed. These are mainly in four directions: resampling, cost-sensitive learning, one class learning, and feature selection. In the following subsections we briefly describe the first three of these directions. We then refer to the assessment metrics used to evaluate the respective classifiers. Related work on feature selection to classify imbalanced data sets constitutes the basis for our present work and will be detailed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Resampling</head><p>There are several techniques regarding data resampling, which differ mainly in their nature (random or informed resampling). The two most common data resampling techniques are random oversampling and random undersampling. Random oversampling duplicates randomly selected examples of the minority class. While this does help to balance the class distribution, no new information is added to the data set thus leading potentially to overfitting <ref type="bibr" target="#b34">[35]</ref>. Also, this procedure increases the size of the training set, causing longer model training times. Random undersampling randomly discards instances from the majority class, which may lead to an important loss of information <ref type="bibr" target="#b34">[35]</ref>.</p><p>Chawla et al. <ref type="bibr" target="#b8">[9]</ref> proposed SMOTE, a ''Synthetic Minority Over-sampling TEchnique'', which generates new examples for the minority class. These are created artificially by interpolating the preexisting minority instances, which may help to improve the classification performance on imbalanced data sets <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cost-sensitive learning</head><p>Cost-sensitive techniques are based on the concept of a cost matrix, which can be considered as a numerical representation of the penalty when classifying instances to the wrong class. For example, we define C À as the cost of misclassifying a majority class instance as a minority class instance and let C þ represent the cost of the contrary case. Typically, there is no cost associated with correct classification and the cost of misclassification in the target class is higher than the contrary case, i.e., C þ &gt; C À . The objective of cost-sensitive learning then is to develop a classifier that minimizes the overall cost on the training data set.</p><p>There are different ways of implementing cost-sensitive learning. A detailed analysis would go beyond the scope of this paper since we concentrate on a methodological development independent of a particular application where the respective misclassification costs could be specified. The interested reader is referred to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">One-class learning</head><p>When negative examples greatly outnumber the positive ones, most classifiers tend to overfit <ref type="bibr" target="#b7">[8]</ref>. This is particulary true when facing high-dimensional data sets <ref type="bibr" target="#b34">[35]</ref>. In this case, one-class SVM trained only with the target class may lead to a better predictive performance <ref type="bibr" target="#b33">[34]</ref>. Here, the method attempts to measure the similarity between a query object and the target class, where classification is accomplished by imposing a threshold on the similarity value <ref type="bibr" target="#b7">[8]</ref>. It has been shown <ref type="bibr" target="#b33">[34]</ref> that the one-class approach to classify high-dimensional imbalanced data sets can be an interesting alternative to feature selection in such situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Assessment metrics for imbalanced classification</head><p>Traditionally, the most frequently used metric for binary classification is the accuracy, which is the proportion of true results to all results. This metric provides a simple way of describing the classification performance in a given data set. However, it is not appropriate for classification of imbalanced data sets <ref type="bibr" target="#b15">[16]</ref>. For example, if a given data set includes 5% of target class instances and 95% of majority examples, a naïve approach of classifying every instance to the majority class would provide an accuracy of 95%, which could be considered very good. This measure, however, fails to reflect the fact that 0% of the target instances are identified, which we assume with higher misclassification cost <ref type="bibr" target="#b15">[16]</ref>.</p><p>Alternatively, several assessment metrics are frequently adopted in the research community for learning problems with imbalanced data sets. In this work we use the G-mean as the main performance metric. This measure is computed as the geometric mean of the true positive and true negative rates:</p><formula xml:id="formula_0">G À mean ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi TP TP þ FN Ã TN TN þ FP r<label>ð1Þ</label></formula><p>where TP = true positives, TN = true negatives, FP = false positives, and FN = false negatives. Another metric commonly used to assess classification models is the Area under the curve (AUC), see <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related work on classification and feature selection with Support Vector Machines</head><p>This section lays the foundation for the posterior developments by first recalling the SVM approach for binary classification. The subsequent subsection presents related work for feature selection with SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Binary classification with Support Vector Machines</head><p>Given training vectors x i 2 R n ; i ¼ 1; . . . ; m and a vector of binary class labels y 2 R m ; y i 2 À1; þ1 f g ; i ¼ 1; . . . ; m, linear SVM determine the optimal hyperplane f ðxÞ ¼ w T Á x þ b that aims to separate the training patterns according to their classes while achieving best generalization performance for new instances. In order to obtain this optimal hyperplane, SVM maximize its margin, which is the sum of the distances to the closest positive and negative training patterns, and is equivalent to minimizing the norm of w <ref type="bibr" target="#b35">[36]</ref>. Since a perfect separation of all training patterns is not always possible, a slack variable n i is introduced for each training vector x i ; i ¼ 1; . . . ; m and C is a parameter that penalizes the overall training error <ref type="bibr" target="#b35">[36]</ref> as shown in <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_1">min w;b;n 1 2 kwk 2 þ C X m i¼1 n i s:t: y i Á ðw &gt; Á x i þ bÞ P 1 À n i ; i ¼ 1; . . . ; m;<label>ð2Þ</label></formula><formula xml:id="formula_2">n i P 0; i ¼ 1; . . . ; m:</formula><p>In binary classification of imbalanced data sets the case of primary interest is the one where the target (positive) class, labeled þ1, is much smaller than the background (negative) class, labeled À1.</p><p>To achieve a nonlinear classifier based on SVM, the solution will be given by a kernel machine</p><formula xml:id="formula_3">f ðxÞ ¼ sign X m i¼1 y i a Ã i Kðx; x i Þ þ b Ã !<label>ð3Þ</label></formula><p>where training data are mapped to the higher dimensional space H by the function / : x ! /ðxÞ 2 H. The mapping can be represented by a kernel function Kðx; yÞ ¼ /ðxÞ Á /ðyÞ which defines an inner product in H and should satisfy Mercer's condition <ref type="bibr" target="#b35">[36]</ref>. The optimal separating hyperplane in H is the hyperplane with maximal distance to the closest image /ðx i Þ from the respective training pattern. The corresponding dual formulation can be stated as follows:</p><formula xml:id="formula_4">max a X m i¼1 a i À 1 2 X m i;s¼1 a i a s y i y s Kðx i ; x s Þ s:t: X m i¼1 a i y i ¼ 0;<label>ð4Þ</label></formula><formula xml:id="formula_5">0 6 a i 6 C; i ¼ 1; . . . ; m:</formula><p>We base our analysis on the Gaussian kernel, which has led to the best results in many applications and is a common choice in the literature <ref type="bibr" target="#b19">[20]</ref>. It has the following form.</p><formula xml:id="formula_6">Kðx i ; x s Þ ¼ exp À jjx i À x s jj 2 2r 2 !<label>ð5Þ</label></formula><p>where r &gt; 0 is the parameter controlling the kernel width.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Related work on feature selection with SVM</head><p>According to <ref type="bibr" target="#b13">[14]</ref>, three main approaches have been developed for feature selection: filter, wrapper, and embedded methods. The first approach (filter methods) uses statistical properties of the features in order to filter out poorly informative ones, before applying any classification algorithm. Commonly used filter methods are the v 2 statistic, which measures the independence between the distribution of values and classes <ref type="bibr" target="#b34">[35]</ref>; the Information Gain, which is a measure of informational entropy, to the problem of deciding how important a given feature is <ref type="bibr" target="#b34">[35]</ref>, and the Fisher Criterion Score (F), which computes each feature's importance independently of the other features by calculating the difference of that feature's mean values for the two classes <ref type="bibr" target="#b13">[14]</ref>, as shown in <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_7">FðjÞ ¼ l þ j À l À j ðr þ j Þ 2 þ ðr À j Þ 2<label>ð6Þ</label></formula><p>where l þ j (l À j ) represent the mean for the j-th feature in the positive (negative) class and r þ j (r À j ) is the respective standard deviation ðj ¼ 1; . . . ; nÞ.</p><p>Many successful applications of this measure have been reported in literature; see e.g. <ref type="bibr" target="#b10">[11]</ref> where based on the Fisher score a multi-class SVM has been proposed to classify face images. In <ref type="bibr" target="#b38">[39]</ref> this score has been used successfully for text classification.</p><p>Wrapper methods explore the whole set of features to score feature subsets according to their predictive power, which is computationally demanding, but often provides more accurate results than filter methods. Common wrapper strategies are Sequential Forward Selection (SFS) and Sequential Backward Elimination (SBE) <ref type="bibr" target="#b13">[14]</ref> which evaluate the selected feature sets based on a particular classification method. In the first case, starting without any variable, the method tries out the variables one by one and includes in each iteration the most relevant of the remaining ones. On the other hand, SBE starts with all candidate features and tests them one by one for statistical significance. In each iteration the least significant feature is eliminated.</p><p>The third class of feature selection techniques (embedded methods) determines the respective feature subset during classifier construction, which can be seen as a search in the combined space of feature subsets and hypotheses. Similar to wrapper methods, embedded approaches are specific to a given classification method and therefore have the advantage to include the interaction with the classifier when modeling feature dependencies. They are, however, computationally less intensive than wrapper methods <ref type="bibr" target="#b13">[14]</ref>.</p><p>One popular method, which is relevant for the remainder of this paper, is known as Recursive Feature Elimination (RFE-SVM) <ref type="bibr" target="#b14">[15]</ref>. The goal of the linear version of this approach is to find a subset of size r among n variables (r &lt; n), eliminating those features whose removal lead to the largest margin of class separation. This can be achieved using backward elimination, based on the components of the weight vector w.</p><p>The linear RFE-SVM approach presented in <ref type="bibr" target="#b14">[15]</ref> can be also generalized to the nonlinear case which will be presented next and is the basis for our subsequent development. Since the margin of the separating hyperplane is inversely proportional to the Euclidean norm of the weight vector w, this norm can be rewritten in terms of the dual variables of the SVM model:</p><formula xml:id="formula_8">W 2 ðaÞ ¼ X m i;s¼1 a i a s y i y s Kðx i ; x s Þ<label>ð7Þ</label></formula><p>The feature to be removed in each iteration is the one whose removal minimizes the variation of W 2 ðaÞ. While one could choose a single variable to be removed in each iteration, this would be inefficient in many high-dimensional applications (e.g. classifying microarray data) where data sets are often characterized by thousands of features, and the authors usually remove half of the variables in each step <ref type="bibr" target="#b14">[15]</ref>. Successful applications of RFE-SVM e.g. for analyzing EEG-signals have been reported in <ref type="bibr" target="#b16">[17]</ref> where the authors confirm that this method could provide accurate and useful information to aid medical diagnosis.</p><p>Embedded feature selection can also be seen as an optimization problem. This is generally done through enforcing feature selection on the model parameters directly, considering a sparsity term in the objective function. For instance, the Euclidean norm from the classical SVM (Formulation (2)) can be replaced by the l 1 norm XðwÞ ¼</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P</head><p>i jw i j, as presented in the approach l 1 Support Vector Machine (l 1 -SVM) by Bradley and Mangasarian <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_9">min w;b;n X n j¼1 jw j j þ C X m i¼1 n i s:t: y i Á ðw T Á x i þ bÞ P 1 À n i ; i ¼ 1; . . . ; m;<label>ð8Þ</label></formula><formula xml:id="formula_10">n i P 0; i ¼ 1; . . . ; m:</formula><p>An alternative sparsity term is the minimization of the ''zero norm'': XðwÞ ¼ kwk 0 ¼ jfi : w i -0gj. Note that, unlike the l 1 norm, k Á k 0 is not a norm because the triangle inequality does not hold <ref type="bibr" target="#b5">[6]</ref>. Weston <ref type="bibr" target="#b40">[41]</ref> proposed an approach for ''zero-norm'' minimization (l 0 -SVM) by iteratively scaling the variables, multiplying them by the absolute value of the weight vector w obtained from the SVM formulation, until convergence. Variables can be ranked by removing those features whose weights become zero during the iterative algorithm and computing the order of removal. This method considers the following approximation of the l 0 norm:</p><formula xml:id="formula_11">XðwÞ ¼ X n j¼1 logð þ jw j jÞ:<label>ð9Þ</label></formula><p>where is a small positive constant.</p><p>Filter and resampling methods have been used together with positive results in imbalanced data sets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>. Filter techniques do not require further adaptations when facing imbalanced data sets and are compatible to data resampling since the process of feature filtering is independent of the learning process. Some specific filter approaches have been proposed to address the class-imbalance problem directly: FAST <ref type="bibr" target="#b9">[10]</ref> performs feature filtering based on the Area under the curve (AUC), and DBFS <ref type="bibr" target="#b0">[1]</ref>, which considers Information Gain as an alternative for AUC.</p><p>To the best of our knowledge, neither wrapper nor embedded methods have been proposed to treat the class imbalance problem in high-dimensional applications using Support Vector Machines. Fernandez et al. <ref type="bibr" target="#b12">[13]</ref> applied a genetic algorithm to improve the performance of a fuzzy rule-based classification system for imbalanced data sets, but without selecting relevant features. Villar et al. <ref type="bibr" target="#b36">[37]</ref> also proposed a genetic algorithm that, in their case, simultaneously performs feature selection and granularity learning for fuzzy rule-based classification systems for imbalanced data sets. This approach removes irrelevant variables through a backward elimination process using an AUC-based fitness function. This method achieved good results under imbalanced class conditions, but only for low-dimensional data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed feature selection methods for imbalanced data sets</head><p>We propose a family of embedded methods for backward feature selection using Support Vector Machines which are inspired by the backward elimination procedure of SVM-RFE <ref type="bibr" target="#b14">[15]</ref>, as presented in Section 3.2. The rationale behind our approach is that we eliminate those features whose removal has less impact on the final solution, considering a classimbalanced problem. To perform this, we attempt at recreating the main goal for this task: to achieve the best predictive performance in an unseen subset, using a cost-sensitive metric.</p><p>Section 4.1 presents our base algorithm for backward feature elimination. The holdout scheme proposed in Section 4.2 shows how an advanced split of training sets can improve the respective results derived from the base algorithm. Section 4.3 illustrates how SMOTE can be used within the two previously presented methods in order to treat very highly imbalanced data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Base algorithm for backward elimination</head><p>In this subsection we introduce our base algorithm and study different variations, for example by using different loss functions. Following the notation used by Song et al. <ref type="bibr" target="#b30">[31]</ref>, we denote by S the full set of features. We want to find a subset K ðK # SÞ of features, such that the performance of the SVM classifier using this subset's features is maximized, considering a training set T .</p><p>In order to compare the performance of different feature selection strategies, we will generate a vector of features S y sorted in increasing degree of relevance. This is achieved by the following iterative algorithm: S S n I 6.</p><p>S y ðS y ; IÞ 7. until S ¼ ;</p><p>In Step 4 the algorithm determines a set I of features to be eliminated. While one could choose a single element of S, this would be inefficient when there is a large number of irrelevant features. On the other hand, removing too many features at once increases the risk to loose relevant features <ref type="bibr" target="#b14">[15]</ref>. In our experiments, we found a good compromise between speed and feature quality was to remove 10% of the current features at every iteration.</p><p>Since our proposed embedded approach takes explicitly into account feature performance within a set of remaining features at any iteration, the problems typically related to model-free feature screening, do not occur. These problems are (1) any irrelevant feature that is highly correlated with the set of relevant features could be selected and (2) a marginally uncorrelated feature that is jointly correlated with the response might not be selected; see <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref> for a more general discussion of feature screening.</p><p>Different loss functions can be considered within Algorithm 2. In our experiments we used the following ones:</p><p>Standard 0-1 loss function: This measure is based on the total number of errors in the training set. It assumes equal cost for both errors and therefore it is not suitable for imbalanced data sets. Using this loss function we want to prove that feature relevance should be measured considering different costs for different types of errors. Formally, given a solution ða; bÞ, obtained by applying SVM to a training set T , and a set of available features S, the 0-1 loss for a given feature j 2 S is defined as:</p><formula xml:id="formula_12">LOSS 0À1 ðða; bÞ; S n fjg; T Þ ¼ X s2T y s À sgn X i2T a i y i Kðx ðÀjÞ i ; x ðÀjÞ s Þ þ b !<label>ð10Þ</label></formula><p>where</p><formula xml:id="formula_13">x ðÀjÞ i</formula><p>is the training object i with feature j removed and sgn is the signum function. Balanced loss function: In order to make the errors from an imbalanced data set comparable, we use the balanced loss function, which takes the weighted average of Type I and Type II errors. We split the training set T into T þ and T À , containing the positive and negative instances, respectively. The balanced loss for a given feature j 2 S, is defined as: LOSS bl ðða; bÞ; S n fjg; T Þ ¼</p><formula xml:id="formula_14">P s2T À y s À sgn P i2T À a i y i Kðx ðÀjÞ i ; x ðÀjÞ s Þ þ b jT À j þ P s2T þ y s À sgn P i2T þ a i y i Kðx ðÀjÞ i ; x ðÀjÞ s Þ þ b jT þ j<label>ð11Þ</label></formula><p>where jT þ j (jT À j) represents the cardinality of the training set T þ (T À ), i. e. the number of positive (negative) instances. Predefined loss function: A predefined loss function can be used if the relation between the costs of Type I and Type II errors can be estimated for a given application. Since the scope of this work is mainly methodological, we will not consider this measure, but it can be useful in particular applications with imbalanced data sets, such as churn prediction or credit scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Holdout strategy for backward feature elimination</head><p>We propose an additional modification to the backward elimination process, which is the assessment of each attribute's contribution in an ''unseen'' subset of the training data, instead of training the model and constructing the loss function with the same data set. To do so, the training data T is further splitted into a subset TR where SVM is trained, and a validation set V used to compute the respective loss function <ref type="bibr" target="#b19">[20]</ref>. This is achieved by adding a holdout strategy to Algorithm 2, leading to the following algorithm. </p><formula xml:id="formula_15">; x vðÀjÞ l Þ þ b !<label>ð12Þ</label></formula><p>LOSS bl ðða; bÞ; S n fjg; TR; VÞ ¼</p><formula xml:id="formula_16">P l2V À y v l À sgn P i2TR À a i y i Kðx ðÀjÞ i ; x vðÀjÞ l Þ þ b jT À j þ P l2V þ y v l À sgn P i2TR þ a i y i Kðx ðÀjÞ i ; x vðÀjÞ l Þ þ b jT þ j<label>ð13Þ</label></formula><p>where V is the Validation subset and x v l and y v l are this subset's objects and labels, respectively. x vðÀjÞ l means validation object l with feature j removed.</p><p>One of the problems when applying this algorithm is that different runs may lead to different results in terms of selected features, in particular with few training examples and in the presence of imbalanced data sets. A more stable selected feature subset can be achieved by performing k different holdout splits and averaging the loss functions before eliminating features. The attributes whose elimination leads to a better predictive performance according to this average will be added to the ordered list of features S y in Step 4. In our experiments we use k ¼ 5, and the influence of this parameter is further discussed in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Backward feature elimination with SMOTE</head><p>Although the previous algorithms are designed to perform an adequate feature selection under conditions of imbalanced data sets, backward embedded methods require a base classifier (in this case SVM) that successfully discriminates between the two classes. In cases of highly imbalanced and overlapping classes, data resampling may be useful to achieve an adequate classification performance. For the particular case of microarray data, undersampling may not be the right procedure since only few examples are available. In order to overcome this problem, we may generate artificial target class instances using the oversampling method SMOTE on the training set. This procedure allows a better split of the training data set and makes some of the extreme cases of imbalanced data sets tractable. We propose to use SMOTE for the base algorithm Backward Feature Elimination (Algorithm 2) as well as for the base algorithm with holdout strategy (Algorithm 3), completing our proposed family of methods for feature selection based on SVM for imbalanced data sets. The respective algorithms, i.e. Algorithms 4 and 5 follow.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results and discussions</head><p>We applied the proposed approaches for feature selection on six DNA microarray data sets which have already been used for benchmark feature selection algorithms (e.g. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42]</ref>). We compared our proposals for feature selection with the alternative methods such as Fisher Score, l 1 -SVM, l 0 -SVM, and SVM-RFE (for both, linear as well as nonlinear classifiers).</p><p>We first show the data sets in Section 5.1, while Section 5.2 presents a summary of the results obtained for the proposed approaches, considering linear and Gaussian kernels, as well as SMOTE oversampling. The respective details are provided in Appendix A. Finally, a sensitivity analysis of the different parameters and a further discussion from the results is presented in Section 5.3. The lung cancer data set contains the gene expression of 181 samples (31 malignant and 150 normal) described by 12533 features <ref type="bibr" target="#b2">[3]</ref>. We compared our results using the following number of features: 20, 50, 100, 250, 1000, 2000, 4000, and 12,533 (i.e. no features removed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">GLIOMA data set</head><p>The GLIOMA data set contains 50 instances described by 4433 genes in four classes: cancer glioblastomas, non-cancer glioblastomas, cancer oligodendrogliomas, and non-cancer oligodendrogliomas <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42]</ref>. To adapt this data set to binary classification we studied class cancer oligodendrogliomas (seven instances) versus the rest, using the following number of features: 20, 50, 100, 250, 1000, 2000, 4433 (i.e. no features removed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">SRBCT data set</head><p>The SRBCT data set contains 83 samples in four classes: Ewing family of tumors, Burkitt lymphoma, neuroblastoma, and rhabdomyosarcoma <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42]</ref>. Each sample contains 2308 genes. To adapt this data set to binary classification we studied class Burkitt lymphoma (eleven instances) versus the rest, using the following number of features: 20, 50, 100, 250, 1000, 2308 (i.e. no features removed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.">LUNG data set (LUNG2)</head><p>The LUNG2 data set contains 203 samples described by 3312 genes in five classes: adenocarcinomas, squamous cell lung carcinomas, pulmonary carcinoids, small-cell lung carcinomas, and normal lung <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref>. To adapt this data set to binary classification we studied class small-cell lung carcinomas (20 instances) versus the rest, using the following number of features: 20, 50, 100, 250, 1000, 2000, 3312 (i.e. no features removed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5.">CAR data set</head><p>The CAR data set contains 174 samples described by 9182 genes in eleven classes: prostate, bladder/ureter, breast, colorectal, gastroesophagus, kidney, liver, ovary, pancreas, lung adenocarcinomas and lung squamous cell carcinoma <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">42]</ref>. To adapt this data set to binary classification we studied class kidney (eleven instances) versus the rest, using the following number of features: 20, 50, 100, 250, 1000, 2000, 4000 and 9182 (i.e. no features removed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.6.">Bullinger data set (BULL)</head><p>The Bullinger data set <ref type="bibr" target="#b6">[7]</ref> stems from a study for Adult Acute Myeloid Leukemia classification. The preprocessed data contains the gene expression of 94 samples (4 with preceding malignancy and 90 without preceding malignancy) described by 17,404 features. We compared our results using the following number of features: 20, 50, 100, 250, 1000, 2000, 4000, 8000, and 17,404 (i.e. no variables removed).</p><p>Table <ref type="table" target="#tab_3">1</ref> summarizes the relevant information for each benchmark data set.</p><p>For model evaluation we chose leave-one-out (LOO) cross-validation, since all six data sets have relatively few instances, as is usually the case for microarray data sets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref>. In each iteration of the LOO approach, feature selection is performed in the training set, ranking the features according to their respective contribution measures. Several SVM classifiers are finally constructed for an increasing number of ranked features, evaluating their performance with the test data. The classification performance (accuracy and gmean, see Section 2.4) is finally computed by averaging the test results.</p><p>In order to study the classification performance of the proposed linear feature selection approaches we compared the results for a given number of features with different feature selection algorithms. For linear SVM we used a penalty parameter C ¼ 1. For nonlinear SVM classifiers with Gaussian kernel we used C ¼ 1000 and r ¼ 3, as suggested in Rakotomamonjy <ref type="bibr" target="#b26">[27]</ref>. We present the results for the following proposed feature selection models. BFE À SVM 0À1 : BFE-SVM for feature ranking (Algorithm (2)) using linear kernel and 0-1 loss function <ref type="bibr" target="#b9">(10)</ref>. BFE À SVM bl : BFE-SVM for feature ranking (Algorithm (2)) using linear kernel and balanced loss function <ref type="bibr" target="#b10">(11)</ref>. HO À BFE 0À1 : HO-BFE for feature ranking (Algorithm (3)) using linear kernel and 0-1 loss function <ref type="bibr" target="#b11">(12)</ref>. HO À BFE bl : HO-BFE for feature ranking (Algorithm (3)) using linear kernel and balanced loss function <ref type="bibr" target="#b12">(13)</ref>. BFE À SVM 0À1k : BFE-SVM for feature ranking (Algorithm (2)) using Gaussian kernel and 0-1 loss function <ref type="bibr" target="#b9">(10)</ref>. BFE À SVM blk : BFE-SVM for feature ranking (Algorithm (2)) using Gaussian kernel and balanced loss function <ref type="bibr" target="#b10">(11)</ref>. HO À BFE 0À1k : HO-BFE for feature ranking (Algorithm (3)) using Gaussian kernel and 0-1 loss function <ref type="bibr" target="#b11">(12)</ref>. HO À BFE blk : HO-BFE for feature ranking (Algorithm (3)) using Gaussian kernel and balanced loss function <ref type="bibr" target="#b12">(13)</ref>. All previous approaches considering SMOTE resampling over the training subset.</p><p>Additionally, the following well-known feature selection algorithms are applied for comparison purposes. l 1 -SVM r : l 1 -SVM for feature ranking (Formulation (8)), using standard SVM as classifier (Formulation (2)). SVM À RFE l : SVM-RFE for feature ranking (linear kernel, Algorithm (1)), using standard SVM as classifier (Formulation (2)). l 0 -SVM: l 0 -SVM for feature ranking (Formulation (9)), using standard SVM as classifier (Formulation (2)). Fisher + SVM: Fisher Score for feature ranking (Formulation (6)), using standard SVM as classifier (Formulation (2)). SVM À RFE nl : SVM-RFE for feature ranking (Gaussian kernel, Algorithm (1)), using kernel-based SVM as classifier (Formulation (4)). All previous approaches considering SMOTE resampling over the training subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Summary of classification results for imbalanced microarray datasets</head><p>In this section, we present a summary of the results obtained from our experiments; the details are provided in Appendix A. In order to assess the best performance and the stability of the respective approaches, we provide the maximum and average leave-one-out (LOO) gmean values for all predefined subsets of attributes for all six microarray datasets; see Table <ref type="table" target="#tab_6">2</ref>.</p><p>According to the results presented in Table <ref type="table" target="#tab_6">2</ref>, the approaches from our proposed family of methods provide best overall performance in four out of six data sets, and in the remaining two the performance is similar compared to the best alternative approaches. Within the proposed strategies, the Holdout approaches tend to perform better than the basic backward approach that considers only the training samples. The balanced loss function outperforms the standard 0-1 loss function on imbalanced data sets, with the only exception of the GLIOMA dataset.</p><p>Another important result is that in general there is a low influence of data resampling on classification performance. Only in the BULL data set resampling was relevant, achieving 69.9% gmean with linear HO À BFE bl for a data set that was not possible to shatter. We conclude that in extremely imbalanced and overlapping data sets, SMOTE can be very useful to improve the classifier's performance, but it does not provide a significant improvement otherwise.</p><p>Excluding data sets where a classifier using all available features (i.e. no feature selection) obtains already very high classification performance, an adequate feature selection significatively improves predictive performance.</p><p>It may sound counterintuitive that less information is actually better than using all variables. However, several researchers have already proved the advantages of feature selection to mitigate the curse of dimensionality <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. This is especially true for microarray data sets where the importance of feature selection has been emphasized (see e.g. <ref type="bibr" target="#b30">[31]</ref>) since usually only very few observations can be obtained that are typically described by a huge number of genes (features).</p><p>Another important observation is that linear feature selection methods perform consistently better than kernel-based approaches. We infer that the difficulty of setting the right combination of parameters C and r without using a time consuming crossvalidation strategy (which may also lead to overfitting) explains this loss in performance. For this work, we set a fixed value of C for linear methods and a fixed tuple ðC; rÞ for kernel-based methods, as has been proposed in the literature (see, for example, <ref type="bibr" target="#b26">[27]</ref>). Empirically, we observe a strong relationship between the number of variables in the solution and the optimal value for r (an SVM classifier normally requires a larger r when the number of features increases <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>), and therefore a model selection procedure for each number of ranked features is recommended. This topic is further discussed in the following subsection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Sensitivity analysis of the relevant parameters and discussion</head><p>In this subsection we study the performance of the proposed methodologies by performing sensitivity analysis of the relevant parameters, characterizing their influence on the final solution. We also discuss issues related to stability, the presence of redundant variables in the final solution, and computational time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Sensitivity analysis</head><p>To illustrate the influence of the parameters on the classifier, we varied C; r (kernel-based approaches) and the percentage of variables removed at every iteration of the algorithm considering the GLIOMA data set. This data set was selected because it has the highest variance in the results, and therefore it is adequate to illustrate difference between models. Table <ref type="table" target="#tab_4">3</ref> presents the results of varying C along the values C 2 f2 À3 ; 2 À2 ; . . . ; 2 4 ; 2 5 g, for all proposed linear feature selection methods and eliminating 10% of the attributes at each iteration. Subsequently, Table <ref type="table" target="#tab_5">4</ref> presents the results of varying r along the values r 2 f3 1 ; 3 2 ; . . . ; 3 5 g, for all proposed nonlinear feature selection methods, for a fixed value of C (C ¼ 1000) and eliminating 10% of the attributes at every iteration. Finally, Table <ref type="table" target="#tab_7">5</ref> presents the results of varying the percentage of variables eliminated at every iteration of the best approach, along 10%, 20%, 30%, 40%, and 50%. In all cases, we present the average and maximum performance (in terms of gmean) for all subsets of selected features.</p><p>From Table <ref type="table" target="#tab_4">3</ref> we observe that results are very stable along the values above 1. The difference between all values are not significant according to an ANOVA test (p value 0.848). In contrast, in Table <ref type="table" target="#tab_5">4</ref> we observe that prediction performance can be strongly affected by the choice of the kernel, and the range of proper values is very small. Additionally, the choice of the kernel depends on the number of selected attributes: the kernel width should start with high values and decrease together with the number of selected features. Finally, from Table <ref type="table" target="#tab_7">5</ref> we observe that predictive performance slightly decreases when more attributes are removed simultaneously, as the intuition suggests, but these differences are not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Stability</head><p>To study the method's stability regarding the holdout step, different runs of the algorithm were considered for both variants: one holdout step and five holdout steps. For all runs we consider the same parameters (linear kernel with C ¼ 1 and 10% of feature elimination). Applying an ANOVA test, we detect non-significant differences for all methods (p ¼ 0:588 for HO À BFE 0À1 with one holdout step, p ¼ 0:117 HO À BFE bl with one holdout step, p ¼ 0:959 for HO À BFE 0À1 with five holdout steps, and p ¼ 0:353 for HO À BFE bl with five holdout steps), but the methods based on 5 holdout steps are statistically more stable than the ones based on one holdout step.</p><p>Another possible issue is that the approach is order-dependent in case of similar values of the loss function. The percentage of ''ties'' in the values of the loss function can be computed for all methods to assess the influence of the order in which the variables are presented. Averaging along all subsets of features, we obtain 90.0% of ''ties'' for BFE À SVM 0À1 , 72.7% for BFE À SVM bl , 73.0% for HO À BFE 0À1 , and 0% for HO À BFE bl . We conclude that the holdout steps and the use of the balanced loss function effectively mitigates the risk of eliminating relevant features given by order-dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Redundant variables</head><p>Our next experiment explores the appearance of (highly) redundant variables in the solution. Wrapper and embedded methods take the relationship between variables, and therefore they should be more effective in identifying and eliminating redundancy. In contrast, if several attributes are simultaneously removed at every iteration, this may result in a higher risk of removing relevant attributes. For instance, when the weight vector is used to eliminate variables, relevant but redundant variables may share weight in the hyperplane and be removed together. To reduce this risk, our approach attempts at recreating the main goal for this task: to achieve the best predictive performance in an ''unseen'' subset, using a cost-sensitive metric. Empirically, we proved that this strategy correctly identifies those attributes that are relevant to discriminate in a class-imbalanced problem, given the results presented in previous section. To demonstrate that our approach also correctly identifies those irrelevant and redundant attributes, we studied the percentage of highly redundant attributes (Pearson's q &gt; 0:85) for all methods and for all different subset of selected features. These results are presented in Table <ref type="table" target="#tab_8">6</ref>. According to Table <ref type="table" target="#tab_8">6</ref>, Fisher Score has a significantly higher percentage of redundant variables, compared to all other wrapper and embedded approaches. The difference between these approaches is not significant, although RFE-SVM and BFE À SVM bl are the best approaches in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">Computational time</head><p>Table <ref type="table" target="#tab_9">7</ref> reports the average CPU times of one fold for all algorithms, considering similar parameters (ranking construction and SVM classification). For these experiments we use an Intel i7-3630QM system with CPU at 2.4 GHz and 16 GB of RAM.</p><p>From Table <ref type="table" target="#tab_9">7</ref> we observe that the running times for all proposed algorithms are relatively similar, with the only exception of the HO-BFE version, whose running time is about five times higher, as expected, since it performs five repetitions of the algorithm for stability reasons. Fisher Score is the fastest approach, followed by the l 0 -SVM. There is no significant difference between the use of different contribution metrics nor the use of a holdout step versus using the entire training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5.">Conclusions from sensitivity analysis</head><p>We draw the following conclusions from these experiments:</p><p>Results obtained by varying parameter C are very stable above C ¼ 1, and therefore it can be fixed to a sufficiently large value during the experimental framework.</p><p>In contrast, kernel-based methods behave very unstable in terms of performance. The main reason for this is the difficulty in selecting the optimum value for parameter r. The right r value seems to be strongly dependent to the number of selected features for the classification task. To achieve better performance with kernel methods it would be necessary to perform a grid search at each step of the backward algorithm, leading to a very time-consuming validation strategy.</p><p>The holdout step provides a framework that adequately emulates the ultimate goal of feature selection, which is selecting those features that are relevant for predicting new data, resulting in an important gain in terms of performance. This method, however, requires the average of several holdout steps in order to assure stable results. This fact is particularly important in microarray applications, where only limited number of training samples are available. Empirical results demonstrate the effectiveness and stability of this approach, which can be performed in tractable running times. The correct identification of redundant attributes is an important topic in high-dimensional applications, such as classification of microarray data. Backward approaches with batch elimination of attributes have a high risk of eliminating relevant variables at the first iterations, leading to poor predictive results. Furthermore, filter approaches that do not take into account correlations between attributes may include a high percentage of redundant variables in their final selection. We empirically assess both approaches, by first varying the percentage of batch elimination in our investigation (Table <ref type="table" target="#tab_7">5</ref>), and second computing the percentage of highly redundant attributes selected by each method (Table <ref type="table" target="#tab_8">6</ref>). The above-mentioned sensitivity analysis confirms the good results obtained in terms of performance by our approach varying the batch elimination criterion, finding a good trade off between performance and running time around 10-20%. Given these results we conclude that the method successfully finds those attributes that are relevant to construct the hyperplane of the respective classifier. We also confirm that embedded approaches are more effective at eliminating redundancy than filter methods, according to Table <ref type="table" target="#tab_8">6</ref>. Regarding computational complexity of our approaches, the backward elimination process is similar to RFE-SVM, differing only in the holdout step and the computation of the contribution metrics. Computationally speaking, these two steps are significantly less expensive than the backward elimination process. The complexity of RFE-SVM is of the order of maxðn; mÞm 2 considering the operations of SVM (computation of the kernel matrix and its inversion) and the successive iterations with a decreasing number of features (assuming a reduction by a factor of 2 at each iteration) <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>We introduced a family of methods based on a backward elimination approach for feature ranking and embedded classification using Support Vector Machines, which has been adapted to select those attributes that are relevant to discriminate between classes under imbalanced data conditions.</p><p>Four different strategies were designed to accomplish this objective: We combined two different backward elimination strategies, one based on training performance (BFE-SVM) and another based on predictive performance via successive holdout steps (HO-BFE). We applied two different evaluation measures, a standard 0-1 loss function and the balanced loss function to explicitly favor those attributes whose elimination leads to less errors in the minority class. Our approaches present the following advantages, based on a comparison with other feature selection approaches for SVM in high-dimensional applications with imbalanced datasets (such as microarray datasets). The proposed approaches outperform other feature ranking techniques in terms of predictive performance for different SVM-based feature selection techniques, achieving particularly good results on highly imbalanced data sets, based on their ability to identify irrelevant variables using the classifier and minimizing the number of errors in the minority class, which is assumed to have a higher cost. The proposed methods allow for the explicit incorporation of misclassification costs in the assessment of each attribute's contribution, leading to a feature selection process especially designed for a particular application. Our strategies are very flexible and allow the use of different kernel functions for nonlinear feature selection and classification using SVM. Furthermore, they can also be generalized to various classification methods, other than SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. of Selected Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LOOCV GMEAN</head><p>In Section 5 we proposed an experimentation setup to avoid an uneven comparison between linear and nonlinear methods, considering similar efforts in both cases. Although no significant gain was obtained in our experiments by using kernel methods, the proposed strategies have the potential to achieve better results under a more exhaustive model selection, by embedding the feature selection process. Given the good results obtained while combining SVM for feature ranking with the same approach for classification, we suggested performing a grid search for parameters C and r in each iteration of the algorithm. This was through monitoring the performance in order to obtain a final solution along the process, without considering feature selection and classification as independent problems. According to our results, the success of a nonlinear backward elimination strategy is strongly dependent on the right definition of the decision boundary, and in particular on the correct setting of the parameter r for high-dimensional applications.</p><p>The experimentation procedure also allows the comparison with feature selection approaches that are not explicitly adapted to imbalanced data sets. According to our results, the adapted version of HO-BFE using balanced loss leads to consistently better results in terms of accuracy and gmean compared to alternative feature selection approaches, resulting in an attractive alternative for high-dimensional problems with imbalanced data sets. The main drawback, however, is the higher running time of the algorithm when performing different holdout partitions and averaging them, instead of using only the training sample. However, the predictive performance of this procedure is significantly higher in most cases. Furthermore, the order of the algorithm for BFE-SVM with balanced loss is exactly the same as that of RFE-SVM, and it may lead to better results by defining an adequate loss function for classification performance with imbalanced data sets. Another important conclusion was the need for an intelligent oversampling in extreme cases of class imbalance and overlap, in which no adequate classifier can be found, since embedded and wrapper feature selection strongly depend on the classification method. In other cases SMOTE oversampling did not improve the solutions found without data resampling.</p><p>There are several opportunities for future work. First, classification of imbalanced data sets can be performed by using one-class SVM approaches, such as SVDD (Support Vector Data Description <ref type="bibr" target="#b33">[34]</ref>). We are currently working on embedded feature selection strategies for this approach in order to identify those features that are relevant in constructing the data description. Secondly, the costs of Type I and Type II errors may be different and the balanced loss function could be adapted to different profit or cost functions in a particular domain. Credit scoring, fraud detection, and churn prediction are some examples of applications with imbalanced data sets where a profit function can be constructed to evaluate both the performance of the classifier and the relevance of its features. In applications where high correlation among sets of features is to be expected, simultaneous feature grouping and selection as suggested e.g. by <ref type="bibr" target="#b43">[44]</ref> could be an interesting approach to improve problem understanding and classifier performance. Along the same line it would be interesting in a given application to have a domain expert interpreting the selected features and establishing their relevance in the particular case. where RFE-SVM with Gaussian kernel achieved the best performance. All methods behave similarly when using 500 variables or more. No improvements are obtained when using SMOTE (see Fig.</p><p>A.2(b)), while some approaches worsen their performance significantly. For the SRBCT data set all linear methods achieve very good and stable results, while kernel approaches failed at identifying relevant attributes in some cases; see A.3(a). Best performance is obtained by our approach BFE-SVM with balanced loss and RFE-SVM with linear kernel. Results are very similar when using SMOTE oversampling; see Fig. <ref type="bibr">A.3(b)</ref>.</p><p>A similar situation can be observed for LUNG2 (Fig. <ref type="bibr">A.4)</ref>, where perfect results are achieved by the approaches HO-BFE with balanced loss and RFE-SVM with linear kernel. Kernel approaches fail at correctly identifying relevant features, especially when using SMOTE oversampling. Here we observe similar performance for all approaches when using 500 attributes or more, and then our approaches HO-BFE with balanced loss (best proposed linear method) HO-BFE with 0-1 loss (best kernel-based method) and outperform alternative feature selection strategies, improving classification performance significantly. Once again, no significant improvement is achieved when using SMOTE resampling, as shown in Fig. <ref type="bibr">A.5(b)</ref>.</p><p>Results for the BULL data set, the one with the highest imbalance ratio, are presented in Fig. A.6. For the case without resampling, no approach is able to perform feature selection and classification adequately, and the classifiers tend to predict all instances to the majority class. SMOTE helped to find discriminative classifiers with an adequate balanced classification performance with fewer features for the proposed approaches only. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 2 .</head><label>2</label><figDesc>Backward Algorithm for Feature Elimination (BFE-SVM)Input: The original set of features S Output: An ordered vector of features S y 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 4 .a</head><label>4</label><figDesc>Backward Algorithm for Feature Elimination using SMOTE Input: The original set of features S Output: An ordered vector of features S y 1. SVM Training using T 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5. 1 .</head><label>1</label><figDesc>Description of data sets and validation procedure 5.1.1. Lung cancer data set (LUNG)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. A. 1 .</head><label>1</label><figDesc>Fig. A.1. GMEAN versus the number of ranked variables for different feature selection approaches. LUNG dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. A. 2 .</head><label>2</label><figDesc>Fig. A.2. GMEAN versus the number of ranked variables for different feature selection approaches. GLIOMA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. A. 3 .</head><label>3</label><figDesc>Fig. A.3. GMEAN versus the number of ranked variables for different feature selection approaches. SRBCT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. A. 5 (</head><label>5</label><figDesc>a) presents the results for the CAR data set without resampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. A. 5 .Fig. A. 6 .</head><label>56</label><figDesc>Fig. A.5. GMEAN versus the number of ranked variables for different feature selection approaches. CAR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 3. Holdout Algorithm for Backward Feature Elimination (HO-BFE)We redefine the loss measures used in Algorithm 3:</figDesc><table><row><cell cols="4">Input: The original set of features S</cell></row><row><cell cols="4">Output: An ordered vector of features S y</cell></row><row><cell>1. S y</cell><cell>;</cell><cell></cell></row><row><cell cols="2">2. repeat</cell><cell></cell></row><row><cell>3.</cell><cell cols="2">ðTR; VÞ</cell><cell>Holdout using T</cell></row><row><cell>4. 5.</cell><cell cols="3">a SVM Training using TR I argmin I P j2I LOSS ða; S n fjg; TR; VÞ; I &amp; S</cell></row><row><cell>6.</cell><cell>S</cell><cell>S n I</cell></row><row><cell>7.</cell><cell>S y</cell><cell cols="2">ðS y ; IÞ</cell></row><row><cell cols="3">8. until S ¼ ;</cell></row></table><note><p>LOSS 0À1 ðða; bÞ; S n fjg; TR; VÞ ¼ X l2V y v l À sgn X i2TR a i y i Kðx ðÀjÞ i</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Number of features, number of examples, percentage of each class, and Imbalance Ratio (IR) for all six data sets.</figDesc><table><row><cell>Dataset</cell><cell>#Features</cell><cell>#Examples</cell><cell>%Class (min.,maj.)</cell><cell>IR</cell></row><row><cell>LUNG</cell><cell>12,533</cell><cell>181</cell><cell>(17.1, 82.9)</cell><cell>4.85</cell></row><row><cell>GLIOMA</cell><cell>4433</cell><cell>50</cell><cell>(14.0, 86.0)</cell><cell>6.14</cell></row><row><cell>SRBCT</cell><cell>2308</cell><cell>83</cell><cell>(13.3, 86.7)</cell><cell>6.55</cell></row><row><cell>LUNG2</cell><cell>3312</cell><cell>203</cell><cell>(9.8, 90.2)</cell><cell>9.15</cell></row><row><cell>CAR</cell><cell>9182</cell><cell>174</cell><cell>(6.3, 93.7)</cell><cell>14.8</cell></row><row><cell>BULL</cell><cell>17404</cell><cell>94</cell><cell>(4.3, 95.7)</cell><cell>22.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 G</head><label>3</label><figDesc>-mean, in percentage. Average and maximum along all ranked features. Sensitivity analysis for parameter C.</figDesc><table><row><cell></cell><cell>BFE À SVM 0À1</cell><cell></cell><cell>BFE-SVM bl</cell><cell></cell><cell>HO À BFE 0À1</cell><cell></cell><cell>HO-BFE bl</cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell>Max</cell><cell>Mean</cell><cell>Max</cell><cell>Mean</cell><cell>Max</cell><cell>Mean</cell><cell>Max</cell></row><row><cell>2 À3</cell><cell>37.1</cell><cell>75.6</cell><cell>37.1</cell><cell>75.6</cell><cell>37.8</cell><cell>90.4</cell><cell>37.6</cell><cell>73.8</cell></row><row><cell>2 À2</cell><cell>48.7</cell><cell>92.6</cell><cell>48.7</cell><cell>92.6</cell><cell>45.6</cell><cell>82.5</cell><cell>42.3</cell><cell>74.7</cell></row><row><cell>2 À1</cell><cell>67</cell><cell>92.6</cell><cell>71.7</cell><cell>92.6</cell><cell>51.4</cell><cell>81.5</cell><cell>67.1</cell><cell>75.6</cell></row><row><cell>2 0</cell><cell>77.4</cell><cell>92.6</cell><cell>80.8</cell><cell>92.6</cell><cell>59.1</cell><cell>97.6</cell><cell>70.7</cell><cell>90.4</cell></row><row><cell>2 1</cell><cell>82.2</cell><cell>92.6</cell><cell>82.2</cell><cell>92.6</cell><cell>53.2</cell><cell>95.2</cell><cell>75.8</cell><cell>82.5</cell></row><row><cell>2 2</cell><cell>82.3</cell><cell>92.6</cell><cell>82.1</cell><cell>92.6</cell><cell>58.3</cell><cell>73.8</cell><cell>76.3</cell><cell>83.5</cell></row><row><cell>2 3</cell><cell>82.3</cell><cell>92.6</cell><cell>82.1</cell><cell>92.6</cell><cell>65.1</cell><cell>75.6</cell><cell>81.7</cell><cell>90.4</cell></row><row><cell>2 4</cell><cell>82.2</cell><cell>92.6</cell><cell>82.1</cell><cell>92.6</cell><cell>62</cell><cell>76.2</cell><cell>74.1</cell><cell>81.5</cell></row><row><cell>2 5</cell><cell>82.2</cell><cell>92.6</cell><cell>82.1</cell><cell>92.6</cell><cell>52.4</cell><cell>74.7</cell><cell>77.4</cell><cell>88.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 G</head><label>4</label><figDesc>-mean, in percentage. Average and maximum along all ranked features. Sensitivity analysis for parameter r.</figDesc><table><row><cell></cell><cell>BFE À SVM 0À1k</cell><cell></cell><cell>BFE À SVM blk</cell><cell></cell><cell>HO À BFE 0À1k</cell><cell></cell><cell>HO À BFE blk</cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell>Max</cell><cell>Mean</cell><cell>Max</cell><cell>Mean</cell><cell>Max</cell><cell>Mean</cell><cell>Max</cell></row><row><cell>3 0</cell><cell>29.7</cell><cell>89.3</cell><cell>29.7</cell><cell>89.3</cell><cell>26.4</cell><cell>74.7</cell><cell>26.3</cell><cell>72.9</cell></row><row><cell>3 1</cell><cell>53.9</cell><cell>82.5</cell><cell>72.5</cell><cell>92.6</cell><cell>54.1</cell><cell>82.5</cell><cell>55.9</cell><cell>84.5</cell></row><row><cell>3 2</cell><cell>77.7</cell><cell>92.6</cell><cell>77.7</cell><cell>92.6</cell><cell>73</cell><cell>83.5</cell><cell>68.4</cell><cell>75.6</cell></row><row><cell>3 3</cell><cell>63.6</cell><cell>92.6</cell><cell>63.6</cell><cell>92.6</cell><cell>59</cell><cell>83.5</cell><cell>58</cell><cell>83.5</cell></row><row><cell>3 4</cell><cell>41.9</cell><cell>75.6</cell><cell>41.9</cell><cell>75.6</cell><cell>37</cell><cell>74.7</cell><cell>37.1</cell><cell>75.6</cell></row><row><cell>3 5</cell><cell>15.9</cell><cell>73.8</cell><cell>15.9</cell><cell>73.8</cell><cell>13.9</cell><cell>73.8</cell><cell>14</cell><cell>73.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc>Maximum and average LOO gmean, in percentage. Average over all ranked features. Comparison among all approaches. Best results for each dataset are presented in bold.</figDesc><table><row><cell></cell><cell></cell><cell>LUNG</cell><cell></cell><cell>GLIOMA</cell><cell></cell><cell>SRBCT</cell><cell></cell><cell>LUNG2</cell><cell></cell><cell>CAR</cell><cell></cell><cell>BULL</cell><cell></cell></row><row><cell></cell><cell></cell><cell>max.</cell><cell>avg.</cell><cell>max.</cell><cell>avg.</cell><cell>max.</cell><cell>avg.</cell><cell>max.</cell><cell>avg.</cell><cell>max.</cell><cell>avg.</cell><cell>max.</cell><cell>avg.</cell></row><row><cell>No. resampling</cell><cell>l 1 -SVMr</cell><cell>96.4</cell><cell>96.7</cell><cell>63.6</cell><cell>74.7</cell><cell>99.9</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>92.6</cell><cell>95</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>SVM À RFE l</cell><cell>96.6</cell><cell>96.7</cell><cell>66.5</cell><cell>75.6</cell><cell>99.9</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>91.6</cell><cell>95.3</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>Fisher + SVM</cell><cell>97.8</cell><cell>98.4</cell><cell>66.9</cell><cell>73.8</cell><cell>99.9</cell><cell>100</cell><cell>99.3</cell><cell>100</cell><cell>90.5</cell><cell>90.5</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>l 0 -SVM</cell><cell>96.4</cell><cell>96.7</cell><cell>66.5</cell><cell>75.6</cell><cell>99.9</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>92.6</cell><cell>95</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>SVM À RFE nl</cell><cell>98.0</cell><cell>98.4</cell><cell>59.9</cell><cell>83.5</cell><cell>70.4</cell><cell>100</cell><cell>81.9</cell><cell>100</cell><cell>63.6</cell><cell>90.5</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>BFE À SVM 0À1</cell><cell>95.8</cell><cell>98.4</cell><cell>83.7</cell><cell>92.9</cell><cell>95.3</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>90.5</cell><cell>95.3</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>BFE-SVM bl</cell><cell>95.8</cell><cell>98.4</cell><cell>80.8</cell><cell>92.6</cell><cell>99.9</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>92.1</cell><cell>95.3</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>HO À BFE 0À1</cell><cell>97.5</cell><cell>100</cell><cell>83.0</cell><cell>92.6</cell><cell>95.3</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>91.0</cell><cell>95.3</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>HO-BFE bl</cell><cell>98.1</cell><cell>100</cell><cell>81.7</cell><cell>90.4</cell><cell>96.5</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>93.1</cell><cell>100</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>BFE À SVM 0À1k</cell><cell>94.7</cell><cell>96.7</cell><cell>53.9</cell><cell>82.50</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>BFE À SVM blk</cell><cell>94.7</cell><cell>96.7</cell><cell>72.5</cell><cell>92.6</cell><cell>58.2</cell><cell>95.3</cell><cell>100</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>HO À BFE 0À1k</cell><cell>97.6</cell><cell>99.7</cell><cell>54.1</cell><cell>82.5</cell><cell>55.1</cell><cell>89.8</cell><cell>71.6</cell><cell>100</cell><cell>58.7</cell><cell>100</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>HO À BFE blk</cell><cell>96.9</cell><cell>98.0</cell><cell>55.9</cell><cell>84.5</cell><cell>58.2</cell><cell>95.3</cell><cell>72</cell><cell>100</cell><cell>55.8</cell><cell>95.3</cell><cell>0</cell><cell>0</cell></row><row><cell>SMOTE</cell><cell>l 1 -SVMr</cell><cell>92.1</cell><cell>96.7</cell><cell>58.9</cell><cell>72.9</cell><cell>99.9</cell><cell>100</cell><cell>99.1</cell><cell>100</cell><cell>91.6</cell><cell>95.3</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>SVM À RFE l</cell><cell>97.1</cell><cell>98.4</cell><cell>64.5</cell><cell>74.7</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>91.6</cell><cell>95.3</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>Fisher + SVM</cell><cell>97.7</cell><cell>98.4</cell><cell>69.7</cell><cell>73.8</cell><cell>99.9</cell><cell>100</cell><cell>99.3</cell><cell>100</cell><cell>90.5</cell><cell>90.5</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>l 0 -SVM</cell><cell>96.6</cell><cell>98.4</cell><cell>65.6</cell><cell>75.6</cell><cell>99.8</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>92.1</cell><cell>95.3</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>SVM À RFE nl</cell><cell>92.8</cell><cell>100</cell><cell>62.0</cell><cell>81.5</cell><cell>81.9</cell><cell>100</cell><cell>85.2</cell><cell>100</cell><cell>82.8</cell><cell>95.3</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>BFE À SVM 0À1</cell><cell>95.3</cell><cell>99.7</cell><cell>82.7</cell><cell>92.6</cell><cell>96.8</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>91.5</cell><cell>95.3</cell><cell>25.6</cell><cell>64.1</cell></row><row><cell></cell><cell>BFE-SVM bl</cell><cell>92.9</cell><cell>98.4</cell><cell>72.7</cell><cell>78.4</cell><cell>99.2</cell><cell>100</cell><cell>99.2</cell><cell>100</cell><cell>92.6</cell><cell>95.3</cell><cell>14.3</cell><cell>48.6</cell></row><row><cell></cell><cell>HO À BFE 0À1</cell><cell>93.2</cell><cell>98.4</cell><cell>78.8</cell><cell>92.6</cell><cell>98.7</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>92.1</cell><cell>95.3</cell><cell>17.8</cell><cell>67.1</cell></row><row><cell></cell><cell>HO-BFE bl</cell><cell>97.1</cell><cell>100</cell><cell>74.8</cell><cell>82.5</cell><cell>95.8</cell><cell>100</cell><cell>98.3</cell><cell>100</cell><cell>92.6</cell><cell>95.3</cell><cell>11.9</cell><cell>69.9</cell></row><row><cell></cell><cell>BFE À SVM 0À1k</cell><cell>82.0</cell><cell>98.4</cell><cell>65.4</cell><cell>83.5</cell><cell>56.1</cell><cell>89.8</cell><cell>71.5</cell><cell>100</cell><cell>63.4</cell><cell>99.7</cell><cell>10.5</cell><cell>57.7</cell></row><row><cell></cell><cell>BFE À SVM blk</cell><cell>83.7</cell><cell>98.0</cell><cell>51.8</cell><cell>75.6</cell><cell>69.1</cell><cell>95.3</cell><cell>81.3</cell><cell>100</cell><cell>61.2</cell><cell>94.5</cell><cell>25.3</cell><cell>59.2</cell></row><row><cell></cell><cell>HO À BFE 0À1k</cell><cell>85.8</cell><cell>98.4</cell><cell>70.8</cell><cell>84.5</cell><cell>53.6</cell><cell>90.5</cell><cell>79.4</cell><cell>100</cell><cell>63.4</cell><cell>99.7</cell><cell>25.7</cell><cell>64.5</cell></row><row><cell></cell><cell>HO À BFE blk</cell><cell>81.4</cell><cell>98.4</cell><cell>60.8</cell><cell>91.5</cell><cell>65.2</cell><cell>94.7</cell><cell>78.6</cell><cell>100</cell><cell>89.0</cell><cell>90.5</cell><cell>16.6</cell><cell>68.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 G</head><label>5</label><figDesc>-mean, in percentage. Average and maximum along all ranked features. Sensitivity analysis for the percentage of variables eliminated at each iteration of the algorithm.</figDesc><table><row><cell></cell><cell>BFE À SVM 0À1</cell><cell></cell><cell>BFE-SVM bl</cell><cell></cell><cell>HO À BFE 0À1</cell><cell></cell><cell>HO-BFE bl</cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell>Max</cell><cell>Mean</cell><cell>Max</cell><cell>Mean</cell><cell>Max</cell><cell>Mean</cell><cell>Max</cell></row><row><cell>10%</cell><cell>82.2</cell><cell>92.6</cell><cell>78.8</cell><cell>92.6</cell><cell>64.9</cell><cell>82.5</cell><cell>77.1</cell><cell>83.5</cell></row><row><cell>20%</cell><cell>81.2</cell><cell>92.6</cell><cell>81.8</cell><cell>92.6</cell><cell>73.1</cell><cell>82.5</cell><cell>74.1</cell><cell>88.2</cell></row><row><cell>30%</cell><cell>80.3</cell><cell>92.6</cell><cell>79.9</cell><cell>92.6</cell><cell>64.9</cell><cell>89.3</cell><cell>75.9</cell><cell>82.5</cell></row><row><cell>40%</cell><cell>80.4</cell><cell>91.5</cell><cell>79.9</cell><cell>92.6</cell><cell>53.1</cell><cell>74.7</cell><cell>77.5</cell><cell>92.6</cell></row><row><cell>50%</cell><cell>77.7</cell><cell>92.6</cell><cell>79.3</cell><cell>91.5</cell><cell>71.7</cell><cell>82.5</cell><cell>71.4</cell><cell>73.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>Percentage of highly redundant attributes for all methods, and for all different subset of selected features.</figDesc><table><row><cell></cell><cell>Fisher + SVM</cell><cell>l 0 -SVM</cell><cell>SVM-RFE l</cell><cell>BFE-SVM 0À1</cell><cell>BFE-SVM bl</cell><cell>HO-BFE 0À1</cell><cell>HO-BFE bl</cell></row><row><cell>4434</cell><cell>0.12</cell><cell>0.12</cell><cell>0.12</cell><cell>0.12</cell><cell>0.12</cell><cell>0.12</cell><cell>0.12</cell></row><row><cell>2000</cell><cell>0.38</cell><cell>0.16</cell><cell>0.14</cell><cell>0.09</cell><cell>0.09</cell><cell>0.08</cell><cell>0.09</cell></row><row><cell>1000</cell><cell>0.83</cell><cell>0.24</cell><cell>0.15</cell><cell>0.09</cell><cell>0.09</cell><cell>0.10</cell><cell>0.11</cell></row><row><cell>500</cell><cell>1.57</cell><cell>0.28</cell><cell>0.17</cell><cell>0.12</cell><cell>0.12</cell><cell>0.15</cell><cell>0.14</cell></row><row><cell>200</cell><cell>2.17</cell><cell>0.47</cell><cell>0.13</cell><cell>0.17</cell><cell>0.17</cell><cell>0.28</cell><cell>0.23</cell></row><row><cell>100</cell><cell>3.09</cell><cell>0.22</cell><cell>0.10</cell><cell>0.42</cell><cell>0.34</cell><cell>0.22</cell><cell>0.24</cell></row><row><cell>50</cell><cell>3.76</cell><cell>0.24</cell><cell>0.16</cell><cell>0.41</cell><cell>0.65</cell><cell>0.24</cell><cell>0.24</cell></row><row><cell>20</cell><cell>4.74</cell><cell>0.53</cell><cell>0.00</cell><cell>0.00</cell><cell>0.53</cell><cell>1.05</cell><cell>0.53</cell></row><row><cell>10</cell><cell>4.44</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>2.22</cell><cell>2.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc>Average running times, in seconds, for all methods.</figDesc><table><row><cell>Model</cell><cell>Time (s)</cell></row><row><cell>Fisher + SVM</cell><cell>1.32</cell></row><row><cell>l 0 -SVM</cell><cell>2.89</cell></row><row><cell>SVM-RFE l</cell><cell>4.33</cell></row><row><cell>BFE-SVM 0À1</cell><cell>6.18</cell></row><row><cell>BFE-SVM bl</cell><cell>9.12</cell></row><row><cell>HO-BFE 0À1</cell><cell>5.78</cell></row><row><cell>HO-BFE bl (1 runs)</cell><cell>8.71</cell></row><row><cell>HO-BFE bl (5 runs)</cell><cell>38.89</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>S. Maldonado et al. / Information Sciences 286 (2014) 228-246</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Support from the Millennium Science Institute on Complex Engineering Systems (www.isci.cl; ICM: P-05-004-F, CONICYT: FBO16) is greatly acknowledged. The first author was supported by FONDECYT Project 11121196.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Detailed feature selection results</head><p>The results of the leave-one-out crossvalidation (in terms of g-mean) are shown in Figs. A.1 (LUNG dataset), A.2 (GLIOMA dataset), A.3 (SRBCT dataset), A.4 (LUNG2 dataset), A.5 (CAR dataset) and A.6 (BULL dataset). The figures present the best proposed and alternative approaches using linear and Gaussian kernel respectively for an increasing number of selected features. The upper figure shows the results without resampling, while the lower figure depicts the results obtained using the same approaches but with SMOTE oversampling.</p><p>From Fig.</p><p>A.1(a) we observe that the proposed approach HO-BFE using balanced loss and linear kernel has the best predictive performance, achieving perfect classification when using 20, 100, and 250 features, followed by the best proposed approach using Gaussian kernel (HO-BFE using 0-1 loss). Alternative approaches behave relatively similar in this data set. The results are relatively similar when using SMOTE (see </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBFS: an effective density based feature selection scheme for small sample size and high dimensional imbalanced data sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alibeigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hamzeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="page" from="67" to="103" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ultrahigh dimensional feature screening via RKHS embeddings</title>
		<author>
			<persName><forename type="first">K</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Sixteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gene-expression profiles predict survival of patients with lung adenocarcinoma</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Beer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Kardia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Misek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Gharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Lizyness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kuick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hayasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Iannettoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Orringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hanash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="816" to="824" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification of human lung carcinomas by mRNA expression profiling reveals distinct adenocarcinoma subclasses</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staunton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ladd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beheshti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gillette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sugarbaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="13790" to="13795" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class prediction for high-dimensional class-imbalanced data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Blagus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lusa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">523</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature selection vía concave minimization and support vector machines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings of the Fifteenth International Conference (ICML&apos;98)</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Use of gene-expression profiling to identify prognostic subclasses in adult acute myeloid leukemia</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bullinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dohner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frohling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Schlenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dohner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England J. Med</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1605" to="1616" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Editorial: special issue on learning from imbalanced data sets</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kotcz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SMOTE: synthetic minority oversampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FAST: a roc-based feature selection metric for small samples and imbalanced data classification problems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wasikowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2009</date>
			<biblScope unit="page" from="124" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face recognition by generalized two-dimensional FLD method and multi-class support vector machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nasipuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4282" to="4292" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ultrahigh dimensional feature selection: beyond the linear model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Samworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2013" to="2038" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the 2-tuples based genetic tuning performance for fuzzy rule based classification systems in imbalanced data-sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1268" to="1291" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikravesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
		<title level="m">Feature Extraction, Foundations and Applications</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gene selection for cancer classification using support vector machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barnhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Application of SVM-RFE on EEG signals for detecting the most relevant scalp regions linked to affective valence processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Hidalgo-Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vázquez-Marrufo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galvao-Carmona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tomé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2102" to="2108" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ringner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Saal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ladanyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Antonescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Meltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="673" to="679" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting credit card customer churn in banks using data mining</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Data Anal. Techn. Strategies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="28" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A wrapper method for feature selection using Support Vector Machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernel-penalized SVM for feature selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Basak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="128" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gene expression-based classification of malignant gliomas correlates better with survival than histological classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Nutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Betensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Cairncross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ladd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Batchelor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deimling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Pomeroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Res</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1602" to="1607" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Complexity</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
	<note>Reading</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The design of polynomial function-based neural network predictors for detection of software defects</title>
		<author>
			<persName><forename type="first">B.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Ohb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page" from="40" to="57" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Class Imbalances versus class overlapping: an analysis of a learning system behavior, MICAI 2004: Advances in Artificial Intelligence</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Monard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2972</biblScope>
			<biblScope unit="page" from="312" to="321" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A novel SVM modeling approach for highly imbalanced and overlapping classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Data Anal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="319" to="341" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variable selection using SVM-based criteria</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1357" to="1370" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soft margins for AdaBoost</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="287" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Comparison of approaches to alleviate problems with high-dimensional and class-imbalanced data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Shanab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hulse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Information Reuse and Integration (IRI)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond accuracy, F-score and ROC: a family of discriminant measures for performance evaluation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sokolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4304</biblScope>
			<biblScope unit="page" from="1015" to="1021" />
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature selection via dependence maximization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1393" to="1434" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Molecular classification of human carcinomas by use of gene expression signatures</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Sapinoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Moskaluk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Frierson</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Hampton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Res</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="7388" to="7393" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cost-sensitive boosting for classification of imbalanced data, Pattern Recogn</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3358" to="3378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature selection with high-dimensional imbalanced data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Napolitano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE International Conference ICDMW &apos;09</title>
		<meeting>the 2009 IEEE International Conference ICDMW &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="507" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature selection and granularity learning in genetic fuzzy rule-based classification systems for highly imbalanced data-sets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="397" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Review on feature selection techniques and the impact of SVM for cancer classification using gene expression profile</title>
		<author>
			<persName><forename type="first">G</forename><surname>Victo Sudha George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Cyril</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Sci. Eng. Surv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="16" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A feature selection method based on improved fisher&apos;s discriminant ratio for text sentiment classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="8696" to="8702" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combating the small sample class imbalance problem using feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wasikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1388" to="1400" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The use of zero-norm with linear models and kernel methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1439" to="1461" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A stable gene selection in microarray data analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>BMC Bioinform</publisher>
			<biblScope unit="page">228</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature selection for text categorization on imbalanced data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="89" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simultaneous grouping pursuit and feature selection over an undirected graph</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">502</biblScope>
			<biblScope unit="page" from="713" to="725" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
