<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Sparse Channel Estimation under Symmetric alpha-Stable Noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Konstantinos</forename><surname>Pelekanakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tropical Marine Science Institute</orgName>
								<orgName type="laboratory">Acoustic Research Laboratory</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>119223</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tropical Marine Science Institute</orgName>
								<orgName type="laboratory">Acoustic Research Laboratory</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>119223</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117576</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Sparse Channel Estimation under Symmetric alpha-Stable Noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FACE9FFC4B2B45161CEB9E0C9DF34616</idno>
					<idno type="DOI">10.1109/TWC.2014.042314.131432</idno>
					<note type="submission">received August 5, 2013; revised December 18, 2013 and February 10, 2014; accepted March 6, 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Robust system identification</term>
					<term>outlier rejection</term>
					<term>robust statistics</term>
					<term>M-estimate algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tackle the problem of channel estimation in environments that exhibit both sparse, time-varying impulse responses and impulsive noise with Symmetric alpha-Stable (SαS) statistics. Two novel frameworks are proposed for designing online adaptive algorithms that exploit channel sparseness and achieve robust performance against impulses. The first framework generates recursive least-squares (RLS)-type algorithms based on a differentiable cost function that combines robust nonlinear methods with sparse-promoting L0 norm regularization. The second framework employs the natural gradient (NG) and incorporates non-linear methods for the channel prediction error as well as the L0 norm of the channel taps. From these frameworks, we derive linear and quadratic complexity algorithms. The improved performance of the proposed RLS-type and NGtype algorithms relative to conventional robust algorithms, such as the recursive least M-estimate (RLM) algorithm and the recursive least p-norm (RLP) algorithm, is validated by using extensive computer simulations as well as signal analysis from an underwater acoustic communications experiment. In addition, we discovered that RLM is not robust under specific SαS noise conditions, contrary to the claim in [34]. Finally, our results also demonstrate the clear superiority of the NG-type algorithms over their RLS-type counterparts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S PARSE channels are typically encountered in wireless links such as digital TV <ref type="bibr" target="#b0">[1]</ref>, acoustic echo <ref type="bibr" target="#b1">[2]</ref>, and underwater acoustic <ref type="bibr" target="#b2">[3]</ref>. They are called sparse since most of the energy of the impulse response is concentrated in a small fraction of its duration. Exploiting sparseness in channel estimation by using adaptive filters has gained considerable interest since the late 1990s <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. It is well known that improved estimation performance in terms of steady-state misadjustment and channel tracking can be achieved by using sparse prior information. In addition, a receiver that explicitly adapts to a sparse channel can attain reduced complexity if only the significant channel coefficients are retained.</p><p>Proportionate-type algorithms, i.e., algorithms that update each channel coefficient in proportion to its estimated magnitude were among the first paradigms to use sparse prior information <ref type="bibr" target="#b1">[2]</ref>. Popular examples are the improved proportionate NLMS (IPNLMS) <ref type="bibr" target="#b6">[7]</ref> algorithm and the improved proportionate affine projection algorithm (IPAPA) <ref type="bibr" target="#b7">[8]</ref>. The first algorithm that used a sparseness-promoting L p norm (p ∈ (0, 1]) within its cost function was the pNLMS <ref type="bibr" target="#b8">[9]</ref>. Based on the expectation-maximization (EM) algorithm, a sparse RLS, termed as sparse RLS (SPARLS), was studied in <ref type="bibr" target="#b9">[10]</ref>. An L 1 norm-regularized RLS based on the least-absolute shrinkage and selection operator (Lasso) approach was proposed in <ref type="bibr" target="#b10">[11]</ref>. On a different track, Slavakis et al <ref type="bibr" target="#b11">[12]</ref> proposed a sparse online algorithm using projections on closed convex sets and Murakami et al <ref type="bibr" target="#b12">[13]</ref> introduced an Adaptive Proximal Forward-Backward Splitting (APFBS) scheme. Recently, the authors introduced an algorithmic framework that leveraged on natural gradient (NG) adaptation combined with L 0 norm regularization <ref type="bibr" target="#b13">[14]</ref>.</p><p>The above papers, among many others, assume that the observation noise is Gaussian and so using the L 2 norm of the channel prediction error (i.e., the difference between the observed signal and the filter output) in the cost function is optimal. However, a number of man-made and physical noise processes depart from the Gaussian assumption due to their impulsive nature. Examples are: multiple access interference in radio channels <ref type="bibr" target="#b14">[15]</ref>, double talk in acoustic echo cancellation <ref type="bibr" target="#b15">[16]</ref>, biological noise <ref type="bibr" target="#b16">[17]</ref> or ice cracking <ref type="bibr" target="#b17">[18]</ref> in various underwater acoustic channels. Such environments require the use of robust adaptive filters since L 2 norm-based algorithms suffer severe performance degradation.</p><p>Studies that propose sparse channel estimation in the presence of impulsive noise are scarce. Vega et al proposed a variable step-size IPNLMS algorithm <ref type="bibr" target="#b18">[19]</ref>. Subsequently, an improved proportionate affine projection sign algorithm (RIP-APSA) based on the L 1 norm of the error signal was introduced by <ref type="bibr" target="#b19">[20]</ref>. Yamamoto et al <ref type="bibr" target="#b20">[21]</ref> robustified the APFBS scheme by employing a Huber loss function <ref type="bibr" target="#b21">[22]</ref>. A notable issue with the aforementioned algorithms is that they were tested in impulsive noise that obeys a Gaussian-mixture density function. Gaussian-mixture models and the related Middleton class A model are often used to model impulsive noise environments (e.g., man-made impulse noise against a Gaussian noise background) <ref type="bibr" target="#b22">[23]</ref>. Although these models lend themselves to computer simulations, they are not suitable for modeling many natural noise sources (e.g., snapping shrimp noise in warm shallow waters <ref type="bibr" target="#b16">[17]</ref>). Another drawback of the Gaussian-mixture distribution is that its tail decays exponentially while empirical evidence manifests that algebraic decay of heavy-tailed noise processes often occurs in communications as well as in various fields of engineering, physics, and economics <ref type="bibr" target="#b23">[24]</ref>.</p><p>The family of alpha-stable distributions provides an accu-rate model for heavy-tailed noise <ref type="bibr" target="#b24">[25]</ref>. The significance and validity of alpha-stable models is justified by the stability property and the Generalized Central Limit Theorem (GCLT).</p><p>In this work, we deal with the family of Symmetric alpha-Stable (SαS) distributions due to its ability to model many impulsive noise processes in communications channels, and, in fact, includes the Gaussian density as a special case. For signal processing applications, the pivotal property of SαS random processes is their infinite second and higher order moments. As shown in <ref type="bibr" target="#b24">[25]</ref>, the minimum dispersion criterion is an appropriate measure of optimality since minimizing the error signal dispersion, the average estimation error is simultaneously minimized. In addition, minimizing the error dispersion is equivalent to minimizing the fractional lower order moment (FLOM) of the estimation error. Variants of LMS and RLS algorithms based on FLOMs were proposed in the stable signal processing community. Characteristic examples are the least mean p-norm (LMP) <ref type="bibr" target="#b24">[25]</ref>, the recursive least p-norm (RLP) <ref type="bibr" target="#b25">[26]</ref>, and recursive least mean p-norm (RLMP) <ref type="bibr" target="#b26">[27]</ref>. It is worth stressing that both LMP and RLP are tailored to real-valued (passband) channels. Recently, the authors introduced an algorithmic framework for complexvalued (baseband) channels <ref type="bibr" target="#b27">[28]</ref>. That paper serves as a brief precursor to this work.</p><p>Our main contribution is the development of two new algorithmic frameworks that systematically generate sparse adaptive filters robust against SαS noise. The first framework generates RLS-type algorithms. This framework uses an objective function with two parts: a data fidelity component that is robust against outliers and a L 0 norm regularization component. The second framework leverages on natural gradient (NG) adaptation by using a sparse-aware Riemannian distance. In addition, this framework incorporates robust non-linear methods for the channel prediction error and the L 0 norm of the channel taps. Better flexibility in terms of computational complexity than the RLS-type framework is shown. New linear and quadratic complexity algorithms are derived. Finally, all proposed algorithms are tested by identifying experimental sparse underwater acoustic channels in simulated and reallife noise with SαS properties. Their improved performance relative to conventional robust algorithms is demonstrated. Contrary to Chan and Zou <ref type="bibr" target="#b33">[34]</ref>, we show that the recursive least M-estimate (RLM) algorithm is not robust under specific noise regimes. Furthermore, our analysis manifests the superiority of the NG-type framework over its RLS-type counterpart.</p><p>The remainder of this paper is organized as follows: Section II describes the system model. The RLS-type and the NG-type frameworks are presented, respectively, in Sections III and IV. Simulation and experimental results are reported in Section V. Finally, the paper is concluded in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. NOTATION AND SYSTEM MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notation</head><p>Superscripts T , † , and * stand for transpose, Hermitian transpose, and conjugate, respectively. Column vectors (matrices) are denoted by boldface lowercase (uppercase) letters. The N × N identity matrix is denoted as</p><formula xml:id="formula_0">I N . Let z ∈ C and p ≥ 1. The L p norm of z is defined as |z| p (| Re{z}| p + | Im{z}| p ) 1/p</formula><p>. The sign function of z is defined as csgn(z) sgn(Re{z}) + j•sgn(Im{z}), where sgn(•) stands for the sign function of a real scalar. Let z ∈ C N . The sign function of z is given by the column vector csgn(z) with elements csgn(z i ), i=0, . . . , N -1. The L p norm of z is defined as z p (</p><formula xml:id="formula_1">N -1 i=0 |z i | p p ) 1/p .</formula><p>The L 0 norm of z, denoted as z 0 , equals the number of the non-zero entries of z. The complex gradient of a scalar function f (z) with respect to z is denoted as ∇ z f (z) and is defined in <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. System model</head><p>We employ the baseband representation of the channel, transmitted/received signals, and additive noise process. Let us consider an impulse response, which is described by the unknown K-tap vector h</p><formula xml:id="formula_2">[n]=[h 0 [n] h 1 [n] . . . h K-1 [n]] T at discrete time n.</formula><p>In addition, we assume that h[n] is slowly time-varying and sparse, namely, most of the coefficients are close to zero and only few of them are large. The received signal is expressed as</p><formula xml:id="formula_3">y[n] = h[n] † u[n] + w[n],<label>(1)</label></formula><p>where . The characteristic exponent α ∈ (0, 2] describes the impulsiveness of the noise (smaller α leads to more frequent occurrence of impulses) and the dispersion γ&gt;0 controls the spread of the distribution around its location parameter (which is zero for our purposes). When α=2, the SαS probability density function (pdf) boils down to the Gaussian pdf and γ is equal to half the variance. For mathematical and practical reasons (it is rare to find SαS noise with α&lt;1 in practical systems), we restrict our work to the class of SαS distributions where α ∈ (1, 2] <ref type="bibr" target="#b24">[25]</ref>. The objective of this paper is to perform recursive estimation of h[n] with limited complexity and memory given sequential observations {y[i], u[i]} n i=1 . In digital communications, performance analysis is often reported against the signal-to-noise ratio (SNR) per information symbol, denoted as E s /N 0 , where E s is the energy of the information symbol and N 0 is the power spectral density (PSD) of the noise. Since the concept of the PSD cannot be applied in SαS noise, we must resort to a different SNR measure. An SNR measure for baseband signals is rather involved because the in-phase and quadrature components of the SαS noise are generally dependent <ref type="bibr" target="#b29">[30]</ref>. In passband, however, the parameters α and γ can be easily estimated and so an SNR definition can be readily defined as</p><formula xml:id="formula_4">u[n]=[u[n] u[n -1] . . . u[n -K + 1]] T contains</formula><formula xml:id="formula_5">E s /N 0 (dB) 10 log 10 N s P s 2γ 2/α , (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>where N s is the ratio of the symbol interval over the sample interval, P s is the received signal power, and γ 2/α plays the same role as the variance. When α=2, equation (2) becomes the usual E s /N 0 definition in Gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RLS-TYPE FRAMEWORK</head><p>The RLS algorithm is one of the most important adaptive filter algorithms due to its fast convergence rate in nonstationary environments, insensitivity to the eigenvalue spread This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p><p>of the input correlation matrix, and modular structure that offers fast implementations (e.g., lattice structure). Hence, it is desirable to have RLS-type algorithms that are robust in SαS noise and exploit channel sparseness for improved performance.</p><p>Let us express the a posterior and prior error as</p><formula xml:id="formula_7">ē[i]=y[i] - ĥ[n] † u[i] and e[i]=y[i] -ĥ[n -1] † u[i],</formula><p>respectively, where i ≤ n and ĥ[n] stands for the estimate of h[n]. Elaborating on the robust filtering approach <ref type="bibr" target="#b30">[31]</ref>, we consider a real, nonnegative valued loss function, denoted as f (ē), whose purpose is to down weight large errors due to impulses. We also define the complex score function ψ(ē)=∂f (ē)/∂ē and the complex weight function q(ē)=ψ(ē)/ē * .The proposed cost function is written as:</p><formula xml:id="formula_8">J[n] = n i=0 λ n-i f (ē[i]) + ζ ĥ[n] 0 , (<label>3</label></formula><formula xml:id="formula_9">)</formula><p>where λ ∈ (0, 1) is the forgetting factor and ζ ≥ 0 is a regularization parameter. The regularizing term ĥ[n] 0 helps to further accelerate the convergence of the inactive (close to zero) filter taps. It is obvious that if different functions f (ē) and L 0 norm proxies are employed, different algorithms will be generated. Specific examples of f (ē) are given below. For the remainder of this paper, the L 0 norm is approximated by the differentiable function 1 ĥ</p><formula xml:id="formula_10">[n] 0 K-1 k=0 1 -e -η| ĥk [n]| 1 , η &gt; 0,<label>(4)</label></formula><p>which is a complex extension of the real L 0 norm used in <ref type="bibr" target="#b31">[32]</ref>. The parameter =1/η defines the interval [-, ] such that all Re ĥk [n] and Im ĥk [n] that fall within that interval are attracted towards the zero value. Note that equation ( <ref type="formula" target="#formula_8">3</ref>) is not a convex cost function, however, if η is close to 10 and ζ is chosen sufficiently small the algorithm converges to meaningful solutions, as indicated in <ref type="bibr" target="#b31">[32]</ref>.</p><p>The RLS-type algorithm is derived in Appendix A by setting ∇ ĥ[n] * J[n]=0. The following equations summarize the result:</p><formula xml:id="formula_11">e[n] = y[n] -ĥ[n -1] † u[n],<label>(5)</label></formula><formula xml:id="formula_12">k[n] = q(e[n])Φ[n -1] -1 u[n] λ + q(e[n])u[n] † Φ[n -1] -1 u[n] ,<label>(6)</label></formula><formula xml:id="formula_13">Φ[n] -1 = λ -1 Φ[n -1] -1 -k[n]u[n] † Φ[n -1] -1 , (7) ν k [n] = e -η| ĥk [n]| 1 csgn ĥk [n] , k = 0, . . . , K -1,<label>(8) ĥ</label></formula><formula xml:id="formula_14">[n] = ĥ[n -1] + k[n]e[n] * + λ -1 λ ζη 2 I K -k[n]u[n] † Φ[n -1] -1 ν[n -1]. (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>The algorithm is initialized for ĥ</p><formula xml:id="formula_16">[0]=0 and Φ[0] -1 =κ -1 I K , κ being a small positive real number. It is worthy to note that if f (ē[i])=ē[i]ē[i] * is employed in (3), then q(e[n])=1</formula><p>and so ( <ref type="formula" target="#formula_11">5</ref>)-( <ref type="formula" target="#formula_14">9</ref>) will be called the L 0 -RLS algorithm hereafter. The L 0 -RLS algorithm requires O(K 2 ) algebraic operations per datum. If in addition ζ=0, then the resulting algorithm becomes the standard RLS <ref type="bibr" target="#b32">[33]</ref>. 1 Strictly speaking the function is not differentiable along the real and imaginary axis but this is not a problem in practice since we allow the channel taps to be arbitrarily close to these axes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The L 0 -RLM algorithm</head><p>The work in <ref type="bibr" target="#b33">[34]</ref> introduces the recursive least M-estimate (RLM) algorithm, a real-valued adaptive algorithm based on Hampel's three-part redescending M-estimate cost function. The algorithm is designed to cope with contaminated Gaussian noise, namely, the observed noise consists of two components: a Gaussian component and an impulsive interference component. In addition, the authors claim that RLM is robust under SαS noise, however, no performance results were reported towards that front. We check the validity of this claim in Section V.</p><p>Our aim is to improve RLM to yield lower misadjustment in sparse channels. To this end, we modify Hampel's threepart redescending M-estimate function so that it conforms with the chosen complex gradient operator <ref type="bibr" target="#b28">[29]</ref>. Dropping the time index for notational convenience, the loss function has the form</p><formula xml:id="formula_17">f (ē) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ ēē * 2ξ |ē| 2 -ξ 2 ξ(T +Δ)-ξ 2 +ξ (|ē| 2 -T ) 2 Δ-T ξ(T + Δ) -ξ 2 , 0 ≤ |ē| 2 &lt;ξ , ξ ≤ |ē| 2 &lt;Δ , Δ &lt; |ē| 2 &lt;T , T &lt; |ē| 2 , (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>where the threshold parameters ξ, Δ,and T are used for outlier suppression. The score and weight function are computed as <ref type="bibr" target="#b10">(11)</ref> and</p><formula xml:id="formula_19">ψ(ē) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ē * ξ |ē| 2 ē * ξ |ē| 2 -T Δ -T ē * |ē| 2 0 , 0 ≤ |ē| 2 &lt; ξ , ξ ≤ |ē| 2 &lt; Δ , Δ &lt; |ē| 2 &lt; T , T &lt; |ē| 2</formula><formula xml:id="formula_20">q(ē) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 1 ξ |ē| 2 ξ |ē| 2 -T Δ -T 1 |ē| 2 0 , 0 ≤ |ē| 2 &lt; ξ , ξ ≤ |ē| 2 &lt; Δ , Δ &lt; |ē| 2 &lt; T , T &lt; |ē| 2 , (<label>12</label></formula><formula xml:id="formula_21">)</formula><p>respectively.</p><p>We now describe a way to continuously estimate the threshold parameters ξ, Δ,and T under the assumption of contaminated Gaussian noise. We stress that our system model in <ref type="bibr" target="#b0">(1)</ref> has no background Gaussian noise. Based on <ref type="bibr" target="#b33">[34]</ref>, a robust estimation of the variance of the real part of the baseband noise is computed by using the median operator as follows:</p><formula xml:id="formula_22">σ 2 r [n] = λ σ σ 2 r [n -1] + c(1 -λ σ )med(a[n]),<label>(13)</label></formula><p>where λ σ is a forgetting factor, a[n]=[e  <ref type="formula" target="#formula_20">12</ref>) and ( <ref type="formula" target="#formula_11">5</ref>)-( <ref type="formula" target="#formula_14">9</ref>) will be called L 0 -RLM hereafter. The channel update of the L 0 -RLM requires O(N w log 2 (N w )) additional operations for the computation of σ 2 [n] as compared to L 0 -RLS. Finally, note that if ζ=0, then L 0 -RLM reduces to the complex form of RLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The L 0 -RLSA</head><p>As discussed above, the parameters ξ, Δ, T of the L 0 -RLM are based on the steady-state error signal. Consequently, the algorithm performance may be compromised when large channel fluctuations or large impulses occur during its convergence period. Note that the performance results in <ref type="bibr" target="#b33">[34]</ref> are based on the condition that no impulses occur during the convergence period of RLM. Clearly, that is not the case for the channels considered here.</p><p>Here, we propose a loss function f (ē) that does not depend on any threshold parameters and is resilient to large impulses through the merits of the L 1 norm, that is,</p><formula xml:id="formula_23">f (ē[i])=|ē[i]| 1 . The score function is computed as ψ(ē[i]) = 0.5(csgn(ē[i])) * (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>and the weight function in <ref type="bibr" target="#b5">(6)</ref> becomes</p><formula xml:id="formula_25">q(e[n]) = 0.5(csgn(e[n])/e[n]) * . (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>The algorithm described by ( <ref type="formula" target="#formula_25">15</ref>) and ( <ref type="formula" target="#formula_11">5</ref>)-( <ref type="formula" target="#formula_14">9</ref>) will be called the L 0 -RLSA (L 0 norm-recursive least sign algorithm) hereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The L 0 -RLP algorithm</head><p>The work in <ref type="bibr" target="#b25">[26]</ref> introduces the RLP algorithm, a realvalued adaptive algorithm that exhibits robust performance in SαS noise. The RLP is established on the important observation that the mean square error (MSE) is not a valid optimality criterion since SαS distributions lack moments of order p ≥ α. However, all moments of order p&lt;α do exist and so the minimum dispersion error is mathematically meaningful as an optimality criterion. This fact motivates the usage of the L p norm (p ∈ [1, α)) of the a posterior error in the cost function.</p><p>Here, we enhance RLP so that it yields lower misadjustment in sparse channels. Inspired by the structure of the RLM loss function, we use a mixture of L 2 and L p norms as follows:</p><formula xml:id="formula_27">f (ē) ⎧ ⎨ ⎩ ēē * |ē| p p |Δ| p , 0 ≤ |ē| 2 &lt; ξ , ξ ≤ |ē| 2 &lt; Δ , Δ ≤ |ē| 2 , (<label>16</label></formula><formula xml:id="formula_28">)</formula><p>where the threshold parameters ξ, Δ are proportional to the dispersion γ of the observed passband SαS noise. The score and weight function are computed as</p><formula xml:id="formula_29">ψ(ē) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ē * p 2 [|Re{ē}| p-1 sgn(Re{ē}) -j |Im{ē}| p-1 sgn(Im{ē})] 0 , 0 ≤ |ē| 2 &lt;ξ , ξ ≤ |ē| 2 &lt;Δ , Δ ≤ |ē| 2<label>(17)</label></formula><p>and</p><formula xml:id="formula_30">q(ē) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 1 p 2ē * [|Re{ē}| p-1 sgn(Re{ē}) -j |Im{ē}| p-1 sgn(Im{ē})] 0 , 0 ≤ |ē| 2 &lt;ξ , ξ ≤ |ē| 2 &lt;Δ , Δ ≤ |ē| 2 , (<label>18</label></formula><formula xml:id="formula_31">)</formula><p>respectively. The algorithm described by ( <ref type="formula" target="#formula_11">5</ref>)-( <ref type="formula" target="#formula_14">9</ref>) and ( <ref type="formula" target="#formula_30">18</ref>) will be called L 0 -RLP hereafter. The L 0 -RLP requires O(K 2 ) algebraic operations per datum. If in addition ζ=0, then the L 0 -RLP reduces to the complex form of RLP.</p><p>IV. FRAMEWORK BASED ON THE NATURAL GRADIENT Below, we propose two algorithmic frameworks that leverage on natural gradient (NG) adaptation. Before we embark on our discussion, it is instructive to define the a priori error vector,</p><formula xml:id="formula_32">e[n] * = [e[n] * e[n -1] * . . . e[n -L + 1] * ] T = y[n] * -U[n] † ĥ[n -1],<label>(19)</label></formula><p>and a posteriori error vector,</p><formula xml:id="formula_33">ē[n] * = [ē[n] * ē[n -1] * . . . ē[n -L + 1] * ] T = y[n] * -U[n] † ĥ[n],<label>(20)</label></formula><p>where </p><formula xml:id="formula_34">U[n]=[u[n] u[n-1] . . . u[n-L+1]] is the K ×L matrix of input</formula><p>Then, the a-posteriori error vector be also be written as:</p><formula xml:id="formula_36">ē[n] * = e[n] * -U[n] † r[n],<label>(22)</label></formula><p>where <ref type="bibr" target="#b21">(22)</ref> follows from substituting <ref type="bibr" target="#b20">(21)</ref> into <ref type="bibr" target="#b19">(20)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background</head><p>The general framework for online linear prediction algorithms proposed by Kivinen and Warmuth <ref type="bibr" target="#b3">[4]</ref> is the stepping stone for the cost functions proposed in this section. This framework suggests that an efficient online algorithm must exhibit a balance between its requirements to be conservative (i.e., a radical change from ĥ[n-1] to ĥ[n] should be avoided) and corrective (i.e., better channel prediction must occur if the same input and output were to be observed at two consecutive times). Usually, the correctiveness and conservativeness are on opposite ends, thus an efficient cost function (to be minimized with respect to ĥ[n]) could be written as</p><formula xml:id="formula_37">J[n] = f (ē[n]) + δ D( ĥ[n], ĥ[n -1]),<label>(23)</label></formula><p>where f (ē[n]) is a scalar loss function and D( ĥ[n], ĥ[n -1]) denotes the scalar distance function between ĥ[n] and ĥ[n-1].</p><p>The distance function may not be a metric. For instance, the Kullback-Leibler (KL) divergence is used as D in <ref type="bibr" target="#b3">[4]</ref>.</p><p>The magnitude of the positive parameter δ keeps the relative balance between correctiveness, induced by the loss function, and conservativeness, induced by the distance function. In addition, this framework lends itself to incorporating additional properties for ĥ[n]. For example, if the filter energy is constrained to be equal to X, then this constraint is introduced via a new Lagrangian multiplier ζ, as follows:</p><formula xml:id="formula_38">J[n] = f (ē[n]) + δ D( ĥ[n], ĥ[n -1]) +ζ ĥ[n] 2 2 -X . (<label>24</label></formula><formula xml:id="formula_39">)</formula><p>This framework is the basis for many adaptive algorithms; cf <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quadratic complexity framework</head><p>Using the above algorithmic framework, the merits of both robust methods and sparseness constraints can be integrated into a single objective function as:</p><formula xml:id="formula_40">J[n] = n i=n-L+1 f (ē[i]) + δ r[n] † P[n -1]r[n] + ζ ĥ[n] 0 . (<label>25</label></formula><formula xml:id="formula_41">)</formula><p>The loss function f (ē[i]) ensures robustness against outliers, δ, ζ ≥0 are regularization parameters and L is the length of the observation window. The choice of L is upper-limited (usually L ≤10) by the time-variation of the channel as well as the hardware memory/complexity requirements. The matrix P[n] is Hermitian and positive definite whose entries depend on ĥ[n], i.e., P[n] is a K × K Riemannian metric tensor. Thus, the term r[n] † P[n -1]r[n] denotes the Riemannian distance between ĥ[n] and ĥ[n -1]. The fact that ĥ[n] lies in a non-isotropic (Riemannian) space is based on the prior knowledge that h[n] must be close to some axis of C K since most of the filter taps are near zero. In addition, in non-isotropic (Riemannian) spaces, it is well known that the ordinary Euclidean gradient does not represent the steepest ascent direction of a cost function <ref type="bibr" target="#b35">[36]</ref>. Thus, regularizing J[n] with a Riemannian distance is well motivated. In addition, the regularizing term ĥ[n] 0 (approximated by ( <ref type="formula" target="#formula_10">4</ref>)) accelerates the convergence of the zero-valued coefficients. The parameter ζ should be tuned close to 10 -5 for moderate SNR, as suggested in <ref type="bibr" target="#b31">[32]</ref>.</p><p>A plausible question is: "how could one find P for any sparse channel?". The study in <ref type="bibr" target="#b34">[35]</ref> suggests that the parameter space of a sparse channel may be visualized as a space having the following warping: for regions close to the coordinate axes, distances in any direction orthogonal to those axes should be larger than the Euclidean distances. A good choice is</p><formula xml:id="formula_42">P[n] -1 =G[n],</formula><p>where G[n] is the proportionate matrix of the PNLMS algorithm <ref type="bibr" target="#b1">[2]</ref>. Given the superiority of IPNLMS <ref type="bibr" target="#b6">[7]</ref> over PNLMS, we choose G[n] to be a diagonal matrix with elements {g k [n]} K-1 k=0 , which are computed as follows:</p><formula xml:id="formula_43">k [n] = (1 -β) ĥ[n] 1 K + (1 + β) ĥk [n] 1 , (<label>26</label></formula><formula xml:id="formula_44">)</formula><formula xml:id="formula_45">g k [n] = k [n] K-1 k=0 k [n] , (<label>27</label></formula><formula xml:id="formula_46">)</formula><p>where β ∈ [-1, 1]. Substituting ( <ref type="formula" target="#formula_43">26</ref>) in ( <ref type="formula" target="#formula_45">27</ref>), we have</p><formula xml:id="formula_47">g k [n] = 1 -β 2K + (1 + β) ĥk [n] 1 2 ĥ[n] 1 + ε , (<label>28</label></formula><formula xml:id="formula_48">)</formula><p>where ε denotes a small positive constant to avoid division by zero during initialization of the algorithm. Note that if β=-1, P[n] becomes proportional to the identity matrix, i.e., the channel space is Euclidean (the channel is assumed nonsparse). For very sparse channels, β should be chosen between 0 and 0.5. Furthermore, δ is chosen as <ref type="bibr" target="#b6">[7]</ref> δ</p><formula xml:id="formula_49">= (1 -β)δ /2K, (<label>29</label></formula><formula xml:id="formula_50">)</formula><p>where δ is the regularization parameter of the normalized least-mean-square (NLMS) algorithm.</p><p>The general type of the algorithm is derived by computing ∇ r[n] * J[n]=0. The algebra is presented in the Appendix B. The channel update equations are summarized below:</p><formula xml:id="formula_51">e[n] * = y[n] * -U[n] † ĥ[n -1],<label>(30)</label></formula><formula xml:id="formula_52">A[n] = G[n -1]U[n],<label>(31)</label></formula><formula xml:id="formula_53">B[n] = U[n] † A[n] + δ Q[n] -1 -1 , (32) C[n] = A[n]B[n],<label>(33)</label></formula><formula xml:id="formula_54">D[n] = G[n -1]-C[n]A[n] † , (<label>34</label></formula><formula xml:id="formula_55">)</formula><formula xml:id="formula_56">ν k [n] = e -η| ĥk [n]| 1 csgn ĥk [n] , k = 0, . . . , K -1,<label>(35)</label></formula><formula xml:id="formula_57">ĥ[n] = ĥ[n -1] + μC[n]e[n] * - μζη 2δ D[n]ν[n -1],<label>(36)</label></formula><p>where μ ∈ (0, 1] is the step-size parameter. Since G[n] is diagonal and L K, the required number of algebraic operations is O(K 2 ). Initialization of the algorithm starts with ĥ[0]=0.</p><p>In light of the above framework, new and existing algorithms are derived as follows:</p><p>• if f (ē)=ēē * , then Q[n]=I L and ( <ref type="formula" target="#formula_51">30</ref>)-( <ref type="formula" target="#formula_57">36</ref>) describe the L 0 -IPAPA <ref type="bibr" target="#b13">[14]</ref>. If in addition β=-1 and ζ=0, then the L 0 -IPAPA reduces to the affine projection algorithm (APA) <ref type="bibr" target="#b32">[33]</ref>,</p><p>• if q(ē) is given by ( <ref type="formula" target="#formula_20">12</ref>), ( <ref type="formula" target="#formula_51">30</ref>)-(36) will be called L 0 -IPMAPA (L 0 norm-improved-proportionate M-estimate affine projection algorithm) hereafter. If in addition ζ=0, then the L 0 -IPMAPA reduces to IPMAPA (note O(K) computational complexity), • if q(ē) is given by ( <ref type="formula" target="#formula_30">18</ref>), ( <ref type="formula" target="#formula_51">30</ref>)-( <ref type="formula" target="#formula_57">36</ref>) will be called L 0 -IPpNAPA (L 0 norm-improved-proportionate p-norm affine projection algorithm) hereafter. If in addition ζ=0, then the L 0 -IPpNAPA reduces to IPpNAPA (note O(K) computational complexity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Linear complexity framework</head><p>The O(K 2 ) complexity of L 0 -IPMAPA and L 0 -IPpNAPA may become objectionable from a hardware perspective when long filters are required. Such is the case in acoustic echo or broadband underwater acoustic channels. Towards reducing the computational complexity, we propose the following cost function:</p><formula xml:id="formula_58">J[n] = n i=n-L+1 f (ē[i]) (37) subject to r[n] † P[n -1]r[n] ≤ μ 2 . (<label>38</label></formula><formula xml:id="formula_59">)</formula><p>Using Lagrange multipliers, the modified cost function becomes</p><formula xml:id="formula_60">J[n] = n i=n-L+1 f (ē[i]) + δ r[n] † G -1 [n -1]r[n] -μ 2</formula><p>(39) where δ here is the Lagrange multiplier.</p><p>Setting ∇ r[n] * J[n]=0, we have</p><formula xml:id="formula_61">- n i=n-L+1 ψ(ē[i])u[i] + δG -1 [n -1]r[n] = 0. (<label>40</label></formula><formula xml:id="formula_62">)</formula><p>Note that it is tedious to solve for r[n] using (40) since {ψ(ē[i])} n i=n-L+1 depends on ĥ[n]. We circumvent this issue by assuming ē[i] e[i], i=n -L + 1, . . . , n, at steady-state. Then, r[n] is expressed as:</p><formula xml:id="formula_63">r[n] = 1 δ G[n -1]U[n]ψ[n],<label>(41)</label></formula><p>where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ψ[n]=[ψ(e[n]) . . . ψ(e[n -L + 1])]</head><p>T . To obtain the Lagrange multiplier δ, we substitute ( <ref type="formula" target="#formula_63">41</ref>) into <ref type="bibr" target="#b37">(38)</ref>. Hence, we have</p><formula xml:id="formula_64">δ = 1 μ x[n] 2 2 , (<label>42</label></formula><formula xml:id="formula_65">)</formula><formula xml:id="formula_66">x[n] = G[n -1] 1/2 U[n]ψ[n],<label>(43)</label></formula><p>where</p><formula xml:id="formula_67">G[n] 1/2 denotes the Cholesky decomposition of G[n]. Recall that G[n] is diagonal and so G[n] 1/2 is equal to the square root of the entries of G[n].</formula><p>Hence, the channel update equation is given by the formula</p><formula xml:id="formula_68">ĥ[n] = ĥ[n -1] + μ G[n -1] 1/2 x[n] κ + x[n] 2 2 , (<label>44</label></formula><formula xml:id="formula_69">)</formula><p>where μ&gt;0 and κ is a small positive constant used to avoid possible division by zero during initialization of the algorithm.</p><p>Since L K, it is straightforward to see that (44) requires O(K) operations per datum. Also note the following:</p><formula xml:id="formula_70">• if ψ(e[i]</formula><p>) is given by <ref type="bibr" target="#b10">(11)</ref>, then (44) will be called NGMAPA (natural gradient-based M-estimate affine projection algorithm) hereafter,</p><formula xml:id="formula_71">• if ψ(e[i]</formula><p>) is given by <ref type="bibr" target="#b16">(17)</ref>, then (44) will be called NG-pNAPA (natural gradient-based p-norm affine projection algorithm) hereafter,</p><formula xml:id="formula_72">• if ψ(e[i]</formula><p>) is given by ( <ref type="formula" target="#formula_23">14</ref>), then (44) will be called NGAPSA (natural gradient-based affine projection sign algorithm) hereafter. If β=-1, then the NGAPSA reduces to the complex form of the APSA <ref type="bibr" target="#b36">[37]</ref>. If L=1, then the NGAPSA reduces to the NGSA (natural gradient sign algorithm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PERFORMANCE UNDER SαS NOISE</head><p>A theoretical analysis of the proposed frameworks is exceptionally difficult because SαS distributions do not possess second order moments. In this section, the effectiveness of all algorithms derived by the two frameworks is tested by running numerical simulations as well as analyzing experimental data. Impulse responses obtained from broadband underwater acoustic communications channels are used to support our findings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation results</head><p>The time-varying channel to be estimated is shown in Figure <ref type="figure" target="#fig_1">1</ref>. This channel corresponds to a wideband underwater acoustic link that was measured during the Focused Acoustic Fields (FAF) experiment. Each channel snapshot was estimated at rate of 6250 Hz. Figure <ref type="figure" target="#fig_1">1</ref> is generated by keeping each channel snapshot fixed for 8 ms. For further details about the FAF experimental setup, the interested reader is directed to <ref type="bibr" target="#b13">[14]</ref>. The channel to be identified is clearly sparse and the filter length required to capture the entire impulse response is 371 taps. The simulated input signal is independent white complex Gaussian noise with unit power and the output is generated according to (1). The simulated SαS noise is generated in passband using <ref type="bibr" target="#b37">[38]</ref> and then is shifted to baseband using the lowpass filter of the FAF experiment. Four types of noise series are considered, i.e., 1) high rate of impulses in low SNR (α =1.2, E s /N 0 =15 dB), 2) high rate of impulses in high SNR (α =1.2, E s /N 0 =25 dB), 3) low rate of impulses in low SNR (α =1.65, E s /N 0 =15 dB), and 4) low rate of impulses in high SNR (α =1.65, E s /N 0 =25 dB). The performance measure is the normalized misadjustment (in dB), 20 log 10 (||h[n]-ĥ[n]|| 2 /||h[n]|| 2 ), and is computed after averaging 100 independent runs.</p><p>The parameters of all algorithms are chosen as follows:</p><p>• λ=0.995 for all RLS-type filters,</p><p>• λ σ =0.99 for all M-estimate filters,</p><p>• η=10 for all filters with L 0 norm, • β=0.5, L=4 for all NG-type filters,</p><p>• μ=0.1, δ =10, ζ=5 • 10 -4 for L 0 -IPpNAPA, IPpNAPA, L 0 -IPMAPA and IPMAPA, • ζ=0.5 for all RLS-type filters with L 0 norm; • μ=0.25, ξ=2γ, Δ=100γ for NGpNAPA and NGMAPA;</p><p>• p=α-0.15 <ref type="foot" target="#foot_0">2</ref> , ξ=2γ for L 0 -RLP, RLP, L 0 -IPpNAPA, and NGpNAPA.</p><p>In Figures <ref type="figure" target="#fig_2">2(a</ref>)-(d), we have plotted the curves of all RLStype algorithms for different values of α and E s /N 0 . The following observations are in order:   This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p><p>• L 0 -RLP is consistently robust and exhibits the lowest misadjustment for α=1.2 regardless the E s /N 0 . • In spite of the SNR, L 0 -RLM cannot cope with impulsive noise for α=1.2. RLM <ref type="bibr" target="#b33">[34]</ref> shows similar behavior (not shown for brevity). On the other hand, L 0 -RLM shows similar misadjustment with L 0 -RLP and L 0 -RLSA for α=1.65. These results are contrary to the general claim that RLM is robust in SαS noise. • The L 0 -RLSA is consistently robust but exhibits slower convergence rate than L 0 -RLP for α=1.2. Note though that L 0 -RLSA is easier to use than L 0 -RLM or L 0 -RLP since it depends on fewer parameters. • The sparseness effect of the L 0 norm improves RLP <ref type="bibr" target="#b25">[26]</ref> for all pairs of α and E s /N 0 . Similar results hold for RLM and RLSA and are omitted for brevity. • As expected, L 0 -RLS is not robust against impulsive noise (its misadjustment is greater than 0 dB for α=1.2 and is not shown for visualization purposes). Figures <ref type="figure" target="#fig_3">3(a</ref>) through (d) present a comparison of the misadjustment of all algorithms derived from ( <ref type="formula" target="#formula_51">30</ref>)- <ref type="bibr" target="#b35">(36)</ref>. We make the following observations:</p><p>• L 0 -IPpNAPA is consistently robust and exhibits the lowest misadjustment for α=1.2 regardless the E s /N 0 . • In spite of the SNR, L 0 -IPMAPA cannot cope with impulsive noise for α=1.2. However, it shows similar performance with L 0 -IPpNAPA for α=1.65. • The sparseness effect of the L 0 norm is validated since L 0 -IPpNAPA is consistently superior over both IPpNAPA and IPMAPA. • L 0 -IPAPA <ref type="bibr" target="#b13">[14]</ref> fails for α=1.2, it has poor performance for α=1.65 and E s /N 0 =15dB, but it shows similar performance with L 0 -IPpNAPA when α=1.65 and E s /N 0 =25 dB. All linear complexity algorithms stemming from (44) are compared in Figures <ref type="figure">4(a)-(d)</ref>. Note the following:</p><p>• The NGpNAPA consistently achieves the best convergence rate all pairs of α and E s /N 0 . • NGpNAPA, NGMAPA and NGAPSA consistently demonstrate similar channel tracking. • Regarding the effect of the observation window L , the NGSA shows consistently slower convergence speed among all other sparse algorithms, however, the algorithm exhibits the best tracking for E s /N 0 =15 dB regardless the choice of α. This result is justified by noting that the error term in the cost function becomes smaller (recall NGSA uses L=1) in steady state and so NGSA becomes more robust (conservative) against impulses. • The sparseness of all proposed algorithms is confirmed since the non-sparse (but robust) APSA <ref type="bibr" target="#b36">[37]</ref> exhibits poor performance for all pairs of α and E s /N 0 . In Figures <ref type="figure">5(a)-(d)</ref>, we compare the best algorithms from each framework. In particular, L 0 -RLP is compared with L 0 -IPpNAPA and NGpNAPA. In light of the results presented, we observe that:</p><p>• The NG-type algorithms outperform L 0 -RLP.</p><p>• Although L 0 -IPpNAPA uses a smaller step-size μ than NGpNAPA, it shows faster convergence for α=1.2. Furthermore, L 0 -IPpNAPA exhibits up to 2dB better tracking than NGpNAPA when α=1.6 despite the SNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental results</head><p>The dataset analyzed here was recorded during the ROMA-NIS 2010 experiment in the area of Selat Pauh in Singapore waters. The transmitter was mounted on a rigid tripod 2.5 m above the seabed. The receiver was a 1.3 m diameter, 2dimensional circular array consisted of 508 acoustic sensors <ref type="bibr" target="#b39">[40]</ref>. Here, we analyze data from one sensor of the array. The average water depth was about 15 m and the transmitterreceiver range was about 80 m. The transmitted signal was a 10 4 bits/s-rate, BPSK-modulated, pseudo-random data. The PN-sequence was pulse-shaped by a square-root cosine filter with roll-off factor 0.25 and truncation length ± 5 symbol intervals. The resulting waveform was modulated onto a 30 kHz carrier frequency.</p><p>A notable feature of this channel is its very high SNR due to the very short range. To test our algorithms in a realistic scenario, on-site recorded ambient noise is appropriately scaled and added to the passband received signal so that E s /N 0 =15 dB. Prior to adding extra noise to the received signal, the noise series is bandpass filtered for estimating the SαS parameters α (based on <ref type="bibr" target="#b40">[41]</ref>) and γ (based on <ref type="bibr" target="#b41">[42]</ref>). In particular, we find that α=1.44 and γ=715.28. Figure <ref type="figure" target="#fig_5">6(a)</ref> illustrates the received passband signal after noise addition and Figure <ref type="figure" target="#fig_5">6</ref>(b) validates that the ambient noise is SαS distributed.</p><p>Prior to channel estimation, the received signal is shifted to baseband, low-pass filtered, and downsampled to 1 sample/bit. Since the channel is unknown, the misadjustment cannot be applied here. Consequently, the mean absolute error (MAE), defined as</p><formula xml:id="formula_73">MAE[n] = 1 n n i=1 |y[i] -ĥ[i -1] † u[i]| 1 = 1 n n i=1 h[i] -ĥ[i -1] † u[i] + w[n] 1<label>(45)</label></formula><p>is utilized as a performance metric. Note that when the channel estimate is very close to the true channel, the MAE converges to the first order moment E {|w[n]| 1 }. The dB scale of the MAE is defined as 20 log 10 (MAE). It is important to stress that we do not change the parameter values used in simulations except for Δ=10γ (L 0 -RLP and L 0 -IPpNAPA) and N w =12 (L 0 -RLM and L 0 -IPMAPA).</p><p>The time evolution of the estimated channel amplitude is shown in Figure <ref type="figure" target="#fig_6">7</ref>. For a sampling rate of 1 sample/bit, the required length of the adaptive filter is 206 taps. Clearly, the acoustic channel has a sparse multipath structure and any amplitude fluctuations are attributed to environmental changes since both the transmitter and the receiver are stationary.</p><p>In light of the results shown in Figure <ref type="figure" target="#fig_7">8</ref>(a), we observe that L 0 -RLP shows the best convergence rate but similar channel tracking with L 0 -RLSA. In addition, L 0 -RLP outperforms RLP <ref type="bibr" target="#b25">[26]</ref> validating the sparseness effect of the L 0 norm. L 0 -RLM achieves inferior performance against L 0 -RLP and L 0 -RLSA. These results are in close agreement with the simulation results shown in Figure <ref type="figure" target="#fig_2">2(a)</ref>.</p><p>Figure <ref type="figure" target="#fig_7">8</ref>(b) shows the MAE performances of all algorithms derived by ( <ref type="formula" target="#formula_51">30</ref>)- <ref type="bibr" target="#b35">(36)</ref>. L 0 -IPpNAPA presents better convergence rate but same tracking as L 0 -IPMAPA. As expected, L 0 -IPAPA shows higher MAE than its robust counterparts. It is reassuring to note that these results are in agreement with those shown in Figure <ref type="figure" target="#fig_3">3(c)</ref>.    <ref type="formula" target="#formula_68">44</ref>). Clearly, the NGpNAPA exhibits the best performance. The NGSA shows the largest MAE among all sparse algorithms. As expected, the sparse-indifferent APSA <ref type="bibr" target="#b36">[37]</ref> shows the worst behavior. Observe that these experimental results are in close agreement with the simulation results shown in Figure <ref type="figure">4(c)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION AND CONCLUSIONS</head><p>We investigated two frameworks for developing complexvalued sparse robust adaptive filters. Although the focus of this paper was on sparse channel estimation in the presence of SαS noise, the proposed algorithms can be applied in any field where noisy samples are obtained from a sparse linear time-varying system.</p><p>The first framework was inspired by the RLS algorithm and proposed a cost function that coupled robust methods for outlier suppression with a L 0 norm penalty. Three new O(K 2 ) algorithms were generated: the L 0 -RLM, the L 0 -RLP, and the L 0 -RLSA. Both computer simulations and experimental data analysis verified that these algorithms improve the traditional robust algorithms RLM <ref type="bibr" target="#b33">[34]</ref> and RLP <ref type="bibr" target="#b25">[26]</ref>. We also found that L 0 -RLP and L 0 -RLSA were consistently robust regardless of noise parameter α while L 0 -RLM/RLM lost robustness when α was close to one. Our results contradict the claim in <ref type="bibr" target="#b33">[34]</ref> that RLM is generally robust in SαS noise.</p><p>The second framework took advantage of the non-isotropic (Riemannian) space of the channel and generated robust algorithms based on NG adaptation. Two O(K 2 ) algorithms, i.e., the L 0 -IPMAPA and the L 0 -IPpNAPA were introduced. Our data analysis revealed that the L 0 -IPpNAPA was despite the of α but the L 0 -IPMAPA (like L 0 -RLM) lost robustness when α was close to one. In addition, three O(K) algorithms were introduced, i.e., the NGMAPA, the NGpNAPA and the NGAPSA. These algorithms demonstrated firmly robust performance for all SαS noise regimes and exhibited comparable channel tracking with their O(K 2 ) counterparts. This result is very promising from a hardware implementation point of view since one could start with an O(K 2 ) algorithm to achieve fast convergence followed by an O(K) algorithm for fast tracking.</p><p>Elaborating on the loss function f (ē), our results demonstrated that a mixture of L 2 and L p norms is more efficient in terms of convergence rate than the L 1 norm or the Hampel's M-estimate function.</p><p>Finally, our results demonstrated the clear superiority of the NG-framework over the RLS-framework in sparse channels. This is due to the fact that the NG-type filters use the Riemannian distance to modify the gradient search direction for faster adaptation. It would be intriguing to incorporate a Riemannian distance in the RLS-type framework to test its algorithmic performance. We leave this challenge as a future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A DERIVATION OF (5)-(9)</head><p>Computing ∇ ĥ[n] * J[n], where J[n] is given by (3), we have:</p><formula xml:id="formula_74">∇ ĥ[n] * J[n] = ∇ ĥ[n] * n i=0 λ n-i f (ē[i]) +∇ h[n] * ζ ĥ[n] 0 . (<label>46</label></formula><formula xml:id="formula_75">)</formula><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. Computing each of the above terms individually, we have</p><formula xml:id="formula_76">∇ ĥ[n] * n i=0 λ n-i f (ē[i]) = n i=0 λ n-i ∂f (ē[i]) ∂ē[i] ∂ē[i] ∂ ĥ[n] * (47) = - n i=0 λ n-i ψ(ē[i])u[i] = - n i=0 λ n-i q(ē[i])ē[i] * u[i] (48) = - n i=0 λ n-i q(ē[i]) y[i] * -u[i] † ĥ[n] u[i] (49) = - n i=0 λ n-i q(ē[i])y[i] * u[i] + n i=0 λ n-i q(ē[i]) u[i]u[i] † ĥ[n]<label>(50)</label></formula><p>where (47) holds due to the chosen gradient operator <ref type="bibr" target="#b28">[29]</ref>. The gradient of ĥ From the above equation, it is not straightforward to find ĥ[n] since knowledge of ē[i] implies knowledge of ĥ[n]. However, at steady-state, it is plausible to assume that ē[n] e[n]. Following <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b25">[26]</ref>, we define the weighted complex autocorrelation matrix, Φ[n], and the weighted complex crosscorrelation vector, p[n], as: (57)</p><formula xml:id="formula_77">Φ[n] =</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>samples and y[n]=[y[n] y[n -1] . . . y[n -L + 1]] T contains the L most recent output samples. Let us also denote the channel update vector as r[n] = ĥ[n] -ĥ[n -1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The time-varying channel used in simulations. The x-axis shows multipath delay, the y-axis shows absolute time and the z-axis shows the channel amplitude in linear scale.</figDesc><graphic coords="6,315.36,53.83,252.35,146.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Learning curves of RLS-type algorithms for different α and Es/N 0 ; RLM: (a) Nw=12, (b) Nw=12, (c) Nw=40, (d) Nw=40. RLP/L 0 -RLP: (a) Δ=10γ, (b) Δ=40γ, (c) Δ=6γ, (d) Δ=100γ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Learning curves for different α and Es/N 0 . L 0 -IPMAPA: (a) Nw=12, (b) Nw=12, (c) Nw=12, (d) Nw=40. L 0 -IPpNAPA: (a) Δ=10γ, (b) Δ=40γ, (c) Δ=10γ, (d) Δ=100γ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Learning curves of APSA, NGSA, NGAPSA, NGMAPA, and NGpNAPA for different α and Es/N 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) received passband signal for Es/N 0 =15dB; (b) goodness of fit of passband ambient noise series to SαS distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Snapshots of the ROMANIS channel. The x-axis shows multipath delay, the y-axis shows absolute time and the z-axis shows the channel amplitude in linear scale. The snapshots are generated at the bit rate.</figDesc><graphic coords="10,52.61,230.78,253.33,146.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 (</head><label>8</label><figDesc>Figure8(c) compares all linear complexity algorithms generated by (44). Clearly, the NGpNAPA exhibits the best performance. The NGSA shows the largest MAE among all sparse algorithms. As expected, the sparse-indifferent APSA<ref type="bibr" target="#b36">[37]</ref> shows the worst behavior. Observe that these experimental results are in close agreement with the simulation results shown in Figure4(c). Figure 8(d) combines the results from Figures 8(a)-(c) and compares L 0 -RLP with L 0 -IPpNAPA and NGpNAPA. The NG-type algorithms outperform L 0 -RLP validating the superiority of the NG-type framework. The L 0 -IPpNAPA achieves faster convergence than NGpNAPA. On the other hand, NGpNAPA demonstrates slightly better channel tracking. Again, note that these results are in close agreement with the simulation results shown in Figure 5(a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 (</head><label>8</label><figDesc>Figure8(c) compares all linear complexity algorithms generated by (44). Clearly, the NGpNAPA exhibits the best performance. The NGSA shows the largest MAE among all sparse algorithms. As expected, the sparse-indifferent APSA<ref type="bibr" target="#b36">[37]</ref> shows the worst behavior. Observe that these experimental results are in close agreement with the simulation results shown in Figure4(c). Figure 8(d) combines the results from Figures 8(a)-(c) and compares L 0 -RLP with L 0 -IPpNAPA and NGpNAPA. The NG-type algorithms outperform L 0 -RLP validating the superiority of the NG-type framework. The L 0 -IPpNAPA achieves faster convergence than NGpNAPA. On the other hand, NGpNAPA demonstrates slightly better channel tracking. Again, note that these results are in close agreement with the simulation results shown in Figure 5(a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Learning curves of proposed algorithms based on experimental data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>[n] 0</head><label>0</label><figDesc>with respect to h k [n] * , k = 0, . . . , K -1, is equal to∇ h k [n] * ζ ĥ[n] 0 = ζη 2 e -η| ĥk [n]| 1 csgn ĥk [n] . (51)We now define the vector ν[n] with entriesν k [n] = e -η| ĥk [n]| 1 csgn ĥk [n] , k = 0, . . . , K -1. (52)Computing ∇ ĥ[n] * J[n]=0 and after some algebra we haven i=0 λ n-i q(ē[i]) u[i]u[i] † ĥ[n] = n i=0 λ n-i q(ē[i])y[i] * u[i] -ζη 2 ν[n].(53)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>i q(e[i]) u[i]u[i] † = λΦ[n -1] + q(e[n]) u[n]u[n] † , i q(e[i])y[i] * u[i] = λp[n -1] + q(e[n])y[n] * u[n],(55)respectively. Hence, (53) can be written asΦ[n] ĥ[n] = p[n] -ζη 2 ν[n] (56)and so ĥ[n] = Φ[n] -1 p[n] -ζη 2 ν[n] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the threshold parameters are chosen by the following expressions: ξ=2.45σ[n] (i.e., Pr{|e[n]| 2 &lt; ξ}=0.95), Δ=2.72σ[n] (i.e., Pr{|e[n]| 2 &lt; Δ}=0.975), and T =3.03σ[n] (i.e., Pr{|e[n]| 2 &lt; T }=0.99). The algorithm described by (</figDesc><table /><note><p>2 r [n] . . . e 2 r [n -N w + 1]] T is the real part of the prior error signal, and c=1.483(1 + 5/(N w -1)) is a finite sample correction factor that ensures consistent estimates. The variance of the imaginary part of the baseband noise, σ 2 i [n], is computed similarly. Using the Rayleigh distribution for |e[n]| 2 with parameter σ 2 [n]=0.5(σ 2 r [n] + σ 2 i [n]),</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>p must be as close to α as possible but not equal to α. In practice, α is estimated so one should be conservative on the choice of p.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors are indebted to the personnel of the NATO Undersea Research Centre (NURC) for carrying out the FAF experiment. Also, the authors would like to thank Dr. Venugopalan Pallayil and Mr. Unnikrishnan K. C. for their leadership and help during the ROMANIS'10 experiment.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The associate editor coordinating the review of this paper and approving it for publication was Z. Wang.</p><p>This work was partially supported by MOE Tier-1 grant R-263-000-521-133.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To find a recursive solution similar to RLS, we need to find a recursion for</p><p>before invoking the matrix inversion lemma to compute Φ[n] -1 . Elaborating as in <ref type="bibr" target="#b38">[39]</ref>, we note that</p><p>where ( <ref type="formula">61</ref>) is obtained by assuming ν[n-1] ν[n]. Applying the matrix inversion lemma to compute Φ[n] -1 , we have that</p><p>where k[n] is given by</p><p>Hence, the update for ĥ[n] becomes ĥ</p><p>APPENDIX B DERIVATION OF (30)- <ref type="bibr" target="#b35">(36)</ref> Computing ∇ r[n] * J[n], where J[n] is given by (25), we have:</p><p>Computing each of the above terms individually, we have</p><p>where</p><p>). We also have</p><p>The gradient of ĥ[n]</p><p>and combining terms we have the following vector equation:</p><p>From the above equation, we note that it is tedious to solve for r[n] since ν[n] depends on ĥ[n] in a non-linear fashion. At steady-state, however, it is plausible to assume that ē</p><p>Using this assumption, we can solve for r[n] by using the matrix inversion lemma <ref type="bibr" target="#b32">[33]</ref>. Thus, we have:</p><p>where</p><p>Furthermore, to exercise control over the change of the tap values from one iteration to the next, we introduce a step size parameter, μ ∈ (0, 1]. Thus, the channel update equation is deduced as follows: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Advanced television systems for terrestrial broadcasting: some problems and some proposed solutions</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="958" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Proportionate normalized least-mean-squares adaptation in echo cancelers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Duttweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech, Audio Process</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="508" to="518" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparse equalization for realtime digital underwater acoustic communications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kocic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stojanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1995 MTS/IEEE OCEANS</title>
		<meeting>1995 MTS/IEEE OCEANS</meeting>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1417" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exponentiated gradient versus gradient descent for linear predictors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Comput</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prior knowledge and preferential structures in gradient descent learning algorithms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="311" to="355" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting sparsity in adaptive filters</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal. Process</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1883" to="1894" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An improved PNLMS algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2002 IEEE ICASSP</title>
		<meeting>2002 IEEE ICASSP</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1881" to="1884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A generalized proportionate variable step-size algorithm for fast changing acoustic environments</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hoshuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Goubran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2004 IEEE ICASSP</title>
		<meeting>2004 IEEE ICASSP</meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="161" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive filtering algorithms for promoting sparsity</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2003 IEEE ICASSP</title>
		<meeting>2003 IEEE ICASSP</meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="361" to="I364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SPARLS: the sparse RLS algorithm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalouptsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tarokh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4013" to="4025" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online adaptive estimation of sparse signals: where RLS meets the 1 -norm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Angelosante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bazerque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3436" to="3447" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online sparse system identification and signal reconstruction using projections onto weighted balls</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kopsinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slavakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="936" to="952" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A sparse adaptive filtering using time-varying softthresholding techniques</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Murakami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2010 IEEE ICASSP</title>
		<meeting>2010 IEEE ICASSP</meeting>
		<imprint>
			<biblScope unit="page" from="3734" to="3737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">New sparse adaptive algorithms based on the natural gradient and the L 0 -norm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pelekanakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Ocean. Eng</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Peformance of DS/SSMA communications in impulsive channels-part I: linear correlation receivers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Aazhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1179" to="1187" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Double-talk robust fast converging algorithms for network echo cancelation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gänsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech, Audio Process</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="656" to="663" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal and near-optimal signal detection in snapping shrimp dominated ambient noise</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Chitre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Ocean. Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="497" to="503" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparison of adaptive and robust receivers for signal detection in ambient underwater noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bouvet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="621" to="626" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A family of robust algorithms exploiting sparsity in adaptive filters</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Vega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech, Audio Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="572" to="581" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Proportionate affine projection sign algorithms for network echo cancellation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech, Audio Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2273" to="2284" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive proximal forward-backward splitting for sparse system identification under impulsive noise</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2012 EUSIPCO</title>
		<meeting>2012 EUSIPCO</meeting>
		<imprint>
			<biblScope unit="page" from="2620" to="2624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
		<title level="m">Robust Statistics. John</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-Gaussian noise models in signal processing for telecommunications: new methods and results for class A and class B noise models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Middleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1129" to="1149" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A Practical Guide to Heavy Tails Statistical Techniques for Analyzing Heavy-Tailed Distributions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Taqqu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Birkhauser</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Signal processing with fractional lower order moments: stable processes and their applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Nikias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="986" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive Bayesian decision feedback equaliser for alpha-stable noise environments</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Georgiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mulgrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Process</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="1603" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A sliding window RLS-like adaptive algorithm for filtering alpha-stable noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="86" to="89" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A class of affine projection filters that exploit sparseness under symmetric alpha-stable noise</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pelekanakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2013 IEEE/MTS Oceans</title>
		<meeting>2013 IEEE/MTS Oceans</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A complex gradient operator and its application in adaptive array theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Brandwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microwaves, Optics and Antennas</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PSK communication with passband additive symmetric α-stable noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chitre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Armand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2990" to="3000" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust Huber adaptive filter</title>
		<author>
			<persName><forename type="first">P</forename><surname>Petrus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance analysis of L 0 -norm constraint least mean square algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="2223" to="2235" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Adaptive Filter Theory</title>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A recursive least M-estimate algorithm for robust adaptive filtering in impulsive noise: fast algorithm and convergence performance analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="975" to="991" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Normalized natural gradient adaptive filtering for sparse and nonsparse systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Gay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2002 IEEE ICASSP</title>
		<meeting>2002 IEEE ICASSP</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1405" to="1408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Why natural gradient?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 IEEE ICASSP</title>
		<meeting>1998 IEEE ICASSP</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1213" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An affine projection sign algorithm robust against impulsive interferences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="327" to="330" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
		<ptr target="http://academic2.american.edu/˜jpnolan/stable/stable.html" />
		<title level="m">STABLE program for Windows</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sparsity regularized recursive least squares adaptive filtering</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Eksioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Signal Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="480" to="487" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ambient noise imaging in warm shallow waters; robust statistical algorithms and range estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chitre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuselan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pallayil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="838" to="847" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simple consistent estimators of stable distribution parameters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Statist. -Simula</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1109" to="1136" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parameter estimates for symmetric stable distributions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Fama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="331" to="338" />
			<date type="published" when="1971-06">June 1971</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
