<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Novel Deep Learning-Based Collaborative Filtering Model for Recommendation System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingsheng</forename><surname>Fu</surname></persName>
							<email>uestc_fumingsheng@126.com</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hong</forename><surname>Qu</surname></persName>
							<email>hongqu@uestc.edu.cn</email>
							<idno type="ORCID">0000-0001-6114-3441</idno>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Zhang</forename><surname>Yi</surname></persName>
							<email>zhangyi@scu.edu.cn</email>
							<idno type="ORCID">0000-0002-5867-9322</idno>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Li</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yongsheng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
						</author>
						<author>
							<persName><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>610054</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Application of Intelligent Learning</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>610054</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>610054</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Novel Deep Learning-Based Collaborative Filtering Model for Recommendation System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B3EEA54FF16DF03A57BC7125390F7D72</idno>
					<idno type="DOI">10.1109/TCYB.2018.2795041</idno>
					<note type="submission">received September 8, 2017; revised January 3, 2018; accepted January 11, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaborative filtering (CF)</term>
					<term>deep learning</term>
					<term>feedforward neural networks</term>
					<term>recommender system</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The collaborative filtering (CF) based models are capable of grasping the interaction or correlation of users and items under consideration. However, existing CF-based methods can only grasp single type of relation, such as restricted Boltzmann machine which distinctly seize the correlation of user-user or item-item relation. On the other hand, matrix factorization explicitly captures the interaction between them. To overcome these setbacks in CF-based methods, we propose a novel deep learning method which imitates an effective intelligent recommendation by understanding the users and items beforehand. In the initial stage, corresponding lowdimensional vectors of users and items are learned separately, which embeds the semantic information reflecting the useruser and item-item correlation. During the prediction stage, a feed-forward neural networks is employed to simulate the interaction between user and item, where the corresponding pretrained representational vectors are taken as inputs of the neural networks. Several experiments based on two benchmark datasets (MovieLens 1M and MovieLens 10M) are carried out to verify the effectiveness of the proposed method, and the result shows that our model outperforms previous methods that used feed-forward neural networks by a significant margin and performs very comparably with state-of-the-art methods on both datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>based model. Content-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> use features that are extracted from the profile of the users or description of items to make new recommendations. CF-based methods <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> adopt users or items previous history of interactions, such as the rating given to the item by the user, which have attracted more attention due to their better prediction performance than the content-based method.</p><p>There are two effective CF-based methods with the impressive performance: 1) matrix factorization (MF) <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> and 2) restricted Boltzmann machine (RBM) like methods <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. MF directly learns the latent vectors of the users and the items from the user-item ratings matrix and captures the interaction between the user and the item. However, complex interactions cannot be captured by MF since its estimated rating is produced by the simple inner product between corresponding latent vectors of user and item. The RBM-like methods explicitly make recommendation from either user or item side via constructing independent models for users or items, respectively. However, in this model the correlation can only be considered from a single side, that is item-item or user-user, thus ignoring the other completely. Besides, the RBM-like method cannot retain the item-user interaction and they are not deep enough to capture complex features.</p><p>The most powerful approach to capture complex relations is to use deep learning techniques, which have made tremendous successes on many other pattern recognition tasks, such as computer vision <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>, natural language processing <ref type="bibr" target="#b14">[15]</ref>, and speech recognition <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Recently, deep neural networks for RS are also beginning to gain enormous attention <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Specially, for content-based model, numerous methods applying deep learning to recommendation had been proposed <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref>.</p><p>However, there are very limited researches on employing deep neural networks to CF-based model. Dziugaite and Roy <ref type="bibr" target="#b24">[25]</ref> proposed a neural networks MF (NNMF) model to estimate rating by replacing the inner product in traditional MF to a feed-forward neural networks. Nonetheless, its performance is not competitive with the state-of-the-art methods. Wang et al. <ref type="bibr" target="#b25">[26]</ref> introduced CDL, which constructed the latent vector of item from item's content text by autoencoder and then the ratings are estimated via MF using this latent vector. Deng et al. <ref type="bibr" target="#b26">[27]</ref> proposed a deep learning-based MF, which employed deep autoencoder to generate initial vectors of users and items and adopted MF with the pretrained vectors to prediction for recommendation in social rating networks. However, it also requires additional information of user relationship and interest to produce estimation. Clearly, these methods still are the extensions of the traditional MF.</p><p>To overcome the setbacks of the CF-based methods above and develop a deep neural networks-based model beyond the simple extensions of MF, there are two crucial issues that are needed to be addressed: 1) the representations of users and items and 2) the architecture of the prediction neural networks. For the first issue, an effective features representation can significantly improve the performance on the relative tasks, such as the low-dimensional dense embedding for word and sentences <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and the image representation extracted from fixed pretraining neural networks <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>. However, unlike the form of representation of content which has been intensively exploited, generating the features of the user and the item from the ratings matrix (the usual data sources of CF-based method) for deep neural networks is still an open problem. For the second issue, the neural networks with special architecture adapted to a certain task have better performance than the normal neural networks, such as convolutional neural network (CNN) for computer vision <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> and the recurrent neural network (RNN) for NLP <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Hence, a well-designed architecture of the neural networks for CF-based RS is a pressing problem.</p><p>To effectively address the issues above, we propose to build prior-knowledge on users and items, then make prediction leveraging these obtained knowledge. Intuitively, the priorknowledge can facilitate and benefit the prediction of user behavior <ref type="bibr" target="#b36">[37]</ref>. This prior knowledge may originate from the past experience of the user <ref type="bibr" target="#b38">[38]</ref>. To build the prior-knowledge of users from their past experience, inspired by the word embedding in NLP <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b39">[39]</ref>, which can encode syntactic and semantic information of words into low-dimensional vector based on the context. We believe the semantic information of user can also be captured by learning the corresponding embedding from the "context" of the user, where the user-user co-occurrence in the user's past experience can be considered as the context of the user. Likewise, the knowledge of items can also be learned via item-item co-occurrence. Afterward, we propose using neural network to generate prediction from the prelearned embeddings of user and item. Consequently, This framework can be generally divided into two major stages: 1) understanding and 2) prediction. In the first stage, the embeddings of user and item can capture the user-user and item-item co-occurrence, respectively. In the second stage, the interaction between item and user can be simulated by the predictive neural network.</p><p>To construct the corresponding embeddings from the ratings matrix, we propose two different but complemental representational learning models. These include the constraint model (CM) and the rating independent model (RIM). To effectively extract the high-level features from the pretrained representations and estimate the rating accurately, we introduce several neural network methods that capture the interactions between users and items from two different directions: 1) current user and item and 2) the historical records of the items and users. For each direction, we implement two types of feed-forward neural networks that address different types of representations. Inspired by the multiview neural networks for content-base model <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref>, we developed a multiview feed-forward neural networks that will take the information of both given user and item with their corresponding historical information into consideration. Several experiments based on two benchmark datasets (MovieLens 1M and MovieLens 10M) show results close to the state of the art feed-forward neural network and shear outperforming results when compared to previous CF-based methods.</p><p>The main contributions of this paper are as follows.</p><p>1) We introduce an effective pipeline to address implementation of deep feed-forward neural networks to CF, which first learns the embeddings of users and items, and then builds neural networks on them. 2) Two different but complemental representational learning models are proposed to generate the low-dimensional local and global representations for both users and items. These representations grasp the co-occurrences of users and items. 3) A multiview feed-forward neural networks are proposed to take all factors into consideration to capture the interaction between user and item, which includes the view of current user and item as well as their historical background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OVERVIEW OF OUR MODEL</head><p>Generally, in our overall model, the target rating is estimated by a neural network which takes corresponding lowdimensional dense embeddings with respect to the given user and item, where the user and item embeddings are pretrained via special representation learning models before training the neural network. As can be seen in Fig. <ref type="figure" target="#fig_0">1</ref>, the operations in our model can be subdivided into two major stages.</p><p>1) Learning the Representations: The representations learning model generates the corresponding low-dimensional dense embeddings of users and items from the ratings matrix according to co-occurrence of user-user and item-item relation as shown in steps (a) and (b), respectively, where the processes of learning the representations of users and items are independent of each other. In this paper, there are two models CM and RIM which learn both user and item representations. These models are presented in Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Training the Prediction Neural Networks:</head><p>The target rating are estimated form a multiview neural network proposed in Section IV, whose inputs are the corresponding prelearned representations of the user and item these are CM and RIM learned representation as shown in step (c). The neural networks are trained from the rating-matrix according to the difference between the estimated rating and the real one as shown step (d).</p><p>Note that the same rating-matrix are used to train the embeddings and neural network. However, the perspectives to the rating-matrix are different, the embeddings are learned from the perspective of user-user and itemitem co-occurrence, but the network is learned from the perspective of user-item interaction. Our overall model has the following advantages. 1) The pretrained representations contain the co-occurrence information with respect to the user and item. In addition, the pretrained representations can be flexible and thus freely be used in the multiviews neural networks since they are weakly coupled. 2) Training the embedding and neural networks separately is more effective and it yields a better performance in our overall model than jointly training them. In addition, the separated learning processes can capture both user-user, item-item co-occurrence, and the user-item interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. REPRESENTATION LEARNING MODELS</head><p>Representational learning model aims to retain the semantic information by using an embedding of item and user denotation. In this section, we propose an effective method of learning the corresponding dense vector denotation of users and item, respectively, that captures the inter-relational information. First the co-occurrence of users and items of our model is introduced in Section III-A. Then, the model is extended to take account the rating information in Section III-B, in which we introduce two different yet complemental strategies. Finally, the approaches used for the model training are portrayed in Section III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning Representations via Co-Occurrence</head><p>Inspired by the idea of Pennington's method <ref type="bibr" target="#b39">[39]</ref> which can learn valid word vectors with semantic information by using the word-word co-occurrence in contexts, we believe that the similarity between items or users in RS can also be indicated by their co-occurrence.</p><p>Suppose that there are M items, N users for a given RS S and R N * M is the ratings matrix in S, we describe notations separately in Table <ref type="table">I</ref>.</p><p>Especially,</p><formula xml:id="formula_0">x j i = |U i ∩ U j | and xj i = |T i ∩ T j |.</formula><p>e i and r i should be learned in our method.</p><p>To learn item embeddings from C, similar to MF, we represent C as <ref type="figure">2</ref>. General flow of learning the embeddings of items includes four steps. In the first step (a), a set of users who marked a given item are extracted from the rating matrix in which each row and column represent a user and an item, respectively. Here, the value of an element is mapped in accordance with related rating [0 (no rating), 1 (negative rating), and 5 (positive rating)]. Afterward, the co-occurrence values of items are calculated according to the corresponding user sets in step (b), then in the following step (c) the itemitem co-occurrence matrix is constructed from the original rating matrix and the precomputed values of co-occurrence. Finally, in step (d), the item vectors can be learned from the item-item co-occurrence matrix via minimizing (4).</p><formula xml:id="formula_1">C = Ẽ ÊT = ẽ1 , ẽ2 • • • ẽM * ê1 , ê2 • • • êM T Fig.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I NOTATION DESCRIPTIONS</head><p>where Ẽ and Ê are two different matrices of embeddings of items which contain different semantic information of items, ẽi and êi are the column vectors denoted item i in Ẽ and Ê, respectively.</p><p>Since x j i = ẽi êj T , and the real value of x j i can be obtained by statistics on R. Thus, ẽi and êj can be learned by minimizing the difference between x j i and ẽi êj T . To smooth the co-occurrence value, log x j i is employed to replace the original x j i in our method. Consequently, the expression to capture the co-occurrence between t i and t j by the corresponding latent vectors can be denoted as log</p><formula xml:id="formula_2">x j i = ẽT i êj<label>(1)</label></formula><p>where ẽi and êi are two different vectors which should be learned by x j i , and each of them contains different semantic information, respectively.</p><p>Then, the overall semantic information of item t i can be denoted by</p><formula xml:id="formula_3">e i = ẽi , êi<label>(2)</label></formula><p>where e i is the concatenation of the vectors ẽi and êi includes the semantic information of both ẽi and êi . The concatenated vector can also reduce over-fitting and noise <ref type="bibr" target="#b39">[39]</ref>.</p><p>In addition, involving additional biases b i and b j with respect to items t i and t j into (1) can further enhance the effectiveness of embeddings, and the final expression becomes log</p><formula xml:id="formula_4">x j i = ẽi T êj + b i + b j . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>Therefore, items' corresponding embeddings can be learned via minimizing the squared error between the predicting cooccurrence count of items with the real one</p><formula xml:id="formula_6">min ẽ * ,ê * ,b * T t i ,t j ∈T,i =j,x j i &gt;0 (ẽ T i êj + b i + b j -log x j i ) 2 (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where T is the set of items. The main flow of learning embeddings of items is illustrated in Fig. <ref type="figure">2</ref>.</p><p>Similarly, for the user</p><formula xml:id="formula_8">Ĉ = Ũ ÛT = r1 , r2 • • • rN * r1 , r2 • • • rN T .</formula><p>Then, the cost function to learning r and r is denoted to</p><formula xml:id="formula_9">min r * ,r * , b * U u i ,u j ∈U,i =j,x j i &gt;0 rT i rj + bi + bj -log xj i 2 (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where bi is the bias of user i. Similar to (2), the embedding r i of u i can be denoted to</p><formula xml:id="formula_11">r i = [r i , ri ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Representations Using Ratings Information</head><p>So far our representation model without explicit consideration of rating can lead to the imprecise rating prediction in the later neural network. To address the rating information effectually, we propose two different yet complemental methods derived from the method in Section III-A: 1) CM and 2) RIM, which capture the overall co-occurrence and the local co-occurrence given by certain rating, respectively.</p><p>1) Constraint Model: Intuitively, the items are more similar if they are acquired more same rating from users. Otherwise, the items are irrelevant whose rating pairs are different usually. However, the basic model mentioned before cannot distinguish the pair of ratings. To address this problem of the basic model, a effective way is to count only the items giving the same ratings and drop the ones giving different ratings, this is named as CM.</p><p>Suppose that there are K kinds of ratings in S, let U k i be the set of users who give the item t i to k rating and z j,l i,k = |U k i ∩U l j | be the count of co-occurrence to the item t i with k rating and the item t j with l rating. Then,</p><formula xml:id="formula_12">y j i = K k=1 z j,k i,k</formula><p>is the total count of co-occurrence between t i and t j with the constraint, which takes account the one with same rating but drops the ones in different. Consequently, the cost function of CM can be denoted to</p><formula xml:id="formula_13">min ẽ * ,ê * ,b * J t = T t i ,t j ∈T,i =j,y j i &gt;0 ẽT i êj + b i + b j -log y j i 2 . (6)</formula><p>Similarly, the user representations can be learned by the cost function as follow:</p><formula xml:id="formula_14">min r * ,r * , b * J u = U u i ,u j ∈U,i =j,ŷ j i &gt;0 rT i rj + bi + bj -log ŷj i 2<label>(7)</label></formula><p>where</p><formula xml:id="formula_15">ŷj i = K k=1 ẑj,k i,k and ẑj,k i,k =| T k i ∩ T l j | and T k i is the set of items given rating k by u i .</formula><p>The main flow of CM (illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>) is similar to the one in Fig. <ref type="figure">2</ref>, but the way of counting the co-occurrence are replaced to the restrictive way in step (b). Thus, the generated co-occurrence matrix in Fig. <ref type="figure" target="#fig_1">3</ref> is different from the one in Fig. <ref type="figure">2</ref>, which conduces to produces more effective representation.</p><p>2) Rating Independent Model: CM forces on capturing the global relationship between items. However, the item with different ratings also implies different local information of item, and which cannot be revealed by CM. To address this problem, RIM is proposed, in which the item giving different ratings are denoted by different and independent embeddings.</p><p>Given different ratings, suppose that each item has K kinds of embeddings to represent an item, and e k i is the embedding of item t i with the kth rating. The value of co-occurrence between e k i and e l j is equal to z j,l i,k . Consequently, the cost function of this model is Similarly, RIM also can be adopted to learn the representations of users as follow:</p><formula xml:id="formula_16">min ẽ * ,ê * ,b * Ĵt = T t k i ,t l j ∈T,k =l,z tj,l ti,k &gt;0 ẽk i T êl j + b i + b j -log z tj,l ti,k 2 . (<label>8</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">min r * ,r * , b * Ĵu = U u k i ,u l j ∈U,k =l,ẑ j,l i,k &gt;0 rk i T rl j + bi + bj -log ẑtj,l ti,k 2<label>(9)</label></formula><p>where rk i and rk i are the user vectors for u i with rating k. The main flow of RIM to learning representations is illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>. The converted co-occurrence matrix is different from the ones in Figs. <ref type="figure">2</ref> and<ref type="figure" target="#fig_1">3</ref>, where each row and each column denote an item giving a certain rating. Thus, the calculation of a particular co-occurrence value related to certain row and column does not take the full range of ratings into account. Besides, the distinction between an item given different ratings can be exhibited by RIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization</head><p>To learning the item and user embeddings of CM, AdaGrad <ref type="bibr" target="#b42">[42]</ref> is employed to minimize the cost functions ( <ref type="formula">6</ref>)- <ref type="bibr" target="#b8">(9)</ref>. The derivations of the cost functions considering all parameters can be calculated as</p><formula xml:id="formula_19">∂J t ∂ ẽi = 2ψ ẽi , ∂J t ∂ êj = 2ψ êj , ∂J t ∂b i = 2ψ, ∂J t ∂b j = 2ψ ∂J u ∂ ri = 2ϕr i , ∂J u ∂ rj = 2ϕr j , ∂J u ∂ bi = 2ϕ, ∂J u ∂ bj = 2ϕ<label>(10)</label></formula><p>where</p><formula xml:id="formula_20">ψ = ẽT i êj + b i + b j -log y j i ϕ = rT i rj + bi + bj -log ŷj i .</formula><p>Similarly, to learning the representations of RIM, the derivations of ( <ref type="formula" target="#formula_16">8</ref>) and ( <ref type="formula" target="#formula_18">9</ref>) can be stated as</p><formula xml:id="formula_21">∂ Ĵt ∂ ẽk i = 2 ψ ẽk i , ∂ Ĵt ∂ êl j = 2 ψ êl j , ∂ Ĵt ∂b i = 2 ψ, ∂ Ĵt ∂b j = 2 ψ ∂ Ĵu ∂ rk i = 2 φr k i , ∂ Ĵu ∂ rl j = 2 φr l j , ∂ Ĵu ∂ bi = 2 φ, ∂ Ĵu ∂ bj = 2 φ (11)</formula><p>where</p><formula xml:id="formula_22">ψ = (ẽ k i ) T êl j + b i + b j -log z j,l i,k φ = (r k i ) T rl j + bi + bj -log ẑj,l i,k .</formula><p>Then, the parameters can be updated by</p><formula xml:id="formula_23">ẽi ← ẽi -η ∂J t ∂ ẽi , êj ← êj -η ∂J t ∂ êj , ẽk i ← ẽk i -η ∂ Ĵt ∂ ẽk i êl j ← êl j -η ∂ Ĵt ∂ êl j , ri ← ri -η ∂J u ∂ ri , rj ← rj -η ∂J u ∂ rj rk i ← rk i -η ∂ Ĵu ∂ rk i , rl j ← rl j -η ∂ Ĵu ∂ rl j (<label>12</label></formula><formula xml:id="formula_24">)</formula><p>where η is the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison</head><p>In contrast with RIM, CM captures the global cooccurrence, while RIM captures the local co-occurrence in certain ratings. We use a case to show the difference clear. Given items A and B, there are three kinds of different ratings combinations: 1) A@4-B@5; 2) A@4-B@4; and 3) A@5-B@5, then A@4-B@5 indicates items A is given 4-star rating, and items B is given 5-star.</p><p>In this case, There are a total of five RIM embedding. Two local RIM embeddings of item A ẽ4</p><p>A and ẽ5 A are employed to capture the semantic information of item A from the combinations. Similarly, there are two RIM embeddings ẽ4 B and ẽ5 B for item B. In here, A@4-B@5 can be captured by ẽ4 A and ê5 B . Likewise, A@4-B@4 can be captured by ẽ4</p><p>A and ê4 B , as well ẽ5 A and ê5 B for A@5-B@5. On the other hand, there are only two global CM embeddings ẽA and êB for items A and B to capture the overall co-occurrence of A and B. Note that A@4-B@4 and A@5-B@5 are considered in learning ẽA and êB , but A@4-B@5 is ignored, since we want capture rating similarity in CM rather than the differences.</p><p>As we can see from above example, the local co-occurrences with respect to different ratings can be explicitly revealed by RIM embeddings. For example, ẽ4</p><p>A indicates special information of item A on 4-star rating. But, the co-occurrence captured by CM is an overall representation. Generally, RIM can retain more general information, but it is slightly sensitive. On the other hand, CM is more robust, but it can only capture constrained information. In this, RIM and CM are of different perspective yet complemental. In addition, experiment results show the performance of using both of CM and RIM in the prediction neural networks is superior to the ones using only CM or RIM. CM and RIM are different from the traditional MF method. In MF, the learned user and item representations are directly related to rating prediction. On the other hand, our learned representations are not directly correlated to rating prediction, they focus on embedding the semantic information of user and item in to low-dimensional vectors. To generate the prediction, additional neural networks are employed, and the representations of CM or RIM are fed as inputs of the neural networks (the detail of prediction networks are exhibited in Section IV). Thus, the co-occurrence relationships of useruser and item-item can be grasped by our methods and the user-item interaction can be captured by the subsequent neural networks to predict the target rating. Capturing both relationships of co-occurrence and interaction is the advantage of our overall method with the traditional MF. Moreover, experiment results show it can outperform MF in terms of root mean square error (RMSE) and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PREDICTION NEURAL NETWORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overall Architecture of Multiviews Neural Networks</head><p>The learned CM and RIM embeddings include the semantic information with respect to the user and item. However, the embeddings cannot directly generate target rating. Thus, the additional component is required to make prediction from these embeddings. In this paper, neural networks are addressed to be the predictive component since neural networks can effectively extract features from prelearned embeddings and make accurate prediction. In addition, it can effectively merge the information from different sides.</p><p>The most direct path to follow in generating target rating of given user and item via neural network is to feed the corresponding pretrained CM or RIM embeddings of user and item into a feed-forward neural network, and the network will output the target rating. This way the prediction can be seen from the perspective of the user and item as it only depends on the two embedding vectors. However, its performance is not good enough. To improve the performance, we take additional historical-information into account, inspired by the SVD++ Fig. <ref type="figure">6</ref>. Architecture of the neural networks using only CM embeddings on the view of current user and item. The inputs are CM embeddings r i and e j with respect to user u i and item t j , respectively. α u and α t are the output of transformation layers for user and item, respectively, (denoted by green and red connection lines, respectively).</p><p>which considers additional item history information to make prediction. Thus, we propose a multiviews neural networks relying on both current and historical information. Moreover, both CM or RIM are fed into the multiviews neural networks to further enhance the performance since CM or RIM capture different kinds of information.</p><p>The overall architecture of the multiview neural networks is illustrated in Fig. <ref type="figure" target="#fig_3">5</ref>. The networks take the pretrained CM and RIM embeddings with respect to user and item as input, and it outputs the target rating, and the overall network can be separated into two major parts: 1) multiview features extracting and 2) integrated prediction.</p><p>To predict the rating that a user u i giving to t j , in the part of extracting feature, there are two kinds of views to be taken account: 1) current user and item: u i and t j themselves and 2) historical records: history items of u i and set of users who have rated t j .</p><p>During extracting features, each kind of representations is taken into account independently. The fully connected transformation layer and the convolutional transformation layer are fed to CM and RIM, respectively, to extract effective features from the original representations.</p><p>During integration of predictions, a merged layer is adapted to concatenate the features produced by the previous layers. After merging these predictions, a traditional feed-forward multilayers neural networks are built on the concatenated features to produce the final rating estimation.</p><p>Suppose that for the user u i and an item t j , r i is u i 's vector of CM and r k j is u i ' vector of RIM with rating k, where k = 1, 2, ....K. e j is t j 's vector of CM and e k j is a vector with rating k, where k = 1, 2, ....K. Let ẽi and rj be the historical features of u i and t j , respectively. Then, let α u and α t be the CM extracted features from the perspective of current user and item, and β u and β t be the RIM extracted features on the perspective of current user and item. Similarly, γ u and γ t be the CM extracted features from the perspective of historical records, and δ j i be the RIM extracted features from the perspective of historical records.</p><p>Thus, the concatenating features in the merged layer can be denoted as</p><formula xml:id="formula_25">μ t u = α u (ui), α t (tj), β u (ui), β t (tj), γ u (ui) γ t (tj), δ u (ui), δ t (tj)</formula><p>Fig. <ref type="figure">7</ref>. Architecture of the neural networks using only the RIM embeddings on the view of current user and item. Given ratings 1, 2, and 3, there are three RIM embeddings r 1 i , r 2 i and r 3 i for user u i . Likewise, there are three RIM embeddings e 1 j e 2 j and e 3 j for item t j . All of them are fed into the neural networks. β u and β t are the outputs of the convolutional transformation layers for user and item, respectively. and the rest of the model which is the prediction neural networks is, p i,j = f (μ t u ), where f is a feed-forward neural networks and p i,j is the predicted rating with respect to ui given to tj, and the activation function g in f is rectified linear unit which can be defined as g(x) = max(x, 0), however, the activation function does not apply to the last layer because p i,j is the real value.</p><p>The details of extracting features on the views of current and historical records are introduced in Sections IV-B and IV-C, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Networks on the Current User and Item View</head><p>Essentially, the transformation layer converts the representations in the original space into a new space, where the transformed features are more suitable for the final task. To address this transformation task for vectors of CM and RIM, we propose two kinds of neural networks architectures as follow.</p><p>1) Fully Connected Transformation Layer for CM Embeddings: For the CM embeddings of users and items, there exist two different original embedding spaces, the user space and the item space. To transform the original CM embeddings r i and e j , a traditional fully connected layers are used since the embeddings is overall and identical to the item or user. Let W e and W r be the fully connected weight matrices for the item and the user, respectively, then the fully connected layers can be denoted as follows:</p><formula xml:id="formula_26">α t (tj) = g(W e e j ), α u (ui) = g(W r r i ). (<label>13</label></formula><formula xml:id="formula_27">)</formula><p>Note that by using only CM embeddings, we can construct a single view prediction neural network in the perspective of current user and item as the architecture illustrated in Fig. <ref type="figure">6</ref>.</p><p>2) Convolutional Transformation Layer for RIM Embeddings: Given K kinds of rating scores, in RIM, user u i and item t j can be denoted as</p><formula xml:id="formula_28">ν i (ui) = r 1 i , r 2 i , . . . , r k i , μ j (tj) = e 1 j , e 2 j , . . . , e k j (<label>14</label></formula><formula xml:id="formula_29">)</formula><p>where ν i (ui) and μ j (tj) are the unique overall embeddings of u i and t j , respectively. Note that zeros vectors are used to denote the r k i and e k j which do not appear in the training set. In order to extract effective features from ν i and μ j , the most direct way is to subject each local user and item embeddings r k i or e k j to fully connected transformation matrices W k r and W e k , respectively, which are similar to the way single user and item in CM embeddings are processed. In addition, the multiple fully connected layers with respect to K rating scores can also be seen as two special locally connected layers handling ν i and μ j . Let β u and β t be the outputs of the locally connected layers handling ν i and μ j , respectively, the local connection layers can be denoted as follows:</p><formula xml:id="formula_30">β u (ui) = ν 1 i , ν 2 i , . . . ν k i , β t (tj) = μ 1 i , μ 2 i , . . . μ k i (<label>15</label></formula><formula xml:id="formula_31">)</formula><p>where ν k i and μ k i can be yielded by</p><formula xml:id="formula_32">ν k i = g W r k ν i (ui)[(k -1) × l r : k × l r ] μ k j = g W e k μ j (tj)[(k -1) × l e : k × l e ] (<label>16</label></formula><formula xml:id="formula_33">)</formula><p>where l u and l e are the dimensions of r k i and e k j , respectively, ν i (ui)[(k -1) × l e : k × l e ] is the subvector of ν i (ui) from the (k -1) × lth element to the k × lth element.</p><p>Because the user RIM embeddings regarded as different rating scores that actually exist on the same user space, various W k r in ( <ref type="formula" target="#formula_32">16</ref>) can be alternated by a shared transformation matrixes Ŵr between different ratings. Ŵr can be viewed as the weight to transform the original user space to the new features space. Similarly, different W k e in ( <ref type="formula" target="#formula_32">16</ref>) can be alternated by a shared Ŵe . Thus, ( <ref type="formula" target="#formula_32">16</ref>) can be modified to</p><formula xml:id="formula_34">ν k i (ui) = g Ŵr ν i (ui)[(k -1) × l r : k × l r ] μ k j (tj) = g Ŵe μ j (tj)[(k -1) × l e : k × l e ] . (<label>17</label></formula><formula xml:id="formula_35">)</formula><p>Note that these shared transformation layers can be seen as special convolutional layers. The architecture of the neural network using only RIM embeddings on the view of current user and item is illustrated in Fig. <ref type="figure">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Neural Networks on the Historical View</head><p>To effectively take the historical records into account, we propose simple but effective methods to generate features of historical records from the original CM and RIM embeddings of users and items, and then fit them into corresponding subneural networks on the two different views.</p><p>1) Fully Connected Transformation Layer for CM Historical Features: Suppose that T i is the set of historical items rated by user u i , and U j is the set of users marking item t j . The most direct way to represent u i 's historical records h(ui) is the averaged vectors of items in T i . t j 's historical records ĥ(tj) also can be obtained by the similar way. Formally, h(ui) and ĥ(tj) can be denoted as</p><formula xml:id="formula_36">h(ui) = t q ∈T i e q |T i | , ĥ(tj) = u q ∈U j r q U j . (<label>18</label></formula><formula xml:id="formula_37">)</formula><p>Similar to the neural network with the current user and item's view, addressed in CM embedding, fully connected transformation layers can be employed to handle the historical features which can be denoted as</p><formula xml:id="formula_38">γ u (ui) = g Whu h(ui) , γ t (tj) = g Wht ĥ(tj) (<label>19</label></formula><formula xml:id="formula_39">)</formula><p>where Whu and Wht are the fully connected transformation weight matrices to h and ĥ, respectively. The architecture of the neural network using CM historical features is similar to the one shown in Fig. <ref type="figure">6</ref>.</p><p>2) Convolutional Transformation Layer for RIM Historical Features: Similar to CM, the historical representations h k (ui) and ĥk (tj) with respect to u i and t j in the kth rating, respectively, can also be directly represented by the averaged embeddings. Let T k i be the set of items giving k rating by u i , and U k j be the set of users marking t j to k rating. Thus, h k (ui) and ĥk (th) can be denoted by</p><formula xml:id="formula_40">h k (ui) = t q ∈T k i e k q T k i , ĥk (tj) = u q ∈U k j r k q U k j . (<label>20</label></formula><formula xml:id="formula_41">)</formula><p>Then, the representations of user and item regarding K kinds of ratings p(ui) and p(tj) can be denoted as</p><formula xml:id="formula_42">p(ui) = h 1 (ui), h 2 (ui), . . . h K (ui) p(tj) = ĥ1 (tj), ĥ2 (tj), . . . ĥK (tj) . (<label>21</label></formula><formula xml:id="formula_43">)</formula><p>Similar to <ref type="bibr" target="#b14">(15)</ref>, δ u and δ t can be obtained by</p><formula xml:id="formula_44">δ u (ui) = p(ui) 1 , p(ui) 2 , . . . p(ui) K δ t (tj) = p(tj) 1 , p(tj) 2 , . . . p(tj) K (<label>22</label></formula><formula xml:id="formula_45">)</formula><p>where p(ui) k and p(tj) k can be yielded by</p><formula xml:id="formula_46">p(ui) k = g Ŵr ĥu i [(k -1) × l r : k × l r ] p(tj) k = g Ŵe ĥt j [(k -1) × l e : k × l e ] (<label>23</label></formula><formula xml:id="formula_47">)</formula><p>where Ŵr and Ŵe are shared weight matrices with respect to all kinds of ratings for the user and the item, respectively. Thus, this architecture is similar to the counterpart in RIM on the view of current as illustrated in Fig. <ref type="figure">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization</head><p>The cost function of the neural networks is Mean Square Error: 2 , where f ϑ denotes the multiview neural networks and D is the training set and r j i is the actual rating that u i gave to t j .</p><formula xml:id="formula_48">min (1/|D|) u i ,t j ∈D (f ϑ (u i , t j ) -r j i )</formula><p>The prediction neural networks are trained via stochastic gradient descent (SGD), the detailed processes of training is sketched in Fig. <ref type="figure" target="#fig_4">8</ref>, where the early stop condition is the improvement of loss in the validation set. Especially, the learning rate η is adjusted to the half of current η while the improvement is lower than the tolerance value ε, which can further reduce the loss in practice.</p><p>Note that we do not update any user and item vectors in term of CM and RIM in training prediction neural networks because of additional learning dose damage to the original semantic meaning of the representations. In addition, a certain user or item representation could occur in different branches of the neural networks which results in extremely sophisticated representations for training. </p><formula xml:id="formula_49">u i ,t j (f ϑ (u i , t j ) -r j i ) 2 .</formula><p>"bestloss" is the minimums Valloss obtained beforehand. η is the training rate and it will be adjusted in training. ε is the tolerance value determining the improvement in the learning rate and λ is the tolerance value determining the continuity of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Datasets</head><p>We evaluated the performance of our proposed method on two real-world datasets MovieLens 1M and MovieLens 10M. The details of the datasets are shown as follow.</p><p>1) MovieLens 1M contains around 1 million ratings of approximately 3900 movies by 6040 users in which there are five grade ratings and each user rated at least 20 items. 2) MovieLens 10M contains about 10 million ratings of 10 681 movies by 71 567 users in which there are ten grade ratings and each user also rated at least 20 items. Following LLORMA <ref type="bibr" target="#b43">[43]</ref>, 10% of the ratings in both datasets are randomly selected as the test set, leaving the remaining 90% of the samples as the training set. Among the samples in the training set, 1% are randomly chosen as the validation set. We use the averaged rating in training set to assign the items which do not contain training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of Learned Representations 1) Visualization of Learned Representations:</head><p>To exhibit the learned representations clearly, we visualize the items' RIM vectors in two dimensions by t-SNE <ref type="bibr" target="#b44">[44]</ref>, where the vectors are learned from MovieLen 1M. There are two visualized results exhibited in Fig. <ref type="figure" target="#fig_5">9</ref>(a) and (b).</p><p>In Fig. <ref type="figure" target="#fig_5">9</ref> (a), the items are classified into three classes including negative (rating 1 and 2), neutral (rating 3), and positive (rating 4 and 5). As can be seen in Fig. <ref type="figure" target="#fig_5">9</ref>(a), items are clustered very well and the positions of clusters have a sentiment decreasing relations, where the clusters of negative, neutral, and positive lie on the bottom, center, and top, respectively.</p><p>In Fig. <ref type="figure" target="#fig_5">9</ref>(b), the items are divided into five classes (the types of ratings). As can be seen in Fig. <ref type="figure" target="#fig_5">9</ref>(b), the adjacent clusters  are partially overlapped, such as 5 to 4 and 1 to 2 because closer ratings are too similar.</p><p>2) Analysis of Similarity: In CM and RIM, closer vectors are relative to meaning similarity. To show this property, we employ three cases and their top five most similar movies retrieved by the cosine similarity between vectors, and the results are showed in Tables II and III for RIM and CM, respectively.</p><p>As can be seen in Table <ref type="table" target="#tab_0">II</ref>, retrieved movies are similar to their corresponding cases. For example, in the case of Star Trek: First Contact, all of movies belong to the series of Star Trek. In the case of Toy story, all of them are cartoon.</p><p>As can be seen in Table <ref type="table" target="#tab_0">III</ref>, the results are different with the ones in Table <ref type="table" target="#tab_0">III</ref>, but they are also reasonable. In the case of Star Trek: First Contact, the retrieved movies also belong to the series of Star Trek. In the top five most similar movies of Toy story, although only Toy story 2 and Aladdin belong to the animated film, all of them have imaginative fantasy plot.</p><p>Consequently, both kinds of embeddings exhibit the characteristics on closer vectors with meaning similarity. In addition, for a item, different kinds of representations have varying perspective which conduces to their complementary property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Rating Prediction Accuracy 1) Evaluation Metric:</head><p>To measure the performance of prediction, we adopt the most popular metrics RMSE, RMSE = ( S i=1 (r ip i ) 2 /|S|), where S denotes the test set and |S| denotes the number of ratings in S. Following LLORMA <ref type="bibr" target="#b43">[43]</ref>, we report the average RMSE on test set over five different splits and compare our methods with some strong baseline methods.</p><p>2) Experiment Settings: In both datasets, the dimensions of the vectors of users and items in term of CM and RIM are equal to 400. To effectively train the multiviews prediction neural network using the pretrained vectors, we adopt SGD as the training method and L2-regularization are employed to prevent over-fitting. The number of layers of neural networks is equal to 5 (one input layer, one transformation layer, one merge layer, one hidden layer, and one output layer). The learning rates for both datasets are 0.0005. The bath-size for both datasets are 64. In both datasets, the numbers of cells in the FC layers in both views of current and history are both equal to 100. The numbers of cells in the CONV layers in the views of current and history are 200 and 150, respectively. The number of cells in the final hidden layer is 2000. The ratios of L2 in both datasets are 0.0015 and 0.0005, respectively.</p><p>3) Baseline Methods: We compare our model with the following strong baseline algorithms.</p><p>1) Bias Matrix Factorization (BMF) <ref type="bibr" target="#b6">[7]</ref>: It involves the biases of user and item to further improves the performance of traditional MF 2) LLORMA <ref type="bibr" target="#b43">[43]</ref>: Multiple low-rank submatrices are employed to represent the original rating matrix. 3) RBM-CF <ref type="bibr" target="#b8">[9]</ref>: RBM makes recommendation via reconstructing the missing value from known values. 4) Autorec <ref type="bibr" target="#b9">[10]</ref>: Similar to RBM-CF, but the RBM is replaced to auto-encoder. 5) CF-NADE <ref type="bibr" target="#b10">[11]</ref>: CF-NADE predict the rating by NADE which is also an alternative to RBM and it achieves stateof-the-art performance. 6) NNMF <ref type="bibr" target="#b24">[25]</ref>: A feed-forward neural networks estimate rating from the user and item, which is most similar model with our methods among the baseline methods. 7) Zanotti's Methods <ref type="bibr" target="#b45">[45]</ref>: It estimates the rating according to the similarity of the neighbors of the features, where the CBOW and Skip-gram models are adopted to training the features of users and items. In contrast with popular MF methods, our methods outperform or equal to LLORMA-LOCAL (state-of-the-art of MF method) in relatively small scale dataset MovieLens 1M. But multiview neural networks exceeds LLORMA-LOCAL by a relatively big margin (0.008) in the large scale MovieLens 10M. These results show that our methods have impressive performance on different scale datasets because our deep neural networks can capture more complex interaction than the simple vector inner product in MF.</p><p>Zanotti combined traditional MF with pretrained user and item vectors, which were trained via word2vec (a popular method to generate the word vectors in NLP). However, its best performance only has 0.905 in MovieLens 10M. This result shows that combination of our pretrained embeddings with the neural network is much more effective than Zanotti's model. NNMF replaces the inner product in MF to a feed-forward neural networks, our multiview neural networks significantly outperforms it by a large margin, which shows that using pretrained representations with the architecture of multiview can further improve the performance of the method using feed-forward neural networks.</p><p>As can be seen in Table <ref type="table" target="#tab_2">IV</ref>, RBM-like methods exhibit impressive performances. However, the RBM-like method cannot consistently achieve these results in a certain side. In the methods of RBM-CF, the best performances in MovieLens 1M and MovieLens 10M are acquired from the item side (I-RBM) and user side (U-RBM), respectively. Similarly, the best performances of CF-NADE in MovieLens 1M and MovieLens 10M are also obtained from different sides. In contrast with these models, our methods make recommendation from both the user side and the item side, and attain very comparable performance with the best model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance on Top-N Task 1) Metric and Experiment Settings:</head><p>The testing methodology adopted in this section is similar to <ref type="bibr" target="#b46">[46]</ref>, and the details of experiment are shown as follows. We measure the Top-N performance of our model on Movielens 1M similar to the experiments in Section V-C. Ninety percentage of ratings are randomly selected as the training set, but the test set contains only 5-star ratings of the remaining 10% samples.</p><p>We adopt the metric of recall to justify the performance of top-N task. In order to measure recall, we first train the model over the ratings in training set, where the training detail is the same as the one in Section V-C. Afterward, for each item i rated 5-stars by user u in test-set, We randomly select 1000 additional items unrated by user u. Further, we use the trained model first to obtain the estimated ratings of additional 1000 items as well as item i. Then, we form a ranked list l by ordering all the 1001 items according to their predicted ratings from large to small. Finally, We form a top-N recommendation list l -N by picking the N top ranked items from the list l. If item i is in list l -N, we have a hit to item i, otherwise we have a miss. Let #hits be the count of hitting items in test-set, and |T| be the number of ratings in test-set, the overall recall in test-set can be denoted as recall(N) = (#hits/|T|).</p><p>2) Baseline Methods: We compare our method with the baseline methods as follow.</p><p>1) Movie Average <ref type="bibr" target="#b46">[46]</ref>: Recommends top-N items with the highest average rating. 2) Top Popular <ref type="bibr" target="#b46">[46]</ref>: Recommends top-N items with the largest number of ratings. 3) BMF: As shown in Section V-C. 4) SVD++ <ref type="bibr" target="#b47">[47]</ref>: A hybrid CF method associated with bias MF with item-oriented neighborhood-based CF. 3) Performance Comparision: Fig. <ref type="figure" target="#fig_6">10</ref> reports the performance of different methods on the Movielens 1M. It is clear that our method can significantly outperform the baseline methods in terms of recall, where it can obtain 0.2816 in terms of recall at N = 10. The result shows that our method not only has impressive rating prediction accuracy but  also has a good Top-N recommendation accuracy. However, the methods with relatively higher RMSE performance cannot guarantee good performance in terms of top-N accuracy. For example, the biased MF can obtain a good performance 0.845 in terms of RMSE, while it can only achieve 0.1802 in terms of recall at N = 10 which is lower than 0.2489 achieved by the simple nonpersonalized method Top Popular at N = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Impact of Different Views</head><p>In Table <ref type="table" target="#tab_2">V</ref>, we report the rating prediction accuracy of the neural network utilizing various views including single-view and multiviews. In the view of current user and item, we construct two kinds of model which use the CM embedding or the RIM embedding. Likewise, there are also two kinds of embedding model used in the view of history. Note that the models using both CM and RIM embeddings are classified as multiview model. Among these single-view models, the one using RIM embedding on the historical record view is the best among them which can achieve RMSE of 0.835. This performance is close to the result 0.833 which is acquired by the final multiviews method. This suggests that embedding model of RIM on historical records has the greatest impact on the prediction of multiviews neural networks. While joint implementation of RIM and CM embeddings are synthetically considered, their performance (0.843) exceeds the corresponding single-view counterparts which are 0.846 and 0.845, respectively. Although the performance acquired by the method using both RIM and CM embeddings in the view of historical records does not outperform the single-view counterpart with RIM representations, their results are comparably very close. Therefore, the performances of multiviews models are generally better or equal to the counterparts using single view.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Impact of Transformation Layers</head><p>In Table <ref type="table" target="#tab_2">VI</ref>, we also report the RMSE performances of the models using different transformation layers. To compare conveniently, the neural networks are only considered in the view of current user and item. In the models using CM embedding, the one without fully connected layer can only achieve 0.852 RMSE value, while the other one with fully connected layer achieves a reduced value of RMSE 0.846. Similarly, among these methods using RIM embedding, the ones using different transformation layers also exceed the one without transformation layer. These results indicate that the method using transformation layer is better than the one without the transformation layer. In addition, the results of using local connection transformation layer and convolutional transformation layer (0.849 and 0.845) suggest that the performance can be improved by replacing the distinct connection layers to the shared connection layers for different ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Impact of Types of Embeddings</head><p>To investigate the effects on the overall performance from the embeddings standpoint, we present the results of prediction of the neural networks including view of current user and item with different kinds of embeddings in Table <ref type="table" target="#tab_2">VII</ref>. First, the result of using random embeddings of user and item is only equal to 0.9672 and it is much lower than the results of our methods, which suggests that the embeddings learned by our models can capture the effective relationship with respect to the users and the items. Skip-gram is a popular model for learning word vectors which are directly addressed of the learning of item and user embeddings <ref type="bibr" target="#b45">[45]</ref>. However, its performance is not completive with our methods because it does not take account of the rating information. Similarly, the performance of our basic model without the consideration of rating is also lower than CM and RIM, which explicitly considers the ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Complexity and Convergence Analysis</head><p>The convergence curves of multiviews neural networks on both datasets are illustrated in Fig. <ref type="figure" target="#fig_7">11</ref>(a) and (b), respectively, which show that RMSE of multiviews neural networks becomes stable after about 80 and 30 iterations on MovieLens 1M and MovieLens 10M, respectively. Thus, multiviews neural networks have a satisfying convergence rate and can be trained efficiently.</p><p>We trained the multiviews neural networks on a single GPU (Titan GTX). Table <ref type="table" target="#tab_4">VIII</ref> shows the running time of one epoch and the number of parameters of multiviews neural networks. Although training time in our model is relatively large. However, during prediction, the running time is negligible, where it only takes 1 min on 100K samples for 1M dataset and 8 min on 1M for 10M dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper presented a novel CF framework based on deep learning. The framework contains two stages: 1) learning low-dimensional embeddings for both users and items and 2) generating predicted ratings by using a multiview feedforward neural networks. In the future, we intend to improve our method form three aspects: 1) construct a end-to-end neural networks on history view. For example: a) RNN <ref type="bibr" target="#b34">[35]</ref> to consider the temporal relations between the history items of user and b) CNN <ref type="bibr" target="#b33">[34]</ref> to address overall historical data of users and items; 2) consider the content information, such as text, images, and video via appending additional view of content into our multiview neural networks; and 3) improving the training time of our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of our proposed model.</figDesc><graphic coords="2,322.79,49.49,227.26,173.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Main flow of learning CM item embeddings.</figDesc><graphic coords="4,311.89,53.36,250.77,185.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Main flow of learning RIM item embeddings.</figDesc><graphic coords="5,46.43,45.17,256.08,233.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Architecture of the multiviews neural networks includes two parts: multiview features extraction and prediction integration. In the multiview features extraction, the inputs are pretrained user and item vectors which are learned via CM and RIM methods and generated historical features. "FC" and "CONV" denoting fully connected transformation layer and convolutional transformation layer, respectively; FC and CONV are exhibited in Sections IV-B1 and IV-B2.</figDesc><graphic coords="6,64.37,52.59,219.61,139.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Framework of training the prediction neural networks takes prelearned and pregenerated vectors as inputs. "Valloss" is the MSE loss of validation set V in current epoch, which can be obtained by (1/|V|) u i ,t j (f ϑ (u i , t j )r</figDesc><graphic coords="8,344.99,45.17,184.50,175.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. t-SNE visualization of the RIM embedding of items. (a) Three classes, negative (red), neutral (blue), and positive (green). (b) Four classes, 1 rating (magenta), 2 rating (red), 3 rating (blue), 4 rating (yellow), and 5 rating (green).</figDesc><graphic coords="9,72.17,255.27,91.75,92.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Movielens 1M : recall at N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Convergence curves of our overall model on two datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,316.37,53.09,242.41,213.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II SIMILAR</head><label>II</label><figDesc>MOVIES ACQUIRED BY THE RIM EMBEDDING TABLE III SIMILAR MOVIES ACQUIRED BY THE CM EMBEDDING</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Multiview Neural Networks: It is our basic method, which includes the view of current and history. 9) Multiview Neural Networks+BiasMF: It is a special multiview neural networks. But it is has an additional branch on the view of current user and item, which employs the users and items vectors obtained from the pretraining BiasMF model.4) Prediction Performance Comparison:Table IV illustrates experimental results measured by RMSE on two datasets. we show results of our methods, multiview neural networks and multiview neural networks+BiasMF, and compare them with some strong baseline methods.</figDesc><table><row><cell>TABLE IV</cell></row><row><cell>PREDICTION PERFORMANCE COMPARISON ON TWO DATASETS</cell></row><row><cell>8)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell>MOVIELENS 1M: RMSE ON DIFFERENT VIEWS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VIII SCALABILITY</head><label>VIII</label><figDesc>OF OUR OVERALL MODEL</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Science Foundation of China under Grant 61573081 and Grant 61432012, and in part by the Foundation for Youth Science and Technology Innovation Research Team of Sichuan Province under Grant 2016TD0018. This paper was recommended by Associate Editor J. Liu.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>His current research interests include neural networks, machine learning, and recommendation system.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Amazon.com recommendations: Item-to-item collaborative filtering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003-02">Jan./Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Personalized news recommendation based on click behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM 15th Int. Conf. Intell. User Interfaces</title>
		<meeting>ACM 15th Int. Conf. Intell. User Interfaces<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved neighborhood-based collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD Cup Workshop 13th ACM SIGKDD Int. Conf. Knowl. Disc. Data Min</title>
		<meeting>KDD Cup Workshop 13th ACM SIGKDD Int. Conf. Knowl. Disc. Data Min</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lazy collaborative filtering for data sets with missing values</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1822" to="1834" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rating knowledge sharing in crossdomain collaborative filtering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1068" to="1082" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast maximum margin matrix factorization for collaborative prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Conf. Mach. Learn</title>
		<meeting>22nd Int. Conf. Mach. Learn<address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="713" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A nonnegative latent factor model for large-scale sparse matrices in recommender systems via alternating direction method</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="579" to="592" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Restricted Boltzmann machines for collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM 24th Int. Conf. Mach. Learn</title>
		<meeting>ACM 24th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM 24th Int. Conf. World Wide Web</title>
		<meeting>ACM 24th Int. Conf. World Wide Web<address><addrLine>Corvallis, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A neural autoregressive approach to collaborative filtering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Int. Conf. Mach. Learn</title>
		<meeting>33rd Int. Conf. Mach. Learn<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep pain: Exploiting long short-term memory networks for facial expression classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep collaborative filtering via marginalized denoising auto-encoder</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th ACM Int. Conf. Inf. Knowl. Manag</title>
		<meeting>24th ACM Int. Conf. Inf. Knowl. Manag<address><addrLine>Melbourne, VIC, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Organizing books and authors by multilayer SOM</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W S</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2537" to="2550" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Workshop Deep Learn</title>
		<meeting>1st Workshop Deep Learn<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multi-view deep learning approach for cross domain user modeling in recommendation systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Elkahky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th ACM Int. Conf. World Wide Web</title>
		<meeting>24th ACM Int. Conf. World Wide Web<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="278" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep neural networks for YouTube recommendations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th ACM Conf. Recommender Syst</title>
		<meeting>10th ACM Conf. Recommender Syst<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Neural network matrix factorization</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06443</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21th ACM SIGKDD Int. Conf. Knowl. Disc. Data Min</title>
		<meeting>21th ACM SIGKDD Int. Conf. Knowl. Disc. Data Min<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On deep learning for trust-aware recommendations in social networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1164" to="1177" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Int. Conf. Mach. Learn</title>
		<meeting>31st Int. Conf. Mach. Learn<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Workshops<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A generic deep-learning-based approach for automated surface inspection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Int. Speech Commun. Assoc. (INTERSPEECH)</title>
		<meeting>Conf. Int. Speech Commun. Assoc. (INTERSPEECH)<address><addrLine>Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey on visual surveillance of object motion and behaviors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="334" to="352" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">What are they up to? The role of sensory evidence and prior knowledge in action understanding</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chambon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">2011. 17133</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empir</title>
		<meeting>Conf. Empir</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Conf. Mach. Learn</title>
		<meeting>28th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for multidocument extractive summarization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3230" to="3242" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Local low-rank matrix approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Infusing collaborative recommenders with distributed representations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zanotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T K G</forename><surname>Immedisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gemmell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st ACM Workshop Deep Learn. Recommender Syst</title>
		<meeting>1st ACM Workshop Deep Learn. Recommender Syst</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Performance of recommender algorithms on top-N recommendation tasks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Conf. Recommender Syst</title>
		<meeting>4th ACM Conf. Recommender Syst<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: A multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th ACM SIGKDD Int. Conf. Knowl. Disc. Data Min</title>
		<meeting>14th ACM SIGKDD Int. Conf. Knowl. Disc. Data Min<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
