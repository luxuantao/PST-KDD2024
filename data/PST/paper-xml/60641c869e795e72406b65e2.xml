<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bidirectional Distillation for Top-ğ¾ Recommender System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-05">5 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wonbin</forename><surname>Kweon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seongku</forename><surname>Kang</surname></persName>
							<email>seongku@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
							<email>hwanjoyu@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bidirectional Distillation for Top-ğ¾ Recommender System</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-05">5 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449878</idno>
					<idno type="arXiv">arXiv:2106.02870v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems â†’ Collaborative filtering</term>
					<term>Learning to rank</term>
					<term>Retrieval efficiency Recommender System, Collaborative Filtering, Knowledge Distillation, Learning to Rank, Model Compression, Retrieval Efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommender systems (RS) have started to employ knowledge distillation, which is a model compression technique training a compact model (student) with the knowledge transferred from a cumbersome model (teacher). The state-of-the-art methods rely on unidirectional distillation transferring the knowledge only from the teacher to the student, with an underlying assumption that the teacher is always superior to the student. However, we demonstrate that the student performs better than the teacher on a significant proportion of the test set, especially for RS. Based on this observation, we propose Bidirectional Distillation (BD) framework whereby both the teacher and the student collaboratively improve with each other. Specifically, each model is trained with the distillation loss that makes to follow the other's prediction along with its original loss function. For effective bidirectional distillation, we propose rank discrepancy-aware sampling scheme to distill only the informative knowledge that can fully enhance each other. The proposed scheme is designed to effectively cope with a large performance gap between the teacher and the student. Trained in the bidirectional way, it turns out that both the teacher and the student are significantly improved compared to when being trained separately.</p><p>Our extensive experiments on real-world datasets show that our proposed framework consistently outperforms the state-of-the-art competitors. We also provide analyses for an in-depth understanding of BD and ablation studies to verify the effectiveness of each proposed component.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Nowadays, the size of recommender systems (RS) is continuously increasing, as they have adopted deep and sophisticated model architectures to better understand the complex relationships between users and items <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>. A large recommender with many learning parameters usually has better performance due to its high capacity, but it also has high computational costs and long inference time. This problem is exacerbated for web-scale applications having numerous users and items, since the number of the learning parameters increases proportionally to the number of users and items. Therefore, it is challenging to adopt such a large recommender for real-time and web-scale platforms.</p><p>To tackle this problem, a few recent work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23</ref>] has adopted Knowledge Distillation (KD) to RS. KD is a model-agnostic strategy that trains a compact model (student) with the guidance of a pretrained cumbersome model (teacher). The distillation is conducted in two stages; First, the large teacher recommender is trained with user-item interactions in the training set with binary labels (i.e., 0 for unobserved interaction and 1 for observed interaction.). Second, the compact student recommender is trained with the recommendation list predicted by the teacher along with the binary training set. Specifically, in <ref type="bibr" target="#b22">[23]</ref>, the student is trained to give high scores on the top-ranked items of the teacher's recommendation list. Similarly, in <ref type="bibr" target="#b15">[16]</ref>, the student is trained to imitate the teacher's prediction scores with particular emphasis on the high-ranked items in the teacher's recommendation list. The teacher's predictions provide additional supervision, which is not explicitly revealed from the binary training set, to the student. By distilling the teacher's knowledge, the student can achieve comparable performance to the teacher with fewer learning parameters and lower inference latency.</p><p>Despite their effectiveness, they still have some limitations. First, they rely on unidirectional knowledge transfer, which distills knowledge only from the teacher to the student, with an underlying assumption that the teacher is always superior to the student. However, based on the in-depth analyses provided in Section 2, we argue that the knowledge of the student also could be useful for the teacher. Indeed, we demonstrate that the teacher is not always superior to the student and further the student performs better than the teacher on a significant proportion of the test set. In specific, the student recommender better predicts 36âˆ¼42% of the test interactions than the teacher recommender. These proportions are remarkably large compared to 4âˆ¼9% from our experiment on the computer vision task. In this regard, we claim that both the student and the teacher can take advantage of each other's complementary knowledge, and be improved further. Second, the existing methods have focused on distilling knowledge of items ranked highly by the teacher. However, we observe that most of the items ranked highly by the teacher are already ranked highly by the student (See Section 2 for details). Therefore, merely high-ranked items may not be informative to fully enhance the other model, which leads to limiting the effectiveness of the distillation.</p><p>We propose a novel Bidirectional Distillation (BD) framework for RS. Unlike the existing methods that transfer the knowledge only from the pre-trained teacher recommender, both the teacher and the student transfer their knowledge to each other within our proposed framework. Specifically, they are trained simultaneously with the distillation loss that makes to follow the other's predictions along with the original loss function. Trained in the bidirectional way, it turns out that both the teacher and the student are significantly improved compared to when being trained separately. In addition, the student recommender trained with BD achieves superior performance compared to the student trained with conventional distillation from a pre-trained teacher recommender.</p><p>For effective bidirectional distillation, the remaining challenge is to design an informative distillation strategy that can fully enhance each other, considering the different capacities of the teacher and the student. As mentioned earlier, items merely ranked highly by the teacher cannot give much information to the student. Also, it is obvious that the student's knowledge is not always helpful to improve the teacher, as the teacher has a much better overall performance than the student. To tackle this challenge, we propose rank discrepancy-aware sampling scheme differently tailored for the student and the teacher. In the scheme, the probability of sampling an item is defined based on its rank discrepancy between the two recommenders. Specifically, each recommender focuses on learning the knowledge of the items ranked highly by the other recommender but ranked lowly by itself. Taking into account the performance gap between the student and the teacher, we enable the teacher to focus on the items that the student has very high confidence in, whereas making the student learn the teacher's broad knowledge on more diverse items.</p><p>The proposed BD framework can be applicable in many application scenarios. Specifically, it can be used to maximize the performance of the existing recommender in the scenario where there is no constraint on the model size (in terms of the teacher), or to train a small but powerful recommender as targeted in the conventional KD methods (in terms of the student). The key contributions of our work are as follows:</p><p>â€¢ Through our exhaustive analyses on real-world datasets, we demonstrate that the knowledge of the student recommender also could be useful for the teacher recommender. Based on the results, we also point out the limitation of the existing distillation methods for RS under the assumption that the teacher is always superior to the student. â€¢ We propose a novel bidirectional KD framework for RS, named BD, enabling that the teacher and the student can collaboratively improve with each other during the training. BD also adopts the rank discrepancy-aware sampling scheme differently designed for the teacher and the student considering their capacity gap. â€¢ We validate the superiority of the proposed framework by extensive experiments on real-world datasets. BD considerably outperforms the state-of-the-art KD competitors. We also provide both qualitative and quantitative analyses to verify the effectiveness of each proposed component<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ANALYSIS: TEACHER VS STUDENT</head><p>In this section, we provide detailed analyses of comparison between the teacher and the student on two real-world datasets: CiteULike <ref type="bibr" target="#b25">[26]</ref> and Foursquare <ref type="bibr" target="#b27">[28]</ref>. Following <ref type="bibr" target="#b15">[16]</ref>, we hold out the last interaction of each user as test interaction, and the rest of the user-item interactions are used for training data (see Section 5.1 for details of the experiment setup). We use NeuMF <ref type="bibr" target="#b4">[5]</ref> as a base model, and adopt NeuMF-50 as the teacher and NeuMF-5 as the student. The number indicates the dimension of the user and item embedding; the number of learning parameters of the teacher is 10 times bigger than that of the student, and accordingly, the teacher shows a much better overall performance than the student (reported in Table <ref type="table" target="#tab_2">2</ref>). Note that the teacher and the student are trained separately without any distillation technique in the analyses.</p><p>The teacher is not always superior to the student. Figure <ref type="figure" target="#fig_0">1a</ref> shows bar graphs of the rank difference between the teacher and the student on the test interactions (the bars in the graphs are sorted in increasing order along with the x-axis). The rank difference on a test interaction (ğ‘¢, ğ‘–) is defined as follows:</p><formula xml:id="formula_0">Rank diff. ğ‘¢ (ğ‘–) = ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘¢ ğ‘‡ (ğ‘–) âˆ’ ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘¢ ğ‘† (ğ‘–),<label>(1)</label></formula><p>where ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘¢ ğ‘‡ (ğ‘–) and ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘¢ ğ‘† (ğ‘–) denote the ranks assigned by the teacher and the student on the item ğ‘– for the user ğ‘¢ respectively, and ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘¢ * (ğ‘–) = 1 is the highest ranking. If the rank difference is bigger than zero, it means that the student assigns a higher rank than the teacher on the test interaction; the student better predicts the test interaction than the teacher. We observe that the student model assigns higher ranks than the teacher model on the significant proportion of the entire test interactions: 36% for CiteULike and 42% for Foursquare. These proportions are remarkably large compared to 4% for CIFAR-10 and 9% for CIFAR-100 from our experiments on computer vision task <ref type="foot" target="#foot_1">2</ref> .</p><p>The results raise a few questions. Why the student can perform better than the teacher on the test data and why this phenomenon is intensified for RS? We provide some possible reasons as follows: Firstly, not all user-item interactions require sophisticated calculations in high-dimensional space for correct prediction. As shown in the computer vision <ref type="bibr" target="#b10">[11]</ref>, a simple image without complex background or patterns can be better predicted at the lower layer than the final layer in a deep neural network. Likewise, some user-item relationships can be better captured based on simple operations without expensive computations. Moreover, unlike the image classification task, RS has very high ambiguity in nature; in many applications, the user's feedback on an item is given in the binary form: 1 for observed interaction, 0 for unobserved interaction. However, the binary labels do not explicitly show the user's preference. For example, "0" does not necessarily mean the user's negative preference for the item. It can be that the user may not be aware of the item. Due to the ambiguity in the ground-truth labels, the lower training loss does not necessarily guarantee a better ranking performance. Specifically, in the case of NeuMF, which is trained with the binary cross-entropy loss, the teacher can better minimize the training loss and thus better discriminate the observed/unobserved interactions in the training set. However, this may not necessarily result in better predicting user's preference on all the unobserved items due to the ambiguity of supervision.</p><p>The high-ranked items are not that informative as expected. Figure <ref type="figure" target="#fig_0">1b</ref> shows the rank comparison between the teacher and the student. We plot the high-ranked items in the recommendation list from the teacher and that from the student for all users <ref type="foot" target="#foot_2">3</ref> . Each point corresponds to (ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘¢ ğ‘† (ğ‘–), ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘¢ ğ‘‡ (ğ‘–)). We observe that most of the points are located near the lower-left corner, which means that most of the items ranked highly by the teacher are already ranked highly by the student. In this regard, unlike the motivation of the previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>, items merely ranked highly by the teacher may be not informative enough to fully enhance the student, and thus may limit the effectiveness of the distillation. We argue that each model can be further improved by focusing on learning the knowledge of items ranked highly by the other recommender but ranked lowly by itself (e.g., points located along the x-axis and y-axis).</p><p>In summary, the teacher is not always superior to the student, so the teacher can also learn from the student. Their complementarity should be importantly considered for effective distillation in RS. Also, the distillation should be conducted with consideration of the rank discrepancy between the two recommenders. Based on the observations, we are motivated to design a KD framework that the teacher and the student can collaboratively improve with each other based on the rank discrepancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>Let the set of users and items be denoted as U = {ğ‘¢ 1 , ğ‘¢ 2 , ..., ğ‘¢ ğ‘› } and I = {ğ‘– 1 , ğ‘– 2 , ..., ğ‘– ğ‘š }, where ğ‘› is the number of users and ğ‘š is the number of items. Let R âˆˆ {0, 1} ğ‘›Ã—ğ‘š be the user-item interaction matrix, where ğ‘Ÿ ğ‘¢ğ‘– = 1 if the user ğ‘¢ has an interaction with the item ğ‘–, otherwise ğ‘Ÿ ğ‘¢ğ‘– = 0. Also, for a user ğ‘¢, I âˆ’ ğ‘¢ = {ğ‘– âˆˆ I|ğ‘Ÿ ğ‘¢ğ‘– = 0} denotes the set of unobserved items and I + ğ‘¢ = {ğ‘– âˆˆ I|ğ‘Ÿ ğ‘¢ğ‘– = 1} denotes the set of interacted items. A top-ğ¾ recommender system aims to find a recommendation list of unobserved items for each user. To make the recommendation list, the system predicts the score rğ‘¢ğ‘– = ğ‘ƒ (ğ‘Ÿ ğ‘¢ğ‘– = 1|ğ‘¢, ğ‘–) for each item ğ‘– in I âˆ’ ğ‘¢ for each user ğ‘¢, then ranks the unobserved items according to their scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>We propose a novel Bidirectional Distillation (BD) framework for top-ğ¾ recommender systems. Within our proposed framework, both the teacher and the student transfer their knowledge to each other. We first provide an overview of BD (Section 4.1). Then, we formalize the distillation loss to transfer the knowledge between the two recommenders (Section 4.2). We also propose rank discrepancyaware sampling scheme differently tailored for the teacher and the student to fully enhance each other (Section 4.3). Lastly, we provide the details of the end-to-end training process within the proposed framework (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows an overview of BD. Unlike the existing KD methods for RS, both the teacher and the student are trained simultaneously by using each other's knowledge (i.e., recommendation list) along with the binary training set (i.e., user-item interactions). First, the teacher and the student produce the recommendation list for each user. Second, BD decides what knowledge to be transferred for each distillation direction based on the rank discrepancy-aware sampling scheme. Lastly, the teacher and the student are trained with the distillation loss along with the original collaborative filtering loss. To summarize, each of them is trained as follows:</p><formula xml:id="formula_1">L ğ‘‡ (ğœƒ ğ‘‡ ) = L ğ¶ğ¹ (ğœƒ ğ‘‡ ) + ğœ† ğ‘†â†’ğ‘‡ â€¢ L ğµğ· (ğœƒ ğ‘‡ ; ğœƒ ğ‘† ), L ğ‘† (ğœƒ ğ‘† ) = L ğ¶ğ¹ (ğœƒ ğ‘† ) + ğœ† ğ‘‡ â†’ğ‘† â€¢ L ğµğ· (ğœƒ ğ‘† ; ğœƒ ğ‘‡ ),<label>(2)</label></formula><p>where ğ‘‡ and ğ‘† denote the teacher and the student respectively, ğœƒ * is the model parameters. L ğ¶ğ¹ is the collaborative filtering loss depending on the base model which can be any existing recommender, and L ğµğ· is the bidirectional distillation loss. Lastly, ğœ† ğ‘†â†’ğ‘‡ and ğœ† ğ‘‡ â†’ğ‘† are the hyperparameters that control the effects of the distillation loss in each direction. Within BD, the teacher and the student are collaboratively improved with each other based on their complementarity. Also, during the training, the knowledge distilled between the teacher and the student gets gradually evolved along with the recommenders; the improvement of the teacher leads to the acceleration of the student's learning, and the accelerated student again improves the teacher's learning. Trained in the bidirectional way, both the teacher and the student are significantly improved compared to when being trained separately. As a result, the student trained with BD outperforms the student model trained with the conventional distillation that relies on a pre-trained and fixed teacher. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distillation Loss</head><p>We formalize the distillation loss that transfers the knowledge between the recommenders. By following the original distillation loss that matches the class distributions of two classifiers for a given image <ref type="bibr" target="#b6">[7]</ref>, we design the distillation loss for the user ğ‘¢ as follows:</p><formula xml:id="formula_2">L ğµğ· (ğœƒ ğ‘‡ ; ğœƒ ğ‘† ) = âˆ‘ï¸ ğ‘— âˆˆR D S ğ‘†â†’ğ‘‡ ( I âˆ’ ğ‘¢ ) L ğµğ¶ğ¸ ( rğ‘‡ ğ‘¢ ğ‘— , rğ‘† ğ‘¢ ğ‘— ) L ğµğ· (ğœƒ ğ‘† ; ğœƒ ğ‘‡ ) = âˆ‘ï¸ ğ‘— âˆˆR D S ğ‘‡ â†’ğ‘† ( I âˆ’ ğ‘¢ ) L ğµğ¶ğ¸ ( rğ‘† ğ‘¢ ğ‘— , rğ‘‡ ğ‘¢ ğ‘— ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">L ğµğ¶ğ¸ (ğ‘, ğ‘) = ğ‘ log ğ‘ + (1 âˆ’ ğ‘) log(1 âˆ’ ğ‘) is the binary cross- entropy loss, rğ‘¢ ğ‘— = ğ‘ƒ (ğ‘Ÿ ğ‘¢ ğ‘— = 1|ğ‘¢, ğ‘—)</formula><p>is the prediction of a recommender and RDS * (I âˆ’ ğ‘¢ ) is a set of the unobserved items sampled by the rank discrepancy-aware sampling. rğ‘¢ ğ‘— is computed by ğœ (ğ‘§ ğ‘¢ ğ‘— /ğ‘‡ ) where ğœ (â€¢) is the sigmoid function, ğ‘§ ğ‘¢ ğ‘— is the logit, and ğ‘‡ is the temperature that controls the smoothness.</p><p>Our distillation loss is a binary version of the original KD loss function. Similar to the original KD loss transferring the knowledge of class probabilities, our loss transfers a user's potential positive and negative preferences on the unobserved items. Specifically, in the binary training set, the unobserved interaction ğ‘Ÿ ğ‘¢ ğ‘— is only labeled as "0". However, as mentioned earlier, it is ambiguous whether the user actually dislikes the item or potentially likes the item. Through the distillation, each recommender can get the other's opinion of how likely (and unlikely) the user would be interested in the item, and such information helps the recommenders to better cope with the ambiguous nature of RS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Rank Discrepancy-aware Sampling</head><p>We propose the rank discrepancy-aware sampling scheme that decides what knowledge to be transferred for each distillation direction. As we observed in Section 2, most of the items ranked highly by the teacher are already ranked highly by the student and vice versa. Thus, the existing methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref> that simply choose the high-ranked items cannot give enough information to the other recommender. Moreover, for effective bidirectional distillation, the performance gap between the teacher and the student should be carefully considered in deciding what knowledge to be transferred, as it is obvious that not all knowledge of the student is helpful to improve the teacher.</p><p>In this regard, we develop a sampling scheme based on the rank discrepancy of the teacher and the student, and tailor it differently for each distillation direction with consideration of their different capacities. The underlying idea of the scheme is that each recommender can get informative knowledge by focusing on the items ranked highly by the other recommender, but ranked lowly by itself. The sampling strategy for each distillation direction is defined as follows:</p><p>Distillation from the teacher to the student. As the teacher has a much better overall performance than the student, the opinion of the teacher should be considered more reliable than that of the student in most cases. Thus, for this direction of the distillation, we make the student follow the teacher's predictions on many rankdiscrepant items. Formally, for each user ğ‘¢, the probability of an item ğ‘– to be sampled is computed as follows:</p><formula xml:id="formula_4">ğ‘ ğ‘‡ â†’ğ‘† (ğ‘–) âˆ ğ‘¡ğ‘ğ‘›â„(max((ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘† (ğ‘–) âˆ’ ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘‡ (ğ‘–)) â€¢ ğœ– ğ‘¡ , 0)),<label>(4)</label></formula><p>where ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘‡ (ğ‘–) and ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘† (ğ‘–) denote the ranks assigned by the teacher and the student on the item ğ‘– for the user ğ‘¢ respectively, and ğ‘Ÿğ‘ğ‘›ğ‘˜ * (ğ‘–) = 1 is the highest ranking <ref type="foot" target="#foot_3">4</ref> . We use a hyper-parameter ğœ– ğ‘¡ (&gt; 0) to control the smoothness of the probability. With this probability function, we sample the items ranked highly by the teacher but ranked lowly by the student. Since ğ‘¡ğ‘ğ‘›â„(â€¢) is a saturated function, items with rank discrepancy above a particular threshold would be sampled almost uniformly. As a result, the student learns the teacher's broad knowledge of most of the rank-discrepant items.</p><p>Distillation from the student to the teacher. As shown in Section 2, the teacher is not always superior to the student, especially for RS. That is, the teacher can be also further improved by learning from the student. However, at the same time, the large performance gap between the teacher and the student also needs to be considered for effective distillation. For this direction of the distillation, we make the teacher follow the student's predictions on only a few selectively chosen rank-discrepant items. The distinct probability function is defined as follows:</p><formula xml:id="formula_5">ğ‘ ğ‘†â†’ğ‘‡ (ğ‘–) âˆ ğ‘’ğ‘¥ğ‘ ((ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘‡ (ğ‘–) âˆ’ ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘† (ğ‘–)) â€¢ ğœ– ğ‘’ ),<label>(5)</label></formula><p>where ğœ– ğ‘’ (&gt; 0) is a hyper-parameter to control the smoothness of the probability. We use the exponential function to put particular emphasis on the items that have large rank discrepancies. Therefore, this probability function enables the teacher to follow the student's Update ğœƒ ğ‘† predictions only on the rank-discrepant items that the student has very high confidence in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Training</head><p>Algorithm 1 describes a pseudo code for the end-to-end training process within BD framework. The training data D consists of observed interactions (ğ‘¢, ğ‘–). First, the model parameters ğœƒ ğ‘‡ and ğœƒ ğ‘† are warmed up only with the collaborative filtering loss (line 1), as the predictions during the first few epochs are very unstable. Second, we make the recommenders produce the recommendation lists for the subsequent sampling. Since it is time-consuming to produce the recommendation lists every epoch, we conduct this step every ğ‘ epochs (line 3-4). Next, we decide what knowledge to be transferred in each distillation direction via the rank discrepancy-aware sampling. It is worth noting that the unobserved items sampled by the rank discrepancy-aware sampling can be used also for the collaborative filtering loss. Finally, we compute the losses with the sampled items for the teacher and the student, respectively, and update the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we validate our proposed framework on 9 experiment settings (3 real-world datasets Ã— 3 base models). We first introduce our experimental setup (Section 5.1). Then, we provide a performance comparison supporting the superiority of the BD (Section 5.2). We also provide two analyses for the in-depth understanding of BD (Section 5.3, 5.4). Lastly, We provide an ablation study to verify the effectiveness of rank discrepancy-aware sampling (Section 5.5) and analyses for important hyperparameters of BD (Section 5.6). Foursquare <ref type="foot" target="#foot_5">6</ref> (Tokyo Check-in) <ref type="bibr" target="#b27">[28]</ref> and Yelp<ref type="foot" target="#foot_6">7</ref>  <ref type="bibr" target="#b5">[6]</ref>. We only keep users who have at least five ratings for CiteULike and Foursquare, ten ratings for Yelp as done in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. Data statistics after the preprocessing are presented in Table <ref type="table" target="#tab_1">1</ref>. We also report the experimental results on ML100K and AMusic, which are used for CD <ref type="bibr" target="#b15">[16]</ref>, in Appendix for the direct comparison.</p><p>5.1.2 Evaluation Protocol and Metrics. We adopt the widely used leave-one-out evaluation protocol. For each user, we hold out the last interacted item for testing and the second last interacted item for validation as done in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>. If there is no timestamp in the dataset, we randomly take two observed items for each user. Then, we evaluate how well each method can rank the test item higher than all the unobserved items for each user (i.e., I âˆ’ ğ‘¢ ). Note that instead of randomly choosing a predefined number of candidates (e.g., 99), we adopt the full-ranking evaluation that uses all the unobserved items as candidates. Although it is time-consuming, it enables a more thorough evaluation compared to using random candidates <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>As we focus on top-ğ¾ recommendation for implicit feedback, we employ two widely used metrics for evaluating the ranking performance of recommenders: Hit Ratio (H@ğ¾) <ref type="bibr" target="#b17">[18]</ref> and Normalized Discounted Cumulative Gain (N@ğ¾) <ref type="bibr" target="#b7">[8]</ref>. H@ğ¾ measures whether the test item is present in the top-ğ¾ list and N@ğ¾ assigns a higher score to the hits at higher rankings in the top-ğ¾ list. We compute those two metrics for each user, then compute the average score. Lastly, we report the average value of five independent runs for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Base Models</head><p>. BD is a model-agnostic framework applicable for any top-ğ¾ RS. We validate BD with three base models that have different model architectures and learning strategies. Specifically, we choose two widely used deep learning models and one latent factor model as follows:</p><p>â€¢ NeuMF <ref type="bibr" target="#b4">[5]</ref>: A deep recommender that adopts Matrix Factorization (MF) and Multi-Layer Perceptron (MLP) to capture complex and non-linear user-item relationships. NeuMF uses the pointwise loss function for the optimization.</p><p>â€¢ CDAE <ref type="bibr" target="#b26">[27]</ref>: A deep recommender that adopts Denoising Autoencoders (DAE) <ref type="bibr" target="#b24">[25]</ref> for the collaborative filtering. CDAE uses the point-wise loss function for the optimization.</p><p>â€¢ BPR <ref type="bibr" target="#b20">[21]</ref>: A learing-to-rank recommender that adopts MF <ref type="bibr" target="#b12">[13]</ref> to model the user-item interaction. BPR uses the pair-wise loss function for the optimization under the assumption that observed items are more preferred than unobserved items. H@100 N@50 N@100 H@50 H@100 N@50 N@100 H@50 H@100 N@50 N@100 Teacher 0.1566 â€¢ Ranking Distillation (RD) <ref type="bibr" target="#b22">[23]</ref>: A pioneering KD method for top-ğ¾ RS. RD makes the student give high scores on top-ranked items by the teacher. â€¢ Collaborative Distillation (CD) <ref type="bibr" target="#b15">[16]</ref>: A state-of-the-art KD method for top-ğ¾ RS. CD makes the student imitate the teacher's scores on the items ranked highly by the teacher.</p><p>5.1.5 Implementation Details. For all the base models and baselines, we use PyTorch <ref type="bibr" target="#b19">[20]</ref> for the implementation. For each dataset, hyperparameters are tuned by using grid searches on the validation set. We use Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with L2 regularization and the learning rate is chosen from {0.00001, 0.0001, 0.001, 0.002}, and we set the batch size as 128. For NeuMF, we use 2-layer MLP for the network. For CDAE, we use 2-layer MLP for the encoder and the decoder, and the dropout ratio is set to 0.5. The number of negative samples is set to 1 for NeuMF and BPR, 5 * |I + ğ‘¢ | for CDAE as suggested in the original paper <ref type="bibr" target="#b26">[27]</ref>.</p><p>For the distillation, we adopt as many learning parameters as possible for the teacher model until the ranking performance is no longer increased on each dataset. Then, we build the student model by employing only one-tenth of the learning parameters used by the teacher. The number of model parameters of each base model is reported in Table <ref type="table" target="#tab_4">3</ref>. For KD competitors (i.e., RD, CD), ğœ† ğ¾ğ· is chosen from {0.01, 0.1, 0.5, 1}, the number of items sampled for the distillation is chosen from {10, 15, 20, 30, 40, 50}, and the temperature ğ‘‡ for logits is chosen from {1, 1.5, 2}. For other hyperparameters, we use the values recommended from the public implementation and the original papers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>. For BD, ğœ† ğ‘‡ â†’ğ‘† and ğœ† ğ‘†â†’ğ‘‡ are set to 0.5, the number of items sampled by rank discrepancy-aware sampling is chosen from {1, 5, 10}, ğœ– ğ‘¡ is chosen from {10 âˆ’2 , 10 âˆ’3 , 10 âˆ’4 , 10 âˆ’5 }, ğœ– ğ‘’ is chosen from {10 âˆ’3 , 10 âˆ’4 , 10 âˆ’5 }, the temperature ğ‘‡ for logits is chosen from {2, 5, 10} and the rank updating period ğ‘ (in Algorithm 1) is set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Comparison</head><p>Table <ref type="table" target="#tab_2">2</ref> shows the recommendation performance of each KD method on three real-world datasets and three different base models. In Table <ref type="table" target="#tab_2">2</ref>, "Teacher" and "Student" indicate the base models trained separately without any distillation technique, "BD-Teacher" and "BD-Student" are the teacher and the student trained simultaneously with BD. "CD" and "RD" denote the student trained with CD and RD, respectively. We analyze the experimental results from various perspectives. We first observe that the teacher recommender is consistently improved by the knowledge transferred from the student with BD by up to 20.79% (for H@50 on CiteULike). This result verifies our claim that the knowledge of the student also could be useful for improving the teacher. Also, this result strongly indicates that the distillation process of BD effectively resolves the challenge of a large performance gap, and successfully transfer the informative knowledge from the student. In specific, the ranking discrepancyaware sampling enables the teacher to focus on the items that the student has very high confidence in. This strategy can minimize the adverse effects from the performance gap, making the teacher be further improved based on the complementary knowledge from the student.</p><p>Second, the student recommender is significantly improved by the knowledge transferred from the teacher with BD by up to 39.88% (for N@50 on CiteULike). Especially, the student trained with BD considerably outperforms the student trained with the existing KD methods (i.e., RD, CD) by up to 13.94% (for H@100 on Yelp). The superiority of BD comes from the two contributions; BD makes the student follow the teacher's predictions on the rank-discrepant items which are more informative than the merely high-ranked items. Also, within BD, the improvement of the teacher leads to the acceleration of the student's learning, and the accelerated student again improves the teacher's learning. With better guidance from the improved teacher, the student with BD can achieve superior performance than the student with the existing KD methods. To verify the effectiveness of each contribution, we conduct in-depth analyses in the next sections.</p><p>Lastly, Table <ref type="table" target="#tab_4">3</ref> shows the model size and online inference efficiency for each base model. For making the inferences, we use PyTorch with CUDA on GTX Titan X GPU and Intel i7-7820X CPU. As the student has only one-tenth of the learning parameters used by the teacher, the student requires less computational costs, thus achieves lower inference latency. Moreover, the student trained with BD shows comparable or even better recommendation performance to the teacher (e.g., NeuMF and CDAE on Foursquare). On CiteULike, we observe that the student achieves comparable performance by employing 20âˆ¼30% of learning parameters used by the teacher. These results show that BD can be effectively adopted to train a small but powerful recommender. Moreover, as mentioned earlier, BD is also applicable in setting where there is no constraint on the model size or inference time. Specifically, BD can be adopted to maximize the performance of a large recommender with numerous learning parameters (i.e., the teacher).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Size Analysis</head><p>We control the size of the teacher and the student to analyze the effects of the capacity gap between two recommenders on BD. We report the results of a deep model and a latent factor model (i.e., NeuMF and BPR) on CiteULike. Figure <ref type="figure">3a</ref> shows the performance of the teacher trained with the student of varying sizes and Figure <ref type="figure">3b</ref> shows the performance of the student trained with the teacher of varying sizes. "S" indicates the size of the student (10% of the teacher), "M" indicates the medium size (50% of the teacher), and "T" indicates the size of the teacher. Also, "fixed S" refers to the pretrained student recommender with the size of "S", "fixed T" refers to the pre-trained teacher recommender with the size of "T". Note that "fixed S/T" are not updated during the training (i.e., unidirectional knowledge distillation).</p><p>First, we observe that both the teacher and the student achieves the greatest performance gain when the capacity gap between two recommenders is largest; the teacher shows the best performance with the smallest student (i.e., S in Fig. <ref type="figure">3a</ref>) and the student performs best with the largest teacher (i.e., T in Fig. <ref type="figure">3b</ref>). As mentioned in Section 2, the teacher and the student have complementarity as some user-item relationships can be better captured without expensive computations. In this regard, such complementarity can be maximized when the capacity gap between the two recommenders is large enough. This result supports our claim that the performance gain comes from the different but reciprocal knowledge of two recommenders. Also, it is worth noting that there are still performance improvements when two recommenders have identical sizes (i.e., T in Fig. <ref type="figure">3a</ref>, S in Fig. <ref type="figure">3b</ref>). This can be understood as a kind of self-distillation effect <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref> when the teacher has the same size as the student. Although they have very similar kinds of knowledge, they can still regularize each other, preventing its counterpart from being overfitted to a few observed interactions.</p><p>Second, the teacher is more improved when it is trained along with the learning student than when it is trained with the fixed student (i.e., S vs. fixed S in Fig. <ref type="figure">3a</ref>). Similarly, the student is more improved when it is trained together with the learning teacher than when it is trained with the fixed teacher (i.e., T vs. fixed T in Fig. <ref type="figure">3b</ref>). In the unidirectional distillation, the pre-trained recommender (i.e., fixed S/T) is no longer improved, thus always conveys the same knowledge during the training. On the contrary, within BD, both recommenders are trained together, thus the knowledge distilled between the recommenders gets gradually evolved as they are improved during the training. As a result, each recommender can be further improved based on the evolved knowledge of its  counterparts. This result again shows the superiority of our bidirectional distillation over the unidirectional distillation of the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Synchronization Analysis</head><p>We perform in-depth analysis to investigate how well the teacher and the student learn each other's complementary knowledge within the proposed framework. Specifically, we evaluate how much synchronized the two recommenders are within BD, which shows that they are improved based on each other's complementarity. To quantify the degree of the synchronization, we define (normalized) Average Rank Difference as follows:</p><formula xml:id="formula_6">Average Rank Diff. = 1 ğ‘› â€¢ ğ‘š âˆ‘ï¸ D ğ‘¡ğ‘’ğ‘ ğ‘¡ |Rank Diff. ğ‘¢ (ğ‘–)|,<label>(6)</label></formula><p>where ğ‘› and ğ‘š are the numbers of users and items, respectively, and D ğ‘¡ğ‘’ğ‘ ğ‘¡ is the test set that contains the held-out observed interaction for each user. The rank difference (Rank Diff. ğ‘¢ (ğ‘–)) is defined in Equation <ref type="formula" target="#formula_0">1</ref>.</p><p>Table <ref type="table" target="#tab_5">4</ref> shows the change in the average rank difference after the training with BD. We observe that the average rank difference gets consistently decreased on all the datasets. This result indicates that the teacher and the student get synchronized by transferring their knowledge to each other. Figure <ref type="figure" target="#fig_3">4</ref> shows the rank difference between the teacher and the student after the training with BD. We adopt NeuMF as the base model as done in Section 2. Note that the (normalized) average rank difference is proportional to the extent of the blue area. We observe that the blue area in Figure <ref type="figure" target="#fig_3">4</ref> shrinks compared to that in Figure <ref type="figure" target="#fig_0">1</ref> after the training with BD. Interestingly, we observe that the teacher is significantly improved on CiteULike dataset (by up to 20.79%) which has the largest average rank difference change before and after the training with BD. The large change indicates that the two recommenders get synchronized well during the training, which leads to significant improvements based on each other's knowledge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Sampling Scheme Analysis</head><p>We examine the effects of diverse sampling schemes on the performance of BD to verify the superiority of the proposed sampling scheme. Note that the schemes decide what knowledge to be distilled within BD. We compare five different sampling schemes as follows: 1) Rank discrepancy-aware sampling, 2) Rank-aware sampling <ref type="bibr" target="#b15">[16]</ref>, 3) Top-ğ‘ selection <ref type="bibr" target="#b22">[23]</ref>, 4) Uniform sampling, 5) Swapped rank discrepancy-aware sampling.</p><p>The rank discrepancy-aware sampling, which is our proposed scheme, focuses on the rank-discrepant items between the teacher and the student. On the other hand, the rank-aware sampling (adopted in CD) and top-ğ‘ selection (adopted in RD) focus on the items ranked highly by one recommender. The uniform sampling randomly chooses items from the entire recommendation list. Finally, the swapped rank discrepancy-aware sampling, which is the ablation of the proposed scheme, swaps the sampling probability function of each distillation direction; we make the teacher follow the student on most of the rank-discrepant items (with tanh), and make the student imitate only a few predictions of the teacher (with exp).</p><p>Table <ref type="table" target="#tab_6">5</ref> shows the performance of BD with different sampling schemes. We first observe that the proposed scheme achieves the best result both for the teacher and the student. This result verifies the effectiveness of two components of the proposed scheme: 1) sampling based on the rank discrepancy, 2) probability functions differently designed for each distillation direction. Specifically, we can see the effectiveness of rank discrepant-based sampling by comparing our sampling scheme with rank-aware sampling. As observed in Section 2, the items merely ranked highly by the teacher may be not informative enough to fully enhance the student. With the proposed scheme, BD focuses on the rank-discrepant items, thus can further improve the recommenders. Also, we observe that swapping the probability function of each distillation direction (i.e., the swapped rank discrepancy-aware) significantly degrades the performance. This result strongly indicates that each probability function is well   designed to effectively cope with the large performance gap of two recommenders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Hyperparameter Analysis</head><p>In this section, we provide thorough analyses that examine the effects of important hyperparameters on BD. For the sake of the space, we report the results of NeuMF on CiteULike dataset. We observe similar tendencies with other base models and datasets.</p><p>First, Figure <ref type="figure" target="#fig_4">5</ref> shows the performance of the teacher and the student with varying ğœ† ğ‘‡ â†’ğ‘† and ğœ† ğ‘†â†’ğ‘‡ which control the effects of the distillation losses. The best performance is achieved when both ğœ† ğ‘‡ â†’ğ‘† and ğœ† ğ‘†â†’ğ‘‡ are around 0.5. Also, we observe that both the recommenders are considerably improved when ğœ† ğ‘‡ â†’ğ‘† and ğœ† ğ‘†â†’ğ‘‡ have similar values (i.e., diagonal entries). Moreover, we observe that the performance of the student is more robust with respect to ğœ† than that of the teacher. We believe that this is because the student has a lower overall performance than the teacher, thus can easily take advantage of the teacher in broad settings.</p><p>Second, Figure <ref type="figure" target="#fig_5">6</ref> shows the performance of the teacher and the student with varying ğœ– ğ‘¡ and ğœ– ğ‘’ which control the smoothness of the probability functions in rank discrepancy-aware sampling. When ğœ– ğ‘¡ and ğœ– ğ‘’ are small, the probability functions become smooth. The best performance is achieved when both parameters are around 10 âˆ’4 .</p><p>When ğœ– ğ‘’ is bigger than 10 âˆ’3 , the probability ğ‘ ğ‘†â†’ğ‘‡ (â€¢) gets too sharp. Thus, the teacher cannot fully learn the student's complementary knowledge, which leads to degraded performance.</p><p>Lastly, Figure <ref type="figure" target="#fig_6">7</ref> shows the performance of the teacher and the student with varying rank updating period ğ‘ (in Algorithm 1). Since it is time-consuming to generate the recommendation list every epoch, we update the recommendation list of the two models every ğ‘ epoch. In this paper, we use ğ‘ = 10 which shows the comparable performance to the upper-bound (ğ‘ = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Reducing inference latency of RS. Several methods have been proposed for reducing the model size and inference time of recommender systems. First, a few work adopt discretization techniques to reduce the size of recommenders <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>. They learn discrete representations of users and items to make portable recommenders and successfully reduce the model size. However, their recommendation performance is highly limited due to the restricted capability and thus the loss of recommendation performance is unavoidable <ref type="bibr" target="#b15">[16]</ref>. Second, several methods try to accelerate the inference phase by adopting model-dependent techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>. For example, the order-preserving transformations <ref type="bibr" target="#b0">[1]</ref> and the pruning techniques <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref> have been adopted to reduce the computational costs of the inner product. Although they can reduce the inference latency, they are applicable only to specific models (e.g., inner product-based models).</p><p>To tackle this challenge, a few recent methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23</ref>] have adopted knowledge distillation to RS. KD is a model-agnostic strategy and we can employ any recommender system as the base model. RD <ref type="bibr" target="#b22">[23]</ref> firstly proposes a KD method that makes the student give high scores on the top-ranked items of the teacher's recommendation list. Similarly, CD <ref type="bibr" target="#b15">[16]</ref> makes the student imitate the teacher's prediction scores with particular emphasis on the items ranked highly by the teacher. The most recent work, RRD <ref type="bibr" target="#b8">[9]</ref>, formulates the distillation process as a relaxed ranking matching problem between the ranking list of the teacher and that of the student. Since it is daunting for the small student to learn all the prediction results from the large teacher, they focus on the high-ranked items, which can affect the top-ğ¾ recommendation performance <ref type="bibr" target="#b8">[9]</ref>. By using such supplementary supervisions from the teacher, they have successfully improved the performance of the student. However, they have some critical limitations in that 1) they rely on the unidirectional distillation, 2) the high-ranked items are not informative enough. These limitations are thoroughly analyzed and resolved in this work.</p><p>Training multiple models together. There have been successful attempts to train multiple models simultaneously for better generalization performance in computer vision and natural language processing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>. In computer vision, Deep Mutual Learning (DML) <ref type="bibr" target="#b31">[32]</ref> trains a cohort of multiple classifiers simultaneously in a peer-teaching scenario where each classifier is trained to follow the predictions of the other classifiers along with its original loss. Since each classifier starts from a different initial condition, each classifier's predicted probabilities of the next most likely class vary. DML claims that those secondary quantities provide extra information to the other classifiers that can help them to converge to a more robust minima. Also, Collaborative Learning <ref type="bibr" target="#b21">[22]</ref> trains several classifier heads of the same network simultaneously by sharing intermediate-level representations. Collaborative Learning argues that the consensus of multiple views from different heads on the same data provides both supplementary information and regularization to each classifier head. In natural language processing, Dual Learning <ref type="bibr" target="#b2">[3]</ref> proposes a learning mechanism that two machine translators teach each other to reduce the costs of human labeling. Pointing out that the machine translation can be considered as a dual-task forming a closed loop (e.g., English-to-French/ French-to-English), Dual Learning generates informative feedback by transferring their outputs to each other and achieves considerable performance improvements without the involvement of a human labeler. Although the aforementioned methods have effectively increased performance by training multiple models simultaneously, they mostly employ models with the same sizes focusing only on achieving better generalization based on the consensus of various views.</p><p>In this paper, we propose a novel Bidirectional Distillation framework whereby two recommenders of different sizes are trained together by transferring their knowledge to each other. Pointing out that the large recommender and the small recommender have different but complementary knowledge, we design an effective distillation mechanism that considers their huge capacity gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We propose a novel Bidirectional Distillation framework for top-ğ¾ recommender systems whereby the teacher and the student are collaboratively improved with each other during the training. Within our proposed framework, both the teacher and the student transfer their knowledge to each other with the distillation loss. Also, our framework considers the capacity gap between the teacher and the student with differently tailored rank discrepancy-aware sampling.</p><p>Our extensive experiments on real-world datasets show that BD significantly outperforms the state-of-the-art competitors in terms of the student recommender. Furthermore, the teacher model also gets the benefit of the student and performs better than when being trained separately. We also provide analyses for the in-depth understanding of BD and verifying the effectiveness of each proposed component.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance analyses of the teacher and the student. In (a), rank differences are computed on the test interactions and the test interactions (x-axis) are sorted in increasing order by the rank difference. In (b), each dot represents a sampled unobserved interaction.</figDesc><graphic url="image-1.png" coords="2,55.20,83.69,124.84,83.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of Bidirectional Distillation (BD) for top-ğ¾ recommender systems.</figDesc><graphic url="image-5.png" coords="4,104.24,83.69,403.52,131.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )Figure 3 :</head><label>a3</label><figDesc>Figure 3: Effect of the model size on BD. The blue dashed line indicates the performance of the model trained separately.</figDesc><graphic url="image-6.png" coords="7,55.20,83.69,124.84,83.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Rank difference after the training with BD.</figDesc><graphic url="image-10.png" coords="8,53.80,204.20,120.12,80.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Recommendation performance (H@50) of teacher and student with varying lambda.</figDesc><graphic url="image-12.png" coords="9,53.80,83.69,240.23,90.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Recommendation performance (H@50) of teacher and student with varying smoothing factor.</figDesc><graphic url="image-13.png" coords="9,53.80,212.54,120.12,80.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Recommendation performance (H@50) of teacher and student with varying updating period.</figDesc><graphic url="image-15.png" coords="9,53.80,331.06,120.12,80.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1: Bidirectional Distillation Framework. Input : Training data D, the number of total epochs ğ‘’, rank updating period ğ‘ Output : Teacher model (ğœƒ ğ‘‡ ), Student model (ğœƒ ğ‘† ) 1 Warm up ğœƒ ğ‘‡ and ğœƒ ğ‘† with only L ğ¶ğ¹ 2 for ğ‘¡ = 0, 1, ..., (ğ‘’ âˆ’ 1) do ğ¶ğ¹ (ğœƒ ğ‘‡ ) and L ğµğ· (ğœƒ ğ‘‡ ; ğœƒ ğ‘† )</figDesc><table><row><cell>3</cell><cell>if ğ‘¡ % ğ‘ == 0 then</cell></row><row><cell>4</cell><cell cols="2">Teacher and Student update their recommendation</cell></row><row><cell></cell><cell>lists</cell></row><row><cell>5</cell><cell>for (ğ‘¢, ğ‘–) âˆˆ D do</cell></row><row><cell></cell><cell>/* Train Teacher</cell><cell>*/</cell></row><row><cell>6</cell><cell>Draw RDS ğ‘†â†’ğ‘‡ (I âˆ’ ğ‘¢ ) with probability ğ‘ ğ‘†â†’ğ‘‡ (â€¢)</cell></row><row><cell>8</cell><cell>Update ğœƒ ğ‘‡</cell></row><row><cell></cell><cell>/* Train Student</cell><cell>*/</cell></row></table><note>7 Compute L 9 Draw RDS ğ‘‡ â†’ğ‘† (I âˆ’ ğ‘¢ ) with probability ğ‘ ğ‘‡ â†’ğ‘† (â€¢) 10 Compute L ğ¶ğ¹ (ğœƒ ğ‘† ) and L ğµğ· (ğœƒ ğ‘† ; ğœƒ ğ‘‡ ) 11</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Data Statistics</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Users #Items #Ratings Sparsity</cell></row><row><cell>CiteULike</cell><cell>5,219</cell><cell>25,187 130,788</cell><cell>99.90%</cell></row><row><cell cols="2">Foursquare 2,293</cell><cell>61,858 537,167</cell><cell>99.62%</cell></row><row><cell>Yelp</cell><cell cols="2">25,677 25,815 730,623</cell><cell>99.89%</cell></row><row><cell cols="3">5.1 Experimental Setup</cell><cell></cell></row><row><cell cols="4">5.1.1 Datasets. We use three real-world datasets: CiteULike 5 [26],</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison. Improv.T denotes the improvement of the teacher over the original teacher model ("Teacher" in this table), Improv.B denotes the improvement of the student over the best competitive method and Improv.S denotes the improvement of the student over the original student model ("Student" in this table). * and * * indicate ğ‘ â‰¤ 0.05 and ğ‘ â‰¤ 0.01 for the paired t-test of BD vs. the best competitor.</figDesc><table><row><cell>Base Model Method</cell><cell>H@50</cell><cell>CiteULike</cell><cell>Foursquare</cell><cell>Yelp</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Model size and online inference efficiency. Time denotes the wall time used for generating recommendation list for every user.</figDesc><table><row><cell>Base Model</cell><cell cols="6">CiteULike #Params. Time #Params. Time #Params. Time Foursquare Yelp</cell></row><row><cell>NeuMF-T</cell><cell>3.05M</cell><cell>17.39s</cell><cell>6.42M</cell><cell>14.77s</cell><cell>5.15M</cell><cell>85.12s</cell></row><row><cell>NeuMF-S</cell><cell>0.30M</cell><cell>14.12s</cell><cell>0.64M</cell><cell>7.99s</cell><cell>0.51M</cell><cell>69.53s</cell></row><row><cell>CDAE-T</cell><cell>2.80M</cell><cell>14.73s</cell><cell>6.36M</cell><cell>7.95s</cell><cell>3.89M</cell><cell>64.45s</cell></row><row><cell>CDAE-S</cell><cell>0.30M</cell><cell>10.35s</cell><cell>0.69M</cell><cell>5.78s</cell><cell>0.41M</cell><cell>53.39s</cell></row><row><cell>BPR-T</cell><cell>1.52M</cell><cell>9.11s</cell><cell>3.21M</cell><cell>7.71s</cell><cell>2.57M</cell><cell>46.83s</cell></row><row><cell>BPR-S</cell><cell>0.15M</cell><cell>7.94s</cell><cell>0.32M</cell><cell>5.39s</cell><cell>0.26M</cell><cell>40.49s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Average Rank Difference before and after the training with BD.</figDesc><table><row><cell cols="2">Base Model</cell><cell cols="2">CiteULike Foursquare</cell><cell>Yelp</cell></row><row><cell>NeuMF</cell><cell>before BD after BD</cell><cell>0.1944 0.1323</cell><cell>0.1020 0.0957</cell><cell>0.0918 0.0824</cell></row><row><cell>CDAE</cell><cell>before BD after BD</cell><cell>0.1190 0.0871</cell><cell>0.0963 0.0883</cell><cell>0.0544 0.0472</cell></row><row><cell>BPR</cell><cell>before BD after BD</cell><cell>0.1380 0.1064</cell><cell>0.1180 0.1082</cell><cell>0.0560 0.0511</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Recommendation performance (H@50) of BD with different sampling schemes on CiteULike. Numbers in bold face are the best results.</figDesc><table><row><cell>Base Model</cell><cell>Sampling Scheme</cell><cell cols="2">Teacher Student</cell></row><row><cell></cell><cell>Rank discrepancy-aware</cell><cell>0.1722</cell><cell>0.0924</cell></row><row><cell></cell><cell>Rank-aware [16]</cell><cell>0.1617</cell><cell>0.0812</cell></row><row><cell>NeuMF</cell><cell>Top-ğ‘ selection [23]</cell><cell>0.1480</cell><cell>0.0766</cell></row><row><cell></cell><cell>Uniform</cell><cell>0.1590</cell><cell>0.0726</cell></row><row><cell></cell><cell>Swapped rank discrepancy-aware</cell><cell>0.1512</cell><cell>0.0747</cell></row><row><cell></cell><cell>Rank discrepancy-aware</cell><cell>0.1983</cell><cell>0.0943</cell></row><row><cell></cell><cell>Rank-aware [16]</cell><cell>0.1818</cell><cell>0.0891</cell></row><row><cell>CDAE</cell><cell>Top-ğ‘ selection [23]</cell><cell>0.1757</cell><cell>0.0870</cell></row><row><cell></cell><cell>Uniform</cell><cell>0.1788</cell><cell>0.0819</cell></row><row><cell></cell><cell>Swapped rank discrepancy-aware</cell><cell>0.1733</cell><cell>0.0851</cell></row><row><cell></cell><cell>Rank discrepancy-aware</cell><cell>0.1479</cell><cell>0.0853</cell></row><row><cell></cell><cell>Rank-aware [16]</cell><cell>0.1367</cell><cell>0.0805</cell></row><row><cell>BPR</cell><cell>Top-ğ‘ selection [23]</cell><cell>0.1264</cell><cell>0.0772</cell></row><row><cell></cell><cell>Uniform</cell><cell>0.1306</cell><cell>0.0749</cell></row><row><cell></cell><cell>Swapped rank discrepancy-aware</cell><cell>0.1281</cell><cell>0.0803</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We provide the source code of BD at https://github.com/WonbinKweon/BD_ WWW2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We compare the predictions of the teacher and the student on the test set. We adopt ResNet-80 as the teacher and ResNet-8 as the student<ref type="bibr" target="#b3">[4]</ref> and train them separately for the image classification task on CIFAR-10 and CIFAR-100 dataset<ref type="bibr" target="#b14">[15]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We sample the items by using the rank-aware sampling scheme suggested in<ref type="bibr" target="#b15">[16]</ref>. Note that in<ref type="bibr" target="#b15">[16]</ref>, the sampled items are used for the distillation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">we omit the superscript ğ‘¢ from ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘¢ * (ğ‘–) for the simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/changun/CollMetric/tree/master/citeulike-t</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://sites.google.com/site/yangdingqi/home/foursquare-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://github.com/hexiangnan/sigir16-eals/blob/master/data/yelp.rating</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the NRF grant funded by the MSIT (South Korea, No. 2020R1A2B5B03097210), and the IITP grant funded by the MSIT (South Korea, No. 2018-0-00584, 2019-0-01906).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>In this section, we report the experimental results on ML100K and AMusic, which are used for CD <ref type="bibr" target="#b15">[16]</ref>, for the direct comparison with CD. We do not include this result in our main table, because we consider those datasets are relatively small to simulate the realworld evaluation (ML100K has only 943 users and 1682 items). We adopt CDAE as the base model and we employ 2-layer MLP for the encoder and the decoder of CDAE. Experimental results on their experimental settings are as follows: All results are the average of five iterations and statistically significant with p=0.01. The proposed approach (BD) still outperforms the best competitor (CD) both on ML100K and AMusic. Moreover, the ranking performance of the teacher also increases with BD.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces</title>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liran</forename><surname>Katzir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Nice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Paquet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Born again neural networks</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast matrix factorization for online recommendation with implicit feedback</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName><forename type="first">Kalervo</forename><surname>JÃ¤rvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaana</forename><surname>KekÃ¤lÃ¤inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS</title>
		<imprint>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DE-RRD: A Knowledge Distillation Framework for Recommender System</title>
		<author>
			<persName><forename type="first">Seongku</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonbin</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shallow-deep networks: Understanding and mitigating network overthinking</title>
		<author>
			<persName><forename type="first">Yigitcan</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><surname>Dumitras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On Sampled Metrics for Item Recommendation</title>
		<author>
			<persName><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collaborative Distillation for Top-N Recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FEXIPRO: fast and exact inner product retrieval in recommender systems</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Tsz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><forename type="middle">Lung</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName><surname>Mamoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Relaxed Ranking-Based Factor Model for Recommender System from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Huayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Xing Xie, and Longbing Cao. 2017. Discrete Content-Aware Matrix Factorization</title>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Collaborative learning for deep neural networks</title>
		<author>
			<persName><forename type="first">Guocong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ranking distillation: Learning compact ranking models with high performance for recommender system</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lemp: Fast retrieval of large entries in a matrix product</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Mykytiuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In SIGMOD</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collaborative topic regression with social regularization for tag recommendation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu-Jun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collaborative denoising auto-encoders for top-n recommender systems</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling user activity preference by leveraging user spatial temporal characteristics in LBSNs</title>
		<author>
			<persName><forename type="first">Dingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="129" to="142" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discrete Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discrete personalized ranking for fast collaborative filtering from implicit feedback</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guowu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning binary codes for collaborative filtering</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
