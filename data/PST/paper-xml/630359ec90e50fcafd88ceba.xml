<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JuryGCN: Quantifying Jackknife Uncertainty on Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jian</forename><surname>Kang</surname></persName>
							<email>jiank2@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Qinghai</forename><surname>Zhou</surname></persName>
							<email>qinghai2@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign Hanghang Tong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">JuryGCN: Quantifying Jackknife Uncertainty on Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539286</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph neural networks</term>
					<term>uncertainty quantification</term>
					<term>jackknife</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Network (GCN) has exhibited strong empirical performance in many real-world applications. The vast majority of existing works on GCN primarily focus on the accuracy while ignoring how confident or uncertain a GCN is with respect to its predictions. Despite being a cornerstone of trustworthy graph mining, uncertainty quantification on GCN has not been well studied and the scarce existing efforts either fail to provide deterministic quantification or have to change the training procedure of GCN by introducing additional parameters or architectures. In this paper, we propose the first frequentist-based approach named JuryGCN in quantifying the uncertainty of GCN, where the key idea is to quantify the uncertainty of a node as the width of confidence interval by a jackknife estimator. Moreover, we leverage the influence functions to estimate the change in GCN parameters without re-training to scale up the computation. The proposed JuryGCN is capable of quantifying uncertainty deterministically without modifying the GCN architecture or introducing additional parameters. We perform extensive experimental evaluation on real-world datasets in the tasks of both active learning and semi-supervised node classification, which demonstrate the efficacy of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>‚Ä¢ Information systems ‚Üí Data mining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Convolutional Network (GCN) has become a prevalent learning paradigm in many real-world applications, including financial fraud detection <ref type="bibr" target="#b44">[45]</ref>, drug discovery <ref type="bibr" target="#b18">[19]</ref> and traffic prediction <ref type="bibr" target="#b6">[7]</ref>.</p><p>To date, the vast majority of existing works do not take into account the uncertainty of a GCN regarding its prediction, which is alarming especially in high-stake scenarios. For example, in automated financial fraud detection, it is vital to let expert banker to take controls if a GCN-based detector is highly uncertain about its predictions in order to prevent wrong decisions on suspending banking account(s).</p><p>A well established study on uncertainty quantification of GCN could bring several crucial benefits. First, it is a cornerstone in trustworthy graph mining. Uncertainty quantification aims to understand to what extent the model is likely to be incorrect, and thus provides natural remedy to questions like how uncertain is a GCN in its own predictions? Second, an accurate quantification of GCN uncertainty could potentially answer how to improve GCN predictions by leveraging its uncertainty in many graph mining tasks. For example, in active learning on graphs, nodes with high uncertainty could be selected as the most valuable node to query the oracle; in node classification, the node uncertainty could help calibrate the confidence of GCN predictions, thereby improving the overall classification accuracy.</p><p>Important as it could be, very few studies on uncertainty quantification of GCN exist, which mainly focuses on two different directions: Bayesian-based approaches and deterministic quantificationbased approaches. Regarding Bayesian-based approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b47">48]</ref>, they either drops edge(s) with certain sampling strategies or leverages random graph model (e.g., stochastic block model) to assign edge probabilities for training. However, these models fall short in explicitly quantifying the uncertainty on model predictions. Another type of methods, i.e., deterministic quantification-based approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref>, directly quantifies uncertainty by parameterizing a Dirichlet distribution as prior to estimate the posterior distribution under a Bayesian framework. Nevertheless, it changes the training procedures of a graph neural network by introducing additional parameters (e.g., parameters for Dirichlet distribution) or additional architectures (e.g., teacher network) in order to precisely estimate the uncertainty.</p><p>To address the aforementioned limitations, we provide the first study on frequentist-based analysis of the GCN uncertainty, which we term as the JuryGCN problem. Building upon the general principle of jackknife (leave-one-out) resampling <ref type="bibr" target="#b33">[34]</ref>, the jackknife uncertainty of a node is defined as the width of confidence interval constructed by a jackknife estimator when leaving the corresponding node out. In order to estimate the GCN parameters without exhaustively re-training GCN, we leverage influence functions <ref type="bibr" target="#b27">[28]</ref> to quantify the change in GCN parameters by infinitesimally upweighting the loss of a training node. Compared with existing works, our method brings several advantages. First, our method provides deterministic uncertainty quantification, which is not available in Bayesian-based approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b47">48]</ref>. Second, different from existing works on deterministic uncertainty quantification <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref>, our method does not introduce any additional parameters or components in the GCN architecture. Third, our method can provide post-hoc uncertainty quantification. As long as the input graph and a GCN are provided, our method can always quantify node uncertainty without any epoch(s) of model training.</p><p>The major contributions of this paper are summarized as follows.</p><p>‚Ä¢ Problem definition. To our best knowledge, we provide the first frequentist-based analysis of GCN uncertainty and formally define the JuryGCN problem. ‚Ä¢ Algorithm and analysis. We propose JuryGCN to quantify jackknife uncertainty on GCN. The key idea is to leverage a jackknife estimator to construct a leave-one-out predictive confidence interval for each node, where the leave-one-out predictions are estimated using the influence functions with respect to model parameters. ‚Ä¢ Experimental evaluations. We demonstrate the effectiveness of JuryGCN through extensive experiments on real-world graphs in active learning on node classification and semisupervised node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION</head><p>In this section, we first introduce preliminary knowledge on the Graph Convolutional Network (GCN), predictive uncertainty and jackknife resampling. Then, we formally define the problem of jackknife uncertainty quantification on GCN (JuryGCN).</p><p>Unless otherwise specified, we use bold upper-case letters for matrices (e.g., A), bold lower-case letters for vectors (e.g., x), calligraphic letters for sets (e.g., G) and fraktur font for high-dimensional tensors (‚Ñå). We use supercript ùëá for matrix transpose and superscript ‚àí<ref type="foot" target="#foot_0">1</ref> for matrix inversion, i.e., A ùëá and A ‚àí1 are the transpose and inverse of A, respectively. We use conventions similar to PyTorch in Python for indexing. For example, A[ùëñ, ùëó] represents the entry of A at the ùëñ-th row and ùëó-th column; A[ùëñ, :] and A[:, ùëó] demonstrate the ùëñ-th row and ùëó-th column of A, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>1 -Graph Convolutional Network (GCN) Let G = {V, A, X} denote a graph whose node set is V, adjacency matrix is A and node feature matrix is X. For the ùëô-th hidden layer in an ùêø-layer GCN, we assume E (ùëô) is the output node embeddings (where E (0) = X) and W (ùëô) is the weight matrix. Mathematically, the graph convolution at the ùëô-th hidden layer can be represented by</p><formula xml:id="formula_0">E (ùëô) = ùúé ( √ÇE (ùëô‚àí1) W (ùëô) )</formula><p>where ùúé is the activation and √Ç = D‚àí 1 2 (A + I) D‚àí 1 2 is the renormalized graph Laplacian with D being the degree matrix of (A + I). 2 -Uncertainty quantification is one of the cornerstones in safecritical applications. It provides accurate quantification on how confident a mining model is towards its predictions. In general, uncertainty can be divided into two types: aleatoric uncertainty and epistemic uncertainty <ref type="bibr" target="#b0">[1]</ref>. Aleatoric uncertainty (or data uncertainty) refers to the variability in mining results due to the inherent randomness in input data, which is irreducible due to complexity of input data (e.g., noise); whereas epistemic uncertainty (or model uncertainty) measures how well the mining model fits the training data due to the lack of knowledge on the optimal model parameters, which is reducible by increasing the size of training data.</p><p>3 -Jackknife resampling is a classic method to estimate the bias and variance of a population <ref type="bibr" target="#b40">[41]</ref>. It often relies on a jackknife estimator which is built by leaving out an observation from the entire population (i.e., leave-one-out) and evaluating the error of the model re-trained on the held-out population. Suppose we have (1) a set of ùëõ data points D = {(x ùëñ , ùë¶ ùëñ )|ùëñ = 1, . . . , ùëõ}, (2) a test point (x test , ùë¶ test ), (3) a mining model ùëì ùúÉ () parameterized by ùúÉ (e.g., a neural network) where ùëì ùúÉ (x) is the prediction of input feature x and (4) a target coverage level (1 ‚àí ùõº) such that the label ùë¶ is covered by the predictive confidence interval with probability (1 ‚àí ùõº). Mathematically, the confidence interval constructed by the naive jackknife <ref type="bibr" target="#b12">[13]</ref> is upper bounded by C + (x test ) = ùëÑ 1‚àíùõº (R + ) and lower bounded by C ‚àí (x test ) = ùëÑ ùõº (R ‚àí ), where ùëÑ ùõº finds the ùõº quantile of a set and</p><formula xml:id="formula_1">R ùõæ = {ùëì ùúÉ (x test ) + ùõæ ‚Ä¢ |ùë¶ test ‚àí ùëì ùúÉ ‚àíùëñ (x test )||ùëñ = 1, . . . , ùëõ} for ùõæ ‚àà {‚àí, +} and |ùë¶ test ‚àíùëì ùúÉ ‚àíùëñ (x test )|</formula><p>is the error residual of the re-trained model on the dataset D\{(x ùëñ , ùë¶ ùëñ )} (i.e., parameterized by ùúÉ ‚àíùëñ ). 1 Hence, R + and R ‚àí represent the sets of upper and lower uncertainty bound on the original model prediction (i.e., ùëì ùúÉ (x test )). Furthermore, jackknife+ <ref type="bibr" target="#b2">[3]</ref> constructs the predictive confidence interval for exchangeable data as</p><formula xml:id="formula_2">C + (x test ) = ùëÑ 1‚àíùõº (P + ) C ‚àí (x test ) = ùëÑ ùõº (P ‚àí )<label>(1</label></formula><p>) where P ùõæ for ùõæ ‚àà {‚àí, +} is defined as</p><formula xml:id="formula_3">P ùõæ = {ùëì ùúÉ ‚àíùëñ (x test ) + ùõæ ‚Ä¢ |ùë¶ ùëñ ‚àí ùëì ùúÉ ‚àíùëñ (x ùëñ )||ùëñ = 1, . . . , ùëõ}.</formula><p>Similarly, P ‚àí and P + represent the sets of the lower and upper uncertainty bound of the leave-one-out prediction ùëì ùúÉ ‚àíùëñ (x test ), respectively. With the assumption on data exchangeability, it yields a (1 ‚àí 2ùõº) coverage rate theoretically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Definition</head><p>Existing works on deterministic uncertainty quantification on a graph neural network (GNN) mainly rely on changing the training procedures of a vanilla GNN (i.e., graph neural network without consideration of uncertainty) <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49]</ref>. As such, given a well-trained GNN, it requires epoch(s) of re-training to quantify its uncertainty. Nevertheless, it would cost a lot of computational resources to retrain it, especially when the model has already been deployed in an operational environment. Additionally, it remains a challenging problem to further comprehend the predictive results of GNN from the perspective of uncertainty, and to answer the following question: to what extent the GNN model is confident of the current prediction? Therefore, it is essential to investigate the uncertainty quantification in a post-hoc manner, i.e., quantifying uncertainty without further (re-)training on the model.</p><p>Regarding post-hoc uncertainty quantification for IID (i.e., nongraph) data, Alaa and van der Schaar <ref type="bibr" target="#b1">[2]</ref> propose a frequentistbased method inspired by jackknife resampling. It uses high-order influence functions to quantify the impact of a data point on the underlying neural network. Given that the parameters of a neural network are derived by learning with data points, high-order influence functions are capable of understanding how much a data point will affect the model parameters. Then, the change in model parameters can be used to infer the uncertainty of the corresponding data point on the neural network by a jackknife estimator <ref type="bibr" target="#b2">[3]</ref>. Under mild assumption (such as the algorithmic stability assumption and the IID/exchangebility of data), the naive jackknife estimator <ref type="bibr" target="#b12">[13]</ref> and its variants <ref type="bibr" target="#b2">[3]</ref> bear strong theoretical guarantee in terms of the coverage such that the confidence interval will cover the true model parameters with a high probability.</p><p>Building upon the jackknife resampling <ref type="bibr" target="#b33">[34]</ref> and the general principle outlined in <ref type="bibr" target="#b1">[2]</ref>, we seek to bridge the gap between frequentistbased uncertainty quantification and graph neural networks. To be specific, given an input graph and a GCN, we aim to estimate the uncertainty of a node as the impact of leaving out its loss when computing the overall loss. Formally, we define the problem of jackknife uncertainty quantification on GCN, which is referred to as JuryGCN problem. Find: An uncertainty score U Œò (ùë¢) for any node ùë¢ in graph G with respect to the GCN parameters Œò and the task-specific loss function ùëÖ(G, Y, Œò).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JURYGCN: MEASURE AND COMPUTATION</head><p>In this section, we start by discussing the general strategy of quantifying jackknife uncertainty with influence functions, and formally define the node jackknife uncertainty. After that, we present mathematical analysis on the influence function computation for GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Jackknife Uncertainty of GCN</head><p>In this paper, we consider an ùêø-layer GCN with ReLU activation for node-level tasks (e.g., node classification). We further assume that the nodes of the input graph G are exchangeable data. <ref type="foot" target="#foot_1">2</ref>We first observe that the loss function of many node-level tasks can often be decomposed into a set of node-specific subproblems. Mathematically, it can be written as</p><formula xml:id="formula_4">Œò * = argmin Œò ùëÖ(G, Y train , Œò) = argmin Œò 1 |V train | ‚àëÔ∏Å ùë£ ‚ààV train ùëü (ùë£, y ùë£ , Œò) (2)</formula><p>where V train ‚äÜ V is the set of training nodes, |V train | is the number of training nodes, Y train = {y ùë£ |ùë£ ‚àà V train } is the set of groundtruth training labels and y ùë£ is the label of node ùë£. In this case, the overall loss function ùëÖ(G, Y train , Œò) is decomposed into several subproblems, each of which minimizes the node-specific loss function ùëü (ùë£, y ùë£ , Œò) for a node ùë£. An example is the cross entropy with node-specific loss as ùëü (ùë£,</p><formula xml:id="formula_5">y ùë£ , Œò) = ‚àí ùëê ùëñ=1 y ùë£ [ùëê] log GCN(ùë£, Œò) [ùëê]</formula><p>, where ùëê is the number of classes, GCN(ùë£, Œò) is the vector of predicted probabilities for each class using the GCN with parameters Œò. If we upweight the importance of optimizing the loss of a node ùëñ with some small constant ùúñ, we have the following loss function.</p><formula xml:id="formula_6">Œò * ùúñ,ùëñ = argmin Œò ùúñùëü (ùëñ, y ùëñ , Œò) + 1 |V train | ‚àëÔ∏Å ùë£ ‚ààV train ùëü (ùë£, y ùë£ , Œò)<label>(3)</label></formula><p>Influence function is a powerful approach to evaluate the dependence of the estimator on the value of the data examples <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b51">52]</ref>. In order to obtain Œò * ùúñ,ùëñ without re-training the GCN, we leverage the influence function <ref type="bibr" target="#b27">[28]</ref>, which is essentially the Taylor expansion over the model parameters.</p><formula xml:id="formula_7">Œò * ùúñ,ùëñ ‚âà Œò * + ùúñI Œò * (ùëñ) (4) where I Œò * (ùëñ) = ùëëŒò * ùúñ,ùëñ</formula><p>ùëëùúñ | ùúñ=0 is the influence function with respect to node ùëñ. The influence function I Œò * (ùëñ) can be further computed using the classical result in <ref type="bibr" target="#b8">[9]</ref> as</p><formula xml:id="formula_8">I Œò * (ùëñ) = H ‚àí1 Œò * ‚àá Œò ùëü (ùëñ, y ùëñ , Œò * )<label>(5)</label></formula><p>where</p><formula xml:id="formula_9">H Œò * = 1 | V train | ‚àá 2 Œò ùëÖ(G, Y train , Œò * )</formula><p>is the Hessian matrix with respect to model parameters Œò * . For a training node ùëñ, by setting ùúñ = ‚àí 1 | V train | , Eq. ( <ref type="formula">4</ref>) efficiently estimates the leave-one-out (LOO) parameters Œò * ùúñ,ùëñ if leaving out the loss of node ùëñ. After that, by simply switching the original model parameters to Œò * ùúñ,ùëñ , we estimate the leave-one-out error err ùëñ of node ùëñ as follows.</p><formula xml:id="formula_10">err ùëñ = ‚à•y ùëñ ‚àí GCN(ùëñ, Œò * ùúñ,ùëñ )‚à• 2<label>(6)</label></formula><p>where GCN(ùë¢, Œò * ùúñ,ùëñ ) represents the output of node ùë¢ using the GCN with leave-one-out parameters Œò * ùúñ,ùëñ . With Eq. ( <ref type="formula" target="#formula_10">6</ref>), we use jackknife+ <ref type="bibr" target="#b2">[3]</ref>, which requires the data to be exchangeable instead of IID, to construct the confidence interval of node ùë¢. Mathematically, the lower bound C ‚àí Œò (ùë¢) and upper bound</p><formula xml:id="formula_11">C + Œò (ùë¢) of the predictive confidence interval of node ùë¢ are C ‚àí Œò * (ùë¢) = ùëÑ ùõº ({‚à•GCN(ùë¢, Œò * ùúñ,ùëñ )‚à• 2 ‚àí err ùëñ |‚àÄùëñ ‚àà V train \ {ùë¢}}) C + Œò * (ùë¢) = ùëÑ 1‚àíùõº ({‚à•GCN(ùë¢, Œò * ùúñ,ùëñ )‚à• 2 + err ùëñ |‚àÄùëñ ‚àà V train \ {ùë¢}})<label>(7)</label></formula><p>where ùëÑ ùõº and ùëÑ 1‚àíùõº are the ùõº and (1 ‚àí ùõº) quantile of a set. Since a wide confidence interval of node ùë¢ means that the model is less confident with respect to node ùë¢, it implies that node ùë¢ has high uncertainty. Following this intuition, the uncertainty of node ùë¢ can be naturally quantified by the width of the corresponding confidence interval (Eq. ( <ref type="formula" target="#formula_11">7</ref>)). Since the uncertainty is quantified using the confidence interval constructed by a jackknife estimator, we term it as jackknife uncertainty which is formally defined in Definition 1.</p><p>Definition 1. (Node Jackknife Uncertainty) Given an input graph G with node set V, a set of training nodes V train ‚äÜ V and an ùêø-layer GCN with parameters Œò, ‚àÄùëñ ‚àà V train and ‚àÄùë¢ ‚àà V, we assume (1) the nodes are exchangeable, and denote that (2) the LOO parameters are Œò ùúñ,ùëñ , (3) the error is defined as Eq. ( <ref type="formula" target="#formula_10">6</ref>) and ( <ref type="formula">4</ref>) the lower bound C ‚àí Œò (ùë¢) and the upper bound C + Œò (ùë¢) of predictive confidence interval are defined as Eq. ( <ref type="formula" target="#formula_11">7</ref>), the jackknife uncertainty of node ùë¢ is</p><formula xml:id="formula_12">U Œò (ùë¢) = C + Œò (ùë¢) ‚àí C ‚àí Œò (ùë¢)<label>(8)</label></formula><p>We note that Alaa and van Der Schaar <ref type="bibr" target="#b1">[2]</ref> leverage high-order influence functions to quantify the jackknife uncertainty for IID data. Though Eq. ( <ref type="formula">4</ref>) shares the same form as in <ref type="bibr" target="#b1">[2]</ref> when the order is up to 1, our work bears three subtle differences. First, <ref type="bibr" target="#b1">[2]</ref> views the model parameters as statistical functionals of data distribution and exploits von Mises expansion over the data distribution to estimate the LOO parameters, <ref type="foot" target="#foot_2">3</ref> which is fundamentally different from our Taylor expansion-based estimation. Specifically, von Mises expansion requires that the perturbed data distribution should be in a convex set of the original data distribution and all possible empirical distributions <ref type="bibr" target="#b15">[16]</ref>. Since the node distribution of a graph is often unknown, the basic assumption of von Mises expansion might not hold on graph data. However, our definition relies on the Taylor expansion over model parameters which are often drawn independently from Gaussian distribution(s). Thus, our method is able to generalize on graphs. Second, <ref type="bibr" target="#b1">[2]</ref> works for regression or binary classification tasks by default, whereas we target more general learning settings on graphs (e.g., multi-class node classification). Third, jackknife uncertainty is always able to quantify aleatoric uncertainty and epistemic uncertainty simultaneously on IID data. Nevertheless, as shown in Proposition 1, it requires additional assumption to quantify both types of uncertainty on GCN simultaneously for a node ùë¢.</p><p>Proposition 1. (Necessary condition of aleatoric and epistemic uncertainty quantification on GCN) Given an input graph G whose node set is V, a node ùë¢ ‚àà V, a set of training nodes V train and an ùêø-layer GCN, jackknife uncertainty quantifies the aleatoric uncertainty and the epistemic uncertainty as long as ùë¢ is outside the ùêø-hop neighborhood of an arbitrary training node ùë£ ‚àà V train \ {ùë¢}.</p><p>Proof. See Appendix. ‚ñ° Remark. For GCN, jackknife uncertainty cannot always measure the aleatoric uncertainty and epistemic uncertainty simultaneously. In fact, if a node ùë¢ is one of the neighbors within ùêø hops with respect to any training node ùë£ ‚àà V train \ {ùë¢}, jackknife uncertainty only quantifies the epistemic uncertainty due to lack of knowledge on the loss of node ùë¢. In this case, jackknife uncertainty cannot quantify aleatoric uncertainty because leaving out the loss of node ùë¢ does not necessarily remove node ùë¢ in the graph. More specifically, the aleatoric uncertainty of node ùë¢ can still be transferred to its neighbors through neighborhood aggregation in graph convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Influence Functions of GCN</head><p>In order to quantify jackknife uncertainty (Eq. ( <ref type="formula" target="#formula_12">8</ref>)), we need to compute the influence functions to estimate the leave-one-out parameters by Eq. ( <ref type="formula">4</ref>). Given a GCN with Œò being the set of model parameters, to compute the influence functions of node ùëñ (Eq. ( <ref type="formula" target="#formula_8">5</ref>)), we need to compute two key terms, including (1) first-order derivative ‚àá Œò ùëü (ùëñ, y ùëñ , Œò) and (2) second-order derivative H Œò . We first give the following proposition for the computation of first-order derivative in Proposition 2. <ref type="foot" target="#foot_3">4</ref> Based on that, we present the main results for computing the second-order derivative in Theorem 1. Finally, we show details of influence function computation in Algorithm 1.</p><p>Proposition 2. (First-order derivative of GCN <ref type="bibr" target="#b24">[25]</ref>) Given an ùêølayer GCN whose parameters are Œò, an input graph G = {V, A, X}, a node ùëñ with its label y ùëñ and a node-wise loss function ùëü (ùëñ, y ùëñ , Œò) for node ùëñ, the first-order derivative of loss function ùëü (ùëñ, y ùëñ , Œò) with respect to the parameters W (ùëô) in the ùëô-th graph convolution layer is</p><formula xml:id="formula_13">‚àá W (ùëô ) ùëü (ùëñ, y ùëñ , Œò) = √ÇE (ùëô‚àí1) ùëá ùúïùëü (ùëñ, y ùëñ , Œò) ùúïE (ùëô) ‚Ä¢ ùúé ‚Ä≤ ( √ÇE (ùëô‚àí1) W (ùëô) )<label>(9)</label></formula><p>where</p><formula xml:id="formula_14">√Ç = D‚àí 1 2 (A + I) D‚àí 1 2</formula><p>is the renormalized graph Laplacian with D being the degree matrix of A + I, ùúé ‚Ä≤ is the derivative of the activation function ùúé, ‚Ä¢ is the element-wise product, and</p><formula xml:id="formula_15">ùúïùëü (ùëñ,y ùëñ ,Œò) ùúïE (ùëô )</formula><p>can be iteratively calculated by</p><formula xml:id="formula_16">ùúïùëü (ùëñ, y ùëñ , Œò) ùúïE (ùëô) = √Çùëá ùúïùëü (ùëñ, y ùëñ , Œò) ùúïE (ùëô+1) ‚Ä¢ùúé ‚Ä≤ ( √ÇE (ùëô) W (ùëô+1) ) W (ùëô+1) ùëá (10)</formula><p>Proof. See Appendix. ‚ñ°</p><p>Since the parameters are often represented as matrices in the hidden layers, the second-order derivative will be a 4-dimensional tensor (i.e., a Hessian tensor). Building upon the results in Proposition 2, we first present the computation of the Hessian tensor in Theorem 1. Then we discuss efficient computation of influence function, which is summarized in Algorithm 1.</p><p>Theorem 1. (The Hessian tensor of GCN) Following the settings of Proposition 2, denoting the overall loss ùëÖ(G, Y train , Œò) as ùëÖ and ùúé ‚Ä≤ ùëô as ùúé ‚Ä≤ ( √ÇE (ùëô‚àí1) W (ùëô) ), the Hessian tensor ‚Ñå ùëô,ùëñ = ùúï 2 ùëÖ ùúïW (ùëô ) ùúïW (ùëñ ) of ùëÖ with respect to W (ùëô) and W (ùëñ) has the following forms.</p><p>Case</p><formula xml:id="formula_17">1.ùëñ = ùëô, ‚Ñå ùëô,ùëñ = 0 Case 2.ùëñ = ùëô ‚àí 1 ‚Ñå ùëô,ùëñ [:, :, ùëê, ùëë] = √Ç ùúïE (ùëô‚àí1) ùúïW (ùëñ) [ùëê, ùëë] ùëá ùúïùëÖ ùúïE (ùëô) ‚Ä¢ ùúé ‚Ä≤ ùëô (11)</formula><p>where</p><formula xml:id="formula_18">ùúïE (ùëô ‚àí1)</formula><p>ùúïW (ùëñ ) [ùëê,ùëë ] is the matrix whose entry at the ùëé-th row and the ùëè-th column is</p><formula xml:id="formula_19">ùúïE (ùëô‚àí1) [ùëé, ùëè] ùúïW (ùëô‚àí1) [ùëê, ùëë] = ùúé ‚Ä≤ ùëô‚àí1 [ùëé, ùëè] √ÇE (ùëô‚àí2) [ùëé, ùëê]I[ùëè, ùëë]<label>(12)</label></formula><p>Case 3.ùëñ &lt; ùëô ‚àí 1 -Apply Eq. ( <ref type="formula" target="#formula_19">12</ref>) for the ùëñ-th hidden layer.</p><p>-Forward to the (ùëô ‚àí 1)-th layer iteratively with</p><formula xml:id="formula_20">ùúïE (ùëô‚àí1) ùúïW (ùëñ) [ùëê, ùëë] = ùúé ‚Ä≤ ùëô‚àí1 ‚Ä¢ √Ç ùúïE (ùëô‚àí2) ùúïW (ùëñ) [ùëê, ùëë] W (ùëô‚àí1)<label>(13)</label></formula><p>-Apply Eq. (11). </p><formula xml:id="formula_21">ùúï 2 ùëÖ ùúïE (ùëô) ùúïW (ùëñ) [ùëê, ùëë] = √Çùëá ùúï 2 ùëÖ ùúïE (ùëô+1) ùúïW (ùëñ) [ùëê, ùëë] ‚Ä¢ ùúé ‚Ä≤ ùëô+1 W (ùëô+1) ùëá (15) -Apply Eq. (14).</formula><p>Proof. See Appendix. ‚ñ° Even with Proposition 2 and Theorem 1, it is still non-trivial to compute the influence of node ùë¢ due to (C1) the high-dimensional nature of the Hessian tensor and (C2) the high computational cost of Eq. ( <ref type="formula" target="#formula_8">5</ref>) due to the inverse operation. Regarding the first challenge (C1), for any node ùë¢, we observe that each element in the firstorder derivative ‚àá W (ùëô ) ùëÖ is the element-wise first-order derivative, i.e., ‚àá W  <ref type="foot" target="#foot_4">5</ref> Thus, the key idea to solve the first challenge (C1) is to vectorize the first-order derivative into a column vector and compute the element-wise second-order derivatives accordingly, which naturally flatten the Hessian tensor into a Hessian matrix. More specifically, we first vectorize the firstorder derivatives of ùëÖ with respect to W (ùëô) , ‚àÄùëô ‚àà {1, . . . , ùêø} in to column vectors and stack them vertically as follows,</p><formula xml:id="formula_22">f ùëÖ = Ô£Æ Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£∞ vec(‚àá W (1) ùëÖ) = vec( ùúïùëÖ ùúïW (1) ) . . . vec(‚àá W (ùêø) ùëÖ) = vec( ùúïùëÖ ùúïW (ùêø) ) Ô£π Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£ª<label>(16)</label></formula><p>where vec() vectorizes a matrix to a column vector. Then the flattened Hessian matrix is a matrix H flat whose rows are of the form</p><formula xml:id="formula_23">H flat [(ùëñ ‚Ä¢ ùëê + ùëë), :] = ùúïf ùëÖ ùúïW (ùëñ) [ùëê, ùëë] ùëá = vec(‚Ñå ùëô,ùëñ [:, :, ùëê, ùëë]) ùëá (17)</formula><p>Finally, we follow the strategy to compute the influence of node ùë¢:</p><p>(1) Compute ‚àá W (ùëô ) ùëü (ùë¢, y ùë¢ , Œò) for all ùëô-th hidden layer; (2) Vectorize ‚àá W (ùëô ) ùëü (ùë¢, y ùë¢ , Œò) and stack to column vector f ùë¢ as shown in Eq. ( <ref type="formula" target="#formula_22">16</ref>);</p><p>(3) Compute the influence function I(ùë¢) = H ‚àí1 flat f ùë¢ . Regarding the second challenge (C2), the key idea is to apply Hessian-vector product (Algorithm 1) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>, which approximates I(ùë¢) = H ‚àí1 flat f ùë¢ using the power method. Mathematically, it treats H ‚àí1 flat f ùë¢ as one vector and iteratively computes</p><formula xml:id="formula_24">H ‚àí1 flat f ùë¢ = f ùë¢ + (I ‚àí ƒ§flat ) H ‚àí1 flat f ùë¢<label>(18)</label></formula><p>where ƒ§flat H ‚àí1 flat f ùë¢ is viewed as a vector and ƒ§flat is the flattened Hessian matrix with respect to a set of sampled nodes at current iteration. The workflow of the Hessian-vector product is presented in Algorithm 1. For any ùëô-th hidden layer (step 2), we first compute the first-order derivative ‚àá W (ùëô ) ùëü (ùë¢, y ùë¢ , Œò) with respect to W (ùëô) (step 3). Then we vectorize it to a column vector and stack it to f ùë¢ that stores the first-order derivatives of all hidden layers (step 4). After all first-order derivatives are computed, we apply the power method to compute the Hessian-vector product. In each iteration, we first sample a batch of ùë° training nodes, which helps reduce both noise and running time, and then compute the empirical loss over these nodes (steps 7 -8). After that, we compute the secondorder derivatives with Theorem 1 and flatten it to a matrix with the strategy shown in Eq. ( <ref type="formula">17</ref>) (step 9). We finish this iteration by computing Eq. ( <ref type="formula" target="#formula_24">18</ref>) (step 10). The power method (steps 7 -10) iterates until the maximum number of iteration is reached to ensure the convergence. Consequently, Algorithm 1 offers a computationally friendly way to approximate influence functions without involving both tensor-level operations and the computationally expensive matrix inversion. Remark. We observe that both the first-order derivative (Proposition 2) and the second-order derivative (Theorem 1) can be computed in the style of neighborhood aggregation. Due to the wellknown over-smoothness of GCN and the homophily nature of neighborhood aggregation, the resulting influence functions by Algorithm 1 may follow the homophily principle as well. For two nodes under homophily, due to similarity between their influences, their corresponding LOO parameters and LOO errors could be similar as well, which in turn could cause similar uncertainty scores by Definition 1. The potential homophily phenomenon in jackknife uncertainty is consistent with the homophily assumption with respect to uncertainty/confidence in existing works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>.  Vectorize ‚àá W (ùëô ) ùëü (ùë¢, y ùë¢ , Œò) and stack it to f ùë¢ as Eq. ( <ref type="formula" target="#formula_22">16</ref>);</p><formula xml:id="formula_25">5 Initialize H ‚àí1 flat f ùë¢ 0 ‚Üê f ùë¢ ; 6 for iter = 1 ‚Üí m do 7</formula><p>Uniformly sample ùë° training nodes and get V s ;</p><formula xml:id="formula_26">8 Compute empirical loss ùëÖ ùë† ‚Üê 1 | V s | ùëñ ‚ààV s ùëü (ùëñ, y ùëñ , Œò);<label>9</label></formula><p>Compute ƒ§flat of ùëÖ ùë† with Theorem 1 and Eq. ( <ref type="formula">17</ref>);</p><formula xml:id="formula_27">10 Compute H ‚àí1 flat f ùë¢ iter ‚Üê f ùë¢ + (I ‚àí ƒ§flat ) H ‚àí1 flat f ùë¢ iter‚àí1 11 return H ‚àí1 flat f ùë¢ ùëö ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">JURYGCN: ALGORITHM AND APPLICATIONS</head><p>In this section, we present our proposed method named JuryGCN to quantify node jackknife uncertainty (Algorithm 2) followed by discussions on applications and generalizations of JuryGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">JuryGCN Algorithm</head><p>With Algorithm 1, the LOO parameters of each node can be efficiently computed by proper initialization on the perturbation coefficient (ùúñ in Eq. ( <ref type="formula" target="#formula_8">5</ref>)). After that, the LOO predictions and LOO errors can be efficiently inferred by simply switching the original parameters to the LOO parameters, resulting in efficient jackknife uncertainty quantification. Based on that, Algorithm 2 presents the general workflow of our proposed JuryGCN to quantify the jackknife uncertainty of a node. In detail, with proper initialization (step 1), we loop through each training node ùëñ to quantify their influences (step 2). For each training node ùëñ, it estimates the LOO parameters by leaving out training node ùëñ (steps 3 -6), outputs the LOO predictions of nodes ùëñ and ùë¢ (step 7) and compute the LOO error of each training node ùëñ (step 8). After the LOO predictions of node ùë¢ and the LOO errors of all training nodes are obtained, we compute the lower bound and upper bound of the predictive confidence interval (steps 9 -10). Finally, the uncertainty of the node is computed as the width of the predictive confidence interval (step 11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">JuryGCN Applications</head><p>After quantifying the jackknife uncertainty of each node, we utilize the node uncertainty in (1) active learning on node classification and (2) semi-supervised node classification. The details of uncertaintyaware active learning and node classification are as follows. Application # 1: Active Learning on Node Classification. In general, active learning sequentially selects a subset of data points to query according to an acquisition function, which is designed to identify the most informative samples, and hence improves the model performance from the obtained labels. In active learning on node classification, we are given (1) an unlabelled training set of nodes (i.e., V train ), (2) a node classifier (e.g., GCN), <ref type="bibr" target="#b2">(3)</ref>   <ref type="figure">Œò</ref>) where the node-specific loss (i.e., ùëü (‚Ä¢)) is factored by an identical weight (i.e., 1  |V train | ). Intuitively, training nodes have various levels of uncertainty during training, the easily predicted samples (i.e., small uncertainty) may comprise the majority of the total loss and hence dominate the gradient, which makes the training inefficient. To address this problem, based on jackknife uncertainty estimation, we introduce a dynamic scale factor, ùõΩ, to adjust the importance for nodes with different levels of uncertainty. Specifically, given the cross-entropy loss that is utilized in semi-supervised node classification, we define the uncertainty-aware node-specific loss as,</p><formula xml:id="formula_28">ùëü ùë¢ = ‚àíùõΩ ùúè ùë¢ log(ùëù (ùëñ) ùë¢ ) where ùëù (ùëñ)</formula><p>ùë¢ is the predictive probability of the ùëñ-th class from GCN and ùúè is a hyperparameter. The scale factor of node ùë¢ is computed by normalizing the uncertainty over all training nodes, i.e., ùõΩ ùë¢ =</p><formula xml:id="formula_29">|U Œò (ùë¢) | ‚àöÔ∏É ùëñ‚ààV train |U Œò (ùëñ) | 2</formula><p>. By introducing ùõΩ ùë¢ , the loss of node with larger uncertainty (i.e., U Œò ) would be upweighted in the total loss and hence the training is guided toward nodes with high uncertainty. Therefore, when training a node classification model, we leverage Algorithm 2 to estimate the jackknife uncertainty of the training nodes and then apply the obtained uncertainty results to update the loss every few number of epochs.</p><p>In addition to Tasks 1 and 2, our proposed JuryGCN is generalizable to other learning tasks and graph mining models. Due to the space limit, we only present brief descriptions of each generalization direction, which could be a future direction of JuryGCN. Applications beyond Node Classification. The proposed Ju-ryGCN can be applied to a variety of learning tasks. For example, it can estimate the confidence interval of the predictive dependent variable in a regression task by replacing the cross-entropy loss for node classification with mean squared error (MSE) <ref type="bibr" target="#b1">[2]</ref>. Besides active learning, reinforcement learning (RL) is extensively explored to model the interaction between agent and environment. The proposed framework is applicable to RL in the following two aspects. First, in the early stage of training an RL model, the uncertainty quantification results can be utilized to guide exploration. Second, through effectively quantifying the uncertainty of the reward after taking certain actions, JuryGCN can also benefit the exploitation. Specifically, as a classic topic of RL, multi-armed bandit can also be a potential application for JuryGCN where uncertainty is highly correlated with decision making. Beyond JuryGCN: Generalizations to Other GNN Models. The proposed JuryGCN is able to be generalized to other graph mining models with the help of automatic differentiation in many existing packages, e.g., PyTorch, TensorFlow. In this way, only model parameters and the loss function are required in computing the first-order and second-order derivatives for influence function computation (Algorithm 1), which is further used in Algorithm 2 for jackknife uncertainty quantification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>In this section, we conduct experiments to answer the following research questions: RQ1. How effective is JuryGCN in improving GCN predictions? RQ2. How efficient is JuryGCN in terms of time and space? RQ3. How sensitive is JuryGCN to hyperparameters?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>1 -Datasets. We adopt four widely used benchmark datasets, including Cora <ref type="bibr" target="#b37">[38]</ref>, Citeseer <ref type="bibr" target="#b37">[38]</ref>, Pubmed <ref type="bibr" target="#b34">[35]</ref> and Reddit <ref type="bibr" target="#b19">[20]</ref>. Table <ref type="table" target="#tab_1">1</ref> summarizes the statistics of the datasets. 2 -Comparison Methods. We compare the proposed algorithm with node classification methods in the following two categories, including (1) active learning-based approaches, which aim to select the most informative nodes to query, and (2) semi-supervised methods. In active learning on node classification, we include the following methods for comparison, AGE <ref type="bibr" target="#b4">[5]</ref>, ANRMAB <ref type="bibr" target="#b17">[18]</ref>, Coreset <ref type="bibr" target="#b38">[39]</ref>, Centrality, Random and SOPT-GCN <ref type="bibr" target="#b35">[36]</ref>. For semi-supervised node classification, we have two uncertainty-based approaches, including S-GNN <ref type="bibr" target="#b48">[49]</ref> and GPN <ref type="bibr" target="#b39">[40]</ref>, as well as GCN <ref type="bibr" target="#b26">[27]</ref> and GAT <ref type="bibr" target="#b42">[43]</ref>.</p><p>For detailed description of the baselines, please refer to Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">-Evaluation Metric.</head><p>In this paper, we use Micro-F1 to evaluate the effectiveness. In terms of efficiency, we compare the running time (in seconds) and the memory usage (in MB). 4 -Implementation Details. We introduce the implementation details of the experiments in Appendix. 5 -Reproducibility. All datasets are publicly available. All codes are programmed in Python 3.6.9 and PyTorch 1.4.0. All experiments are performed on a Linux server with 96 Intel Xeon Gold 6240R CPUs and 4 Nvidia Tesla V100 SXM2 GPUs with 32 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness Results (RQ1)</head><p>1 -Active Learning on Node Classification. Table <ref type="table" target="#tab_2">2</ref> presents the results of active learning at different query steps. We highlight the best performing approach in bold and underline the best competing one respectively. We have the following observations:</p><p>(1) At the final query step (i.e., the 5-th line for each dataset), Ju-ryGCN selects more valuable training nodes than other baseline methods, resulting in higher Micro-F1. Though AGE outperforms JuryGCN on Pubmed at 40 queries, the superiority is very marginal (i.e., 0.1%); (2) In general, JuryGCN achieves a much better performance when the query size is small. For example, on Cora dataset, JuryGCN outperforms SOPT-GCN by 2.3% at 20 queries while the gain becomes 0.6% at 80 queries. One possible explanation is that newly-selected query nodes may contain less fruitful information about new classes/features for a GCN classifier that is already trained with a certain number of labels; (3) As the query size increases, the gained performance of each method is diminishing, which is consistent with the second observation. For instance, on Reddit dataset, ANRMAB improves 13.5% when query size becomes 100 (from 50), while the gain is only 1.5% at 250 queries.</p><p>2 -Semi-supervised Node Classification. We classify nodes by training with various numbers of training nodes. Figure <ref type="figure" target="#fig_0">1</ref> summarizes the results on four datasets. We can see that, in general, under the conventional setting of semi-supervised node classification (i.e., the right most bar where the number of training nodes is similar with <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43]</ref>), JuryGCN improves the performance of GCN to certain extent w.r.t. Micro-F1 (dark blue vs. yellow). Nonetheless, GCN can achieve a slightly better performance than JuryGCN on Cora. In the mean time, as the number of training nodes becomes smaller, we can observe a consistent superiority of the proposed JuryGCN over other methods. For example, on Citeseer, when 20 training labels are provided, JuryGCN outperforms the best competing method (i.e., GPN) by 2.3% w.r.t. Micro-F1, which further demonstrate the effectiveness of JuryGCN in quantifying uncertainty when the training labels are sparse.</p><p>To summarize, the uncertainty obtained by the proposed Ju-ryGCN is mostly valuable when either the total query budget (for active learning) or the total available labels (for semi-supervised node classification) is small. This is of high-importance especially for high-stake applications (e.g., medical image segmentation) where the cost of obtaining high-quality labels is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Efficiency Results (RQ2)</head><p>We evaluate the efficiency of JuryGCN on Reddit in terms of running time and memory usage (Figure <ref type="figure" target="#fig_6">2</ref>). Regarding running time, we compare the influence function-based estimation with re-training when leaving out one sample at a time. In Figure <ref type="figure" target="#fig_6">2a</ref>, as the number of training nodes increases, the total running time of re-training becomes significantly larger than that of JuryGCN. For example, JuryGCN can achieve over 130√ó speed-up over re-training with 10, 000 training labels. In terms of memory usage in Figure <ref type="figure" target="#fig_6">2b</ref>, compared to GPN and S-GNN, JuryGCN (i.e., blue diamond at the upper-left corner) reaches the best balance between Micro-F1 and memory usage, with the best effectiveness and low memory usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Parameter and Sensitivity Analysis (RQ3)</head><p>We investigate the sensitivity of JuryGCN w.r.t. (1) the coverage parameter ùõº in active learning at the third query step and (2) the hyperparameter ùúè in semi-supervised node classification, where the number of training labels corresponds to the third value on x-axis in Figure <ref type="figure" target="#fig_0">1</ref>. Figure <ref type="figure" target="#fig_8">3</ref> presents the results of sensitivity analysis. For active learning on node classification (i.e., Figure <ref type="figure" target="#fig_8">3a</ref>), the Micro-F1 results represent the performance at the third query step (i.e., the middle line of each dataset in Table <ref type="table" target="#tab_2">2</ref>). And for semi-supervised classification, the number of labels corresponds to the third value on x-axis in Figure <ref type="figure" target="#fig_0">1</ref>. Regarding the sensitivity of ùõº, results in Figure <ref type="figure" target="#fig_8">3a</ref> shows that Micro-F1 slightly decreases as ùõº increases. It might because smaller ùõº indicates a larger target coverage (1 ‚àí 2ùõº) <ref type="foot" target="#foot_5">6</ref> , resulting in wider confidence intervals. Hence, different levels of uncertainty can be accurately captured for selecting valuable query nodes. Regarding the sensitivity of ùúè, we can observe from Figure <ref type="figure" target="#fig_8">3b</ref> that JuryGCN is comparatively robust to ùúè from 0 (i.e., cross-entropy loss) to 3. Meanwhile, by adjusting the importance of training node with the scale factor, Micro-F1 of JuryGCN improves, which is consistent with our findings in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Graph neural networks (GNNs) often reveal the state-of-theart empirical performance on many tasks like classification <ref type="bibr" target="#b26">[27]</ref>, anomaly detection <ref type="bibr" target="#b11">[12]</ref> and recommendation <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b50">51]</ref>. Bruna et al. <ref type="bibr" target="#b3">[4]</ref> leverages a learnable diagonal matrix to simulate the convolution operation in graph signal processing. Defferrard et al. <ref type="bibr" target="#b10">[11]</ref> improve the efficieny of convolution operation on graphs with the Chebyshev expansion of the graph Laplacian. Kipf and Welling <ref type="bibr" target="#b26">[27]</ref> approximates the spectral graph convolution with neighborhood aggregation over one-hop neighbors. Hamilton et al. <ref type="bibr" target="#b19">[20]</ref> inductively learn node representations by sampling and aggregating node representations over the local neighborhood of a node. Veliƒçkoviƒá et al. <ref type="bibr" target="#b42">[43]</ref> introduce the self-attention mechanism to graph neural networks. Chen et al. <ref type="bibr" target="#b7">[8]</ref> enable batch training on GCN by sampling the receptive fields in each hidden layer. Rong et al. <ref type="bibr" target="#b36">[37]</ref> drops a certain number of edges during each training epoch of a graph neural network. Different from <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37]</ref> that utlizes dropout or sampling based strategies to learn node representations with uncertainty, our work deterministically quantifies the uncertainty in a post-hoc manner. Wang et al. <ref type="bibr" target="#b46">[47]</ref> calibrate the confidence of GCN by utilizing another GCN as the calibration function. Our work differs from <ref type="bibr" target="#b46">[47]</ref> that we do not rely on any additional model to learn the confidence interval of GCN. We refer to recent survey <ref type="bibr" target="#b49">[50]</ref> for more related works on GNNs. Uncertainty quantification aims to understand to what extent a model is likely to misclassify a data sample. There has been a rich collection of research works in quantifying uncertainty for IID data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>. More related works on IID data can be     found in recent survey <ref type="bibr" target="#b0">[1]</ref>. Regarding uncertainty quantification for graph data, Dallachiesa et al. <ref type="bibr" target="#b9">[10]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we study the problem of jackknife uncertainty quantification on Graph Convolutional Network (GCN) from the frequentist perspective. We formally define the jackknife uncertainty of a node as the width of confidence interval by a jackknife (leaveone-out) estimator. To scale up the computation, we rely on the influence functions for efficient estimation of the leave-one-out parameters without re-training. The proposed JuryGCN framework is applied to both active learning, where the most uncertain nodes are selected to query the oracle, and semi-supervised node classification, where the jackknife uncertainty serves as the importance of loss to focus on nodes with high uncertainty. Extensive evaluations on real-world datasets demonstrate the efficacy of JuryGCN in both active learning and semi-supervised node classification. Our proposed JuryGCN is able to generalize on other learning tasks beyond GCN, which is the future direction we would like to investigate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX Additional Experimental Settings</head><p>1 -Comparison Methods. In this section, we present the detailed description of comparison methods that are used in the experiments.</p><p>For the application of active learning on node classification, we have the following methods.</p><p>‚Ä¢ AGE <ref type="bibr" target="#b4">[5]</ref> measures the informativeness of each node by considering the following perspectives, including (1) the entropy of the prediction results, (2) the centrality score, and (3) the distance between the corresponding representation and its nearest cluster center. At each step of query, nodes with the highest scores are selected.</p><p>‚Ä¢ ANRMAB <ref type="bibr" target="#b17">[18]</ref> leverages the same selection criterion as in AGE. Additionally, ANRMAB proposes a multi-armed bandit framework to dynamically adjust weights for the three perspectives. ANRMAB utilizes the performance score of previous query steps as the rewards to learn the optimal combination of weights during the query process. ‚Ä¢ Coreset <ref type="bibr" target="#b38">[39]</ref> is originally proposed for Convolutional Neural networks and performs k-means clustering on the vector representations from the last hidden layer. We follow Hu et al. <ref type="bibr" target="#b23">[24]</ref> and apply Coreset on the node representations obtained by GCN. At each query step, we select the node which is the closet to the cluster center to label. ‚Ä¢ Centrality selects the nodes with the highest scores of betweenness centrality at each query step. ‚Ä¢ Degree queries the node with the highest degree.</p><p>‚Ä¢ Random annotates node randomly at each query step.</p><p>‚Ä¢ SOPT-GCN <ref type="bibr" target="#b35">[36]</ref> utilizes Œ£-optimal (SOPT) acquisition function as the active learner <ref type="bibr" target="#b30">[31]</ref>, which requires the graph Laplacian and the indices of labeled nodes. We follow the same setting as in <ref type="bibr" target="#b35">[36]</ref> to conduct the experiments. For semi-supervised node classification, we compare the proposed framework with the following approaches.</p><p>‚Ä¢ S-GNN <ref type="bibr" target="#b48">[49]</ref> is an uncertainty-aware estimation framework which leverages a graph-based kernel Dirichlet distribution to estimate different types of uncertainty associated with the prediction results. We utilize the obtained node-level representations to perform classification in the experiments. ‚Ä¢ GPN <ref type="bibr" target="#b39">[40]</ref> derives three axioms for characterizing the predictive uncertainty and proposes Graph Posterior Network (GPN) to perform Bayesian posterior updates over predictions based on density estimation and diffusion. Similarly, we utilize the node representations for classification task.</p><p>‚Ä¢ GCN <ref type="bibr" target="#b26">[27]</ref> learns node-level representations by stacking multiple layers of spectral graph convolution. ‚Ä¢ GAT <ref type="bibr" target="#b42">[43]</ref> computes the representation of each node by introducing the learnable attention weights from its neighbors.</p><p>2 -Implementation Details. In the experiment, we conduct empirical evaluations in two applications, including (1) Application #1: active learning on node classification, and (2) Application #2: semi-supervised node classification, as described in Section 4.2.</p><p>In Application #1, we evaluate all methods on a randomly constructed test set of 1, 000 nodes for Cora, Citeseer, Pubmed datasets and 139, 779 nodes for Reddit. The validation sets for the first three citation networks contains 500 nodes, and 23, 296 is the size of validation set for Reddit. The remaining nodes comprise the training set (i.e., V train ) where the nodes for query are selected. For Cora, Citeseer, Pubmed and Reddit, (1) we have 10, 10, 5 and 20 randomly selected labels, respectively, to initiate the computation of jackknife uncertainty using Algorithm 2; (2) the query budgets are set as 100, 100, 50 and 250; and (3) the query step sizes are 20, 20, 10 and 50.</p><p>In Application #2, we randomly selected 100, 100, 50 and 200 nodes from Cora, Citeseer, Pubmed and Reddit respectively as the training nodes. The sizes of test sets are the same as those in Application 1. During training, we run Algorithm 2 to perform uncertainty estimation over the training nodes every 10 epochs and update the scale factor ùõº accordingly. In addition, we also evaluate the model performance when the number of training nodes is significantly small.</p><p>In both applications, we adopt a two-layer GCN with 16 hidden layer dimension. For training, we use the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> with learning rate 0.01 and train the GCN classifier for 100 epochs. The coverage parameter (ùõº) in Algorithm 2 is 0.025 and the hyperparameter ùúè in Application 2 is set as 2. For all other comparison methods, we use the original settings . We report the average results after 20 runs of each method on two applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF OF PROPOSITION 1</head><p>For an arbitrary node ùë£ ‚àà V in the graph, its receptive field after ùêø graph convolution layers is the set of all neighbors within ùêø hops. Then if the node ùë¢, whose uncertainty is going to be quantified, is not within the ùêø-hop neighborhood of any training node ùë£ ‚àà V train \ {ùë¢}, whether to leave out the loss of node ùë¢ during training will have no impact on hidden node representations and final predictions of ùë£, which is equivalent to the leave-one-out settings for IID data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF OF PROPOSITION 2</head><p>To prove it, we derive the element-wise computation of ‚àá W (ùëô ) ùëü (ùëñ, y ùëñ , Œò) and then write out the matrix form. We first apply the chain rule and get ùúïùëü (ùëñ, y ùëñ , Œò) ùúïW (ùëô)   ‚Ä¢ùúé ‚Ä≤ √ÇE (ùëô‚àí1) W (ùëô) [:, ùëè] (21) where ‚Ä¢ is the element-wise product. Finally, we get Eq. ( <ref type="formula" target="#formula_13">9</ref>) by writing Eq. ( <ref type="formula">21</ref>) into matrix form.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Problem 1 .</head><label>1</label><figDesc>JuryGCN: Jackknife Uncertainty Quantification on Graph Convolutional Network Given: (1) An undirected graph G = {V, A, X}; (2) an ùêø-layer GCN with the set of weights Œò; (3) a task-specific loss function ùëÖ(G, Y, Œò) where Y is the set of node labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 ‚Ñå</head><label>1</label><figDesc>Case 4.ùëñ = ùëô + ùëô,ùëñ [:, :, ùëê, ùëë] = ( √ÇE (ùëô‚àí1) ) ùëá ùúï 2 ùëÖ ùúïE (ùëô) ùúïW (ùëñ) [ùëê, ùëë] ‚Ä¢ ùúé ‚Ä≤ ùëô (14) where ùúï 2 ùëÖ ùúïE (ùëô ) [ùëé,ùëè ]ùúïW (ùëô +1) [ùëê,ùëë ] = I[ùëè, ùëê] √Çùëá ùúïùëÖ ùúïE (ùëô +1) ‚Ä¢ ùúé ‚Ä≤ ùëô+1 [ùëé, ùëë]. Case 5.ùëñ &gt; ùëô + 1 -Compute ùúï 2 ùëÖ ùúïE (ùëñ‚àí1) ùúïW (ùëñ ) [ùëê,ùëë ] whose (ùëé, ùëè)-th entry has the form ùúï 2 ùëÖ ùúïE (ùëñ‚àí1) [ùëé,ùëè ]ùúïW (ùëñ ) [ùëê,ùëë ] = I[ùëè, ùëê] √Çùëá ùúïùëÖ ùúïE (ùëñ ) ‚Ä¢ùúé ‚Ä≤ ùëñ [ùëé, ùëë] -Backward to (ùëô + 1)-th layer iteratively with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(ùëô ) ùëÖ [ùëé, ùëè] = ùúïùëÖ ùúïW (ùëô ) [ùëé,ùëè ] . Likewise, for the Hessian tensor, we have ‚Ñå ùëô,ùëñ [ùëé, ùëè, ùëê, ùëë] = ùúï 2 ùëÖ ùúïW (ùëô ) [ùëé,ùëè ]ùúïW ùëñ [ùëê,ùëë ] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Hessian-Vector Product Input : An input graph G, training nodes V train , ground-truth labels Y train , node ùë¢ with label y ùë¢ , an ùêø-layer GCN with parameters Œò, a node-wise loss function ùëü , sampling batch size ùë°, #iterations ùëö; Output : The influence I Œò (ùë¢) of node ùë¢. 1 Initialize f ùë¢ = [] as an empty column vector; 2 for ùëô = 1 ‚Üí ùêø do 3 Compute ‚àá W (ùëô ) ùëü (ùë¢, y ùë¢ , Œò) by Eq. (9);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Node classification results under various numbers of labels. Higher is better. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Efficiency results w.r.t. time and memory usage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Micro-F1 vs. 2ùõº. (b) Micro-F1 vs. ùúè.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Parameter study on the coverage parameter ùõº and the hyperparameter ùúè.found in recent survey<ref type="bibr" target="#b0">[1]</ref>. Regarding uncertainty quantification for graph data, Dallachiesa et al.<ref type="bibr" target="#b9">[10]</ref> classify nodes in consideration of uncertainty in edge existence. Hu et al.<ref type="bibr" target="#b22">[23]</ref> learns node</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>, y ùëñ , Œò) ùúïE(ùëô) [ùëê, ùëë] ùúïE(ùëô) [ùëê, ùëë] ùúïW(ùëô) [ùëé, ùëè]    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>( 19 )</head><label>19</label><figDesc>where ‚Ñé ùëô is the hidden dimension of ùëô-th layer. Regarding the computation ofùúïE (ùëô ) [ùëê,ùëë ] ùúïW (ùëô ) [ùëé,ùëè ] , since E (ùëô) = ùúé ( √ÇE (ùëô‚àí1) W (ùëô) ), we have ùúïE (ùëô) [ùëê, ùëë] ùúïW (ùëô) [ùëé, ùëè] = ùúïùúé √ÇE (ùëô‚àí1) W (ùëô) [ùëê, ùëë] ùúï √ÇE (ùëô‚àí1) W (ùëô) [ùëê, ùëë] ùúï √ÇE (ùëô‚àí1) W (ùëô) [ùëê, ùëë] ùúïW (ùëô) [ùëé, ùëè] = ùúé ‚Ä≤ √ÇE (ùëô‚àí1) W (ùëô) [ùëê, ùëë] √ÇE (ùëô‚àí1) [ùëê, ùëé]I[ùëë, ùëè](20) where ùúé ‚Ä≤ is the first-order derivative of the activation function ùúé. Combining Eqs.<ref type="bibr" target="#b18">(19)</ref> and (20) together, we have the element-wise computation of ‚àá W (ùëô ) ùëü (ùëñ, y ùëñ , Œò) as follows.ùúïùëü (ùëñ, y ùëñ , Œò) ùúïW(ùëô) [ùëé, ùëè] = √ÇE (ùëô‚àí1) ùëá [ùëé, :] ùúïùëü (ùëñ, y ùëñ , Œò) ùúïE(ùëô)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>step size ùëè, and (4) the query budget ùêæ. At each query step, according to Algorithm 2: JuryGCN: Jackknife Uncertainty Quantification Input : An input graph G = {V, A, X} with training nodes V train , a node ùë¢, a GCN with parameters Œò, a node-wise loss function ùëü , a coverage parameter ùõº; Output : The uncertainty U Œò (ùë¢) of node ùë¢. 1 Initialize ùúñ ‚Üê ‚àí 1 |V train | ; 2 for ùëñ ‚àà V train do Compute node-wise loss ùëü ùëñ,Œò ‚Üê ùëü ùëñ, y ùëñ , Œò ; Compute node-wise derivative ‚àá Œò ùëü ùëñ,Œò ; Output LOO predictions GCN(ùëñ, Œò ùúñ,ùëñ ) and GCN(ùë¢, Œò ùúñ,ùëñ ); Compute LOO error err ùëñ ‚Üê ‚à•y ùëñ ‚àí GCN(ùëñ, Œò ùúñ,ùëñ ) ‚à• 2 ; ùëÑ ùõº ({‚à•GCN(ùë¢, Œò ùúñ,ùëñ ) ‚à• 2 ‚àí err ùëñ |ùëñ ‚àà V train \ {ùë¢}});</figDesc><table><row><cell>5</cell><cell>Compute I Œò (ùëñ) ‚Üê H ‚àí1 Œò ‚àá Œò ùëü ùëñ,Œò using Algorithm 1;</cell></row><row><cell>8</cell><cell></cell></row><row><cell cols="2">9 Compute lower bound</cell></row><row><cell></cell><cell>C ‚àí Œò (ùë¢) ‚Üê</cell></row></table><note>3 4 6Estimate LOO model parameters Œò ùúñ,ùëñ ‚Üê Œò + ùúñI Œò (ùëñ);7   10  Compute upper boundC + Œò (ùë¢) ‚Üê ùëÑ 1‚àíùõº ({‚à•GCN(ùë¢, Œò ùúñ,ùëñ )‚à• 2 + err ùëñ |ùëñ ‚àà V train \ {ùë¢}}); 11 return U Œò (ùë¢) ‚Üê C + Œò (ùë¢) ‚àí C ‚àí Œò (ùë¢);the acquisition function, we select ùëè nodes from the remaining unlabelled nodes in the training set V train to query and then re-train the classifier. The query step is repeated until the query budget ùêæ is exhausted. Intuitively, a node with high predictive uncertainty is a better query candidate compared to the one with certain prediction.From Algorithm 2, we can obtain the jackknife uncertainty of each node in V train , hence, we define the acquisition function as follows, Acq(V train ) = argmax ùë¢ ‚ààV train U Œò (ùë¢), where U Œò (ùë¢) is the jackknife uncertainty of node ùë¢ from the remaining unlabelled nodes in V train and is computed in Eq.<ref type="bibr" target="#b7">(8)</ref>. Therefore, at each step, we select ùëè unlabelled nodes with the top-ùëè jackknife uncertainty. The detailed experimental settings are introduced in Appendix. Application # 2: Semi-supervised Node Classification. In training a GCN-based node classification model, existing approaches treat each training node equally and compute the mean of loss from all training nodes, i.e., ùëÖ = 1 | V train | ùëñ ‚ààV train ùëü (ùëñ, y ùëñ ,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell cols="4">Datasets Cora Citeseer PubMed</cell><cell>Reddit</cell></row><row><cell># nodes</cell><cell>2, 708</cell><cell>3, 327</cell><cell>19, 717</cell><cell>232, 965</cell></row><row><cell># edges</cell><cell>5, 429</cell><cell>4, 732</cell><cell cols="2">44, 338 114, 615, 892</cell></row><row><cell cols="2"># features 1, 433</cell><cell>3, 703</cell><cell>500</cell><cell>602</cell></row><row><cell># classes</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison results on active learning on node classification w.r.t. Micro-F1. Higher is better.</figDesc><table><row><cell>Data</cell><cell cols="3">Query size JuryGCN (Ours) ANRMAB</cell><cell>AGE</cell><cell>Coreset Centrality Degree</cell><cell>Random SOPT-GCN</cell></row><row><cell></cell><cell>20</cell><cell>51.1 ¬± 1.2</cell><cell cols="3">46.8 ¬± 0.5 49.4 ¬± 1.0 43.8 ¬± 0.8 41.9 ¬± 0.6 38.5 ¬± 0.7 40.5 ¬± 1.6</cell><cell>48.8 ¬± 0.7</cell></row><row><cell></cell><cell>40</cell><cell>64.7 ¬± 0.8</cell><cell cols="3">61.2 ¬± 0.8 58.2 ¬± 0.7 55.4 ¬± 0.5 57.3 ¬± 0.7 48.4 ¬± 0.3 56.8 ¬± 1.3</cell><cell>62.6 ¬± 0.8</cell></row><row><cell>Cora</cell><cell>60</cell><cell>69.9 ¬± 0.9</cell><cell cols="3">67.8 ¬± 0.7 65.7 ¬± 0.8 62.2 ¬± 0.6 63.1 ¬± 0.5 58.8 ¬± 0.6 64.5 ¬± 1.5</cell><cell>67.9 ¬± 0.6</cell></row><row><cell></cell><cell>80</cell><cell>74.2 ¬± 0.7</cell><cell cols="3">73.3 ¬± 0.6 72.5 ¬± 0.4 70.2 ¬± 0.5 69.1 ¬± 0.4 67.6 ¬± 0.4 69.7 ¬± 1.6</cell><cell>73.6 ¬± 0.5</cell></row><row><cell></cell><cell>100</cell><cell>75.5 ¬± 0.6</cell><cell cols="3">74.9 ¬± 0.4 74.2 ¬± 0.3 73.8 ¬± 0.4 74.1 ¬± 0.3 73.0 ¬± 0.2 74.2 ¬± 1.2</cell><cell>75.5 ¬± 0.7</cell></row><row><cell></cell><cell>20</cell><cell>38.4 ¬± 1.5</cell><cell cols="3">35.9 ¬± 1.0 33.1 ¬± 0.9 30.2 ¬± 1.2 35.6 ¬± 1.1 31.5 ¬± 0.9 30.3 ¬± 2.3</cell><cell>36.1 ¬± 0.7</cell></row><row><cell></cell><cell>40</cell><cell>51.1 ¬± 0.9</cell><cell cols="3">46.7 ¬± 1.3 49.5 ¬± 0.6 42.1 ¬± 0.8 49.8 ¬± 1.3 39.8 ¬± 0.7 41.1 ¬± 1.8</cell><cell>49.2 ¬± 0.5</cell></row><row><cell>Citeseer</cell><cell>60</cell><cell>58.2 ¬± 0.8</cell><cell cols="3">55.2 ¬± 0.9 56.1 ¬± 0.5 52.1 ¬± 0.9 57.1 ¬± 0.7 50.1 ¬± 1.1 49.8 ¬± 1.3</cell><cell>56.4 ¬± 0.5</cell></row><row><cell></cell><cell>80</cell><cell>63.8 ¬± 1.1</cell><cell cols="3">63.2 ¬± 0.7 61.5 ¬± 0.8 59.9 ¬± 0.6 63.3 ¬± 1.0 58.8 ¬± 0.6 58.1 ¬± 1.1</cell><cell>63.2 ¬± 0.8</cell></row><row><cell></cell><cell>100</cell><cell>64.3 ¬± 1.2</cell><cell cols="3">64.1 ¬± 0.5 63.2 ¬± 0.7 62.8 ¬± 0.4 63.9 ¬± 0.6 61.8 ¬± 0.5 62.9 ¬± 0.8</cell><cell>63.8 ¬± 0.6</cell></row><row><cell></cell><cell>10</cell><cell>61.8 ¬± 0.9</cell><cell cols="3">60.5 ¬± 1.3 58.9 ¬± 1.1 53.1 ¬± 0.7 55.8 ¬± 1.2 56.4 ¬± 1.5 52.4 ¬± 1.7</cell><cell>59.5 ¬± 0.6</cell></row><row><cell></cell><cell>20</cell><cell>70.2 ¬± 0.6</cell><cell cols="3">66.8 ¬± 1.1 68.7 ¬± 0.7 62.8 ¬± 0.5 67.2 ¬± 1.4 64.3 ¬± 1.0 60.5 ¬± 1.4</cell><cell>67.9 ¬± 0.9</cell></row><row><cell>Pubmed</cell><cell>30</cell><cell>73.9 ¬± 0.3</cell><cell cols="3">71.6 ¬± 0.8 72.8 ¬± 1.0 68.9 ¬± 0.3 73.5 ¬± 0.9 70.1 ¬± 0.7 68.9 ¬± 1.1</cell><cell>72.3 ¬± 0.8</cell></row><row><cell></cell><cell>40</cell><cell>74.6 ¬± 0.4</cell><cell cols="3">73.2 ¬± 0.6 74.7 ¬± 0.8 72.8 ¬± 0.8 74.1 ¬± 0.7 72.0 ¬± 0.8 71.8 ¬± 1.2</cell><cell>73.8 ¬± 0.7</cell></row><row><cell></cell><cell>50</cell><cell>75.4 ¬± 0.5</cell><cell cols="3">74.7 ¬± 0.4 75.1 ¬± 0.5 73.5 ¬± 0.6 74.2 ¬± 0.6 72.9 ¬± 0.5 73.1 ¬± 1.0</cell><cell>75.2 ¬± 0.5</cell></row><row><cell></cell><cell>50</cell><cell>69.7 ¬± 1.7</cell><cell cols="3">67.8 ¬± 0.9 64.2 ¬± 1.1 62.1 ¬± 0.6 65.5 ¬± 1.2 62.5 ¬± 1.4 63.7 ¬± 2.4</cell><cell>68.1 ¬± 1.2</cell></row><row><cell></cell><cell>100</cell><cell>82.9 ¬± 1.5</cell><cell cols="3">81.3 ¬± 1.0 79.5 ¬± 0.8 81.2 ¬± 1.0 78.2 ¬± 0.9 81.1 ¬± 1.2 80.5 ¬± 1.6</cell><cell>80.4 ¬± 1.3</cell></row><row><cell>Reddit</cell><cell>150</cell><cell>86.0 ¬± 1.4</cell><cell cols="3">84.3 ¬± 0.7 83.2 ¬± 0.4 84.8 ¬± 0.9 84.1 ¬± 1.1 82.5 ¬± 1.2 81.5 ¬± 1.4</cell><cell>85.0 ¬± 1.5</cell></row><row><cell></cell><cell>200</cell><cell>88.1 ¬± 0.9</cell><cell cols="3">86.1 ¬± 0.8 85.8 ¬± 0.5 85.5 ¬± 0.8 87.5 ¬± 0.8 85.4 ¬± 0.7 83.1 ¬± 1.8</cell><cell>87.2 ¬± 0.9</cell></row><row><cell></cell><cell>250</cell><cell>89.2 ¬± 0.8</cell><cell cols="3">87.6 ¬± 0.7 87.1 ¬± 0.4 86.6 ¬± 1.1 88.7 ¬± 0.6 86.1 ¬± 1.0 87.3 ¬± 1.5</cell><cell>87.8 ¬± 1.1</cell></row><row><cell cols="2">(a) Cora</cell><cell></cell><cell>(b) Citeseer</cell><cell></cell><cell>(c) Pubmed</cell><cell>(d) Reddit</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use ùõæ to represent the symbol before the leave-one-out error, i.e., ùëì ùúÉ (x test ) ‚àí |ùë¶ test ‚àí ùëì ùúÉ ‚àíùëñ (x test ) | when ùõæ = ‚àí, or ùëì ùúÉ (x test ) + |ùë¶ test ‚àí ùëì ùúÉ ‚àíùëñ (x test ) | otherwise.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">A sequence of random variables is exchangeable if and only if the joint distribution of the random variables remains unchanged regardless of their ordering in the sequence<ref type="bibr" target="#b43">[44]</ref>. This assumption is commonly used in random graph models, e.g., Erd≈ës-R√©nyi model<ref type="bibr" target="#b13">[14]</ref>, stochastic block model<ref type="bibr" target="#b21">[22]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">A statistical functional is a map that maps a distribution to a real number.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">The method to compute the first-order derivative was first proposed in<ref type="bibr" target="#b24">[25]</ref> for a different purpose, i.e., ensuring degree-related fairness in GCN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">We use ùëÖ to represent ùëÖ ( G, Y train , Œò) for notational simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">The theoretical coverage of jackknife+ is 1 ‚àí 2ùõº.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This work is supported by National Science Foundation under grant No. 1947135, and 2134079 by the NSF Program on Fairness in AI in collaboration with Amazon under award No. 1939725, by DARPA HR001121C0165, by NIFA award 2020-67021-32799, and Army Research Office (W911NF2110088). The content of the information in this document does not necessarily reflect the position or the policy of the Government or Amazon, and no official endorsement should be inferred. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Regarding the computation of ùúïùëü (ùëñ,y ùëñ ,Œò) ùúïE (ùëô ) , the key idea is to write out a recursive function with respect to ùúïùëü (ùëñ,y ùëñ ,Œò) ùúïE (ùëô )   based on the chain rule. More specifically, we first consider the element-wise computation and apply the chain rule as follows.</p><p>We take derivative on both sides of E (ùëô) = ùúé ( √ÇE (ùëô‚àí1) W (ùëô) ) and get</p><p>Combining Eqs. ( <ref type="formula">22</ref>) and ( <ref type="formula">23</ref>) together, we get the following elementwise first-order derivative.</p><p>ùúïùëü (ùëñ, y ùëñ , Œò)</p><p>Written Eq. ( <ref type="formula">24</ref>) into matrix form, we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF OF THEOREM 1</head><p>We prove case by case.</p><p>Case 1. When ùëñ = ùëô, since the activation function ùúé is the ReLU function, it is trivial that the subgradient of its second-order derivative is always 0 since the first-order derivative is the indicator function. Thus, ‚Ñå ùëô,ùëô = 0.</p><p>Case 2. When ùëñ = ùëô ‚àí 1, to get Eq. ( <ref type="formula">11</ref>), it is trivial to prove by taking derivative on both sides of Eq. <ref type="bibr" target="#b8">(9)</ref>. See the proof of Proposition 2 for the proof of Eq. <ref type="bibr" target="#b11">(12)</ref>.</p><p>Case 3. When ùëñ &lt; ùëô ‚àí 1, we first take derivative on both sides of Eq. ( <ref type="formula">9</ref> </p><p>It is trivial to get Eq. ( <ref type="formula">13</ref>) by writing out the matrix form of Eq. ( <ref type="formula">26</ref>).</p><p>Case 4. When ùëñ = ùëô + 1, to get Eq. ( <ref type="formula">14</ref>), it is trivial to prove by taking derivative on both sides of Eq. ( <ref type="formula">9</ref>). Regarding the computation of <ref type="bibr">[ùëê,ùëë ]</ref> , by Eq. ( <ref type="formula">10</ref>), we have ùúïùëÖ ùúïE (ùëô) [ùëé, ùëè]</p><p>Then we take derivative on both sides and get ‚Ä¢ ùúé ‚Ä≤ ùëô+1 [ùëé, ùëë] (28) Case 5. When ùëñ &gt; ùëô + 1, we get Eq. ( <ref type="formula">15</ref>) by taking derivative on both sides of Eq. <ref type="bibr" target="#b9">(10)</ref>.</p><p>Putting everything (Cases 1 -5) together, we complete the proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Moloud</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Pourpanah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadiq</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Rezazadegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><surname>Rajendra Acharya</surname></persName>
		</author>
		<title level="m">A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges. Information Fusion</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher-Order Influence Functions</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><surname>Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predictive Inference with the Jackknife+</title>
		<author>
			<persName><forename type="first">Rina Foygel</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="486" to="507" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Active Learning for Graph Embedding</title>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05085</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts</title>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z√ºgner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G√ºnnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1356" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gated Residual Recurrent Graph Neural Networks for Traffic Prediction</title>
		<author>
			<persName><forename type="first">Cen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="485" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Residuals and Influence in Regression</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanford</forename><surname>Weisberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Chapman and Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Node Classification in Uncertain Graphs</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Dallachiesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSDBM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">Micha√´l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-Shot Network Anomaly detection via cross-network meta-learning</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jackknife-After-Bootstrap Standard Errors and Influence Functions</title>
		<author>
			<persName><surname>Bradley Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological</title>
		<imprint>
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the Evolution of Random Graphs</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Erd≈ës</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfr√©d</forename><surname>R√©nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. Math. Inst. Hung. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="17" to="60" />
			<date type="published" when="1960">1960. 1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Power of Certainty: A Dirichlet-Multinomial Model for Belief Propagation</title>
		<author>
			<persName><forename type="first">Dhivya</forename><surname>Eswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G√ºnnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Von Mises Calculus for Statistical Functionals</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Turrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernholz</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout as A Bayesian Approximation: Representing Model Uncertainty in Deep Learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Active Discriminative Network Representation Learning</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2142" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Utilizing Graph Machine Learning within Drug Discovery and Development</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gaudelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyothish</forename><surname>Arian R Jamasb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gertrude</forename><surname>Regep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">Br</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Hayter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bayesian Graph Neural Networks with Adaptive Connection Sampling</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
		<idno>ICML. 4094-4104</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic Blockmodels: First Steps</title>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">Blackmond</forename><surname>Paul W Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="109" to="137" />
			<date type="published" when="1983">1983. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On Embedding Uncertain Graphs</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc-Alexandre</forename><surname>C√¥t√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13463</idno>
		<title level="m">Graph policy network for transferable active learning on graphs</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rawls-GCN: Towards Rawlsian Difference Principle on Graph Convolutional Network</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding Black-Box Predictions via Influence Functions</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Balaji Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Uncertainty Aware Graph Gaussian Process for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Zhao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shao-Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songcan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Jun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4957" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Œ£-Optimality for Active Learning on Gaussian Random Fields</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Simple Baseline for Bayesian Uncertainty in Deep Learning</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Wesley J Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timur</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><forename type="middle">P</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predictive Uncertainty Estimation via Prior Networks</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Jackknife-A Review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rupert</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1974">1974. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Query-Driven Active Surveying for Collective Classification</title>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Edu</surname></persName>
		</author>
		<idno>MLG. 1</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayesian Semi-Supervised Learning with Graph Gaussian Processes</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cheng Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicol√≤</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DropEdge: Towards Deep Graph Convolutional Networks on Node Classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<title level="m">Collective Classification in Network Data. AI Magazine</title>
				<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00489</idno>
		<title level="m">Active Learning for Convolutional Neural Networks: A Core-set Approach</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph Posterior Network: Bayesian Predictive Uncertainty for Node Classification</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z√ºgner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G√ºnnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bias and Confidence in Not-Quite Large Sample</title>
		<author>
			<persName><forename type="first">John</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">614</biblScope>
			<date type="published" when="1958">1958. 1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Uncertainty Estimation Using a Single Deep Deterministic Neural Network</title>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veliƒçkoviƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Algorithmic Learning in a Random World</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Shafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Semi-Supervised Graph Attentive Network for Financial Fraud Detection</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bayesian Graph Convolutional Neural Networks for Semi-supervised Classification</title>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumyasundar</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Ustebay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5829" to="5836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Uncertainty Aware Semi-Supervised Learning on Graph Data</title>
		<author>
			<persName><forename type="first">Xujiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Hee</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Neural Networks: A Review of Methods and Applications</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards Real Time Team Optimization</title>
		<author>
			<persName><forename type="first">Qinghai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE BigData</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attent: Active attributed network alignment</title>
		<author>
			<persName><forename type="first">Qinghai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
