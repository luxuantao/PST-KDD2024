<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Strong planning under partial observability</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Piergiorgio</forename><surname>Bertoli</surname></persName>
							<email>bertoli@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department">ITC-IRST</orgName>
								<address>
									<addrLine>Via Sommarive 18</addrLine>
									<postCode>38055</postCode>
									<settlement>Povo, Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alessandro</forename><surname>Cimatti</surname></persName>
							<email>cimatti@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department">ITC-IRST</orgName>
								<address>
									<addrLine>Via Sommarive 18</addrLine>
									<postCode>38055</postCode>
									<settlement>Povo, Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Roveri</surname></persName>
							<email>roveri@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department">ITC-IRST</orgName>
								<address>
									<addrLine>Via Sommarive 18</addrLine>
									<postCode>38055</postCode>
									<settlement>Povo, Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paolo</forename><surname>Traverso</surname></persName>
							<email>traverso@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department">ITC-IRST</orgName>
								<address>
									<addrLine>Via Sommarive 18</addrLine>
									<postCode>38055</postCode>
									<settlement>Povo, Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Strong planning under partial observability</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">649ABBF115AF422B5B60D24913B85F78</idno>
					<idno type="DOI">10.1016/j.artint.2006.01.004</idno>
					<note type="submission">Received 4 April 2004; received in revised form 1 May 2005; accepted 10 January 2006</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Planning under partial observability</term>
					<term>Planning in nondeterministic domains</term>
					<term>Heuristic search in belief space</term>
					<term>Symbolic model checking</term>
					<term>Binary decision diagrams</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rarely planning domains are fully observable. For this reason, the ability to deal with partial observability is one of the most important challenges in planning. In this paper, we tackle the problem of strong planning under partial observability in nondeterministic domains: find a conditional plan that will result in a successful state, regardless of multiple initial states, nondeterministic action effects, and partial observability.</p><p>We make the following contributions. First, we formally define the problem of strong planning within a general framework for modeling partially observable planning domains. Second, we propose an effective planning algorithm, based on and-or search in the space of beliefs. We prove that our algorithm always terminates, and is correct and complete. In order to achieve additional effectiveness, we leverage on a symbolic, BDD-based representation for the domain, and propose several search strategies. We provide a thorough experimental evaluation of our approach, based on a wide selection of benchmarks. We compare the performance of the proposed search strategies, and identify a uniform winner that combines heuristic distance measures with mechanisms that reduce runtime uncertainty. Then, we compare our planner MBP with other state-of-the art-systems. MBP is able to outperform its competitor systems, often by orders of magnitude.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Very often, planning domains are partially observable and nondeterministic: at execution time, the state of the world can not be completely observed, and actions may have several possible outcomes. In this case, the status of the domain can not be uniquely determined. Planning for nondeterministic and partially observable domains is a significant, well known and difficult problem. It is a significant problem since several realistic applications require to deal with sources of nondeterminism, within the general case of partial observability. The special cases of full observability, where every state variable can be observed at every step, and null observability, where no observation is ever possible, only cover a limited set of realistic situations. Indeed, the problem has been extensively addressed in the literature. Different approaches include extensions to techniques for classical planning, see e.g., <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b87">88]</ref>, Partially Observable Markov Decision Processes (POMDPs), see e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b82">83]</ref>, logical frameworks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b50">51]</ref>, planning based on Quantified Boolean Formulas (QBF) <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b77">78]</ref>, and planning based on symbolic model checking <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b78">79]</ref>. The problem has been shown to be hard, both theoretically and experimentally. Compared to planning under full observability, planning under partial observability must deal with uncertainty about the state in which the actions will be executed. This makes the search space no longer the set of states of the domain, but its powerset, i.e. the space of "belief states" <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43]</ref>. Compared to the case of null observability, called conformant planning, plans are no longer sequential, but conditional, in order to represent a conditional course depending on the observations performed at execution time.</p><p>In this paper, we address the problem of strong planning under partial observability, i.e., the problem of generating conditional plans that are guaranteed to achieve the goal in spite of the nondeterminism and partial observability of the domain. Strong planning can be formalized as a problem of search in the space of a (possibly cyclic) and-or graph induced by the domain <ref type="bibr" target="#b8">[9]</ref>, where each node in the graph corresponds to the set of possible states for the current situation, i.e., a belief state. Or-nodes correspond to alternative actions that can be applied to a belief state, while andnodes correspond to observations that partition a belief state into subsets for all of which a solution must be found. We present a novel approach to the problem and provide evidence that the approach is well-founded and practical:</p><p>-We present a planning algorithm that performs heuristic search over the and-or graph. Differently from AO* <ref type="bibr" target="#b65">[66]</ref>,</p><p>we can not assume that the graph is acyclic, and differently from LAO* <ref type="bibr" target="#b52">[53]</ref>, we do not accept cyclic plans, which might not terminate. As such, our algorithm must generate conditional plans that are guaranteed to achieve the goal by ruling out possible cycles. This is achieved by keeping track of the part of the graph that has been explored so far in an effective way. We prove that the algorithm is correct and complete, i.e., it finds a solution if it exists and, if no solution exists, it terminates with failure. -We show that the search in the and-or graph can be implemented by means of symbolic model checking techniques <ref type="bibr" target="#b63">[64]</ref>. In particular, Binary Decision Diagrams (BDDS) <ref type="bibr" target="#b24">[25]</ref> can be used to compactly represent and efficiently manipulate belief states. -We show that our algorithm can easily embed different search strategies, and that BDD-based techniques can be fruitfully exploited to speed up the computation of heuristic values that drive those strategies. -We implement the algorithm in the Model Based Planner (MBP) <ref type="bibr" target="#b4">[5]</ref>. We provide an extensive experimental evaluation of our approach, considering various search strategies, and comparing our system with other planners. The experimental evaluation includes problems available from the distribution set of other planners. We show that MBP outperforms the other planners that can deal with nondeterministic domains under partial observability.</p><p>The paper is organized as follows. We provide a formal definition of partially observable planning domains, problems, and conditional plans (Section 2). We describe how planning under partial observability can be seen as searching through an and-or graph of belief states (Section 3). In Section 4, we define the planning algorithm, and prove its properties. In Section 5, we discuss how BDD-based, symbolic techniques can be effectively used in the search. We present the experimental evaluation in Section 7. Finally, we discuss related work in Section 8, while Section 9 draws some conclusions and discusses future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Domains, observations, plans and problems</head><p>Partially observable planning domains can be generally presented by means of two components: a nondeterministic state-transition system, describing the effect of actions, and an observation function that describes the available sensing over the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Actions in nondeterministic domains</head><p>A nondeterministic state-transition system is defined in terms of its states, its actions, and of a transition function that describes how (the execution of) an action leads from one state to possibly many different states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.1 (State transition system).</head><p>A nondeterministic state transition system is a tuple Σ = S, A, R , where S is a finite set of states, A is a finite set of actions, and R : S × A → 2 S is the transition function. The transition function associates to each state s ∈ S and to each action a ∈ A the set R(s, a) ⊆ S of next states. An action a is applicable in a state s (denoted with applicable(a, s)) iff |R(s, a)| &gt; 0; it is deterministic (nondeterministic) in s iff |R(s, a)| = 1 (|R(s, a)| &gt; 1). If a is applicable in s, then R(s, a) is the set of states that can be reached from s by performing a.</p><p>Example 2.2. Fig. <ref type="figure" target="#fig_0">1</ref> shows a simple robot navigation domain and its corresponding state transition system. The robot can be in four positions, corresponding to the states of the domain: S = {NW, NE, SW, SE}. It can move in the four directions, corresponding to actions A = {GoNorth, GoSouth, GoEast, GoWest}. An action is applicable only if there is no wall in the direction of motion, e.g. R(NW, GoSouth) = ∅. All the actions are deterministic (e.g., R(SW, GoEast) = {SE}), except for action GoEast when on NW, where the robot may slip and end up either in NE or SE, i.e. R(NW, GoEast) = {NE, SE}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Observations</head><p>A domain may not be completely observable-different states may be impossible to distinguish. For instance, it may be possible for the robot to observe the presence or absence of walls in its neighboring locations, but it may not be possible to observe the absolute location. In order to model partial observability, we define a set of observations, which intuitively correspond to the possible readings conveyed by sensors at each time instant; an observation function defines the correspondence between the (hidden) states of the domain and the possible observations (of which, of course, only one will be nondeterministically chosen to hold by the domain, at each runtime step).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.3 (Observation function).</head><p>Let Σ = S, A, R be a nondeterministic state transition system. Let O be a finite set of observations. An observation function over S and O is a function X : S → 2 O \ ∅, which associates to each state s the nonempty set of possible observations X (s) ⊆ O.</p><p>Our model of observation is inspired by the idea of automatic sensing <ref type="bibr" target="#b85">[86]</ref>, where information is conveyed by sensors without an explicit action being required, and each state is associated with at least one observation. In the following, X -(o) denotes the set of states which are compatible with the observation o, i.e. X -(o) = {s: o ∈ X (s)}. We also use o, called observation complement, to indicate that observation o does not occur. We write O to denote the set of all observation complements, i.e. O = {o | o ∈ O}. We denote with X -(o) the set of states which are compatible with any observation other than o, i.e. X -(o) = {s: ∃o ∈ X (s), o = o}.</p><p>Despite its simplicity, our model of observation is rather general. The special cases of full observability is modeled by defining the observation function X to be a bijection between O and S. In our robot domain, the observation function would associate to state SW an observation sw, to SE an observation se, and so on. The special case of null observability (which is the assumption for conformant planning) is captured when O is a singleton: since the same observation is associated to all the states, it conveys no information. With the following example, we show how we model the general situation where sensing does not give complete information on the current state of the domain. This happens when different states may result in the same observation, i.e. when X (s 1 ) ∩ X (s 2 ) = ∅ for s 1 = s 2 .</p><p>Example 2.4. If we assume that the robot can only perceive the position of the walls neighboring its location, then it cannot distinguish between the two positions NW and SW. This situation can be modeled with a set of observations O = {nsw, ne, se}, and an observation function X (NW) = X (SW) = {nsw}, X (NE) = {ne}, and X (SE) = {se}.</p><p>Our model also allows for noisy observations, i.e. the case where the same state can be associated with several different observations. Formally, we say that an observation o is noisy iff for some s ∈ X -(o), |X (s)| &gt; 1.</p><p>Example 2.5. Let us consider the case where the robot sensors may sometimes not work, and return a meaningless value noinfo: X (NW) = X (SW) = {nsw, noinfo}, X (NE) = {ne, noinfo}, and X (SE) = {se, noinfo}. In this case, all the observations are noisy; e.g., X -(nsw) = {NW}, and |X (NW)| = 2; this indicates that in state NW the observation nsw may or may not occur.</p><p>Finally, as shown in the following example, our framework is capable of modeling situations where the ability to observe is conditioned to the prior execution of "sensor-enabling" actions.</p><p>Example 2.6. Consider the case where the robot has to perform a WallScan sensing action in order to "set up" the sensors that allow it to observe the configuration of the surrounding walls; prior to executing such action, sensors return no information. This domain can be modeled as depicted in Fig. <ref type="figure">2</ref>, by adding to the domain the new states NW0, SW0, SE0 and NE0, which represent states NW, SW, SE and NE prior to the execution of the WallScan action (depicted as a dashed arrow). The observation function is the same as the one defined in Example 2.4 for the original states, while it returns a noinfo value for the new states: the intuition is that if WallScan has not been just executed, the same observation noinfo is bound to occur, thus conveying no information.</p><p>A planning domain is composed by a nondeterministic state transition system, and an observation function over a finite set of observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.7 (Planning domain).</head><p>A planning domain D is a tuple Σ, O, X , where:</p><p>-Σ = S, A, R is a nondeterministic state transition system, -O is a finite set of observations, and -X is an observation function over S and O.</p><p>We remark that, in our framework, domains are described at a semantic level, independently of a concrete language that can be used to describe them: states and observations are atomic entities. In practice, however, we use a concrete language where variables can be used in order to express domains in a compact manner: the sets of states and observations are presented by means of state variables and observation variables, while the transition and observation functions are expressed as formulae over such variables. Fig. <ref type="figure">2</ref>. The simple domain where sensors may be broken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Conditional plans</head><p>Differently from classical planning, we consider tree-structured plans with conditional courses of action, where different behaviors can be determined by different run-time observations. Definition 2.8 (Conditional plan). The set of conditional plans Π for a domain D = Σ, O, X is the minimal set such that:</p><formula xml:id="formula_0">-ε ∈ Π ; -if a ∈ A and π ∈ Π , then a • π ∈ Π ; -if o ∈ O, and π 1 , π 2 ∈ Π , then if o then π 1 else π 2 ∈ Π .</formula><p>ε is the empty plan, representing the end of the execution. The plan a • π is the sequential composition of the action a with plan π . The plan if o then π 1 else π 2 is a conditional plan, that branches on the occurrence of the observation o. In the following, we only consider finite conditional plans, and we use π, π , π , π 1 , π 2 , . . . to denote them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Syntactic properties of plans</head><p>We call an action-observation path (or simply a path) an element of (A ∪ O ∪ O) * , i.e. a finite, possibly empty sequence composed of actions, observation, and observation complements. Definition 2.9 (Path of a plan). An action-observation path p is a path of a plan π iff either</p><formula xml:id="formula_1">-p = ε; or -π = a • π , p = a • p , and p is a path of π ; or -π = if o then π 1 else π 2 , and either (i) p = o • p 1 and p 1 is a path of π 1 , or (ii) p = o • p 2 and p 2 is a path of π 2 .</formula><p>The symbol ε denotes the empty path; we assume, as standard for strings, that p • ε = ε • p = p. We use p, p , p , p 1 , p 2 , . . . to denote paths, and we write Paths(π) for the set of paths in π . We notice that, if p ∈ Paths(π), then p ∈ Paths(π) for every p prefix of p. Furthermore, p • o ∈ Paths(π) iff p • o ∈ Paths(π). We say that p ∈ Paths(π) is a maximal path of π if p is not a prefix of any other p ∈ Paths(π); we write MaxPaths(π) for the set of maximal paths of π . If p is a path and P is a set of paths, we write p • P for {p • p | p ∈ P }. Given two paths p, p , we indicate with p p (p &lt; p ) the fact that p is a (strict) prefix of p .</p><p>Intuitively, a path p of a plan π identifies a (partial) course of action among the possible ones that are compatible with π , depending on the observations conveyed by the sensors. A path also identifies a subplan, i.e. the rest of the plan to be executed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.10 (Subplan).</head><p>Let p be a path of π . Then the subplan of π identified by p, written π(p), is defined as follows:</p><formula xml:id="formula_2">-if p = ε, then π(p) = π ; -if π = a • π and p = a • p , then π(p) = π (p ); -if π = if o then π 1 else π 2 and p = o • p , then π(p) = π 1 (p ); -if π = if o then π 1 else π 2 and p = o • p , then π(p) = π 2 (p ).</formula><p>We say that π is a (strict) subplan of π iff there exists a (nonempty) p ∈ Paths(π) such that π = π(p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Semantic notions: Plan applicability and execution</head><p>In general, we are interested in applicable plans, i.e. plans whose execution guarantees that an action is never attempted unless it is applicable, regardless of nondeterminism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.11 (Applicability).</head><p>A plan π is applicable in state s iff either π is the empty plan ε; or π is a • π 1 , a is applicable in s, and π 1 is applicable in every s ∈ R(s, a); or π is if o then π 1 else π 2 , and:</p><p>-</p><formula xml:id="formula_3">if s ∈ X -(o), π 1 is applicable in s, and -if s ∈ X -(o), π 2 is applicable in s.</formula><p>Considering the case of noisy sensing, where X -(o) ∩ X -(o) = ∅, the definition deals with branching by requiring that both π 1 and π 2 are applicable to states associated both to o and to o.</p><p>The execution of a plan is defined in terms of the runs associated to it. Intuitively, a run contains the states, observations and actions encountered while executing the plan. Depending on the nondeterministic behavior of the domain, and on the uncertainty on the initial state, a plan can be associated with different runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.12 (Runs of a plan). A run is a sequence</head><formula xml:id="formula_4">σ = (s 0 , o 0 ) • a 1 • (s 1 , o 1 ) • a 2 • • • • • a n • (s n , o n ), where s i ∈ S, o i ∈ O, and a i ∈ A. A sequence σ is a run of a plan π from state s iff either -π = ε, and σ = (s, o) with o ∈ X (s); or -π = a • π 1 and σ = (s, o) • a • σ 1 with o ∈ X (s), and σ 1 is a run for π 1 for some s 1 ∈ R(s, a) = ∅; or -π = if o then π 1 else π 2 , s ∈ X -(o), and σ is a run of π 1 starting from (s, o); or -π = if o then π 1 else π 2 , s ∈ X -(o), o ∈ X (s),</formula><formula xml:id="formula_5">GoEast • (if ne then GoSouth • GoWest else GoWest)</formula><p>Action GoEast is executed first; then, if observation ne occurs, then GoSouth • GoWest is executed; otherwise, the plan executes GoWest. The plan is applicable in the two states NW and SW: GoEast is applicable in both states, and (if ne then GoSouth • GoWest else GoWest) is applicable in the resulting states {NE, SE}, since if the observation ne occurs, GoSouth • GoWest is applicable in NE, and otherwise GoWest is applicable in SE. Fig. <ref type="figure">3</ref> depicts the set of runs of the plan in Example 2.4, starting from the states NW and SW; in each run, the robot position is indicated as a soft cornered box. There are three possible runs, which correspond to the fact that we consider two possible initial Fig. <ref type="figure">3</ref>. Runs of a conditional plan.</p><p>states, and for one of them, the first executed action GoEast may produce two different possible outcomes. Thus, the execution of the plan may originate one of runs σ 1 , σ 2 and σ 3 , where:</p><p>-</p><formula xml:id="formula_6">σ 1 = (NW, nsw) • GoEast • (NE, ne) • GoSouth • (SE, se) • GoWest • (SW, nsw), -σ 2 = (NW, nsw) • GoEast • (SE, se) • GoWest • (SW, nsw), and -σ 3 = (SW, nsw) • GoEast • (SE, se) • GoWest • (SW, nsw).</formula><p>We see that this plan is guaranteed to reach SW for all its runs, from any of the initial states, either with three actions (if the outcome of the first action is NE), or with two actions (if it is SE).</p><p>In the following, we write Runs(s, π) to denote the set of runs of π starting from s. We write FinalStates(s, π) to indicate the set of the final states of the runs of Runs(s, π). These notions can be generalized to sets of states: if B is a set of states, we write Runs(B, π) for s∈B Runs(s, π), and FinalStates(B, π) to indicate the set of final states of the runs of Runs(B, π).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">The problem of strong planning</head><p>A planning problem under partial observability for a given domain is described by a set of possible initial states, and a set of goal states. We are interested in strong solutions, i.e. plans that are guaranteed to be applicable in all the initial states, and that are guaranteed to end in a goal state for any possible run starting from an initial state. Other definitions of solution are also possible: for instance, a more optimistic approach would be to look for weak solutions, which only require the goal to be reached for some possible run. (See <ref type="bibr" target="#b33">[34]</ref> for a general discussion in the case of full observability.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Strong planning in the belief space</head><p>A major consequence of partial observability is that uncertainty must be explicitly dealt with at planning time. It may be impossible to detect at run time which is the actual initial state among the possible ones, even with the support of observations; for instance, in Example 2.13, the robot can not uniquely identify its initial state, since the states NW and SW are associated with the same observation nsw. Similarly, even if the current state is known, nondeterministic action effects may result in a set of possible states which is impossible to distinguish.</p><p>We model the impossibility of uniquely identifying the state of the domain by means of belief states <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">43]</ref>. A belief state is a set of states, intuitively all the states which are possible but indistinguishable. In the following, B, B 0 , B 1 , . . . are belief states. The basic notions related to plan execution can be naturally extended to the case of belief states as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.1 (Applicability in a belief</head><p>). An action a is applicable in the belief state B, written applicable(a, B), iff it is applicable in all the states in B.</p><p>This definition captures the idea that, since the states in B are indistinguishable, we never "risk" by attempting the execution of an action which might not be applicable. Plan applicability over a belief is also defined similarly, based on Definition 2.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.2 (Execution in a belief).</head><p>The execution of a in B, written Exec(B, a), is defined as s∈B R(s, a) when a is applicable in B, and ∅ otherwise.</p><p>Since the states in B are indistinguishable, when the action is executed, every state that may result from executing the action on any of the states in B must be taken into account. The effect of an observation can be modeled as follows. Let B be the belief state containing all the states of the domain that are possible but indistinguishable. If the observation o occurs, then our belief can be refined to B ∩ X -(o): that is, we can rule out the states that are not compatible with an occurrence of o. Similarly, if o (i.e., an observation other than o) occurs, then our belief is given by B ∩ X -(o), since we can rule out the states that are not compatible with o not occurring. Given a belief state, it is sometimes possible to know prior to observing whether an observation o occurs or not. In this case, we say the observation is predetermined, and observing does not convey any additional information over the current belief. Formally, an observation o is predetermined over a belief B iff either B ∩ X -(o) = ∅ (i.e. we know o will take place), or B ∩ X -(o) = ∅ (i.e. we know o will not take place).</p><p>Example 3.3. Let us consider the robot domain in Example 2.4, and assume the robot can be either in NE or in SE. The corresponding belief state B = {NE, SE} can be refined when either observation ne or se occurs. With observation ne, we have that B ∩ X -(ne) = NE and B ∩ X -(ne) = SE. When observation se occurs, we have that B ∩ X -(se) = SE and B ∩ X -(se) = NE. We notice that ne and se convey equivalent information, since they split the belief in the same way. The observation nsw is predetermined, since B ∩ X -(nsw) = ∅: that is, we know that nsw can not occur on B, and indeed,</p><formula xml:id="formula_7">B ∩ X -(nsw) = B.</formula><p>Given this, we can now define the way a belief evolves along the execution of one of the possible paths of a plan: Definition 3.4 (Belief execution along a path). Let π be a plan applicable in a belief state B, and let p ∈ Paths(π). The belief execution along p, written Exec(B, p), is defined as follows:</p><formula xml:id="formula_8">-if p = ε, then Exec(B, p) = B; -if p = a • p , then Exec(B, p) = Exec(Exec(B, a), p ); -if p = o • p , then Exec(B, p) = Exec(B ∩ X -(o), p ); -if p = o • p , then Exec(B, p) = Exec(B ∩ X -(o), p ).</formula><p>Intuitively, the belief execution along a path is the belief resulting from the execution of the actions and the occurrence of observations in the path. We write FinalBels(B, π) = {Exec(B, p) | p ∈ MaxPaths(π)} for the set of beliefs associated to maximal paths in π .</p><p>The following theorem shows that it is possible to define strong plans in terms of belief states; this allows recasting strong planning under partial observability as a problem of search in the belief space. The proof is reported in Appendix A. We can collect together the belief executions along the paths of a plan; once these are associated with their paths, they form a tree structure which represents the way beliefs can be reached by acting or observing. We call this structure a Belief Execution Tree, and represent it as a labelled directed hypergraph, that is a graph structure whose nodes are connected by labelled directed hyperarcs. Each hyperarc associates a source node to a set of target nodes; in particular, the hyperarcs of a Belief Execution Tree associate a node (a belief) either to a single node, representing the execution of an action, or to a pair of nodes, representing the two possible outcomes of executing an observation. A Belief Execution Tree can be univoquely associated to a plan, where a subtree corresponds to a subplan; these features will be relevant in the design of a search algorithm based on iterative extension of a tree. Since each path in Paths(π) is associated with exactly one node in BET(π, B), in the following we also simply write p to indicate the node p, Exec(B, p) . Given the properties of Paths and the structure of plans, it is easy to see that the Belief Execution Tree is indeed a tree: every non-leaf node is the source of one outcoming arc, and every non-root node is the target to one incoming arc.</p><p>More specifically, the root of the tree is the node associated with ε, and the leaves are the nodes associated to maximal paths (and thus, to the beliefs in FinalBels(B, π)). Intuitively, the belief associated with a leaf node corresponds to a complete execution of a maximal path of the plan. The father and son relations between nodes are defined in the obvious way. We also say that two nodes are brothers iff they are indexed by two paths of the form p • o and p • o; the node p is an ancestor of node p if p is a strict prefix of p . From now on, we will also represent a Belief Execution Tree as a triple n 0 , N, E , with N and E the sets of nodes and arcs, and n 0 the root node. We will say a tree Tis a (strict) prefix of a tree T if the nodes and edges of Tare (strict) subsets of those in T .</p><p>Example 3.7. Fig. <ref type="figure" target="#fig_3">4</ref> depicts the belief execution tree associated to the strong solution of Example 2.15. The set {Bs6} is the corresponding final belief set, where Bs6 is the belief associated to both leaf nodes in the Belief Execution Tree.</p><p>Given a belief execution tree T and a node n belonging to it, we can extract the subtree of T rooted at n: Definition 3.8 (Subtree). Given a belief execution tree T and a node n of T , associated with path p n , its subtree from n, indicated with SubTree(T , n), is a belief execution tree such that:</p><p>-for every node of T indexed with a path p s.t. p n p, there exists a node n ∈ SubTree(T , n). If p = p n , then n is indexed by path ε; if p = p n • p , then n is indexed by path p . -for every arc that associates nodes of T that have correspondent nodes in SubTree(T , n), there exists one correspondent arc in SubTree(T , n), connecting the correspondent nodes.</p><p>It can be easily argued that a subtree is unique, and is a belief execution tree. Moreover, the association of a belief execution tree with an executable plan is bijective: Definition 3.9 (Plan associated to belief execution tree). Given a belief execution tree T = n 0 , N, E , its associated plan PlanOf (T ) is defined as follows:</p><p>-</p><formula xml:id="formula_9">if E = ∅, then PlanOf (T ) = ε; -if n 0 , a, n ∈ E, then PlanOf (T ) = a • PlanOf (SubTree(T , n)); -if n 0 , o, n 1 , n 2 ∈ E, then PlanOf (T ) = if o then π 1 else π 2 ,</formula><p>where π 1 = PlanOf (SubTree(T , n 1 )) and π 2 = PlanOf (SubTree(T , n 2 )).</p><p>Also in this case, uniqueness is immediate (given the uniqueness of subtrees). If π is a plan applicable on belief I, PlanOf (BET(π, I)) = π , as it is possible to prove by recursion on the plan structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Planning as and-or search</head><p>In this section, we present the algorithm for strong planning under partial observability (Section 4.1), and formally prove its properties (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The planning algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Top level</head><p>Our approach for strong planning under partial observability is based on progressing from the initial belief state I towards the goal G. The belief space is explored by incrementally constructing a search tree, which collects together a set of belief execution trees, each of them being associated to one plan that has been expanded during the search. The search takes place by constructing belief states, either by applying actions or by means of observations. The application of an action starting from a belief state results in a belief state, while applying observations produces sets of belief states (that is, the starting belief state is split amongst the possible observations). We essentially perform a kind of and-or search with loop detection. The application of an action corresponds to an "or" expansion: to reach the goal from a belief B, it is enough to find a strong solution for any of the actions applicable in B. Applying an observation induces an "and" expansion: once we decide which observation o to consider, the plan must take into account both cases where o holds and does not hold. Indeed, the selection of the observation to be considered is per se "or" choice; however, we avoid introducing an explicit "or-arc" and corresponding node to represent this choice; instead, we represent the chosen observation as a label of the "and-arc", consistently with what we do in Belief Execution Trees.</p><p>By analyzing the beliefs associated to the ancestors of a node, we are able to detect and avoid loop-backs, i.e. paths that traverse two nodes associated to the same belief. In this way, the search restricts to plans that can never traverse a belief more than once during the execution. Loop detection and avoidance is crucial since, differently from wellknown algorithms as AO* <ref type="bibr" target="#b65">[66]</ref>, we can not assume that the search space is acyclic, and differently from more recent extensions such as LAO* <ref type="bibr" target="#b52">[53]</ref>, we do not accept cyclic solutions, whose execution is not guaranteed to terminate. A certain node is closed with success if a plan exists that leads from it to the goal; it is closed with failure if all possible plans lead to loop-backs.</p><p>The search terminates when either the expanded search tree has its root tagged as successful, or when no further expansion is possible and no solution has been found. Example 4.1. Fig. <ref type="figure" target="#fig_4">5</ref> represents the search tree for the domain in Example 2.4. For instance, node n2 is expanded into node n3 by the only applicable action on it, GoWest, and into nodes n4 and n5 by the observation se. The figure does not report predetermined observations, such as nsw for node n1. Also, for sake of clarity, we do not report in the figure actions or observations that behave exactly as other (reported) actions or observations, since intuitively they are redundant, and would originate replicated portions of the search tree. (E.g., we omit observation ne, applied to node n2, since it has the same effect of observation se.) Notice that even in this simple case, loops are possible; for instance, node n11 loops back to node n2. Also notice that several nodes associated to the same belief appear in the search tree, e.g. n4, n8 and n12 are all associated to Bs4. This represents the fact that the same situation can be reached by different paths (in general, belonging to different plan prefixes). Each of such paths may or may not correspond to a loop: in this example, n12 is associated to a looping path, while n4 and n8 are not.   The planning algorithm is described in Fig. <ref type="figure" target="#fig_5">6</ref>. It takes as input the initial belief state and the goal belief state, while the domain representation is assumed to be globally available to the subroutines. The state of the search is represented within a search tree structure ST, which represents an acyclic portion of the search space of beliefs. Each node n in the search tree is associated with an action-observation path PATH(n), and with a belief state BEL(n), that is reached by executing the action-observation path. In particular, the root ROOT(ST ) is associated to the initial belief, and to the empty path. In addition, each node generated in the search is associated with a 'tag', with three possible values: Success, meaning that a plan is found that leads from the node to the goal; Failure, meaning that no such plan exists; or Undetermined, meaning that neither a plan has been found for the node, nor the algorithm has been able (yet) to detect that no such plan exists. The corresponding predicates are ISSUCC, ISFAIL, and ISUNDET. The TAGNODE primitive sets the value of the tag for a node. The set of the undetermined leaves of the search tree is called frontier.</p><p>The algorithm first initializes the search tree with the start node (line 2), and checks if the empty plan is a solution to the problem (lines 3-4). Then, the main loop is entered (lines <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, where the tree is iteratively expanded until the root is tagged either as success or as failure. When the loop is exited (lines 17-20), a plan is constructed and returned in case of success; otherwise, Failure is returned.</p><p>The body of the main loop proceeds by selecting and extracting a node from the frontier, and expanding it. With the first step in the loop, at line 7, a node is selected for expansion, and is extracted from the frontier. The EXTRACTN-ODEFROMFRONTIER primitive embodies the selection criterion, and is responsible for the style of the search (e.g. depth-first versus breadth-first). The selection criterion does not affect the properties of the algorithm, namely completeness, correctness and termination.</p><p>At line 8, node is expanded, considering every possible action and observation. For each applicable action, a new node is generated, and then connected to node via an or-arc. For every observation that is not predetermined, two new nodes are generated, and then connected to node via an and-arc. This expansion step also decides the status of each newly generated node n, updating the frontier consistently: namely, n is tagged Success if BEL(n) ⊆ G; it is tagged Failure iff there exists an ancestor node associated to the same belief, i.e. a loop has been generated; it is tagged Undetermined otherwise, and added to the frontier.</p><p>If it is possible to state the success of node based on the status of the newly introduced sons (primitive SON-SYIELDSUCCESS at line 9), i.e. if an or-son is successful node, or a pair of and-sons are successful, then node is tagged as success (line 10). In this case (line 11), the recursive PROPAGATESUCCESSONTREE primitive propagates success bottom-up on the search tree, to the ancestors of node, towards the initial node.</p><p>If the node is not detected to be successful, the SONSYIELDFAILURE primitive tries to state if a node is a failure. This happens when every or-son causes a loop, and for every and-arc, at least one of the and-sons causes a loop. In this case, the node is tagged as failure, and the failure is propagated bottom-up, in order to cut the search in branches which are bound to fail because some leaf has failed.</p><p>Example 4.2. Fig. <ref type="figure" target="#fig_6">7</ref> describes a possible behavior of the algorithm for the example of Fig. <ref type="figure" target="#fig_3">4</ref>, by depicting the search tree at the start of each iteration of the main loop. In the tree, nodes tagged as Undetermined are white, nodes tagged with Failure are shaded with a light grey, and nodes tagged with Success are shaded with a deeper grey. In this example, we consider a selection strategy such that, at each iteration, the frontier node which is leftmost in Fig. <ref type="figure" target="#fig_6">7</ref> is picked and expanded; the arcs built by the expansion are represented by continuous lines, while loops discovered at each iteration are shown as dashed lines. In this case, failure propagation only takes place at iteration 5, while success propagation takes place at iterations 6 and 7 (where it reaches the root of the tree, causing the algorithm to stop). Notice that we do not report irrelevant observations, such as, e.g., those that can be considered in the initial belief, since the algorithm, after detecting them, does not insert them into the search tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Expansion primitives</head><p>We now detail each of the expansion primitives in the algorithm, in order to provide a clear basis for discussion of the properties of the algorithm. We remark that, for the sake of clarity, we describe here naive implementations of the primitives, disregarding optimizations such as caching of previous results, or lazy evaluation. EXTENDTREE The EXTENDTREE primitive, presented in Fig. <ref type="figure" target="#fig_7">8</ref>, expands a frontier node by every non-predetermined observation and applicable action. For every applicable action, a node and an or-arc are respectively added to the nodes Nodes(ST) and to the arcs Arcs(ST) of the current search tree. Similarly, for every non-predetermined observation, a pair of nodes and an and-arc are added to the current search tree. For every new node n generated, the TAGNEWNODE subroutine evaluates whether it is a success (i.e. BEL(n) ⊆ G), a failure (i.e. some node in the set of ancestors of n, ANCESTORS(n, ST), is associated to the same belief), or neither success nor failure; on the basis of this evaluation, n is associated to the corresponding tag.</p><p>We briefly observe here that in the actual implementation, we will represent states, actions and observations by using state variables, action variables and observations variables respectively. This representation proves convenient under many aspects. First of all, in terms of modeling, it makes it easy to represent parameterized actions, and to modularly describe independent features of the domain state. Second, it makes it possible to implement a lazy approach when extending the tree: rather than assigning every action (observation) variable, which corresponds to selecting a single action (observation), we can assign a subset of them, and generate a node that collects together the outcomes of a set of actions (observations). This idea is particularly simple to realize for observations. More specifically, we repre- sent observations by boolean observation variables, and we apply observations to a node by splitting (in a binary way) on a single observation variable at a time. Since observation variables naturally model sensors, this may be thought of as splitting on one binary sensor at a time. As a result, we introduce nodes where sets of observations (intended to be in disjunction) are evaluated together. In many cases, evaluating a limited set of observation variables is enough to gather a sufficient amount of knowledge, whereas it would be otherwise necessary to test a wide set of observations; in these cases, we effectively reduce the branching in the search and improve its efficiency.</p><formula xml:id="formula_10">1 procedure TAGNEWNODE(n, ST) 2 if BEL(n ) ⊆ G then 3 TAGNODE(n , Success) 4 else if ∃n a ∈ ANCESTORS(n , ST): BEL(n ) = BEL(n a ) then 5 TAGNODE(n , Failure) 6 else 7 TAGNODE(n , Undetermined) 8 fi 9 end 1 procedure EXTENDTREE(n, ST) 2 forall a ∈ A 3 if applicable(BEL(n), a) then 4 n := Exec(BEL(n), a), PATH(n) • a 5 Nodes(ST) := Nodes(ST) ∪ {n } 6 A r c s (ST) := Arcs(ST) ∪ n, a, n 7 TAGNEWNODE(n , ST) 8 fi 9 endfor 10 forall o ∈ O 11 B T := BEL(n) ∩ X -(o) 12 B F := BEL(n) ∩ X -(o) 13 if (B T = ∅) and (B F = ∅) 14 n T := B T , PATH(n) • o 15 n F := B F , PATH(n) • o 16 Nodes(ST) := Nodes(ST) ∪ {n T , n F } 17 Arcs(ST) := Arcs(ST) ∪ n, o, n T , n F 18 TAGNEWNODE(n T , ST) 19 TAGNEWNODE(n F , ST) 20 endfor 21 end</formula><p>While relevant to the performance of the algorithm, we will leave out these representation-dependent implementation details here, in order not to clutter the conceptual representation of the algorithm (intuitively, the main modification consists in having observation variables, rather than observations, considered in the loop at lines 10-20; but this also impacts on the way plans are built by BUILDPLAN). We remark that these representation choices have no impact on the properties of the algorithm. SONSYIELDSUCCESS, SONSYIELDFAILURE The SONSYIELDSUCCESS primitive, see Fig. <ref type="figure" target="#fig_8">9</ref>, returns true on node n iff either (a) there exists an or-arc from n to n 1 in the tree, and n 1 is a success node in the tree, or (b) there exists an and-arc from n to n T and n F in the tree, and both n T and n F are success nodes in the tree. The SONSYIELDFAILURE primitive returns true for a node n if all the actions applied on it lead to failure, and for every observation taken, at least one of the results leads to a failed node.</p><p>Notice that for any non-leaf node n, SONSYIELDFAIL(n) and SONSYIELDSUCC(n) can not hold together. This comes from the fact that, for SONSYIELDSUCC(n) to hold, n must have at least a successful or-son, or a pair of successful brother and-sons; but in that case, it can not be true that every or-son is a failure and that for every pair of brother and-sons, at least one is a failure, so SONSYIELDFAIL(n) can not hold.  </p><formula xml:id="formula_11">1 procedure PROPAGATESUCCONTREE(n, ST) 1 procedure PROPAGATEFAILONTREE(n, ST) 2 if (n = ROOT(ST)) then 2 if (n = ROOT(ST)) then 3 n fat := FATHER(n) 3 n fat := FATHER(n) 4 if (SONSYIELDSUCC(n fat , ST)) then 4 if SONSYIELDFAIL(n fat , ST) then 5 TAGNODE(n fat , Success) 5 TAGNODE(n fat , Failure) 6 PROPAGATESUCCONTREE(n fat , ST) 6 PROPAGATEFAILONTREE(n fat , ST) 7 fi 7 fi 8 fi 8 fi 9 end 9 end</formula><p>Fig. <ref type="figure" target="#fig_29">10</ref>. The procedures to propagate success/failure of a node.</p><formula xml:id="formula_12">1 function BUILDPLAN(n, ST) 2 if BEL(n) ⊆ G then 3 return ε 4 else if ∃ n, a, n ∈ Arcs(ST): ISSUCC(n ) else then 5 pick n, a, n ∈ Arcs(ST): ISSUCC(n ) 6 π := BUILDPLAN(n , ST) 7 return a • π 8 else 9 pick n, o, n T , n F ∈ Arcs(ST): ISSUCC(n T ) ∧ ISSUCC(n F ) 10 π T := BUILDPLAN(n T , ST) 11 π F := BUILDPLAN(n F , ST) 12 return if o then π T else π F )</formula><p>Fig. <ref type="figure" target="#fig_29">11</ref>. The plan building procedure.</p><p>PROPAGATESUCCESSONTREE, PROPAGATEFAILUREONTREE The success and failure propagation routines, see Fig. <ref type="figure" target="#fig_29">10</ref>, are defined recursively, and simply traverse the tree bottom up, from the current node. During this propagation, PROPAGATESUCCONTREE tags a node with Success if its success can be established by SONSYIELDSUCC, while PROPAGATEFAILONTREE tags a node with Failure if its failure can be established by SONSYIELDFAIL.</p><p>BUILDPLAN The BUILDPLAN primitive is reported in Fig. <ref type="figure" target="#fig_29">11</ref>, and is used to build the plan associated to a successful node n. It assumes that n is either a successful leaf, or it has at least a successful or-son, or a pair of successful and-sons, and that the same is valid for its descendants (i.e. every successful descendant is either a leaf, or has a successful or-son, or a pair of successful and-sons). The function implements a top-down recursion scheme over the search tree structure, starting from n, and, under the above assumption, it stops at successful leaf nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Properties of the algorithm</head><p>We now show that the algorithm is correct, complete and terminating. To do so, we first introduce a structural notion of search tree to represent the status of the search at a given iteration of the algorithm, and we observe that the algorithm restricts the search to a class of plans that feature no loops nor predetermined observations; we show that this restriction gives no loss of generality. Finally, we state and prove the termination, correctness and completeness properties. For the sake of readability, the full body of lemmas and proofs are reported in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Search tree</head><p>We define a search tree as the tree resulting from the union of the belief execution trees associated to each plan in a set. This relies on an notion of node equivalence: two nodes are equivalent if they are associated to the same belief, and to the same path. This notion corresponds to the search tree which is expanded by the algorithm at each iteration. In the following, we denote with ST <ref type="bibr">[i]</ref> the search tree at the beginning of the ith iteration of the loop of the planning algorithm in Fig. <ref type="figure" target="#fig_5">6</ref>, i.e. prior to the evaluation of the stop condition at line 6 of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4.3 (Search tree).</head><p>Let {π 1 , . . . , π n } be a set of plans for a domain D, where each plan π i is applicable in a belief B. The search tree associated to the set of plans {π 1 , . . . , π n } is the graph such that:</p><p>-its set of nodes is the collection of every node in BET(π i , B); -its set of arcs is the collection of every arc in BET(π i , B).</p><p>We say that a search tree ST contains a node n if there is a node of ST equivalent to n; a search tree ST contains a belief execution tree T iff it contains every node and arc of T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Non-redundant solutions</head><p>The design of the extension primitives, the mechanism of loop detection and avoidance, and the stopping criteria are such that the algorithm restricts its search to the space of non-redundant plans, i.e. plans that are applicable, loop-free, terminate as soon as the goal is known to be reached, and never consider predetermined observations. We formally define non-redundant solutions, and prove that restricting the search to such class of solutions preserves completeness. Intuitively, a strong solution is non-redundant iff no belief execution along any of its paths may traverse the same belief more than once, and no belief execution along any of its paths may continue after achieving the goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4.4 (Non-redundant plan and solution).</head><p>Let π be an applicable plan for a problem D, I, G . Then π is non-redundant iff the following conditions hold:</p><p>1. For any maximal path p of π , the belief execution associated to p can not traverse two nodes associated to the same belief; 2. For any path p of π , if the belief execution associated to p traverses a node n associated with a belief B ⊆ G, then n is the terminal node of the execution.</p><p>A strong solution π for D, I, G is called a non-redundant solution iff π is non-redundant.</p><p>We observe that any (possibly redundant) plan can be normalized into a unique non-redundant plan by the NORMALIZE procedure reported in Fig. <ref type="figure" target="#fig_10">12</ref>. The following theorem states that restricting the search to non-redundant plans does not affect completeness of the algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Main properties</head><p>We now state the main properties of the algorithm.</p><p>Theorem 4.6 (Termination). Given a planning problem, the execution of the planning algorithm in Fig. <ref type="figure" target="#fig_5">6</ref> terminates.</p><p>Termination is proved by showing that BUILDPLAN terminates, and that the main loop of the algorithm terminates. Intuitively, BUILDPLAN terminates because the generated plans are finite, and each recursion in BUILDPLAN adds either an action or an observation to the produced plan. To prove that the main loop terminates, we prove that each iteration of the main loop terminates, and that the number of such iterations is finite.</p><formula xml:id="formula_13">1 function NORMALIZE(π, B, G) 2 if (π = ε) then 3 return π 4 else if (B ⊆ G) else then 5 return ε 6 else if B ∈ TRAVBELS(BET(π, B)) then 7 pick n B ∈ TRAVERSED(BET(π, B)): BEL(n B ) = B 8 return NORMALIZE(PlanOf (SubTree(BET(π, B), n B ))) 9 else if π = a • π then 10 return a • NORMALIZE(π , Exec(B, a), G) 11 else if (π = if o then π 1 else π 2 ) then 12 if B ∩ X -(o) = ∅ then 13 return NORMALIZE(π 1 , B, G) 14 else if B ∩ X -(o) = ∅ then 15 return NORMALIZE(π 2 , B, G) 16 else 17 π 1 := NORMALIZE(π 1 , B ∩ X -(o), G) 18 π 2 := NORMALIZE(π 2 , B ∩ X -(o), G) 19 return if if o then π 1 else π 2 20 fi 21 end</formula><p>The first statement is simple to prove: the success/failure propagation routines terminate, since they implement bottom-up traversal of a finite tree, and EXTENDTREE terminates, since the number of actions and observations that can be applied to a node is finite.</p><p>To prove that the number of iterations is finite, we observe that in the worst case, the algorithms terminates after having generated every possible acyclic paths, and having detected, for each of them, that it is impossible to further expand them without looping. But this involves a number of iterations bounded by the (finite) number of acyclic paths in the belief space, plus the (finite) number of beliefs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4.7 (Correctness). When the algorithm returns a plan π , then π is a strong solution for the problem.</head><p>The proof relies on the following facts:</p><p>-Success and Failure tags are "persistent" during the search: once a node is tagged with one of those values, it will remain tagged with the same value in any successive iteration of the algorithm; -at a given iteration i, a node n becomes tagged Success only if a strong solution exists, within the search tree</p><formula xml:id="formula_14">ST [i]</formula><p>, that leads from n to the goal; -if a plan exists from a node n to the goal, BUILDPLAN returns one such plan. These statements are valid for every node of ST [i] , at each iteration i of the algorithm; once considered together and instantiated on the root node, correctness follows immediately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4.8 (Completeness). If a strong solution exists, then the algorithm returns a strong solution.</head><p>The proof is performed by reductio ad absurdum: we assume that after a certain number k of iterations, the algorithm returns Failure, while a (non-redundant) solution π to the problem exists.</p><p>But this can not be the case because, would such execution exist, we show that ST <ref type="bibr">[k]</ref> would contain a prefix of the belief execution tree associated with π , and that each node of ST <ref type="bibr">[k]</ref> in that prefix (including the root of ST [k] ) could not be marked Failure, which is necessary for the algorithm to terminate with Failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Symbolic implementation</head><p>In this section, we show how Symbolic Model Checking techniques can be extended to provide an efficient implementation platform for the search paradigm presented in previous section. Model Checking is a formal verification technique based on the exploration of finite state automata <ref type="bibr" target="#b29">[30]</ref>. Symbolic model checking <ref type="bibr" target="#b63">[64]</ref> is a particular form of model checking using Binary Decision Diagrams (BDDS) <ref type="bibr" target="#b24">[25]</ref> to compactly represent and efficiently analyze finite state automata. The introduction of symbolic techniques into model checking led to a breakthrough in the size of models which could be analyzed <ref type="bibr" target="#b3">[4]</ref>, and made it possible for model checking to be routinely applied in industry, especially in the design of semiconductors (see <ref type="bibr" target="#b37">[38]</ref> for a survey).</p><p>In the rest of this section, we will give an introduction to BDDS (Section 5.1), then we will show how BDDS are used to represent planning domains (Section 5.2); we will discuss the extension which allows to symbolically represent belief states and their transformations, thus allowing for an efficient implementation of the algorithm described in previous section (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Binary decision diagrams</head><p>A Reduced Ordered Binary Decision Diagram <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> (improperly called BDD) is a directed acyclic graph (DAG). The terminal nodes are either True or False. Each non-terminal node is associated with a boolean variable, and two BDDS, called left and right branches. Fig. <ref type="figure" target="#fig_11">13</ref>(a) depicts a BDD for (a</p><formula xml:id="formula_15">1 ↔ b 1 ) ∧ (a 2 ↔ b 2 ) ∧ (a 3 ↔ b 3 ).</formula><p>At each non-terminal node, the left [right, respectively] branch is depicted as a solid [dashed, resp.] line, and represents the assignment of the value True [False, resp.] to the corresponding variable. A BDD represents a boolean function. For a given truth assignment to the variables in the BDD, the value of the function is determined by traversing the graph from the root to the leaves, following each branch indicated by the value assigned to the variables. 1 The reached leaf node is labeled with the resulting truth value. If φ is a BDD, its size φ is the number of its nodes. If n is a node, var(n) indicates the variable indexing node n. 1 A path from the root to a leaf can visit nodes associated with a subset of all the variables of the BDD. See for instance the path associated with a 1 , ¬b 1 in Fig. <ref type="figure" target="#fig_29">13(a)</ref>.</p><p>BDDS are a canonical representation of Boolean functions. The canonicity follows by imposing a total order &lt; over the set of variables used to label nodes, such that for any node n and respective non-terminal child m, their variables must be ordered, i.e. var(n) &lt; var(m), and requiring that the BDD contains no isomorphic subgraphs.</p><p>BDDS can be combined with the usual boolean transformations (e.g. negation, conjunction, disjunction). Given two BDDS, for instance, the conjunction operator builds and returns the BDD corresponding to the conjunction of its arguments. Substitution can also be represented as BDD transformations. In the following, if v is a variable, and Φ and ψ are BDDS, we indicate with Φ[v/ψ] the BDD resulting from the substitution of v with ψ in Φ. If v is a vector of BDD variables, we indicate with v the number of elements of the vector. If v 1 and v 2 are vectors of distinct variables such that v 1 = v 2 , we indicate with Φ[v 1 /v 2 ] the parallel substitution in Φ of the variables in vector v 1 with the (corresponding) variables in v 2 .</p><p>BDDS also allow for transformations described as quantifications, in the style of Quantified Boolean Formulae (QBF). QBF is a definitional extension to propositional logic, where propositional variables can be universally and existentially quantified. In QBF, quantifiers can be arbitrarily applied and nested. In general, a QBF formula has an equivalent propositional formula, but the conversion is subject to an exponential blow-up. If Φ is a formula, and v i is one of its variables, the existential quantification of v i in Φ,</p><formula xml:id="formula_16">written ∃v i .Φ(v 1 , . . . , v n ), is equivalent to Φ(v 1 , . . . , v n )[v i /False] ∨ Φ(v 1 , . . . , v n )[v i /True]. Analogously, the universal quantification ∀v i .Φ(v 1 , . . . , v n ) is equivalent to Φ(v 1 , . . . , v n )[v i /False] ∧ Φ(v 1 , . . . , v n )[v i /True].</formula><p>In terms of BDD computations, a quantification corresponds to a transformation mapping the BDD of Φ and the variable v i being quantified into the BDD of the resulting (propositional) formula.</p><p>The time complexity of the algorithm for computing a truth-functional boolean transformation</p><formula xml:id="formula_17">f 1 op f 2 is O( f 1 • f 2 ).</formula><p>As far as quantifications are concerned, the time complexity is quadratic in the size of the BDD being quantified, and linear in the number of variables being quantified, i.e. O( v • f 2 ) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>BDD packages are efficient implementations of such data structures and algorithms (see <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b88">89]</ref>). Basically, a BDD package deals with a single multi-rooted DAG, where each node represents a boolean function. Memory efficiency is obtained by using a "unique table", and by sharing common subgraphs between BDDS. The unique table is used to guarantee that at each time there are no isomorphic subgraphs and no redundant nodes in the multi-rooted DAG. Before creating a new node, the unique table is checked to see if the node is already present, and only if this is not the case a new node is created and stored in the unique table. The unique table allows to perform the equivalence check between two BDDS in constant time (since two equivalent functions always share the same subgraph) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b81">82]</ref>. Time efficiency is obtained by maintaining a "computed table", which keeps track of the results of recently computed transformations, thus using memorization to avoid the re-computation.</p><p>A critical computational factor with BDDS is the order of the variables. (Fig. <ref type="figure" target="#fig_11">13</ref> shows an example of the impact of a change in the variable ordering on the size of a BDD.) For a certain class of boolean functions, the size of the corresponding BDD is exponential in the number of variables for any possible variable ordering <ref type="bibr" target="#b25">[26]</ref>. In many practical cases, however, finding a good variable ordering is rather easy. Beside affecting the memory used to represent a Boolean function, finding a good variable ordering can have a big impact on computation times, since the complexity of the transformation algorithms depends on the size of the operands. Most BDD packages provide heuristic algorithms for finding good variable orderings, which can be called to try to reduce the overall size of the stored BDDS. The reordering algorithms can also be activated dynamically by the package, during a BDD computation, when the total amount of nodes in the package reaches a predefined threshold (dynamic reordering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Symbolic representation of planning domains</head><p>A planning domain D = Σ, O, X , with Σ = S, A, R , can be represented symbolically using BDDS. Let us first focus on the representation of the nondeterministic state transition system of the domain. A set of (distinct) BDD variables, called state variables (or fluents), is devoted to the representation of the states S of the domain. For nonboolean variables, a logarithmic bit encoding is performed as common practice in Symbolic Model Checking. For instance, the robot navigation domain of Fig. <ref type="figure" target="#fig_12">14</ref> is described by means of two fluents x and y, both having range {0, 1, 2, 3}. Each range can be encoded using two boolean variables (x 1,2 and y 1,2 , respectively), with the following relation to the values of the two state variables. x</p><formula xml:id="formula_18">x = 0 → ¬x 1 ∧ ¬x 2 y = 0 → ¬y 1 ∧ ¬y 2</formula><formula xml:id="formula_19">= 1 → ¬x 1 ∧ x 2 y = 1 → ¬y 1 ∧ y 2 x = 2 → x 1 ∧ ¬x 2 y = 2 → y 1 ∧ ¬y 2 x = 3 → x 1 ∧ x 2 y = 3 → y 1 ∧ y 2</formula><p>In the following, we write x for the vector of (BDD variables representing the) state variables of the domain. Because the particular order is irrelevant (but for performance issues), in the rest of this section we will not distinguish between a proposition and the corresponding BDD representation.</p><p>A state is a complete set of assignments to state variables. For each state s, there is a corresponding assignment to the state variables x, i.e. the assignment where each variable corresponding to a proposition p holding in s is assigned to True, and each other variable is assigned to False. We represent s with the BDD ξ(s), having such an assignment as its unique satisfying assignment. For instance, the BDD for the state (0, 2), written ξ((0, 2)), is (¬x</p><formula xml:id="formula_20">1 ∧ ¬x 2 ) ∧ (y 1 ∧ ¬y 2 ).</formula><p>The above representation naturally extends to any set of states Q ⊆ S as follows:</p><formula xml:id="formula_21">ξ(Q) = s∈Q ξ(s)</formula><p>In other words, we associate a set of states with the generalized disjunction of the BDDS representing each of the states. Notice that the satisfying assignments of the ξ(Q) are exactly the assignments representing the states in Q. This representation mechanism is very natural. For instance, the BDD ξ(I) representing the set of initial states I = {(0, 1), (0, 2)} of the robot navigation domain of Fig. <ref type="figure" target="#fig_29">14 is</ref>:</p><formula xml:id="formula_22">ξ(I) = (¬x 1 ∧ ¬x 2 ) ∧ (¬y 1 ∧ y 2 ) ∨ (y 1 ∧ ¬y 2 )</formula><p>while for the set of goal states G = {(3, 1), (3, 2)} the corresponding BDD is</p><formula xml:id="formula_23">ξ(G) = (x 1 ∧ x 2 ) ∧ (¬y 1 ∧ y 2 ) ∨ (y 1 ∧ ¬y 2 )</formula><p>A BDD is also used to represent the set S of all the states of the planning domain. In the robot navigation example we are considering, ξ(S) = True, since every assignment to x corresponds to a state in S.</p><p>In general, a BDD represents the set of (states which correspond to) its models. As a consequence, set theoretic transformations are naturally represented by propositional operations, as follows.</p><formula xml:id="formula_24">ξ(S \ Q) = ξ(S) ∧ ¬ξ(Q) ξ(Q 1 ∪ Q 2 ) = ξ(Q 1 ) ∨ ξ(Q 2 ) ξ(Q 1 ∩ Q 2 ) = ξ(Q 1 ) ∧ ξ(Q 2 )</formula><p>The main efficiency of this symbolic representation lies in the fact that the cardinality of the represented set is not directly related to the size of the BDD. For instance, ξ([[x = 0]]) = ξ({(0, 0), (0, 1), (0, 2), (0, 3)}) = ¬x 1 ∧ ¬x 2 uses two (non-terminal) nodes to represent four states (see Fig. <ref type="figure" target="#fig_29">17</ref>), while the set of states where state variable y has a value in the set {1, 2} is represented with three nonterminal nodes. ξ(I) uses five nonterminal nodes to represent two states. As limit cases, for this example ξ(S) and ξ(∅) are (the leaf BDDS) True and False, respectively. Indeed, symbolic representation is extremely efficient in dealing with irrelevant information. Notice, for instance, that only the boolean variable</p><formula xml:id="formula_25">x 1 occurs in ξ([[x ∈ {2, 3}]]) = ξ({(2, {0, 1, 2, 3}), (3, {0, 1, 2, 3})}) = x 1 .</formula><p>For this reason, a symbolic representation can have dramatic advantages over an explicit, enumerative representation. This is what allows symbolic, BDD-based model checkers to handle finite state automata with a very large number of states (see for instance <ref type="bibr" target="#b3">[4]</ref>). In the following, we will collapse a set of states and the BDD representing it.</p><p>To represent actions, we use another set of BDD variables, called action variables, distinct from state variables, and denoted a in the following. Under the assumption of sequentiality of the system, i.e. that no concurrent actions are allowed, it is possible to encode actions into action variables using a logarithmic schema, similarly to what we did for state variables. This schema is actually used in our implementation. However, for the sake of simplicity, here we refer to an encoding where each possible action in A is associated with one action variable in a. Intuitively, a BDD action variable is true if and only if the corresponding action is being executed. We use a BDD, SEQ(a), to express that exactly one of the action variables must be true at each time. For the robot navigation problem, where A contains four actions, we use the four BDD variables GoNorth, GoSouth, GoWest and GoEast, while we express the serial encoding constraint with the following BDD:</p><formula xml:id="formula_26">SEQ(a) = (GoNorth ∨ GoSouth ∨ GoWest ∨ GoEast) ∧ ¬(GoNorth ∧ GoSouth) ∧ ¬(GoNorth ∧ GoWest) ∧ ¬(GoNorth ∧ GoEast) ∧ ¬(GoSouth ∧ GoWest) ∧ ¬(GoSouth ∧ GoEast) ∧ ¬(GoWest ∧ GoEast).</formula><p>Similarly to the case of state variables, we are referring to BDD action variables with symbolic names for the sake of simplicity. The position of the BDD action variables in the ordering of the BDD package is irrelevant from the logical point of view.</p><p>To represent the transition function, another vector x of BDD variables, called next state variables, is allocated in the BDD package; these variables are used to represent the states resulting from transitions triggered by actions. We write ξ (s) for the representation of the state s in the next state variables. With ξ (Q) we denote the construction of the BDD corresponding to the set of states Q, using each variable in the next state vector x instead of each current state variables x. We require that x = x , and assume that variables in equal position in x and in x correspond. We define the representation of a set of states in the next variables as ξ (s) = ξ(s)[x/x ]. We call the operation Φ[x/x ] "forward shifting", because it transforms the representation of a set of "current" states in the representation of a set of "next" states. The dual operation Φ[x /x] is called "backward shifting". In the following, we call x current state variables to distinguish them from next state variables x . A transition is a 3-tuple composed of a state (the initial state of the transition), an action (the action being executed), and a state (the resulting state of the transition); thus, it is represented as an assignment to x, a and x . For the robot navigation domain, the transition corresponding to the application of action GoEast in state (0, 0), resulting in state (1, 0), is represented by the following BDD ξ (0, 0), GoEast,</p><formula xml:id="formula_27">(1, 0) = ξ((0, 0)) ∧ GoEast ∧ ξ ((1, 0))</formula><p>The transition function R of the automaton corresponding to a planning domain is simply a set of transitions, and is thus represented by a BDD R(x, a, x ) in the BDD variables x, a and x , where each satisfying assignment represents a possible transition:</p><formula xml:id="formula_28">ξ(R) = SEQ(a) ∧ t∈R ξ(t)</formula><p>The APPL(x, a) BDD representing the applicability relation can be directly obtained with the following computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPL(x, a) = ∃x .R(x, a, x )</head><p>The BDD representing the states reachable from Q by executing an action a ∈ A is directly obtained by means of the following computation (where we write Q(x) to stress the fact that the BDD representing Q is over the state variables in x):</p><formula xml:id="formula_29">Exec(a, Q(x)) = ∃x.∃a. R(x, a, x ) ∧ Q(x) ∧ ξ(a) [x /x].</formula><p>The BDD representing the states reachable from Q in one step is obtained with the following computation:</p><formula xml:id="formula_30">∃x.∃a. R(x, a, x ) ∧ Q(x) [x /x].</formula><p>Let us consider now the symbolic encoding of observations. We introduce a new set y of BDD variables, called observation variables, distinct from state and action variables. Similarly to what we have shown for state variables, we associate each observation o ∈ O to a complete assignment to observation variables ξ(o), that is a BDD in the variables y. A set of observations {o 1 , . . . , o n } is associated to the</p><formula xml:id="formula_31">BDD ξ(o 1 ) ∨ • • • ∨ ξ(o n ).</formula><p>Example 5.1. In a 4 × 4 robot navigation grid, each observation corresponds to a wall configuration around the robot, for a total-in principle-of sixteen combinations. (In the specific case of the domain in Fig. <ref type="figure" target="#fig_12">14</ref>, twelve observations can actually occur: nowall, e, s, ne, nw, sw, se, ew, ns, nsw, nwe, swe, with the obvious interpretation that nsw corresponds to a wall north, a wall south, and a wall west, and similarly for the others.) The observation set can be represented by means of 4 observation variables WALLW, WALLE, WALLN, and WALLS; each of these is intended to be true if wall is perceived in one of the directions. The correspondence between observations and assignments to observation variables is straightforward: for instance, ξ(nsw) is the (complete) assignment WALLN ∧ WALLS ∧ WALLW ∧ ¬WALLE. Sets of observations can be represented similarly: ξ({nsw, ns}) is the partial assignment identified by the BDD WALLN ∧ WALLS ∧ ¬WALLE. We notice again the impact of the symbolic representation, that enables for the manipulation of sets of observations. For instance, the WALLN BDD represents the set of eight observations {n, ns, nw, ne, nsw, nse, nwe, nswe}, while ¬WALLN represents the complement set of observations {s, w, e, sw, se, we, swe, nowall}.</p><p>For the symbolic representation of the observation function, we consider that X is a function over O × S, and it is therefore possible to proceed as in the case of the transition function. We represent each state-observation pair s, o with ξ(s) ∧ ξ(o), that is a BDD in the x and y variables. The representation of the observation function is therefore the</p><formula xml:id="formula_32">BDD ξ(X ) = s,o ∈X ξ(s) ∧ ξ(o)</formula><p>Rather than a monolithic representation of the observation function, we adopt a partitioned approach, and we construct, for each observation variable y, the two sets of states X -(y) (the states compatible with y), and X -(y) (the states compatible with ¬y). This can be done by means of the following symbolic operations:</p><formula xml:id="formula_33">ξ X -(y) = ∃y. ξ(X ) ∧ y ξ X -(y) = ∃y. ξ(X ) ∧ ¬y</formula><p>Notice that, when no noisy sensing is present, there is no need to introduce two distinct X -(y) and X -(y), since X -(y) = S \ X -(y). However, in the case of noisy sensing, for some observation variable y, X -(y) ∩ X -(y) may be nonempty.</p><p>Example 5.2. Fig. <ref type="figure" target="#fig_13">15</ref> depicts the information associated to sensors for the domain in Fig. <ref type="figure" target="#fig_12">14</ref>. In the first row, each of the pictures shows the information associated to each of the observation variables WALLN, WALLS, WALLW and WALLE. (A triangle is meant to represent the "echo" of a sensor in one of the directions.) For the first three variables, sensing is not noisy, and a partition of the state space is induced: for instance, X -(WALLW) is the set of all cells without the triangle, while X -(WALLW) is the complement set S \ X -(WALLW) containing the cells with the triangle. In the case of WALLE (fourth picture), we assume noisy sensing in the four central cells; grey triangles represents the overlapping between X -(WALLE) and X -(WALLE):</p><formula xml:id="formula_34">X -(WALLE) ∩ X -(WALLE) = {(1, 1), (2, 2), (2, 1), (1, 2)}.</formula><p>The rightmost picture depicts the case where the observation WALLE is not noisy, and thus a partition of the state space is induced as for the other three observations.</p><p>The second row shows the information associated to pairs of observation variables (assuming WALLE to be noisy). Each of the shapes in a cell corresponds to a different assignment to observation variables. For instance, in the leftmost picture, we see that WALLN and WALLS partition the state space in the four different classes {(0, 0), (2, 0), (2, 3), (2, 3)}, {(1, 0), (3, 0)}, {(1, 2), (2, 2), (0, 3), (3, 3)}, {(1, 1), (3, 1), (0, 2), (3, 2)}. In the rightmost picture, we see that WALLE and WALLW identify the following sets {(0, 0), (2, 0), (0, 1), (2, 1), (1, 2), (1, 3)}, {(1, 0), (3, 0), (1, 1), (2, 2), (3, 2), (3, 3)}, {(2, 1), (3, 1), (0, 2), (1, 2), (0, 3)}, and {(2, 3), (1, 1), (2, 2)}. We notice that, due to noisy sensing, some classes overlap.</p><p>The third row shows the information associated to triples of observation variables. Finally, the information associated to the combination of all the observation variables is depicted in the last row, in the case of WALLE being noisy (left), and not noisy (right). We see that some states are indistinguishable (e.g. (0, 0), (2, 0) and (1, 3), or (3, 1) and (0, 2), or (1, 0) and (3, 0)) even in the more informative case of non-noisy sensing (right). We also see that some states (e.g. (2, 1), (1, 2), and (2, 2)) can be uniquely identified, but this is no longer the case when noisy sensing is considered (left): for instance, (2, 2) can no longer be distinguished from (3, 3), and (1, 2) can not be distinguished from (0, 3).</p><p>In the rest of this paper, we assume that the BDD representation of a planning problem is given. In particular, we assume as given the vectors of variables x, x , a and y, the encoding functions ξ and ξ , and we simply call S, R, X , I and G the BDD representing the states of the domain, the transition functions, the observation function, the initial states and the goal states, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Symbolic search in the belief space</head><p>In symbolic model checking, BDDS provide a way for compactly representing and efficiently expanding the search frontier for the exploration of the space of states. This machinery is used, with small variations, in <ref type="bibr" target="#b36">[37]</ref>, where conditional plans are constructed by symbolic breadth-first search in the space of states under the hypothesis of full observability. However, this machinery can not be applied to tackle planning under partial observability. Our approach is rather based on the fact that a belief state Bs ⊆ S is directly represented by the corresponding BDD Bs(x). The basic step of the algorithms is the expansion of a belief state, that results in a list of newly expanded nodes.</p><p>Given a belief state Bs, FWDEXPANDBS(Bs) computes the set of belief state action pairs Bs i , a i such that a i is applicable in Bs and Exec(Bs, a i ) = Bs i . This computation can be performed symbolically as follows:</p><formula xml:id="formula_35">FWDEXPANDBS Bs(x) = ∃x. Bs(x) ∧ R(x, a, x ) ∧ ∀x. Bs(x) → APPL(x, a) [x /x].</formula><p>The result is a BDD, in variables x and a, whose satisfying assignments represent the belief state action pairs Bs i , a .</p><p>To apply observations starting from a belief B(x), we exploit the representation of observations in terms of the observation variables y, and the BDD-based representation of the states X -(y), X -(y).</p><p>To restrict B(x) to the states compatible with a given value of the observation variable y, it is enough to conjoin it with the BDD representing X -(y) or X -(y). To restrict it to the states compatible with an observation, it is enough to conjoin it with the BDDS associated to the assignments of the observation variables for that observation.</p><p>Notice that the symbolic representation helps us to reduce the branching associated to observations. In fact, rather than extending the plan with a decision on the occurrence of an observation, we consider the partition induced by the value of observation variables. We use the following EXPANDOBSVAR primitive, that returns a pair of beliefs, given a starting belief:</p><formula xml:id="formula_36">EXPANDOBSVAR Bs(x), y = Bs(x) ∩ X -(y), Bs(x) ∩ X -(y) .</formula><p>Considering plans which branch on the value of an observation variable amounts to branching on sets of observations: the plan if y then π 1 else π 2 can be seen as a (much more compact) representation of the plan if o 1 then π 1 else if o 2 then π 1 else . . . if o n then π 1 else π 2 , where o i are the observations compatible with y. In fact, the effect of this representation is even more striking when we consider that it represents all the plans of the above form, for different orderings of the o i . Notice also that this representation is complete, i.e. every plan branching on an observation if o then π 1 else π 2 can be represented by a plan branching on observation variables.</p><p>Example 5.3. Consider the robot navigation in Fig. <ref type="figure" target="#fig_29">16</ref>, where the initial states are {(1, 2), (1, 3), (2, 2), (2, 3)} and the goal is</p><formula xml:id="formula_37">{(3, 1), (3, 2)}. Let us call π 2 = GOEAST • GONORTH and π 1 = GOEAST • GOEAST • GONORTH.</formula><p>Then the plan if WALLW then π 1 else π 2 is a strong solution for the problem. Since WALLW corresponds to the set of observations {nw, sw, ew, nsw, nwe, swe} the corresponding plan in terms of observations is significantly more complex:</p><formula xml:id="formula_38">if nw then π 1 else if sw then π 1 else . . . if nwe then π 1 else if swe then π 1 else π 2</formula><p>In terms of the implementation, we exploit BDD technology in a fundamental way, to directly link the symbolic machinery with the search framework. Given the canonical form of BDDS, a belief state is simply a pointer to the unique corresponding BDD in the BDD package (see Fig. <ref type="figure" target="#fig_29">17</ref>). The (BDD pointers representing) beliefs generated during the search are stored by means of a hash table, external to the BDD package; this is accessed directly using BDD pointers as keys. In this way, it is very easy to associate additional information to belief states (e.g., the corresponding plan, heuristic information, or tag values such as those described in Section 4). There are several algorithm phases that benefit of these features, in our implementation. In particular, we observe that a solution for a given search node n can be reused for any node associated to the same belief of n. Since equivalence between beliefs represented as BDDS can be performed in constant time, reuse of subplans is a simple and effective optimization that is applied Fig. <ref type="figure" target="#fig_29">16</ref>. A simple nondeterministic robot navigation domain. when expanding a node, and when propagating bottom-up the success of a node. An additional optimization can be applied by considering that a solution for a node n is also a solution for any node associated to a belief entailed by that of n, and exploiting the effective implementation of the BDD primitive for entailment check. Moreover, in certain cases it is possible to effectively establish the failure of a node based on the failure of belief-equivalent nodes, or of nodes associated to entailed beliefs (by also comparing the paths associated to the nodes). Although these additional optimizations are present in our implementation, their effectiveness may vary depending on the domains, and for the purposes of this paper, only the reuse of solution over belief-equivalent nodes is considered.</p><p>The symbolic machinery is also crucial to effectively transform a BDD B (x, a) in the state and action variables, representing the result of an expansion of a belief B(x), into a list of pairs, each pair coupling one action (a BDD in the variables a) to the belief state (a BDD in x) resulting from the expansion of B via that action. This operation is necessary for the planning algorithm to deal separately with the actions and their results, in order to construct the tree representing the portion of search space being visited. This is efficiently performed by the EXTRACTBSACTIONPAIRS routine depicted in Fig. <ref type="figure" target="#fig_29">18</ref>, recursively traversing the action variables in B (x, a) and identifying the BDDS in the x variables associated to the leaves of such traversal.</p><p>For the sake of clarity, let us assume that action variables precede state variables in the BDD ordering, and let us not consider observation and next state variables. The routine interprets as a belief state every node associated with a state variable, having a parent node associated with an action variable, and associates to it the action traversed so far, i.e. the set of assignments to the action variables accumulated during the traversal (in the top level call, this assignment must be set as true). If the level of state variables has not been reached yet, the routine recurs on the "if" and "then" branches of the BDD, accumulating the respective assignments to the current action variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Search strategies</head><p>It is well known that different search strategies may have a dramatic impact on the effectiveness of the search. In this section, we discuss several possible search strategies for the algorithm presented in Section 4. We remark that the algorithm retains all the properties (termination, correctness and completeness) for all the strategies that correctly implement the EXTRACTNODEFROMFRONTIER primitive. Also, we will not focus on strategies that guarantee optimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Basic strategies</head><p>The first strategy we consider is Depth-First Search, which be easily implemented within our framework by means of a stack-based management of the list of open nodes. In this way, we can simulate the algorithm presented in <ref type="bibr" target="#b8">[9]</ref>. The converse is not possible, since the algorithm in <ref type="bibr" target="#b8">[9]</ref> implements a depth-first strategy in a hardcoded manner, and can not simulate other search styles. We consider two variants of this strategy: in the first, FIX, the order in which the effect of actions and observations are expanded is fixed throughout the search. <ref type="foot" target="#foot_0">2</ref> In the second, RND, the choice is randomized by shuffling the results of the expansion primitives. The main problem of these strategies is that, because of a bad choice, the search may get trapped into a dead-end, i.e. a portion of the search space from where the goal is unreachable.</p><p>A possible alternative is to consider some heuristic measure to decide which node should be expanded next. Such an attempt was made in an earlier version of this work <ref type="bibr" target="#b6">[7]</ref>, where the node to be expanded is selected, among all the open nodes, according to the following function: higher score is given to nodes with (i) highest number of open nodes with the same belief state, and (ii) least number of failed nodes with the same belief state. This heuristic turned out to be not effective for at least two reasons. First, it does not take into account any distance to the goal. Second, it disregards structural information (e.g. the depth of the nodes, or the number of open nodes for the plan associated to the node). In the rest of this section, we present some new, more advanced search strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Strategies based on strong distance</head><p>The Strong Distance heuristics (SD in the following), proposed in <ref type="bibr" target="#b35">[36]</ref> in the setting of conformant planning, is a way to estimate the distance between a belief state and the goal in a nondeterministic domain. Intuitively, the SD heuristics is based on the relaxation of the problem to the case of full observability: for each individual state in the belief state, we denote with SD(s) the shortest worst-case (for all possible nondeterministic action effects) distance to the goal; for a belief B, we write SD(B) for max s∈B SD(s).</p><p>In the first strategy based on Strong Distance, called SDPURE, we select, among all open nodes in the frontier, the one with lowest strong distance to the goal. We call this strategy SDPURE, since it resembles "pure" search, where the cost of reaching the node is not taken into account.</p><p>With SDPURE, the search does not guarantee that a given plan is expanded until its failure or success. This can be often undesirable: for instance, after a plan branch resulting from an observation is discovered to be successful, this strategy may decide to expand a branch of a different plan prefix, wasting the previous effort. With the strategy SD, we select the best node (according to SD) among the ones that have been produced by the last expansion step.</p><p>Although the SD heuristic may be very informative, it may still be too coarse an approximation for a search in the belief states. In fact, it disregards the amount of knowledge associated to the search nodes. Thus, the search greedily proceeds towards the goal, even when the necessary degree of knowledge has not been gathered. This is the case, for instance, when a fluent f has several values in a belief state, but only a specific value in the goal. (See <ref type="bibr" target="#b35">[36]</ref> for a more thorough discussion.) Therefore, we propose the SDOBS strategy, that is a further refinement of SD, where we favour information gathering by giving priority to nodes resulting from observations over nodes resulting from actions. Notice also that one action can always be postponed to observing, while relevant information can be no longer available after acting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Strategies based on fluent distance</head><p>The exact computation of the SD function can be a serious bottleneck in the case of large domains, even when exploiting symbolic techniques (see <ref type="bibr" target="#b36">[37]</ref> for the details). For this reason, we propose the FD strategy, that is basically a modification of SDOBS, where Strong Distance is replaced by another function, the Fluent Distance, that is often much simpler to compute. Basically, the Fluent Distance of a belief state from the goal is the sum, over each of the fluents f in the domain, of its f -distance to the goal.</p><p>We say that v is a possible value for a fluent f in a belief state B if f has value v in some state s belonging in B. Then, the f -distance for B to G is defined as follows: if the number of possible values of f in G is less than the number of values of f in B, <ref type="foot" target="#foot_1">3</ref> then f -distance for B to G is set to a high, threshold value M f . Otherwise, the distance is based on the following construction. Let LB[•] be the sequence of sets of values for f , such that LB[0] is the set of values of f in G, and</p><formula xml:id="formula_39">LB[i + 1] is the union of LB[i] with the values of f in the preimage of LB[i]. Then, the f -distance of B from G is the least i such that LB[i] contains all the possible values of f in B.</formula><p>Some remarks are in order. We notice that the iteration is guaranteed to terminate, since the range of values for f is finite. The threshold value M f is actually the number of iterations needed to reach a fix point. The sequence of LB layers can be computed by means of symbolic operations; in particular, the range of f in B is basically the results of the quantification in B of all the fluents but f . The computation is much easier than the strong distance, since the fluents are analyzed independently, and the accumulated BDDS contain a much lower number of variables. The layers can be computed once and for all before the search is started.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Strategies based on dynamic target selection</head><p>The last strategy we propose, called FDDT, is an extension of FD based on the idea of Dynamic Target selection. The idea is to "guess" a subset of G as a current target, say t i , and to evaluate each node based on the fluent distance to t i rather than to G. Given that the range of each fluent in t i is likely to be smaller than in G, we hope that the corresponding fluent distance is somewhat more informative.</p><p>The search is an enforced hill climbing on the selected target: once a node with distance N is expanded, the next node to be expanded will one of its sons that has either distance which is lower or equal to N . When it is impossible to decrease the distance, a new target is guessed, based on the idea that the selected target may not be informative enough.</p><p>In order to mitigate the risk of a bad target, we exploit the idea of invariant fluents. Intuitively, a fluent is invariant in a domain if its value can not be modified by any action. <ref type="foot" target="#foot_2">4</ref> Invariant fluents are often necessary to model hidden structural information (e.g. the status of a fault variable in a plant). A fluent f can be detected to be invariant by means of simple symbolic operations. <ref type="foot" target="#foot_3">5</ref>Once invariants have been discovered, any selected target is a subset of the goal containing a selected invariant configurations that is possible in the current belief. The use of invariants also provides a sufficient condition for a belief state B to be unsolvable. This is the case when B contains a configuration of invariants which is not present in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental evaluation</head><p>In this section, we experimentally evaluate the algorithm and the search strategies proposed in previous sections. We do this by reporting two sets of experiments. In the first, we show the comparison between the different heuristics, both in terms of nodes and in terms of run-times. In the second, we compare the run times of our planner MBP with a selected search strategy and two other competitor systems.</p><p>Both experiments are on a set of over 700 problems, that include most 6 of the established benchmark domains for planning under partial observability. All the experiments were run on a Pentium III Xeon 700 MHz with 6 GB RAM running Linux, fixing a memory limit of 512 MB. The MBP executable and all the models used for the experiments can be found at http://sra.itc.it/tools/mbp/AIJ_PO.tar.gz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">The MBP planner</head><p>The algorithm and the search strategies described in previous sections have been all implemented within the MBP planner <ref type="bibr" target="#b4">[5]</ref>. MBP is a system for planning in nondeterministic domains, based on the planning via symbolic model checking paradigm. MBP is implemented on top of the NUSMV symbolic model checker <ref type="bibr" target="#b28">[29]</ref>, from which it inherits the language and the machinery to describe and explore finite-state nondeterministic systems. The language of NuSMV has been extended with constructs to specify observation variables as boolean relations. In addition, MBP is extended with a symbolic representation of beliefs based on the use of BDDS.</p><p>MBP integrates capabilities for plan generation, plan simulation and plan verification for a variety of classes of planning problems. These include strong planning for planning under full, partial or null observability <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37]</ref>; strong cyclic planning under full observability <ref type="bibr" target="#b33">[34]</ref>; planning for temporal and intentional goals under full observability <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b75">76]</ref>. For most of these problems, more than one algorithm is available, e.g. exploiting regression vs. progression techniques. MBP is also the basis for the reactive planning platforms described in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">The test domains</head><p>We considered several test domains. Most of them are parameterized, in order to show the asymptotic behaviour of the different search strategies and systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1.">Empty room</head><p>This is a robot navigation domain consisting of an empty square room of size N × N , surrounded by border walls. A robot may start from any position in the room, and has to reach the center of the room. The robot is equipped with four wall proximity sensors, one for each direction, and may be moved in the four directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2.">Maze</head><p>The maze domain we consider is taken from <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b85">86]</ref> and adds some complexity to the empty room domain, by having walls also inside the room, while maintaining the same actions and sensing scheme. Considering connected mazes whose topmost left corner is reachable, the problem is again that of positioning the robot in that corner, given that its initial position is unknown.</p><p>For this domain, we consider mazes of increasing size; for each size, we consider a set of 16 randomly generated mazes, differing for the way the walls are positioned and density of the walls in the room.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3.">Ring</head><p>The ring domain, first described in <ref type="bibr" target="#b34">[35]</ref>, is a different navigation domain, where a ring of N rooms can be traversed by a robot. Each room features a window, which can be either open, closed or locked; the robot can observe the status of the window in its current room, and close it (if it is open) or lock it (if it is closed). Initially, the windows can be in any state, and the goal is to have all of them locked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.4.">Bomb in the toilet</head><p>This domain follows the formulation presented in <ref type="bibr" target="#b87">[88]</ref>: the domain consists of a number of toilets, and a number of packages. Packages may contain bombs that can only be disarmed by dunking them into the toilets. Once a package is dunked into a toilet, the toilet gets clogged, and no packages can be further dunked into it prior to flushing (which 6 The benchmark domains were chosen to be able to compare all systems on every domain.</p><p>unclogs the toilet). A sensing action, "sniff package", is available to tell whether a certain package contains the bomb. The goal is to disarm the bomb that is contained in one of the packages, its position being initially unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.5.">Medical</head><p>In the medical domain <ref type="bibr" target="#b87">[88]</ref>, the goal is to heal a patient that suffers from one of many diseases. For each disease, a curing action is available; but if a wrong cure is applied, it causes the death of the patient. A number of boolean sensing actions ("medical exams") are also available, whose results split the possible diagnoses in two subsets. Notice that solutions to this problem basically consist in first identifying a perfect diagnosis, that is fully resolving the state uncertainty, and only then applying a single curing action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.6.">Power supply restoration</head><p>The Power Supply Restoration domain was first presented in <ref type="bibr" target="#b83">[84]</ref>, and then tackled in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. A network is composed of electrical lines, connected by switch devices in a generic topology; some open line end is fed by power engines through breakers. When a short-circuit (fault) takes place on some line, the breaker feeding that line opens to protect the power engine; notice that in doing so, also other non-faulty lines may be left without power. Faults can not be directly observed; however, each switch and breaker is equipped with a directional sensor that signals whether there has been a fault "downstream" the power flow, since the last time the device has been traversed by power. Switches and breakers can be operated by opening and closing them. The goal is that of feeding every line that can be fed; this can be achieved under the proviso that devices and sensors are reliable.</p><p>We experiment for a fixed, medium size network, consisting of 6 lines, 3 breakers and 7 switches (see <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>). We report results for two different kinds of problems: when the number of faults in the network is stated to be equal to a given number (denoted with PSR Eq), and when it is lower than or equal to a given number (denoted with PSR Leq).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Comparison between search strategies 7.3.1. Empty room</head><p>The results for this set of tests are shown in Fig. <ref type="figure" target="#fig_15">19</ref>. We observe that the FDDT and FD strategies are the ones that work best, and perform similarly. FIX is also able to scale up reasonably well, while the others performs rather poorly.</p><p>As expected for SDPURE and SD, strong distance alone may be a very misleading measure for search in the belief space: the search repeatedly directs the robot to the center of the room before its position has been actually localized, which leads to a large number of long backtracking instances. FDDT and FD behave well, since they demote nodes with high uncertainty, and try to gather as much knowledge as possible by means of observations before aiming at the goal position.</p><p>It is interesting to compare the behavior of FIX and RND: in this domain, the fixed 'preference order' among actions used by FIX seems to result in a systematic visit of the room, while RND results in complex, "curly" trajectories that fail because of loopbacks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2.">Maze</head><p>The results are presented in Fig. <ref type="figure" target="#fig_16">20</ref>. In order to be able to move the robot satisfying the action preconditions, the search often ends up performing enough observations to localize the robot position. In fact, most of the search time seems to be spent in the generation of the first localization subplans. Once this is done, the success propagation mechanism works in such a way that increasingly large areas of the maze are marked as solved-and once a few paths in the maze are detected as successful, there is a high chance that, from anywhere in the maze, a few moves lead to a "successful" cell.</p><p>The problem has a rather constrained nature: undirected search such as FIX and RND scale up well, similarly to the more informed FD and FDDT. We also notice a difference in performance, since FD is heavier to compute than FDDT.</p><p>Strategies based on Strong Distance are less effective in terms of expanded nodes, and also heavier to compute. The difference between SDPURE and SD is due to the fact that SDPURE analyses the whole search frontier, and may decide to expand branches of unrelated plans, therefore delaying the effective identification of a solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3.">Ring</head><p>The results for this comparison are shown in Fig. <ref type="figure" target="#fig_17">21</ref>. The most effective search is obtained by the most informed FD and FDDT: they tend to gather knowledge as much as possible, their search seems to be directed well enough to achieve a good performance, expanding a limited number of nodes.</p><p>FIX is also surprisingly effective: using a fixed preference order in FIX leads to a systematic traversal of the state space, which turns out to be convenient to avoid long loops. The randomized RND performs very poorly, since extremely long belief sequences may be traversed before encountering a loop, or meeting the goal. SDPURE is much  In terms of CPU time, we notice that the computation of strong distances in SDPURE and SD is rather expensivethis explains the relative degrade for SDPURE. Again, we see that FDDT is less expensive than FD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.4.">Bomb in the toilet</head><p>The results for this comparison are shown in Fig. <ref type="figure" target="#fig_18">22</ref> (we report the results for the case with 20 toilets; the other cases are qualitatively similar). We see that the behavior in this domain is also rather constrained, and all the search strategies behave reasonably well. The only exception is SDPURE, which analyses the whole frontier at each step for selecting a node to expand. Given the little information provided by strong distance, this results in a breadth-first search, with a strong degrade in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.5.">Medical</head><p>The results for this comparison are shown in Fig. <ref type="figure" target="#fig_19">23</ref>. For this domain, FDDT and FD are substantially superior to all the others, for their ability to gather knowledge, and to approximate distances. In this problem, it is possible to somministrate medicines even after the patient is dead. This makes a large portion of the reachable belief state to be unsolvable. This is a serious problem for RND and FIX, which often end up in these dead-end areas. SD does not scale up well, since the relaxation of the problem to full observability provides little guidance; however, thanks to the backward computation of strong distance, it is possible to detect unsolvable belief states, from which the advantage over RND and FIX. SDPURE is significantly better than SD in terms of expanded nodes, but the evaluation of all the open nodes results in a strong penalty in terms of CPU time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.6.">Power supply restoration</head><p>The results for this comparison are shown in Fig. <ref type="figure" target="#fig_20">24</ref>. In this domain, the solution for these problems requires setting the switches in configurations that depend on the (initially unknown) fault position. Therefore, it is important to gather knowledge before attempting to restore the network. For this reason, strong distance alone does not seem to drive the search effectively.</p><p>Furthermore, such initial conditions are likely to feature a mix of open and closed switches, and using a fixed action preference leads as in FIXleads to aiming at "all-open" or "all-closed" configurations, which are unlikely to be satisfactory. This causes a large number of computationally expensive backtracking instances. Instead, the randomized search RND scale up reasonably well. FD and FDDT are also very effective. The dynamic target mechanism of FDDT basically partitions the problem, and is able to exploit distance-directed search in each subproblem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.7.">General remarks</head><p>It is clear from the experiments that the effectiveness of the search in MBP strongly depends on the implemented search strategy. In particular, uninformed DFS search may perform reasonably well either where the domain is rather constrained (such as e.g. the Maze domain), or where a large set of solutions exist (such as e.g. the Empty Room, the Ring or the Bomb in the Toilet domains). For some domains, such as e.g. the Empty Room, choosing a fixed preference order amongst actions can lead to better performance than randomizing, since this may prevent forming long self-loops. On the other side, using distance alone is not enough in a setting where the degree of uncertainty plays a key role; as such, the Strong Distance Heuristics fail to scale up in most cases. Heuristics which favour knowledge gathering, and combine this idea with the usage of approximated distance measures, seem to work better, especially when problems can be partitioned by detecting selected targets inside the goal (like in the PSR domain).</p><p>Additional considerations can be drawn on the structure of plans. For domains such as the Maze, featuring many constraints on action executability and a rather rich sensing, knowledge gathering is (easily) solved in the first plan steps, and thus plans feature a limited branching factor. Plans found for 51 × 51 mazes feature around 500 branches, and an average depth of 300 actions. In other domains, such as Medical, the core of the problem consists in knowledge gathering; in these cases, solution plans are shallow and feature a bigger branching factor. For instance, plans in the Medical domains are only 3 actions deep, but feature a number of branches equal to the number of possible illnesses in the problem.</p><p>Also, we remark that the success reuse mechanism in the search algorithm induces an internal plan representation as a DAG, rather than a tree. The effect of this is evident in domains such as Empty Room, Maze, or Ring, where the number of nodes of the internal plan representation is only a small portion of what would be if a tree representation was adopted instead. For instance, the FDDT-generated DAGs for plans for a 51 × 51 feature around 2500 nodes, while their correspondent trees would feature over 5 × 10 5 nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Competitor planners</head><p>We compared MBP using FDDT with the two most advanced competitor systems, GPT <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and JUSSIPOP <ref type="bibr" target="#b80">[81]</ref>. We do not directly include other relevant systems such as SGP <ref type="bibr" target="#b87">[88]</ref>, CASSANDRA <ref type="bibr" target="#b72">[73]</ref>, YKA <ref type="bibr" target="#b78">[79]</ref> and QBFPLAN <ref type="bibr" target="#b76">[77]</ref>. An indirect comparison with SGPand CASSANDRA, showing they are outperformed by GPT and MBP, can be derived from <ref type="bibr" target="#b8">[9]</ref>. JUSSIPOP consistently improves over YKA and QBFPLAN (personal communication by Jussi Rintanen).</p><p>For each of the systems, we attempted to maximize the performance by using existing hand-crafted encodings, and acquiring direct information from the authors about optimizing the encodings and the behavior of their planner.</p><p>For each of the planners, we used a fixed setup throughout the whole evaluation. The default parameters have been adopted for every planner. For GPT and JUSSIPOP, this follows the authors' indication. For MBP, this means that the reuse of failures and successes via equivalence classes is enabled, and the remaining optimizations are disabled. This setup represents a reasonable trade-off between the overhead introduced by the computation of the optimizations, and their induced speed-up. Indeed, while reusing failures and successes by equivalence classes can be perceived as generic search controls, crucial to inhibit useless expansions of relevant portions of search space, the remaining optimizations can be more or less effective depending on the features of the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1.">JUSSIPOP</head><p>JUSSIPOP is a regression planner for nondeterministic, partially observable domains, based on the ideas of YKA. It relies on BDD-based representation of observational classes, which are constructed as a pre-processing of the domain. The search exploits such observational classes to recombine beliefs (also represented via BDDS), and is driven by a cardinality heuristic that tries to achieve the maximal belief: when progressing backward, the intuition is that the larger the belief, the more likely it is that the initial situation is included, and a solution is found. The input language of JUS- SIPOP is an extension of PDDL that allows for nondeterministic action effects and multiple initial states; observability is modeled by declaring a subset of state variables to be observable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2.">GPT</head><p>GPT models planning domains as Markov Decision Processes, and, based on the probability distributions, produces an optimal policy associating actions to belief states. The search in GPT exploits the Labeled-RTDP algorithm described in <ref type="bibr" target="#b16">[17]</ref>; for partially observable domains such as those we are interested in, the search is performed in belief space (for POMDPs). LRTDP is an optimal and complete algorithm for these models, computing a partial optimal policy closed with respect to the initial (belief) state.</p><p>GPT adopts an extension of PDDL (different to JUSSIPOP's) that allows describing sensing actions, nondeterministic (also probabilistic) action effects, multiple initial states, and functions, which can either be defined by PDDL operators, or computed by external C++ modules.</p><p>Notice that the problem solved by GPT is quite different from that tackled by MBP and by JUSSIPOP. First, GPT produces a (memoryless) policy, rather than a conditional plan. Second, it solves an optimization problem. These differences must be kept in mind when evaluating the experimental results. Also notice that, differently from JUS- SIPOP and from our implementation, GPT does not exploit a symbolic representation of beliefs; indeed, it explicitly manipulates each state in a belief. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Comparison between planners 7.5.1. Empty room</head><p>The results are reported in Fig. <ref type="figure" target="#fig_21">25</ref>, for increasing room sizes. This domain appears not particularly difficult for MBP and JUSSIPOP, while the behavior of GPT degrades rapidly, due to the increasing number of initial states for which a policy has to be computed, and to the exponential number of optimal policies in the problem. While MBP and JUSSIPOP both rely on symbolic, BDD-based representations to avoid explicitly dealing with each single state, GPT does not, which also explains the differently scaling behaviors. Notice that MBP and JUSSIPOP exhibit a similar curve, with MBP consistently outperforming JUSSIPOP by about two orders of magnitude. The difference between the performances of MBP and JUSSIPOP, in this case, may be explained by the fact that JUSSIPOP's problem representation is constrained to adopting boolean fluents, which is not convenient to represent N-valued coordinate positions, leading to an unnecessarily high number of fluents in the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.2.">Maze</head><p>The results reported in Fig. <ref type="figure" target="#fig_22">26</ref>, left part, present average CPU times for each maze size for the three systems. For MBP and JUSSIPOP, the evaluation of averages is simple: MBP terminates all tests within the limit, and for a given size of the problem, JUSSIPOP either completes all the tests, or does not complete any of them. The situation is more complex for GPT, which presents a high variance in his behavior on mazes of the same size, and completes every sample only up to size 33. In this case, the averages from size 35 onward only refer to the completed tests. The results reported in the two scatterplots in Fig. <ref type="figure" target="#fig_22">26</ref>, right part, report a pairwise comparison between MBP and the two competing systems; they give a more detailed view of the results. Here, as it is standard for scatterplots, overtimes are reported on the x and y axis.</p><p>Interestingly, the hardness of this domain seems comparable to that of the empty room for MBP, while it seems much simpler than the empty room for GPT, and much harder for JUSSIPOP.</p><p>Our intuition for the improved GPT behavior w.r.t. the empty room is that the number of initial states is lower, and the number of optimal plans to reach the desired situation can only be exponential in very particular maze topologies,  and is in many cases rather limited. For most cases of large mazes, GPT is capable of tackling the problem within the given time limit (even though less efficiently than MBP, due to the explicit state handling).</p><p>We believe that JUSSIPOP's degraded performance is largely due to regression: the number of possible different observational classes that result from backward composition via observations can grow extremely vast. Furthermore, due to the fact that observations must be represented as "observable state variables", the results of observations after executing one action must be computed together with the effects of the action. While for a domain for a simple structure such as the empty room this introduces no great complexity, for the maze domain this causes the need to introduce complex conditional effects into actions, which may make the domain very hard to analyze for the planner. As a result, JUSSIPOP seems to exhibit an exponential behavior, while MBP maintains the polynomial behavior shown for the empty room domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.3.">Ring</head><p>The results for this domain are reported in Fig. <ref type="figure" target="#fig_23">27</ref>, for different values of N. This problem seems very simple for MBP, but hard for GPT (due to the huge number of states that have to be explicitly dealt with) and JUSSIPOP (probably due to the fact that the cardinality heuristic is not informed enough to direct the search effectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.4.">Bomb in the toilet</head><p>The results (for the problem instances with toilets and packages ranging from 1 to 30) are reported in Fig. <ref type="figure" target="#fig_24">28</ref>. Each graph refers to a fixed number of toilets; on the x axis, the number of packages is reported. In Fig. <ref type="figure" target="#fig_25">29</ref> the corresponding scatterplots are also reported. The results are qualitatively similar to those obtained for the empty room domain: the problem seems not particularly difficult for MBP and JUSSIPOP while GPT's performance degrades rapidly as the size of the problem increases. Again, our intuition is that this is due to GPT's explicit state handling, opposed to MBP and JUSSIPOP's underlying symbolic engine. Notice that the behavior of MBP and JUSSIPOP are also quantitatively comparable for the simpler problems, with MBP scaling up more nicely for the harder problems and dominating JUSSIPOP by more than two orders of magnitude on the bigger instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.5.">Medical</head><p>The results for this domain are presented in Fig. <ref type="figure" target="#fig_26">30</ref>, for varying number of illnesses (and related sensing). For MBP, the problem seems simple, as it scales up quite smoothly. For JUSSIPOP, the problem appears to be quite hard; once more, this is likely to be due to the high number of observational classes that have to be constructed in the pre-processing phase. Although GPT seems to scale well, we notice that the time spent in finding a solution largely depends on the discount factor, a parameter used in building the heuristics. We report the data referring to a discount factor of 0.9; higher times are obtained with either higher or lower factors. We remark that, while the same 0.9 value is used in all the test cases, this instability phenomena appears only in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.6.">Power supply restoration</head><p>The results are reported in Fig. <ref type="figure" target="#fig_27">31</ref>. MBP is able to efficiently solve all the problems, exhibiting an easy-hard-easy pattern for the problems with exact number of faults, and an easy-hard pattern for the problems with maximal number of faults. The same behavior is evident for the GPT planner; notice however that the GPT modeling of the problem is rather that of an optimization problem, whose goal is that of minimizing the number of unfed, feedable lines. JUSSIPOP  also seem to exhibit a similar behavior, but it times out for the most difficult problems. Notice that, since neither MBP nor JUSSIPOP handle PDDL's directional axioms, the encoding of problems must "unfold" the ramifications in the consequences of propagating faults; this means that the size of such modelings, which we build via a script, may grow up exponentially (see <ref type="bibr" target="#b84">[85]</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.7.">Concluding remarks</head><p>The experiments show that MBP is able to deal with all the domains considered in our test, and consistently outperforms both GPT and JUSSIPOP.</p><p>The comparison with JUSSIPOP is interesting, given the similarities of the two systems: in fact JUSSIPOP tackles the very same problem of MBP (strong planning under partial observability), and adopts a similar internal representation of beliefs. The most evident difference is in the fact that JUSSIPOP is regressive; this seems to be somewhat of a problem when the backward combination of a large number of observation classes can not be easily represented with symbolic techniques. Moreover, while a forward search ensures that every represented state is legal, a backward search may involve reasoning about illegal states, which can contribute to the overall complexity. Finally, we believe that JUSSIPOP's simple cardinality heuristic may often provide too little guidance to be effective.</p><p>The comparison between MBP and GPT is somewhat harder, since GPT tackles a different problem (identifying optimal, possibly cyclic policies). This may offer a partial explanation to the different scale-up behaviors in some of the domains. More importantly, we believe that GPT suffers from the space explosion problem since it does not rely on a symbolic representation. Notice that for some specific domains, whose recasting as an optimization problem is convenient, or where suboptimal policies are easily identified and pruned, GPT proves extremely competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Related work</head><p>Different approaches have been devised to address the problem of planning under partial observability in nondeterministic domains. The problem has been deeply investigated in the framework of Markov Decision Processes (MDP) (see <ref type="bibr" target="#b11">[12]</ref> for an extensive survey). MDP planning allows for stochastic domains, i.e., nondeterministic domains where probabilities can be assigned to transitions. There are substantial differences with our approach, both conceptual and practical. From the conceptual point of view, the MDP framework is richer than ours. It allows for expressing and dealing with information about costs and probabilities of action transitions and with rewards associated to states. In MDP, problem instances can thus be expressed in more detail, and algorithms that optimize a utility function are defined on the basis of costs/rewards. While in this paper we propose an algorithm that is guaranteed to return strong solutions, in MDP, whether the solution is strong depends on the probability and cost distribution. For instance, in <ref type="bibr" target="#b52">[53]</ref>, cycles are not avoided, and solutions are not guaranteed to terminate. On the one hand, the MDP approach pro-vides the ability to find solutions that have detailed requirements on costs and rewards. On the other hand, in some applications, like safety critical domains, it is crucial to guarantee that the plan will terminate and will lead to the desired states. A further conceptual difference is the fact that in this paper we focus on reachability goals. In MDP, costs and rewards provide the ability to represent goals that are more expressive than reachability goals. The extensions of MDP to express goals in temporal logic in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b86">87]</ref> are somehow paralleled by similar extensions in our framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>From the standpoint of complexity, the problem we tackle has been shown to be 2-EXP-complete <ref type="bibr" target="#b79">[80]</ref>, while policy existence in POMDPs is NP-complete for stationary policies, and PSPACE-complete for time-dependent ones <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b74">75]</ref>. However, from the practical point of view, the expressiveness of the MDP approach is more difficult to manage in the case of large state spaces. Our approach allows us to fully exploit the BDDS-based symbolic model checking techniques to tackle problems of significant size. A lot of work has been done to tackle the state-explosion problem in MDP. Notable examples are based on abstraction, reachability analysis, and decomposition (see <ref type="bibr" target="#b11">[12]</ref> for a review). The SPUDD planner <ref type="bibr" target="#b51">[52]</ref> tackles MDP problems by making use of Algebraic Decision Diagrams, data structures similar to BDDS. In SPUDD, decision diagrams are used to represent much more detailed information than in MBP (e.g., the probabilities associated to transitions). This partly reduces the main practical advantages of decision diagrams as they are used in MBP. SPUDD is restricted to the case of full observability; some other probabilistic models, instead, are specialized to the case of null observability, i.e., conformant planning, like the work on Conformant Probabilistic Planning <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b64">65]</ref>.</p><p>The more general MDP approach to partially observable domains (POMDP) has also been extensively studied <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b82">83]</ref> (see also <ref type="bibr" target="#b11">[12]</ref> for a more exhaustive list of references). The POMDP approach inherits from MDP the conceptual and practical differences with our work. Moreover, in its general formulation, the POMDP problem is complicated by the fact that belief states are defined as probability distributions over the set of states of the domain, and the set of belief states is not finite. However, practical approaches to this problem have been devised. This is the case of the solution implemented in GPT, the available planner based on MDP which can deal with partial observability. As the experimental results in Section 7 show, MBP outperforms GPT, since an enumerative approach in the case of large state spaces and-or high uncertainty can hardly scale up. However, the experimental comparison should be interpreted very carefully, keeping in mind that POMDP planners solve a different problem. In <ref type="bibr" target="#b56">[57]</ref>, the classical planner TLplan <ref type="bibr" target="#b19">[20]</ref> is extended to the possibilistic/probabilistic conditional planner PTLplan. Similarly to TLplan, PTLplan makes use of knowledge encoded in a temporal logic to guide the search for a conditional plan in a probabilistic domain.</p><p>The problem of planning in nondeterministic domains and under partial observability has also been addressed with approaches different from MDP and probabilistic planning in a qualitative setting. A first attempt is described in <ref type="bibr" target="#b46">[47]</ref>, which presents an off-line planning algorithm performing a breadth-first search on an and-or graph. Other initial attempts have extended classical planning to the problem of planning in nondeterministic domains (see for instance <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74]</ref>). More recent approaches extend planners based on planning graphs <ref type="bibr" target="#b12">[13]</ref> and satisfiability <ref type="bibr" target="#b60">[61]</ref>. Most of them are limited either to full observability, or to conformant planning, see, e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref>. Among those that can deal with partial observability, the first significant result was SGP <ref type="bibr" target="#b87">[88]</ref>, an extension of GRAPHPLAN that provides significant improvements in performance compared with previous extensions to classical planners. SGP produces acyclic conditional plans, but it is unable to deal with nondeterministic action effects, i.e., uncertainty is limited to the initial condition. The algorithm is enumerative in nature: a planning graph is built for each initial state that can be distinguished by observation. For this reason, SGP is not competitive with more recent planners considered in this paper. For instance, in the empty room problem, SGP was unable to solve the 3 × 3 version of the problem in 12 hours of CPU time.</p><p>Among the planners based on satisfiability, only QBFPLAN <ref type="bibr" target="#b76">[77]</ref> can deal with partial observability. The planning problem is reduced to a QBF satisfiability problem, which is input to an efficient solver <ref type="bibr" target="#b77">[78]</ref>. QBFPLAN is limited to bounded-length planning, i.e. it looks for a strong solution of specified length l. When this does not exist, it iteratively increases l until a solution is found or a specified limit is reached. QBFPLAN is thus unable to detect when the problem is unsolvable. QBFPLAN exploits its symbolic approach to avoid exponential blow up caused by the explicit enumeration of states, but seems unable to scale up to large problems.</p><p>A different approach to the problem of planning under partial observability is the idea of "Planning at the Knowledge Level", implemented in the PKS planner <ref type="bibr" target="#b70">[71]</ref>. This approach is based on a representation of incomplete knowledge and sensing at a higher level of abstraction. While this approximation can effectively improve perfor-mance, it makes it impossible to model a variety of problems that can be dealt with an exact representation like the one proposed in this paper. The extension presented in <ref type="bibr" target="#b71">[72]</ref> provides a limited solution to the problem of deriving complete conclusions from observations. The work presented in this paper is closely related to the research line of "Planning via symbolic model checking", where the special cases of full observability <ref type="bibr" target="#b33">[34]</ref> and conformant planning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> have been tackled. The main difference is that here we provide a solution to the general problem of planning under partial observability. This increased generality comes to a price: we can expect that tackling both special cases with the general approach described in this paper would be significantly less effective than the specialized algorithms proposed in <ref type="bibr" target="#b33">[34]</ref> and in <ref type="bibr" target="#b35">[36]</ref>. On the other hand, we remark that the MBP planner implements all the above algorithms in a uniform framework, and the selection of the most appropriate algorithm depending on the problem at hand can be completely automatized. It is also interesting to notice that the problem of strong planning under full observability is tackled in <ref type="bibr" target="#b33">[34]</ref> with a BDD-based, backward and-or search algorithm, which is significantly different from the one proposed in this paper. In principle, it would be possible to tackle the problem of strong planning under partial observability by means of the very same approach of <ref type="bibr" target="#b33">[34]</ref>; however, this would require the construction of the powerset automaton of the domain, which is potentially associated with an exponential blow up. In addition, the progression algorithm presented in this paper has the advantage that, with respect to a backward search algorithm, it always restricts the search to reachable belief states.</p><p>This paper substantially extends some preliminary results presented in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. In particular, none of those papers provided a full formal definition of the framework, nor a proof of the properties of the algorithm; the work in <ref type="bibr" target="#b8">[9]</ref> is based on a hardcoded depth-first search algorithm, not amenable to the usage of heuristic measures; and the experimental analysis in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b6">[7]</ref> is much less detailed of the one we present here.</p><p>Other BDD-based approaches to the problem of strong planning under partial observability have been proposed in the YKA <ref type="bibr" target="#b78">[79]</ref> and JUSSIPOP planners <ref type="bibr" target="#b80">[81]</ref>. Similarly to our approach, these planners adopt an and-or search schema, but contrary to MBP they perform a backward search in the space of beliefs. As such, observations are used to recombine beliefs, according to a fixed cardinality-based heuristics. YKA is consistently outperformed by JUSSIPOP (which is, in turn, outperformed by MBP, as shown in <ref type="bibr">Section 7)</ref>.</p><p>Other planners that are based on symbolic model checking techniques restrict to the case of full observability, see, e.g., UMOP <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>, or to classical planning, see, e.g., MIPS <ref type="bibr" target="#b41">[42]</ref>. Other approaches are based on different model checking techniques, e.g., on explicit-state representations, and most of them are also limited to the case of full observability. This is the case of SIMPLAN <ref type="bibr" target="#b57">[58]</ref>. <ref type="bibr" target="#b39">[40]</ref> presents an automata based approach to formalize planning in deterministic domains. The work in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref> presents a method where model checking with timed automata is used to verify that generated plans meet timing constraints.</p><p>In all the approaches mentioned above, uncertainty is dealt with at planning time. Methods that interleave planning and execution (see, e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b61">62]</ref>) can be considered alternative (and orthogonal) approaches to the problem of planning off-line with large state spaces. On one side, they open up the possibility to deal with larger state spaces. On the other side, these methods can not guarantee to find a solution, unless assumptions are made about the domain.</p><p>Finally, we remark that our algorithm differs from and-or search algorithms such as AO* and LAO*, that have been around for a long time. AO* <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b67">68]</ref> and its variations such as HS <ref type="bibr" target="#b66">[67]</ref> or CF <ref type="bibr" target="#b62">[63]</ref> aim at discovering a cost-minimal solution for an acyclic and-or graph whose arcs represent action outcomes associated with a cost. The assumption of acyclicity of the search space, necessary for AO*'s cost revision step, does not generally hold for the belief spaces associated to planning problems for nondeterministic, partially observable planning domains; this inhibits the usage of AO* and its variants in such a setting. On the other hand, LAO* <ref type="bibr" target="#b52">[53]</ref> is able to deal with cyclic graphs, but the notion of acceptable solution includes cyclic plans-whose execution would not be guaranteed to terminate. In this paper, we require solutions to be strong, i.e. to guarantee that they reach the goal in a finite number of execution steps, a feature LAO* can not guarantee. The idea of "mimicking" the requirement of acyclicity in LAO* by playing with the weights appears unnatural from a representational point of view, and difficult to achieve in a principled manner.</p><p>Recent work <ref type="bibr" target="#b17">[18]</ref> focuses on the problem finding optimal worst-case cost solutions in cyclic and-or graphs. This is based on an IDA* search mechanism, where backtracking is triggered by the inconsistency of the cost value associated to a node with those associated to its children. The focus on optimality and the iterative DFS kind of search make the approach and the algorithm rather different from the one we propose here. A similar problem is tackled in <ref type="bibr" target="#b53">[54]</ref>; again, the focus on optimization of the cost leads to a complex algorithm which relies on bottom-up cost updating, and which is therefore very different from the one we present here.</p><p>Very recent work extends the symbolic conformant planner CFF <ref type="bibr" target="#b18">[19]</ref> to deal with contingent planning by combining AO* with a cycle detection mechanism <ref type="bibr" target="#b49">[50]</ref>. There are several differences with respect to our approach. First, CFF is based on SAT rather than on BDDS, and uses planning graphs to compute heuristic measures. Second, CFF is only able to deal with a limited form of nondeterministic action effects. Third, for efficiency reasons CFF only implements an approximate cycle detection mechanism, while MBP has an exact cycle detection mechanism, thanks to the BDD machinery. In <ref type="bibr" target="#b49">[50]</ref>, CFF is compared against MBP implementation of the techniques proposed in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions and future work</head><p>In this paper we have presented a new, comprehensive approach to the problem of planning under partial observability. First, we formally defined the problem of strong planning under partial observability, within an expressive framework that captures a wide class of nondeterministic, partially observable domains. Then, we proposed a new algorithm for strong planning under partial observability. The algorithm is proved to always terminate, and to be correct and complete. We discussed how the algorithm is amenable to a symbolic, BDD-based implementation, and proposed several search strategies. We carried out a thorough experimental evaluation, over a wide class of problems in the literature, evaluated the various search strategies, and compared our approach with the other available systems. Our MBP planner is able to outperform its competitors, often by orders of magnitude.</p><p>In the future, we plan to extend this work along the following directions. First, we will investigate the use of heuristic search to enhance the quality of the search, e.g. by exploiting the ideas in <ref type="bibr" target="#b35">[36]</ref>, and the use of classical planning technologies such as planning graphs. Second, we will investigate variations and extensions of the problem, in particular strong cyclic planning, and planning for temporally extended goals. Finally, we will address how to integrate the planning algorithm within the setting of interleaving planning and execution.</p><p>As a result, it is easy to see that the normalization of a (possibly redundant) solution π returns a plan π such that every belief associated to a node of BET(π , I) is also associated to some node of BET(π, I), i.e. the execution of π may traverse a subset of the beliefs that can be traversed by executing π . Proof. We prove the theorem by induction on the size of the plan π .</p><p>In the base case, |π| = 0, i.e. π = ε; then, NORMALIZE(π, B, G) = π = ε. Since the plan is a solution and ε is not redundant, the theorem holds.</p><p>In the induction step, we reason by cases, following the definition of the NORMALIZE routine: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Loop invariants</head><p>In order to prove the key properties of termination, correctness and completeness of the algorithm, it is convenient to state and prove a number of loop invariants over the search tree constructed by the main loop. In the following, we denote with ST [i] the search tree at the beginning of the ith iteration of the loop of the planning algorithm in Fig. <ref type="figure" target="#fig_5">6</ref>, i.e. prior to the evaluation of the stop condition at line 6 of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma B.3. For every non-leaf node n ∈ ST [i] , the following conditions hold:</head><p>-if a ∈ A is applicable on BEL(n), then ST </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A simple domain represented as a state transition system.</figDesc><graphic coords="3,142.03,67.58,256.08,112.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 2 . 14 (Example 2 . 15 .</head><label>214215</label><figDesc>Planning problem and strong solution). A planning problem is a 3-tuple D, I, G , where D = Σ, O, X is a planning domain, ∅ = I ⊆ S is the set of initial states, and ∅ = G ⊆ S is the set of goal states. The plan π is a strong solution to the problem D, I, G iff π is applicable in every state of I, and -every run of π from a state in I ends in G, i.e. FinalStates(I, π) ⊆ G. Consider the planning problem given by the domain in Example 2.4, the initial set of states I = {NW, SW}, and the goal states G = {SW}. The plan in Example 2.13 is a strong solution to the planning problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 3 . 5 .</head><label>35</label><figDesc>Let D, I, G be a planning problem, and let π be applicable in I. Then π is a strong solution to D, I, G iff FinalBels(I, π) ⊆ G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The Belief Execution Tree associated to plan π 2 .</figDesc><graphic coords="9,43.75,67.49,452.64,153.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The search tree for the example.</figDesc><graphic coords="11,85.22,67.60,369.60,262.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The basic planning algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The behavior of the algorithm for the example.</figDesc><graphic coords="13,189.19,66.97,161.76,523.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The node expansion primitive and the node marking subroutine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The procedures to detect failure or success of an expanded node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Theorem 4 . 5 .</head><label>45</label><figDesc>Let π be a strong solution for a problem D, B, G . Then NORMALIZE(π, B, G) is a non-redundant solution for D, B, G .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The plan normalization routine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Two BDD for the formula (a 1 ↔ b 1 ) ∧ (a 2 ↔ b 2 ) ∧ (a 3 ↔ b 3 ).</figDesc><graphic coords="18,112.31,399.86,324.12,229.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. A simple nondeterministic robot navigation domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. The information associated to observation variables in the robot navigation domain of Fig. 14.</figDesc><graphic coords="23,53.52,67.63,433.20,353.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 17 .Fig. 18 .</head><label>1718</label><figDesc>Fig. 17. A symbolic representation of belief states for the robot navigation domain.</figDesc><graphic coords="25,93.50,67.45,353.04,174.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Results for the Empty Room domain.</figDesc><graphic coords="29,57.16,525.77,425.76,143.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Results for the Maze domain.</figDesc><graphic coords="30,61.46,67.58,425.76,144.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. Results for the Ring domain.</figDesc><graphic coords="30,61.46,250.56,425.76,144.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. Results for the BMTC domain.</figDesc><graphic coords="31,57.16,67.58,425.76,144.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 23 .</head><label>23</label><figDesc>Fig. 23. Results for the Medical domain.</figDesc><graphic coords="31,57.16,249.81,425.76,144.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 24 .</head><label>24</label><figDesc>Fig. 24. Results for the PSR domain.</figDesc><graphic coords="32,61.33,67.60,426.00,298.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 25 .</head><label>25</label><figDesc>Fig. 25. Tests for the Empty Room domain.</figDesc><graphic coords="34,183.75,67.79,181.20,125.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 26 .</head><label>26</label><figDesc>Fig. 26. Tests for the Maze domain.</figDesc><graphic coords="34,73.01,568.07,402.72,101.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 27 .</head><label>27</label><figDesc>Fig. 27. Tests for the Ring domain.</figDesc><graphic coords="35,193.36,67.73,153.36,106.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 28 .</head><label>28</label><figDesc>Fig. 28. Tests for the BMTC domain.</figDesc><graphic coords="36,41.48,67.54,465.84,410.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 29 .</head><label>29</label><figDesc>Fig. 29. Scatterplots for the BMTC domain.</figDesc><graphic coords="36,141.41,511.84,266.64,100.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Fig. 30 .</head><label>30</label><figDesc>Fig. 30. Tests for the Medical domain.</figDesc><graphic coords="37,198.69,67.46,142.81,100.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 31 .</head><label>31</label><figDesc>Fig. 31. Tests for the PSR domain.</figDesc><graphic coords="37,117.55,200.73,304.80,119.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Theorem 4 . 5 .</head><label>45</label><figDesc>Let π be a strong solution for a problem D, B, G . Then NORMALIZE(π, B, G) is a non-redundant solution for D, B, G .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>1 .</head><label>1</label><figDesc>When B ⊆ G, the empty plan ε is returned, which is a non-redundant solution if B ⊆ G; 2. (1) does not hold and B ∈ TRAVBELS(BET(π, B)).Then, SubTree(BET(π, B), n B ) is a belief execution tree whose root is associated to B. Since its leaves are leaves of BET(π, B), PlanOf (SubTree(BET(π, B), n B )) is a subplan of π which is a solution for D, B, G . Also, |PlanOf (SubTree(BET(π, B), n B ))| &lt; |π|; so, for the inductive hypothesis, the returned plan NORMALIZE(PlanOf (SubTree(BET(π, B), n B )), B, G) is a non-redundant solution for the problem.3. (1) and (2) do not hold, and π = a • π . Then π is a solution for D, Exec(B, a), G , and since |π | &lt; |π|, by the inductive hypothesis, NORMALIZE(π , Exec(B, a), G) is a non-redundant solution for D, Exec(B, a), G . Then since (2) does not hold, the returned plan a • NORMALIZE(π , Exec(B, a), G) is a non-redundant solution for D, B, G . 4. (1), (2), (3) do not hold, π = if o then π 1 else π 2 and B ∩ X -(o) = ∅. Then π 1 is a solution for D, B, G (since B ∩ X -(o) = B), and since |π 1 | &lt; |π|, by the induction step, the returned plan NORMALIZE(π 1 , B, G) is a non-redundant solution for the same problem. 5. (1), (2), (3), (4) do not hold, π = if o then π 1 else π 2 and B ∩ X -(o) = ∅. Then π 2 is a solution for D, B, G (since B ∩ X -(o) = B), and since |π 2 | &lt; |π|, by the induction step, the returned plan NORMALIZE(π 2 , B, G) is a non-redundant solution for the same problem. 6. (1), (2), (3), (4), (5) do not hold, and π = if o then π 1 else π 2 . Then π 1 is a solution for D, B ∩ X -(o), G . By the inductive hypothesis, since |π 1 | &lt; |π|, π 1N = NORMALIZE (π 1 , B ∩ X -(o), G) is a non-redundant solution for D, B ∩ X -(o), G . By a similar reasoning, π 2N = NORMALIZE(π 2 , B ∩ X -(o), G) is a non-redundant solution for D, B ∩ X -(o), G . Then, since (2), (3), (4) do not hold, if o then π 1N else π 2N is a non-redundant solution. 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>[i]  contains a node n = Exec(BEL(n), a), PATH(n) • a , and an or-arc n, a, n ;-if o ∈ O is not predetermined on BEL(n), then ST [i] contains two nodes n T = BEL(n) ∩ X -(o), PATH(n) • o and n F = BEL(n) ∩ X -(o), PATH(n) • o ,and an and-arc n, o, n T , n F . Proof. We prove the theorem by induction on the number of top-level iterations performed by the algorithm. The proof is trivial for the initial search tree ST [0] , whose only node is a leaf. It is also easy to see that, it the theorem holds for ST [i] , it holds for ST [i+1] , since the internal nodes of ST [i+1] are those of ST [i] , plus the one node n having been</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,124.08,454.54,300.72,215.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and σ is a run of π 2 starting from (s, o ). Consider the following plan for the robot domain of Example 2.4:</figDesc><table><row><cell>Example 2.13.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>8 EXTENDTREE(node, ST); 9 if (SONSYIELDSUCCESS(node, ST)) then 10 TAGNODE(node, Success); 11 PROPAGATESUCCESSONTREE(node, ST); 12 else if (SONSYIELDFAILURE(node, ST)) then 13</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>TAGNODE(node, Failure);</cell></row><row><cell>14</cell><cell>PROPAGATEFAILUREONTREE(node, ST);</cell></row><row><cell>15</cell><cell>fi</cell></row><row><cell cols="2">16 done</cell></row><row><cell cols="2">17 if ISSUCC(ROOT(ST)) then</cell></row><row><cell>18</cell><cell>return BUILDPLAN(ROOT(ST), ST);</cell></row><row><cell cols="2">19 else</cell></row><row><cell>20</cell><cell>return Failure;</cell></row><row><cell>21 fi</cell><cell></cell></row><row><cell>22 end</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The actual order depends on the order of actions in the input file and on the actual BDD encoding.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Intuitively, reaching the goal requires reducing the uncertainty over f : if for f more values are possible in B than in G, then B can not be contained in G.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We point out two major differences with respect to the idea of invariants in classical planning. First, here we are dealing with nondeterministic action effects, and our analysis has to take into account all possible outcomes. Second, invariant fluents can not be simplified away to constants, since the same fluent can occur both positively and negatively within the same belief state.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>It is enough to check if f = f is a logical consequence of the transition relation.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Proofs of theorems on strong planning and belief execution</head><p>Theorem A.1. Let π be a plan applicable in belief state B. Then the following properties hold:</p><p>-if π = a • π , then FinalBels(B, π) = FinalBels(Exec(a, B), π )); -if π = if o then π 1 else π 2 , then FinalBels(B, π) = FinalBels(B ∩ X -(o)), π 1 ) ∪ FinalBels(B ∩ X -(o), π 2 ).</p><p>Proof. (i) Let π = a •π . Then MaxPaths(π) = a •MaxPaths(π ). This induces a one-to-one correspondence between MaxPaths(π) and MaxPaths(π ): any p ∈ MaxPaths(π) has the form p = a • p for some p ∈ MaxPaths(π ), and for each p ∈ MaxPaths(π ), a • p ∈ MaxPaths(π). Let p = a • p be a generic element of MaxPaths <ref type="bibr">(π)</ref> Proof. We prove the stronger statement that, for any belief B ⊆ S, if π is applicable in B, then FinalStates(B, π) = FinalBels(B, π). We proceed by induction on the structure of π :</p><note type="other">.</note><p>-if π is ε, FinalStates(B, π) = B, and FinalBels(B, π) = {B}; -if π is a • π , then a is applicable in B, and π is applicable in Exec(a, B).</p><p>From the inductive hypothesis, FinalStates(Exec(a, B), π ) = FinalBels(Exec(a, B), π). From Definition 2.12, we have that FinalStates(Exec(a, B), π ) = FinalStates(B, a • π ).</p><p>From Theorem A.1, we have that FinalBels(Exec(a, B), π ) = FinalBels(B, a</p><p>From the inductive hypothesis,</p><p>From Definition 2.12, we have that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Properties of the algorithm</head><p>This section contains the formal proofs showing that the algorithm is correct, complete and terminating, together with the necessary lemmas and introductory structural notions. We first introduce some additional structural size and distance notions (Section B.1); then, we show that the algorithm restricts the search to a class of plans that feature no loops nor predetermined observations, and we prove this gives no loss of generality (Section B.2). Then, we prove some invariants about the main loop of the algorithm (Section B.3), which are finally used in the proofs of termination, correctness and completeness (Section B.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Structural notions</head><p>For the purpose of performing structural induction on search tree and plan structures, we define a notion of distance of a node to the leaves, which intuitively provides the maximum number of arcs that must be traversed from that node to reach a leaf of the tree, and a notion of plan size, indicating the maximal depth of the associated tree structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition B.1 (Maximal distance from leaves).</head><p>The maximal distance from leaves of a node n of a belief execution tree T is defined as</p><p>It is easy to see that, if n F is the father node of n, then d MAX (n F , T ) d MAX (n, T ) + 1, and that d MAX (n, T ) is finite for every node of a belief execution tree T . The same notion also applies to a search tree ST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition B.2 (Size of plan).</head><p>The size of a plan π , indicated with |π|, is the length of its longest maximal path, and can be defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Non-redundant solutions</head><p>Here, we prove that restricting the search to non-redundant plans does not affect completeness of the search: any (possibly redundant) plan can be normalized into a unique non-redundant plan by the NORMALIZE procedure in Fig. <ref type="figure">12</ref>.</p><p>In the procedure, TRAVERSED(T) identifies the nodes of T other than its root, and TRAVBELS(T ) the associated set of beliefs. Intuitively, NORMALIZE simplifies a plan π according to the following ideas:</p><p>-every possible execution of the plan stops as soon as the goal is reached (lines 4-5); -if the plan traverses a belief B more than once, the prefix that causes multiple traversals of B is removed (lines 6-8); -every predetermined observation is removed, keeping only the subplan associated to the one possible observation branch (lines 11-15).</p><p>expanded during the ith iteration by EXTENDTREE; but EXTENDTREE is defined in a way that directly maps into the property holding also for n. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma B.4. For every non-leaf node of ST [i]</head><p>, if SONSYIELDSUCC holds, the node is tagged Success; if SONSYIELDFAIL holds, the node is tagged Failure; otherwise, it is tagged Undetermined.</p><p>Proof. We prove the theorem by induction on the number of top-level iterations performed by the algorithm. In the base case, the tree ST [0] is built exclusively by a leaf node, so the theorem trivially holds. Now, suppose the theorem holds for the tree ST <ref type="bibr">[i]</ref> . At the (i + 1)th iteration, a leaf n is chosen and expanded, and its tag is set depending on whether SONSYIELDFAIL(n) or SONSYIELDSUCC(n) hold; as such, node n obeys the requirement of the theorem. If neither SONSYIELDFAIL(n) or SONSYIELDSUCC(n) hold, n is left as Undetermined, and no success or failure propagation takes place. In this case, the theorem holds, since by the induction step, it holds for every node of ST [i+1] different from n. If SONSYIELDSUCC(n) holds, due to the induction step, the only nodes for which the theorem has to be proved are the ancestors of n, which may have to be set as successful. But it is easy to see that the PROPAGATESUCCONTREE routine recursively recomputes the tag of every ancestor for which SONSYIELDSUCC holds, proving the theorem in this case. Similarly for the case of SONSYIELDFAIL(n).</p><p>, where j &gt; i, and is tagged Success</p><p>Proof. We first observe that, since ST [i+1] has every node of ST <ref type="bibr">[i]</ref> , plus possibly some additional nodes resulting from the expansion of a frontier node, every node of</p><p>. By induction, for every j &gt; i, every node of</p><p>We prove the statement by induction on the number of top-level iterations of the algorithm: we prove it holds considering j = i + 1, and this implies it holds for any j &gt; i. For a single step of the induction, we prove the theorem by induction on the frontier distance of nodes: we first prove it holds for nodes of ST [i] at distance 0, then we prove that, if it holds for nodes at distance D, it holds for nodes at distance D + 1.</p><p>Success/failure leaves of ST [i] are also success/failure leaves of ST [i+1] , so the base step is immediate. Now consider a determined node of ST [i] at distance D + 1 from the frontier. Its sons are all at distance at most D, so, if their tag is Success or Failure in ST <ref type="bibr">[i]</ref> , according to the inductive hypothesis, they have the same tag in ST <ref type="bibr">[i+1]</ref> . Given this, it is easy to see that if SONSYIELDSUCC holds for n on ST <ref type="bibr">[i]</ref> , it also holds on ST [i+1] , regardless of the fact that some son of n may be undetermined in ST <ref type="bibr">[i]</ref> , and determined in ST <ref type="bibr">[i+1]</ref> . The same goes for SONSYIELDFAIL. This, together with Lemma B.4, proves the theorem. 2 Lemma B.6. If the frontier of ST [i] is empty, then the root node is tagged either as a failure or as a success.</p><p>Proof. We prove the following stronger statement: let n be a node of ST <ref type="bibr">[i]</ref> , and let N be the set of its descendant leaves (or {n}, if n is a leaf). Then, if every node in N is determined, so is n.</p><p>We prove this statement by induction on the number of iterations performed by the algorithm. The base case is trivial: ST [0] only has a (leaf)node, and for a leaf node the theorem is a tautology. Now let us assume the theorem holds on the tree ST [i] produced after i iterations. The tree ST [i+1] is produced by expanding a leaf l of ST <ref type="bibr">[i]</ref> , tagging it according it to SONSYIELDFAIL and SONSYIELDSUCC, and, if l is tagged Failure or Success, propagating bottomup the failure or success to its ancestors. Consider a generic node n of ST [i+1] ; one of the following conditions hold:</p><p>n is a leaf of ST [i+1] , i.e. either a leaf of ST <ref type="bibr">[i]</ref> or one of the newly introduced leaves. The statement in this case reduces to a tautology; n is l; also in this case, since its tag is established by SONSYIELDSUCC and SONSYIELDFAIL, the statement is proved immediately; n is a non-leaf node of ST <ref type="bibr">[i]</ref> , and either l is not one of its descendants, or l has not been tagged Success or Failure after its expansion. Then it is easy to see neither n nor any of its descendants have their tag affected by the tree expansion; then, given the inductive hypothesis, the theorem holds for n;</p><p>n is a non-leaf node of ST <ref type="bibr">[i]</ref> , having l as one of its descendants, and l has been tagged Success or Failure after its expansion. To prove that the theorem holds for n, we reason inductively on the maximal frontier distance of its descendant nodes. The theorem trivially holds for its descendants which are leaves (at distance 0). If the theorem holds for nodes at distance D, it holds for nodes at distance D + 1, since they are either ancestors of l (in which case the bottom-up propagation of failure/success guarantees they are properly tagged), or determined nodes of ST <ref type="bibr">[i]</ref> (their descendant leaves are determined, and belonging to</p><p>) is a solution for the problem D, BEL(n), G .</p><p>Proof. We prove the theorem by induction over the maximal distance from leaves of the nodes: we first prove it for nodes at distance 0, and then we prove that, if it holds for nodes at distance D, it holds for nodes at distance D + 1.</p><p>If n is a leaf, at distance 0, the proof is trivial: a leaf is tagged success iff its associated belief is entailed by the goal, and BUILDPLAN returns the empty plan. Now assume that a node n at distance d &gt; 0 is tagged success; then, its successful sons are at a distance d d -1; thus, by the inductive hypothesis, they admit at least a solution, produced by BUILDPLAN. Then, we observe that:</p><p>1. There is a solution for D, BEL(n), G .</p><p>If  Proof. We prove the statement by induction on the structure of the belief execution tree T , based on the maximal distance of nodes from the frontier. The proof goes as follows:</p><p>-base step: all nodes at distance 0, i.e. the leaves of T , are non-failure nodes; -induction step: if all nodes at distance D are non-failures, all those at distance D + 1 are non-failures. This is easily shown by the fact that a node n at distance D + 1 features at least one or-son or a pair of brother and-sons at distance at most n which are not failures (namely, the sons in T ), thus SONSYIELDFAIL(n ) returns false, and as such n is not tagged Failure (see Lemma B.4). 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma B.9. Let π be a non-redundant solution for D, I, G . A leaf of BET(π, I) is either a successful leaf of ST</head><p>or a node not in ST <ref type="bibr">[i]</ref> , in which case it has an ancestor in the frontier of ST <ref type="bibr">[i]</ref> .</p><p>Proof. We prove the theorem by induction on the number of iterations performed by the algorithm.</p><p>In the base case, the theorem is immediate. If I ⊆ G, the one non-redundant solution is ε, and the only leaf of BET(ε, I) is the success leaf of ST <ref type="bibr">[0]</ref> . If I ⊆ G, the only node of ST [0] is a leaf, and, for any plan π , is also the root of BET(π, I), thus an ancestor to every leaf of BET(π, I). Now assume the theorem holds for ST <ref type="bibr">[i]</ref> ; ST [i+1] is obtained by picking a frontier node n of ST [i] , expanding it, tagging it and propagating success/failure if those are detected. Now consider a non-redundant solution π and a leaf l of BET(π, I). The following cases are possible:</p><p>l is a node of ST <ref type="bibr">[i]</ref> ; thus it is also a node of ST [i+1] , and, given the inductive hypothesis, the theorem holds for l; l is not a node of ST <ref type="bibr">[i]</ref> , and its ancestor on the frontier of ST [i] is a node n different from n. Then n is also its ancestor in the frontier of ST [i+1] . l is not a node of ST <ref type="bibr">[i]</ref> , and its ancestor on the frontier of ST <ref type="bibr">[i]</ref> is n. Then two cases are possible:</p><p>l is one of the nodes resulting from the expansion of n. But then, since l is associated to a belief entailed by G (it is a leaf of BET(π, I)), the EXTENDTREE procedure has it marked as Success, so it is a successful leaf of ST [i+1] ; l is not one of the nodes resulting from the expansion of n. Since EXTENDTREE expands n by every applicable action and non-predetermined observation, a node n resulting from such expansion is an ancestor of l. But notice that, since π is non-redundant, n cannot be associated to a belief entailed by G, so it is not marked Success. Also, since π is non-redundant, no belief on its belief path is traversed more than once; so n cannot be causing a loop, and is not marked as Failure. Proof. This originates from Lemma B.7: if the node is successful, a solution (of finite size) exists, and is produced by BUILDPLAN. But the number of recursions of BUILDPLAN is bounded by the number of arcs in the belief execution tree associated to the plan, since each recursion adds one (or-or and-) arc to the resulting tree. This number is finite, proving termination. 2 Theorem 4.6 (Termination). Given a planning problem, the execution of the planning algorithm in Fig. <ref type="figure">6</ref> terminates.</p><p>Proof. We first observe that it is enough to prove that the main loop of the algorithm terminates; to do so, we first prove that each iteration of the main loop terminates, and then we prove that the number of such iterations is finite. The first statement derives directly from the above theorems, and from the termination of EXTENDTREE, which is immediate to prove given the finiteness of the actions and observations that can be applied to a node.</p><p>To prove the second, we observe that at each loop, the algorithm (a) removes a node from the frontier, and (b) may add some undetermined nodes on the frontier, each being associated to an acyclic belief path. We observe that the number of acyclic belief paths is finite (since beliefs are finite), and so is the number of possible nodes in the frontier (since they correspond to acyclic belief paths). Let N be the number of acyclic belief paths for a domain D.</p><p>Then, after at most N iterations, every acyclic path will be generated, and the frontier will contain N M N nodes, each corresponding to a maximal acyclic belief path. Then, after further N M iterations, each node will be removed from the frontier, and since its extension generates a cyclic path, no more nodes will be added to the frontier. That is, the frontier will become empty, causing the failure of the root node (see Lemma B.6), and as such the termination of the algorithm. 2 Theorem 4.7 (Correctness). When the algorithm returns a plan π , then π is a strong solution for the problem.</p><p>Proof. This is a direct derivation of Lemma B.7, instantiated for the root node of the tree. 2 Proof. We prove the theorem by reductio ad absurdum. Assume that after k iterations, the algorithm returns Failure; let ST [k] be the search tree expanded by the algorithm after the k-th iteration.</p><p>Assume that a non-redundant solution π exists, and consider its associated belief execution tree T = BET(I, π). According to Lemma B.9, any leaf node l of T is either a successful node of ST <ref type="bibr">[k]</ref> , or a node not in ST <ref type="bibr">[k]</ref> with an ancestor on the frontier of ST <ref type="bibr">[k]</ref> . Thus, ST [k] contains a prefix T of T whose leaves are tagged either Success or Undetermined.</p><p>But then, according to Lemma B.8, the internal nodes of T cannot be failure nodes, including its root, which is also the root of ST <ref type="bibr">[k]</ref> . As such, the algorithm cannot have stopped with failure. 2</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating safe assumption-based plans for partially observable, nondeterministic domains</title>
		<author>
			<persName><forename type="first">A</forename><surname>Albore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bertoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-04</title>
		<meeting>AAAI-04<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rewarding behaviors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bacchus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Grove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-96</title>
		<meeting>AAAI-96<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured solution methods for non-Markovian decision processes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bacchus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Grove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-97</title>
		<meeting>AAAI-97<address><addrLine>RI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Symbolic model checking: 10 20 states and beyond</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. and Comput</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="170" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MBP: a model based planner</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bertoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pistore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Traverso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ICAI-2001 workshop on Planning under Uncertainty and Incomplete Information</title>
		<meeting>eeding of ICAI-2001 workshop on Planning under Uncertainty and Incomplete Information<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="93" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A framework for planning with extended goals and partial observability</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bertoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pistore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Traverso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICAPS-03</title>
		<meeting>ICAPS-03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional planning under partial observability as heuristic-symbolic search in belief space</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bertoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Planning (ECP)</title>
		<meeting>the European Conference on Planning (ECP)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Heuristic search + symbolic model checking = efficient conformant planning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bertoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Nebel</surname></persName>
		</editor>
		<meeting>the Seventeenth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="467" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Planning in nondeterministic domains under partial observability via symbolic model checking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bertoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Traverso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="473" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">solving power supply restoration problems with planning via model checking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bertoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thiebaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECAI-02</title>
		<meeting>ECAI-02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interleaving execution and planning for nondeterministic, partially observable domains</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bertoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Traverso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECAI-04</title>
		<meeting>ECAI-04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decision-theoretic planning: structural assumptions and computational leverage</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Intelligence Res. (JAIR)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="94" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast planning through planning graph analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Furst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="279" to="298" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning sorting and decision trees with POMDPs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Geffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Planning with incomplete information as heuristic search in belief space</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Geffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Artificial Intelligence Planning and Scheduling</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Chien</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Planning with incomplete information as heuristic search in belief space</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Geffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AIPS-00</title>
		<meeting>AIPS-00</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Labeled RTDP: improving the convergence of real-time dynamic programming</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Geffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICAPS-03</title>
		<meeting>ICAPS-03</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An algorithm better than AO*</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Geffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 20th National Conf. on Artificial Intelligence (AAAI-05)</title>
		<meeting>20th National Conf. on Artificial Intelligence (AAAI-05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1343" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conformant planning via heuristic forward search: a new approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICAPS-04</title>
		<meeting>ICAPS-04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using temporal logic to express search control knowledge for planning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bacchus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kabanza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="191" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Heuristic guidance measures for conformant planning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bryce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICAPS&apos;03 Workshop on Planning under Uncertainty and Incomplete Information</title>
		<meeting>ICAPS&apos;03 Workshop on Planning under Uncertainty and Incomplete Information<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A POMDP formulation of preference elicitation problems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI Proceedings</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Planning with extended goals and partial observability</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bertoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pistore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICAPS-04</title>
		<meeting>ICAPS-04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient implementation of a BDD package</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Brace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rudell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th ACM/IEEE Design Automation Conference</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="40" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph-based algorithms for boolean function manipulation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. C</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the complexity of VLSI implementations and graph representations of boolean functions with application to integer multiplication</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="213" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Symbolic boolean manipulation with ordered binary-decision diagrams</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="318" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GPT meets PSR</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thiebaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICAPS-03</title>
		<meeting>ICAPS-03</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">NUSMV: a new symbolic model checker</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giunchiglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Software Tools Technol. Transfer (STTT)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic verification of finite-state concurrent systems using temporal logic specifications</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Emerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Sistla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Programming Languages Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="244" to="263" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SAT-based planning in complex domains: concurrency, constraints and nondeterminism</title>
		<author>
			<persName><forename type="first">C</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Giunchiglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence J</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">1,2</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Acting optimally in partially observable stochastic domains</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cassandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-94</title>
		<meeting>AAAI-94<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Coudert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Madre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touati</surname></persName>
		</author>
		<title level="m">TiGeR version 1.0 user guide</title>
		<imprint>
			<date type="published" when="1993-12">December 1993</date>
		</imprint>
		<respStmt>
			<orgName>Digital Paris Research Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weak, strong, and strong cyclic planning via symbolic model checking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pistore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Traverso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="35" to="84" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conformant planning via symbolic model checking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Intelligence Res. (JAIR)</title>
		<imprint>
			<biblScope unit="page" from="305" to="338" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conformant planning via symbolic model checking and heuristic search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bertoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="127" to="206" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Strong planning in non-deterministic domains via model checking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Traverso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AIPS-98</title>
		<meeting>AIPS-98</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Formal methods: state of the art and future directions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="626" to="643" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Planning with a language for extended goals</title>
		<author>
			<persName><forename type="first">U</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pistore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Traverso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-02)</title>
		<meeting>the Eighteenth National Conference on Artificial Intelligence (AAAI-02)<address><addrLine>Edmonton, Alberta</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press/MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="447" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automata-theoretic approach to planning for temporally extended goals</title>
		<author>
			<persName><forename type="first">G</forename><surname>De Giacomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Vardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Fifth European Conference on Planning</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Biundo</surname></persName>
		</editor>
		<meeting>eeding of the Fifth European Conference on Planning<address><addrLine>Durham, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A logic programming approach to knowledge-state planning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Eiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polleres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="157" to="211" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the implementation of MIPS</title>
		<author>
			<persName><forename type="first">S</forename><surname>Edelkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Helmert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIPS-Workshop on Model-Theoretic Approaches to Planning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Vardi</surname></persName>
		</author>
		<title level="m">Reasoning about Knowledge</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Complexity issues in Markov decision processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goldsmith</surname></persName>
		</author>
		<author>
			<persName><surname>Mundhenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th Conference on Computational Complexity</title>
		<meeting>13th Conference on Computational Complexity</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic abstraction planning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Musliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Krebsbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Boddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Innovative Applications of Artificial Intelligence Conference (AAAI 97), (IAAI 97)</title>
		<meeting>the Fourteenth National Conference on Artificial Intelligence and Ninth Innovative Applications of Artificial Intelligence Conference (AAAI 97), (IAAI 97)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="680" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Using model checking to plan hard real-time controllers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Musliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pelican</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the AIPS2k Workshop on Model-Theoretic Approaches to Planning</title>
		<meeting>eeding of the AIPS2k Workshop on Model-Theoretic Approaches to Planning<address><addrLine>Breckeridge, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Time-saving tips for problem solving with incomplete information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Genesereth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nourbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hard real-time mode logic synthesis for hybrid control: a CIRCA-based approach</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Musliner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working notes of the 1999 AAAI Spring Symposium on Hybrid Control</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Hyafil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bacchus</surname></persName>
		</author>
		<title level="m">Conformant probabilistic planning via CSPs</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Proceedings of ICAPS-03</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Contingent planning via heuristic forward search with implicit belief states</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICAPS-05</title>
		<meeting>ICAPS-05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action representation and partially observable planning using epistemic logic</title>
		<author>
			<persName><forename type="first">A</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marquis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-03)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI-03)<address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">SPUDD: Stochastic planning using decision diagrams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>St-Aubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence (UAI-99)</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Laskey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Prade</surname></persName>
		</editor>
		<meeting>the 15th Conference on Uncertainty in Artificial Intelligence (UAI-99)<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">LAO*: a heuristic search algorithm that finds solutions with loops</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="35" to="62" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An efficient algorithm for searching implicit AND/OR graphs with cycles</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">OBDD-based universal planning for synchronized agents in non-deterministic domains</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Intelligence Res. (JAIR)</title>
		<imprint>
			<biblScope unit="page" from="189" to="226" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">OBDD-based optimistic and strong cyclic adversarial planning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECP-01</title>
		<meeting>ECP-01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Karlson</surname></persName>
		</author>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Conditional progressive planning under uncertainty</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Planning control rules for reactive agents</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kabanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barbeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>St-Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="113" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An algorithm for probabilistic planning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kushmerick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="239" to="286" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pushing the envelope: planning, propositional logic, and stochastic search</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-96</title>
		<meeting>AAAI-96<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Solving robot navigation problems with initial pose uncertainty using real-time heuristic search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Artificial Intelligence Planning Systems (AIPS-98)</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Simmons</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</editor>
		<meeting>the Second International Conference on Artificial Intelligence Planning Systems (AIPS-98)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">AND/OR graph heuristics search methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="51" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<title level="m">Symbolic Model Checking</title>
		<imprint>
			<publisher>Kluwer Academic</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">MAXPLAN: a new approach to probabilistic planning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Majercik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AIPS-1998</title>
		<meeting>AIPS-1998</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Additive AND/OR graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-73</title>
		<meeting>IJCAI-73<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Optimising decision trees through heuristically guided search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1025" to="1039" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Principles of Artificial Intelligence</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Nillson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Tioga Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Value-directed belief state approximation for POMDPs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Vector-space analysis of belief-state approximation for POMDPs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A knowledge-based approach to planning with incomplete information and sensing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Petrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bacchus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Artificial Intelligence Planning and Scheduling (AIPS-02)</title>
		<meeting>the Sixth International Conference on Artificial Intelligence Planning and Scheduling (AIPS-02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Extending the knowledge-based approach to planning with incomplete information and sensing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Petrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bacchus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS-04)</title>
		<meeting>the International Conference on Automated Planning and Scheduling (ICAPS-04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Planning for contingency: a decision based approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pryor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Intelligence Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="81" to="120" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Conditional nonlinear planning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on AI Planning Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Hendler</surname></persName>
		</editor>
		<meeting>the First International Conference on AI Planning Systems<address><addrLine>College Park, MD</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The complexity of Markov decision processes</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="450" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Planning as model checking for extended goals in non-deterministic domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pistore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Traverso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-01</title>
		<meeting>IJCAI-01<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Constructing conditional plans by a theorem-prover</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rintanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Intellegence Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="323" to="352" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Improvements to the evaluation of quantified boolean formulae</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rintanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1192" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Backward plan construction for planning as search in belief space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rintanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on AI Planning and Scheduling (AIPS)</title>
		<meeting>the International Conference on AI Planning and Scheduling (AIPS)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Complexity of planning with partial observability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rintanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Automated Planning and Scheduling (ICAPS&apos;04)</title>
		<meeting>the 14th International Conference on Automated Planning and Scheduling (ICAPS&apos;04)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Research on conditional planning with partial observability: the jussi-POP/BBSP planning system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rintanen</surname></persName>
		</author>
		<ptr target="http://www.informatik.uni-freiburg.de/rintanen/planning.html" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">CUDD: CU decision diagram package-release 2.1.2. Department of</title>
		<author>
			<persName><forename type="first">F</forename><surname>Somenzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997-04">April 1997</date>
		</imprint>
		<respStmt>
			<orgName>Electrical and Computer Engineering-University of Colorado at Boulder</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The optimal control of partially observable Markov decision processes over the infinite horizon: discounted costs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Sondik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="282" to="304" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Supply restoration in power distribution systems: a benchmark for planning under uncertainty</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thiebaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Cordier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECP-01</title>
		<meeting>ECP-01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">In defense of PDDL axioms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thiebaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 18th International Joint Conference on Artificial Intelligence (IJCAI-03)</title>
		<meeting>18th International Joint Conference on Artificial Intelligence (IJCAI-03)<address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Gridworlds as testbeds for planning with incomplete information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tovey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koenig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence</title>
		<meeting>the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="819" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Anytime state-based solution methods for decision processes with non-Markovian rewards</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thiebaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kabanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Extending graphplan to handle uncertainty and sensing actions and sensing actions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th National Conference on Artificial Intelligence (AAAI-98) and of the 10th Conference on Innovative Applications of Artificial Intelligence (IAAI-98)</title>
		<meeting>the 15th National Conference on Artificial Intelligence (AAAI-98) and of the 10th Conference on Innovative Applications of Artificial Intelligence (IAAI-98)<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A performance study of BDD-Based model checking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>O'hallaron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Coudert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Somenzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Formal Methods on Computer-Aided Design</title>
		<imprint>
			<biblScope unit="page" from="255" to="289" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
