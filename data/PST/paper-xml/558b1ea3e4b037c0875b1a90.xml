<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ON ACOUSTIC SURVEILLANCE OF HAZARDOUS SITUATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stavros</forename><surname>Ntalampiras</surname></persName>
							<email>sntalampiras@upatras.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Patras</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Music Technology and Acoustics</orgName>
								<orgName type="institution">Technological Educational Institute of Crete</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilyas</forename><surname>Potamitis</surname></persName>
							<email>potamitis@stef.teicrete.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Patras</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikos</forename><surname>Fakotakis</surname></persName>
						</author>
						<title level="a" type="main">ON ACOUSTIC SURVEILLANCE OF HAZARDOUS SITUATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">143332C9596AE4A8D7D423F8402579EA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>acoustic surveillance</term>
					<term>content based audio recognition</term>
					<term>MPEG-7</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The present study presents a practical methodology for automatic space monitoring based solely on the perceived acoustic information. We consider the case where atypical situations such as screams, explosions and gunshots take place in a metro station environment. Our approach is based on a two stage recognition schema, each one exploiting HMMs for approximating the density function of the corresponding sound class. The main objective is to detect abnormal events that take place in a noisy environment. A thorough evaluation procedure is carried out under different SNR conditions and we report high detection rates with respect to false alarm and miss probabilities rates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Research in the area of automatic surveillance systems is mainly focused on detecting abnormal events based on the acquired video information <ref type="bibr" target="#b1">[1]</ref>. Current implementations typically consist of a large number of cameras distributed in an area and connected to a central control room. While this kind of analysis provides valuable information we concentrate on detecting atypical events by exploiting only the acoustic modality. This approach offers several advantages such as: a) low computational needs, b) the illumination conditions of the space to be monitored and possible occlusion do not have an immediate affect on sound. Previous approaches on the subject of acoustic monitoring include cases such as in <ref type="bibr" target="#b2">[2]</ref> where a gunshot detection system is presented based on features derived from the time-frequency domain and GMM classifier. They use different SNRs during the training phase for achieving 10% and 5% false rejection and false detection rate respectively. In <ref type="bibr" target="#b3">[3]</ref> they present an emotional recognition scheme for public safety. Their main objective is fear vs. neutral classification and by using different models for voiced and unvoiced speech they reach 30% error rate. In <ref type="bibr" target="#b4">[4]</ref> they report on building a parallel classification system based on GMMs for discrimination of ambient noise, scream and gunshot sounds. After a feature selection algorithm they result in 90% precision and 8% false rejection rate. Last but not least, an audio-based surveillance system in a typical office environment is described in <ref type="bibr" target="#b5">[5]</ref>. The background noise model is continuously updated for serving interesting event detection while both supervised and k-means data clustering are inspected. In <ref type="bibr">[6]</ref> audio data recorded using simultaneously 4 microphones are classified with two different approaches -GMM and SVM -for shout detection in a railway environment. The proposed implementation exploits PLP features combined with the SVM classifier.</p><p>The main goal of this paper is to efficiently characterize the acoustic environment in terms of threatening/non-threatening conditions while using a single microphone. The outcome of the system is to help/warn authorized personnel to take the appropriate actions for preventing crime and/or property damage. In order for such an implementation to be useful and practical it must offer very low false alarm rate while keeping detection accuracy as high as possible under noisy conditions. Our approach is basically motivated by the fact that sound provides information that is hard or impossible to obtain by any other means. On top of that, such a method comprises a low cost and relatively easy during setup, solution. In this article we concentrate on detecting atypical sound events (scream, gunshot and explosion) in a metro station environment. The current methodology is inspired by the work of Wilpon et al <ref type="bibr" target="#b7">[7]</ref> regarding keyword spotting. We extend this idea to the field of keysound spotting, where screams, gunshots and explosions are considered as keysound effects. In our case the noninteresting/garbage model is the metro station soundscene which presents highly non-stationary properties (it includes horns, opening/closing doors, people talking in the background, train movement etc). We have carried out extensive experimentation regarding the best set of features to be included in the feature extraction process. The final set is consisted of the well known Mel frequency cepstral coefficients augmented by a second group of parameters based on the MPEG-7 audio standard. Subsequently feature sequences are modeled by probability density functions represented by GMMs and HMMs. The next of this paper is organized as follows: in section 2 a brief overview of the system is given along with the description of MFCC and MPEG-7 sets of parameters. Section 3 explains the experimental protocol that was used while our conclusions are reported in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM OVERVIEW</head><p>Our system is designed as a two stage topology which was proven to provide better recognition rates than the single stage one. The incoming signal is first classified as normal (metro station environment) or abnormal (scream, gunshot or explosion) and in case it is decided to be abnormal the system proceeds into a second processing stage where the type of abnormality is identified. The proposed architecture comprises a fully probabilistic structure based on ergodic HMMs for describing each sound category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature Extraction Analysis</head><p>In this section we comment on the groups of descriptors that were employed in order to train probabilistic models that represent the a priori knowledge we have about the sound classes. We make use of the Mel-scale filterbank because of its ability to lower the dimensionality of the Fourier transformed vector. We also include the logarithmic portioning of the data, a process which mimics the natural frequency selectivity of the human middle ear to some extent. Secondly, the MPEG-7 protocol is employed since it currently constitutes the state of the art mechanism for automated content-based generic audio recognition. We adopt the next four low-level descriptors: Waveform Min, Waveform Max, Audio Fundamental Frequency and Audio Spectrum Flatness (ASF). The above mentioned feature sets were evaluated both separately and combined using different values of parameters. They were chosen because they capture different aspects of the information provided by the MFCC features. MFCCs are a Mel-scaled projection of the log spectra while ASF constitutes a higher level description of the audio waveform indicating how flat a given signal is. It should be noted that the incorporation of a set of parameters based on Teager energy operator (critical band based TEO autocorrelation envelope area) proposed in <ref type="bibr" target="#b8">[8]</ref> was also tested. They are used for stress classification but their combination with the above mentioned features did not provide improved results. In this work we are dealing with keysound spotting under subway environment acoustic signals characterized by a long duration need to be processed for the purpose of atypical sound event detection. Thus the instantaneous value of each feature is computed over a larger frame size than the one commonly used in speech recognition (namely 30ms). After several experiments it was decided that all sound samples should be cut into frames of 200ms with 50% overlap. Mean removal and variance scaling are also applied. A short analysis of the feature extraction processes follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mel-Frequency Cepstral Coefficients</head><p>For MFCC's derivation we compute the power of the short time Fourier transform for every frame and pass them through a triangular Mel scale filterbank so that signal components which play an important role to human perception are emphasized. Afterwards the data are compressed and decorrelated using the logarithmic scale and the discrete cosine transform respectively. Thirteen coefficients are kept (including the 0-th coefficient which reflects upon the energy of the signal) and in combination with their respective derivatives a twenty six-dimension vector is formed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPEG-7 Audio Protocol Descriptors</head><p>Provide a general framework for efficient audio management. Furthermore, it includes a group of fundamental descriptors and description schemes for indexing and retrieval of audio data. We employed three audio descriptors namely: Spectrum Flatness (ASF), Waveform (AWF) and Fundamental Frequency (AFF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Classification Schemas</head><p>We employed Gaussian mixture models and Hidden Markov models with two different topologies (left-right and fullyconnected). Subsequently the previously created models are used for computing a degree of resemblance (e.g. log-likelihood) between each model and an unknown input signal. This type of score is compared against the rest and the final decision is made with a simple maximum log-likelihood determination. Torch implementation (provided at http://www.torch.ch) of GMM and HMM, written in C++ was used during the whole process. The maximum number of K-means iterations for initialization was 50 while both the EM and Baum-Welch algorithms had and upper limit of 25 iterations with a threshold of 0.001 between subsequent iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SET-UP</head><p>Natural corpora with extreme emotional manifestation and atypical sounds events for surveillance applications are not publicly available because of the private character of the data, their scarcity and unpredictability. Our corpus consists of audio acquired from professional sound effects collections. These kinds of collections comprise an enormous source of high quality recordings used by the movie industry. An important detail, which is not widely known, is that the audio in a movie is not the exact audio recorded at a scene but it is processed and in most cases added separately to the audio stream later. Therefore, there is a vast corpus of real vocal and non-vocal audio available for the construction of trained probabilistic classification models. Sound samples from the following compilations: (i) BBC Sound Effects Library, (ii) Sound Ideas Series 6000, (iii) Sound Ideas: the art of Foley, (iv) Best Service Studio Box Sound Effects and (v) sound effects from internet search constructed the final corpus.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Construction and Recognition Accuracy</head><p>The data belonging to each class were splitted into 75% for training and 25% for testing in a random way. A fully-connected HMM was built for each category to capture this property while testing consists of a simple comparison of log-likelihoods. Due to the system architecture we first constructed two kinds of models: typical (metro station soundscape) and atypical (including explosion, gunshot and scream). After extensive experimentations we used 6 states each one modeled with 19 Gaussian components and 98.87% average recognition rate was achieved. Regarding the second stage three HMMs were built for describing each atypical situation. The same parameters provided the highest average recognition accuracy -93.05% -and the corresponding confusion matrix is tabulated in Table <ref type="table" target="#tab_1">2</ref>. We observe that scream sound events are recognized with the best accuracy. This is due to the different spectral/energy distribution that scream vocal reactions exhibit when compared with the rest of the classes. The lowest accuracy is obtained regarding to explosion sound events, of which 11.62% is misclassified as gunshots. Many of the errors occur because of the great variability among sound samples of the same category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Responded</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Atypical Sound Event Detection in a Metro Station</head><p>Emergency situations located in a metro station were created by merging abnormal sound events with subway recordings at different SNRs (from -5dB to 15dB with 5dB step). The proposed architecture was tested using Detection Error Tradeoff (DET). Two series of experiments were conducted dedicated to each stage of our implementation. The DET curves for both stages are depicted in Fig. <ref type="figure" target="#fig_1">2</ref> and Fig. <ref type="figure">3</ref> for stage 1 and stage 2 respectively. Figure <ref type="figure" target="#fig_1">2</ref> provides results of atypical event detection regarding to all three different sound events. The log-likelihood values of two statistical models (typical/atypical) were utilized for the DET curves creation.</p><p>Results follow a rapid degradation when the SNR condition of the test signals decreases. Abnormal sounds are adequately detected even at extremely low SNR conditions. Average equal error rate (EER) regarding all types of events at -5dB SNR is 8.53% while its minimum value (best detection rate) is achieved in the gunshot class. The audio signals that are most vulnerable to background noise corruption are the explosion ones with 12.88% EER at -5dB SNR. For surveillance tasks an energy ratio of 0dB represents the real world conditions appropriately. The proposed framework demonstrates very good performance in the respective ratio, having EER of 4.8% and false alarm probability of 1.83% which is particularly important for this kind of applications. Figure <ref type="figure">3</ref> illustrates system's capabilities regarding the detection of each atypical sound category alone merged with metro station recordings at different SNR levels. The misclassifications that occur at this processing level comprise errors that are of less importance in comparison to the previous ones. Here a threatening situation has been detected and the system tries to identify which type of abnormality is present while an authorized person has already been alerted in order to take the appropriate action. Thus, at this stage of recognition our main interest is to obtain very low miss probability and then try to have as low false alarm rate as possible. The output log-likelihoods obtained by the probabilistic models which describe each atypical sound class, were used during this phase. We can observe that gunshot events are detected with relatively low EERs across all SNR values in contrast with the two other kinds of atypical events which are detected with satisfying accuracy. As expected, miss detection probability falls as the SNR conditions increase from -5dB to 15dB. More precisely explosion sounds, corrupted by metro station environmental noise with -5dB ratio are detected with EER of 13.2%, gunshot sounds with 24.5% and scream sounds with 28.2%. Additionally, our implementation provides very good false alarm probability with a mean value of 6% among the three sound event categories with 0dB SNR conditions. The corresponding EERs achieved by the system as regards explosion, gunshot and scream detection are 8.54%, 24.5% and 21.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this work we presented and evaluated a two stage probabilistic framework for acoustic monitoring in a metro station environment. Its main aim is to identify on time the sensed situation and deliver the necessary warning messages to an authorized officer. The proposed methodology is practical, can operate in real-time and elaborates on three abnormal sound events corrupted by highly non-stationary metro station. The recognition results under a variety of background environmental noise are very encouraging. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of the probabilistic based acoustic surveillance system.</figDesc><graphic coords="2,92.85,72.34,429.06,177.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. 1 st stage DET curves regarding to atypical sound events as the target class under different SNRs. Each sub-figure corresponds to results obtained by mixing the subway signal with one of the three atypical sound events.</figDesc><graphic coords="4,93.83,393.67,172.44,250.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The parts of the final corpus</figDesc><table><row><cell>Category</cell><cell>Number of recordings</cell><cell>Duration (sec)</cell></row><row><cell>Explosion</cell><cell>131</cell><cell>13.77</cell></row><row><cell>Gunshot</cell><cell>187</cell><cell>32.94</cell></row><row><cell>Scream</cell><cell>270</cell><cell>4.04</cell></row><row><cell>Subway</cell><cell>32</cell><cell>44.88</cell></row><row><cell>Total</cell><cell>620</cell><cell>23.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Confusion Matrix for three Atypical Sound Events (%)</figDesc><table><row><cell></cell><cell cols="3">Explosion Gunshot Scream</cell></row><row><cell>Presented</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Explosion</cell><cell>86.06</cell><cell>11.62</cell><cell>2.32</cell></row><row><cell>Gunshot</cell><cell>1.72</cell><cell>93.10</cell><cell>5.17</cell></row><row><cell>Scream</cell><cell>0</cell><cell>0</cell><cell>100</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGMENTS</head><p>This work is under the EC FP 7 th grant Prometheus 214901.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">W4: real-time surveillance of people and their activities</title>
		<author>
			<persName><forename type="first">I</forename><surname>Haritaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="809" to="830" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event detection for an audio-based surveillance system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ehrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fear-type emotion recognition for future audio-based surveillance systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ehrette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="page" from="487" to="503" />
			<date type="published" when="2008">2008</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scream and gunshot detection in noisy environments</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gerosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valenzise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Antonacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP</title>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
			<pubPlace>Poznan, Poland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic surveillance of the acoustic activity in our living environment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Harma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skowronek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Audio Events Detection in Public Transport Vehicle</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Rouas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ambellouis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference</title>
		<meeting><address><addrLine>Toronto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic Recognition of Keywords in Unconstrained Speech Using Hidden Markov Models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Wilpon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="1870" to="1878" />
			<date type="published" when="1990-11">November 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fig. 3. 2 nd stage DET curves each one corresponding to a specific atypical sound event as the target class under different SNRs. The sub-figures from up to bottom refer to results obtained for explosion, gunshot and scream audio categories respectively</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<date type="published" when="2001-03">March 2001</date>
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
	<note>Nonlinear Feature Based Classification of Speech Under Stress</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
