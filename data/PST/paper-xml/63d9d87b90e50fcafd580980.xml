<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Object Detection via Variational Feature Aggregation</title>
				<funder ref="#_TXnmR68">
					<orgName type="full">National Nature Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Rv6Qrsp #_rMSWaMV">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-01-31">31 Jan 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaming</forename><surname>Han</surname></persName>
							<email>hanjiaming@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">NERCMS</orgName>
								<orgName type="institution" key="instit2">Wuhan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab. LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqiang</forename><surname>Ren</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Ding</surname></persName>
							<email>jian.ding@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">NERCMS</orgName>
								<orgName type="institution" key="instit2">Wuhan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab. LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
							<email>guisong.xia@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">NERCMS</orgName>
								<orgName type="institution" key="instit2">Wuhan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab. LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Object Detection via Variational Feature Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-01-31">31 Jan 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2301.13411v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As few-shot object detectors are often trained with abundant base samples and fine-tuned on few-shot novel examples, the learned models are usually biased to base classes and sensitive to the variance of novel examples. To address this issue, we propose a meta-learning framework with two novel feature aggregation schemes. More precisely, we first present a Class-Agnostic Aggregation (CAA) method, where the query and support features can be aggregated regardless of their categories. The interactions between different classes encourage class-agnostic representations and reduce confusion between base and novel classes. Based on the CAA, we then propose a Variational Feature Aggregation (VFA) method, which encodes support examples into class-level support features for robust feature aggregation. We use a variational autoencoder to estimate class distributions and sample variational features from distributions that are more robust to the variance of support examples. Besides, we decouple classification and regression tasks so that VFA is performed on the classification branch without affecting object localization. Extensive experiments on PASCAL VOC and COCO demonstrate that our method significantly outperforms a strong baseline (up to 16%) and previous state-of-the-art methods (4% in average).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>This paper studies the problem of few-shot object detection (FSOD), a recently-emerged challenging task in computer vision <ref type="bibr" target="#b38">(Yan et al. 2019;</ref><ref type="bibr" target="#b15">Kang et al. 2019)</ref>. Different from generic object detection <ref type="bibr" target="#b10">(Girshick et al. 2014;</ref><ref type="bibr" target="#b27">Redmon et al. 2016;</ref><ref type="bibr" target="#b28">Ren et al. 2017)</ref>, FSOD assumes that we have abundant samples of some base classes but only a few examples of novel classes. Thus, a dynamic topic is how to improve the recognition capability of FSOD on novel classes by transferring the knowledge of base classes to novel ones.</p><p>In general, FSOD follows a two-stage training paradigm. In stage-I, the detector is trained with abundant base samples to learn generic representations required for the object detection task, such as object localization and classification. In stage-II, the detector is fine-tuned with only K shots (K=1, 2, 3, . . . of this paradigm, the learned models are usually biased to base classes due to the imbalance between base and novel classes. As a result, the model will confuse novel objects with similar base classes. See Fig. <ref type="figure">5</ref> (top) for an instance, the novel class, cow, has high similarities with several base classes such as dog, horse and sheep. Besides, the model is sensitive to the variance of novel examples. Since we only have K shots examples per class, the performance highly depends on the quality of the support sets. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, appearance variations are common in FSOD. Previous methods <ref type="bibr" target="#b38">(Yan et al. 2019)</ref> consider each support example as a single point in the feature space and average all features as class prototypes. However, it is difficult to estimate the real class centers with a few examples.</p><p>In this paper, we propose a meta-learning framework to address this issue. Firstly, we build a strong meta-learning baseline based on Meta R-CNN <ref type="bibr" target="#b38">(Yan et al. 2019)</ref>, which even outperforms a representative two-stage fine-tuning approach TFA <ref type="bibr" target="#b32">(Wang et al. 2020)</ref>. By revisiting the feature aggregation module in meta-learning frameworks, we propose Class-Agnostic Aggregation (CAA) and Variational Feature Aggregation (VFA) to reduce class bias and improve the robustness to example's variances, respectively.</p><p>Feature aggregation is a crucial design in FSOD, which defines how query and support examples interact. Previous works such as Meta R-CNN adopt a class-specific aggregation scheme (Fig. <ref type="figure" target="#fig_1">2 (a</ref>)), i.e., query features are aggregated with support features of the same class, ignoring cross-class interactions. In contrast, we propose CAA (Fig. <ref type="figure" target="#fig_1">2 (b)</ref>) which allows feature aggregation between different classes. Since CAA encourages the model to learn class-agnostic representations, the bias towards base classes is reduced. Besides, the interactions between different classes simultaneously model class relations so that novel classes will not be confused with base classes.</p><p>Based on CAA, we propose VFA which encodes support examples into class-level support features. Our motivation is that intra-class variance (e.g. appearance variations) is shared across classes and can be modeled with common distributions <ref type="bibr" target="#b24">(Lin et al. 2018</ref>). So we can use base classes' distributions to estimate novel classes' distributions. We achieve this by modeling each class as a common distribution with variational autoencoders (VAEs). We firstly train the VAE on abundant base examples and then fine-tune it on few-shot novel examples. By transferring the learned intra-class variance to novel classes, our method can estimate novel classes' distributions with only a few examples (Fig. <ref type="figure" target="#fig_0">1</ref>). Finally, we sample support features from distributions and aggregate them with query features to produce more robust predictions.</p><p>We also propose to decouple classification and regression tasks so that our feature aggregation module can focus on learning translation-invariant features without affecting object localization. We conduct extensive experiments on two FSOD datasets, PASCAL VOC <ref type="bibr" target="#b5">(Everingham et al. 2010)</ref> and COCO <ref type="bibr" target="#b23">(Lin et al. 2014)</ref> to demonstrate the effectiveness of our method. We summarize our contributions as follows:</p><p>? We build a strong meta-learning baseline Meta R-CNN++ and propose a simple yet effective Class-Agnostic Aggregation (CAA) method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Generic Object Detection. Object detection has witnessed significant progress in the past decade, which can be roughly divided into two groups: one-stage and two-stage detectors.</p><p>One-stage detectors predict bounding boxes and class labels by presetting dense anchor boxes <ref type="bibr" target="#b27">(Redmon et al. 2016;</ref><ref type="bibr" target="#b25">Liu et al. 2016;</ref><ref type="bibr" target="#b22">Lin et al. 2017)</ref>, points <ref type="bibr" target="#b18">(Law and Deng 2018;</ref><ref type="bibr" target="#b42">Zhou, Wang, and Kr?henb?hl 2019)</ref>, or directly output sparse predictions <ref type="bibr" target="#b1">(Carion et al. 2020;</ref><ref type="bibr" target="#b3">Chen et al. 2021)</ref>. Two-stage detectors <ref type="bibr" target="#b10">(Girshick et al. 2014;</ref><ref type="bibr" target="#b9">Girshick 2015;</ref><ref type="bibr" target="#b28">Ren et al. 2017)</ref> first generate a set of object proposals with Region Proposal Network (RPN) and then perform proposal-wise classification and regression. However, most generic detectors are trained with abundant samples and not designed for data-scarce scenarios.</p><p>Few-Shot Object Detection. Early attempts <ref type="bibr" target="#b15">(Kang et al. 2019;</ref><ref type="bibr" target="#b38">Yan et al. 2019;</ref><ref type="bibr" target="#b33">Wang, Ramanan, and Hebert 2019)</ref> in FSOD adopt meta-learning architectures. FSRW <ref type="bibr" target="#b15">(Kang et al. 2019)</ref> and Meta R-CNN <ref type="bibr" target="#b38">(Yan et al. 2019</ref>) aggregate image/RoI-level query features with support features generated by a meta learner. Following works explore different designs of meta-learning architectures, e.g., feature aggregation scheme <ref type="bibr" target="#b36">(Xiao and Marlet 2020;</ref><ref type="bibr" target="#b6">Fan et al. 2020;</ref><ref type="bibr" target="#b14">Hu et al. 2021;</ref><ref type="bibr">Zhang et al. 2021;</ref><ref type="bibr" target="#b11">Han et al. 2021</ref>) and feature space augmentation <ref type="bibr">(Li et al. 2021a;</ref><ref type="bibr">Li and Li 2021</ref> Meta-Learning Based FSOD. We take Meta R-CNN <ref type="bibr" target="#b38">(Yan et al. 2019</ref>) for an example. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the main framework is a siamese network with a query feature encoder F Q , a support feature encoder F S , a feature aggregator A and a detection head F D . Typically, F Q and F S share most parameters and A refers to the channel-wise product operation. Meta R-CNN follows the episodic training paradigm <ref type="bibr" target="#b31">(Vinyals et al. 2016)</ref>. Each episode is composed of a set of support images and binary masks of annotated objects, {x i , M i } N i=1 , where N is the number of training classes. Specifically, we first feed the support set {x i , M i } N i=1 to F S to generate class-specific support features {S i } i?C , and the query image to F Q to generate a set of RoI features {Q m } (m is the index of RoIs). Then we aggregate each Q m and S i with the feature aggregator A. Finally, the aggregated features Q m i are fed to the detection head F D to produce final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta R-CNN++: Stronger Meta-Learning Baseline</head><p>Meta-learning has proved a promising approach, but the fine-tuning based approach receives more and more attention recently due to its superior performance. Here we aim to bridge the gap between the two approaches. We choose Meta R-CNN and TFA as baselines and explore how to build a strong FSOD baseline with meta-learning.</p><p>Although both methods follow a two-stage training paradigm, TFA optimizes the model with advanced techniques in the fine-tuning stage: (a) TFA freezes most network parameters, and only trains the last classification and regression layers so that the model will not overfit to fewshot examples. (b) Instead of randomly initializing the classification layer, TFA copies pre-trained weights of base classes and only initializes the weights of novel classes. (c) TFA adopts cosine classifier <ref type="bibr" target="#b8">(Gidaris and Komodakis 2018)</ref> rather than a linear classifier.</p><p>Considering the success of TFA, we build Meta R-CNN++, which follows the architecture of Meta R-CNN but aligns most hyper-parameters with TFA. Here we explore different design choices to mitigate the gap between Different from TFA, Meta R-CNN++ with the cosine classifier does not surpass the linear classifier in nAP (41.6 vs. 42.0), but its performance on base classes is better than the linear classifier (68.2 vs. 64.9). (c) Alleviate base forgetting. We follow TFA and copy the pre-trained classifier weights of base classes. We find Meta R-CNN++ can also maintain the performance on base classes (76.8 vs. 77.6).</p><p>The above experiments indicate that meta-learning remains a promising approach for FSOD as long as we carefully handle the fine-tuning stage. Therefore, we choose Meta R-CNN++ as our baseline in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-Agnostic Aggregation</head><p>Feature aggregation is an important module in meta-learning based FSOD <ref type="bibr" target="#b15">(Kang et al. 2019;</ref><ref type="bibr" target="#b38">Yan et al. 2019)</ref>. Many works adopt a class-specific aggregation (CSA) scheme. Let us assume that a query image has an object of class C Q = {i} and the corresponding RoI features {Q m i }. In the training phase, as shown in Fig. <ref type="figure" target="#fig_1">2</ref> (a), CSA aggregates each RoI feature Q m i with the support features S i of the same class:</p><formula xml:id="formula_0">Q m ii = A(Q m i , S i ).</formula><p>In the testing phase, CSA aggregates the RoI feature with support features of all classes:</p><formula xml:id="formula_1">Q m ij = A(Q m i , S j ), j ? C,</formula><p>and each support feature S j is to predict objects of its corresponding class. Notably, if the query image contains multiple classes, CSA aggregates the query features with each support feature in</p><formula xml:id="formula_2">C Q : Q m ij = A(Q m i , S j ), j ? C Q .</formula><p>But CSA still follows the classspecific way, as support features not belonging to C Q will never be aggregated with the query feature.</p><p>As discussed before, the learned models are usually biased to base classes due to the imbalance between base and novel classes. Therefore, we revisit CSA and propose a simple yet effective Class-Agnostic Aggregation (CAA). See </p><formula xml:id="formula_3">Q m ij * = A(Q m i , S j * ), j * ? C.<label>(1)</label></formula><p>Then we feed the aggregated feature</p><formula xml:id="formula_4">Q m ij * to the detection head F D to output classification scores p = F D ( Q m ij * )</formula><p>, which is supervised with the label of class i. Note that CAA is used for training; the testing phase still follows CSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variational Feature Aggregation</head><p>Prior works usually encode support examples into single feature vectors that are difficult to represent the whole class distribution. Especially when the data is scarce and example's variations are large, we cannot make an accurate estimation of class centers. Inspired by recent progress in variational feature learning <ref type="bibr" target="#b24">(Lin et al. 2018;</ref><ref type="bibr" target="#b39">Zhang et al. 2019;</ref><ref type="bibr" target="#b37">Xu et al. 2021)</ref>, we transform support features into class distributions with VAEs. Since the estimated distribution is not biased to specific examples, features sampled from the distribution are robust to the variance of support examples. Then we can sample class-level features for robust feature aggregation. The framework of VFA is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Variational Feature Learning. Formally, we aim to transform the support feature S into a class distribution N , and sample the variational feature z from N for feature aggregation. We optimize the model in a similar way to VAEs, but our goal is to sample the latent variable z instead of the reconstructed feature S . Following the definition of VAEs, we assume z is generated from a prior distribution p(z) and S is generated from a conditional distribution p(S|z). As the process is hidden and z is unknown, we model the posterior distribution with variational inference. More specifically, we approximate the true posterior distribution p(z|S) with another distribution q(z|S) by minimizing the Kullback-Leibler (KL) divergence:</p><formula xml:id="formula_5">D KL (q(z|S)||p(z|S)) = q(z|S) log q(z|S) p(z|S) ,<label>(2)</label></formula><p>which is equivalent to maximizing the evidence lower bound (ELBO):</p><formula xml:id="formula_6">ELBO = E q(z|S) [log p(S|z))] -D KL (q(z|S)||p(z)).<label>(3)</label></formula><p>Here we assume the prior distribution of z is a centered isotropic multivariate Gaussian, p(z) = N (0, I), and set the posterior distribution q(z|S) to be a multivariate Gaussian with diagonal covariance: q(z|S) = N (?, ?). The parameters ? and ? can be implemented by a feature encoder F enc : ?, ? = F enc (S). Then we obtain the variational feature z with the reparameterization trick (Kingma and Welling 2013): z = ? + ? ? , where ? N (0, I). The first term of Eq. 3 can be simplified to a reconstruction loss L rec which is usually defined as the L2 distance between the input S and the reconstructed target S ,</p><formula xml:id="formula_7">L rec = S -S = S -F dec (z) ,</formula><p>(4) where F dec denotes a feature decoder. As for the second term of Eq. 3, we directly minimize the KL divergence of q(z|S) and p(z),</p><formula xml:id="formula_8">L KL = D KL (q(z|S)||p(z)),</formula><p>(5) which forces the variation feature z to follow a normal distribution.</p><p>By optimizing the two objectives, L rec and L KL , we transform the support feature S into a distribution N . Then we can sample the variational feature z from N . Since z still lacks class-specific information, we apply a consistency loss L cons to the reconstructed feature S , which is defined as the cross-entropy between S and its class label c, where F S cls denotes a linear classifier. The introduction of L cons transforms the learned distributions into class-specific distributions. The support feature S i is forced to approximate a parameterized distribution N (? i , ? i ) of class i, so that the sampled z can preserve class-specific information. Variational Feature Aggregation. Since the support features are transformed into class distributions, we can sample features from the distribution and aggregate them with query features. Compared with the original support feature S and reconstructed feature S , the latent variable z contains more generic features of the class <ref type="bibr" target="#b39">(Zhang et al. 2019;</ref><ref type="bibr" target="#b24">Lin et al. 2018)</ref>, which is robust to the variance of support examples.</p><formula xml:id="formula_9">L cons = L CE (F S cls (S ), c),<label>(6)</label></formula><p>Specifically, VFA follows the class-agnostic approach in CAA but aggregates the query feature Q with a variational feature z. Given a query feature Q i of class i and support feature S j of class j, we firstly approximate the class distribution N (? j , ? j ) and sample a variational feature z j = ? j + ? j from N (? j , ? j ). Then we aggregate them together with the following equation:</p><formula xml:id="formula_10">Q ij = A(Q i , z j ) = Q i sig(z j ),<label>(7)</label></formula><p>where means channel-wise multiplication and sig is short for the sigmoid operation. In the training phase, we randomly select a support feature S j (i.e., one support class j) for aggregation. In the testing phase (especially K &gt; 1), we average K support features of class j into one Sj , and approximate the distribution N (? j , ? j ) with the averaged feature, ? j , ? j = F enc ( Sj ). Instead of adopting complex distribution estimation methods, we find the averaging approach works well in our method. Network and Objective. VFA only introduces a light encoder F enc and decoder F dec . F enc contains a linear layer and two parallel linear layers to produce ? and ?, respectively. F dec consists of two linear layers to generate the reconstructed feature S . We keep all layers the same dimension (2048 by default). VFA is trained in an end-to-end man-ner with the following multi-task loss:</p><formula xml:id="formula_11">L = L rpn + L reg + L cls + L cons + L rec + ?L KL ,<label>(8)</label></formula><p>where L rpn is the total loss of RPN, L reg is the regression loss, and ? is a weight coefficient (?=2.5?10 -4 by default).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification-Regression Decoupling</head><p>Generally, the detection head F D contains a shared feature extractor F share and two separate network F cls and F reg for classification and regression, respectively. In previous works, the aggregated feature is fed to F D to produce both classification scores and bounding boxes. However, the classification task requires translation-invariant features, while regression needs translation-covariant features <ref type="bibr" target="#b26">(Qiao et al. 2021)</ref>. Since support features are always translationinvariant to represent class centers, the aggregated feature harms the regression task. Therefore, we decouple the two tasks in the detection head. Let Q and Q denote the original and aggregated query features. Previous methods take Q for both tasks, where the classification score p and predicted bounding boxes b are defined as:</p><formula xml:id="formula_12">p = F cls (F share ( Q)), b = F reg (F share ( Q)). (9)</formula><p>To decouple these tasks, we adopt separate feature extractors and use the original query feature Q for regression,</p><formula xml:id="formula_13">p = F cls (F cls share ( Q)), b = F reg (F reg share (Q)),<label>(10)</label></formula><p>where F cls share and F reg share are the feature extractor for classification and regression, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setting</head><p>Datasets. We evaluate our method on PASCAL VOC (Everingham et al. 2010) and COCO <ref type="bibr" target="#b23">(Lin et al. 2014)</ref>  <ref type="bibr" target="#b32">(Wang et al. 2020)</ref> 10.0 13.7 Retentive <ref type="bibr" target="#b7">(Fan et al. 2021)</ref> 10.5 13.8 FSOD-UP <ref type="bibr" target="#b34">(Wu et al. 2021)</ref> 11.0 15.6 SRR-FSD <ref type="bibr" target="#b43">(Zhu et al. 2021)</ref> 11.3 14.7 CGDP+FSCN <ref type="bibr">(Li et al. 2021b)</ref> 11.3 15.1 FSCE <ref type="bibr" target="#b30">(Sun et al. 2021)</ref> 11.9 16.4 FADI <ref type="bibr" target="#b0">(Cao et al. 2021)</ref> 12.2 16.1 DeFRCN <ref type="bibr">(Qiao et</ref>   more robust to the variance of few-shot examples. Besides, we notice that our gains are stable and consistent. This phenomenon demonstrates that VFA is not biased to specified class sets and can be generalized to more common scenarios. Furthermore, VFA obtains a 56.1% average score and surpasses the second-best result by 4.2%, which further demonstrates its effectiveness. COCO. As shown in Tab. 3, VFA achieves the best nAP among meta-learning based methods and second-best results among all methods. We notice that a fine-tuning based method, DeFRCN <ref type="bibr" target="#b26">(Qiao et al. 2021</ref>), outperforms our method in nAP. To concentrate on the feature aggregation module in meta-learning, we do not utilize advanced techniques, e.g., the gradient decoupled layer <ref type="bibr" target="#b26">(Qiao et al. 2021)</ref> in DeFRCN. We believe the performance of VFA can be further boosted with more advanced techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>We conduct a series of ablation experiments on Novel Set 1 of PASCAL VOC.</p><p>Effect of different modules. As shown in Tab. 4, we evaluate the effect of different modules by gradually applying the proposed modules to Meta R-CNN++. Although Meta R-CNN++ is competitive enough, we show CRD improves the performance on nAP, where the absolute gains exceed 4%. Besides, we find CRD significantly improves the recall on all classes (Fig. <ref type="figure" target="#fig_3">4</ref>) and narrows the gap between base and novel classes because it uses separate networks to learn translation-invariant and -covariant features. Then, we apply CAA to the model and obtain further improvements. The confusions between different classes are reduced. Finally, we build VFA and achieve a new state-of-the-art. The 1-shot performance is even comparable with 5-shot Meta R-CNN++ in nAP, indicating that VFA is robust to the variance of support examples especially when the data is scarce. Visual analysis of different feature aggregation. Fig. <ref type="figure">5</ref> gives a visual analysis of different feature aggregation methods. Due to the imbalance between base and novel classes, some novel classes are confused with base classes in Meta R-CNN++ (with CSA), e.g., a novel classe, cow have higher similarity (&gt;0.8) with horse and sheep. In contrast, CAA reduces class bias and confusion by learning class-agnostic representations. The inter-class similarities are also reduced so that a novel example will not be classified to base classes.  Additional Main Results.</p><p>Results on Generalized FSOD. We evaluate our method on the Generalized FSOD benchmark <ref type="bibr" target="#b32">(Wang et al. 2020</ref>).</p><p>The result is an average of multiple random seeds. Following <ref type="bibr" target="#b26">(Qiao et al. 2021)</ref>, we report nAP of different methods with 10 random seeds. Since many methods only report their results on the traditional FSOD benchmarks, we collect as many methods that report the G-FSOD results as possible.</p><p>PASCAL VOC: Similar to the results of our main paper, our method performs well on PASCAL VOC. As shown in Tab. 7, our method achieves the best (12/15) or second-best (3/15) among all settings. Especially when the shot is low, our method shows significant improvements. For example, our 1-shot gains are 7.2%, 4.2% and 8.8% on the Novel Set 1, 2 and 3, respectively. COCO: We also compare VFA with other methods on COCO, where our method achieves the second-best results on nAP. We notice that the gap between VFA and DeFRCN is narrowed in the G-FSOD setting (0.9% vs. 2.3% on 10-shot nAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Ablation Studies</head><p>Design of VFA. By default, the feature encoder F enc and decoder F dec consist of one input layer and output layer of 1024-d. In Fig. <ref type="figure">7</ref>, we ablate the number of input layers and hidden channels. VFA is sensitive to these hyper-parameters when the shot is low (up to 4% in 1 shot). The performance becomes more stable as the shot increases, e.g., the gap between different settings is reduced to 1% in 3 and 5 shots.</p><p>VFA with/without fine-tuning. In the few-shot fine-tuning stage, we fine-tune the variational feature encoder F enc and decoder F dec by default. Tab. 9 shows that F enc and F dec can work without fine-tuning. The gap between two settings, freeze parameters vs. trainable, is relatively small (about 0.1% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization</head><p>We visualize the detection results in Fig. <ref type="figure">9</ref>. In the base training stage, we pre-train the model on base classes of PAS-CAL VOC. Then we fine-tune the model on the {1, 3, 5} shots of Novel Set 1 and visualize the detection results. As the support set grows, our model produces more confident results, e.g., the scores of detected novel objects are increasing from 1-shot to 5-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Training Details</head><p>Our method follows the two-stage training strategy in FSOD <ref type="bibr" target="#b38">(Yan et al. 2019;</ref><ref type="bibr" target="#b15">Kang et al. 2019)</ref>, i.e., base training and few-shot fine-tuning. In the base training stage, we build a query dataset and a support dataset, where the query dataset contains the whole data from base classes and the support dataset is obtained by balanced sampling from the query dataset. We train the model on the two datasets and update all network parameters. In the few-shot fine-tuning stage, the support dataset is usually the same as the query dataset with only K shot instances. We only train (a) our F enc and F dec in VFA and (b) the last classification and regression layers. We freeze other parameters except for the Region Proposal Network (RPN) by default. RPN is fixed in our PASCAL VOC experiments but not frozen in COCO experiments because the model on COCO will not overfit to novel classes (10?30 shots).</p><p>0 . 4 8 0 . 4 0 . 4 1 0 . 4 4 0 . 3 8 0 . 5 6 0 . 4 6 0 . 3 4 0 . 6 0 . 6 1 0 . 3 3 0 . 4 3 0 . 6 5 0 . 4 0 . 4 1 0 . 8 0 . 3 9 0 . 6 7 0 . 4 1 0 . 3 8 0 . 3 6 0 . 2 6 0 . 4 7 0 . 3 3 0 . 4 3 0 . 2 7 0 . 3 1 0 . 3 1 0 . 3 0 . 3 0 . 2 4 0 . 2 9 0 . 3 0 . 5 0 . 3 7 0 . 3 9 0 . 7 1 0 . 3 1 0 . 4 0 . 3 9 0 . 3 9 0 . 3 3 0 . 3 3 0 . 3 3 0 . 3 5 0 . 5 3 0 . 3 8 0 . 2 8 0 . 6 0 . 6 8 0 . 2 9 0 . 3 4 0 . 7 1 0 . 3 5 0 . 3 4 0 . 6 7 0 . 3 1 0 . 7 6 0 . 3 3 0 . 3 2 0 . 3 6 0 . 4 3 0 . 3 0 . 3 3 0 . 4 3 0 . 2 4 0 . 3 2 0 . 3 1 0 . 2 6 0 . 3 3 0 . 2 8 0 . 3 0 . 2 9 0 . 3 5 0 . 3 0 . 4 1 0 . 4 0 . 3 3 0 . 7 3 0 . 3 8 0 . 3 4 0 . 2 5 0 . 3 0 . 2 4 0 . 3 3 0 . 2 5 0 . 4 9 0 . 3 6 0 . 2 7 0 . 2 8 0 . 2 4 0 . 2 6 0 . 2 5 0 . 3 0 . 2 9 0 . 3 8 0 . 3 9 0 . 3 2 0 . 3 8 0 . 7 9 0 . 4 0 . 3 7 0 . 3 3 0 . 4 1 0 . 3 3 0 . 4 3 0 . 4 0 . 2 8 0 . 5 4 0 . 4 2 0 . 2 7 0 . 3 8 0 . 4 5 0 . 3 3 0 . 3 5 0 . 6 9 0 . 3 9 0 . 5 2 0 . 3 5 0 . 3 6 0 . 3 0 . 2 4 0 . 3 8 0 . 2 8 0 . 4 0 . 2 7 0 . 2 3 0 . 2 5 0 . 3 3 0 . 2 9 0 . 2 6 0 . 2 9 0 . 2 9 0 . 4 9 0 . 3 0 . 3 9 0 . 6 9 0 . 3 4 0 . 3 8 0 . 3 2 0 . 2 7 0 . 3 0 . 2 5 0 . 2 5 0 . 3 1 0 . 3 4 0 . 3 1 0 . 2 1 0 . 5 0 . 5 8 0 . 2 1 0 . 2 9 0 . 5 5 0 . 3 2 0 . 2 8 0 . 5 2 0 . 3 4 0 . 6 5 0 . 2 8 0 . 2 9 0 . 2 7 0 . 3 8 0 . 2 1 0 . 2 8 0 . 4 0 . 2 1 0 . 2 4 0 . 2 6 0 . 2 2 0 . 2 8 0 . 2 9 0 . 2 8 0 . 2 3 0 . 2 9 0 . 2 2 0 . 3 5 0 . 3 8 0 . 2 8 0 . 7 1 0 . 3 1 0 . 2 8 0 . 1 9 0 . 2 1 0 . 1 8 0 . 2 7 0 . 2 2 0 . 4 8 0 . 3 0 . 2 8 0 . 2 1 0 . 2 2 0 . 2 3 0 . 1 7 0 . 2 2 0 . 2 9 0 . 3 6 0 . 3 2 0 . 2 9 0 . 3 1 0 . 7 7 0 . 2 9 0 . 2 2 0 . 2 4 0 . 2 2 0 . 2 3 0 . 3 2 0 . 2 5 0 . 1 4 0 . 3 4 0 . 2 8 0 . 2 2 0 . 2 3 0 . 3 4 0 . 2 2 0 . 2 1 0 . 8 8 0 . 2 4 0 . 3 4 0 . 2 1 0 . 1 6 0 . 3 0 . 2 1 0 . 3 0 . 2 1 0 . 3 7 0 . 2 2 0 . 2 2 0 . 1 6 0 . 3 0 . 2 7 0 . 1 5 0 . 1 8 0 . 2 4 0 . 4 3 0 . 2 6 0 . 2 4 0 . 8 6 0 . 2 8 0 . 2 0 . 2 1 0 . 3 3 0 . 2 7 0 . 2 3 0 . 2 6 0 . 2 9 0 . 2 9 0 . 2 7 0 . 1 8 0 . 4 9 0 . 5 6 0 . 2 3 0 . 2 3 0 . 5 4 0 . 2 8 0 . 2 5 0 . 3 4 0 . 2 8 0 . 6 8 0 . 2 6 0 . 3 0 . 2 9 0 . 3 6 0 . 1 8 0 . 2 7 0 . 3 6 0 . 2 0 . 2 0 . 1 7 0 . 2 3 0 . 3 3 0 . 2 0 . 1 9 0 . 2 2 0 . 2 5 0 . 1 8 0 . 2 1 0 . 2 0 . 2 6 0 . 8 4 0 . 2 1 0 . 3 8 0 . 2 1 0 . 2 1 0 . 3 1 0 . 2 7 0 . 2 1 0 . 4 5 0 . 3 0 . 2 4 0 . 3 1 0 . 2 0 . 2 2 0 . 1 6 0 . 2 0 . 2 6 0 . 1 6 0 .  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparisons of different support feature encoding methods. Previous methods use plain fully-connected (FC) layers to encode support features and obtain class prototypes by averaging these features: x p = Avg(x 1 , x 2 , . . . ). In contrast, our method uses variational autoencoders (VAEs) pretrained on abundant base examples to estimate the distributions of novel classes. Since intra-class variance is shared across classes and can be modeled with common distributions (Lin et al. 2018), we use a shared VAE to transfer the distributions of base classes to novel classes. Finally, we can sample class prototypes x p from the distributions N (?, ?) that are robust to the variance of few-shot examples. rm.: remove.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of two feature aggregation methods. S i /Q i : support and query features of class i. A: feature aggregation. L: loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of our framework. F Q and F S denote query and support feature extractors, respectively. F enc and F dec are the variational feature encoder and decoder. F D : the detection head. A: feature aggregation. Note that we do not visualize RPN and the regression branch for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparisons of recall without/with CRD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure5: Similarity matrix visualization. We calculate cosine similarities of support features in the 5-shot setting of PASCAL VOC Novel Set 1. sofa, motorbike, cow, bus and bird are novel classes. Warmer color denotes higher similarity. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Finally, we use VFA to transforms support examples into class distributions. By learning intra-class variances from abundant base examples, we can estimate novel classes' distributions even with a few examples. In Fig. 5 (bottom), we can see VFA significantly improves intra-class similarities.Robust and accurate class prototypes. In the testing phase, detectors take the mean feature of K-shot examples as the class prototype. As shown in Fig.6, our estimated class prototypes are more robust and accurate than the baseline. The distances to real class centers do not increase much as the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Analysis of different feature aggregation. We follow the same setting as Fig.5 of our main paper. Differently, the number in each cell is the averaged cosine similarity of 5 shot examples.</figDesc><graphic url="image-35.png" coords="11,270.43,451.89,84.52,63.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We then split the dataset into base classes C b and novel classes C n where C b ? C n = C and C b ? C n = ?.</figDesc><table><row><cell>setting</cell><cell cols="2">TFA Meta R-CNN  *</cell><cell>Meta R-CNN++</cell></row><row><cell>param freeze</cell><cell></cell><cell></cell><cell></cell></row><row><cell>cosine cls.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>last layer init.</cell><cell>copy</cell><cell>rand</cell><cell>rand rand copy</cell></row><row><cell>bAP (stage-I)</cell><cell>80.8</cell><cell>72.8</cell><cell>77.6 77.6 77.6</cell></row><row><cell cols="2">bAP (stage-II) 79.6</cell><cell>47.4</cell><cell>64.9 68.2 76.8</cell></row><row><cell>nAP</cell><cell>39.8</cell><cell>20.7</cell><cell>42.0 40.5 41.6</cell></row><row><cell cols="4">Table 1: Difference analysis between Meta R-CNN and</cell></row><row><cell cols="4">TFA. The results are evaluated under the 1 shot setting of</cell></row><row><cell cols="4">PASCAL VOC Novel Set 1. stage-I and stage-II: base train-</cell></row><row><cell cols="4">ing and fine-tuning stages.  *  : Our re-implemented results.</cell></row><row><cell cols="4">of C n with only K shots annotated instances. Existing few-</cell></row><row><cell cols="4">shot detectors usually adopt a two-stage training paradigm:</cell></row><row><cell cols="4">base training and few-shot fine-tuning, where the representa-</cell></row><row><cell cols="4">tions learned from C b are transferred to detect novel objects</cell></row><row><cell cols="2">in the fine-tuning stage.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>). Differ-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ent from meta-learning, Wang et al. propose a simple two-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>stage fine-tuning approach, TFA (Wang et al. 2020). TFA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>shows that only fine-tuning the last layers can significantly</cell></row><row><cell></cell><cell></cell><cell></cell><cell>improve the FSOD performance. Due to the simple structure</cell></row><row><cell></cell><cell></cell><cell></cell><cell>of TFA, a line of works (Sun et al. 2021; Zhu et al. 2021;</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Qiao et al. 2021; Cao et al. 2021) following TFA are pro-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>posed. In this work, we build a strong meta-learning base-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>line that even surpasses the fine-tuning baseline TFA. Then</cell></row><row><cell></cell><cell></cell><cell></cell><cell>we revisit the feature aggregation scheme and propose two</cell></row><row><cell></cell><cell></cell><cell></cell><cell>novel feature aggregation methods, CAA and VFA, achiev-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ing a new state-of-the-art in FSOD.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Variational Feature Learning. Given an input image/fea-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ture, we can transform it into a distribution with VAEs. By</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sampling features from the distribution, we can model intra-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>class variance that defines the class's character. The varia-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tional feature learning paradigm has been used in various</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tasks, e.g., zero/few-shot learning (Zhang et al. 2019; Xu</cell></row><row><cell></cell><cell></cell><cell></cell><cell>et al. 2021; Kim et al. 2019), metric learning (Lin et al.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2018) and disentanglement learning (Ding et al. 2020). In</cell></row><row><cell></cell><cell></cell><cell></cell><cell>this work, we use VAEs trained on abundant base examples</cell></row><row><cell></cell><cell></cell><cell></cell><cell>to estimate novel classes' distributions with only a few ex-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>amples. Besides, we also propose a consistency loss to make</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the model produce class-specific distributions. To our best</cell></row><row><cell></cell><cell></cell><cell></cell><cell>knowledge, we are the first to introduce variational feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell>learning into FSOD.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Background and Meta R-CNN++</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Preliminaries</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Problem Definition. We follow the FSOD settings in previ-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ous works (Yan et al. 2019; Wang et al. 2020). Assume we</cell></row><row><cell></cell><cell></cell><cell></cell><cell>have a dataset D = {(x, y), x ? X, y ? Y } with a set of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>classes C, where x is the input image and y = {c i , b i } N i=1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>is the corresponding class label c and bounding box b an-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>notations. Generally, we have abundant samples of C b and K shots</cell></row><row><cell></cell><cell></cell><cell></cell><cell>samples of C n (K=1, 2, 3, . . . ). The goal is to detect objects</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Yan et al. 2019) ResNet-101 19.9 25.5 35.0 45.7 51.5 10.4 19.4 29.6 34.8 45.4 14.3 18.2 27.5  41.2 48.1 31.1 TFA w/ cos (Wang et al. 2020) ResNet-101 39.8 36.1 44.7 55.7 56.0 23.5 26.9 34.1 35.1 39.1 30.8 34.8 42.8 49.5 49.8 39.9 MPSR<ref type="bibr" target="#b35">(Wu et al. 2020)</ref> Results on PASCAL VOC. The results are sorted by the averaged score (Avg.). See our appendix for the generalized FSOD results.</figDesc><table><row><cell>Method / Shots</cell><cell>Backbone</cell><cell>1</cell><cell>Novel Set 1 2 3 5</cell><cell>10</cell><cell>1</cell><cell>Novel Set 2 2 3 5</cell><cell>10</cell><cell>1</cell><cell>Novel Set 3 2 3 5</cell><cell>10</cell><cell>Avg.</cell></row><row><cell>FSRW (Kang et al. 2019)</cell><cell cols="11">YOLOv2 14.8 15.5 26.7 33.9 47.2 15.7 15.3 22.7 30.1 40.5 21.3 25.6 28.4 42.8 45.9 28.4</cell></row><row><cell>MetaDet (Wang et al. 2019)</cell><cell>VGG16</cell><cell cols="10">18.9 20.6 30.2 36.8 49.6 21.8 23.1 27.8 31.7 43.0 20.6 23.9 29.4 43.9 44.1 31.0</cell></row><row><cell cols="11">Meta R-CNN (ResNet-101 41.7 -51.4 55.2 61.8 24.4 -39.2 39.9 47.8 35.6 -42.3 48.0 49.7</cell><cell>-</cell></row><row><cell>Retentive (Fan et al. 2021)</cell><cell cols="11">ResNet-101 42.4 45.8 45.9 53.7 56.1 21.7 27.8 35.2 37.0 40.3 30.2 37.6 43.0 49.7 50.1 41.1</cell></row><row><cell cols="12">Halluc (Zhang and Wang 2021) ResNet-101 47.0 44.9 46.5 54.7 54.7 26.3 31.8 37.4 37.4 41.2 40.4 42.1 43.3 51.4 49.6 43.2</cell></row><row><cell cols="12">CGDP+FSCN (Li et al. 2021b) ResNet-101 40.7 45.1 46.5 57.4 62.4 27.3 31.4 40.8 42.7 46.3 31.2 36.4 43.7 50.1 55.6 43.8</cell></row><row><cell>CME (Li et al. 2021a)</cell><cell cols="11">ResNet-101 41.5 47.5 50.4 58.2 60.9 27.2 30.2 41.4 42.5 46.8 34.3 39.6 45.1 48.3 51.5 44.4</cell></row><row><cell>SRR-FSD (Zhu et al. 2021)</cell><cell cols="11">ResNet-101 47.8 50.5 51.3 55.2 56.8 32.5 35.3 39.1 40.8 43.8 40.1 41.5 44.3 46.9 46.4 44.8</cell></row><row><cell>FSOD-UP (Wu et al. 2021)</cell><cell cols="11">ResNet-101 43.8 47.8 50.3 55.4 61.7 31.2 30.5 41.2 42.2 48.3 35.5 39.7 43.9 50.6 53.5 45.0</cell></row><row><cell>FSCE (Sun et al. 2021)</cell><cell cols="11">ResNet-101 44.2 43.8 51.4 61.9 63.4 27.3 29.5 43.5 44.2 50.2 37.2 41.9 47.5 54.6 58.5 46.6</cell></row><row><cell>QA-FewDet (Han et al. 2021)</cell><cell cols="11">ResNet-101 42.4 51.9 55.7 62.6 63.4 25.9 37.8 46.6 48.9 51.1 35.2 42.9 47.8 54.8 53.5 48.0</cell></row><row><cell>FADI (Cao et al. 2021)</cell><cell cols="11">ResNet-101 50.3 54.8 54.2 59.3 63.2 30.6 35.0 40.3 42.8 48.0 45.7 49.7 49.1 55.0 59.6 49.2</cell></row><row><cell cols="12">Zhang et al. (Zhang et al. 2021) ResNet-101 48.6 51.1 52.0 53.7 54.3 41.6 45.4 45.8 46.3 48.0 46.1 51.7 52.6 54.1 55.0 49.8</cell></row><row><cell cols="12">Meta FR-CNN (Han et al. 2022) ResNet-101 43.0 54.5 60.6 66.1 65.4 27.7 35.5 46.1 47.8 51.4 40.6 46.4 53.4 59.9 58.6 50.5</cell></row><row><cell>DeFRCN (Qiao et al. 2021)</cell><cell cols="11">ResNet-101 53.6 57.5 61.5 64.1 60.8 30.1 38.1 47.0 53.3 47.9 48.4 50.9 52.3 54.9 57.4 51.9</cell></row><row><cell>VFA (Ours)</cell><cell cols="11">ResNet-101 57.7 64.6 64.7 67.2 67.4 41.4 46.2 51.1 51.8 51.6 48.9 54.8 56.6 59.0 58.9 56.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, following</figDesc><table><row><cell>Method / Shots</cell><cell>10</cell><cell>30</cell></row><row><cell>Fine-tuning</cell><cell></cell><cell></cell></row><row><cell>MPSR (Wu et al. 2020)</cell><cell>9.8</cell><cell>14.1</cell></row><row><cell>TFA w/ cos</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on COCO. The backbone is the same as Tab. 2. The results are sorted by 10-shot nAP. See our appendix for the generalized FSOD results.</figDesc><table><row><cell></cell><cell></cell><cell>Method CRD CAA VFA</cell><cell>1</cell><cell>Shots 3</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell>Meta R-CNN++</cell><cell cols="2">42.0 56.5 58.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">46.0 61.7 62.3</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell cols="2">51.3 62.8 66.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">57.7 64.7 67.2</cell></row><row><cell>al. 2021)</cell><cell cols="2">18.5 22.6</cell><cell></cell></row><row><cell>Meta-learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FSRW (Kang et al. 2019)</cell><cell>5.6</cell><cell>9.1</cell><cell></cell></row><row><cell>MetaDet (Wang, Ramanan, and Hebert 2019)</cell><cell>7.1</cell><cell>11.3</cell><cell></cell></row><row><cell>Meta R-CNN (Yan et al. 2019)</cell><cell>8.7</cell><cell>12.4</cell><cell></cell></row><row><cell>QA-FewDet (Han et al. 2021)</cell><cell cols="2">11.6 16.5</cell><cell></cell></row><row><cell>FSDetView (Xiao and Marlet 2020)</cell><cell cols="2">12.5 14.7</cell><cell></cell></row><row><cell>Meta FR-CNN (Han et al. 2022)</cell><cell cols="2">12.7 16.6</cell><cell></cell></row><row><cell>DCNet (Hu et al. 2021)</cell><cell cols="2">12.8 18.6</cell><cell></cell></row><row><cell>CME (Li et al. 2021a)</cell><cell cols="2">15.1 16.9</cell><cell></cell></row><row><cell>VFA (Ours)</cell><cell cols="2">16.2 18.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell>have</cell><cell></cell></row><row><cell cols="3">K={1, 2, 3, 5, 10} shots settings. For COCO, we set 60 cat-</cell><cell></cell></row><row><cell cols="3">egories disjoint with PASCAL VOC as base classes and the</cell><cell></cell></row><row><cell cols="3">remaining 20 as novel classes. We have K={10, 30} shots</cell><cell></cell></row><row><cell>settings.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Evaluation Metrics. For PASCAL VOC, we report the</cell><cell></cell></row><row><cell cols="3">Average Precision at IoU=0.5 of base classes (bAP) and</cell><cell></cell></row><row><cell cols="3">novel classes (nAP). For COCO, we report the mean AP at</cell><cell></cell></row><row><cell>IoU=0.5:0.95 of novel classes (nAP).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Implementation Details. We implement our method with</cell><cell></cell></row><row><cell cols="3">Mmdetection (Chen et al. 2019). The backbone is ResNet-</cell><cell></cell></row><row><cell cols="3">101 (He et al. 2016) pre-trained on ImageNet (Russakovsky</cell><cell></cell></row><row><cell cols="3">et al. 2015). with batch size</cell><cell></cell></row><row><cell cols="3">32, learning rate 0.02, momentum 0.9 and weight decay 1e-</cell><cell></cell></row><row><cell cols="3">4. The learning rate is changed to 0.001 in the few-shot fine-</cell><cell></cell></row><row><cell cols="3">tuning stage. We fine-tune the model with {400, 800, 1200,</cell><cell></cell></row><row><cell cols="3">1600, 2000} iterations for K={1, 2, 3, 5, 10} shots in PAS-</cell><cell></cell></row><row><cell cols="3">CAL VOC, and {10000, 20000} iterations for K={10, 30}</cell><cell></cell></row><row><cell cols="3">shots in COCO. We keep other hyper-parameters the same</cell><cell></cell></row><row><cell cols="2">as Meta R-CNN (Yan et al. 2019) if not specified.</cell><cell></cell><cell></cell></row><row><cell>Main Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">PASCAL VOC. As shown in Tab. 2, VFA significantly out-</cell><cell></cell></row><row><cell cols="3">performs existing methods. VFA achieves the best (13/16)</cell><cell></cell></row></table><note><p><p><p><p><p>previous works</p><ref type="bibr" target="#b15">(Kang et al. 2019;</ref><ref type="bibr" target="#b32">Wang et al. 2020</ref></p>). We use the data splits and annotations provided by TFA</p><ref type="bibr" target="#b32">(Wang et al. 2020)</ref> </p>for a fair comparison. For PASCAL VOC, we split 20 classes into three groups, where each group contains 15 base classes and 5 novel classes. For each novel set, we or second-best (3/16) results on all settings. In Novel Set 1, VFA outperforms previous best results by 3.2%?7.1%. Our 2-shot result even surpasses previous best 10-shot results (64.6% vs. 63.4%), which indicates that our method is</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effect of different modules.</figDesc><table><row><cell></cell><cell></cell><cell>9 0 . 6 9 0 . 3 9 1 . 3</cell></row><row><cell cols="2">8 6 . 4 8 6 . 9</cell><cell>8 7 . 4 8 7 . 2</cell></row><row><cell>8 4 . 2</cell><cell cols="2">8 4 . 3</cell></row><row><cell>8 1 . 4</cell><cell></cell></row><row><cell>8 0 . 2</cell><cell></cell></row><row><cell>7 6 . 7</cell><cell></cell></row><row><cell cols="2">1 -s h o t 3 -s h o t 5 -s h o t</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of different support features. S and S are the original and reconstructed features. ?, ?, z = ?+ ?? and z = ? + ? are latent variables. avg.: The average score.</figDesc><table><row><cell>Features</cell><cell></cell><cell>S</cell><cell>S</cell><cell>?</cell><cell>?</cell><cell>z</cell><cell>z</cell></row><row><cell>bAP</cell><cell></cell><cell cols="6">78.8 78.1 78.6 78.3 78.0 78.6</cell></row><row><cell></cell><cell>1</cell><cell cols="6">55.2 54.4 56.6 55.4 53.0 57.7</cell></row><row><cell>nAP</cell><cell>3 5</cell><cell cols="6">63.7 63.6 63.7 64.9 63.2 64.7 66.6 66.9 66.7 66.9 66.3 67.2</cell></row><row><cell cols="8">avg. 61.8 61.6 62.3 62.4 60.8 63.2</cell></row><row><cell cols="3">Setting / Shots</cell><cell></cell><cell>1</cell><cell>3</cell><cell>5</cell></row><row><cell cols="2">w/o VFA</cell><cell></cell><cell></cell><cell cols="3">51.3 62.8 66.4</cell></row><row><cell></cell><cell></cell><cell cols="2">w/o Lcons</cell><cell cols="3">53.6 64.3 66.7</cell></row><row><cell cols="2">w/ VFA</cell><cell cols="2">Lcons on S</cell><cell cols="3">52.9 64.1 67.3</cell></row><row><cell></cell><cell></cell><cell cols="2">Lcons on S</cell><cell cols="3">57.7 64.7 67.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Effect of L cons . w/o: without. L cons on S/S : apply L cons to S or S . The results are averages of multiple runs. shot decreases, because our method can fully leverage base classes' distributions to estimate novel classes' distributions. The prototypes sampled from distributions are robust to the variance of support examples. While the baseline is sensitive to the number of support examples.Which feature to aggregate? In Tab. 5, we explore different features for aggregation. All types of features achieve comparable performance on base classes but vary on novel classes. The performance of original feature S and reconstructed feature S lag behind the latent encoding ?, ? and z. We hypothesize that the latent encoding contains more class-generic features. Besides, z = ? + ? ? performs worst among these features due to its indeterminate inference process. Instead, a simplified version z = ? + ? achieves satisfactory results, which is the default setting of VFA. Effect of L cons . We use a shared VAE to encode support features but still need to preserve class-specific information. Therefore, we add a consistency loss L cons to produce classwise distributions. Tab. 6 shows that L cons is important for VFA. L cons applied to S forces the model to produce classconditional distributions so that the latent variable z can retrain meaningful information to represent class centers. Design of VFA. The variational feature encoder F enc and decoder F dec are not sensitive to the number and dimension of hidden layers. Please see our appendix for details.Figure7: Design of VFA. We explore different designs of F enc and F dec . layer: the number of hidden layer. dim: the number of hidden channels.</figDesc><table><row><cell>Conclusion</cell></row><row><cell>This paper revisits feature aggregation schemes in meta-</cell></row><row><cell>learning based FSOD and proposes Class-Agnostic Aggre-</cell></row><row><cell>gation (CAA) and Variational Feature Aggregation (VFA).</cell></row><row><cell>CAA can reduce class bias and confusion between base</cell></row><row><cell>and novel classes; VFA transforms instance-wise support</cell></row><row><cell>features into class distributions for robust feature aggrega-</cell></row><row><cell>tion. Extensive experiments on PASCAL VOC and COCO</cell></row><row><cell>demonstrate our effectiveness.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>). The results indicate that the representation learned from base classes can be directly transferred to novel classes even without fine-tuning. More analysis of different feature aggregation. In the main paper, we give a visual analysis of different feature aggregation methods, i.e., CSA, CAA and VFA. Here we give a quantitative analysis of these methods, shown in Fig 8.Compared with CSA, CAA reduces class confusion between base and novel classes. For example, The similarity between cow and sheep is 0.71, near the intra-class similarity of G-FSOD results on PASCAL VOC. The results are sorted by the averaged score (Avg.).</figDesc><table><row><cell>Method / Shots</cell><cell>1</cell><cell>Novel Set 1 2 3 5</cell><cell>10</cell><cell>1</cell><cell>Novel Set 2 2 3 5</cell><cell>10</cell><cell>1</cell><cell>Novel Set 3 2 3 5</cell><cell>10</cell><cell>Avg.</cell></row><row><cell>FRCN-ft (Ren et al. 2017)</cell><cell cols="10">9.9 15.6 21.6 28.0 52.0 9.4 13.8 17.4 21.9 39.7 8.1 13.9 19.0 23.9 44.6 22.6</cell></row><row><cell>FSRW (Kang et al. 2019)</cell><cell cols="10">14.2 23.6 29.8 36.5 35.6 12.3 19.6 25.1 31.4 29.8 12.5 21.3 26.8 33.8 31.0 25.6</cell></row><row><cell>TFA w/ cos (Wang et al. 2020)</cell><cell cols="10">25.3 36.4 42.1 47.9 52.8 18.3 27.5 30.9 34.1 39.5 17.9 27.2 34.3 40.8 45.6 34.7</cell></row><row><cell cols="11">FSDetView (Xiao and Marlet 2020) 24.2 35.3 42.2 49.1 57.4 21.6 24.6 31.9 37.0 45.7 21.2 30.0 37.2 43.8 49.6 36.7</cell></row><row><cell>DCNet (Hu et al. 2021)</cell><cell cols="10">33.9 37.4 43.7 51.1 59.6 23.2 24.8 30.6 36.7 46.6 32.3 34.9 39.7 42.6 50.7 39.2</cell></row><row><cell>FSCE (Sun et al. 2021)</cell><cell cols="10">32.9 44.0 46.8 52.9 59.7 23.7 30.6 38.4 43.0 48.5 22.6 33.4 39.5 47.3 54.0 41.2</cell></row><row><cell>DeFRCN (Qiao et al. 2021)</cell><cell cols="10">40.2 53.6 58.2 63.6 66.5 29.5 39.7 43.4 48.1 52.8 35.0 38.3 52.9 57.7 60.8 49.4</cell></row><row><cell>VFA (Ours)</cell><cell cols="10">47.4 54.4 58.5 64.5 66.5 33.7 38.2 43.5 48.3 52.4 43.8 48.9 53.3 58.1 60.0 51.4</cell></row><row><cell>Method</cell><cell></cell><cell>Shots 10 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">TFA w/ cos (Wang et al. 2020)</cell><cell>9.1 12.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">FSDetView (Xiao and Marlet 2020) 10.7 15.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FSCE (Sun et al. 2021)</cell><cell></cell><cell>11.1 15.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeFRCN (Qiao et al. 2021)</cell><cell></cell><cell>16.8 21.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VFA (Ours)</cell><cell></cell><cell>15.9 18.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>G-FSOD results on COCO. The results are sorted by 10-shot nAP.</figDesc><table><row><cell>F enc , F dec</cell><cell>1</cell><cell>nAP 3</cell><cell>5</cell><cell>1</cell><cell>bAP 3</cell><cell>5</cell></row><row><cell>freeze</cell><cell cols="6">57.6 64.5 67.1 71.5 75.9 76.7</cell></row><row><cell>trainable</cell><cell cols="6">57.7 64.7 67.2 71.6 76.0 76.7</cell></row><row><cell>?</cell><cell cols="5">-0.1 -0.2 -0.1 -0.1 -0.1</cell><cell>0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Freeze or fine-tune F enc and F dec in VFA. ?: The difference between the first and second row.</figDesc><table><row><cell>cow (0.76). While in CAA, the similarity between cow and</cell></row><row><cell>sheep is reduced to 0.55 and the gap of intra-class and</cell></row><row><cell>inter-class similarity is enlarged to 0.10 (0.65 vs. 0.55). By</cell></row><row><cell>applying VFA to the model, the inter-class similarity is fur-</cell></row><row><cell>ther reduced. For each novel class, the gap between intra-</cell></row><row><cell>class and inter-class similarity is enlarged to 0.12?0.54 (the</cell></row><row><cell>range is 0.05?0.3 in CSA). The results further demonstrate</cell></row><row><cell>that our proposed CAA and VFA learn more discriminative</cell></row><row><cell>and transferable features.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was partially supported by <rs type="funder">National Nature Science Foundation of China</rs> under the grants No. <rs type="grantNumber">U22B2011</rs>, No.<rs type="grantNumber">41820104006</rs>, and No.<rs type="grantNumber">61922065</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TXnmR68">
					<idno type="grant-number">U22B2011</idno>
				</org>
				<org type="funding" xml:id="_Rv6Qrsp">
					<idno type="grant-number">41820104006</idno>
				</org>
				<org type="funding" xml:id="_rMSWaMV">
					<idno type="grant-number">61922065</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Few-Shot Object Detection via Association and DIscrimination</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open MMLab Detection Toolbox and Benchmark</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pix2seq: A language modeling framework for object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10852</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Guided variational autoencoder for disentanglement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7920" to="7929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fewshot object detection with attention-RPN and multi-relation detector</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalized fewshot object detection without forgetting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4527" to="4536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Query adaptive few-shot object detection with heterogeneous graph convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3263" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meta faster r-cnn: Towards accurate few-shot object detection with attentive feature alignment</title>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="780" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dense relation distillation with context-aware aggregation for fewshot object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10185" to="10194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8420" to="8429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variational prototyping-encoder: One-shot learning with prototypical images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9462" to="9470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transformation invariant few-shot object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3094" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond max-margin: Class margin equilibrium for few-shot object detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="7363" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Few-shot object detection via classification refinement and distractor retreatment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vadakkepat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15395" to="15403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep variational metric learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8681" to="8690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fsce: Few-shot object detection via contrastive proposal encoding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7352" to="7362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06957</idno>
		<title level="m">Frustratingly simple few-shot object detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Metalearning to detect rare objects</title>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9925" to="9934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Universalprototype enhancing for few-shot object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9567" to="9576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-scale positive sample refinement for few-shot object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="456" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Few-shot object detection and viewpoint estimation for objects in the wild</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="192" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Variational Feature Disentangling for Fine-Grained Few-Shot Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8812" to="8821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Meta r-cnn: Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9577" to="9586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Variational few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1685" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Accurate few-shot object detection with support-query mutual guidance and hybrid loss</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14424" to="14432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hallucination improves few-shot object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13008" to="13017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as Points</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semantic relation reasoning for shot-stable few-shot object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8782" to="8791" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
