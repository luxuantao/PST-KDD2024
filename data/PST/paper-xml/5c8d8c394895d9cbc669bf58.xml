<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symmetrical Dense-Shortcut Deep Fully Convolutional Networks for Semantic Segmentation of Very-High-Resolution Remote Sensing Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Guanzhou</forename><surname>Chen</surname></persName>
							<email>chenguanzhou123@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Dai</surname></persName>
							<email>daifan@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanfu</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Symmetrical Dense-Shortcut Deep Fully Convolutional Networks for Semantic Segmentation of Very-High-Resolution Remote Sensing Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">58FADEC26B0B551B5DC4F55C76419383</idno>
					<idno type="DOI">10.1109/JSTARS.2018.2810320</idno>
					<note type="submission">received October 22, 2017; revised December 6, 2017 and January 18, 2018; accepted February 22, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural networks (CNN)</term>
					<term>deep learning (DL)</term>
					<term>fully convolutional networks (FCN)</term>
					<term>remote sensing</term>
					<term>SDFCN</term>
					<term>semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation has emerged as a mainstream method in very-high-resolution remote sensing land-use/landcover applications. In this paper, we first review the state-of-the-art semantic segmentation models in both computer vision and remote sensing fields. Subsequently, we introduce two semantic segmentation frameworks: SNFCN and SDFCN, both of which contain deep fully convolutional networks with shortcut blocks. We adopt an overlay strategy as the postprocessing method. Based on our frameworks, we conducted experiments on two online ISPRS datasets: Vaihingen and Potsdam. The results indicate that our frameworks achieve higher overall accuracy than the classic FCN-8s and Seg-Net models. In addition, our postprocessing method can increase the overall accuracy by about 1%-2% and help to eliminate "salt and pepper" phenomena and block effects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>general, OBIC alleviates "salt and pepper" phenomena to a certain degree and achieves higher classification accuracy than PBIC. OBIC frameworks, however, introduce a new source of error during the image segmentation step. OBIC labeling results are easily affected by the segmentation process. Moreover, features (including spectral, spatial, and texture features) of each object are extracted by rigidly designed artificial methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The parameters of segmentation and feature extraction are difficult to train or fine-tune using the training dataset directly.</p><p>In recent years, various deep learning architectures have been applied in PBIC and OBIC applications. They achieved higher performance than traditional machine learning methods <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b20">[21]</ref>. However, the segmentation is still independent of the entire process. To make the whole model to be trained end-to-end, the deep fully convolutional neural networks (FCNs) based semantic segmentation framework was proposed afterward <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b26">[27]</ref>. The VHR remotely sensed images can be segmented and classified simultaneously by a fine-tuned end-to-end FCN model. Many FCN-based frameworks achieve higher overall accuracy than traditional OBIC or PBIC methods for remote sensing images semantic segmentation, and have been successfully applied to LULC <ref type="bibr" target="#b27">[28]</ref>, object detection <ref type="bibr" target="#b28">[29]</ref>, urban mapping <ref type="bibr" target="#b29">[30]</ref>, etc. However, most of them exploited extra data (DSM/NDSM data) or prior knowledge (ImageNet <ref type="bibr" target="#b30">[31]</ref> pretrained model) to enhance performance. Moreover, they did not discuss the overlay policy of a large size of image that how to slice and merge image patches quantitatively.</p><p>In this paper, consequently, we design two deep FCN frameworks for semantic segmentation of remote sensing images without using DSM data and trained them on ISPRS semantic segmentation dataset <ref type="bibr" target="#b31">[32]</ref> from scratch. In addition, we executed a series of experiments to assess the effectiveness and performance of these frameworks and different overlay policies. In summary, the contributions of this paper are as follows.</p><p>1) We designed SNFCN and SDFCN frameworks with dense-shortcut connection structures, which achieved higher overall accuracy than traditional methods. 2) We tested and analyzed two types of basic blocks (VGGlike and shortcut block) in SNFCN and SDFCN. 3) An overlay strategy was adopted as the postprocessing method, which helps to eliminate "salt and pepper" phenomena and block effects. This paper contains four sections. Section II provides the overview of convolutional neural networks (CNNs) and FCN frameworks and their applications in fields of computer vision and remote sensing. In Section III, we explain in details of the design of our symmetrical normal-shortcut FCN (SNFCN) and symmetrical dense-shortcut FCN (SDFCN) frameworks, including the basic blocks in networks, architectures and postprocessing methods. Results and an analysis of experiments are conducted in Section IV. Conclusions and future work are discussed in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. From CNN to FCN</head><p>The first convolutional neural network (CNN) LeNet-5 was proposed in 1990, inspired by visual perception mechanism of nervous system <ref type="bibr" target="#b32">[33]</ref>. A few years later, it was applied to classify handwritten digits and achieved superior performance on the MNIST dataset <ref type="bibr" target="#b33">[34]</ref>. Lacking large training datasets and computing resources, LeNet-5 did not perform well on more complicated tasks at that time <ref type="bibr" target="#b34">[35]</ref>.</p><p>Since 2006, many techniques for training deep networks have been put forward, such as deep belief networks (DBNs) <ref type="bibr" target="#b35">[36]</ref>, autoencoders (AEs) <ref type="bibr" target="#b36">[37]</ref>, deep boltzmann machines (DBMs) <ref type="bibr" target="#b37">[38]</ref>, and stacked denoising autoencoders (SDAEs) <ref type="bibr" target="#b38">[39]</ref>. These approaches improved efficiency when training with large-scale samples. In 2012, the classic deep CNN model AlexNet was proposed. It achieved the best result on the ImageNet classification benchmark by then <ref type="bibr" target="#b30">[31]</ref>. Later, increasingly deeper ZFNet <ref type="bibr" target="#b39">[40]</ref>, VGGNet (VGG16 and VGG19) <ref type="bibr" target="#b40">[41]</ref> and GoogleNet <ref type="bibr" target="#b41">[42]</ref> models were designed. Some tricks or techniques like the parametric rectified linear unit (PReLU) <ref type="bibr" target="#b42">[43]</ref>, dropout <ref type="bibr" target="#b43">[44]</ref>, and batch normalization (BN) <ref type="bibr" target="#b44">[45]</ref> have also been widely applied to accelerate the CNNs training process.</p><p>Except for the first input layer and the last output soft-max layer, the standard structure of a CNN contains three main types of layers: convolutional layers, pooling layers, and fully connected layers. For a convolutional layer, the output z l j of each filter is calculated by</p><formula xml:id="formula_0">z l j = f ⎛ ⎝ i∈M j x l-1 i • ω l i,j + b l j ⎞ ⎠</formula><p>where ω l i,j and b l j are the weight and bias term of the jth filter of the lth layer, and the f (x) denotes the nonlinear activation function. In practical use, rectified linear unit (ReLU) <ref type="bibr" target="#b45">[46]</ref> is the most common activation function in CNNs ReLU(x) = max(x, 0).</p><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>Inside a convolutional layer, the weights and bias of each filter are shared at any location of an input feature map. This mechanism makes the model simpler and easier to train. The pooling layer is designed to provide shift-invariance by reducing the resolution of the feature map. Average-pooling and max-pooling are two common types of pooling layers. After several convolutional and pooling layers, the feature maps become smaller and more abstract. Normally, these deep feature maps are connected by a sequence of fully connected layers. The fully connected layers transform the abstract two-dimensional (2-D) feature maps into 1-D features. Finally, a prediction is made by the last softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FCNs for Semantic Segmentation</head><p>These CNNs are powerful and effective for deep feature extraction and scene-wise image classification. However, it is difficult to apply CNNs in pixel-wise semantic segmentation. Fully convolutional networks (FCNs) are more suitable for end-to-end training in pixel-wise semantic segmentation tasks than CNNs. FCNs exceeded the state of the art on semantic segmentation competitions when first proposed. FCNs are composed of two parts: encoders and decoders. Encoders are like the traditional CNNs used to extract the deep abstract features, whereas the decoders transform these features into dense label map whose size is the same as the input image.</p><p>Long et al. first proposed three end-to-end FCNs models in 2015: FCN-8s, FCN-16s, and FCN-32s <ref type="bibr" target="#b46">[47]</ref>. They up-sampled the deep feature maps to the labeling results by using transposeconvolutional layers (also called deconvolutional layers) instead of simple bilinear interpolation as the decoders. However, transpose-convolutional layers are memory-cost <ref type="bibr" target="#b47">[48]</ref> and difficult to train; the labeling result also did not perform well around objects' boundaries.</p><p>To overcome this drawback, several innovative FCNs were put forward. Chen et al. introduced an extra conditional random fields (CRFs) after FCNs to improve boundaries of objects in label map <ref type="bibr" target="#b48">[49]</ref>. Zheng et al. formulated the mean-field approximate inference for CRF with Gaussian pairwise potentials as recurrent neural fields (RNNs) <ref type="bibr" target="#b49">[50]</ref>. The unary energy of the CRF was trained with an end-to-end CRF-as-RNN network. Unlike CRF-as-RNN network only predict unary energy, Lin et al. designed a deep structured model to predict both unary energy and pairwise energy. They achieved better performance on the challenging PASCAL VOC 2012 dataset <ref type="bibr" target="#b50">[51]</ref>.</p><p>Different from the idea of integrating CRF with CNNs discussed, some publications have emphasized the symmetry of the encoder-decoder structure. Ronneberger et al. devised a multiscale U-net model with VGG-like encoders and a symmetric decoder. This was successfully applied to neuronal structures segmentation in microscopic stacks <ref type="bibr" target="#b51">[52]</ref>. Badrinarayanan et al. also designed a similar network and used pretrained weights from VGGNet in encoder part <ref type="bibr" target="#b52">[53]</ref>. The innovation is that they reused pooling indices which improves boundary delineation and reduced the number of parameters in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semantic Segmentation in RS</head><p>In general, there are two types of semantic segmentation frameworks for labeling VHR remote sensing imagery. One of them is patch-based labeling framework (PBLF), first proposed by Mnih in 2013. This framework aims to predict output image patch O from input patch I; where I denotes an image patch in a certain size, and the O patch is centered in the I patch with a smaller size. Mnih <ref type="bibr" target="#b53">[54]</ref> designed a CNN model to extract buildings and roads specifically in a form of 16 × 16 centered binary labeled patch from a 64 × 64 image patch. They exploited CRFs to smooth the output patch results. Paisitkriangkrai et al. <ref type="bibr" target="#b54">[55]</ref> also put forward a PBLF model. The author exploited three CNNs at different scales to predict labels of each pixel. In addition, a random forest classifier was trained on hand-crafted features extracted from original image. Finally, both two results were combined as the input of a CRF model to inference the final labeling mappings.</p><p>However, PBLF is not rotation-invariant <ref type="bibr" target="#b21">[22]</ref>. It costs more time and is not memory-efficient during the training process, because the size of input patch is much bigger than the output patch.</p><p>To deal with these problems with PBLF, fully convolutional architectures have been proposed in the recent literature. Zhong et al. <ref type="bibr" target="#b22">[23]</ref> modified the standard FCN-8s/16s/32s models to extract roads and buildings from RGB imagery. Marmanis et al. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, Sherrah <ref type="bibr" target="#b25">[26]</ref>, and Audebert et al. <ref type="bibr" target="#b26">[27]</ref> proposed their FCN-based semantic segmentation methods and tested on IS-PRS dataset <ref type="bibr" target="#b31">[32]</ref> using IRRG image and DSM as input data. Marmanis et al. designed FCN models based on a series of pretrained networks (e.g., VGG16 CNN pretrained on ImageNet dataset and FCN-pascal network pretrained on Pascal VOC database) with fully connected CRF (FCRF) as postprocessing method <ref type="bibr" target="#b23">[24]</ref>. They improved their model later by predicting not only labeled images but also the class boundary maps, which significantly improved overall accuracy on ISPRS Vaihingen benchmark <ref type="bibr" target="#b24">[25]</ref>. Unlike normal FCN models, Sherrah adopted a deep FCN and CRF semantic model without downsampling and transpose-convolutional layers to preserve the resolution of output labeling results <ref type="bibr" target="#b25">[26]</ref>. To make full use of deep features in earth observation data, Audebert et al. set IRRG images and DSM/NDSM/NDVI as separate input data sources in a SegNetlike multiscale and multikernel hybrid FCN. They also adopted CRF as the post processing method. According to the result, this hybrid model achieved the state of the art in ISPRS Vaihingen dataset <ref type="bibr" target="#b26">[27]</ref>.</p><p>Considering the generalization of the model in practice, only spectral data (infrared, red, green, and blue bands) are taken into consideration as the input data source in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYMMETRICAL DENSE-SHORTCUT FCN-BASED SEMANTIC SEGMENTATION FRAMEWORK</head><p>The FCN-based frameworks have been the mainstream methods in VHR imagery semantic segmentation tasks. In this section, we first introduce the internal encoder-decoder structure of our SNFCN and SDFCN frameworks. Then the basic blocks and architectures of our frameworks are discussed. Finally, we describe the post processing method of our frameworks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoders and Decoders</head><p>Most FCN-based models consist of two parts: encoders and decoders. The structure of encoders is like the VGG16 network <ref type="bibr" target="#b40">[41]</ref>, even the pretrained VGG16 network can be transplanted to the encoder module directly <ref type="bibr" target="#b23">[24]</ref>. However, decoders of them are usually simpler and shallower than encoders. Decoders are always composed of several transpose convolutional layers <ref type="bibr" target="#b46">[47]</ref> or upsampling layers <ref type="bibr" target="#b27">[28]</ref>.</p><p>In our framework, we design a symmetrical encoder-decoder network structure. The coarse structure of the framework is shown in Figs. <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_2">3</ref>. Encoders and decoders of our networks are symmetrical. This encoder-decoder structure can be regarded as stacked autoencoders <ref type="bibr" target="#b36">[37]</ref> in form of 2-D end-to-end CNNs.</p><p>1) Encoders: They are responsible for extracting deep and abstract features, whose structure is like nonfully connected part of CNNs.</p><p>Encoders contain several basic blocks. In common cases, each basic block can be described as a function or map that transforms an input multichannel data x with a size of width × height × channels to downsampled 2-D feature maps y. A basic block is stacked by one or several layers of convolutional filters, ReLU activation layers (provide nonlinearity) and a maxpooling layer (for downsampling). Fig. <ref type="figure" target="#fig_1">2</ref>(a) shows a typical basic block in encoders (maxpooling layers are not shown in that subfigure). In practice, such as FCN-8s/16s/32s and SegNet models, encoders are usually transplanted from the VGG16 network, because it is easier to exploit the parameters of existing CNNs pretrained on other large training datasets <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b52">[53]</ref>. In this paper, however, we only train networks on remote sensing datasets.</p><p>2) Decoders: They are responsible for generating the accurate boundary localization and labeling result from features extracted by the encoder part. It is the essential difference between FCNs and CNNs.</p><p>The implementations of decoders vary. In FCN-8s/16s/32s model, the decoder consists of a transpose convolutional layer <ref type="bibr" target="#b46">[47]</ref>. SegNet model was devised to make decoders symmetrical to encoders, which made networks more balanced. In addition, SegNet model adopts pooling-indices-reused based upsampling layer instead of deconvolutional technique <ref type="bibr" target="#b52">[53]</ref>.</p><p>In our framework, decoders are also stacked by basic blocks and each block is symmetrically same to the corresponding block in encoders, except that the transpose convolutional layers replace convolutional layers. The specific structure of our network model will be described in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Basic Shortcut Convolutional Block</head><p>Fig. <ref type="figure" target="#fig_1">2</ref>(a) describes an old-style convolutional basic block in CNNs, which contains two convolutional layers with a 3 × 3 window size and two ReLU activation layers. It can be written as follows:</p><formula xml:id="formula_2">F old-style (x) = Conv R (Conv R (x, W 1 ), W 2 ).<label>(2)</label></formula><p>Here x denotes the input data, function Conv R (x, W ) represents the convolutional mapping with ReLU activation function.<ref type="foot" target="#foot_0">1</ref> W 1  and W 2 are the weights to learn in each convolutional layer. However, in back-propagation (BP) algorithm, this old-style block easily tends to be affected by the covariate shift problem when training a deep network <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b44">[45]</ref>. VGG-like block [see Fig. <ref type="figure" target="#fig_6">2(b)</ref>] is another popular convolutional block <ref type="bibr" target="#b40">[41]</ref>. It was exploited to overcome drawbacks of old-style block by adding a batch normalization layer after convolutional layers. Batch normalization layers can solve the internal covariate shift problem and accelerate training process because it transforms the mean activation of input data close to 0 and the standard deviation close to 1 BN R (x) = max(</p><p>x -E(x)</p><formula xml:id="formula_3">Var(x) + • γ + β, 0)<label>(3)</label></formula><p>where BN R (•) denotes the output of BN layer with ReLU activation, E(x) and Var(x) are the mean and variance of input data x, is a positive small constant value, γ and β are the scale and shift parameter of a batch of data. As a result, VGG-like block can be described by this function</p><formula xml:id="formula_4">F VGG-like (x) = BN R (Conv(Conv(x, W 1 ), W 2 )). (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>The basic convolutional block in our proposed frameworks is the shortcut block (see Fig. <ref type="figure" target="#fig_6">2(c</ref>), also known as identity mapping residual block). It contains two branches: the main one is the bottlenecks block <ref type="bibr" target="#b44">[45]</ref> and the fine one is an identity mapping (a single 1 × 1 window size convolutional layer and a batch normalization layer) <ref type="bibr" target="#b56">[57]</ref>. It can be represented as</p><formula xml:id="formula_6">F shortcut (x) = ReLU(F main shortcut (x) + F fine shortcut (x))<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">F main shortcut (x) = BN(Conv(Conv(Conv(x, W 1 ), W 2 ), W 3 ))<label>(6)</label></formula><p>and</p><formula xml:id="formula_8">F fine shortcut (x) = BN(Conv(x, W 0 )).<label>(7)</label></formula><p>In this type of block, the main bottlenecks block F main shortcut (x) contains three layers with different window sizes, unlike two 3 × 3 × C convolutional layers in the VGG-like block. The bottlenecks block is designed to reduce memory in deep networks, which was first proposed in <ref type="bibr" target="#b56">[57]</ref>. The main branch learns the residual information of input data. The shortcut branch F fine shortcut helps to propagate gradients directly from output to input when training large and deep networks. In addition, the gradients in architectures with shortcut block are far more resistant to shattering decaying sublinearly than VGG-like block and bottlenecks block <ref type="bibr" target="#b57">[58]</ref>.</p><p>In Section IV-A2, we performed a set of comparative experiments between VGG-like blocks and shortcut blocks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architectures</head><p>Based on our shortcut block, we design two encodersdecoders network architectures: the symmetrical normalshortcut fully convolutional networks [SNFCN, Fig. <ref type="figure" target="#fig_2">3(a)</ref>] and the symmetrical dense-shortcut fully convolutional networks [SDFCN, Fig. <ref type="figure" target="#fig_2">3(b)]</ref>.</p><p>SNFCN is a sequential network model and consists of an input layer, encoders, decoders, a softmax layer, and an output layer. Encoders are stacked by shortcut blocks and maxpooling layers, whereas decoders are stacked by transpose-convolutional layers and shortcut blocks. The softmax layer is responsible for transforming 2-D deep features into a classification map.</p><p>Compared to SNFCN, SDFCN adds three additional identity mapping shortcut connections between symmetrical encoderdecoder pairs [The orange striped arrows in Fig. <ref type="figure" target="#fig_2">3(b)</ref>]. It ensures that the gradients information can be passed directly to the upper layers of network. As a result, the output of ith layer of decoders in SDFCN is</p><formula xml:id="formula_9">F output (x i , x i ) = F main shortcut (x i ) + F fine shortcut (x i ) + x i (8)</formula><p>where x i is the input data of the ith layer, x i is the output data of the corresponding layer in encoders, F main shortcut (x) and F fine shortcut (x) are the same as ( <ref type="formula" target="#formula_7">6</ref>) and <ref type="bibr" target="#b6">(7)</ref>.</p><p>The additional identity mapping branch in SDFCN makes F main shortcut (x) learn the second-order residual of the input data.</p><p>Consequently, the output of each decoder is influenced by not only itself, but also its corresponding layer in encoder. Because of the identity mapping, this mechanism helps the gradients propagate and improves the network's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Post processing</head><p>Although the size of a VHR RS imagery scene is variable, however in our framework, a full input RS image is sliced into patches because of the memory limitation. Each patch has 128 × 128 pixels. During the training stage, the patches are sequentially trained by FCNs. During the prediction period, the trained FCNs will predict each patch from the input RS image, and results be merged with different overlay strategies (0-overlay, 20%-overlay, 50%-overlay, 75%-overlay, and 87.5%-overlay) in a majority voting method <ref type="bibr" target="#b58">[59]</ref>. The entire prediction procedure in our framework is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Experiments were conducted on two VHR image datasets provided by ISPRS <ref type="bibr" target="#b31">[32]</ref>. These were the ISPRS Vaihingen dataset [see Fig. <ref type="figure" target="#fig_3">4(a)-(c)</ref>] and the ISPRS Potsdam dataset [see Fig. <ref type="figure" target="#fig_3">4(d)-(f)</ref>]. Both these two datasets are digital orthophoto maps (DOMs) and digital surface models (DSMs) generated by aerial images in Germany, with a ground sample distance (GSD) of 9and 5 cm. In our research, only DOMs are taken into consideration for generalization purposes. The task in our experiments was to classify all pixels in the images into six categories: impervious surfaces (white), building (blue), low vegetation (cyan), tree (green), car (yellow), and background (red). Pixel overall accuracy (OA), kappa coefficient (K, Kappa) <ref type="bibr" target="#b59">[60]</ref>, mean intersection over union (mIoU) <ref type="bibr" target="#b60">[61]</ref>, and F1-score were adopted as the accuracy assessment metrics.</p><p>Experiments were divided into two parts. The first part described the training details of our SNFCN and SDFCN, analyzed the effectiveness of these two architectures, and tested our complete frameworks with postprocessing methods on ISPRS Vaihingen dataset. In the last part, we trained SDFCN and made a comparison between SDFCN and other mainstream FCNs models on ISPRS Potsdam dataset. The entire process of training and predicting was discussed in Section III and Fig. <ref type="figure" target="#fig_0">1</ref>. To make fair results, we have submitted our results to the ISPRS server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on Vaihingen dataset 1) Training Details:</head><p>The Vaihingen dataset contains 33 DOM image tiles with near infrared (IR), red (R), and green (G) spectral bands, 16 of which were manually labeled. In our training scheme, the DSM data were neglected. The labeled DOM data (spectral data) were divided into two parts: training data (ID <ref type="bibr">1, 3, 11, 13, 15, 17, 21, 26, 28, 30, 32, and 34)</ref> and validation data <ref type="bibr">(ID 5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37)</ref>. The input data have a shape of 128 × 128 × 3, thus, training data and validation data are sliced into patches from the original tiles with a 48-pixel-stride (62.5% overlay) and 128-pixel-stride (100% overlay) separately. In total, there are 23 449 patches in training dataset and 1023 patches in the validation dataset.  All FCNs in our experiments were trained by the BP algorithm <ref type="bibr" target="#b33">[34]</ref> with a minibatch size of 16. Adadelta <ref type="bibr" target="#b61">[62]</ref> was adopted as the weights updating optimization algorithm. With adadelta, no default learning rate needs to be set because of its unique updating rule. The loss function is the logloss function (categorical cross entropy function). Each FCN model was trained for 30 epochs.  In our experiments, we used the following transformations for training data augmentation.</p><p>(1) Random scaling in the range [1, 1.2].</p><p>(2) Random translation by [-5, 5] pixels.   (3) Random vertically and horizontally flipping.</p><p>2) Experiments of Basic Blocks: In this part, we first trained SNFCN-67 (SNFCN with 67 layers) and SDFCN-67 (SDFCN with 67 layers). As comparison, VGGFCN-43 (VGGFCN with 43 layers<ref type="foot" target="#foot_4">3</ref> ) architecture was also trained. VGGFCN-43 is same to SNFCN-67 except that all shortcut blocks [see Fig. <ref type="figure" target="#fig_6">2(c)</ref>] were replaced by VGG-like basic blocks [see Fig. <ref type="figure" target="#fig_6">2(a)</ref>].</p><p>To find the relationship between depth and performance of different basic blocks, we then trained three deeper networks: VGGFCN-91, SNFCN-139, and SDFCN-139. The basic blocks of these deeper networks are both stacked by three shallow basic blocks.</p><p>In addition, we also trained two popular FCN models: Seg-Net <ref type="bibr" target="#b52">[53]</ref> and FCN-8s <ref type="bibr" target="#b46">[47]</ref>. The training OA and validation OA and training time per epoch (TTP) of these models are listed in Table <ref type="table" target="#tab_0">I</ref>.</p><p>The result indicates that VGG-like basic blocks performed the best among shallow networks. If networks go deeper, VGG-like blocks lead to a worse output than shortcut blocks. Moreover, the training OA and validation OA of SDFCN were both slightly better than SNFCN, which means the dense block-wise shortcut connections help to improve networks' performance.</p><p>The specific training process of VGGFCN and SNFCN are shown in Fig. <ref type="figure" target="#fig_4">5</ref>. This figure also shows that shortcut blocks have a better result and a more stable validation OA curve than VGG-like blocks when training deeper networks.</p><p>Four examples of output image patches are listed in Fig. <ref type="figure" target="#fig_5">6</ref>. As seen in the comparative figure, all models could produce good semantic segmentation results most of the time even in shadow areas. However, models cannot easily identify inconsistent stitching error (the first row in the figure) and there is a slight probability of confusion between low vegetation and trees (the second and third row).</p><p>3) Postprocessing: In our framework, we adopted an overlay fusion strategy with majority voting as the postprocessing method. SNFCN-139 and SDFCN-139 are evaluated with different degrees of overlap (0-overlay, 20%-overlay, 50%-overlay, 75%-overlay, and 87.5%-overlay). The complete results are shown in Table <ref type="table" target="#tab_1">II</ref> and Fig. <ref type="figure" target="#fig_7">7</ref>.</p><p>On the one hand, from Fig. <ref type="figure" target="#fig_7">7</ref>, the OA, K, and mIoU increased as the degree of overlay increases. Fig. <ref type="figure" target="#fig_8">8</ref> shows visually a comparison of the output qualitatively supporting this conclusion. The 0-overlay policy generated a result with a block effect, whereas the 75%-overlay and 87.5%-overlay policy really makes the results better and boundary more continuous. In terms of calculation cost, the 75%-overlay strategy is enough in practice.</p><p>One the other hand, the "salt and pepper" phenomena still exist in the results from SNFCN-139, even with an 87.5% degree of overlay. However, SDFCN-139 greatly alleviated this problem. In addition, SDFCN-139 also performed slightly better qualitatively, as shown by numerical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Online Comparison:</head><p>We predicted all images that ISPRS do not provide ground truth by SDFCN-139 with 87.5% overlay policy and then submitted the generated labeling results to ISPRS server. <ref type="foot" target="#foot_5">4</ref>Our detail semantic segmentation result is shown in  lenge are listed in Table <ref type="table" target="#tab_3">IV</ref>. 5, 6 All quality measures except for OA are F1-scores in this table. From the results, impervious surfaces, building, and tree could be accurately classified. Some cars are misclassified to impervious while low vegetations are easy to confuse with trees. We can find that most of algorithms are dealing with IRRG image data with DSM and nDSM data. Our result achieved state-of-the-art performance among all methods only by processing IRRG images, except for CASIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Potsdam Dataset 1) Training Details:</head><p>To assess the capability of our framework for generalization, we trained FCN-8s, SegNet, and SDFCN-139 on the ISPRS Potsdam dataset. The Potsdam dataset contains 38 image tiles with near infrared (IR), red (R), green (G), and blue (B) spectral bands, 24 of which were manually labeled. Like the Vaihingen dataset, the whole labeled tiles were divided into training data (ID 2_10, 2_11, 3_10, 3_11, 4_10, 4_11, 5_10, 5_11, 6_7, 6_8, 6_9, 6_10, 6_11, 7_7, 7_8, 7_9, 7_10, and 7_11) and validation data (ID 2_12, 3_12, 4_12, 5_12, 6_12, and 7_12). The input shape of the network was 128 × 128 × 4. In total, there were 169 280 patches in training dataset and 8464 in the validation dataset. We directly trained these three networks in 30 epochs using adadelta, only the best validation OAs were recorded. Other training details are the same as shown in Section IV-A. 5 In this table, all values represent the F1 score of each category except OA and all results are sorted by overall accuracy. Our result generated by SDFCN-139 is named CVEO. 6 The whole results of ISPRS Vaihingen 2-D Semantic Labeling challenge can be found at http://www2.isprs.org/vaihingen-2d-semantic-labeling-contest.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Postprocessing:</head><p>The training process is recorded in Table V <ref type="foot" target="#foot_6">7</ref> and the results from different overlay strategies are shown in Table <ref type="table" target="#tab_0">VI</ref>. From the two tables, we found that FCN-8s and our SDFCN-139 got better results than SegNet. The 87.5% overlay strategy increased separately the OA, K, and mIoU by 0.014, 0.015, and 0.018, respectively. These results again indicate that our SDFCN-139 network and overlay strategy are better balanced between model size and performance than the classical FCN models.</p><p>3) Online Comparison: We also predicted all images in Potsdam dataset that ISPRS do not provide ground truth by SDFCN-139 with 87.5% overlay policy and then submitted the generated labeling results to ISPRS server.</p><p>Our detail semantic segmentation result is shown in Table <ref type="table" target="#tab_6">VII</ref>. Part of results of ISPRS Potsdam 2-D Semantic Labeling challenge are listed in Table <ref type="table" target="#tab_6">VIII</ref>. 9, 10  From the result, impervious surfaces, building and tree could still be accurately classified. Unlike the result in Vaihingen dataset, cars and low vegetations can be easier to segment and identify. Table VIII still shows that our SDFCN-139 and overlay policy have achieved mainstream performance on remote sensing semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we introduced two semantic segmentation frameworks: SNFCN and SDFCN to deal with VHR remote sensing images. These frameworks include deep FCNs model and postprocessing method. Based on experiments on the IS-PRS Vaihingen and Potsdam datasets, we found that our SDFCN model performed better than the classic FCN-8s and SegNet models. Moreover, we found that the shortcut basic blocks converge more stably and have a better OA in deeper FCNs than VGG-like blocks. We adopted an overlay fusion strategy with simple majority voting as the postprocessing method, which increased the overall accuracy by 1%-2% that helps to eliminate "salt and pepper" phenomena and block effects.</p><p>In the future, we will research the three following issues. First, we will research CRF-based FCN models that predict both unary and pairwise terms of CRFs in one model to improve the visual performance. Second, we will design new FCNs models to minimize the memory cost and accelerate the prediction process, which make it possible to run in embedded devices. Third, we will study how to transfer a trained deep networks model to fit for different image data sources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Whole prediction procedure of our framework.</figDesc><graphic coords="3,40.44,68.14,246.24,150.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Old-style block. (b) VGG-like block which contains 18C parameters. (c) Shortcut block whose main branch (left) contains 14C parameters and shortcut block (right) contains 4C parameters. In these figures, "Conv" denotes a convolutional layer, "ReLU" denotes ReLU activation layer, and "BN" means a batch normalization layer.</figDesc><graphic coords="3,340.96,68.58,171.13,319.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Architectures of SNFCN and SDFCN. Both SNFCN and SDFCN are stacked by encoders, decoders, and last softmax output layers. Encoders and decoders are symmetrically designed. Encoders are stacked by shortcut blocks and maxpooling layers, whereas decoders are stacked by transpose-convolutional layers and shortcut blocks. The difference between SNFCN and SDFCN is that the latter contains three additional identity mapping shortcut connections between symmetrical encoder-decoder pairs. (a) SNFCN. (b) SDFCN.</figDesc><graphic coords="4,96.12,67.92,406.29,404.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Datasets in our experiments. (a) ISPRS Vaihingen dataset. (b) Tile 37 in Vaihingen dataset (1996 × 1995). (c) Ground truth. (d) ISPRS Potsdam dataset. (e) Tile 2-10 in Potsdam dataset (6000 × 6000). (f) Ground truth. (g) Legend of ground truth maps.</figDesc><graphic coords="5,100.55,68.07,388.89,271.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Validation OA curves of VGGFCN and SNFCN with different depths. We found the 91-layer VGGFCN performed worse than the 43-layer VGGFCN, whereas the 139-layer SNFCN achieved a better OA than the 67-layer SNFCN. It shows that shortcut blocks have a better result and a more stable validation OA curve than VGG-like blocks when training deeper networks. (a) VGGFCN. (b) SNFCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example output of four 128×128 image patches on Vaihingen data. (a) Original image. (b) Ground truth. (c) SegNet. (c) FCN-8s. (d) SNFCN-139. (e) SDFCN-139. (f) Legend.</figDesc><graphic coords="7,66.17,68.41,457.75,359.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Accuracy assessment chart of different overlay policy. The OA, K, and mIoU are represented in form of bar, solid line, and dash line in the figure. From the figure, SDFCN-139 always performed slightly better than SNFCN-139.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Example output of two 600×800 image patches in Vaihingen data. The first row of each example is the result of SNFCN-139 and the second row is SDFCN-139. (a) Original image and ground truth. (b) 0-overlay. (c) 50% overlay. (d) 75% overlay. (e) 87.5% overlay. (f) Legend.</figDesc><graphic coords="8,77.76,68.33,443.16,485.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I METRICS</head><label>I</label><figDesc>OF TRAINING PROCESS ON VAIHINGEN DATASET</figDesc><table><row><cell></cell><cell>Training OA</cell><cell>Validation OA</cell><cell>Parameters</cell><cell>TTP(s)</cell></row><row><cell>VGGFCN-43</cell><cell>0.8825</cell><cell>0.8571</cell><cell>13 715 230</cell><cell>657</cell></row><row><cell>SNFCN-67</cell><cell>0.8834</cell><cell>0.8520</cell><cell>29 115 166</cell><cell>702</cell></row><row><cell>SDFCN-67</cell><cell>0.9009</cell><cell>0.8553</cell><cell>29 115 166</cell><cell>728</cell></row><row><cell>VGGFCN-91</cell><cell>0.8589</cell><cell>0.8460</cell><cell>38 509 342</cell><cell>923</cell></row><row><cell>SNFCN-139</cell><cell>0.8985</cell><cell>0.8663</cell><cell>52 590 366</cell><cell>901</cell></row><row><cell>SDFCN-139</cell><cell>0.9184</cell><cell>0.8668</cell><cell>52 590 366</cell><cell>979</cell></row><row><cell>SegNet</cell><cell>0.8590</cell><cell>0.8467</cell><cell>24 717 662</cell><cell>620</cell></row><row><cell>FCN-8s</cell><cell>0.8829</cell><cell>0.8499</cell><cell>184 631 780</cell><cell>638</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="4">ACCURACY ASSESSMENT OF DIFFERENT OVERLAY STRATEGY</cell></row><row><cell cols="3">ON VAIHINGEN VALIDATION DATASET</cell><cell></cell></row><row><cell></cell><cell>OA</cell><cell>K</cell><cell>mIoU</cell><cell>T(s)</cell></row><row><cell>SNFCN-139</cell><cell cols="3">0.8681 0.8359 0.5978</cell><cell>30.2</cell></row><row><cell>SNFCN-139 with 50% overlay</cell><cell cols="3">0.8752 0.8428 0.6068</cell><cell>61.8</cell></row><row><cell>SNFCN-139 with 75% overlay</cell><cell cols="4">0.8777 0.8435 0.6117 190.1</cell></row><row><cell cols="5">SNFCN-139 with 87.5% overlay 0.8769 0.8445 0.6131 706.3</cell></row><row><cell>SDFCN-139</cell><cell cols="3">0.8681 0.8337 0.6083</cell><cell>37.2</cell></row><row><cell>SDFCN-139 with 50% overlay</cell><cell cols="3">0.8757 0.8426 0.6193</cell><cell>73.4</cell></row><row><cell>SDFCN-139 with 75% overlay</cell><cell cols="4">0.8774 0.8446 0.6233 243.6</cell></row><row><cell cols="5">SDFCN-139 with 87.5% overlay 0.8779 0.8452 0.6238 864.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III OUR</head><label>III</label><figDesc>RESULT OF SDFCN-139 OF ISPRS VAIHINGEN 2-D SEMANTIC LABELING CHALLENGE</figDesc><table><row><cell>↓ Predicted // Reference →</cell><cell>Impervious surfaces</cell><cell cols="2">Building Low vegetation</cell><cell>Tree</cell><cell>Car</cell><cell>Background</cell></row><row><cell>Impervious surfaces</cell><cell>0.935</cell><cell>0.026</cell><cell>0.031</cell><cell cols="2">0.006 0.002</cell><cell>0</cell></row><row><cell>Building</cell><cell>0.068</cell><cell>0.909</cell><cell>0.021</cell><cell>0.002</cell><cell>0</cell><cell>0</cell></row><row><cell>Low vegetation</cell><cell>0.051</cell><cell>0.019</cell><cell>0.803</cell><cell>0.127</cell><cell>0</cell><cell>0</cell></row><row><cell>Tree</cell><cell>0.012</cell><cell>0.002</cell><cell>0.09</cell><cell>0.896</cell><cell>0</cell><cell>0</cell></row><row><cell>Car</cell><cell>0.194</cell><cell>0.076</cell><cell>0.006</cell><cell cols="2">0.001 0.722</cell><cell>0</cell></row><row><cell>Background</cell><cell>0.328</cell><cell>0.375</cell><cell>0.017</cell><cell cols="2">0.002 0.041</cell><cell>0.237</cell></row><row><cell>Precision/Correctness</cell><cell>0.877</cell><cell>0.94</cell><cell>0.831</cell><cell cols="2">0.874 0.882</cell><cell>0.991</cell></row><row><cell>Recall/Completeness</cell><cell>0.935</cell><cell>0.909</cell><cell>0.803</cell><cell cols="2">0.896 0.722</cell><cell>0.237</cell></row><row><cell>F1</cell><cell>0.905</cell><cell>0.924</cell><cell>0.817</cell><cell cols="2">0.885 0.794</cell><cell>0.383</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV PART</head><label>IV</label><figDesc>OF THE ONLINE RESULTS OF ISPRS VAIHINGEN TEST DATASET</figDesc><table><row><cell>Name</cell><cell>Impervious surfaces</cell><cell cols="3">Building Low vegetation Tree</cell><cell>Car</cell><cell>OA</cell><cell>Remark</cell></row><row><cell>CASIA</cell><cell>92.7</cell><cell>95.3</cell><cell>84.3</cell><cell>89.6</cell><cell cols="2">80.8 90.6</cell><cell></cell></row><row><cell>HUSTW4</cell><cell>91.2</cell><cell>94.7</cell><cell>83.2</cell><cell>89.2</cell><cell cols="3">81.6 89.5 With DSM and nDSM data</cell></row><row><cell>CVEO</cell><cell>90.5</cell><cell>92.4</cell><cell>81.7</cell><cell>88.5</cell><cell cols="2">79.4 88.3</cell><cell>Our SDFCN-139 model</cell></row><row><cell>ADL_3</cell><cell>89.5</cell><cell>93.2</cell><cell>82.3</cell><cell>88.2</cell><cell>63.3</cell><cell>88</cell><cell>With DSM and nDSM data</cell></row><row><cell>WUH_C4</cell><cell>89.7</cell><cell>94.7</cell><cell>79.9</cell><cell>87.1</cell><cell cols="3">68.2 87.9 With DSM and nDSM data</cell></row><row><cell>RIT_L8</cell><cell>89.6</cell><cell>92.2</cell><cell>81.6</cell><cell>88.6</cell><cell>76</cell><cell cols="2">87.8 With DSM and nDSM data</cell></row><row><cell>RIT_4</cell><cell>88</cell><cell>93</cell><cell>79.4</cell><cell>88.1</cell><cell>68.7</cell><cell>87</cell><cell>With DSM and nDSM data</cell></row><row><cell>CONC_2</cell><cell>88.5</cell><cell>91.4</cell><cell>78.3</cell><cell>87.1</cell><cell cols="2">77.7 86.5</cell><cell></cell></row><row><cell>HUST</cell><cell>86.9</cell><cell>92</cell><cell>78.3</cell><cell>86.9</cell><cell>29</cell><cell>85.9</cell><cell></cell></row><row><cell>UPB</cell><cell>87.5</cell><cell>89.3</cell><cell>77.3</cell><cell>85.8</cell><cell cols="2">77.1 85.1</cell><cell></cell></row><row><cell>UCal</cell><cell>86.8</cell><cell>90.8</cell><cell>73</cell><cell>84.6</cell><cell cols="2">42.2 84.1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table III .</head><label>III</label><figDesc>Part of results of ISPRS Vaihingen 2-D Semantic Labeling chal-</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII OUR</head><label>VII</label><figDesc>RESULT OF ISPRS POTSDAM 2-D SEMANTIC LABELING CHALLENGE</figDesc><table><row><cell cols="3">↓ predicted // reference → impervious surfaces</cell><cell cols="3">building low vegetation</cell><cell>tree</cell><cell>car</cell><cell>background</cell></row><row><cell cols="2">impervious surfaces</cell><cell>0.937</cell><cell>0.023</cell><cell cols="2">0.026</cell><cell>0.008</cell><cell>0</cell><cell>0.005</cell></row><row><cell>building</cell><cell></cell><cell>0.028</cell><cell>0.958</cell><cell cols="2">0.008</cell><cell>0.002</cell><cell>0</cell><cell>0.003</cell></row><row><cell cols="2">low vegetation</cell><cell>0.037</cell><cell>0.011</cell><cell cols="2">0.895</cell><cell>0.052</cell><cell>0</cell><cell>0.005</cell></row><row><cell>tree</cell><cell></cell><cell>0.036</cell><cell>0.006</cell><cell cols="2">0.111</cell><cell cols="2">0.844 0.002</cell><cell>0.002</cell></row><row><cell>car</cell><cell></cell><cell>0.012</cell><cell>0.015</cell><cell>0</cell><cell></cell><cell cols="2">0.007 0.954</cell><cell>0.011</cell></row><row><cell cols="2">background</cell><cell>0.376</cell><cell>0.153</cell><cell cols="2">0.17</cell><cell cols="2">0.018 0.008</cell><cell>0.275</cell></row><row><cell cols="2">Precision/Correctness</cell><cell>0.888</cell><cell>0.932</cell><cell cols="2">0.835</cell><cell cols="2">0.907 0.953</cell><cell>0.749</cell></row><row><cell cols="2">Recall/Completeness</cell><cell>0.937</cell><cell>0.958</cell><cell cols="2">0.895</cell><cell cols="2">0.844 0.954</cell><cell>0.275</cell></row><row><cell>F1</cell><cell></cell><cell>0.912</cell><cell>0.945</cell><cell cols="2">0.864</cell><cell cols="2">0.874 0.954</cell><cell>0.402</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE VIII</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">PART OF THE ONLINE RESULTS OF ISPRS POTSDAM TEST DATASET</cell></row><row><cell>Name</cell><cell>Impervious surfaces</cell><cell cols="3">Building Low vegetation Tree</cell><cell>Car</cell><cell>OA</cell><cell></cell><cell>Remark</cell></row><row><cell>AMA_1</cell><cell>93.4</cell><cell>96.8</cell><cell>87.7</cell><cell>88.8</cell><cell>96</cell><cell>91.2</cell><cell></cell></row><row><cell>CASIA2</cell><cell>93.3</cell><cell>97</cell><cell>87.7</cell><cell>88.4</cell><cell cols="2">96.2 91.1</cell><cell></cell></row><row><cell>AZ3</cell><cell>93.1</cell><cell>96.3</cell><cell>87.2</cell><cell>88.6</cell><cell>96</cell><cell cols="3">90.7 With DSM and nDSM data</cell></row><row><cell>RIT6</cell><cell>92.5</cell><cell>97</cell><cell>86.5</cell><cell>87.2</cell><cell cols="4">94.9 90.2 With DSM and nDSM data</cell></row><row><cell>BUCT_1</cell><cell>92.4</cell><cell>97</cell><cell>86.3</cell><cell>86.4</cell><cell>93.9</cell><cell>90</cell><cell cols="2">With DSM and nDSM data</cell></row><row><cell>CVEO</cell><cell>91.2</cell><cell>94.5</cell><cell>86.4</cell><cell>87.4</cell><cell>95.4</cell><cell>89</cell><cell cols="2">Our SDFCN-139 model</cell></row><row><cell>WuhZ</cell><cell>91.4</cell><cell>95.4</cell><cell>85.4</cell><cell>86.8</cell><cell cols="2">94.6 88.9</cell><cell></cell></row><row><cell>KLab_2</cell><cell>89.7</cell><cell>92.7</cell><cell>83.7</cell><cell>84</cell><cell cols="2">92.1 86.7</cell><cell></cell></row><row><cell>UZ_1</cell><cell>89.3</cell><cell>95.4</cell><cell>81.8</cell><cell>80.5</cell><cell cols="2">86.5 85.8</cell><cell></cell></row><row><cell>GU</cell><cell>87.1</cell><cell>94.7</cell><cell>77.1</cell><cell>73.9</cell><cell cols="4">81.2 82.9 With DSM and nDSM data</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>All convolutional layers mentioned in this paper are with a stride of</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We adopt zero-padding policy to make the output of a convolutional layer have the same shape as its input (padding = "same" in Keras<ref type="bibr" target="#b55">[56]</ref> framework).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>All the experimental results were processed on a desktop PC with Intel Core i7 6700K (4C8T),</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>32GB RAM. and Nvidia GeForce GTX1080 Ti (11264MB memory). The implementation of the framework was based on Keras 2.0.7<ref type="bibr" target="#b55">[56]</ref> and Tensorflow 1.3.0<ref type="bibr" target="#b62">[63]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>In fact, VGGFCN-43 has the same depth with SNFCN-67 and SDFCN-67 because there is no extra shortcut branch in VGGFCN-43.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>Our result on Vaihingen dataset can be found at: http://ftp.ipi.unihannover.de/ISPRS_WGIII_website/ISPRSIII_4_Test_results/2D_labeling_ vaih/2D_labeling_Vaih_details_CVEO/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>The parameters of all models are slightly more than Vaihingen dataset because images have four spectral bands in Potsdam dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p><ref type="bibr" target="#b7">8</ref> Our result on Potsdam dataset can be found at: http://ftp.ipi.unihannover.de/ISPRS_WGIII_website/ISPRSIII_4_Test_results/2D_labeling_ potsdam/2D_labeling_Potsdam_details_CVEO/index.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>html<ref type="bibr" target="#b8">9</ref> In this table, all values represent the F1 score of each category except OA and all results are sorted by overall accuracy. Our result generated by SDFCN-139 is named</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>CVEO.<ref type="bibr" target="#b9">10</ref> The whole results of ISPRS Potsdam 2-D Semantic Labeling challenge can be found at http://www2.isprs.org/potsdam-2d-semantic-labeling.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors would like to thank the ISPRS for providing the research community with the wonderful datasets. The authors would also like to thank the developers in the Keras and Tensorflow developer community for their open source deep learning projects.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the LIESMARS Special Research Funding and in part by the Fundamental Research Funds for the Central Universities.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What&apos;s wrong with pixels? Some recent developments interfacing remote sensing and GIS</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Strobl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GeoBIT/GIS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="17" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geographic object-based image analysis (GEO-BIA): A new name for a new discipline</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Object-Based Image Analysis</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="75" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object based image analysis for remote sensing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="16" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geographic object-based image analysis towards a new paradigm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="180" to="191" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object-based approaches to change analysis and thematic map update: challenges and limitations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mcdermid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Linke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Mclane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="462" to="466" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparing machine learning classifiers for object-based land cover classification using very high resolution imagery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object-based image analysis for coral reef benthic habitat mapping with several classification algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wahidin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Siregar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nababan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wouthuyzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Environ. Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="222" to="227" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pixel-based and objectbased classifications using high-and medium-spatial-resolution imageries in the urban and suburban landscapes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Estoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Murayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Akiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geocarto Int</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1113" to="1129" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object-based urban land cover classification using rule inheritance over very high-resolution multisensor and multitemporal data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GISci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="182" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Objectoriented and pixel-based classification approaches to classify tropical successional stages using airborne high spatial resolution images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Piazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Vibrans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liesenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Refosco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GISci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="226" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object-based landcover supervised classification for very-high-resolution UAV images using stacked denoising autoencoders</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3373" to="3385" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comparison of pixelbased and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using SPOT-5 HRG imagery</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Duro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Dub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Change detection from remotely sensed images: From pixel-based to object-based approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object-based image analysis: strengths, weaknesses, opportunities and threats (SWOT)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Conf. OBIA</title>
		<meeting>1st Int. Conf. OBIA</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="4" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Advances in geographic object-based image analysis with ontologies: A review of main contributions and limitations from a remote sensing perspective</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arvor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Durieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Andrs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Laporte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="125" to="137" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning-based classification of hyperspectral data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sens</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">258619</biblScope>
			<date type="published" when="2015">2015</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised deep feature extraction for remote sensing image classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName><surname>Valls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1349" to="1362" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep feature extraction and classification of hyperspectral images based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6232" to="6251" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing data: A technical tutorial on the state of the art</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object-based convolutional neural network for high-resolution imagery classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3386" to="3396" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for large-scale remote-sensing image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="645" to="657" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for building and road extraction: Preliminary results</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Geosci. Remote Sens. Symp</title>
		<meeting>IEEE Geosci. Remote Sens. Symp</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1591" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic segmentation of aerial images with an ensemble of CNSS</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Anna. Photogramm., Remote Sens. Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="473" to="480" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classification with an edge: Improving semantic image segmentation with boundary detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogram. Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02585</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic segmentation of earth observation data using multimodal and multi-scale deep networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="180" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning dual multi-scale manifold ranking for semantic segmentation of high-resolution images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">500</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2009 IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>2009 IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The ISPRS benchmark on urban object classification and 3D building reconstruction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="298" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recent advances in convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recog</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Conf</title>
		<meeting>12th Int. Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Machine Learn</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Conf. Mach. Learn</title>
		<meeting>27th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recog</title>
		<meeting>Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer. Vis</title>
		<meeting>IEEE Int. Conf. Computer. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recog</title>
		<meeting>Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-12">Dec. 2017</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Machine learning for aerial image labeling</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Toronto, ON, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Effective semantic pixel labelling with convolutional networks and Conditional Random Fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Janney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van-Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2015 IEEE Conf. Comput. Vis. Pattern Recog. Workshops</title>
		<meeting>2015 IEEE Conf. Comput. Vis. Pattern Recog. Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Github</forename><surname>Keras</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08591</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The elementary statistics of majority voting</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Penrose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="57" />
			<date type="published" when="1946">1946</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A reappraisal of the kappa coefficient</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Epidemiol</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="949" to="958" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">ADADELTA: An adaptive learning rate method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
