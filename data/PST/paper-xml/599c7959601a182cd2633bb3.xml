<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimal Approximation with Sparsely Connected Deep Neural Networks *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Helmut</forename><surname>Bölcskei</surname></persName>
							<email>hboelcskei@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology and Electrical Engineering and Department of Mathematics</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philipp</forename><surname>Grohs</surname></persName>
							<email>philipp.grohs@univie.ac.at</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Faculty of Mathematics</orgName>
								<orgName type="department" key="dep2">Research Platform Data-Science@UniVienna</orgName>
								<orgName type="institution">University of Vienna</orgName>
								<address>
									<postCode>1090</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Vienna</orgName>
								<address>
									<postCode>1090</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gitta</forename><surname>Kutyniok</surname></persName>
							<email>kutyniok@math.tu-berlin.de</email>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Institut für Mathematik</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<postCode>10623</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philipp</forename><surname>Petersen</surname></persName>
							<email>philipp.petersen@maths.ox.ac.uk</email>
							<affiliation key="aff4">
								<orgName type="department">Mathematical Institute</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<postCode>OX2 6GG</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimal Approximation with Sparsely Connected Deep Neural Networks *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E976CCDB8B1D6809A7C66CDA0EE3AE5C</idno>
					<idno type="DOI">10.1137/18M118709X</idno>
					<note type="submission">Received by the editors May 14, 2018; accepted for publication (in revised form) November 29, 2018;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural networks</term>
					<term>function approximation</term>
					<term>optimal sparse approximation</term>
					<term>sparse connectivity</term>
					<term>wavelets</term>
					<term>shearlets AMS subject classifications. 41A25</term>
					<term>82C32</term>
					<term>42C40</term>
					<term>42C15</term>
					<term>41A46</term>
					<term>68T05</term>
					<term>94A34</term>
					<term>94A12</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We derive fundamental lower bounds on the connectivity and the memory requirements of deep neural networks guaranteeing uniform approximation rates for arbitrary function classes in L 2 (R d ). In other words, we establish a connection between the complexity of a function class and the complexity of deep neural networks approximating functions from this class to within a prescribed accuracy. Additionally, we prove that our lower bounds are achievable for a broad family of function classes. Specifically, all function classes that are optimally approximated by a general class of representation systems-so-called affine systems-can be approximated by deep neural networks with minimal connectivity and memory requirements. Affine systems encompass a wealth of representation systems from applied harmonic analysis such as wavelets, ridgelets, curvelets, shearlets, α-shearlets, and, more generally, α-molecules. Our central result elucidates a remarkable universality property of neural networks and shows that they achieve the optimum approximation properties of all affine systems combined. As a specific example, we consider the class of α -1 -cartoon-like functions, which is approximated optimally by α-shearlets. We also explain how our results can be extended to the approximation of functions on low-dimensional immersed manifolds. Finally, we present numerical experiments demonstrating that the standard stochastic gradient descent algorithm yields deep neural networks with close-to-optimal approximation rates. Moreover, these results indicate that stochastic gradient descent can learn approximations that are sparse in the representation systems optimally sparsifying the function class the network is trained on.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction. Neural networks arose from the seminal work by McCulloch and Pitts</head> <ref type="bibr" target="#b40">[41]</ref> <p>in 1943 which, inspired by the functionality of the human brain, introduced an algorithmic approach to learning with the aim of building a theory of artificial intelligence. Roughly speaking, a neural network consists of neurons arranged in layers and connected by weighted edges; in mathematical terms this boils down to a concatenation of affine linear functions and relatively simple nonlinearities.</p><p>Despite significant theoretical progress in the 1990s <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref>, the area has seen practical progress only during the past decade, triggered by the drastic improvements in computing power and the availability of vast amounts of training data. Deep neural networks, i.e., networks with large numbers of layers, are now state-of-the-art technology for a wide variety of applications, such as image classification <ref type="bibr" target="#b35">[36]</ref>, speech recognition <ref type="bibr" target="#b32">[33]</ref>, and game intelligence <ref type="bibr" target="#b12">[13]</ref>. For an in-depth overview, we refer the reader to the survey paper by LeCun, Bengio, and Hinton <ref type="bibr" target="#b38">[39]</ref> and the recent book <ref type="bibr" target="#b23">[24]</ref>.</p><p>A neural network effectively implements a nonlinear mapping and can be used to either perform classification directly or extract features that are then fed into a classifier, such as a support vector machine <ref type="bibr" target="#b53">[54]</ref>. In the former case, the primary goal is to approximate an unknown classification function based on a given set of input-output value pairs. This is typically accomplished by learning the network's weights through, e.g., the stochastic gradient descent (via back-propagation) algorithm <ref type="bibr" target="#b51">[52]</ref>. In a classification task with, say, two classes, the function to be learned would take only two values, whereas in the case of, e.g., the prediction of the temperature in a certain environment, it would be real-valued. It is therefore clear that characterizing to what extent (deep) neural networks are capable of approximating general functions is a question of significant practical relevance.</p><p>Neural networks employed in practice often consist of hundreds of layers and may depend on billions of parameters; see, for example, the work <ref type="bibr" target="#b31">[32]</ref> on image classification. Training and operation of networks of this scale entail formidable computational challenges. As a case in point, we mention speech recognition on a smartphone such as, e.g., Apple's SIRI system, which operates in the cloud. Android's speech recognition system has meanwhile released an offline version based on a neural network with sparse connectivity. The desire to reduce the complexity of network training and operation naturally leads to the question of the fundamental limits on function approximation through neural networks with sparse connectivity. In addition, the network's memory requirements in terms of the number of bits needed to store its topology and weights are of concern in practice.</p><p>The purpose of this paper is to understand the connectivity and memory requirements of (deep) neural networks induced by demands on their approximation-theoretic properties. Specifically, defining the complexity of a function class C as the rate of growth of the minimum number of bits needed to describe any element in C to within a maximum allowed error approaching zero, we shall be interested in the following question: Depending on the complexity of C, what are the connectivity and memory requirements of a deep neural network approximating every element in C to within an error of ε? We address this question by interpreting the network as an encoder in Donoho's min-max rate distortion theory <ref type="bibr" target="#b18">[19]</ref> and establishing rate-distortion optimality for a broad family of function classes C, namely those classes for which so-called affine systems-a general class of representation systemsyield optimal approximation rates in the sense of nonlinear approximation theory <ref type="bibr" target="#b15">[16]</ref>. Affine systems encompass a wealth of representation systems from applied harmonic analysis such as wavelets <ref type="bibr" target="#b13">[14]</ref>, ridgelets <ref type="bibr" target="#b3">[4]</ref>, curvelets <ref type="bibr" target="#b5">[6]</ref>, shearlets <ref type="bibr" target="#b30">[31]</ref>, α-shearlets, and, more generally, α-molecules <ref type="bibr" target="#b26">[27]</ref>. Our result therefore uncovers an interesting universality property of deep c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to <ref type="bibr">46.148.124.159</ref>. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php neural networks: they exhibit the optimal approximation properties of all affine systems combined. The technique we develop to prove our main statements is interesting in its own right as it constitutes a more general framework for transferring results on function approximation through representation systems to results on approximation by deep neural networks.</p><p>1.1. Deep neural networks. While various network architectures exist in the literature, we focus on the following setup. Definition 1.1. Let L, d, N 1 , . . . , N L ∈ N with L ≥ 2. A map Φ : R d → R N L given by</p><formula xml:id="formula_0">(1.1) Φ(x) = W L ρ (W L-1 ρ (. . . ρ (W 1 (x)))) for x ∈ R d ,</formula><p>with affine linear maps W : R N -1 → R N , 1 ≤ ≤ L, and the nonlinear activation function ρ acting componentwise, is called a neural network. Here, N 0 := d is the dimension of the 0th layer referred to as the input layer, L denotes the number of layers (not counting the input layer), N 1 , . . . , N L-1 stands for the dimensions of the L -1 hidden layers, and N L is the dimension of the output layer. The affine linear map W is defined via W (x) = A x + b with A ∈ R N ×N -1 and the affine part b ∈ R N . (A ) i,j represents the weight associated with the edge between the jth node in the ( -1)th layer and the ith node in the th layer, while (b ) i is the weight associated with the ith node in the th layer. These assignments are schematized in Figure <ref type="figure" target="#fig_1">1</ref>. The total number of nodes is given by N (Φ) := d + L =1 N . The real numbers (A ) i,j and (b ) i are said to be the network's edge weights and node weights, respectively, and the total number of nonzero edge weights, denoted by M(Φ), is the network's connectivity.</p><p>The term "network" stems from the interpretation of the mapping Φ as a weighted acyclic directed graph with nodes arranged in L+1 hierarchical layers and edges only between adjacent layers. If the network's connectivity M(Φ) is small relative to the number of connections possible (i.e., the number of edges in the graph that is fully connected between adjacent layers), we say that the network is sparsely connected. </p><formula xml:id="formula_1">A 2 = (A 2 ) 1,1 (A 2 ) 1,2 0 0 0 (A 2 ) 2,3 A 1 =   (A 1 ) 1,1 (A 1 ) 1,2 0 0 0 (A 1 ) 2,3 0 0 (A 1 ) 3,3  </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output layer</head><p>Hidden layer ρ Throughout the paper, we consider the case Φ : R d → R, i.e., N L = 1, which includes situations such as the classification and temperature prediction problem described above. We c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to <ref type="bibr">46.148.124.159</ref>. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php emphasize, however, that the general results of sections 3, 4, and 5 are readily generalized to N L &gt; 1.</p><note type="other">Input layer</note><p>We denote the class of networks Φ : R d → R with exactly L layers, connectivity no more than M , and activation function ρ by NN L,M,d,ρ with the understanding that for L = 1, the set NN L,M,d,ρ is empty. Moreover, we let NN ∞,M,d,ρ := L∈N NN L,M,d,ρ , NN L,∞,d,ρ := M ∈N NN L,M,d,ρ , and NN ∞,∞,d,ρ := L∈N NN L,∞,d,ρ . Now, given a function f : R d → R, we are interested in the theoretically best possible approximation of f by a network Φ ∈ NN ∞,M,d,ρ . Specifically, we will want to know how the approximation quality depends on the connectivity M and what the associated number of bits needed to store the network topology and the corresponding quantized weights is. Clearly, smaller M entails lower computational complexity in terms of evaluating (1.1) and a smaller number of bits translates to reduced memory requirements for storing the network. Such a result benchmarks all conceivable algorithms for learning the network topology and weights.</p><p>1.2. Quantifying approximation quality. We proceed to formalizing our problem statement and start with a brief review of a widely used framework in approximation theory <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Fix Ω ⊂ R d . Let C be a compact set of functions in L 2 (Ω), henceforth referred to as function class, and consider a corresponding system D := (ϕ i ) i∈I ⊂ L 2 (Ω) with I countable, termed representation system. We study the error of best M -term approximation of f ∈ C in D. Definition 1.2 (see <ref type="bibr" target="#b16">[17]</ref>).</p><formula xml:id="formula_2">Given d ∈ N, Ω ⊂ R d , a function class C ⊂ L 2 (Ω), and a representation system D = (ϕ i ) i∈I ⊂ L 2 (Ω), we define, for f ∈ C and M ∈ N, Γ D M (f ) := inf I M ⊆I, #I M =M,(c i ) i∈I M f - i∈I M c i ϕ i L 2 (Ω) . (1.2) We call Γ D M (f ) the best M -term approximation error of f in D. Every f M = i∈I M c i ϕ i attaining the infimum in (1.2) is referred to as a best M -term approximation of f in D. The supremal γ &gt; 0 such that sup f ∈C Γ D M (f ) ∈ O(M -γ ), M → ∞,</formula><p>will be denoted by γ * (C, D). Here, O(g(•)) denotes the class of functions bounded asymptotically by g in the sense of standard Landau notation. We say that the best M -term approximation rate of C in the representation system D is γ * (C, D).</p><p>Function classes C widely studied in the approximation theory literature include unit balls in Lebesgue, Sobolev, or Besov spaces <ref type="bibr" target="#b15">[16]</ref>, as well as α-cartoon-like functions <ref type="bibr" target="#b26">[27]</ref>. A wealth of structured representation systems D is provided by the area of applied harmonic analysis, starting with wavelets <ref type="bibr" target="#b13">[14]</ref>, followed by ridgelets <ref type="bibr" target="#b3">[4]</ref>, curvelets <ref type="bibr" target="#b5">[6]</ref>, shearlets <ref type="bibr" target="#b30">[31]</ref>, parabolic molecules <ref type="bibr" target="#b28">[29]</ref>, and, most generally, α-molecules <ref type="bibr" target="#b26">[27]</ref>, which include all previously named systems as special cases. Further examples are Gabor frames <ref type="bibr" target="#b24">[25]</ref> and wave atoms <ref type="bibr" target="#b14">[15]</ref>. in the spirit of <ref type="bibr" target="#b16">[17]</ref>. Specifically, we shall substitute the concept of best M -term approximation with representation systems by best M -edge approximation through neural networks. In other words, parsimony in terms of the number of participating elements of a representation system is replaced by parsimony in terms of connectivity. More formally, we consider the following setup.</p><formula xml:id="formula_3">Definition 1.3. Given d ∈ N, Ω ⊂ R d , a function class C ⊂ L 2 (Ω)</formula><p>, and an activation function ρ : R → R, we define, for f ∈ C and M ∈ N,</p><formula xml:id="formula_4">Γ NN M (f ) := inf Φ∈NN ∞,M,d,ρ f -Φ L 2 (Ω) . (1.3) We call Γ NN M (f ) the best M -edge approximation error of f . The supremal γ &gt; 0 such that sup f ∈C Γ NN M (f ) ∈ O(M -γ ), M → ∞,</formula><p>will be denoted by γ * NN (C, ρ). We say that the best M -edge approximation rate of C by neural networks with activation function ρ is γ * NN (C, ρ). We emphasize that the infimum in (1.3) is taken over all networks with fixed activation function ρ, fixed input dimension d, no more than M edges of nonzero weight, and arbitrary number of layers L. In particular, this means that the infimum is taken over all possible network topologies. The resulting best M -edge approximation rate is fundamental as it benchmarks all learning algorithms, i.e., all algorithms that map an input function f and an ε &gt; 0 to a neural network that approximates f with error no more than ε. Our framework hence provides a means for assessing the performance of a given learning algorithm in the sense of allowing us to measure how close the M -edge approximation rate induced by the algorithm is to the best M -edge approximation rate γ * NN (C, ρ). 1.4. Previous work. The best-known results on approximation by neural networks are the universal approximation theorems of Hornik <ref type="bibr" target="#b33">[34]</ref> and Cybenko <ref type="bibr" target="#b11">[12]</ref>, stating that every measurable function f can be approximated arbitrarily well by a single-hidden-layer (L = 2 in our terminology) neural network. The literature on approximation-theoretic properties of networks with a single hidden layer continuing this line of work is abundant. Without any claim to completeness, we mention work on approximation error bounds in terms of the number of neurons for functions with bounded first moments <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, the nonexistence of localized approximations <ref type="bibr" target="#b6">[7]</ref>, a fundamental lower bound on approximation rates <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4]</ref>, and the approximation of smooth or analytic functions <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Approximation-theoretic results for networks with multiple hidden layers were obtained in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref> for general functions, in <ref type="bibr" target="#b22">[23]</ref> for continuous functions, and in <ref type="bibr" target="#b46">[47]</ref> for functions together with their derivatives. In <ref type="bibr" target="#b6">[7]</ref> it was shown that for certain approximation tasks deep networks can perform fundamentally better than single-hidden-layer networks. We also highlight two recent papers, which investigate the benefit-from an approximation-theoretic perspectiveof multiple hidden layers. Specifically, in <ref type="bibr" target="#b20">[21]</ref> it was shown that there exists a function which, although expressible through a small three-layer network, can only be represented through a very large two-layer network; here size is measured in terms of the total number of neurons in the network. In the setting of deep neural networks, first results of a nature similar to those in c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php <ref type="bibr" target="#b20">[21]</ref> were reported in <ref type="bibr" target="#b45">[46]</ref>. For the activation function ρ(x) = max{0, x}-usually referred to as ReLU-it was demonstrated in <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b49">50]</ref> that deep networks approximate sufficiently smooth functions with rates higher than those achieved by shallow networks.</p><p>Convolutional neural networks are obtained as a special case of the general networks considered in this paper, namely by taking the affine linear transformations in Definition 1.1 to have circulant A-matrices. Linking the expressivity properties of neural networks to tensor decompositions, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> establish the existence of functions that can be realized by relatively small deep convolutional networks but require exponentially larger shallow convolutional networks. Universal approximation theorems for convolutional neural networks are provided in <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b56">57]</ref>. The approximation-theoretic equivalence results between convolutional networks and general networks established in <ref type="bibr" target="#b48">[49]</ref>, together with the main findings of the present paper, lead to upper and lower bounds on approximation rates attainable with convolutional networks.</p><p>For survey articles on approximation-theoretic aspects of neural networks, we refer the interested reader to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Most closely related to our work is that by Shaham, Cloninger, and Coifman <ref type="bibr" target="#b52">[53]</ref>, which shows that for functions that are sparse in specific wavelet frames, the best M -edge approximation rate of three-layer neural networks is at least as high as the best M -term approximation rate in piecewise linear wavelet frames.</p><p>1.5. Contributions. Our contributions can be grouped into four threads.</p><p>• Fundamental lower bound on connectivity. We quantify the minimum network connectivity needed to allow approximation of all elements of a given function class C to within a maximum allowed error. On a conceptual level, this result establishes a universal link between the complexity of a given function class and the connectivity required by corresponding approximating neural networks. • Transfer from M -term to M -edge approximation. We develop a general framework for transferring best M -term approximation results in representation systems to best M -edge approximation results for neural networks. • Memory requirements. We characterize the memory requirements needed to store the topology and the quantized weights of optimally-approximating neural networks. • Realizability of optimal approximation rates. An important practical question is how neural networks trained by stochastic gradient descent (via back-propagation) <ref type="bibr" target="#b51">[52]</ref> perform relative to the fundamental bounds established in the paper. Interestingly, our numerical experiments indicate that stochastic gradient descent can achieve Medge approximation rates quite close to the fundamental limit.</p><p>1.6. Outline of the paper. Section 2 introduces the novel concept of effective best Medge approximation. The fundamental lower bound on connectivity is developed in section 3. Section 4 describes a general framework for transferring best M -term approximation results in representation systems to best M -edge approximation results for neural networks. In section 5, we apply this transfer framework to the broad class of affine representation systems, and section 6 shows that this leads to optimal M -edge approximation rates for cartoon functions. In section 7, we briefly outline the extension of our main findings to the approximation of functions defined on manifolds. Finally, numerical results assessing the performance of stochastic gradient descent (via back-propagation) relative to our lower bound on connectivity </p><formula xml:id="formula_5">c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Effective best M -term and best M -edge approximation. We proceed by introducing M -term approximation via dictionaries and M -edge approximation via neural networks. These concepts, however, do not allow for a meaningful notion of optimality in practice. A remedy is provided by effective best M -term approximation according to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref> and the new concept of effective best M -edge approximation introduced below.</p><p>2.1. Effective best M -term approximation. The best M -term approximation rate γ * (C, D) according to Definition 1.2 measures the hardness of approximation of a given function class C by a fixed representation system D. It is sensible to ask whether, for a given function class C, there is a fundamental limit on γ * (C, D) when one is allowed to vary over D. As shown in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>, every dense (and countable) D ⊂ L 2 (Ω), Ω ⊂ R d , results in γ * (C, D) = ∞ for all function classes C ⊂ L 2 (Ω). However, identifying the elements in D participating in the best M -term approximation is infeasible as it entails searching through the infinite set D and requires, in general, an infinite number of bits to describe the indices of the participating elements. This insight leads to the concept of "best M -term approximation subject to polynomial-depth search" as introduced by Donoho in <ref type="bibr" target="#b18">[19]</ref>.</p><formula xml:id="formula_6">Definition 2.1. Given d ∈ N, Ω ⊂ R d , a function class C ⊂ L 2 (Ω)</formula><p>, and a representation system D = (ϕ i ) i∈I ⊂ L 2 (Ω), the supremal γ &gt; 0 so that there exist a polynomial π and a constant D &gt; 0 such that</p><formula xml:id="formula_7">sup f ∈C inf I M ⊂{1,...,π(M )}, #I M =M, (c i ) i∈I M , max i∈I M |c i | ≤ D f - i∈I M c i ϕ i L 2 (Ω) ∈ O(M -γ ), M → ∞,<label>(2.1)</label></formula><p>will be denoted by γ * ,eff (C, D) and referred to as effective best M -term approximation rate of C in the representation system D.</p><p>We will demonstrate in section 3.2 that sup D⊂L 2 (Ω) γ * ,eff (C, D) is, indeed, finite under quite general conditions on C and, in particular, depends on the "description complexity" of C. This will allow us to assess the approximation capacity of a given representation system D for C by comparing γ * ,eff (C, D) to the ultimate limit sup D⊂L 2 (Ω) γ * ,eff (C, D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Effective best M -edge approximation.</head><p>We next aim at establishing a relationship in the spirit of effective best M -term approximation for approximation through deep neural networks. To this end, we first note that Definition 1.3 encounters problems similar to those identified for approximation by representation systems; namely the quantity sup ρ:R→R γ * NN (C, ρ) does not reveal anything tangible about the approximation complexity of C in deep neural networks, unless further constraints are imposed on the approximating network. To make this point, we first review the following remarkable result. Theorem 2.2 (see <ref type="bibr" target="#b39">[40]</ref>). There exists a function ρ : R → R that is C ∞ , strictly increasing, and satisfies lim x→∞ ρ(x) = 1 and lim x→-∞ ρ(x) = 0, such that for any d ∈ N, any f ∈ C([0, 1] d ), and any ε &gt; 0, there is a neural network Φ with activation function ρ and three layers, of dimensions N 1 = 3d, N 2 = 6d + 3, and We observe that the number of nodes and the number of layers of the approximating network in Theorem 2.2 do not depend on the approximation error ε. In particular, ε can be chosen arbitrarily small while having M(Φ) bounded. By density of C([0, 1] d ) in L 2 ([0, 1] d ) and hence in all compact subsets of L 2 ([0, 1] d ), this implies the existence of an activation function ρ : R → R such that γ * NN (C, ρ) = ∞ for all compact C ⊂ L 2 ([0, 1] d ), d ∈ N. However, the networks underlying Theorem 2.2 necessarily lead to weights that are (in absolute value) not bounded by |π(ε -1 )| for a polynomial π, a requirement we will have to impose to get rate-distortion-optimal approximation through neural networks (see section 3). To see that the weights indeed do not obey a polynomial growth bound in ε -1 , we note that, thanks to Theorem 2.2, there exist C &gt; 0 and γ &gt; 0 such that sup</p><formula xml:id="formula_8">N 3 = 1, satisfying (2.2) sup x∈[0,1] d |f (x) -Φ(x)| ≤ ε. c 2019</formula><formula xml:id="formula_9">f ∈C inf Φ M ∈NN 3,M,d,ρ f -Φ M L 2 (Ω) ≤ CM -γ for all M ∈ N. (2.3)</formula><p>Now, as ε in Theorem 2.2 can be made arbitrarily small while the connectivity of the corresponding networks remains upper-bounded by 21d 2 + 15d + 3, (2.3) would have to hold for arbitrarily large γ, in particular also for γ &gt; γ * ,eff NN (C, ρ), where γ * ,eff NN (C, ρ) is the effective best M -edge approximation rate according to Definition 2.3. By Theorem 3.4 below, however, γ * ,eff NN (C, ρ) ≤ γ * (C), where γ * (C) is the optimal exponent according to Definition 3.1. Owing to Definition 2.3, we can therefore conclude that the weights of the network achieving the infimum in (2.3) cannot be bounded by a polynomial in M ∼ ε -1 whenever γ * (C) &lt; ∞. Here and in what follows, we write a ∼ b if the variables a and b are proportional, i.e., there exist uniform constants</p><formula xml:id="formula_10">c 1 , c 2 &gt; 0 such that c 1 a ≤ b ≤ c 2 a.</formula><p>The observation just made resembles the problem in best M -term approximation which eventually led to the concept of effective best M -term approximation, where we restricted the search depth in the representation system D to be polynomially bounded in M and the coefficients c i to be bounded according to max i∈I M |c i | ≤ D. Interpreting the weights in the network as the counterpart of the coefficients c i in best M -term approximation, we see that the restriction on the search depth corresponds to restricting the size of the indices enumerating the participating weights. The need for such a restriction is obviated by the tree structure of deep neural networks as exposed in detail in the proof of Proposition 3.6. The second restriction will lead us to a growth condition on the weights, which is more generous than the corresponding requirement of the c i in effective best M -term approximation being bounded.</p><p>In summary, this leads to the novel concept of "best M -edge approximation subject to polynomial weight growth" as formalized next.</p><formula xml:id="formula_11">Definition 2.3. Given d ∈ N, Ω ⊂ R d , a function class C ⊂ L 2 (Ω)</formula><p>, and an activation function ρ : R → R, the supremal γ &gt; 0 so that there exist an L ∈ N and a polynomial π such that 3. Fundamental bounds on effective M -term and M -edge approximation rate. The purpose of this section is to establish fundamental bounds on effective best M -term and effective best M -edge approximation rates by evaluating the corresponding approximation strategies in the min-max rate distortion theory framework as developed in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>.</p><formula xml:id="formula_12">sup f ∈C inf Φ M ∈NN π L,M,d,ρ f -Φ M L 2 (Ω) ∈ O(M -γ ), M → ∞, (2.</formula><p>3.1. Min-max rate distortion theory. Min-max rate distortion theory provides a theoretical foundation for deterministic lossy data compression. We recall the following notions and concepts from <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Let d ∈ N, Ω ⊂ R d , and consider the function class C ⊂ L 2 (Ω). Then, for each ∈ N, we denote by</p><formula xml:id="formula_13">E := E : C → {0, 1}</formula><p>the set of binary encoders of C of length , and we let</p><formula xml:id="formula_14">D := D : {0, 1} → L 2 (Ω)</formula><p>be the set of binary decoders of length . An encoder-decoder pair (E,</p><formula xml:id="formula_15">D) ∈ E × D is said to achieve uniform error ε over the function class C if sup f ∈C D(E(f )) -f L 2 (Ω) ≤ ε.</formula><p>A quantity of central interest is the minimal length ∈ N for which there exists an encoderdecoder pair (E, D) ∈ E × D that achieves uniform error ε over the function class C, along with its asymptotic behavior, as made precise in the following definition.</p><formula xml:id="formula_16">Definition 3.1. Let d ∈ N, Ω ⊂ R d , and C ⊂ L 2 (Ω). Then, for ε &gt; 0, the minimax code length L(ε, C) is L(ε, C) := min ∈ N : ∃(E, D) ∈ E × D : sup f ∈C D(E(f )) -f L 2 (Ω) ≤ ε .</formula><p>Moreover, the optimal exponent γ * (C) is defined as</p><formula xml:id="formula_17">γ * (C) := sup γ ∈ R : L(ε, C) ∈ O ε -1 γ , ε → 0 .</formula><p>The optimal exponent γ * (C) quantifies the minimum growth rate of L(ε, C) as the error ε tends to zero and can hence be seen as quantifying the "description complexity" of the function class C. Larger γ * (C) results in smaller growth rate and hence smaller memory requirements for storing signals f ∈ C such that reconstruction with uniformly bounded error is possible. The quantity γ * (C) is closely related to the concept of Kolmogorov entropy <ref type="bibr" target="#b47">[48]</ref>. Remark 5.10 in <ref type="bibr" target="#b25">[26]</ref> makes this connection explicit.</p><p>The optimal exponent is known for several function classes, such as subsets of Besov spaces B s p,q (R d ) with 1 ≤ p, q &lt; ∞, s &gt; 0, and q &gt; (s + 1/2) -1 , namely all functions in B s p,q (R d ) of bounded norm; see, e.g., <ref type="bibr" target="#b7">[8]</ref>. If C is a bounded subset of B s p,q (R d ), then we have γ * (C) = s/d. In the present paper, we shall be particularly interested in so-called β-cartoon-like functions, for which the optimal exponent is given by β/2 (see <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref>   <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>, which says that, for a given function class C, the optimal exponent γ * (C) constitutes a fundamental bound on the effective best M -term approximation rate of C in any representation system. This gives operational meaning to γ * (C). Theorem 3.2 (see <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>). Let d ∈ N, Ω ⊂ R d , and C ⊂ L 2 (Ω), and assume that the effective best M -term approximation rate of</p><formula xml:id="formula_18">C in D ⊂ L 2 (Ω) is γ * ,eff (C, D). Then, we have γ * ,eff (C, D) ≤ γ * (C).</formula><p>In light of this result, the following definition is natural (see also <ref type="bibr" target="#b25">[26]</ref>).</p><formula xml:id="formula_19">Definition 3.3. Let d ∈ N, Ω ⊂ R d , and assume that the effective best M -term approxima- tion rate of C ⊂ L 2 (Ω) in D ⊂ L 2 (Ω) is γ * ,eff (C, D). If γ * ,eff (C, D) = γ * (C),</formula><p>then the function class C is said to be optimally representable by D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fundamental bound on effective best</head><p>M -edge approximation rate. We now state the first main result of the paper, namely the equivalent of Theorem 3.2 for approximation by deep neural networks. Specifically, we establish that the optimal exponent γ * (C) also constitutes a fundamental bound on the effective best M -edge approximation rate of C. We say below that a function f :</p><formula xml:id="formula_20">R → R is dominated by a function g : R → R if |f (x)| ≤ |g(x)| for all x ∈ R. Theorem 3.4. Let d ∈ N, Ω ⊂ R d be bounded, and let C ⊂ L 2 (Ω).</formula><p>Then, for all ρ : R → R that are Lipschitz-continuous or differentiable with ρ dominated by a polynomial, we have</p><formula xml:id="formula_21">γ * ,eff NN (C, ρ) ≤ γ * (C).</formula><p>The key ingredients of the proof of Theorem 3.4 are developed throughout this section, and the formal proof will be stated at the end of the section. Before embarking, we note that, in analogy to Definition 3.3, what we just found suggests the following.</p><formula xml:id="formula_22">Definition 3.5. For d ∈ N, Ω ⊂ R d bounded, we say that the function class C ⊂ L 2 (Ω) is optimally representable by neural networks with activation function ρ : R → R if γ * ,eff NN (C, ρ) = γ * (C).</formula><p>It is remarkable that the fundamental limits of approximation through representation systems and approximation through deep neural networks are determined by the same quantity, although the approximants in the two cases are vastly different-namely, linear combinations of elements of a representation system with the participating functions identified subject to a polynomial-depth search constraint in the former, and concatenations of affine functions followed by nonlinearities under growth constraints on the weights in the network in the latter case.</p><p>A key ingredient of the proof of Theorem 3.4 is the following result, which establishes a fundamental lower bound on the connectivity of networks with quantized weights achieving uniform error ε over a given function class. </p><formula xml:id="formula_23">3.6. Let d ∈ N, Ω ⊂ R d , ρ : R → R, c &gt; 0, and C ⊂ L 2 (Ω). Further, let Learn : 0, 1 2 × C → NN ∞,∞,d,ρ</formula><p>be a map such that, for each pair (ε, f ) ∈ (0, 1/2) × C, every weight of the neural network Learn(ε, f ) is represented by no more than c log 2 (ε -<ref type="foot" target="#foot_0">1</ref> ) bits 1 while guaranteeing that</p><formula xml:id="formula_24">(3.1) sup f ∈C f -Learn(ε, f ) L 2 (Ω) ≤ ε.</formula><p>Then,</p><formula xml:id="formula_25">(3.2) sup f ∈C M(Learn(ε, f )) / ∈ O ε -1 γ , ε → 0, for all γ &gt; γ * (C).</formula><p>Proof. The proof is by contradiction. To this end, let γ &gt; γ * (C), and assume that sup</p><formula xml:id="formula_26">f ∈C M(Learn(ε, f )) ∈ O(ε -1/γ ), ε → 0. The contradiction will be effected by constructing encoder-decoder pairs (E ε , D ε ) ∈ E (ε) × D (ε) achieving uniform error ε over C with (ε) ≤ C 0 • sup f ∈C [M(Learn(ε, f )) log 2 (M(Learn(ε, f ))) + 1] log 2 (ε -1 ) (3.3) ≤ C 0 • ε -1 γ log 2 ε -1 γ + 1 log 2 (ε -1 ) ≤ C 1 ε -1 γ (log 2 (ε -1 )) 2 + log 2 (ε -1 ) ∈ O ε -1 ν for ε → 0,</formula><p>where C 0 , C 1 &gt; 0 are constants and γ &gt; ν &gt; γ * (C). Such a construction stands in contradiction to the optimality of γ * (C) according to Definition 3.1.</p><p>We proceed to the construction of the encoder-decoder pairs (E ε , D ε ) ∈ E (ε) × D (ε) . Fix f ∈ C. We enumerate the nodes in Learn(ε, f ) by assigning natural numbers, henceforth called indices, increasing from left to right in every layer as schematized in Figure <ref type="figure" target="#fig_5">2</ref>. For the sake of notational simplicity, we also set Φ := Learn(ε, f ) and M := M(Φ). Without loss of generality, we henceforth assume that M is a power of 2 and larger than 1. The case M = 0 will be dealt with in Step 1 below. For all M that are not powers of 2 and for M = 1, we make use of the fact that NN L,M,d,ρ ⊂ NN L,M ,d,ρ , where M is the smallest power of 2 larger than M , and we encode the network like an M -edge network. Since M &lt; M ≤ 2M , this affects (ε) by a multiplicative constant only.</p><p>We recall that the number of layers of Φ is denoted by L, the number of nodes in these layers is N 1 , . . . , N L (see Definition 1.1), and d stands for the dimension of the input layer.</p><p>Denoting the number of nodes in layer = 1, . . . , L -1 associated with edges of nonzero weight in the following layer by N and setting where we let M := M + d. All other nodes do not contribute to the mapping Φ(x) and can hence be ignored. Moreover, we can assume that</p><formula xml:id="formula_27">N L = N L = 1, it follows that d + L =1 N ≤ 2 M , (3.4)</formula><formula xml:id="formula_28">L ≤ M , (3.5)</formula><p>as otherwise there would be at least one layer &gt; 1 such that</p><formula xml:id="formula_29">A = 0.</formula><p>As a consequence, the reduced network</p><formula xml:id="formula_30">x → W L ρ(W L-1 . . . W +1 ρ(0 • x + b ))</formula><p>realizes the same function as the original network Φ but has fewer than L layers. This reduction can be repeated inductively until the resulting reduced network satisfies <ref type="bibr">(3.5)</ref>.</p><p>The bitstring representing Φ is constructed according to the following steps.</p><p>Step 1. If M = 0, we encode the network by a leading 0 followed by the bitstring representing the node weight in the last layer. Upon defining 0 log 2 (0) = 0, we then note that (3.3) holds trivially, and we terminate the encoding procedure. Else, we encode the number of nonzero edge weights, M , by starting the overall bitstring with M 1's followed by a single 0. The length of this bitstring is therefore bounded by M .</p><p>Step 2. We continue by encoding the number of layers in the network. Thanks to (3.5), this requires no more than log 2 ( M ) bits. We thus reserve the next log 2 ( M ) bits for the binary representation of L.</p><p>Step 3. Next, we store the dimension d of the input layer and the numbers of nodes N , = 1, . . . , L, associated with edges of nonzero weight. As by (3.4) d ≤ M and N ≤ 2 M , for all , we can encode (generously) d and each N using log 2 ( M ) + 1 bits. For the sake of concreteness, we first encode d followed by N 1 , . . . , N L in that order. In total, Step 3 requires a bitstring of length</p><formula xml:id="formula_31">(L + 1) • log 2 M + 1 ≤ M + 1 log 2 M + M + 1.</formula><p>In combination with Steps 1 and 2 this yields an overall bitstring of length at most Step 4. We encode the topology of the graph associated with Φ and consider only nodes that contribute to the mapping Φ(x). Recall that we assigned a unique index i, ranging from 1 to N := d + L =1 N , to each of these nodes. By (3.4) each of these indices can be encoded by a bitstring of length log 2 ( M ) + 1. We denote the bitstring corresponding to index i by b(i) ∈ {0, 1} log 2 ( M )+1 and let n(i) be the number of children of the node with index i, i.e., the number of nodes in the next layer connected to the node with index i via an edge. (Here, n( N ) = 0.) For each node i = 1, . . . , N , we form a bitstring of length n(i) • (log 2 ( M ) + 1) by concatenating the bitstrings b(j) for all j such that there is an edge between i and j. We follow this string with an all-zeros bitstring of length log 2 ( M ) + 1 to signal the transition to the node with index i + 1. The enumeration is concluded with an all-zeros bitstring of length log 2 ( M ) + 1 signaling that the last node has been reached. Overall, this yields a bitstring of length</p><formula xml:id="formula_32">(3.6) M log 2 M + 2 log 2 M + 2 M +</formula><formula xml:id="formula_33">(3.7) N i=1 (n(i) + 1) • log 2 M + 1 ≤ 3 M • log 2 M + 1 ,</formula><p>where we used N i=1 n(i) = M &lt; M and (3.4). Combining (3.6) and (3.7) it follows that we have encoded the overall topology of the network Φ using at most</p><formula xml:id="formula_34">(3.8) 5 M + 4 M log 2 M + 2 log 2 M + 1 bits.</formula><p>Step 5. We encode the weights of Φ. By assumption, each weight can be represented by a bitstring of length c log 2 (ε -1 ) . For each node i = 1, . . . , N , we reserve the first c log 2 (ε -1 ) bits to encode its associated node weight and, for each of its children, a bitstring of length c log 2 (ε -1 ) to encode the weight corresponding to the edge between that child and its parent node. Concatenating the results in ascending order of child node indices, we get a bitstring of length (n(i) + 1) • ( c log 2 (ε -1 ) ) for node i and an overall bitstring of length</p><formula xml:id="formula_35">(3.9) N i=1 (n(i) + 1) • c log 2 ε -1 ≤ 3 M • c log 2 ε -1</formula><p>representing the weights of the graph associated with the network Φ. With <ref type="bibr">(3.8)</ref> this shows that the overall number of bits needed to encode the network topology and weights is no more than</p><formula xml:id="formula_36">5 M + 4 M log 2 M + 2 log 2 M + 1 + 3 M • c log 2 ε -1 . (3.10)</formula><p>The network can be recovered by sequentially reading out M, L, d, the N , the topology, and the quantized weights from the overall bitstring. It is not difficult to verify that the individual steps in the encoding procedure were crafted such that this yields unique recovery. As (3.10) can be upper-bounded by <ref type="bibr">(3.11</ref>)  ε) with (ε) satisfying <ref type="bibr">(3.3)</ref>. This concludes the proof.</p><formula xml:id="formula_37">C 0 M log 2 (M ) log 2 ε -1 c 2019 SIAM.</formula><formula xml:id="formula_38">(E, D) ∈ E (ε) × D (</formula><p>Proposition 3.6 applies to networks that have each weight represented by a finite number of bits scaling according to log 2 (ε -1 ) while guaranteeing that the underlying encoder-decoder pair achieves uniform error ε over C. We next show that such a compatibility is possible for networks with activation functions that are either Lipschitz or differentiable such that ρ is dominated by a polynomial. Lemma 3.7. Let d, L, k, M ∈ N, η ∈ (0, 1/2), Ω ⊂ R d be bounded, and let ρ : R → R be either Lipschitz-continuous or differentiable such that ρ is dominated by a polynomial. Let Φ ∈ NN L,M,d,ρ with M ≤ η -k and all its weights be bounded (in absolute value) by η -k . Then, there exist m ∈ N, depending on k, L, and ρ only, and</p><formula xml:id="formula_39">Φ ∈ NN L,M,d,ρ such that Φ -Φ L ∞ (Ω) ≤ η and all weights of Φ are elements of η m Z ∩ [-η -k , η -k ].</formula><p>Proof. We prove the statement for Lipschitz-continuous ρ only. The argument for differentiable activation functions with first derivative not growing faster than polynomial is along similar lines.</p><p>Without loss of generality, we can take the number of nonzero node weights of Φ to be upper-bounded by twice the number of nonzero edge weights. This assumption is justified as else there would be nodes that are not connected to the next layer through an edge of nonzero weight and we could replace the corresponding node weights by zero without altering the mapping Φ. This reduction would then lead to the assumption being justified.</p><p>Let m ∈ N, to be specified later, and denote by Φ the network that results by replacing all weights of Φ by a closest element in η m Z ∩ [-η -k , η -k ]. Set C max := η -k , and denote the maximum of 1 and the total number of nonzero edge weights plus nonzero node weights of Φ by C W . Note that C W ≤ 3M ≤ 3η -k , where the latter inequality is by assumption. For = 1, . . . , L -1, define Φ : Ω → R N as Φ (x) := ρ (W ρ (. . . ρ (W 1 (x)))) for x ∈ Ω, and Φ accordingly, and let, for = 1, . . . , L -1,</p><formula xml:id="formula_40">e := Φ -Φ L ∞ (Ω,R N ) , e L := Φ -Φ L ∞ (Ω)</formula><p>.</p><p>Denote the maximum of 1 and the Lipschitz constant of ρ by C ρ , set C 0 := max{1, sup{|x| :</p><p>x ∈ Ω}}, and let</p><formula xml:id="formula_41">C := max Φ L ∞ (Ω,R N ) , Φ L ∞ (Ω,R N ) for = 1, . . . , L -1.</formula><p>Then, straightforward, albeit somewhat tedious, algebraic manipulations show that for all = 2, . . . , L -1, </p><formula xml:id="formula_42">e 1 ≤ C 0 C ρ C W η m , and e ≤ C ρ C W C -1 η m + C ρ C W C max e -1 .</formula><formula xml:id="formula_43">C ≤ C C 0 (C ρ C W C max ) for all = 1, . . . , L -1.</formula><p>As C W and C max are both bounded by η -k-2 , it follows that C is bounded by η -p for a p ∈ N. We can therefore find n ∈ N such that for all = 1, . . . , L -1,</p><formula xml:id="formula_44">(3.14) max{C 0 C ρ C W , C W C max , C W C L-1 , C ρ C W C -1 , C ρ C W C max } ≤ η -n 2 .</formula><p>Invoking (3.12), we conclude that e ≤ η -n 2 (η m + e -1 ) for all = 1, . . . , L -1, <ref type="bibr">(3.15)</ref> where we set e 0 = 0. We proceed by induction to prove that there exists r ∈ N such that for all = 1, . . . , L -1,</p><formula xml:id="formula_45">e ≤ η m-( -1)n-r . (3.16)</formula><p>Clearly there exists r ∈ N such that e 1 ≤ η m-r . Moreover, one easily verifies that the existence of an r ∈ N such that (3.16) is satisfied for an ∈ {1, . . . , L -2}, thanks to (3.15), implies the existence of an r ∈ N such that (3.16) is satisfied for replaced by + 1. This concludes the induction argument.</p><p>Using (3.14) and (3.16) in (3.13), we finally obtain</p><formula xml:id="formula_46">e L ≤ η m-n 2 + η m-(L-1)n-r 2 ,</formula><p>which yields e L ≤ η for sufficiently large m.</p><p>Remark 3.8. Note that the weights of the network being elements of η m Z ∩ [-η -k , η -k ] implies that each weight can be represented by no more than c log 2 (η -1 ) bits for some constant c &gt; 0.</p><p>Not only does Proposition 3.6 say that the connectivity growth rate of networks achieving uniform approximation error ε over a function class C must exceed O ε -1/γ * (C) , ε → 0, but its proof, by virtue of constructing an encoder-decoder pair that achieves this growth rate, also provides an achievability result. We next establish a matching strong converse in the sense of showing that for γ &gt; γ * (C), the uniform approximation error remains bounded away from zero for infinitely many M ∈ N. A real variable X depending on the variables z i ∈ D i ⊂ R, i = 1, . . . , N , is said to be polynomially bounded in z 1 , . . . , z N if there exists an N -variate polynomial π such that |X| ≤ |π(z 1 , . . . , z N )| for all z i ∈ D i , i = 1, . . . , N . A set of real variables (X j ) j∈J , each depending on z i ∈ D i ⊂ R, i = 1, . . . , N , is uniformly polynomially bounded in z 1 , . . . , z N if there exists an N -variate polynomial π such that |X j | ≤ |π(z 1 , . . . , z N )| for all j ∈ J and all z i ∈ D i , i = 1, . . . , N .</p><p>We will refrain from explicitly specifying the D i in Definition 3.9 whenever they are clear from the context.</p><formula xml:id="formula_47">Remark 3.10. If D i = R \ [-B i , B i ] for some B i ≥ 1, i = 1, . . . , N , then a variable X depending on z i ∈ D i , i = 1, . . . , N, is polynomially bounded in z 1 , . . . , z N if and only if there exists a k ∈ N such that |X| ≤ |z k 1 • z k 2 • . . . • z k N | for all z i ∈ D i . Proposition 3.11. Let d, L ∈ N, Ω ⊂ R d be bounded, π a polynomial, C ⊂ L 2 (Ω)</formula><p>, and ρ : R → R either Lipschitz-continuous or differentiable such that ρ is dominated by a polynomial. Then, for all C &gt; 0 and γ &gt; γ * (C), we have that</p><formula xml:id="formula_48">sup f ∈C inf Φ∈NN π L,M,d,ρ f -Φ L 2 (Ω) ≥ CM -γ for infinitely many M ∈ N. (3.17)</formula><p>Proof. Let γ &gt; γ * (C). Assume, towards a contradiction, that (3.17) holds only for finitely many M ∈ N. Then, there exists a constant C &gt; 0 such that the inequality in (3.17) holds for no M ∈ N and hence there exists C &gt; 0 so that sup</p><formula xml:id="formula_49">f ∈C inf Φ∈NN π L,M,d,ρ f -Φ L 2 (Ω) ≤ C M -γ for all M ∈ N.</formula><p>Setting M ε := (ε/(3C )) -1/γ , it follows that, for every f ∈ C and every ε ∈ (0, 1/2), there exists a neural network Φ ε,f ∈ NN π L,Mε,d,ρ such that</p><formula xml:id="formula_50">f -Φ ε,f L 2 (Ω) ≤ 2 sup f ∈C inf Φ∈NN π L,Mε,d,ρ f -Φ L 2 (Ω) ≤ 2C M -γ ε ≤ 2ε 3 .</formula><p>As the weights of Φ ε,f are polynomially bounded in M ε , they are polynomially bounded in ε -1 . By Lemma 3.7 and Remark 3.10, there hence exists a network Φ ε,f whose weights are represented by no more than c log 2 (ε -1 ) bits, for some constant c &gt; 0, satisfying</p><formula xml:id="formula_51">Φ ε,f -Φ ε,f L 2 (Ω) ≤ ε 3 .</formula><p>Defining</p><formula xml:id="formula_52">Learn : 0, 1 2 × C → NN ∞,∞,d,ρ , (ε, f ) → Φ ε,f , it follows that sup f ∈C f -Learn(ε, f ) L 2 (Ω) ≤ ε with M(Learn(ε, f )) ≤ M ε ∈ O(ε -1 γ ), ε → 0.</formula><p>The proof is concluded by noting that Learn violates Proposition 3. We can now proceed to the proof of Theorem 3.4.</p><p>Proof of Theorem 3.4. Suppose towards a contradiction that γ * ,eff NN (C, ρ) &gt; γ * (C). Let γ ∈ (γ * (C), γ * ,eff NN (C, ρ)). Then, Definition 2.3 implies that there exist a polynomial π, L ∈ N, and C &gt; 0 such that sup</p><formula xml:id="formula_53">f ∈C inf Φ M ∈NN π L,M,d,ρ f -Φ M L 2 (Ω) ≤ CM -γ for all M ∈ N.</formula><p>This, however, constitutes a contradiction to Proposition 3.11.</p><p>We conclude this section with a discussion of the conceptual implications of the results established above. Proposition 3.6 combined with Lemma 3.7 establishes that neural networks with weights polynomially bounded in ε -1 and achieving uniform approximation error ε over C cannot exhibit edge growth rate smaller than O(ε -1/γ * (C) ), ε → 0; in other words, a decay of the uniform approximation error, as a function of M , faster than O(M -γ * (C) ), M → ∞, is not possible.</p><p>Note that requiring uniform approximation error ε only (without imposing the constraint of the network's weights being polynomially bounded in ε -1 ) can lead to arbitrarily large rate γ as exemplified by Theorem 2.2, which proves the existence of networks realizing an arbitrarily small approximation error over L 2 ([0, 1] d ) with a finite number of nodes; in particular, the number of nodes remains constant as ε → 0. However, as argued right after Theorem 2.2, these networks necessarily lead to weights that are not polynomially bounded in ε -1 .</p><p>Finally, we remark that the proofs of Theorem 3.4 and Proposition 3.6, by virtue of explicitly constructing encoder-decoder pairs for neural networks, provide a bound on the minimax code length of these networks. This, in turn, implies a bound on the networks' covering numbers (see <ref type="bibr" target="#b25">[26]</ref>), which, based on classical results from statistical learning theory (see, for example, <ref type="bibr" target="#b10">[11]</ref>), leads to bounds on the generalization error; see, e.g., <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Transitioning from representation systems to neural networks. The remainder of this paper is devoted to identifying function classes that are optimally representable-according to Definition 3.5-by neural networks. The mathematical technique we develop in the process is interesting in its own right as it constitutes a general framework for transferring results on function approximation through representation systems to results on approximation by neural networks. In particular, we prove that for a given function class C and an associated representation system D which satisfies certain technical conditions, there exists a neural network with O(M ) nonzero edge weights that achieves (up to a multiplicative constant) the same uniform error over C as a best M -term approximation in D. This will finally lead to a characterization of function classes C that are optimally representable by neural networks in the sense of Definition 3.5.</p><p>We start by stating technical conditions on representation systems for the transference principle outlined above to apply. Definition 4.1. Let d ∈ N, Ω ⊂ R d , ρ : R → R, and D = (ϕ i ) i∈I ⊂ L 2 (Ω) be a representation system. Then, D is said to be representable by neural networks (with activation function ρ) if there exist L, R ∈ N such that for all η &gt; 0 and every i ∈ I, there is a neural network The next result formalizes our transference principle for networks with weights in R. </p><formula xml:id="formula_54">Φ i,η ∈ NN L,R,d,ρ with ϕ i -Φ i,η L 2 (Ω) ≤ η. c 2019</formula><formula xml:id="formula_55">M = i∈I M c i ϕ i , I M ⊂ I, #I M = M , satisfy f -f M L 2 (Ω) ≤ ε,</formula><p>where ε ∈ (0, 1/2). Then, there exist L ∈ N (depending on D only) and a neural network</p><formula xml:id="formula_56">Φ(f, M ) ∈ NN L,M ,d,ρ with M ∈ O(M ) satisfying f -Φ(f, M ) L 2 (Ω) ≤ 2ε. (4.1)</formula><p>In particular, for all function classes C ⊂ L 2 (Ω), it holds that</p><formula xml:id="formula_57">γ * NN (C, ρ) ≥ γ * (C, D). (4.2)</formula><p>Proof. By representability of D according to Definition 4.1, it follows that there exist L, R ∈ N, such that for each i ∈ I M and for η := ε/ max{1, i∈I M |c i |}, there exists a neural network Φ i,η ∈ NN L,R,d,ρ with</p><formula xml:id="formula_58">ϕ i -Φ i,η L 2 (Ω) ≤ η. (4.3)</formula><p>Let then Φ(f, M ) be the neural network consisting of the networks (Φ i,η ) i∈I M operating in parallel, all with the same input, and summing their one-dimensional outputs (see Figure <ref type="figure" target="#fig_2">3</ref> in section 8 for an illustration) with weights (c i ) i∈I M according to</p><formula xml:id="formula_59">(4.4) Φ(f, M )(x) := i∈I M c i Φ i,η (x) for x ∈ Ω.</formula><p>This construction is legitimate as all networks Φ i,η have the same number of layers and the last layer of a neural network according to Definition 1.1 implements an affine function only (without subsequent application of the activation function ρ). Then, the fact that Φ(f, M ) ∈ NN L,RM,d,ρ and application of the triangle inequality together with (4.3) yields f M -Φ(f, M ) L 2 (Ω) ≤ ε. Another application of the triangle inequality according to</p><formula xml:id="formula_60">f -Φ(f, M ) L 2 (Ω) ≤ f -f M L 2 (Ω) + f M -Φ(f, M ) L 2 (Ω) ≤ 2ε</formula><p>finalizes the proof of (4.1), which by Definitions 1.2 and 1.3 implies (4.2).</p><p>Theorem 4.2 shows that we can restrict ourselves to the approximation of the individual elements of a representation system by neural networks with the only constraint being that the number of nonzero edge weights in the individual networks must admit a uniform upper bound. Theorem 4.2, however, does not guarantee that the weights of the network Φ(f, M ) can be represented with no more than c log 2 (ε -1 ) bits when the overall approximation error is ε. This will again be accomplished through a transfer argument, applied to representation systems D satisfying slightly more stringent technical conditions. Then, for all γ &lt; γ * ,eff (C, D), there exist a polynomial π, constants c &gt; 0, L ∈ N, and a map</p><formula xml:id="formula_61">Learn : 0, 1 2 × L 2 (Ω) → NN π L,∞,d,ρ ,</formula><p>such that for every f ∈ C the weights in Learn(ε, f ) can be represented by no more than c log 2 (ε -1 ) bits while f -Learn(ε, f Proof of Theorem 4.3. Let M ∈ N and γ &lt; γ * ,eff (C, D). According to Definition 2.1, there exist constants C, D &gt; 0 and a polynomial π such that for every f ∈ C, there is a subset</p><formula xml:id="formula_62">) L 2 (Ω) ≤ ε and M(Learn(ε, f )) ∈ O(ε -1/γ ) for ε → 0.</formula><formula xml:id="formula_63">I M ⊂ {1, . . . , π(M )} and coefficients (c i ) i∈I M with max i∈I M |c i | ≤ D so that (4.5) f - i∈I M c i ϕ i L 2 (Ω) ≤ CM -γ 2 =: δ M 2 .</formula><p>We only need to consider the case δ M ≤ 1/2 as will become clear below. By effective representability according to Definition 4.1, there are L, R ∈ N such that for each i ∈ I M and with η := δ M / max{1, 4 i∈I M |c i |}, there exists a neural network Φ i,η ∈ NN L,R,d,ρ (with ρ either Lipschitz-continuous or differentiable such that ρ is dominated by a polynomial) satisfying</p><formula xml:id="formula_64">ϕ i -Φ i,η L 2 (Ω) ≤ η.</formula><p>In addition, the weights of Φ i,η are polynomially bounded in i, η -1 . Let then Φ(f, M ) ∈ NN L,RM,d,ρ be the neural network consisting of the networks (Φ i,η ) i∈I M operating in parallel, according to (4.4). We conclude that</p><formula xml:id="formula_65">i∈I M c i ϕ i -Φ(f, M ) L 2 (Ω) ≤ δ M 4 .</formula><p>As the weights of the networks Φ i,η are polynomially bounded in i, η -1 and i ≤ π(M ), δ M ∼ M -γ , it follows that the weights of Φ(f, M ) are polynomially bounded in</p><formula xml:id="formula_66">δ -1 M , Φ(f, M ) -Φ(f, M ) L 2 (Ω) ≤ δ M 4 ,</formula><p>and all weights of Φ(f, M ) can be represented with no more than c log 2 (δ </p><formula xml:id="formula_67">f -Φ(f, M ) L 2 (Ω) ≤ f - i∈I M c i ϕ i L 2 (Ω) + i∈I M c i ϕ i -Φ(f, M ) L 2 (Ω) + Φ(f, M ) -Φ(f, M ) L 2 (Ω) ≤ δ M = CM -γ . (4.6)</formula><p>For ε ∈ (0, 1/2), we now set</p><formula xml:id="formula_68">Learn(ε, f ) := Φ(f, M ε ),</formula><p>where</p><formula xml:id="formula_69">M ε := C ε 1 γ . (4.7)</formula><p>With this choice of M ε , we have CM -γ ε ≤ ε, which, when used in (4.6), yields</p><formula xml:id="formula_70">f -Learn(ε, f ) L 2 (Ω) ≤ ε. (4.8)</formula><p>Since, by construction, Learn(ε, f ) has RM ε edges and, moreover,</p><formula xml:id="formula_71">M ε ≤ C 1/γ ε -1/γ + 1 ≤ 2C 1/γ ε -1/γ ,</formula><p>it follows that Learn(ε, f ) has at most 2RC 1/γ ε -1/γ edges. Moreover, as all weights of Learn(ε, f ) can be represented by no more than c log 2 (δ -1</p><p>Mε ) bits, it follows from δ Mε ∼ M -γ ε ∼ ε that they can be represented by no more than c log 2 (ε -1 ) bits for some c &gt; 0. This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">All affine representation systems are effectively representable by neural networks.</head><p>This section shows that a large class of representation systems, namely affine systems, defined below, is effectively representable by neural networks. Affine systems include wavelets, ridgelets, curvelets, shearlets, α-shearlets, and, more generally, α-molecules. Combined with Theorem 4.3, the results in this section establish that any function class that is optimally represented by an arbitrary affine system is optimally represented by neural networks in the sense of Definition 3.5.</p><p>Clearly, such strong statements are possible only under restrictions on the choice of the activation function for the approximating neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Choice of activation function.</head><p>We consider two classes of activation functions, namely sigmoidal functions and smooth approximations of rectified linear units. We start with the formal definition of sigmoidal activation functions as considered in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b6">7]</ref>. A differentiable function ρ is called strongly sigmoidal of order k if there exist constants a, b, C &gt; 0 such that</p><formula xml:id="formula_72">Definition 5.1. A continuous function ρ : R → R is called a sigmoidal function of order k ∈ N, k ≥ 2, if there exists C &gt; 0 such that lim x→-∞ 1 x k ρ(x) = 0, lim x→∞ 1 x k ρ(x) = 1,</formula><formula xml:id="formula_73">1 x k ρ(x) ≤ C|x| -a for x &lt; 0, 1 x k ρ(x) -1 ≤ Cx -a for x ≥ 0,<label>and</label></formula><formula xml:id="formula_74">|ρ(x)| ≤ C(1 + |x|) k , d dx ρ(x) ≤ C|x| b for x ∈ R.</formula><p>One of the most widely used activation functions is the so-called rectified linear unit (ReLU) given by x → max{0, x}. The second class of activation functions we consider here are smooth versions of the ReLU.</p><formula xml:id="formula_75">Definition 5.2. Let ρ : R → R + , ρ ∈ C ∞ (R), satisfy ρ(x) = 0 for x ≤ 0, x for x ≥ K,</formula><p>for some constant K &gt; 0. Then, we call ρ an admissible smooth activation function.</p><p>The reason for considering these two specific classes of activation functions resides in the fact that neural networks based thereon allow economical representations of multivariate bump functions, which, in turn, leads to effective representation of all affine systems (built from bump functions) by neural networks. Approximation of multivariate bump functions using sparsely connected neural networks is a classical topic in neural network theory <ref type="bibr" target="#b37">[38]</ref>. What is new here is the aspect of quantized weights and rate-distortion optimality.</p><p>Note that the smooth variants of the ReLU we consider here allow us to build on existing approximation results for bump functions. We emphasize, however, that smooth functions can be approximated-in a rate-distortion optimal fashion-by networks based on the regular ReLU activation function provided that their depth is allowed to scale polylogarithmically in the inverse of the approximation error ε <ref type="bibr" target="#b29">[30]</ref>. However, in sections 5.1 and 6, we require smooth generators of affine systems to be approximated by neural networks with the number of weights remaining constant as ε → 0. This is not possible with the regular ReLU activation function. We emphasize, though, that all results in this paper apart from those in sections 5.1 and 6 are also valid for the regular ReLU activation function.</p><p>A class of bump functions of particular importance in wavelet theory are B-splines. In <ref type="bibr" target="#b6">[7]</ref> it was shown that B-splines can be parsimoniously approximated by neural networks with sigmoidal activation functions. It is instructive to recall this result. To this end, for m ∈ N, we denote the univariate cardinal B-spline of order m ∈ N by N m , i.e., N 1 = χ [0,1] , where χ Additionally, we will need to control the weights in the approximating networks Φ D,ε . We next show that this is, indeed, possible for strongly sigmoidal activation functions.</p><p>Theorem 5.4. Let d, m, k ∈ N, and let ρ be strongly sigmoidal of order k ≥ 2. Further, let L := log 2 (mdd)/ log 2 (k) + 1. Then, there is M ∈ N, and a bivariate polynomial π possibly dependent on d, m, k, such that for all D, ε &gt; 0, there exists a neural network</p><formula xml:id="formula_76">Φ D,ε ∈ NN L,M,d,ρ with N d m -Φ D,ε L 2 ([-D,D] d ) ≤ ε.</formula><p>Moreover, the weights of Φ D,ε are polynomially bounded in D, ε -1 .</p><p>Proof. The neural network Φ D,ε in Theorem 5.3 is explicitly constructed in <ref type="bibr" target="#b6">[7]</ref>. Carefully following the steps in that construction and making explicit use of the strong sigmoidality of ρ, as opposed to plain sigmoidality as in <ref type="bibr" target="#b6">[7]</ref>, yields the desired result.</p><p>Remark 5.5. We observe that the number of edges of the approximating network in Theorem 5.4 does not depend on the approximation error ε.</p><p>While Theorem 5.3 demonstrates that a B-spline of order m can be approximated to arbitrary accuracy by a neural network based on a sigmoidal activation function and of depth depending on m, d, and the order of sigmoidality of the activation function, we next establish that for admissible smooth activation functions, exact representation of a general class of bump functions is possible with a network of three layers only. Before proceeding, we define</p><formula xml:id="formula_77">for f ∈ L 1 (R d ), d ∈ N, the Fourier transform of f by f (ξ) := R d f (x)e -2πi x,ξ dx for ξ ∈ R d .</formula><p>Theorem 5.6. Let ρ be an admissible smooth activation function. Then, for all d ∈ N, there exist M ∈ N and a neural network</p><formula xml:id="formula_78">Φ ρ ∈ NN 3,M,d,ρ such that (i) Φ ρ is compactly supported, (ii) Φ ρ ∈ C ∞ (R),<label>and</label></formula><formula xml:id="formula_79">(iii) Φ ρ (ξ) = 0 for all ξ ∈ [-3, 3] d .</formula><p>Proof. We start by constructing an auxiliary function as follows. For 0 &lt; p 1 ≤ p 2 ≤ p 3 such that p 1 + p 2 = p 3 , define t : R → R as</p><formula xml:id="formula_80">t(x) := ρ(x) -ρ(x -p 1 ) -ρ(x -p 2 ) + ρ(x -p 3 ), x ∈ R. (5.1) Then, t ∈ C ∞ is compactly supported. Letting q = t L ∞ (R) , we define g : R d → R according to g(x) := ρ d i=1 t(x i ) -(d -1) • q , x ∈ R d . (5.2)</formula><p>By construction, g ∈ C ∞ is compactly supported. Moreover, g can be realized through a three-layer neural network thanks to its two-step design per (5.1) and (5.  </p><formula xml:id="formula_81">(5.3) f -Φ D,ε L 2 ([-D,D] d ) ≤ ε.</formula><p>Let A ∈ R d×d be full-rank and b ∈ R d . Then, there exists M ∈ N, depending on M and d only, such that for all E, η &gt; 0, there is</p><formula xml:id="formula_82">Ψ E,η ∈ NN L,M ,d,ρ with |det(A)| 1 2 f (A • -b) -Ψ E,η L 2 ([-E,E] d ) ≤ η.</formula><p>Moreover, if the weights of Φ D,ε are polynomially bounded in D, ε -1 , then the weights of Ψ E,η are polynomially bounded in A ∞ , E, b ∞ , η -1 , where A ∞ and b ∞ denote the max-norm of A and b, respectively. Proof. By a change of variables, we have for every Φ ∈ NN L,M,d,ρ that</p><formula xml:id="formula_83">|det(A)| 1 2 (f (A • -b) -Φ(A • -b)) L 2 ([-E,E] d ) = f -Φ L 2 (A•[-E,E] d -b) ,<label>(5.4)</label></formula><p>and there exists a constant M depending on M and d only such that |det</p><formula xml:id="formula_84">(A)| 1/2 Φ(A • -b) ∈ NN L,M ,d,ρ . We furthermore have that A • [-E, E] d -b ⊂ [-(dE A ∞ + b ∞ ), (dE A ∞ + b ∞ )] d . (5.5) We now set F = dE A ∞ + b ∞ and Ψ E,η := |det(A)| 1/2 Φ F,η (A • -b) and observe that |det(A)| 1 2 f (A • -b) -Ψ E,η L 2 ([-E,E] d ) = f -Φ F,η L 2 (A•[-E,E] d -b) ≤ f -Φ F,η L 2 ([-F,F )] d ) ≤ η,</formula><p>where we applied the same reasoning as in <ref type="bibr">(5.4)</ref> in the first equality and used (5.5) in the first inequality and (5.3) in the second. Moreover, we see that if the weights of Φ D,ε are polynomially bounded in D, ε -1 , then the weights of Ψ E,η are polynomially bounded in Assume that there exist M, L ∈ N such that for all D, ε &gt; 0, there is Φ D,ε ∈ NN L,M,d,ρ with</p><formula xml:id="formula_85">A ∞ , |det(A)|, E, b ∞ , η -1 . Since |det(A)| is polynomially bounded in A ∞ , it</formula><formula xml:id="formula_86">(5.6) f -Φ D,ε L 2 ([-D,D] d ) ≤ ε. Let r ∈ N, (c i ) r i=1 ⊂ R,<label>and</label></formula><formula xml:id="formula_87">(d i ) r i=1 ⊂ R d .</formula><p>Then, there exists M ∈ N, depending on M, d, and r only, such that for all E, η &gt; 0, there is Ψ E,η ∈ NN L,M ,d,ρ with (5.7)</p><formula xml:id="formula_88">r i=1 c i f (• -d i ) -Ψ E,η L 2 ([-E,E] d ) ≤ η.</formula><p>Moreover, if the weights of Φ D,ε are polynomially bounded in D, ε -1 , then the weights of Ψ E,η are polynomially bounded in</p><formula xml:id="formula_89">r i=1 |c i |, E, max i=1,...,r d i ∞ , η -1 .</formula><p>Proof. Let E, η &gt; 0. We start by noting that, for all D, ε &gt; 0,</p><formula xml:id="formula_90">r i=1 c i f (• -d i ) - r i=1 c i Φ D,ε (• -d i ) L 2 ([-E,E] d ) ≤ r i=1 |c i | • f -Φ D,ε L 2 ([-(E+d * ),(E+d * )] d ) ,</formula><p>where d * = max i=1,...,r d i ∞ . Setting D = E + d * and ε = η/ max{1, r i=1 |c i |}, and noting that for every Φ ∈ NN L,M,d,ρ , the function</p><formula xml:id="formula_91">Ψ := r i=1 c i Φ(• -d i )</formula><p>is in NN L,M ,d,ρ with M ∈ N depending on d, r, and M only, it follows that the network</p><formula xml:id="formula_92">Ψ E,η := r i=1 c i Φ D,ε (• -d i )</formula><p>satisfies (5.7). Finally, if the weights of Φ D,ε are polynomially bounded in D, ε -1 , then the weights of Ψ E,η are polynomially bounded in r i=1 |c i |, E, d * , η -1 . Based on the invariance results in Propositions 5.7 and 5.8, we now construct neural networks which approximate functions with a given number of vanishing moments with arbitrary accuracy. The resulting construction will be crucial in establishing representability of affine representation systems (see Definition </p><formula xml:id="formula_93">∈ C(R d ) is said to possess R directional vanishing moments in the x k -direction if, for all ∈ {0, . . . , R -1} and all x 1 , . . . , x k-1 , x k+1 , . . . , x d ∈ R, R x k g(x 1 , . . . , x k , . . . , x d )dx k = 0.</formula><p>The next result establishes that functions with an arbitrary number of vanishing moments in a given coordinate direction can be built from suitable linear combinations of translates of a given continuous function with compact support. Lemma 5.10. Let R, d ∈ N, B &gt; 0, k ∈ {1, . . . , d}, and f ∈ C(R d ) with compact support. Then, the function</p><formula xml:id="formula_94">(5.8) g(x 1 , . . . , x d ) := R-1 =0 R -1 (-1) f x 1 , . . . , x k -B , . . . , x d has R directional vanishing moments in the x k -direction. Moreover, if f (ξ) = 0 for all ξ ∈ [-B, B] d \ {0}, then<label>(5.9) ĝ</label></formula><formula xml:id="formula_95">(ξ) = 0 for all ξ ∈ [-B, B] d with ξ k = 0.</formula><p>Proof. For simplicity of exposition, we consider the case B = 1 only. Taking the Fourier transform of (5.8) yields</p><formula xml:id="formula_96">ĝ(ξ) = R-1 =0 R -1 (-1) e -2πi ξ k f (ξ) = 1 -e -2πiξ k R-1 • f (ξ), (5.10) which implies ∂ ∂ξ k ĝ ξ k =0</formula><p>= 0 for all ∈ {0, . . . , R -1}.</p><p>But, by Definition 5.9, this says precisely that g possesses the desired vanishing moments. Statement (5.9) follows by inspection of (5.10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Affine representation systems.</head><p>We are now ready to introduce the general family of representation systems announced earlier in the paper as affine systems. This class includes all representation systems based on affine transformations of a given "mother function." Special cases of affine systems are wavelets, ridgelets, curvelets, shearlets, α-shearlets, and, more generally, α-molecules, as well as tensor products thereof. The formal definition of affine systems is as follows.</p><p>Definition 5.11. Let d, r, S ∈ N, Ω ⊂ R d be bounded, and let f ∈ L 2 (R d ) be compactly supported. Let δ &gt; 0, (c s i ) r i=1 ⊂ R for s = 1, . . . , S, and let (d i ) r i=1 ⊂ R d . Further, let A j ∈ R d×d , j ∈ N, be full-rank, with the absolute values of the eigenvalues of A j bounded below by 1. Consider the compactly supported functions We define the affine system D ⊂ L 2 (Ω) corresponding to (g s ) S s=1 according to</p><formula xml:id="formula_97">g s := r i=1 c s i f (• -d i ), s = 1, . . . ,</formula><formula xml:id="formula_98">D := g j,b s := |det(A j )| 1 2 g s (A j • -δb) |Ω : s = 1, . . . , S, b ∈ Z d , j ∈ N, and g j,b s = 0 ,</formula><p>and we refer to f as the generator function of D.</p><p>We define the subsystems D s,j := {g j,b s ∈ D : b ∈ Z d }. Since every g s , s = 1, . . . , S, has compact support, |D s,j | is finite for all s = 1, . . . , S and j ∈ N. Indeed, we observe that there exists c b := c b ((g s ) S s=1 , δ, d) &gt; 0 such that for all s ∈ {1, . . . , S}, j ∈ Z, and b ∈ Z d ,</p><formula xml:id="formula_99">g j,b s ∈ D =⇒ b ∞ ≤ c b A j ∞ . (5.11)</formula><p>As the D s,j are finite, we can organize the representation system D according to (5.12)</p><formula xml:id="formula_100">D = (ϕ i ) i∈N = (D 1,1 , . . . , D S,1 , D 1,2 , . . . , D S,2 , . . . ) ,</formula><p>where the elements within each subsystem D s,j may be ordered arbitrarily. This ordering of D is assumed in the remainder of the paper and will be referred to as canonical ordering.</p><p>Moreover, we note that if there exists s o ∈ {1, . . . , S} such that g so is nonzero, then there is a constant</p><formula xml:id="formula_101">c o := c o ((g s ) S s=1 , δ, d) &gt; 0 such that S s=1 |D s,j | ≥ c o |det(A j )| for all j ∈ N.<label>(5.13)</label></formula><p>The next result establishes that all affine systems whose generator functions can be approximated to within arbitrary accuracy by neural networks are (effectively) representable by neural networks.</p><p>Theorem 5.12. Let d ∈ N, ρ : R → R, Ω ⊂ R d be bounded, and let D = (ϕ i ) i∈N ⊂ L 2 (Ω) be an affine system with generator function f . Suppose that there exist constants L, R ∈ N such that for all D, ε &gt; 0, there is Φ D,ε ∈ NN L,R,d,ρ with</p><formula xml:id="formula_102">(5.14) f -Φ D,ε L 2 ([-D,D] d ) ≤ ε.</formula><p>Then, D is representable by neural networks with activation function ρ. If, in addition, the weights of Φ D,ε are polynomially bounded in D, ε -1 , and if there exist a &gt; 0 and c &gt; 0 such that</p><formula xml:id="formula_103">(5.15) 1 ≥ c A 1 ∞ , j-1 k=1 | det(A k )| ≥ c A j a ∞ for all j ∈ N, j ≥ 2,</formula><p>then D is effectively representable by neural networks with activation function ρ.</p><p>Proof. Let (g s ) S s=1 be as in Definition 5.11. If g s = 0 for all s ∈ {1, . . . , S}, then D = ∅ and the result is trivial. Hence, we can assume that there exists at least one s ∈ {1, . . . , S} such that g s = 0, implying that (5. To this end, we need to establish the existence of constants L, R ∈ N such that for all i ∈ N and all η &gt; 0, there exist Φ i,η ∈ NN L,R,d,ρ with</p><formula xml:id="formula_104">ϕ i -Φ i,η L 2 (Ω) ≤ η. (5.16)</formula><p>The elements of D consist of dilations and translations of f according to (5.17)</p><formula xml:id="formula_105">ϕ i = |det(A j i )| 1 2 r k=1 c s i k f (A j i • -δb i -d k )</formula><p>|Ω for some r ∈ N independent of i, and s i ∈ {1, . . . , S}, j i ∈ N, and b i ∈ Z d . Thus <ref type="bibr">(5.16</ref>) follows directly by Propositions 5.7 and 5.8. It remains to show that the weights of Φ D,ε in (5.14) being polynomially bounded in D, ε -1 implies that D is effectively representable by neural networks with activation function ρ, which, by Definition 4.1, means that the weights of Φ i,η are polynomially bounded in i, η -1 . Propositions 5.7 and 5.8 state that the weights of Φ i,η are polynomially bounded in</p><formula xml:id="formula_106">A j i ∞ , D, b i ∞ , r k=1 |c k |, max k=1,...,r d k ∞ , η -1 .</formula><p>Thanks to <ref type="bibr">(5.11)</ref>, we have b i ∞ ∈ O( A j i ∞ ). Moreover, the quantities D, r k=1 |c k |, and max k=1,...,r d k ∞ do not depend on i. We can thus conclude that the weights of Φ i,η are polynomially bounded in <ref type="bibr">(5.18)</ref> A j i ∞ , η -1 .</p><p>To complete the proof, we need to show that the quantities A j i ∞ are polynomially bounded in i. To this end, we first observe that ϕ i according to (5.17) satisfies ϕ i ∈ D s i ,j i for some s i ∈ {1, . . . , S}. Thanks to (5.13) and the canonical ordering (5.12), there exists a constant c &gt; 0 such that i ≥ c</p><formula xml:id="formula_107">j i -1 k=1 | det(A k )|, j i ≥ 2.</formula><p>We finally appeal to (5.15) to conclude that A j i ∞ is polynomially bounded in i for all j i ∈ N, which, together with (5.18), establishes the desired result.</p><p>We remark that condition (5.15) is very weak; in fact, we are not aware of an affine system in the literature that would violate it.</p><p>We now proceed to what is probably the central result of this paper, namely that neural networks provide optimal approximations for all function classes that are optimally approximated by any affine system with generator function that can be approximated to within arbitrary accuracy by neural networks. If, in addition, there is a bivariate polynomial π such that the weights of Φ D,ε are bounded by | π(D, ε -1 )|, there exist a &gt; 0 and c &gt; 0 such that (5.15) holds, and C is optimally represented by D (according to Definition 3.3), then for all γ &lt; γ * (C), there exist a constant c &gt; 0, a polynomial π, and a map</p><formula xml:id="formula_108">Learn : 0, 1 2 × L 2 (Ω) → NN π L,∞,d,ρ ,</formula><p>such that for every f ∈ C the weights in Learn(ε, f ) can be represented by no more than c log 2 (ε -1 ) bits while f -Learn(ε, f</p><formula xml:id="formula_109">) L 2 (Ω) ≤ ε and M(Learn(ε, f )) ∈ O(ε -1/γ ), ε → 0.</formula><p>Proof. The proof follows directly by combining Theorem 5.12 with Theorems 4.2 and 4.3.</p><p>Theorem 5.13 reveals a remarkable universality and optimality property of neural networks: All function classes that can be optimally represented by an affine system with generator f satisfying <ref type="bibr">(5.14)</ref> are also optimally representable by neural networks.</p><p>6. α-shearlets and cartoon-like functions. We next present an explicit pair (C, D) of function class and representation system satisfying γ * NN (C, ρ) = γ * (C, D). Specifically, we take α-shearlets as representation system D ⊂ L 2 (R 2 ) and α -1 -cartoon-like functions as function class C. Cartoon-like functions are piecewise smooth functions with only two pieces. These pieces are separated by a smooth interface. In a sense, they can be understood as a prototype of a two-dimensional classification function with two homogeneous areas corresponding to two classes. Understanding neural network approximation of this function class is hence relevant to classification tasks in machine learning. We point out that the definition of α-shearlets in this paper differs slightly from that in <ref type="bibr" target="#b26">[27]</ref>. Concretely, relative to <ref type="bibr" target="#b26">[27]</ref> our definition replaces α -1 by α so that α-shearlets are a special case of α-molecules, whereas in <ref type="bibr" target="#b26">[27]</ref> α-shearlets are a special case of α -1 -molecules. We will need dilation and shearing matrices defined as D α,a := a 0 0 a α , J := 0 1 1 0 , and S k := 1 k 0 1 .</p><p>This leads us to the following definition which is a slightly modified version of the corresponding definition in <ref type="bibr" target="#b55">[56]</ref>.</p><p>Definition 6.1 (see <ref type="bibr" target="#b55">[56]</ref>). For δ ∈ R + , α ∈ [0, 1], and f, g ∈ L 2 (R 2 ), the cone-adapted α-shearlet system SH α (f, g, δ) generated by f, g ∈ L 2 (R 2 ) is defined as</p><formula xml:id="formula_110">SH α (f, g, δ) := SH 0 α (f, g, δ) ∪ SH 1 α (f, g, δ)</formula><p>, where </p><formula xml:id="formula_111">SH 0 α (f, g, δ) : = f (• -δt) : t ∈ Z 2 , SH 1 α (f, g, δ) : = 2 1+α 2 g(S k D α,2 J τ • -δt) : ∈ N 0 , |k| ≤ 2 (1-α) , t ∈ Z 2 , k ∈ Z, τ ∈ {0,</formula><formula xml:id="formula_112">E β (R 2 ; ν) = {f ∈ L 2 (R 2 ) : f = f 0 + χ B f 1 }, where f 0 , f 1 ∈ C β (R 2 ), supp f 0 , supp f 1 ⊂ (0, 1) 2 , B ⊂ [0, 1] 2 , χ B denotes the characteristic function of B, ∂B ∈ C β , and f 1 C β , f 2 C β , ∂B C β &lt; ν. The elements of E β (R 2 ; ν) are called β-cartoon-like functions.</formula><p>This function class was originally introduced in <ref type="bibr" target="#b19">[20]</ref> as a model class for functions governed by curvilinear discontinuities of prescribed regularity. In this sense, β-cartoon-like functions provide a convenient model for images governed by edges or for the solutions of transport equations which often exhibit curvilinear singularities.</p><p>The optimal exponent γ * (E β (R 2 ; ν)) was found in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Theorem 6.3. For β ∈ [1, 2] and ν &gt; 0, we have</p><formula xml:id="formula_113">γ * (E β (R 2 ; ν)) = β 2 .</formula><p>Proof. The proof of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">Theorem 2]</ref> demonstrates that a general function class C has optimal exponent γ * (C) = (2 -p)/2p if C contains a copy of p 0 . The result now follows, since by <ref type="bibr" target="#b27">[28]</ref>, the function class E β (R 2 ; ν) does indeed contain a copy of p 0 for p = 2/(β + 1). Using Proposition 3.6, this result allows us to conclude that neural networks achieving uniform approximation error ε over the class C of cartoon-like functions, with weights represented by no more than c log 2 (ε -1 ) bits, for some constant c &gt; 0, yield an effective best M -edge approximation rate of at most β/2. Theorem 6.8 below demonstrates achievability for β = 1/α, with α ∈ [1/2, 1].</p><p>The following theorem states that α-shearlets yield optimal best M -term approximation rates for α -1 -cartoon-like functions. Then, there exists δ * &gt; 0 such that for all δ &lt; δ * , the function class E 1/α (R 2 ; ν) is optimally represented by SH α (f, g, δ).</p><p>Remark 6.5. The assumptions on the smoothness and the number of vanishing moments of f and g in Theorem 6.4 follow from [56, eq. 4.9] with s 1 = 3/2, s 0 = 0, p 0 = q 0 = 2/3, and |β| ≤ 4. While these particular choices allow the statement of the theorem to be independent of α, it is possible to weaken the assumptions if a fixed α is considered. For example, for α = 1/2 the smoothness assumptions on f and g reduce to f ∈ C 11 , g ∈ C As our approximation results for neural networks pertain to bounded domains, we require a definition of cartoon-like functions on bounded domains. Definition 6.6. Let (0, 1) 2 ⊂ Ω ⊂ R 2 , α ∈ [1/2, 1], and ν &gt; 0. We define the set of α -1 -cartoon-like functions on Ω by</p><formula xml:id="formula_114">E 1 α (Ω; ν) := f |Ω : f ∈ E 1 α (R 2 ; ν) .</formula><p>Additionally, for δ &gt; 0, f, g ∈ L 2 (R 2 ), we define an α-shearlet system on Ω according to</p><formula xml:id="formula_115">SH α (f, g, δ; Ω) := φ |Ω : φ ∈ SH α (f, g, δ) . Remark 6.7. It is straightforward to check that if E 1/α (R 2 ; ν) is optimally represented by SH α (f, g, δ), then E 1/α (Ω; ν) is optimally represented by SH α (f, g, δ; Ω).</formula><p>We proceed to the main statement of this section. Theorem 6.8. Suppose that (0, 1) 2 ⊂ Ω ⊂ R 2 is bounded and ρ : R → R is either strongly sigmoidal of order k ≥ 2 (see Definition 5.1) or an admissible smooth activation function (see Definition 5.2). Then, for every α ∈ [1/2, 1], the function class E 1/α (Ω; ν) is optimally representable by a neural network with activation function ρ.</p><p>Proof. Let α ∈ [1/2, 1] and ν &gt; 0. We first consider the case of ρ strongly sigmoidal of order k ≥ 2. Since the two-dimensional cardinal B-spline of order 34, denoted by N 2 34 , is 32 times continuously differentiable and N 2 34 (0) = 0 by construction, we conclude that there exists c &gt; 0 such that f := N 2 34 (c•) satisfies f ∈ C 32 (R 2 ) and f = 0 for all ξ ∈ [-3, 3] 2 . Application of Lemma 5.10 then yields the existence of (c i )</p><formula xml:id="formula_116">7 i=1 ⊂ R, (d i ) 7 i=1 ⊂ R 2 such that g := 7 i=1 c i f (• -d i )</formula><p>is compactly supported and has seven vanishing moments in the x 1direction, and ĝ(ξ) = 0 for all ξ ∈ [-3, 3] 2 such that ξ 1 = 0. Then, by Theorem 6.4 and Remark 6.7, there exists δ &gt; 0 such that SH α (f, g, δ; Ω) is optimal for E 1/α (Ω; ν). We define</p><formula xml:id="formula_117">{A j : j ∈ N} := S k D α,2 J τ : ∈ N 0 , |k| ≤ 2 (1-α) , τ ∈ {0, 1} ,</formula><p>where we order (A j ) j∈N such that |det(A j )| ≤ |det(A j+1 )| for all j ∈ N. This construction implies that the α-shearlet system SH α (f, g, δ; Ω) is an affine system with generator function f . Thanks to Theorem 5.4, there exist L, R ∈ N such that for all D, ε &gt; 0, there is a network Φ</p><formula xml:id="formula_118">D,ε ∈ NN L,R,d,ρ with f -Φ D,ε L 2 ([-D,D] d ) ≤ ε.</formula><p>Moreover, the weights of Φ D,ε are polynomially bounded in D, ε -1 . It is not difficult to verify that (5.15) holds, and hence Theorem 5.12 yields that SH α (f, g, δ; Ω) is effectively representable by neural networks with activation function ρ. Finally, since E 1/α (Ω; ν) is optimally representable by SH α (f, g, δ; Ω), we conclude with Theorem 4.3 that E 1/α (Ω; ν) is optimally representable by neural networks with activation function ρ.</p><p>It remains to establish the statement for admissible smooth ρ. In this case, by Theorem 5.6 there exist M ∈ N and a neural network in NN 3,M,d,ρ which realizes a compactly supported c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php f ∈ C ∞ (R) satisfying f (ξ) = 0 for all ξ ∈ [-3, 3] 2 . Lemma 5.10 applied to this f then yields a function g that can be realized by a neural network in NN 3,M ,d,ρ for some M ∈ N, has seven vanishing moments in the x 1 -direction, is compactly supported, and satisfies g ∈ C ∞ (R), and ĝ(ξ) = 0 for all ξ ∈ [-3, 3] 2 such that ξ 1 = 0. By Theorem 6.4 and Remark 6.7, there exists δ &gt; 0 such that E 1/α (Ω; ν) is optimally representable by SH α (f, g, δ; Ω). Note that SH α (f, g, δ; Ω) is an affine system with generator function f . Since f can be realized with zero error by a neural network, Theorem 5.12 yields that SH α (f, g, δ; Ω) is effectively representable by neural networks with admissible smooth activation function ρ. Optimality of SH α (f, g, δ; Ω) for E 1/α (Ω; ν) implies, with Theorem 4.3, that E 1/α (Ω; ν) is optimally representable by neural networks with admissible smooth activation function ρ. Remark 6.9. Theorem 6.4 requires the generators of the shearlet system guaranteeing optimal representability of E 1/α (Ω; ν) for 1/2 ≤ α ≤ 1, ν &gt; 0, Ω ⊂ R 2 to be very smooth. On the other hand, Theorem 6.8 demonstrates that optimally-approximating neural networks are not required to be particularly smooth. Indeed, Theorem 6.8 holds for networks with differentiable but not necessarily twice differentiable activation function. As the proof of Theorem 6.8 reveals, such weak assumptions suffice thanks to Theorem 5.4, which demonstrates that it is possible to approximate arbitrarily smooth B-splines (in the L 2 -norm) to within error ε by neural networks with a number of weights that does not depend on ε as long as the activation function is strongly sigmoidal. Remark 6.10. We observe from the proof of Theorem 6.8 that the depth of the networks required to achieve optimal approximation depends on the activation function only. Indeed, for an admissible smooth activation function, inspection of Theorem 5.6 reveals that networks with three layers can produce optimal approximations in Theorem 6.8. On the other hand, if a sigmoidal activation function is employed, Theorem 5.4 shows that the construction in Theorem 6.8 requires a certain minimum depth depending on the order of sigmoidality.</p><p>7. Generalization to manifolds. Frequently, a function f to be approximated by a neural network models phenomena on (possibly low-dimensional) immersed submanifolds Γ ⊂ R d of dimension m &lt; d. We next briefly outline how our main results can be extended to this situation. Since analogous results, for the case of wavelets as representation systems, appear already in <ref type="bibr" target="#b52">[53]</ref>, we will allow ourselves to be somewhat informal.</p><p>Suppose that f : Γ → R is compactly supported. Let (U i ) i∈N ⊂ Γ be an open cover of Γ such that for each i ∈ N the manifold patch U i can be parametrized as the graph of a function over a subset of the Euclidean coordinates; i.e., there exist coordinates x d 1 , . </p><formula xml:id="formula_119">i := f h i such that (7.1) f = i∈N f i . Every f i : U i → R can be reparametrized to fi : R m → R, (x d 1 , . . . , x dm ) → f i • Ξ i (x d 1 , . . . , x dm ).</formula><p>Suppose that there exist L, M ∈ N and neural networks Φi ∈ NN L,M,m,ρ such that</p><formula xml:id="formula_120">(7.2) fi -Φi L 2 (V i ) ≤ ε.</formula><p>Then, we can construct a neural network Φ i ∈ NN L,M +md,d,ρ according to</p><formula xml:id="formula_121">Φ i (x) := Φ i (P i x),</formula><p>where P i denotes the orthogonal projection of x onto the coordinates (x d 1 , . . . , x dm ). Since P i is linear, Φ i is a neural network. Moreover, since P i is the inverse of the diffeomorphism Ξ i , we get</p><formula xml:id="formula_122">Φ i -f i L 2 (U i ) ≤ Cε,</formula><p>with C &gt; 0 depending on the curvature of Γ| U i only. Now we may build a neural network Φ by setting Φ := i∈N Φ i . Combining (7.2) with the observation that, owing to the compact support of f , only a finite number of summands appears in the definition of f , we have constructed a neural network Φ which approximates f on Γ. In summary, we observe the following.</p><p>Whenever a function class C is invariant with respect to diffeomorphisms (in our construction the functions Ξ i ) and multiplication by smooth functions (in our construction the functions h i ), then approximation results on R m can be lifted to approximation results on m-dimensional submanifolds Γ ⊂ R d . Such invariances are, in particular, satisfied for all function classes characterized by a certain smoothness behavior-for example, the class of cartoon-like functions studied in section 6.</p><p>8. Numerical results. Our theoretical results show that neural networks realizing uniform approximation error ε over a function class C ⊂ L 2 (R d ), d ∈ N, must obey a fundamental lower bound on the growth rate (as ε → 0) of the number of edges of nonzero weight. One of the most widely used learning algorithms is stochastic gradient descent with the gradient computed via back-propagation <ref type="bibr" target="#b51">[52]</ref>. The purpose of this section is to investigate how this algorithm fares relative to our lower bound.</p><p>Interestingly, our numerical experiments below indicate that for a fixed, sparsely connected, network topology inspired by the construction of bump functions according to (5.1) and (5.2), and with the ReLU as activation function, the stochastic gradient descent algorithm generates neural networks that achieve M -edge approximation rates quite close to the c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php fundamental limit. The network topology we prescribe is depicted in Figure <ref type="figure" target="#fig_2">3</ref>. The rationale for choosing this topology is as follows. As mentioned before, admissible smooth activation functions consist of smooth functions which equal a ReLU outside a compact interval. For this class of activation functions, the associated α-shearlet generators were constructed from a function g as specified in (5.2). Choosing p 1 = p 2 = 1 and p 3 = 2 in (5.1) yields hat functions t. This construction implies that six nodes are required in the first layer in each subnetwork. In Figure <ref type="figure" target="#fig_2">3</ref>, we see four network realizations of g in parallel. The output layer realizes a linear combination of the subnetworks. We now train the network using the stochastic gradient descent algorithm. Following (5.2) the weights of the second layer remain fixed, and the weights in the first and third layers only are trained. Training is performed for two different functions, where one is a function with a line singularity (Figure <ref type="figure" target="#fig_3">4</ref>(a)), and the other one is a cartoon-like function (Figure <ref type="figure" target="#fig_22">5</ref>(a)). Specifically, we train the network by drawing samples (x 1 , x 2 ) from an equispaced grid in [-1, 1] 2 . The resulting error is then back-propagated through the network. We repeat this procedure for different network sizes, i.e., for different numbers of subnetworks. We start by discussing the results for the function with a line singularity depicted in Figure <ref type="figure" target="#fig_3">4</ref>(a). The approximation error corresponding to the trained neural network is shown in Figure <ref type="figure" target="#fig_3">4</ref>(b). The faster than linear decay of the approximation error in the semilogarithmic scale indicates faster than exponential decay with respect to the number of edges. This is consistent with the best M -term approximation rate that ridgelets yield for piecewise constant functions with line singularities; see <ref type="bibr" target="#b4">[5]</ref>. It is interesting to observe that the trained subnetworks yield α-molecules for α = 0 (see Figures <ref type="figure" target="#fig_21">4(c)-(e)</ref>). These functions are constant along one direction and vary along another, and hence can be considered part of a ridgelet system, which is, in fact, an optimally sparsifying representation system for line singularities. Moreover, the orientation of the three learned ridge functions matches that of the original function.</p><p>In the second experiment, we draw samples from the function depicted in Figure <ref type="figure" target="#fig_22">5</ref>(a) below, which exhibits a curvilinear singularity. Figures <ref type="figure" target="#fig_22">5(c</ref>  approximation error obtained when simply training with different network sizes did not come close to the rate of M -1 predicted by our theory. However, with a slight adaptation one obtains the result of Figure <ref type="figure" target="#fig_22">5</ref>(b), which demonstrates a decay of roughly M -1 . The specifics of this adaptation are as follows: We first train a large network with ∼ 10000 edges, again by stochastic gradient descent. Then, the weights in the last layer are optimized using the Lasso <ref type="bibr" target="#b54">[55]</ref> to obtain a sparse weight vector c * . We then pick the M largest coefficients of c * and compute the corresponding weighted sum of the associated subnetworks. The resulting approximation error is shown in Figure <ref type="figure" target="#fig_22">5</ref>(b). Finally, we investigate whether the approximation characteristics delivered by this procedure are similar to what would be obtained by best M -term approximation with standard shearlet systems. Recall that shearlet elements at high scales tend to cluster around singularities <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37]</ref>. Figures <ref type="figure" target="#fig_22">5(g)-(</ref>i) depict the corresponding results. Specifically, Figure <ref type="figure" target="#fig_22">5</ref>(g) shows the weighted sum of those subnetworks that have the largest support. In Figure <ref type="figure" target="#fig_22">5</ref>(h), we show weighted sums of subnetworks with medium-sized support, and in Figure <ref type="figure" target="#fig_22">5</ref>(i) we sum up only the subnetworks with the smallest supports. We observe that, indeed, subnetworks of large support approximate the smooth part of the underlying function, whereas the subnetworks associated with small supports resolve the jump singularity. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Assignment of the weights (A )i,j and (b )i of a two-layer network to the edges and nodes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1. 3 .</head><label>3</label><figDesc>Approximation by deep neural networks. The main conceptual contribution of this paper is the development of an approximation-theoretic framework for deep neural networks c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 )</head><label>4</label><figDesc>where NN π L,M,d,ρ denotes the class of networks in NN L,M,d,ρ that have all their weights bounded in absolute value by |π(M )|, will be referred to as effective best M -edge approximation rate of C by neural networks and denoted by γ * ,eff NN (C, ρ). c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php We will show in Corollary 3.4 that sup ρ:R→R γ * ,eff NN (C, ρ) is bounded and depends on the "description complexity" of the function class C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Proposition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Enumeration of nodes as employed in the proof of the theorem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Additionally, we observe thate L ≤ C W C L-1 η m + C W C max e L-1 . (3.13)We now bound the quantity C for = 1, . . . , L -1. A simple computation, exploiting the Lipschitz-continuity of ρ, yieldsC ≤ (|ρ(0)| + C ρ C W C max C -1 ) for all = 1, . . . , L -1.Since ρ is continuous on R, we have |ρ(0)| &lt; ∞, and thus, by C ρ , C W , C max ≥ 1, there exists C &gt; 0 such that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem 4 . 2 .</head><label>42</label><figDesc>Let d ∈ N, Ω ⊂ R d , and ρ : R → R. Suppose that D = (ϕ i ) i∈I ⊂ L 2 (Ω) is representable by neural networks. Let f ∈ L 2 (Ω), and, for M ∈ N, let f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Theorem 4.3. Let d ∈ N, Ω ⊂ R d be bounded, and let C ⊂ L 2 (Ω). Suppose that the representation system D = (ϕ i ) i∈N ⊂ L 2 (Ω) is effectively representable by neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Remark 4 . 4 .</head><label>44</label><figDesc>Theorem 4.3 implies that if D optimally represents the function class C in the sense of Definition 3.3 and at the same time is effectively representable by neural networks, then C is optimally representable by neural networks in the sense of Definition 3.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>and |ρ(x)| ≤ C(1 + |x|) k for x ∈ R. c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>[0,1] denotes the characteristic function of the interval [0, 1], and N m+1 = N m * χ [0,1] for all m ≥ 1. Multivariate B-splines are simply tensor products of univariate B-splines. Specifically, we denote, for d ∈ N, the d-dimensional cardinal B-spline of order m by N d m . Theorem 5.3 ([7], Thm. 4.2). Let d, m, k ∈ N, and take ρ to be a sigmoidal function of order k ≥ 2. Further, let L := log 2 (mdd)/ log 2 (k) + 1. Then, there is M ∈ N, possibly dependent on d, m, k, such that for all D, ε &gt; 0, there exists a neural network Φ D,ε ∈ NN L,M,d,ρ with N d m -Φ D,ε L 2 ([-D,D] d ) ≤ ε. c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>2). Since g ≥ 0 and c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php g = 0, it follows that |ĝ(0)| &gt; 0. By continuity of ĝ there exists a δ &gt; 0 such that |ĝ(ξ)| &gt; 0 for all ξ ∈ [-δ, δ] d . We now set ϕ := g 3 • δ and note that ϕ can be realized through a three-layer neural network Φ ρ ∈ NN 3,M,d,ρ for some M ∈ N. As | φ(ξ)| &gt; 0, for all ξ ∈ [-3, 3] d , Φ ρ satisfies the desired assumptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>5. 2 .</head><label>2</label><figDesc>Invariance to affine transformations. We next leverage Theorems 5.4 and 5.6 to demonstrate that a wide class of representation systems built through affine transformations of B-splines and bump functions as constructed in Theorem 5.6 is effectively representable by neural networks. As a first step towards this general result, we show that representabilityin the sense of Definition 4.1-of a single function f by neural networks is invariant to the operation of taking finite linear combinations of affine transformations of f . Proposition 5.7. Let d ∈ N, ρ : R → R, and f ∈ L 2 (R d ). Assume that there exist M, L ∈ N such that for all D, ε &gt; 0, there is Φ D,ε ∈ NN L,M,d,ρ with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Proposition 5 . 8 .</head><label>58</label><figDesc>follows that the weights of Ψ E,η are polynomially bounded in A ∞ , E, b ∞ , η -1 . This yields the claim. c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Next, we show that representability by neural networks is preserved under finite linear combinations of translates. Let d ∈ N, ρ : R → R, and f ∈ L 2 (R d ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>5.11) by neural networks. c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Definition 5.9. Let R, d ∈ N, and k ∈ {1, . . . , d}. A function g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Theorem 5 . 13 .</head><label>513</label><figDesc>Let d ∈ N, Ω ⊂ R d be bounded, ρ : R → R, and D = (ϕ i ) i∈N ⊂ L 2 (Ω) be an affine system with generator function f . Assume that there exist L, R ∈ N such that for c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php all D, ε &gt; 0, there is Φ D,ε ∈ NN L,R,d,ρ satisfying f -Φ D,ε L 2 ([-D,D] d ) ≤ ε. Then, for all function classes C ⊂ L 2 (Ω), we have γ * NN (C, ρ) ≥ γ * (C, D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Theorem 6 . 4 (</head><label>64</label><figDesc><ref type="bibr" target="#b55">[56]</ref>, Theorem 6.3 and Remark 6.4).Let α ∈ [1/2, 1], ν &gt; 0, f ∈ C 12 (R 2 ), g ∈ C 32 (R 2 ), both compactly supported and such that (i) f (ξ) = 0 for all |ξ| ≤ 1, (ii) g(ξ) = 0 for all ξ = (ξ 1 , ξ 2 ) T ∈ R 2 such that 1/3 ≤ |ξ 1 | ≤ 3 and |ξ 2 | ≤ |ξ 1 |,(iii) g has at least seven vanishing moments in the x 1 -direction, i.e., R x 1 g(x 1 , x 2 )dx 1 = 0 for all x 2 ∈ R, ∈ {0, . . . , 6}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>28 . c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Left: Topology of the neural network trained using stochastic gradient descent. The network consists of a weighted sum of four subnetworks. Right: A single subnetwork.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>)-(e) show that the corresponding trained subnetworks resemble anisotropic molecules with different scales and of different orientations. We report, without showing the results, that the decay rate of the corresponding c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) Function with a line singularity. (b) Approximation error (vertical axis) as a function of number of edges (horizontal axis). (c)-(e) The functions obtained by restricting to the subnetworks with the largest weights in modulus in the final layer.</figDesc><graphic coords="34,89.13,253.62,126.82,100.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. (a) Function with curvilinear singularity to be approximated by the neural network. (b) Approximation error (vertical axis) as a function of the number of edges (horizontal axis). (c)-(f) Shearlet-like subnetworks. (g) Reconstruction using only the 10 subnetworks whose associated functions have the largest supports. (h) Reconstruction using only subnetworks whose associated functions have medium-sized support. (i) Reconstruction using only subnetworks with associated functions of very small support.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license</figDesc><table><row><cell>are reported in section 8.</cell></row><row><cell>Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license</figDesc><table><row><cell>Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and Theorem 6.3). Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 3.2. Fundamental bound on effective best M -term approximation rate. We next recall a result from</figDesc><table /><note><p>c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>To simplify terminology in what follows, we introduce the notion of a polynomially bounded variable. c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Definition 3.9.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>If, in addition, the weights of Φ i,η ∈ NN L,R,d,ρ are polynomially bounded in i, η -1 , and if ρ is either Lipschitz-continuous or differentiable such that ρ is dominated by a polynomial, then we say that D is effectively representable by neural networks (with activation function ρ).</figDesc><table /><note><p>SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /><note><p>S. c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>13) holds. c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Pick D such that Ω ⊂ [-D, D] d . We first show that (5.14) implies representability of D by neural networks with activation function ρ.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>1} . c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Our interest in α-shearlets stems from the fact that they optimally represent α -1 -cartoonlike functions in the sense of Definition 3.3. Definition 6.2. Let β ∈ [1, 2) and ν &gt; 0. Define</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>. . , x dm , open sets V i ⊂ R m , and smooth mappings γ : R m → R, ∈ {1, . . . , d} \ {d 1 , . . . , d m }, such thatU i = {Ξ i (x d 1 , . . . , x dm ) := (γ 1 (x d 1 , . . . , x dm ), . . . , x d 1 , . . . , γ d (x d 1 , . . . , x dm )) : (x d 1 , . . . , x dm ) ∈ V i } .Take a smooth partition of unity (h i ) i∈N , whereh i : Γ → Ris smooth with supp(h i ) ⊂ U i and c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php i∈N h i = 1. Define the localization of f to U i by f</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Throughout the paper, we say that a weight of Learn(ε, f ) is represented by no more than K bits if it is taken from a set that is independent of f and has cardinality at most</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>K . c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. G. K. thanks the Department of Mathematics at Stanford University whose support allowed for completion of a portion of this work. The authors would like to thank J. Bruna, E. Candès, M. Genzel, S. Güntürk, Y. LeCun, K.-R. Müller, H. Rauhut, and F. Voigtländer for interesting discussions, and D. Perekrestenko and R. Gül for very detailed and insightful comments on the manuscript. G. K. and P. P. are grateful to the Faculty of Mathematics at the University of Vienna for the hospitality and support during their visits. c 2019 SIAM. Published by SIAM under the terms of the Creative Commons 4.0 license Downloaded 02/12/19 to 46.148.124.159. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The third author was supported by Stanford University, the Einstein Foundation Berlin, the Einstein Center for Mathematics Berlin (ECMath), the European Commission-Project DEDALE (contract 665044) within the H2020 Framework Program, DFG grant KU 1446/18, DFG-SPP 1798 grants KU 1446/21 and KU 1446/23, and the DFG Research Center Matheon "Mathematics for Key Technologies." The third and fourth authors acknowledge support by the DFG Collaborative Research Center TRR 109 "Discretization in Geometry and Dynamics.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universal approximation bounds for superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="930" to="945" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approximation and estimation bounds for artificial neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Analysis of the Generalization Error: Empirical Risk Minimization over Deep Artificial Neural Networks Overcomes the Curse of Dimensionality in the Numerical Approximation of Black-Scholes Partial Differential Equations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jentzen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1809.03062" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<title level="m">Ridgelets: Theory and Applications</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ridgelets and the representation of mutilated Sobolev functions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<idno type="DOI">10.1137/S003614109936364X</idno>
		<ptr target="https://doi.org/10.1137/S003614109936364X" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Math. Anal</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="347" to="368" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">New tight frames of curvelets and optimal representations of objects with piecewise C 2 singularities</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="219" to="266" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural networks for localized approximation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Mhaskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comp</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="607" to="623" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tree approximation and optimal encoding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dahmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="192" to="226" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the expressive power of deep learning: A tensor analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Learning Theory</title>
		<meeting>the 29th Conference on Learning Theory<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="698" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional rectifier networks as generalized tensor decompositions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="955" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the mathematical foundations of learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Control Signals Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611970104</idno>
		<ptr target="https://doi.org/10.1137/1.9781611970104" />
		<title level="m">Ten Lectures on Wavelets, SIAM, Philadelphia</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wave atoms and sparsity of oscillatory patterns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Demanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="368" to="387" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numer</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="51" to="150" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lorentz</surname></persName>
		</author>
		<title level="m">Constructive Approximation</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Approximation by feed-forward neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oskolkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petrushev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Numer. Math</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="261" to="287" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unconditional bases are optimal bases for data compression and for statistical estimation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="100" to="115" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse components of images and optimal atomic decompositions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constr. Approx</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="353" to="382" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The power of depth for feedforward neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Learning Theory</title>
		<meeting>the 29th Conference on Learning Theory<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="907" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aspects of the numerical analysis of neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ellacott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numer</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="145" to="202" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the approximate realization of continuous mappings by neural networks</title>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Funahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Gröchenig</surname></persName>
		</author>
		<idno>02/12/19 to 46.148.124.159</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
	</analytic>
	<monogr>
		<title level="m">Redistribution subject to SIAM license or copyright; see</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013. 2019</date>
		</imprint>
	</monogr>
	<note>Foundations of Time-Frequency Analysis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimally sparse data representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grohs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Harmonic and Applied Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="199" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Grohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keiper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">α-molecules</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="297" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cartoon approximation with α-curvelets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keiper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fourier Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1235" to="1293" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parabolic molecules</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="299" to="337" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep neural network approximation theory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Perekrestenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Elbrächter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bölcskei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint/>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse multidimensional representations using anisotropic dilation and shear operators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Labate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets and Splines</title>
		<meeting><address><addrLine>Athens, GA; Nashville, TN</addrLine></address></meeting>
		<imprint>
			<publisher>Nashboro Press</publisher>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="page" from="189" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Compactly supported shearlets are optimally sparse</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Q</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Approx. Theory</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="1564" to="1589" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Modèles connexionnistes de l&apos;apprentissage</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>These de Doctorat, Université Paris 6</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lower bounds for approximation by MLP neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Maiorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="81" to="91" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A logical calculus of ideas immanent in nervous activity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Math. Biophys</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Degree of approximation by neural and translation networks with a single hidden layer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Micchelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="151" to="183" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Approximation properties of a multilayered feedforward artificial neural network</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Mhaskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural networks for optimal approximation of smooth and analytic functions</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Mhaskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="164" to="177" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Approximation by superposition of sigmoidal and radial basis functions</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Micchelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="350" to="373" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep vs. shallow networks: An approximation theory perspective</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal. Appl. (Singap.)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="829" to="848" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Approximation of functions and their derivatives: A neural network implementation with applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen-Thien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran-Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Model</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="687" to="704" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Ott</surname></persName>
		</author>
		<title level="m">Chaos in Dynamical Systems</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Equivalence of Approximation by Convolutional Neural Networks and Fully-Connected Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Voigtlaender</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1809.00973" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Optimal approximation of piecewise smooth functions using deep ReLU neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Voigtlaender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="296" to="330" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Approximation theory of the MLP model in neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143" to="195" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Provable approximation properties for deep neural networks</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cloninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="537" to="557" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christmann</surname></persName>
		</author>
		<title level="m">Support Vector Machines, Information Science and Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Analysis vs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pein</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1702.03559" />
	</analytic>
	<monogr>
		<title level="m">Synthesis Sparsity for α-Shearlets</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Universal Approximations of Invariant Maps by Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yarotsky</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1804" />
		<imprint>
			<date type="published" when="2018">10306v1, 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Error bounds for approximations with deep ReLU networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yarotsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="103" to="114" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D.-X</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.10769" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Universality of Deep Convolutional Neural Networks</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
