<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Model-Driven Methodology for Big Data Analytics-as-a-Service</title>
				<funder ref="#_ET9jWVH">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Claudio</forename><forename type="middle">A</forename><surname>Ardagna</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Valerio</forename><surname>Bellandi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paolo</forename><surname>Ceravolo</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ernesto</forename><surname>Damiani</surname></persName>
							<email>ernesto.damiani@kustar.ac.ae</email>
						</author>
						<author>
							<persName><forename type="first">Michele</forename><surname>Bezzi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cedric</forename><surname>Hebert</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department Universit? degli Studi di Milano Crema</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CINI -Consorzio Interuniversitario Nazionale per l&apos;Informatica Rome</orgName>
								<orgName type="institution">EBTIC</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Khalifa University</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Security Research SAP Labs</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Model-Driven Methodology for Big Data Analytics-as-a-Service</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/BigDataCongress.2017.23</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Big Data revolution has promised to build a data-driven ecosystem where better decisions are supported by enhanced analytics and data management. However, critical issues still need to be solved in the road that leads to commodization of Big Data Analytics, such as the management of Big Data complexity and the protection of data security and privacy. In this paper, we focus on the first issue and propose a methodology based on Model Driven Engineering (MDE) that aims to substantially lower the amount of competences needed in the management of a Big Data pipeline and to support automation of Big Data analytics. The proposal is experimentally evaluated in a real-world scenario: the implementation of novel functionality for Threat Detection Systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Big Data management and analytics are among the most critical pressing needs for all organisations that want to move their business a step forward. Big Data market is expected to substantially grow in the next years <ref type="bibr" target="#b0">[1]</ref>, and its technologies are introducing a Copernican revolution in the areas of data storage, processing, and analytics. However, the impact and diffusion of Big Data technologies are lowered, on one side, by the complex, and not standardised, technologies, and, on the other side, by the lack of professional profiles with the necessary background and competence, especially in SMEs.</p><p>A recent trend has underlined the relevance of users' requirements and developed the idea that achieving the full potential of Big Data analytics needs to embrace a data model approach <ref type="bibr" target="#b14">[15]</ref>. Traditional data modeling, which focused on resolving the complexity of relationships among schema-enabled data <ref type="bibr" target="#b13">[14]</ref>, has been discarded as no longer applicable to Big Data scenarios. In fact, in addition to data representation, Big Data models should provide a shared specification of the process to manage data resources (including anonymization and privacy-preservation procedures) and of the computations to be done over them. They also need to provide all the information to carry out Big Data analytics over commodity execution platforms.</p><p>A practical goal for next-generation Big Data is to provide solutions where end-users define their expectations on goals to be achieved with Big Data analytics, while smarter engines manage and compose solutions to deploy Big Data architectures and carry out the expected analytics.</p><p>In this paper, we propose a framework for Model-based Big Data Analytics-as-a-Service (MBDAaaS) <ref type="bibr" target="#b1">[2]</ref>, which supports customers lacking Big Data expertise in managing big data analytics and deploying a full big data pipeline that addresses their goals. The proposed approach is based on a declarative model (Section IV-B), specifying the goals of a given analytics in the form of pairs indicators/objectives. The customers navigate and prioritize indicators through an interface, which can guide her in implementing consistent selections by choosing the objective values of low-priority indicators to maximize the objectives of highpriority indicators. Indicators and objectives are then used to incrementally refine a platform-independent procedural model (Section IV-C) specifying how analytics should be carried out in terms of an abstract OWL-S workflow. Procedural models are finally compiled in a ready-to-be-executed deployment model (Section IV-D), specifying Big Data platform-dependent configurations and supporting automatic provisioning of computational components and resources. Such transformations are implemented in a methodology providing a semi-automatic process to MBDAaaS (Section V), which is practically evaluated (Section VI) in a real-world scenario (Section III).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>MBDAaaS <ref type="bibr" target="#b1">[2]</ref> is interconnecting two trends that are currently shaping the Big Data ecosystem. On one side, Big Data technologies perform the best on-demand and scalable computing infrastructures. On the other side, the high complexity and side-costs of designing, developing and deploying such infrastructures suggest the adoption of model-driven approaches that foster modularity, reusability, and automatisation of design and implementation tasks.</p><p>The first aspect is largely discussed in the literature <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Recent research has focused on incorporating Cloud computing negotiation models based on Service Level Agreement (SLA) <ref type="bibr" target="#b20">[21]</ref>, opening the door to selfconfiguration <ref type="bibr" target="#b6">[7]</ref>, and cost models for evaluating alternative Big Data-as-a-Service solutions <ref type="bibr" target="#b17">[18]</ref>.</p><p>The second aspect, was intended by several authors in connection to software engineering methodologies operating at design time <ref type="bibr" target="#b10">[11]</ref>. However, other authors have investigated methods that operate at execution time. For example, workflow orchestration frameworks were placed on top of distributed data processing applications <ref type="bibr" target="#b7">[8]</ref>, model-driven approaches were adopted to configure runtime observability frameworks <ref type="bibr" target="#b9">[10]</ref>, to adapt the data storage model for improving I/O performances <ref type="bibr" target="#b11">[12]</ref>, or to design visual analytics <ref type="bibr" target="#b3">[4]</ref>. In such a context, where multiple parties are involved in the provisioning, processing, and sharing of heterogeneous data, the conciliation of non-functional requirements related to data protection and information privacy is also a demanding challenge. A compete survey on the different strategies implemented for data protection is available in <ref type="bibr" target="#b19">[20]</ref>. For example, the processing procedures of data mining algorithms can be masked, using generalization, suppression, permutation or perturbation <ref type="bibr" target="#b12">[13]</ref>, or adopting differential privacy <ref type="bibr" target="#b4">[5]</ref>, giving to the data provider the guarantee that the data collector will not be able to consistently read data.</p><p>Although all these aspects were considered in the literature, there is a lack of a comprehensive approach addressing the whole lifecycle of MBDAaaS, from the specification of declarative goals to the deployment of a big data analytics, via the definition of a procedural model representing how the expected analytics should work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. REFERENCE SCENARIO: THREAT DETECTION SYSTEMS</head><p>Threat detection and prevention in software ecosystems <ref type="bibr" target="#b15">[16]</ref> have recently been extended to the application layer. Threat Detection System (TDS) <ref type="foot" target="#foot_0">1</ref> detects potential attacks on the application landscape by gathering and analyzing log data, such as user change logs, security audit logs, remote function call gateway logs, and transaction logs. Logs are usually pre-processed, anonymized, translated into a common format, and analyzed by pattern or anomaly detection algorithms, which can highlight suspicious events. Finally, a detailed investigation is performed by a human expert to decide if a real attack is detected or it is a false positive. However, with the increasing size and complexity of software systems, the volume and diversity of log data are becoming major issues. Customers have in place a large spectrum of different systems and a wide range of data security policies. As a result, including and managing these heterogeneous log files currently need a significant customization effort, especially when they contain sensitive and personal information (e.g., user IDs, IP addresses) coming from logs of multiple customers or are accessed by the third party (e.g., cloud provider) running TDS. Similarly, customers often need different security analysis, depending on the security context, industrial sector, risk management policies, and the analytics functionality need to be often customized, too. Accordingly, major challenges for data analysis by TDS systems are related to devising a flexible framework supporting:</p><p>? the provisioning of customized analytics and reporting approach for stakeholders; ? data anonymization (at different levels, for different customers); ? a scaling approach for variable data loads; ? a limited effort (i.e., semi-automatic) for the integration of new and diverse log files, which also adapts corresponding analytics; In this paper, we present how MBDAaaS approach can support the design of a TDS system, or its extension to include novel functionality, and we illustrate the results of a realization of this approach (Section VI). For sake of simplicity, we will focus on a simple, but relevant, analytics scenario for TDS: advanced anomaly detection analytics, able to limit the number of false positives in alerts. In fact, since TDS are set to minimize the likelihood that a threat is not detected, they often suffer of a large number of false positives, which results in additional investigation effort for security experts. For example, a typical approach for anomaly detection is using deviations from the normalized mean computed from previous events, using metric as zscore (see <ref type="bibr" target="#b2">[3]</ref>) or fixed thresholds. It can be done focusing on a single type of event (e.g., the amount of data sent by a user/system in one day) or a combination of event types (e.g., data sent and received). However, the high variability in human and system activities can lead to a large number of outliers, and more sophisticated approaches are needed, such as automatically detecting specific activities (via clustering) and considering outliers from the clusters as anomaly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODELS FOR BIG DATA ANALYTICS-AS-A-SERVICE</head><p>The methodology proposed in this paper, building on model-driven engineering paradigm <ref type="bibr" target="#b16">[17]</ref>, aims to provide an approach that decouples high-level goals of a Big Data campaign from low-level details of the Big Data architecture. Our methodology defines three models as described in the following of this section: i) a declarative model representing the computation-independent model, ii) a procedural model representing the platform-independent model, and iii) a deployment model representing the platform-dependent model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structuring the Big Data Pipeline</head><p>The systematisation proposed by several authors <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref> describes a Big Data process along different conceptual areas that structure the deployment in interdependent pipelines. A complete process can be split in 5 main areas driving the whole MBDAaaS.</p><p>? Data preparation area specifies all activities aimed to prepare data for analytics. For instance, it defines how to guarantee data owner privacy using anonymization (e.g., hashing, obfuscation), and identifies data cleaning and integration techniques. ? Data representation area specifies how data are represented and expresses representation choices for each analysis process. For instance, it defines the data model (e.g., document-oriented, graph-based, relational) and the data structure (e.g., structured, semi-structured, unstructured). ? Data analytics area specifies the analytics to be computed. For instance, it defines the expected outcome (e.g., descriptive, prescriptive, predictive, diagnostic), the type of analytics (e.g., cluster, classifier, predictor), and the learning approach (e.g., supervised, unsupervised, semi-supervised). ? Data processing area specifies how data are routed and parallelized. For instance, it defines the processing type (e.g., real-time, near real-time, batch) and the elasticity level (e.g. full, bounded, none). ? Data visualisation and reporting area specifies an abstract representation of how the results of analytics are organised for display and reporting. For instance, it defines data display type (e.g., composition, order).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Declarative Models</head><p>Declarative models are platform-and vendor-independent models specifying user goals related to the conceptual areas in Section IV-A. They describe customers' goals expressing the properties a Big Data Campaign (BDC) has to fulfil. On the other side, goals express commitments on service properties made by ICT providers to their customers. A goal G (e.g., data source properties) is measured by an indicator I, which is a label expressing a way to assess the goal (e.g., consistency), and an objective O, which is a threshold on certain scale, either ordinal or metric (e.g., weak, normal, or strong), for I. A declarative model is defined as a set of prioritised goals annotated with metadata specifying constraints on procedural and deployment models, as formally presented in the following definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition IV.1 (Declarative Model). A declarative model d i ?D consists of five elements (a i , G), one for each conceptual area a i ?A, where G specifies a set of prioritised goals</head><formula xml:id="formula_0">G i ={(I i , O i ), C i , pr i }, with I i a label representing an indi- cator of G i , O i the value (objective) of I i , C i ={c 1 ,. . .,c n }</formula><p>a set of constraints on I i driving the configuration of procedural and deployment models, and pr i the priority of the goal G i .</p><p>We note that, to support users with different big data competences, each constraint c j ?C i can be either defined by the user or enforced by the selected technological platform. A constraint is defined as a boolean formula of expressions of the form op(attr,value), where op is an operator in {=, =,&lt;,&gt;,?,?,?}, attr represents an attribute referring to a procedural/deployment model, and value a (set of) value for the given attribute. For instance, a goal G i =(Anonimization_Technique, k-anonymity) on property anonymity may set a constraint c on the cardinality of k. This constraint will be considered when configuring the procedural model or when implementing quality assurance controls for the BDC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Procedural Models</head><p>Procedural models are platform-independent models that formally and unambiguously describe how analytics should be configured and executed. Procedural models are generated following goals and constraints specified in the declarative models. They provide a workflow in the form of a service orchestration that composes, in an arbitrary way, services falling in the areas presented in Section IV-A, as formally defined below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition IV.2 (Procedural Model).</head><p>A procedural model m is a direct acyclic graph G(V,E,?), where a vertex v i ?V refers to a service (e.g., an algorithm, mechanism, or component) in a specific area area(v i ), an edge (v i ,v j )?E is annotated with function call f i to the service represented by v j , and ?:V?SC is a labeling function that associates a set {cp 1 ,. . .,cp n }?SC of configuration parameters with each v i ?V.</p><p>We note that a configuration parameter cp i either derives from constraints c in Definition IV.1 or is given as input by the customers during procedural model definition. For instance, cp i can specify the cardinality k of a data anonymization based on k-anonymity. We also note that each function call annotating an edge in G triggers a state transition and corresponding mechanism execution. For instance, a common procedural model consists of a threestep, sequential workflow that first prepares data (area data preparation), then runs the analytics (area data analytics), and finally presents the results (area display and reporting). Services are selected and orchestrated within the procedural workflow according to the goals stated in the declarative model. For instance, in case goal G =(Analytics_Task, crisp_clustering) is defined, only services implementing a crisp clustering procedure (e.g., k-means) will be available for area data analytics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Deployment Models</head><p>Deployment models specify how procedural models are instantiated and configured on a target platform (platformdependent models), and drive analytics execution in real scenarios. A deployment model G is an instance of a procedural model G, defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition IV.3 (Deployment Model m d ). Let m=G(V,E,?) be a procedural model. A deployment model m d is a direct acyclic graph G (V ,E ,?) where:</head><p>? each vertex v i ?V contains the implementation of the corresponding platform-independent service v i ?V, ? each edge (v i ,v j )?E , corresponding to edge (v i ,v j )?E, is annotated with the endpoint f i referring to the platform-dependent implementation of the service represented by v j , and ? ?:V ?SC associates a set {cp 1 ,. . .,cp n }?SC of platform-dependent configurations with each v i ?V .</p><p>We note that a there exists an isomorphism between procedural model m and deployment model m d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MODEL-DRIVEN BIG DATA ANALYTICS</head><p>MBDAaaS implements the methodology in Section V-A and is responsible for all activities aimed to configure and execute the Big Data analytics involving: i) Big Data customer specifying the goals of its BDC, ii) Big Data consultant helping the Big Data customer in specifying all customizations needed to execute her analytics, iii) MB-DAaaS platform that is responsible for semi-automatically managing and executing a BDC on a Big Data platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MBDAaaS Methodology</head><p>MBDAaaS methodology is based on different building blocks which are described in the following.</p><p>Declarative Model. It allows customers to define a set of goals shaping a BDC and retrieve a set of services compatible with these goals.</p><p>OWL-S Ontology. It specifies the set of abstract services that are available to Big Data customers and consultants for building their BDC. For each service, it defines the interface, a link to the goals in the declarative model, and some constraints driving the definition of a procedural model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OWL-S Workflow.</head><p>It is the procedural model providing all artifacts for specifying an abstract Big Data workflow. It defines how relevant OWL-S services can be composed to carry out the Big Data analytics. It supports traditional composition patterns, such as sequence, alternative, parallel.</p><p>Platform-Dependent Workflow. It is the platformdependent version (deployment model) of an abstract OWL-S workflow, which is ready to be executed on the target Big Data platform. It is provided by a MBDAaaS Compiler that takes as input an abstract OWL-S workflow and produces as output he corresponding platform-dependent workflow. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. From Declarative to Procedural Models</head><p>The methodology in Figure <ref type="figure">1</ref> first transforms a declarative model in a procedural model as described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Declarative Model Definition:</head><p>The process starts with the definition of the declarative model (step 1 in Figure <ref type="figure">1</ref>), which is presented in this paper using a JSON format. Figure <ref type="figure" target="#fig_2">2</ref> propose an excerpt of a declarative model for areas data preparation, analytics and processing, visualization. "preparation": { "Govern and stewards": { "Anonymization Model": "Privacy preserving data publishing", "Anonymization Technique": "Hashing", "Constraints": { "Algorithm": "SHA-256", "Target": "UserID" }} }, "analytics": { "Analytics Aim": { "Models": "Diagnostic", "Task": "Clustering", "Constraints": {"Algorithm": "Crisp Clustering "}, }, "Analytics Quality": { "Positive Predictive Value (Precision)": "Medium", "True Positive Rate (Recall, Sensitivity)": "Medium", "True Negative Rate (Specificity)": "Medium", "False Positive Rate (Fall-out)": "Medium",} }, "processing": { "Mode": { "Analysis Goal": "Near Real-time", "Rounds": "Multiple", "Locality": "Distributed"},}, "display": { "Data display": { "Processing Type": "Relationship", "Dimensionality": "nD", "Data Density": "Low", "Data Type": "Ratio" }, "Operations": { "Interaction": "Filter", "User": "Techie", "Goal": "Cluster/Categorize"} }, [...] } real time (e.g., micro-batches), with an expected quality measured through true positive rate.</p><p>? Area Visualization and Reporting. It specifies goals Data display and Operations, which drive the selection of the visualization approach. This selection considers also specifications in other areas, in particular the need to visualize the results of a clustering analytics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) OWL-S Service Selection:</head><p>The process retrieves those services available in the platform that are compatible with the goals and constraints specified in the declarative model (step 2 in Figure <ref type="figure">1</ref>), using an extended version of OWL-S. The OWL-S ontology is structured in three interrelated subontologies, known as the profile, process model, and grounding. The profile ontology is used to express what "does" the service; the process model describes "how it works"; and the grounding maps the constructs of the process model onto detailed specifications of message formats, protocols, and so forth. In addition to specifying the service, in our methodology, OWL-S is extended to describe a mapping with the goals of the declarative model, which are used for service selection. Figure <ref type="figure" target="#fig_3">3</ref> shows an example of the OWL-S modeling of SHA-256 hashing service of Spark that is mapped to declarative goals (Anonymization Model, &lt;owl:NamedIndividual rdf:about="tdm:spark:hashing"&gt; &lt;rdf:type rdf:resource="tdm:MaterialService"/&gt; &lt;realizeSpecification rdf:resource="tdm:data_preparation:anonymization"/&gt; &lt;/owl:NamedIndividual&gt; &lt;owl:NamedIndividual rdf:about="tdm:data_preparation:anonymization"&gt; &lt;rdf:type rdf:resource="tdm:Anonymization"/&gt; &lt;hasArea rdf:resource="tdm:dataPreparation"/&gt; &lt;hasConstraint rdf:resource="tdm:OWLNamedIndividual_00000"/&gt; &lt;hasIndicator rdf:resource="tdm:data_preparation:anonymization:technique"/&gt; &lt;hasObjective rdf:resource="tdm:data_preparation:anonymization:technique:hashing"/&gt; &lt;/owl:NamedIndividual&gt; &lt;owl:NamedIndividual rdf:about="tdm:OWLNamedIndividual_00000"&gt; &lt;rdf:type rdf:resource="tdm:Constraint"/&gt; &lt;hasURIValue rdf:datatype="http://www.w3.org/2001/XMLSchema#anyURI"&gt; http://toreador-project.eu/example/dataset.csv &lt;/hasURIValue&gt; &lt;onParameter rdf:resource="tdm:normalizedDataset"/&gt; &lt;/owl:NamedIndividual&gt; privacy preserving data publishing) and (Anonymization Technique, hashing) with constraint Algorithm: SHA-256. On the basis of this annotation, SHA-256 hashing service is selected as a service compatible with the specified declarative model. Figure <ref type="figure">4</ref> presents the process model of the hashing anonymization service where a dataset is taken as input and the anonymization algorithm is applied to the "field" parameter. The "AnonymizedDataset" is then returned as output. Figure <ref type="figure" target="#fig_4">5</ref> presents an example of grounding, where the function of the anonymization process is specified. In our example, a reference to the WSDL file of the service ("'HashingAnonymizationService.wsdl"') is used as a placeholder that points to the functionality/library provided by the target Big Data platform and implementing the service itself.</p><p>Following the above approach, a k-means service is selected for area analytics, a map-reduce parallel processing for area processing, and two visualization techniques, namely, a dimension-reduced plane visualization based on scatter-plot and an independent coordinates visualization based on scatter-plot matrices, for area visualization.</p><p>3) OWL-S Workflow Definition: The last step of this process consists in the definition of an OWL-S workflow, representing the procedural model in this paper (step 3 in Figure <ref type="figure">1</ref>). Here, we present a basic workflow example that composes the Anonymization Service and the Clustering Service (Figure <ref type="figure" target="#fig_5">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. From Procedural to Deployment Models</head><p>The methodology in Figure <ref type="figure">1</ref> then transforms the procedural model returned in Section V-B in a deployment  model that can be directly executed on the target platform. This transformation is based on a compiler (step 4 in Figure <ref type="figure">1</ref>) that takes as input the OWL-S workflow returned in step 3 (Figure <ref type="figure" target="#fig_5">6</ref>) and information on the target platform (e.g., installed services/algorithms), and produces as output a technology-dependent workflow. This is achieved using a stylesheet-based transformation. We note that, in case no workflow engine is available on the target platform, the compiler returns the specific calls to the services in the platform matching activities in the OWL-S workflow.   The workflow in Figure <ref type="figure" target="#fig_6">7</ref> is composed of three activities in a sequence that execute i) SHA-256 anonymization, ii) k-means analytics, iii) scatter-plot visualization. There is however a subtlety to consider: the workflow in Figure <ref type="figure" target="#fig_6">7</ref> is not completely specified. In fact, the scripts in the workflow refers to templates that need to be instantiated with parameters in the OWL-S workflow. <ref type="foot" target="#foot_1">2</ref> For instance, let us consider the reference to the k-means script /lib/spark-kmeans.jar in Figure <ref type="figure" target="#fig_6">7</ref>. The corresponding script is completely specified unless the variables associated with the parameters number of clusters ($p1) and numbers of iterations ($p2) in the oozie workflow. When $p1 and $p2 are filled with real values, the workflow is completely specified and ready to be executed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. WORKFLOW EXECUTION: TDS SCENARIO</head><p>The methodology in Figure <ref type="figure">1</ref> finally executes the deployment model returned in step 4 on the target big data platform. In the following, we show the realization of the anomaly detection TDS use case (Section III).</p><p>We focused on devising an anomaly detection functionality for TDS, with a smaller false positive rate compared to traditional systems based on z-score; in particular, we wanted to detect possible anomalies in the amount of data transmitted by users within a software landscape, which may indicate malicious behavior. We used a log data set containing around 12 weeks of activity collected from SAP systems deployed in a test environment. We considered three features, extracted from the log files: Data Sent, Data Received, Data Exchanged (all of them in bytes), aggregated on a duration of 6 hours and per user. For testing purpose, we artificially generated a data set with multi-variate Gaussian distribution having the same average and covariance matrix as the original data points. returned 6 clusters, representing similar events, with silhouette values around 0.5. Red points indicate anomalous data points (outliers) where the distance to the centroid is greater than 99 th percentile of training data; colored data points represent the 6 different clusters; blue lines indicate the value of thresholds computed with the z-score method (using zvalues optimized for each dimension). We can observe that the number of identified outliers is reduced compared to zscore method (see the number of red dots versus the points above the thresholds). Our experiments show the suitability of a MBDAaaS methodology, where Big Data analytics automated with a little involvement of customers that only define goals in a declarative model and are possibly queried for additional parameters of the analytics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>This paper proposed a model for managing a MBDAaaS platform and demonstrated its feasibility in addressing a TDS scenario. Evolving from this point we plan to enrich the set of services supported in our platform to offer an environment, insisting on vertical scenarios, for testing reallife BDC. Users will be requested to deal with the different design stages typically addressed in preparing a BDC, to then compare alternative options and investigating the consequences of their choices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 Figure 1 .</head><label>11</label><figDesc>Figure 1 presents the process of our MBDAaaS that is composed of five main steps as follows. In the first step (Declarative Model Definition), the Big Data customer produces a declarative model d specifying the goals of a Big Data campaign (see Definition IV.1). In the second step</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc>Area preparation. The Govern and stewards goal requires the dataset produced at the previous step to undergo an anonymization process driven by goals anonymization model and anonymization techniques. It requires the adoption of a hashing techniques with constraints on the type of algorithm (i.e., SHA-256) and the target of anonymization (i.e., UserID). ? Areas Analytics and Processing. For simplicity, we consider the subset of the goals that drives the transformation from declarative to procedural model: (Task, crisp_ clustering), (Analysis Goal, near_real_time), (True Positive Rate, medium). This portion of the declarative model requires the adoption of a clustering algorithm to be applied in near { [...]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A fragment of the declarative model in JSON.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. OWL-S based Hashing Anonymization service profile.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. OWL-S based Hashing Anonymization service grounding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Example of an OWL-S workflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7</head><label>7</label><figDesc>Figure 7 shows an example of technology-dependent workflow corresponding to the OWL-S workflow in Figure 6 and compatible with the Oozie workflow engine.The workflow in Figure7is composed of three activities in a sequence that execute i) SHA-256 anonymization, ii) k-means analytics, iii) scatter-plot visualization. There is however a subtlety to consider: the workflow in Figure7is not completely specified. In fact, the scripts in the workflow refers to templates that need to be instantiated with parameters in the OWL-S workflow.2 For instance, let us consider the reference to the k-means script /lib/spark-kmeans.jar in Figure7. The corresponding script is completely specified unless the variables associated with the parameters number of clusters ($p1) and numbers of iterations ($p2) in the oozie workflow. When $p1 and $p2 are filled with real values, the workflow is completely specified and ready to be executed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-18.png" coords="7,312.66,71.13,247.57,206.53" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We refer to these systems as TDS, to distinguish them from network level intrusion detection systems (called IDS or SIEM)<ref type="bibr" target="#b8">[9]</ref>. In addition, we base our description on the SAP Enterprise Threat Detection, but the analysis could be applied to other solutions, including IDS.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In case some parameters are missing, they are asked to the users.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>This project has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="projectName">TOREADOR</rs> project, grant agreement No <rs type="grantNumber">688797</rs>. We thank <rs type="person">Adnene Tekaya</rs> and <rs type="person">Mohammad Ashiqur Rahaman</rs> for providing the results shown in Fig. 8.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ET9jWVH">
					<idno type="grant-number">688797</idno>
					<orgName type="project" subtype="full">TOREADOR</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>&lt;workflow-app xmlns = "uri:oozie:workflow:0.4" name = "CompositeProcess00001"&gt; [...] &lt;!Step preparation --&gt; &lt;action name="Preparation:Anonymization"&gt; &lt;spark xmlns="uri:oozie:spark-action:0.   HANA XS Engine, APL predictive libraries, and, in addition, we use Jupyter for visualization, but with no workflow engine on board. The process execution on platform 1 simply required to execute the Oozie workflow in Figure <ref type="figure">7</ref>, while the one on platform 2 required to manually generate and execute all service calls. Automatic generation of service calls will be the topic of our future work.</p><p>The process orchestrated the three steps in Figure <ref type="figure">7</ref>: 1) Data preparation: log dataset was processed to address data minimization privacy requirement removing the unnecessary personal information. Specifically, the UserID values were pseudo-anonymized by a hashing function. 2) Data processing and analytics, a k-means algorithm was applied to our data points. For simplicity, here, we focused on the key elements only. In the actual implementation, we also considered additional data processing steps, such as a logarithmic transformation of the data, to reduce skewness, scaling (dividing for the standard deviation and subtracting the mean) and optimization of the k value using the elbow method. 3) Data visualization, data were visualized as scatterplot, supporting visual inspection of data by expert users, to select valid alerts. For space restriction, in Figure <ref type="figure">8</ref>, we only present the results retrieved through our MBDAaaS using platform 2. Data processing was realized by the APL HANA library, ran on top of the HANA XS engine; data visualization was done using the Jupyter (notebook) environment. Our experiments</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Nasscom Crisil GR&amp;A Analysis. Big data -the next big thing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Big data analytics as-a-service: Issues and challenges</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ardagna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ceravolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Damiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PSBD 2016</title>
		<meeting>of PSBD 2016<address><addrLine>Washington, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">December 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM CSUR</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model-driven visual analytics for big data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NYSDS 2016</title>
		<meeting>of NYSDS 2016<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">August 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Differential privacy and robust statistics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of STOC 2009</title>
		<meeting>of STOC 2009<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The rise of big data on cloud computing: Review and open research issues</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hashem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yaqoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Anuar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mokhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="98" to="115" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Starfish: A self-tuning system for big data analytics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Herodotou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cidr</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mesos: A platform for fine-grained resource sharing in the data center</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NSDI</title>
		<meeting>of NSDI<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-03">2011. March 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Kaempfer</surname></persName>
		</author>
		<ptr target="http://tinyurl.com/mhl3lay" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-driven observability for big data storage</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gorton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alhmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gemici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saravagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WICSA 2016</title>
		<meeting>of WICSA 2016<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-04">April 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Software engineering for big data projects: Domains, methodologies and gaps</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alencar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Big Data</title>
		<meeting>of Big Data<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">2016. December 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeldriven data layout selection for improving read performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Byna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IPDPSW</title>
		<meeting>of IPDPSW<address><addrLine>Phoenix, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05">2014. May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spoc: A secure and privacypreserving opportunistic computing framework for mobilehealthcare emergency</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPDS</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="614" to="624" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From databases to big data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4" to="6" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Breaking the chains: On declarative data analysis and data independence in the big data era</title>
		<author>
			<persName><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VLDB Endowment</title>
		<meeting>of VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014-08">August 2014</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1730" to="1733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detection of early-stage enterprise infection by mining large-scale log data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alrwais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DNS</title>
		<meeting>of DNS<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">2015. June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Guest editor&apos;s introduction: Model-driven engineering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="25" to="31" />
			<date type="published" when="2006-02">February 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards the evaluation of a big data-as-a-service model: a decision theoretic approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Skourletopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mavromoustakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatzimisios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mastorakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batalla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INFOCOMW 2016</title>
		<meeting>of INFOCOMW 2016<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-04">April 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on security and privacy issues in big data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagiroglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICITST 2015</title>
		<meeting>of ICITST 2015<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Information security in big data: privacy and data mining</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1149" to="1176" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sla-based profit optimization for resource management of big data analytics-as-a-service platforms in cloud computing environments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Calheiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><surname>Sinnott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Big Data</title>
		<meeting>of Big Data<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">2016. 2016. December</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Service-generated big data and big data-as-a-service: an overview</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of BigData Congress</title>
		<meeting>of BigData Congress<address><addrLine>Santa Clara, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">2013. June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
