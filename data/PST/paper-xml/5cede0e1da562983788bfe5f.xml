<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2019 ADVERSARIAL EXAMPLES ARE A NATURAL CONSE-QUENCE OF TEST ERROR IN NOISE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2019 ADVERSARIAL EXAMPLES ARE A NATURAL CONSE-QUENCE OF TEST ERROR IN NOISE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the last few years, the phenomenon of adversarial examples -maliciously constructed inputs that fool trained machine learning models -has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>State-of-the-art computer vision models can achieve superhuman performance on many image classification tasks. Despite this, these same models still lack the robustness of the human visual system to various forms of image corruptions. For example, they are distinctly subhuman when classifying images distorted with additive Gaussian noise <ref type="bibr" target="#b12">(Dodge &amp; Karam, 2017b)</ref>, they lack robustness to different types of blur, pixelation, and changes in brightness <ref type="bibr" target="#b21">(Hendrycks &amp; Dietterich, 2018)</ref>, lack robustness to random translations of the input <ref type="bibr" target="#b2">(Azulay &amp; Weiss, 2018)</ref>, and even make errors when foreign objects are inserted into the field of view <ref type="bibr" target="#b29">(Rosenfeld et al., 2018)</ref>. At the same time, they also are sensitive to small, worst-case perturbations of the input, so-called "adversarial examples" <ref type="bibr" target="#b32">(Szegedy et al., 2014)</ref>. This latter phenomenon has struck many in the machine learning community as surprising and has attracted a great deal of research interest, while the former seems to inspire less surprise and has received considerably less attention.</p><p>Our classification models make errors on two different sorts of inputs: those found by randomly sampling from some predetermined distribution, and those found by an adversary deliberately searching for the closest error to a given point. In this work, we ask what, if anything, is the difference between these two types of error. Given that our classifiers make errors in these corrupted image distributions, there must be a closest such error; do we find that this closest error appears at the distance we would expect from the model's performance in noise, or is it in fact "surprisingly" close?</p><p>The answer to this question has strong implications for the way we approach the task of eliminating these two types of errors. An assumption underlying most of the work on adversarial examples is that solving it requires a different set of methods than the ones being developed to improve model generalization. The adversarial defense literature focuses primarily on improving robustness to small perturbations of the input and rarely reports improved generalization in any distribution.</p><p>We claim that, on the contrary, adversarial examples are found at the same distance scales that one should expect given the performance on noise that we see in practice. We explore the connection between small perturbation adversarial examples and test error in noise in two different ways.</p><p>First, in Sections 4 and 5, we provide empirical evidence of a close relationship between test performance in Gaussian noise and adversarial perturbations. We show that the errors we find close to the clean image and the errors we sample under Gaussian noise are part of the same large set and show some visualizations that illustrate this relationship. (This analysis builds upon prior work <ref type="bibr" target="#b15">(Fawzi et al., 2018;</ref><ref type="bibr">2016)</ref> which makes smoothness assumptions on the decision boundary to relate these two quantities.) This suggests that training procedures designed to improve adversarial robustness might reduce test error in noise and vice versa. We provide results from experiments which show that this is indeed the case: for every model we examined, either both quantities improved or neither did. In particular, a model trained on Gaussian noise shows significant improvements in adversarial robustness, comparable to (but not quite as strong as) a model trained on adversarial examples. We also found that an adversarially trained model on CIFAR-10 shows improved robustness to random image corruptions.</p><p>Finally, in Section 6, we establish a relationship between the error rate of an image classification model in the presence of Gaussian noise and the existence of adversarial examples for noisy versions of test set images. In this setting we can actually prove a rigorous, model-independent bound relating these two quantities that is achieved when the error set is a half space, and we see that the models we tested are already quite close to this optimum. Therefore, for these noisy image distributions, our models are already almost as adversarially robust as they can be given the error rates we see, so the only way to defend against adversarial examples is to reduce test error.</p><p>In this work we will investigate several different models trained on the MNIST, CIFAR-10 and ImageNet datasets. For MNIST and CIFAR-10 we look at the naturally trained and adversarially trained models which have been open-sourced by <ref type="bibr" target="#b26">Madry et al. (2017)</ref>. We also trained the same model on CIFAR-10 with Gaussian data augmentation. For ImageNet, we investigate Wide ResNet-50 trai]ned with Gaussian data augmentation. We were unable to study the effects of adversarial training on ImageNet because no robust open sourced model exists (we considered the models released in <ref type="bibr" target="#b33">Tram√®r et al. (2017)</ref> but found that they only minimally improve robustness to the white box PGD adversaries we consider here). Additional training details can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The broader field of adversarial machine learning studies general ways in which an adversary may interact with an ML system, and dates back to 2004 <ref type="bibr" target="#b8">(Dalvi et al., 2004;</ref><ref type="bibr" target="#b3">Biggio &amp; Roli, 2018)</ref>. Since the work of <ref type="bibr" target="#b32">Szegedy et al. (2014)</ref>, a subfield has focused specifically on the phenomenon of small adversarial perturbations of the input, or "adversarial examples." In <ref type="bibr" target="#b32">Szegedy et al. (2014)</ref> it was proposed these adversarial examples occupy a dense, measure-zero subset of image space. However, more recent work has provided evidence that this is not true. For example, <ref type="bibr" target="#b14">Fawzi et al. (2016)</ref>; <ref type="bibr" target="#b16">Franceschi et al. (2018)</ref> shows that under linearity assumptions of the decision boundary small adversarial perturbations exist when test error in noise is non-zero. <ref type="bibr" target="#b18">Gilmer et al. (2018b)</ref> showed for a specific data distribution that there is a fundamental upper bound on adversarial robustness in terms of test error. <ref type="bibr" target="#b27">Mahloujifar et al. (2018)</ref> has generalized these results to a much broader class of distributions.</p><p>Recent work has proven for a synthetic data distribution that adversarially robust generalization requires more data <ref type="bibr" target="#b30">(Schmidt et al., 2018)</ref>. The distribution they consider when proving this result is a mixture of high dimensional Gaussians. As we will soon discuss, every set E of small measure in the high dimensional Gaussian distribution has large boundary measure. Therefore, at least for the data distribution considered, the main conclusion of this work, "adversarially robust generalization requires more data", is a direct corollary of the statement "generalization requires more data."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TEST ERROR AND ADVERSARIAL ROBUSTNESS</head><p>Understanding the relationship between nearby errors and model generalization requires understanding the geometry of the error set of a statistical classifier, that is, the set of points in the input space on which the classifier makes an incorrect prediction. In particular, the assertion that these adversarial examples are a distinct phenomenon from test error is equivalent to stating that the error set is in some sense poorly behaved. We study two functions of a model's error set E.</p><p>The first quantity, test error under a given distribution of inputs q(x), is the probability that a random sample from the distribution q is in E. We will denote this P x‚àºq [x ‚àà E]; reducing this quantity when q is the natural data distribution is the goal of supervised learning. While one usually takes q to be the distribution from which the training set was sampled, we will also consider other distributions over the course of this paper.</p><p>When q includes points from outside the natural data distribution, a decision needs to be made about the labels in order to define E. The only such cases we will consider in this paper are noisy perturbations of training or test points, and we will always assume that the noise is at a scale which is small enough not to change the label. This assumption is commonly made in works which study model robustness to random corruptions of the input <ref type="bibr" target="#b21">(Hendrycks &amp; Dietterich, 2018;</ref><ref type="bibr" target="#b12">Dodge &amp; Karam, 2017b)</ref>. Some examples noisy images can be found in Figure <ref type="figure" target="#fig_4">7</ref> in the appendix.</p><p>The second quantity is called adversarial robustness. For an input x and a metric on the input space d, let d(x, E) denote the distance from x to the nearest point of E. For any , let E denote the set {x : d(x, E) &lt; }, the set of points within of an error. The adversarial robustness of the model is then P x‚àºq [x ‚àà E ], the probability that a random sample from q is within distance of some point in the error set. Reducing this quantity is the goal of much of the adversarial defense literature. When we refer to "adversarial examples" in this paper, we will always mean these nearby errors.</p><p>In geometric terms we can think of P x‚àºq [x ‚àà E] as a sort of volume of the error set while P x‚àºq [x ‚àà E ] is related to its surface area. More directly, P x‚àºq [x ‚àà E ] is what we will call the -boundary measure, the volume under q of the region within of the surface or the interior.</p><p>The adversarial example phenomenon is then simply that, for small , P x‚àºq [x ‚àà E ] can be large even when P x‚àºq [x ‚àà E] is small. In other words, most correctly classified inputs are very close to a misclassified point, even though the model is very accurate. In high-dimensional spaces this phenomenon is not isolated to the error sets of statistical classifiers. In fact almost every nonempty set of small volume has large -boundary measure, even sets that seem very well-behaved. As a simple example, consider the measure of the set E = {x ‚àà R n : ||x|| 2 &lt; 1} under the Gaussian distribution q = N (0, œÉ 2 I). For n = 1000, œÉ = 1.05/ ‚àö n, and = 0.1, we have P x‚àºq [x ‚àà E] ‚âà 0.02 and P x‚àºq [x ‚àà E ] ‚âà 0.98, so most samples from q will be close to E despite the fact that E has relatively little measure under the Gaussian distribution. If we relied only on our low-dimensional spatial intuition, we might be surprised to find how consistently small adversarial perturbations could be found -98% of our test points would have an error at distance 0.1 or less even though only 2% are misclassified.</p><p>In high dimensions, it is much easier for most points to be close to some set even if that set itself has a small volume. Contrary to what one might expect from our low-dimensional intuition, this does not require the set in question to be somehow pathological; in our example, it was just a ball. Therefore, when we see that some image classifier has errors in some noise distribution q (so that P x‚àºq [x ‚àà E] is appreciably bigger than zero) it is possible that E is much larger even if E is quite simple, so the existence of small worst-case perturbations should be expected given imperfect robustness to large average-case corruptions. In the sections that follow we will make this precise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ERRORS IN NOISE SUGGEST ADVERSARIAL EXAMPLES FOR CLEAN IMAGES</head><p>The Linear Case. For linear models, the relationship between errors in Gaussian noise and small perturbations of a clean image is exact. For an image x, let d(x) be the distance from x to decision boundary and let œÉ(x, ¬µ) be the œÉ for which P x‚àºq [x ‚àà E] is some fixed error rate ¬µ. (As we mentioned in the introduction, we assume that œÉ is small enough that adding this noise does not change the "correct" label.) Then we have d(x) = ‚àíœÉ(x, ¬µ)Œ¶ ‚àí1 (¬µ), where</p><formula xml:id="formula_0">Œ¶(t) = 1 ‚àö 2œÄ t ‚àí‚àû exp(‚àíx 2 /2)dx</formula><p>is the cdf of the univariate standard normal distribution. Note that this equality depends only on the error rate ¬µ and the standard deviation œÉ of a single component, and not directly on the dimension. This might seem at odds with the emphasis on high-dimensional geometry in Section 3. The dimension does appear if we consider the norm of a typical sample from N (0, œÉ 2 I), which is œÉ ‚àö n. As the dimension increases, so does the ratio between the distance to a noisy image and the distance to the decision boundary.</p><p>The decision boundary of a neural network is, of course, not linear. However, by computing the ratio between d(x) and œÉ(x, ¬µ) for neural networks and comparing it to what it would be for a linear model, we can investigate the question posed in the introduction: do we see adversarial examples at the distances we do because of pathologies in the shape of the error set, or do we find them at about the distances we would expect given the error rates we see in noise? We ran experiments on the error sets of several neural image classifiers and found evidence that is much more consistent with the second of these two possibilities. This relationship was also explored in <ref type="bibr" target="#b14">Fawzi et al. (2016;</ref><ref type="bibr">2018)</ref>; here we additionally measure how data augmentation affects this relationship.</p><p>We examined this relationship for neural networks when ¬µ = 0.01. For each test point, we compared œÉ(x, ¬µ) to an estimate of d(x). It is not actually possible to compute d(x) precisely for the error set of a neural network. In fact, finding the distance to the nearest error is NP-hard <ref type="bibr" target="#b23">(Katz et al., 2017)</ref>. Instead, the best we can do is to search for an error using a method like PGD <ref type="bibr" target="#b26">(Madry et al., 2017)</ref> and report the nearest error we can find.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the results for several CIFAR-10 and ImageNet models, including ordinary trained models, models trained on noise with œÉ = 0.4, and an adversarially trained CIFAR-10 model. We also included a line representing how these quantities would be related for a linear model.</p><p>We can see that none of the models we examined have nearby errors at a scale much smaller than we would expect from a linear model. Indeed, while the adversarially trained model does deviate from the linear case to a greater extent than the others, it does so in the direction of greater distances to the decision boundary. Moreover, we can see from the histograms that both of the interventions that increase d(x) also increase œÉ(x, ¬µ). So, to explain the distances to the errors we can find using PGD, it is not necessary to rely on any great complexity in the shape of the error set; a linear model with the same error rates in noise would have errors just as close. ‚àö n = 0.08. Left: An image from the test set (black), a random misclassified Gaussian perturbation at standard deviation 0.08 (blue), and an error found using PGD (red). The estimated measure of the cyan region ("miniature poodle") in the Gaussian distribution is about 0.1%. The small diamond-shaped region in the center of the image is the l ‚àû ball of radius 8/255. Right: A slice at a larger scale with the same black point, together with an error from the clean set (blue) and an adversarially constructed error (red) which are both assigned to the same class ("elephant").</p><p>Visualizing the Decision Boundary. In Figure <ref type="figure" target="#fig_1">2</ref> we drew some pictures of two-dimensional slices of image space through several different triples of points. (Similar visualizations have previously appeared in <ref type="bibr" target="#b15">Fawzi et al. (2018)</ref>, and are called "church window plots.")</p><p>We see some common themes. In the figure on the left, we see that an error found in Gaussian noise lies in the same connected component of the error set as an error found using PGD, and that at this scale that component visually resembles a half space. This figure also illustrates the relationship between test error and adversarial robustness. To measure adversarial robustness is to ask whether or not there are any errors in the l ‚àû ball -the small diamond-shaped region in the center of the image -and to measure test error in noise is to measure the volume of the error set in the defined noise distribution. At least in this slice, nothing distinguishes the PGD error from any other point in the error set apart from its proximity to the center point.</p><p>The figure on the right shows a different slice through the same test point but at a larger scale. This slice includes an ordinary test error along with an adversarial perturbation of the center image constructed with the goal of maintaining visual similarity while having a large l 2 distance. The two errors are both classified (incorrectly) by the model as "elephant." This adversarial error is actually farther from the center than the test error, but they still clearly belong to the same connected component. This suggests that defending against worst-case content-preserving perturbations <ref type="bibr" target="#b17">(Gilmer et al., 2018a)</ref> requires removing all errors at a scale comparable to the distance between unrelated pairs of images. Many more church window plots can be found in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COMPARING ADVERSARIAL TRAINING TO TRAINING ON NOISE</head><p>For a linear model, improving generalization in the presence of noise is equivalent to increasing the distance to the decision boundary. The results from the previous section suggest that a similar relationship should hold for other statistical classifiers, including neural networks. That is, augmenting the training data distribution with noisy images ought to increase the distance to the decision boundary, and augmenting the training distribution with small-perturbation adversarial examples should improve performance in noise. Here we present evidence that this is the case.</p><p>We analyzed the performance of the models described in Section 1 on four different noise distributions: two types of Gaussian noise, pepper noise <ref type="bibr" target="#b21">(Hendrycks &amp; Dietterich, 2018)</ref> subspace spanned by the first 100 principal components of the training set. Pepper noise randomly assigns channels of the image to 1 with some fixed probability. Details of the stAdv attack can be found in Appendix B, but it visually similar to Gaussian blurring where œÉ controls the severity of the blurring. Example images that have undergone each of the noise transformations we used can be found in Appendix I. Each model was also tested for l p robustness with a variety of norms and 's using the same PGD attack as in Section 4.</p><p>For CIFAR-10, standard Gaussian data augmentation yields comparable (but slightly worse) results to adversarial training on all considered metrics. For ImageNet we found that Gaussian data augmentation improves robustness to small l 2 perturbations as well as robustness to other noise corruptions.</p><p>The results are shown in Table <ref type="table" target="#tab_0">1</ref>. This holds both for generalization in all noises considered and for robustness to small perturbations. We found that performing data augmentation with heavy Gaussian noise (œÉ = 0.4 for CIFAR-10 and œÉ = 0.8 for ImageNet) worked best. The adversarially trained CIFAR-10 models were trained in the l ‚àû metric and they performed especially well on worst-case perturbations in this metric. Prior work has observed that Gaussian data augmentation helps small perturbation robustness on <ref type="bibr">MNIST (Kannan et al., 2018)</ref>, but to our knowledge we are the first to measure this on CIFAR-10 and ImageNet.</p><p>Neither augmentation method shows much improved generalization in PCA noise. We hypothesize that adversarially trained models learn to project away the high-frequency information in the input, which would do little to improve performance in PCA noise, which is supported in the low-frequency subspace of the data distribution. Further work would be required to establish this.</p><p>We also considered the MNIST adversarially trained model from <ref type="bibr" target="#b26">Madry et al. (2017)</ref>, and found it to be a special case where although robustness to small perturbations was increased generalization in noise was not improved. This is because this model violates the linearity assumption discussed in Section 4. This overfitting to the l ‚àû metric has been observed in prior work <ref type="bibr" target="#b31">(Sharma &amp; Chen, 2017</ref>). More details can be found in Appendix D.</p><p>Although no l p -robust open sourced ImageNet model exists, recent work has found that the adversarially trained models on Tiny ImageNet from Kannan et al. ( <ref type="formula">2018</ref>) generalize very well on a large suite of common image corruptions <ref type="bibr" target="#b21">(Hendrycks &amp; Dietterich, 2018)</ref>.</p><p>Failed Adversarial Defenses Do Not Improve Generalization in Noise. We performed a similar analysis on seven previously published adversarial defense strategies. These methods have already been shown to result in masking gradients, which cause standard optimization procedures to fail to find errors, rather than actually improving small perturbation robustness <ref type="bibr" target="#b0">(Athalye et al., 2018)</ref>. We find All of these defenses are now known not to improve adversarial robustness <ref type="bibr" target="#b0">(Athalye et al., 2018)</ref>. The defense strategies include bitdepth reduction <ref type="bibr" target="#b19">(Guo et al., 2017)</ref>, JPEG compression <ref type="bibr" target="#b19">(Guo et al., 2017;</ref><ref type="bibr" target="#b13">Dziugaite et al., 2016;</ref><ref type="bibr" target="#b25">Liu et al., 2018;</ref><ref type="bibr" target="#b1">Aydemir et al., 2018;</ref><ref type="bibr" target="#b10">Das et al., 2018;</ref><ref type="bibr">2017)</ref>, Pixel Deflection <ref type="bibr" target="#b28">(Prakash et al., 2018)</ref>, total variance minimization <ref type="bibr" target="#b19">(Guo et al., 2017)</ref>, respresentation-guided denoising <ref type="bibr" target="#b24">(Liao et al., 2018)</ref>, and random resizing and random padding of the input image <ref type="bibr" target="#b37">(Xie et al., 2017)</ref>.</p><p>that these methods also show no improved generalization in Gaussian noise. The results are shown in Figure <ref type="figure" target="#fig_2">3</ref>. Given how easy it is for a method to show improved robustness to standard optimization procedures without changing the decision boundary in any meaningful way, we strongly recommend that future defense efforts evaluate on out-of-distribution inputs such as the noise distributions we consider here. The current standard practice of evaluating solely on gradient-based attack algorithms is making progress more difficult to measure.</p><p>Obtaining Zero Test Error in Noise is Nontrivial. It is important to note that applying Gaussian data augmentation does not reduce error rates in Gaussian noise to zero. For example, we performed Gaussian data augmentation on CIFAR-10 at œÉ = .15 and obtained 99.9% training accuracy but 77.5% test accuracy in the same noise distribution. (For comparison, the naturally trained obtains 95% clean test accuracy.) Previous work <ref type="bibr" target="#b12">(Dodge &amp; Karam, 2017b)</ref> has also observed that obtaining perfect generalization in large Gaussian noise is nontrivial. This mirrors <ref type="bibr" target="#b30">Schmidt et al. (2018)</ref>, which found that small perturbation robustness did not generalize to the test set. This is perhaps not surprising given that error rates on the clean test set are also non-zero. Although the model is in some sense "superhuman" with respect to clean test accuracy, it still makes many mistakes on the clean test set that a human would never make. We collected some examples in Appendix I. More detailed results on training and testing in noise can be found in Appendices C and H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ERRORS IN NOISE IMPLY ADVERSARIAL EXAMPLES FOR NOISY IMAGES</head><p>The Gaussian Isoperimetric Inequality. Let x be a correctly classified image and consider the distribution q of Gaussian perturbations of x with some fixed variance œÉ 2 I. For this distribution, there is a precise sense in which small adversarial perturbations exist only because test error is nonzero. That is, given the error rates we actually observe on noisy images, most noisy images must be close to the error set. This result holds completely independently of any assumptions about the model and follows from a fundamental geometric property of the high-dimensional Gaussian distribution, which we will now make precise.</p><p>For an image x and the corresponding noisy image distribution q, let * q (E) be the median distance from one of these noisy images to the nearest error. (In other words, it is the for which P x‚àºq [x ‚àà E ] = 1 2 .) As before, let P x‚àºq [x ‚àà E] be the probability that a random Gaussian perturbation Figure <ref type="figure">4</ref>: The adversarial example phenomenon occurs for noisy images as well as clean ones. Starting with a noisy image that that is correctly classified, one can apply carefully crafted imperceptible noise to it which causes the model to output an incorrect answer. This occurs even though the error rate among random Gaussian perturbations of this image is small (less than .1% for the ImageNet panda shown above). In fact, we prove that the presence of errors in Gaussian noise logically implies that small adversarial perturbations exists around noisy images. The only way to "defend" against such adversarial perturbations is to reduce the error rate in Gaussian noise.</p><p>of x lies in E. It is possible to deduce a bound relating these two quantities from the Gaussian isoperimetric inequality <ref type="bibr" target="#b4">(Borell, 1975)</ref>. The form we will use is:</p><p>Theorem (Gaussian Isoperimetric Inequality). Let q = N (0, œÉ 2 I) be the Gaussian distribution on R n with variance œÉ 2 I, and let</p><formula xml:id="formula_1">¬µ = P x‚àºq [x ‚àà E]. Write Œ¶(t) = 1 ‚àö 2œÄ t ‚àí‚àû exp(‚àíx 2 /2)dx, the cdf of the univariate standard normal distribution. If ¬µ ‚â• 1 2 , then * q (E) = 0. Otherwise, * q (E) ‚â§ ‚àíœÉŒ¶ ‚àí1 (¬µ)</formula><p>, with equality when E is a half space.</p><p>In particular, for any machine learning model for which the error rate in the distribution q is at least ¬µ, the median distance to the nearest error is at most ‚àíœÉŒ¶ ‚àí1 (¬µ). (Note that Œ¶ ‚àí1 (¬µ) is negative when ¬µ &lt; 1 2 .) Because each coordinate of a multivariate normal is a univariate normal, ‚àíŒ¶ ‚àí1 (¬µ) is the distance to a half space for which the error rate is ¬µ when œÉ = 1. (We have the same indirect dependence on dimension here as we saw in Section 4: the distance to a typical sample from the Gaussian is œÉ ‚àö n.)</p><p>In Appendix E we will give the more common statement of the Gaussian isoperimetric inequality along with a proof of the version presented here. In geometric terms, we can say that a half space is the set E of a fixed volume that minimizes the surface area under the Gaussian measure, similar to how a circle is the set of fixed area that minimizes the perimeter. So among models with some fixed test error P x‚àºq [x ‚àà E], the most robust on this distribution are the ones whose error set is a half space.</p><p>Comparing Neural Networks to the Isoperimetric Bound. We evaluated these quantities for several models and many images from the CIFAR-10 and ImageNet test sets. Just like for clean images, we found that most noisy images are both correctly classified and very close to a visually similar image which is not. (See Figure <ref type="figure">4</ref>.)</p><p>As we mentioned in Section 4, it is not actually possible to compute * q precisely for the error set of a neural network, so we again report an estimate. For each test image, we took 1,000 samples from the corresponding Gaussian and estimated * q using PGD with 200 steps on each sample and reported the median.</p><p>We find that for the five models we considered on CIFAR-10 and ImageNet, the relationship between our estimate of * q (E) and P x‚àºq [x ‚àà E] is already close to optimal. This is visualized in Figure <ref type="figure">5</ref>. Note that in both cases, adversarial training does improve robustness to small perturbations, but the gains are primarily because error rates in Gaussian noise were dramatically improved, and less because the surface area of the error set was decreased. In particular, many test points do not appear on these graphs because error rates in noise were so low that we did not find any errors among the 100,000 samples we used. For example, for the naturally trained CIFAR model, about 1% of the points lie off the left edge of the plot, compared to about 59% for the adversarially trained model and 70% for the model trained on noise. This shows that adversarial training on small perturbations improved generalization to large random perturbations, as the isoperimetric inequality says it must.</p><p>Figure <ref type="figure">5</ref>: These plots give two ways to visualize the relationship between the error rate in noise and the distance from noisy points to the decision boundary (found using PGD). Each point on each plot represents one image from the test set. On the left, we compare the error rate of the model on Gaussian perturbations at œÉ = 0.1 to the distance from the median noisy point to its nearest error. On the right, we compare the œÉ at which the error rate is 0.01 to this same median distance. (The plots on the right are therefore similar to the plots in Figure <ref type="figure" target="#fig_0">1</ref>.) The thick black line at the top of each plot is the upper bound provided by the Gaussian isoperimetric inequality. We include data from a model trained on clean images, an adversarially trained model, and a model trained on Gaussian noise (œÉ = 0.4.) As mentioned in Section 1, we were unable to run this experiment on an adversarially robust ImageNet model. Not all models or functions will be this close to optimal. As a simple example, if we took one of the CIFAR models shown in Figure <ref type="figure">5</ref> and modified it so that the model outputs an error whenever each coordinate of the input is an integer multiple of 10 ‚àí6 , the resulting model would have an error within 1 2 ‚Ä¢ 10 ‚àí6 ‚Ä¢ dim(CIFAR) ‚âà 0.039 of every point. In this case, adversarial examples would be a distinct phenomenon from test performance, since * q (E) would be far from optimal. The contrast between these two settings is important for adversarial defense design. If adversarial examples arose from a badly behaved decision boundary (as in the latter case), then it would make sense to design defenses which attempt to smooth out the decision boundary in some way. However, because we observe that image models are already close to the optimal bound on robustness for a fixed error rate in noise, future defense design should attempt to improve generalization in noise. Currently there is a considerable subset of the adversarial defense literature which develops methods that would remove any small "pockets" of errors but which don't improve model generalization. One example is <ref type="bibr" target="#b37">Xie et al. (2017)</ref> which proposes randomly resizing the input to the network as a defense strategy. Unfortunately, this defense, like many others, has been shown to be ineffective against stronger adversaries <ref type="bibr" target="#b5">(Carlini &amp; Wagner, 2017a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b0">Athalye et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We proved a fundamental relationship between generalization in noisy image distributions and the existence of small adversarial perturbations. By appealing to the Gaussian isoperimetric inequality, we formalized the notion of what it means for a decision boundary to be badly behaved. We showed that, for noisy images, there is very little room to improve robustness without also decreasing the volume of the error set, and we provided evidence that small perturbations of clean images can also be explained in a similar way. These results show that small-perturbation adversarial robustness is closely related to generalization in the presence of noise and that future defense efforts can measure progress by measuring test error in different noise distributions.</p><p>Indeed, several such noise distributions have already been proposed, and other researchers have developed methods which improve generalization in these distributions <ref type="bibr" target="#b21">(Hendrycks &amp; Dietterich, 2018;</ref><ref type="bibr" target="#b12">Dodge &amp; Karam, 2017b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b34">Vasiljevic et al., 2016;</ref><ref type="bibr" target="#b39">Zheng et al., 2016)</ref>. Our work suggests that adversarial defense and improving generalization in noise involve attacking the same set of errors in two different ways -the first community tries to remove the errors on the boundary of the error set while the second community tries to reduce the volume of the error set. The isoperimetric inequality connects these two perspectives, and suggests that improvements in adversarial robustness should result in improved generalization in noise and vice versa. Adversarial training on small perturbations on CIFAR-10 also improved generalization in noise, and training on noise improved robustness to small perturbations.</p><p>In the introduction we referred to a question from <ref type="bibr" target="#b32">Szegedy et al. (2014)</ref> about why we find errors so close to our test points while the test error itself is so low. We can now suggest an answer: despite what our low-dimensional visual intuition may lead us to believe, these errors are not in fact unnaturally close given the error rates we observe in noise. There is a sense, then, in which we simply haven't reduced the test error enough to expect to have removed most nearby errors.</p><p>While we focused on the Gaussian distribution, similar conclusions can be made about other distributions. In general, in high dimensions, the -boundary measure of a typical set is large even when its volume is small, and this observation does not depend on anything specific about the Gaussian distribution. The Gaussian distribution is a special case in that we can easily prove that all sets will have large -boundary measure. <ref type="bibr" target="#b27">Mahloujifar et al. (2018)</ref> proved a similar theorem for a larger class of distributions. For other data distributions not every set has large -boundary measure, but under some additional assumptions it still holds that most sets do. An investigation of this relationship on the MNIST distribution can be found in <ref type="bibr">Gilmer et al. (2018b, Appendix G)</ref>.</p><p>We believe it would be beneficial for the adversarial defense literature to start reporting generalization in noisy image distributions, such as the common corruption benchmark introduced in <ref type="bibr" target="#b21">Hendrycks &amp; Dietterich (2018)</ref>, rather than the current practice of only reporting empirical estimates of adversarial robustness. There are several reasons for this recommendation.</p><p>1. Measuring test error in noise is significantly easier than measuring adversarial robustnesscomputing adversarial robustness perfectly requires solving an NP-hard problem for every point in the test set <ref type="bibr" target="#b23">(Katz et al., 2017)</ref>. Since <ref type="bibr" target="#b32">Szegedy et al. (2014)</ref>, hundreds of adversarial defense papers have been published. To our knowledge, only one <ref type="bibr" target="#b26">(Madry et al., 2017)</ref> has reported robustness numbers which were confirmed by a third party. We believe the difficulty of measuring robustness under the usual definition has contributed to this unproductive situation. 2. Measuring test error in noise would also allow us to determine whether or not these methods improve robustness in a trivial way, such as how the robust MNIST model learned to threshold the input, or whether they have actually succeeded in improving generalization outside the natural data distribution. 3. All of the failed defense strategies we examined failed to improve generalization in noise.</p><p>For this reason, we should be highly skeptical of defense strategies that only claim improved l p -robustness but do not demonstrate robustness in more general settings. 4. Finally, if the goal is improving the security of our models in adversarial settings, errors in the presence of noise are already indicative that our models are not secure. Until our models are perfectly robust in the presence of average-case corruptions, they will not be robust in worst-case settings. The usefulness of l p -robustness in realistic threat models is limited when attackers are not constrained to making small modifications.</p><p>The interest in measuring l p robustness arose from a sense of surprise that errors could be found so close to correctly classified points. But from the perspective described in this paper, the phenomenon is less surprising. Statistical classifiers make a large number of errors outside the data on which they were trained, and small adversarial perturbations are simply the nearest ones.  A TRAINING DETAILS Models trained on CIFAR-10. We trained the Wide-ResNet-28-10 model <ref type="bibr" target="#b38">(Zagoruyko &amp; Komodakis, 2016)</ref> using standard data augmentation of flips, horizontal shifts and crops in addition to Gaussian noise independently sampled for each image in every minibatch. The models were trained with the open-source code by Cubuk et al. ( <ref type="formula">2018</ref>) for 200 epochs, using the same hyperparameters which we summarize here: a weight decay of 5e-4, learning rate of 0.1, batch size of 128. The learning rate was decayed by a factor of 0.2 at epochs 60, 120, 160.</p><p>Models trained on ImageNet. The ResNet-50 model <ref type="bibr" target="#b20">(He et al., 2016)</ref> was trained with a learning rate of 1.6, batch size of 4096, and weight decay of 1e-4. During training, random crops and horizontal flips were used, in addition to the Gaussian noise independently sampled for each image in every minibatch. The models were trained for 90 epochs, where the learning rate was decayed by a factor of 0.1 at epochs 30, 60, and 80. Learning rate was linearly increased from 0 to the value of 1.6 over the first 5 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B NOISE ATTACK DETAILS</head><p>Here we provide more detail for the noise distributions considered in Section 5. The stAdv attack defines a flow field over the pixels of the image and shifts the pixels according to this flow. The field is parameterized by a latent Z. When we measure accuracy against our randomized variant of this attack, we randomly sample Z from a multivariate Gaussian distribution with standard deviation œÉ.</p><p>To implement this attack we used the open sourced code from <ref type="bibr" target="#b36">Xiao et al. (2018)</ref>. PCA-100 noise first samples noise from a Gaussian distribution N (0, œÉ), and then projects this noise onto the first 100 PCA components of the data. For ImageNet, the input dimension is too large to perform a PCA decomposition on the entire dataset. So we first perform a PCA decomposition on 30x30x1 patches taken from different color channels of the data. To general the noise we first sample from a 900 dimensional Gaussian, then project this into the basis spanned by the top 100 PCA components, then finally tile this projects to the full 299x299 dimension of the input. Each color channel is constructed independently in this fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C TRAINING AND TESTING ON GAUSSIAN NOISE</head><p>In Section 5, we mentioned that it is not trivial to learn the distribution of noisy images simply by augmenting the training data distribution. In Tables <ref type="table" target="#tab_2">2 and 3</ref> we present more information about the performance of the models we trained and tested on various scales of Gaussian noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D RESULTS ON MNIST</head><p>MNIST is a special case when it comes to the relationship between small adversarial perturbations and generalization in noise. Indeed prior has already observed that an MNIST model can trivially become robust to small l ‚àû perturbations by learning to threshold the input <ref type="bibr" target="#b30">(Schmidt et al., 2018)</ref>, and observed that the model from <ref type="bibr" target="#b26">Madry et al. (2017)</ref> indeed seems to do this. When we investigated this model in different noise distributions we found it generalizes worse than a naturally trained model, results are shown in Table <ref type="table" target="#tab_3">4</ref>. Given that it is possible for a defense to overfit to a particular l p metric, future work would be strengthened by demonstrating improved generalization outside the natural data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E THE GAUSSIAN ISOPERIMETRIC INEQUALITY</head><p>Here we will discuss the Gaussian isoperimetric inequality more thoroughly than we did in the text.</p><p>We will present some of the geometric intuition behind the theorem, and in the end we will show how the version quoted in the text follows from the form in which the inequality is usually stated.</p><p>The historically earliest version of the isoperimetric inequality, and probably the easiest to understand, is about areas of subsets of the plane and has nothing to do with Gaussians at all. It is concerned with the following problem: among all measurable subsets of the plane with area A, which ones have the smallest possible perimeter?<ref type="foot" target="#foot_0">1</ref> One picture to keep in mind is to imagine that you are required to fence off some region of the plane with area A and you would like to use as little fence as possible. The isoperimetric inequality says that the sets which are most "efficient" in this sense are balls.</p><p>Some care needs to be taken with the definition of the word "perimeter" here -what do we mean by the perimeter of some arbitrary subset of R 2 ? The definition that we will use involves the concept of the -boundary measure we discussed in the text. For any set E and any &gt; 0, recall that we defined the -extension of E, written E , to be the set of all points which are within of a point in E; writing A(E) for the area of E, we then define the perimeter of E to be</p><formula xml:id="formula_2">surf(E) := lim inf ‚Üí0 1 (A(E ) ‚àí A(E)) .</formula><p>A good way to convince yourself that this is reasonable is to notice that, for small , E ‚àí E looks like a small band around the perimeter of E with width . The isoperimetric inequality can then be formally expressed as giving a bound on the quantity inside the limit in terms of what it would be for a ball. (This is slightly stronger than just bounding the perimeter, that is, bounding the limit itself, but this stronger version is still true.) That is, for any measurable set</p><formula xml:id="formula_3">E ‚äÜ R 2 , 1 (A(E ) ‚àí A(E)) ‚â• 2 œÄA(E) + œÄ.</formula><p>It is a good exercise to check that we have equality here when E is a ball.</p><p>There are many generalizations of the isoperimetric inequality. For example, balls are also the subsets in R n which have minimal surface area for a given fixed volume, and the corresponding set on the surface of a sphere is a "spherical cap," the set of points inside a circle drawn on the surface of the sphere. The version we are most concerned with in this paper is the generalization to a Gaussian distribution. Rather than trying to relate the volume of E to the volume of E , the Gaussian isoperimetric inequality is about the relationship between the probability that a random sample from the Gaussian distribution lands in E or E . Other than this, though, the question we are trying to answer is the same: for a given probability p, among all sets E for which the probability of landing in E is p, when is the probability of landing in E as small as possible?</p><p>The Gaussian isoperimetric inequality says that the sets that do this are half spaces. (See Figure <ref type="figure" target="#fig_3">6</ref>.) Just as we did in the plane, it is convenient to express this as a bound on the probability of landing in E for an arbitrary measurable set E. This can be stated as follows:</p><p>Theorem. Consider the standard normal distribution q on R n , and let E be a measurable subset of</p><formula xml:id="formula_4">R n . Write Œ¶(t) = 1 ‚àö 2œÄ t ‚àí‚àû exp(x 2 /2)dx,</formula><p>the cdf of the one-variable standard normal distribution.</p><p>For a measurable subset E ‚äÜ R n , write Œ±(E) = Œ¶ ‚àí1 (P x‚àºq [x ‚àà E]). Then for any ‚â• 0,</p><formula xml:id="formula_5">P x‚àºq [x ‚àà E ] ‚â• Œ¶(Œ±(E) + ).</formula><p>The version we stated in the text involved * q (E), the median distance from a random sample from q to the closest point in E. This is the same as the smallest for which P x‚àºq [x ‚àà E ] = 1 2 . So, when = * q (E), the left-hand side of the Gaussian isoperimetric inequality is 1 2 , giving us that Œ¶(Œ± + * q (E)) ‚â§ 1 2 . Since Œ¶ ‚àí1 is a strictly increasing function, applying it to both sides preserves the direction of this inequality. But Œ¶ ‚àí1 ( 1 2 ) = 0, so we in fact have that * q (E) ‚â§ ‚àíŒ±, which is the statement we wanted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F VISUALIZING THE OPTIMAL CURVES</head><p>The optimal bound according to the isoperimetric inequality gives surprisingly strong bounds in terms of the existence of worst-case l 2 perturbations and error rates in Gaussian noise. In Figure <ref type="figure" target="#fig_4">7</ref> we plot the optimal curves for various values of œÉ, visualize images sampled from x + N (0, œÉ), and visualize images at various l 2 distance from the unperturbed clean image. Even for very large noise (œÉ = .6), test error needs to be less than 10 ‚àí15 in order to have worst-case perturbations be larger than 5.0. In order to visualize worst-case perturbations at varying l 2 distances, we visualize an image that minimizes similarity according to the SSIM metric <ref type="bibr" target="#b35">(Wang &amp; Bovik, 2009)</ref>. These images are found by performing gradient descent to minimize the SSIM metric subject to the containt that ||x ‚àí x adv || 2 &lt; . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G CHURCH WINDOW PLOTS</head><p>In this section we include many more visualizations of the sorts of church window plots we discussed briefly in Section 4. We will show an ordinarily trained model's predictions on several different slices through the same CIFAR test point which illustrate different aspects of the story told in this paper. These images are best viewed in color.</p><p>Figure <ref type="figure">8</ref>: A slice through a clean test point (black, center image), the closest error found using PGD (blue, top image), and a random error found using Gaussian noise (red, bottom image). For this visualization, and all others in this section involving Gaussian noise, we used noise with œÉ = 0.05, at which the error rate was about 1.7%. In all of these images, the black circle indicates the distance at which the typical such Gaussian sample will lie. The plot on the right shows the probability that the model assigned to its chosen class. Green indicates a correct prediction, gray or white is an incorrect prediction, and brighter means more confident.</p><p>Figure <ref type="figure">9</ref>: A slice through a clean test point (black, center image), the closest error found using PGD (blue, top image), and the average of a large number of errors randomly found using Gaussian noise (red, bottom image). The distance from the clean image to the PGD error was 0.12, and the distance from the clean image to the averaged error was 0.33. The clean image is assigned the correct class with probability 99.9995% and the average and PGD errors are assigned the incorrect class with probabilities 55.3% and 61.4% respectively. However, it is clear from this image that moving even a small amount into the orange region will increase these latter numbers significantly. For example, the probability assigned to the PGD error can be increased to 99% by moving it further from the clean image in the same direction by a distance of 0.07.    Figure <ref type="figure" target="#fig_0">14</ref>: Some visualizations of the same phenomenon, but using the "pepper noise" discussed in Section 5 rather than Gaussian noise. In all of these visualizations, we see the slice through the clean image (black, center image), the same PGD error as above (red, bottom image), and a random error found using pepper noise (blue, top image). In the visualization on the left, we used an amount of noise that places the noisy image further from the clean image than in the Gaussian cases we considered above. In the visualization in the center, we selected a noisy image which was assigned to neither the correct class nor the class of the PGD error. In the visualization on the right, we selected a noisy image which was assigned to the same class as the PGD error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H THE DISTRIBUTION OF ERROR RATES IN NOISE</head><p>Using some of the models that were trained on noise, we computed, for each image in the CIFAR test set, the probably that a random Gaussian perturbation will be misclassified. A histogram is shown in Figure <ref type="figure" target="#fig_8">15</ref>. Note that, even though these models were trained on noise, there are still many errors around most images in the test set. While it would have been possible for the reduced performance in noise to be due to only a few test points, we see clearly that this is not the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I A COLLECTION OF MODEL ERRORS</head><p>In this section we first show a collection of iid test errors for the ResNet-50 model on the ImageNet validation set. We also visualize the severity of the different noise distributions considered in this work, along with model errors found by random sampling in these distributions.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparing the distance to decision boundary with the œÉ for which the error rate in Gaussian noise is 1%. Each point represents 50 images from the test set, and the median values for each coordinate are shown. (The PGD attack was run with = 1, so the distances to the decision boundary reported here are cut off at 1.) We also see histograms of the x coordinates. (A misclassified point is assigned œÉ = 0.)</figDesc><graphic url="image-3.png" coords="4,115.41,203.59,178.20,87.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two-dimensional slices of image space through different triples of points together with the classes assigned by a trained model. The black circle in both images has radius 31.4, corresponding to noise with œÉ = 31.4/‚àö n = 0.08. Left: An image from the test set (black), a random misclassified Gaussian perturbation at standard deviation 0.08 (blue), and an error found using PGD (red). The estimated measure of the cyan region ("miniature poodle") in the Gaussian distribution is about 0.1%. The small diamond-shaped region in the center of the image is the l ‚àû ball of radius 8/255. Right: A slice at a larger scale with the same black point, together with an error from the clean set (blue) and an adversarially constructed error (red) which are both assigned to the same class ("elephant").</figDesc><graphic url="image-6.png" coords="5,278.79,81.86,217.80,126.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The performance in Gaussian noise of several previously published defenses for ImageNet, along with a model trained on Gaussian noise at œÉ = 0.4 for comparison. For each point we ran ten trials; the error bars show one standard deviation. All of these defenses are now known not to improve adversarial robustness<ref type="bibr" target="#b0">(Athalye et al., 2018)</ref>. The defense strategies include bitdepth reduction<ref type="bibr" target="#b19">(Guo et al., 2017)</ref>, JPEG compression<ref type="bibr" target="#b19">(Guo et al., 2017;</ref><ref type="bibr" target="#b13">Dziugaite et al., 2016;</ref><ref type="bibr" target="#b25">Liu et al., 2018;</ref><ref type="bibr" target="#b1">Aydemir et al., 2018;</ref><ref type="bibr" target="#b10">Das et al., 2018;</ref> 2017), Pixel Deflection<ref type="bibr" target="#b28">(Prakash et al., 2018)</ref>, total variance minimization<ref type="bibr" target="#b19">(Guo et al., 2017)</ref>, respresentation-guided denoising<ref type="bibr" target="#b24">(Liao et al., 2018)</ref>, and random resizing and random padding of the input image<ref type="bibr" target="#b37">(Xie et al., 2017)</ref>.</figDesc><graphic url="image-7.png" coords="7,127.80,81.86,356.41,165.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: The Gaussian isoperimetric inequality relates the amount of probability mass contained in a set E to the amount contained in its -extension E . A sample from the Gaussian is equally likely to land in the pink set on the left or the pink set on the right, but the set on the right has a larger -extension. The Gaussian isoperimetric inequality says that the sets with the smallest possible -extensions are half spaces.</figDesc><graphic url="image-11.png" coords="15,146.36,81.86,158.40,148.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Top: The optimal curves on Imagenet for different values of œÉ. Middle: Visualizing different coordinates of the optimal curves. First, random samples from x + N (0, œÉI) for different values of œÉ. Bottom: Images at different l 2 distances from the unperturbed clean image. Each image visualized is the image at the given l 2 distance which minimizes visual similarity according to the SSIM metric. Note that images at l 2 &lt; 5 have almost no perceptible change from the clean image despite the fact that SSIM visual similarity is minimized.</figDesc><graphic url="image-14.png" coords="16,108.00,319.75,396.00,268.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: A slice through a clean test point (black, center image), a random error found using Gaussian noise (blue, top image), and the average of a large number of errors randomly found using Gaussian noise (red, bottom image).</figDesc><graphic url="image-17.png" coords="18,187.20,531.93,237.60,155.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: A slice through a clean test point (black, center image) and two random errors found using Gaussian noise (blue and red, top and bottom images). Note that both random errors lie very close to the decision boundary, and in this slice the decision boundary does not appear to come close to the clean image.</figDesc><graphic url="image-18.png" coords="19,187.20,81.86,237.60,158.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Figure12: A slice through three random errors found using Gaussian noise. (Note, in particular, that the black point in this visualization does not correspond to the clean image.)</figDesc><graphic url="image-19.png" coords="19,187.20,320.09,237.60,156.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: The cdf of the error rates in noise for images in the test set. The blue curve corresponds to a model trained and tested on noise with œÉ = 0.1, and the green curve is for a model trained and tested at œÉ = 0.3. For example, the left most point on the blue curve indicates that about 40% of test images had an error rate of at least 10 ‚àí3 .</figDesc><graphic url="image-21.png" coords="20,177.30,81.86,257.41,179.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: A collection of adversarially chosen model errors. These errors appeared in the ImageNet validation set. Despite the high accuracy of the model there remain plenty of errors in the test set that a human would not make.</figDesc><graphic url="image-25.png" coords="21,108.00,155.87,396.00,188.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: A collection of adversarially chosen model errors. These errors appeared in the ImageNet validation set. Despite the high accuracy of the model there remain plenty of errors in the test set that a human would not make.</figDesc><graphic url="image-26.png" coords="21,108.00,415.65,396.00,271.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Visualizing the severity of PCA noise, along with model errors found in this noise distribution.</figDesc><graphic url="image-27.png" coords="22,108.00,81.86,396.00,104.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Visualizing the severity of Gaussian noise, along with model errors found in this noise distribution. Note the model shown here was trained at noise level œÉ = .6.</figDesc><graphic url="image-28.png" coords="22,108.00,243.51,396.00,98.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Visualizing the severity of pepper noise.</figDesc><graphic url="image-29.png" coords="22,108.00,398.71,396.01,101.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Visualizing the severity of the randomized stAdv attack.</figDesc><graphic url="image-30.png" coords="22,108.00,545.99,396.01,99.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-15.png" coords="17,147.60,429.84,316.80,213.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>, and a randomized variant of the stAdv adversarial attack introduced in<ref type="bibr" target="#b36">Xiao et al. (2018)</ref>. We used both ordinary, spherical Gaussian noise and what we call "PCA noise," which is Gaussian noise supported only on the The performance of the models we considered under various noise distributions, together with our measurements of those models' robustness to small l p perturbations. For all the robustness tests we used PGD with 100 steps and a step size of /25. The adversarially trained CIFAR-10 model is the open sourced model from<ref type="bibr" target="#b26">Madry et al. (2017)</ref>.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell>ImageNet</cell><cell></cell></row><row><cell cols="2">Training Vanilla</cell><cell>Noise</cell><cell>Noise</cell><cell>Adv</cell><cell>Vanilla</cell><cell>Noise</cell><cell>Noise</cell></row><row><cell></cell><cell></cell><cell cols="2">œÉ = 0.1 œÉ = 0.4</cell><cell></cell><cell></cell><cell cols="2">œÉ = 0.4 œÉ = 0.8</cell></row><row><cell>Noise Type</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Clean 95.0%</cell><cell>93.5%</cell><cell>84.0%</cell><cell cols="2">87.3% 76.0%</cell><cell>74.4%</cell><cell>72.6%</cell></row><row><cell cols="2">PCA100, œÉ = 0.2 93.2%</cell><cell>92.3%</cell><cell>83.6%</cell><cell cols="2">86.5% 45.5%</cell><cell>56.5%</cell><cell>59.7%</cell></row><row><cell cols="2">PCA100, œÉ = 0.4 82.6%</cell><cell>83.1%</cell><cell>81.0%</cell><cell cols="2">80.6% 13.5%</cell><cell>17.7%</cell><cell>19.7%</cell></row><row><cell cols="2">Pepper, p = 0.1 20.2%</cell><cell>53.3%</cell><cell>81.2%</cell><cell cols="2">38.4% 31.3%</cell><cell>70.0%</cell><cell>69.1%</cell></row><row><cell cols="2">Pepper, p = 0.3 12.3%</cell><cell>18.9%</cell><cell>58.0%</cell><cell>21.1%</cell><cell>5.4%</cell><cell>56.0%</cell><cell>61.5%</cell></row><row><cell cols="2">Gaussian, œÉ = 0.1 29.1%</cell><cell>89.0%</cell><cell>85.1%</cell><cell cols="2">77.8% 60.7%</cell><cell>73.3%</cell><cell>71.7%</cell></row><row><cell cols="2">Gaussian, œÉ = 0.2 13.5%</cell><cell>38.8%</cell><cell>83.5%</cell><cell cols="2">42.1% 27.9%</cell><cell>70.5%</cell><cell>69.3%</cell></row><row><cell cols="2">stAdv, œÉ = 0.5 52.3%</cell><cell>84.4%</cell><cell>77.9%</cell><cell cols="2">81.7% 57.3%</cell><cell>67.3%</cell><cell>69.0%</cell></row><row><cell cols="2">stAdv, œÉ = 2.0 17.4%</cell><cell>30.6%</cell><cell>52.1%</cell><cell cols="2">27.0% 11.4%</cell><cell>27.2%</cell><cell>31.3%</cell></row><row><cell>lp robustness</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>l2, = 0.5</cell><cell>0.3%</cell><cell>39.2%</cell><cell>54.5%</cell><cell>58.3%</cell><cell>7.9%</cell><cell>43.8%</cell><cell>47.7%</cell></row><row><cell>l2, = 1.0</cell><cell>0.0%</cell><cell>9.5%</cell><cell>25.1%</cell><cell>29.7%</cell><cell>0.5%</cell><cell>16.8%</cell><cell>22.5%</cell></row><row><cell cols="2">l‚àû, = 1/255 26.2%</cell><cell>84.4%</cell><cell>76.6%</cell><cell>83.5%</cell><cell>0.8%</cell><cell>20.1%</cell><cell>25.0%</cell></row><row><cell>l‚àû, = 4/255</cell><cell>0.4%</cell><cell>39.8%</cell><cell>49.6%</cell><cell>68.3%</cell><cell>0.0%</cell><cell>0.1%</cell><cell>0.1%</cell></row><row><cell>l‚àû, = 8/255</cell><cell>0.0%</cell><cell>10.3%</cell><cell>20.0%</cell><cell>45.4%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Wide ResNet-28-10 (Zagoruyko &amp; Komodakis, 2016)  trained and tested on CIFAR-10 with Gaussian noise with standard deviation œÉ.</figDesc><table><row><cell>œÉ</cell><cell cols="3">0.00625 0.0125</cell><cell>0.025</cell><cell>0.075</cell><cell>0.15</cell><cell>0.25</cell></row><row><cell>Training Accuracy</cell><cell>100%</cell><cell></cell><cell cols="5">100% 100% 100% 99.9% 99.4%</cell></row><row><cell>Test Accuracy</cell><cell cols="7">96.0% 95.5% 94.8% 90.4% 77.5% 62.2%</cell></row><row><cell>œÉ</cell><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell cols="8">Clean Training Accuracy 91.5% 90.8% 89.9% 87.7% 86.1% 84.6%</cell></row><row><cell>Clean Test Accuracy</cell><cell cols="7">75.9% 75.5% 75.2% 74.2% 73.3% 72.4%</cell></row><row><cell cols="2">Noisy Training Accuracy</cell><cell cols="6">‚àí 89.0% 85.7% 78.3% 71.7% 65.2%</cell></row><row><cell>Noisy Test Accuracy</cell><cell></cell><cell cols="6">‚àí 73.9% 70.9% 65.2% 59.7% 54.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The models from Section 1 trained and tested on ImageNet with Gaussian noise with standard deviation œÉ; the column labeled 0 refers to a model trained only on clean images.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The performance of ordinarily and adversarially trained MNIST models on various noise distributions.</figDesc><table><row><cell></cell><cell></cell><cell>Pepper</cell><cell>Gaussian</cell><cell>stAdv</cell><cell>PCA-100</cell></row><row><cell></cell><cell>Clean</cell><cell>p = 0.2</cell><cell>œÉ = 0.3</cell><cell>œÉ = 1.0</cell><cell>œÉ = 0.3</cell></row><row><cell cols="6">Model Accuracy Accuracy Accuracy Accuracy Accuracy</cell></row><row><cell>Clean</cell><cell>99.2%</cell><cell>81.4%</cell><cell>96.9%</cell><cell>89.5%</cell><cell>63.3%</cell></row><row><cell>Adv</cell><cell>98.4%</cell><cell>27.5%</cell><cell>78.2%</cell><cell>93.2%</cell><cell>47.1%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The name "isoperimetric" comes from a different, but completely equivalent, way of stating the question: among all sets with the same fixed perimeter, which ones have the largest possible area?</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Alptekin Temizel, and Tugba Taskaya Temizel. The effects of jpeg and jpeg2000 compression on attacks using adversarial examples</title>
		<author>
			<persName><forename type="first">Ayse</forename><surname>Elvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aydemir</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10418</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Why do deep convolutional networks generalize so poorly to small image transformations?</title>
		<author>
			<persName><forename type="first">Aharon</forename><surname>Azulay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12177</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wild patterns: Ten years after the rise of adversarial machine learning</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="317" to="331" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Brunn-Minkowski inequality in Gauss space</title>
		<author>
			<persName><forename type="first">Christer</forename><surname>Borell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inventiones mathematicae</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07263</idno>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial classification</title>
		<author>
			<persName><forename type="first">Nilesh</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression</title>
		<author>
			<persName><forename type="first">Nilaksh</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhuri</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Shang-Tse Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duen</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Horng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02900</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Nilaksh</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhuri</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Shang-Tse Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duen</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Horng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06816</idno>
		<title level="m">Shield: Fast, practical defense and vaccination for deep learning using jpeg compression</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Karam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08119</idno>
		<title level="m">Quality resilient deep neural networks</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Communication and Networks (ICCCN), 2017 26th International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Gintare</forename><surname>Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00853</idno>
		<title level="m">A study of the effect of jpg compression on adversarial images</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robustness of classifiers: from adversarial to random noise</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1632" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Empirical study of the topology and geometry of deep networks</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR, number CONF</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07971</idno>
		<title level="m">Robustness of classifiers to uniform p and Gaussian noise</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06732</idno>
		<title level="m">Motivating the rules of the game for adversarial example research</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02774</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">Adversarial spheres. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00117</idno>
		<title level="m">Moustapha Cisse, and Laurens van der Maaten. Countering adversarial images using input transformations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and surface variations</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01697</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Harini Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06373</idno>
		<title level="m">Adversarial logit pairing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reluplex: An efficient smt solver for verifying deep neural networks</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><surname>Mykel</surname></persName>
		</author>
		<author>
			<persName><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="97" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1778" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Feature distillation: Dnn-oriented jpeg compression against adversarial examples</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wujie</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05787</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial examples</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure</title>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Mahloujifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><forename type="middle">I</forename><surname>Diochnos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mahmoody</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03063</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deflecting adversarial attacks with pixel deflection</title>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonella</forename><surname>Dilillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Storer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8571" to="8580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03305</idno>
		<title level="m">The elephant in the room</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11285</idno>
		<title level="m">Dimitris Tsipras, Kunal Talwar, and Aleksander M ƒÖdry. Adversarially robust generalization requires more data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Breaking the madry defense model with l1-based adversarial examples</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10733</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6199" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tram√®r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Examining the impact of blur on recognition by convolutional networks</title>
		<author>
			<persName><forename type="first">Igor</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05760</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mean squared error: Love it or leave it? a new look at signal fidelity measures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02612</idno>
		<title level="m">Spatially transformed adversarial examples</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01991</idno>
		<title level="m">Mitigating adversarial effects through randomization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving the robustness of deep neural networks via stability training</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
				<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4480" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
