<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Bayesian Data Augmentation Approach for Learning Deep Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-10-29">29 Oct 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Toan</forename><surname>Tran</surname></persName>
							<email>toan.m.tran@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Trung</forename><surname>Pham</surname></persName>
							<email>trung.pham@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<email>gustavo.carneiro@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lyle</forename><surname>Palmer</surname></persName>
							<email>lyle.palmer@adelaide.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Public Health</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<email>ian.reid@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">31st Conference on Neural Information Processing Systems (NIPS 2017)</orgName>
								<address>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Bayesian Data Augmentation Approach for Learning Deep Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-10-29">29 Oct 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">E5BF28C135F5B24BEFDC1686EF8C87E9</idno>
					<idno type="arXiv">arXiv:1710.10564v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation is an essential part of the training process applied to deep learning models. The motivation is that a robust training process for deep learning models depends on large annotated datasets, which are expensive to be acquired, stored and processed. Therefore a reasonable alternative is to be able to automatically generate new annotated training samples using a process known as data augmentation. The dominant data augmentation approach in the field assumes that new training samples can be obtained via random geometric or appearance transformations applied to annotated training samples, but this is a strong assumption because it is unclear if this is a reliable generative model for producing new training samples. In this paper, we provide a novel Bayesian formulation to data augmentation, where new annotated training points are treated as missing variables and generated based on the distribution learned from the training set. For learning, we introduce a theoretically sound algorithm -generalised Monte Carlo expectation maximisation, and demonstrate one possible implementation via an extension of the Generative Adversarial Network (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the better performance of our proposed method compared to the current dominant data augmentation approach mentioned abovethe results also show that our approach produces better classification results than similar GAN models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has become the "backbone" of several state-of-the-art visual object classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>, speech recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6]</ref>, and natural language processing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31]</ref> systems. One of the many reasons that explains the success of deep learning models is that their large capacity allows for the modeling of complex, high dimensional data patterns. The large capacity allowed by deep learning is enabled by millions of parameters estimated within annotated training sets, where generalization tends to improve with the size of these training sets. One way of acquiring large annotated training sets is via the manual (or "hand") labeling of training samples by human expertsa difficult and sometimes subjective task that is expensive and prone to mistakes. Another way of producing such large training sets is to artificially enlarge existing training datasets -a process that is commonly known in computer science as data augmentation (DA).</p><p>In computer vision applications, DA has been predominantly developed with the application of simple geometric and appearance transformations on existing annotated training samples in order to generate new training samples, where the transformation parameters are sampled with additive Gaussian or uniform noise. For instance, for ImageNet classification <ref type="bibr" target="#b7">[8]</ref>, new training images can be generated by applying random rotations, translations or color perturbations to the annotated images <ref type="bibr" target="#b18">[19]</ref>. Such a DA process based on "label-preserving" transformations assumes that the noise model over these transformation spaces can represent with fidelity the processes that have produced the labelled images. This is a strong assumption that to the best of our knowledge has not been properly tested. In fact, this commonly used DA process is known as "poor man's" data augmentation (PMDA) <ref type="bibr" target="#b27">[28]</ref> in the statistical learning community because new synthetic samples are generated from a distribution estimated only once at the beginning of the training process. In the current manuscript, we propose a novel Bayesian DA approach for training deep learning models. In particular, we treat synthetic data points as instances of a random latent variable, which are drawn from a distribution learned from the given annotated training set. Effectively, rather than generating new synthetic training data prior to the training process using pre-defined transformation spaces and noise models, our approach generates new training data as the training progresses using samples obtained from an iteratively learned training data distribution. Fig. <ref type="figure" target="#fig_0">1</ref> shows an overview of our proposed data augmentation algorithm.</p><p>The development of our approach is inspired by DA using latent variables proposed by the statistical learning community <ref type="bibr" target="#b28">[29]</ref>, where the motivation is to introduce latent variables to facilitate the computation of posterior distributions. However, directly applying this idea to deep learning is challenging because sampling millions of network parameters is computationally difficult. By replacing the estimation of the posterior distribution by the estimation of the maximum a posteriori (MAP) probability, one can employ the Expectation Maximization (EM) algorithm, if the maximisation of such augmented posteriors is feasible. Unfortunately, this is not the case for deep learning models, where the posterior maximisation cannot reliably produce a global optimum. An additional challenge for deep learning models is that it is nontrivial to compute the expected value of the network parameters given the current estimate of the network parameters and the augmented data.</p><p>In order to address such challenges, we propose a novel Bayesian DA algorithm, called Generalized Monte Carlo Expectation Maximization (GMCEM), which jointly augments the training data and optimises the network parameters. Our algorithm runs iteratively, where at each iteration we sample new synthetic training points and use Monte Carlo to estimate the expected value of the network parameters given the previous estimate. Then, the parameter values are updated with stochastic gradient decent (SGD). We show that the augmented learning loss function is actually equivalent to the expected value of the network parameters, and that therefore we can guarantee weak convergence. Moreover, our method depends on the definition of predictive distributions over the latent variables, but the design of such distributions is hard because they need to be sufficiently expressive to model high-dimensional data, such as images. We address this challenge by leveraging the recent advances reached by deep generative models <ref type="bibr" target="#b10">[11]</ref>, where data distributions are implicitly represented via deep neural networks whose parameters are learned from annotated data. We demonstrate our Bayesian DA algorithm in the training of deep learning classification models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Our proposed algorithm is realised by extending a generative adversarial network (GAN) model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> with a data generation model and two discriminative models (one to discriminate between real and fake images and another to discriminate between the dataset classes). One important contribution of our approach is the fact that the modularity of our method allows us to test different models for the generative and discriminative models -in particular, we are able to test several recently proposed deep learning models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> for the dataset class classification. Experiments on MNIST, CIFAR-10 and CIFAR-100 datasets show the better classification performance of our proposed method compared to the current dominant DA approach.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Augmentation</head><p>Data augmentation (DA) has become an essential step in training deep learning models, where the goal is to enlarge the training sets to avoid over-fitting. DA has also been explored by the statistical learning community <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7]</ref> for calculating posterior distributions via the introduction of latent variables. Such DA techniques are useful in cases where the likelihood (or posterior) density functions are hard to maximize or sample, but the augmented density functions are easier to work. An important caveat is that in statistical learning, latent variables may not lie in the same space of the observed data, but in deep learning, the latent variables representing the synthesized training samples belong to the same space as the observed data.</p><p>Synthesizing new training samples from the original training samples is a widely used DA method for training deep learning models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19]</ref>. The usual idea is to apply either additive Gaussian or uniform noise over pre-determined families of transformations to generate new synthetic training samples from the original annotated training samples. For example, Yaeger et al. <ref type="bibr" target="#b29">[30]</ref> proposed the "stroke warping" technique for word recognition, which adds small changes in skew, rotation, and scaling into the original word images. Simard et al. <ref type="bibr" target="#b25">[26]</ref> used a related approach for visual document analysis. Similarly, Krizhevsky et al. <ref type="bibr" target="#b18">[19]</ref> used horizontal reflections and color perturbations for image classification. Hauberg et al. <ref type="bibr" target="#b12">[13]</ref> proposed a manifold learning approach that is run once before the classifier training begins, where this manifold describes the geometric transformations present in the training set.</p><p>Nevertheless, the DA approaches presented above have several limitations. First, it is unclear how to generate diverse data samples. As pointed out by Fawzi et al. <ref type="bibr" target="#b9">[10]</ref>, the transformations should be "sufficiently small" so that the ground truth labels are preserved. In other words, these methods implicitly assume a small scale noise model over a pre-determined "transformation space" of the training samples. Such an assumption is likely too restrictive and has not been tested properly. Moreover, these DA mechanisms do not adapt with the progress of the learning process-instead, the augmented data are generated only once and prior to the training process. This is, in fact, analogous to the Poor Man's Data Augmentation (PMDA) <ref type="bibr" target="#b27">[28]</ref> algorithm in statistical learning as it is non-iterative. In contrast, our Bayesian DA algorithm iteratively generates novel training samples as the training progresses, and the "generator" is adaptively learned. This is crucial because we do not make a noise model assumption over pre-determined transformation spaces to generate new synthetic training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Generative Models</head><p>Deep learning has been widely applied in training discriminative models with great success, but the progress in learning generative models has proven to be more difficult. One noteworthy work in training deep generative models is the Generative Adversarial Networks (GAN) proposed by Goodfellow et al. <ref type="bibr" target="#b10">[11]</ref>, which, once trained, can be used to sample synthetic images. GAN consists of one generator and one discriminator, both represented by deep learning models. In "adversarial training", the generator and discriminator play a "two-player minimax game", in which the generator tries to fool the discriminator by rendering images as similar as possible to the real images, and the discriminator tries to distinguish the real and fake ones. Nonetheless, the synthetic images generated by GAN are of low quality when trained on the datasets with high variability <ref type="bibr" target="#b8">[9]</ref>. Variants of GAN have been proposed to improve the quality of the synthetic images <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. For instance, conditional GAN <ref type="bibr" target="#b21">[22]</ref> improves the original GAN by making the generator conditioned on the class labels. Auxiliary classifier GAN (AC-GAN) <ref type="bibr" target="#b23">[24]</ref> additionally forces the discriminator to classify both real-or-fake sources as well as the class labels of the input samples. These two works have shown significant improvement over the original GAN in generating photo-realistic images. So far these generative models mainly aim at generating samples of high-quality, high-resolution photo-realistic images. In contrast, we explore generative models (in the form of GANs) in our proposed Bayesian DA algorithm for improving classification models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Augmentation Algorithm in Deep Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bayesian Neural Networks</head><p>Our goal is to estimate the parameters of a deep learning model using an annotated training set denoted by Y = {y n } N n=1 , where y = (t, x), with annotations t ∈ {1, ..., K} (K = # Classes), and data samples represented by x ∈ R D . Denoting the model parameters by θ, the training process is defined by the following optimisation problem:</p><formula xml:id="formula_0">θ * = arg max θ log p(θ|y),<label>(1)</label></formula><p>where the observed posterior p(θ|y) = p(θ|t, x) ∝ p(t|x, θ)p(x|θ)p(θ).</p><p>Assuming that the data samples in Y are conditionally independent, the cost function that maximises (1) is defined as <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_1">log p(θ|y) ≈ log p(θ) + 1 N N n=1 (log p(t n |x n , θ) + log p(x n |θ)),<label>(2)</label></formula><p>where p(θ) denotes a prior on the distribution of the deep learning model parameters, p(t n |x n , θ) represents the conditional likelihood of label t n , and p(x n |θ) is the likelihood of the data x.</p><p>In general, the training process to estimate the model parameters θ tends to over-fit the training set Y given the large dimensionality of θ and the fact that Y does not have a sufficiently large amount of training samples. One of the main approaches designed to circumvent this over-fitting issue is the automated generation of synthetic training samples -a process known as data augmentation (DA).</p><p>In this work, we propose a novel Bayesian approach to augment the training set, targeting a more robust training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Augmentation using Latent Variable Methods</head><p>The DA principle is to increase the observed training data y using a latent variable z that represents the synthesised data, so that the augmented posterior p(θ|y, z) can be easily estimated <ref type="bibr" target="#b27">[28]</ref>, leading to a more robust estimation of p(θ|y). The latent variable is defined by z = (t a , x a ), where x a ∈ R D refers to a synthesized data point, and t a ∈ {1, ..., K} denotes the associated label.</p><p>The most commonly chosen optimization method in these types of training processes involving a latent variable is the expectation-maximisation (EM) algorithm <ref type="bibr" target="#b6">[7]</ref>. In EM, let θ i denote the estimated parameters of the model of p(θ|y) at iteration i, and p(z|θ i , y) represents the conditional predictive distribution of z. Then, the E-step computes the expectation of log p(θ|y, z) with respect to p(z|θ i , y), as follows:</p><formula xml:id="formula_2">Q(θ, θ i ) = E p(z|θ i ,y) log p(θ|y, z) = z log p(θ|y, z)p(z|θ i , y)dz.<label>(3)</label></formula><p>The parameter estimation at the next iteration, θ i+1 , is then obtained at the M-step by maximizing the Q function:</p><formula xml:id="formula_3">θ i+1 = arg max θ Q(θ, θ i ).<label>(4)</label></formula><p>The algorithm iterates until ||θ i+1θ i || is sufficiently small, and the optimal θ * is selected from the last iteration. The EM algorithm guarantees that the sequence {θ i } i=1,2,... converges to a stationary point of p(θ|y) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>, given that the expectation in (3) and the maximization in (4) can be computed exactly. In the convergence proof <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>, it is assumed that θ i converges to θ * as the number of iterations i increases, then the proof consists of showing that θ * is a critical point of p(θ|y).</p><p>However, in practice, either the E-step or M-step or both can be difficult to compute exactly, especially when working with deep learning models. In such cases, we need to rely on approximation methods. For instance, Monte Carlo sampling method can approximate the integration in (3) (the E-step). This technique is known as Monte Carlo EM (MCEM) algorithm <ref type="bibr" target="#b27">[28]</ref>. Furthermore, when the estimation of the global maximiser of Q(θ, θ i ) in ( <ref type="formula" target="#formula_3">4</ref>) is difficult, Dempster et al. <ref type="bibr" target="#b6">[7]</ref> proposed the Generalized EM (GEM) algorithm, which relaxes this requirement with the estimation of θ i+1 , where</p><formula xml:id="formula_4">Q(θ i+1 , θ i ) &gt; Q(θ i , θ i ).</formula><p>The GEM algorithm is proven to have weak convergence <ref type="bibr" target="#b27">[28]</ref>, by showing that p(θ i+1 |y) &gt; p(θ i |y), given that Q(θ i+1 , θ i ) &gt; Q(θ i , θ i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generalized Monte Carlo EM Algorithm</head><p>With the latent variable z, the augmented posterior p(θ|y, z) becomes:</p><p>p(θ|y, z) = p(y, z, θ) p(y, z) = p(z|y, θ)p(θ|y)p(y) p(z|y)p(y) = p(z|y, θ)p(θ|y) p(z|y) ,</p><p>where the E-step is represented by the following Monte-Carlo estimation of Q(θ, θ i ):</p><formula xml:id="formula_6">Q(θ, θ i ) = 1 M M m=1 log p(θ|y, z m ) = log p(θ|y) + 1 M M m=1 (log p(z m |y, θ) -log p(z m |y)),<label>(6)</label></formula><p>where z m ∼ p(z|y, θ i ), for m ∈ {1, ..., M }. In ( <ref type="formula" target="#formula_6">6</ref>), if the label t a m of the m th synthesized sample z m is known, then x a m can be sampled from the distribution p(x a m |θ, y, t a m ). Hence, the conditional distribution p(z|y, θ) can be decomposed as:</p><formula xml:id="formula_7">p(z|y, θ) = p(t a , x a |y, θ) = p(t a |x a , y, θ)p(x a |y, θ),<label>(7)</label></formula><p>where (t a , x a ) are conditionally independent of y given that all the information from the training set y is summarized in θ -this means that p(t a |x a , y, θ) = p(t a |x a , θ), and p(x a |y, θ) = p(x a |θ).</p><p>The maximization of Q(θ, θ i ) with respect to θ for the M-step is re-formulated by first removing all terms that are independent of θ, which allows us to reach the following derivation (making the same assumption as in ( <ref type="formula" target="#formula_1">2</ref>)):</p><formula xml:id="formula_8">Q(θ, θ i ) = log p(θ) + 1 N N n=1 (log p(t n |x n , θ) + log p(x n |θ)) + 1 M M m=1 log p(z m |y, θ)<label>(8)</label></formula><formula xml:id="formula_9">= log p(θ) + 1 N N n=1 (log p(t n |x n , θ) + log p(x n |θ)) + 1 M M m=1 (log p(t a m |x a m , θ) + log p(x a m |θ)).</formula><p>Given that there is no analytical solution for the optimization in (8), we follow the same strategy employed in the GEM algorithm, where we estimate θ i+1 so that Q(θ i+1 , θ i ) &gt; Q(θ i , θ i ).</p><p>As the function Q(•, θ i ) is differentiable, we can find such θ i+1 by running one step of gradient decent. It can be seen that our proposed optimization consists of a marriage between MCEM and GEM algorithms, which we name: Generalized Monte Carlo EM (GMCEM). The weak convergence proof of GMCEM is provided by Lemma 1.</p><p>Lemma 1. Assuming that Q(θ i+1 , θ i ) &gt; Q(θ i , θ i ), which is guaranteed from (8), then the weak convergence (i.e. p(θ i+1 |y) &gt; p(θ i |y)) will be fulfilled.</p><p>Proof. Given Q(θ i+1 , θ i ) &gt; Q(θ i , θ i ), then by taking the expectation on both sides, that is</p><formula xml:id="formula_10">E p(z|y,θ i ) [ Q(θ i+1 , θ i )] &gt; E p(z|y,θ i ) [ Q(θ i , θ i )], we obtain Q(θ i+1 , θ i ) &gt; Q(θ i , θ i )</formula><p>, which is the condition for p(θ i+1 |y) &gt; p(θ i |y) proven from <ref type="bibr" target="#b27">[28]</ref>.</p><p>So far, we have presented our Bayesian DA algorithm in a very general manner. The specific forms that the probability terms in (8) take in our implementation are presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>In general, our proposed DA algorithm can be implemented using any deep generative and classification models which have differentiable optimisation functions. This is in fact an important advantage that allows us to use the most sophisticated extant models available in the field for the implementation of our algorithm. In this section, we present a specific implementation of our approach using state-of-the-art discriminative and generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network Architecture</head><p>Our network architecture consists of two models: a classifier and a generator. For the classifier, modern deep convolutional neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> can be used. For the generator, we select the adversarial generative networks (GAN) <ref type="bibr" target="#b10">[11]</ref>, which include a generative model (represented by a deconvolutional neural network) and an authenticator model (represented by a convolutional neural network). This authenticator component is mainly used for facilitating the adversarial training. As a result, our network consists of a classifier (C) with parameters θ C , a generator (G) with parameters θ G and an Authenticator (A) with parameters θ A . Fig. <ref type="figure">2</ref> compares our network architecture with other variants of GAN recently proposed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. On the surface, our network appears similar to AC-GAN <ref type="bibr" target="#b23">[24]</ref>, where the only difference is the separation of the classifier network from the authenticator network. However, this crucial modularisation enables our DA algorithm to replace GANs by other generative models that may become available in the future; likewise, we can use the most sophisticated classification models for C. Furthermore, unlike our model, the classification subnetwork introduced in AC-GAN mainly aims for improving the quality of synthesized samples, rather than for classification tasks. Nonetheless, one can consider AC-GAN as one possible implementation of our DA algorithm. Finally, our proposed GAN model is similar to the recently proposed triplet GAN <ref type="bibr" target="#b20">[21]</ref> <ref type="foot" target="#foot_0">1</ref> , but it is important to emphasise that triplet GAN was proposed in order to improve the training procedure for GANs, while our model represents a particular realisation of the proposed Bayesian DA algorithm, which is the main contribution of this paper.</p><p>Figure <ref type="figure">2</ref>: A comparison of different network architectures including GAN <ref type="bibr" target="#b10">[11]</ref>, C-GAN <ref type="bibr" target="#b21">[22]</ref>, AC-GAN <ref type="bibr" target="#b23">[24]</ref> and ours. G: Generator, A: Authenticator, C: Classifier, D: Discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimization Function</head><p>Let us define</p><formula xml:id="formula_11">x ∈ R D , θ C ∈ R C , θ A ∈ R A , θ G ∈ R G , u ∈ R 100 , c ∈ {1, .</formula><p>.., K}, the classifier C, the authenticator A and the generator G are respectively defined by</p><formula xml:id="formula_12">f C : R D × R C → [0, 1] K ; (9) f A : R D × R A → [0, 1] 2 ;<label>(10)</label></formula><formula xml:id="formula_13">f G : R 100 × Z + × R G → R D .<label>(11)</label></formula><p>The optimisation function used to train the classifier C is defined as:</p><formula xml:id="formula_14">J C (θ C ) = 1 N N n=1 l C (t n |x n , θ C ) + 1 M M m=1 l C (t a m |x a m , θ C ),<label>(12)</label></formula><p>where</p><formula xml:id="formula_15">l C (t n |x n , θ C ) = -log (softmax(f C (t n = c; x n , θ C ))).</formula><p>The optimisation functions for the authenticator and generator networks are defined by <ref type="bibr" target="#b10">[11]</ref>:</p><formula xml:id="formula_16">J AG (θ A , θ G ) = 1 N N n=1 l A (x n |θ A ) + 1 M M m=1 l AG (x a m |θ A , θ G ),<label>(13)</label></formula><p>where</p><formula xml:id="formula_17">l A (x n |θ A ) = -log (softmax(f A (input = real, x n , θ A )) ;<label>(14)</label></formula><formula xml:id="formula_18">l AG (x a m |θ A , θ G ) = -log (1 -softmax(f A (input = real, x a m , θ G , θ A ))) .<label>(15)</label></formula><p>Following the same training procedure used to train GANs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, the optimisation is divided into two steps: the training of the discriminative part, consisting of minimising J C (θ C ) + J AG (θ A , θ G ) and the training of the generative part consisting of minimising J C (θ C ) -J AG (θ A , θ G ). This loss function can be linked to <ref type="bibr" target="#b7">(8)</ref>, as follows:</p><formula xml:id="formula_19">l C (t n |x n , θ C ) = -log p(t n |x n , θ),<label>(16)</label></formula><formula xml:id="formula_20">l C (t a m |x a m , θ C ) = -log p(t a m |x a m , θ),<label>(17)</label></formula><formula xml:id="formula_21">l A (x n |θ A ) = -log p(x n |θ),<label>(18)</label></formula><formula xml:id="formula_22">l AG (x a m |θ A , θ G ) = -log p(x a m |θ).<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training</head><p>Training the network parameters θ follows the proposed GMCEM algorithm presented in Sec. 3. Accordingly, at each iteration we need to find θ i+1 so that Q(θ i+1 , θ i ) &gt; Q(θ i , θ i ), which can be achieved using gradient decent. However, since the number of training and augmented samples (i.e., N + M ) is large, evaluating the sum of the gradients over this whole set is computationally expensive. A similar issue was observed in contrastive divergence <ref type="bibr" target="#b1">[2]</ref>, where the computation of the approximate gradient required in theory an infinite number of Markov chain Monte Carlo (MCMC) cycles, but in practice, it was noted that only a few cycles were needed to provide a robust gradient approximation. Analogously, following the same principle, we propose to replace gradient decent by stochastic gradient decent (SGD), where the update from θ i to θ i+1 is estimated using only a sub-set of the M + N training samples. In practice, we divide the training set into batches, and the updated θ i+1 is obtained by running SGD through all batches (i.e, one epoch). We found that such strategy works well empirically, as shown in the experiments (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we compare our proposed Bayesian DA algorithm with the commonly used DA technique <ref type="bibr" target="#b18">[19]</ref> (denoted as PMDA) on several image classification tasks (code available at: https: //github.com/toantm/keras-bda). This comparison is based on experiments using the following three datasets: MNIST <ref type="bibr" target="#b19">[20]</ref> (containing 60, 000 training and 10, 000 testing images of 10 handwritten digits), CIFAR-10 <ref type="bibr" target="#b17">[18]</ref> (consisting of 50, 000 training and 10, 000 testing images of 10 visual classes like car, dog, cat, etc.), and CIFAR-100 <ref type="bibr" target="#b17">[18]</ref> (containing the same amount of training and testing samples as CIFAR-10, but with 100 visual classes).</p><p>The experimental results are based on the top-1 classification accuracy as a function of the amount of data augmentation used -in particular, we try the following amounts of synthesized images M : a) M = N (i.e., 2× DA), M = 4N (5× DA), and M = 9N (10× DA). The PMDA is based on the use of a uniform noise model over a rotation range of [-10, 10] degrees, and a translation range of at most 10% of the image width and height. Other transformations were tested, but these two provided the best results for PMDA on the datasets considered in this paper. We also include an experiment that does not use DA in order to illustrate the importance of DA in deep learning.</p><p>As mentioned in Sec. 1, one important contribution of our method is its ability to use arbitrary deep learning generative and classification models. For the generative model, we use the C-GAN <ref type="bibr" target="#b21">[22]</ref> <ref type="foot" target="#foot_1">2</ref> , and for the classification model we rely on the ResNet18 <ref type="bibr" target="#b14">[15]</ref> and ResNetpa <ref type="bibr" target="#b15">[16]</ref>. The architectures of the generator and authenticator networks, which are kept unchanged for all three datasets, can be found in the supplementary material. For training, we use Adadelta (with learning rate=1.0, decay rate=0.95 and epsilon=1e -8) for the Classifier (C), Adam (with learning rate 0.0002, and exponential decay rate 0.5) for the Generator (G) and SDG (with learning rate 0.01) for the Authenticator (A). The noise vector used by the Generator G is based on a standard Gaussian noise. In all experiments, we use training batches of size 100.</p><p>Comparison results using ResNet18 and ResNetpa networks are shown in Figures <ref type="figure">3</ref> and<ref type="figure">4</ref>. First, in all cases it is clear that DA provides a significant improvement in the classification accuracy -in general, larger augmented training set sizes lead to more accurate classification. More importantly, the results reveal that our Bayesian DA algorithm outperforms PMDA by a large margin in all datasets. Given the similarity between the model used by our proposed Bayesian DA algorithm (using ResNetpa <ref type="bibr" target="#b15">[16]</ref>) and AC-GAN, it is relevant to present a comparison between these two models, which is shown in Fig. <ref type="figure">5</ref> -notice that our approach is far superior to AC-GAN. Finally, it is also important to show the evolution of the test classification accuracy as a function of training time -this is reported in Fig. <ref type="figure">6</ref>.</p><p>As expected, it is clear that PMDA produces better classification results at the first training stages, but after a certain amount of training, our Bayesian DA algorithm produces better results. In particular, using the ResNet18 <ref type="bibr" target="#b14">[15]</ref> classifier, on CIFAR-100, our method is better than PMDA after two hours of training; while for MNIST, our method is better after five hours of training.</p><p>It is worth emphasizing that the main goal of the proposed Bayesian DA is to improve the training process of the classifier C. Nevertheless, it is also of interest to investigate the quality of the images produced by the generator G. In Fig.  by the application of Gaussian or uniform noise on pre-determined geometric and appearance transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we have presented a novel Bayesian DA that improves the training process of deep learning classification models. Unlike currently dominant methods that apply random transformations to the observed training samples, our method is theoretically sound; the missing data are sampled from the distribution learned from the annotated training set. However, we do not train the generator distribution independently from the training of the classification model. Instead, both models are jointly optimised based on our proposed Bayesian DA formulation that connects the classical latent variable method in statistical learning with modern deep generative models. The advantages of our data augmentation approach are validated using several image classification tasks with clear improvements over standard DA methods and also over the recently proposed AC-GAN model <ref type="bibr" target="#b23">[24]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of our Bayesian data augmentation algorithm for learning deep models. In this analytic framework, the generator and classifier networks are jointly learned, and the synthesized training set is continuously updated as the training progresses.</figDesc><graphic coords="2,147.52,149.30,316.94,76.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Performance comparison using ResNet18 [15] classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Figure5: Performance comparison with AC-GAN using ResNetpa<ref type="bibr" target="#b15">[16]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b6">7</ref>, we display several examples of the synthetic images produced by G after the training process has converged. In general, the images look reasonably realistic, particularly the handwritten digits, where the synthesized images would be hard to generate</figDesc><table><row><cell></cell><cell>99.8</cell><cell cols="2">Comparison with AC-GAN on MNIST</cell></row><row><cell></cell><cell>99.6</cell><cell></cell></row><row><cell>Accuracy rate</cell><cell>99.2 99.4</cell><cell></cell><cell>AC-GAN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNetpa without DA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNetpa with ours</cell></row><row><cell></cell><cell>99</cell><cell></cell></row><row><cell></cell><cell cols="2">2X</cell><cell>5X</cell><cell>10X</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Increase size of training data</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The triplet GAN<ref type="bibr" target="#b20">[21]</ref> was proposed in parallel to this NIPS submission.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The code was adapted from: https://github.com/lukedeo/keras-acgan</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>TT gratefully acknowledges the support by Vietnam International Education Development (VIED). TP, GC and IR gratefully acknowledge the support of the Australian Research Council through the Centre of Excellence for Robotic Vision (project number CE140100016) and Laureate Fellowship FL130100102 to IR.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning (information science and statistics), 1st edn</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2007</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On contrastive divergence learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Infogan: interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data augmentation for deep neural network acoustic modeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1469" to="1477" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. Series B (methodological)</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive data augmentation for image classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samulowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3688" to="3692" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data augmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Triple generative adversarial nets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/1703.02291</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier gans</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Document Analysis and Recognition</title>
		<meeting>the Seventh International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tools for statistical inference: Observed data and data augmentation methods</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Tanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Statistics</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The calculation of posterior distributions by data augmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">398</biblScope>
			<biblScope unit="page" from="528" to="540" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effective training of a neural network character classifier for word recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="807" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01710</idno>
		<title level="m">Text understanding from scratch</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
