<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovering material information using hierarchical Reformer model on financial regulatory filings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fran?ois</forename><surname>Mercier</surname></persName>
							<email>francois.mercier@lautorite.qc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Autorit? des March?s Financiers / Mila Qu?bec</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Makesh</forename><surname>Narsimhan</surname></persName>
							<email>makeshsreedhar.narsimhan@lautorite.qc.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Autorit? des March?s Financiers / Mila Qu?bec</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discovering material information using hierarchical Reformer model on financial regulatory filings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>NLP</term>
					<term>Transfer Learning</term>
					<term>Financial Reports</term>
					<term>Self-Supervised Learning</term>
					<term>Material Information detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most applications of machine learning for finance are related to forecasting tasks for investment decisions. Instead, we aim to promote a better understanding of financial markets with machine learning techniques. Leveraging the tremendous progress in deep learning models for natural language processing, we construct a hierarchical Reformer ([15]) model capable of processing a large document level dataset, SEDAR, from canadian financial regulatory filings. Using this model, we show that it is possible to predict trade volume changes using regulatory filings. We adapt the pretraining task of HiBERT ([36]) to obtain good sentence level representations using a large unlabelled document dataset. Finetuning the model to successfully predict trade volume changes indicates that the model captures a view from financial markets and processing regulatory filings is beneficial. Analyzing the attention patterns of our model reveals that it is able to detect some indications of material information without explicit training, which is highly relevant for investors and also for the market surveillance mandate of financial regulators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The entire financial services industry is based on the trust investors have in the system. Preserving this trust is a core mandate from all regulators across the world. Machine learning techniques have the potential to improve several aspects in this industry, including trust. However, most interest so far has been on forecasting tasks for investment decisions.</p><p>A better understanding of the markets is useful for everyone in the system. For investors, this translates to a more accurate analysis of the risk-return trade-off. For regulators, it helps to focus their efforts on detecting potential market manipulations and ensuring trust in the overall market.</p><p>In this work, we focus on providing answers to the following research questions:</p><p>(1) Can we predict market events using only a large documentlevel dataset from regulatory filings ? (2) If yes, can we leverage potential predictive power to get insights from markets about filings?</p><p>The main contributions of our work include:</p><p>KDD Workshop on Machine Learning in Finance 2021, August, 2021, Virtual .</p><p>? Using a hierarchical model, inspired by Hibert (Zhang et al. <ref type="bibr" target="#b34">[35]</ref>) and leveraging the efficient Reformer model (Kitaev et al. <ref type="bibr" target="#b14">[15]</ref>) for obtaining good representations long sequences and using these obtained representations successfully for a surrogate downstream task, predicting direction of trade volume changes from potentially long documents ? Providing a qualitative assessment of predictions and using attention patterns to better understand the market point of view on focused documents and sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION 2.1 Background</head><p>"Material change", according to securities legislation, is defined as "a change in the business, operations or capital of the issuer that would reasonably be expected to have a significant effect on the market price or value of any of the securities of the issuer and includes a decision to implement such a change made by the board of directors of the issuer by senior management of the issuer who believe that confirmation of the decision by the board of directors is probable". Regulators require issuers to disclose immediately these "material changes" in order to ensure a level playing field for all investors, and therefore, to ensure trust in financial markets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Formal definition</head><p>In order to better understand potential material changes, we define the following surrogate downstream task: using documents, publicly available for investors, we try to predict the trade volume movement within a 1 business day time horizon from the release date ? for the stock associated with this document. Let ? ? the daily volume for a specific stock for the day ?.</p><formula xml:id="formula_0">daily volume change = ? ?+1 -? ? ? ? = 1 if daily volume change ? 0 0 otherwise<label>(1)</label></formula><p>The model for this surrogate downstream task is designed to extract focus information at sentences level (see section 4). The sentences receiving the most focus are then selected to propose an extractive summary of these documents, ideally with potential further information about material information. The main challenge with using market signals, which can be used as direct inputs for investment decisions, is related to the Efficient Market Hypothesis (EMH), Malkiel and Fama <ref type="bibr" target="#b21">[22]</ref>. The EMH suggests that no one can beat the market without additional risk, which is described with the "No Free Lunch" principle. While there is still a debate regarding the validity of this hypothesis, Fama <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, Malkiel <ref type="bibr" target="#b20">[21]</ref>, research suggests that the semi strong form of EMH, where all past and present public information is reflected in market prices, seems to hold for developed markets, at least to some extent, see Rossi <ref type="bibr" target="#b26">[27]</ref> for literature review. Thus, the consensus is that EMH appears to be a good approximation of market behaviours and one of the main reasons for the difficulty in discovering and using market signals.</p><p>In order to tackle this issue, several strategies are often used like incorporating domain knowledge, such as feature engineering with creation of domain specific lexicon from 10-K filings, Loughran and McDonald <ref type="bibr" target="#b19">[20]</ref>, capturing specific events with open information extraction, Ding et al. <ref type="bibr" target="#b8">[9]</ref>, and also exploiting new dataset, referred as alternative data, such as Twitter to capture investors sentiment, Smailovi? et al. <ref type="bibr" target="#b28">[29]</ref>.</p><p>Due to the strong interest of exploiting market signals for investment decisions, most of research focused on price forecasting or price movement direction, (Chong et al. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref>, Bartram et al. <ref type="bibr" target="#b0">[1]</ref>). Some exceptions can be found for alternative market signals. For price volatility, Theil et al. <ref type="bibr" target="#b29">[30]</ref> suggests SEC 10-k filings have got predictive power. Bordino et al. <ref type="bibr" target="#b2">[3]</ref> suggests web traffic has predictive power for trade volumes. However, to the best of our knowledge, none are leveraging these market signals as interpretation from markets point of views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NLP</head><p>Contextual Word Embeddings. Recently, there has been a shift from using distributional word representations (Mikolov et al. <ref type="bibr" target="#b23">[24]</ref>, Pennington et al. <ref type="bibr" target="#b24">[25]</ref>), which result in a single global representation for each word ignoring their context, to contextual embeddings, where each token is associated with a representation that is a function of the entire input sequence. These context-dependent representations can capture many syntactic and semantic properties of words under diverse linguistic contexts. Previous work (Clark et al. <ref type="bibr" target="#b6">[7]</ref>, Devlin et al. <ref type="bibr" target="#b7">[8]</ref>, Joshi et al. <ref type="bibr" target="#b13">[14]</ref>, Lan et al. <ref type="bibr" target="#b16">[17]</ref>, Liu et al. <ref type="bibr" target="#b18">[19]</ref>, Peters et al. <ref type="bibr" target="#b25">[26]</ref>) has shown that contextual embeddings pretrained on large-scale unlabelled corpora achieve state-of-the-art performance on a wide range of natural language processing tasks, such as text classification, question answering and text summarization.</p><p>Document level embeddings. HIerachical Bidirectional Encoder Representations from Transformers (Zhang et al. <ref type="bibr" target="#b35">[36]</ref>) builds upon BERT (Devlin et al. <ref type="bibr" target="#b7">[8]</ref>) and proposes a pretraining scheme for document level embeddings. To obtain the representation of a document, they use two encoders: a sentence encoder to transform each sentence in the document to a vector and a document encoder to learn sentence representations given their surrounding sentences as context. Both the sentence encoder and document encoder are based on the Transformer encoder (Vaswani et al. <ref type="bibr" target="#b31">[32]</ref>) nested in a hierarchical fashion. They use a variant of the Masked Language Modeling paradigm using sentences as the basic unit instead of words. i.e. they predict masked out sentences given the context. They show that such a pretraining scheme is highly effective and allows them to achieve SOTA results on summarization tasks.</p><p>Self-Attention Variants. Recently, there has been a lot of interest in breaking the quadratic self attention used in transformer (Beltagy et al. <ref type="bibr" target="#b1">[2]</ref>, Kitaev et al. <ref type="bibr" target="#b14">[15]</ref>, Wang et al. <ref type="bibr" target="#b32">[33]</ref>, Zaheer et al. <ref type="bibr" target="#b33">[34]</ref>) with lower time and memory complexities, enabling the processing of larger sequences and giving rise to better models. Simplest methods in this category just employ a sliding window, but in general, most work fits into the following general paradigm: using some other mechanism select a smaller subset of relevant contexts to feed in the transformer and optionally iterate. In this work, we use the Reformer model which introduces the following improvements: (1) using reversible layers to remove the need to store intermediary activations for the backpropagation algorithm; (2) splitting activations inside the feed-forward layers and processing them in chunks;</p><p>(3) approximating attention computation based on locality-sensitive hashing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODEL 4.1 Base hierarchical model: Document representation</head><p>We The base model is the module shared by all the different tasks and is composed of two main submodules, both Reformer models (Kitaev et al. <ref type="bibr" target="#b14">[15]</ref>):</p><p>? The first submodule transforms the sequence of subword embeddings (? </p><formula xml:id="formula_1">) = ReformerModel ???????? (? 1 1 , ? 1 2 , ..., ? 1 |? | )</formula><p>To recap, the base model is defined as follow:</p><formula xml:id="formula_2">(? 1 , ? 2 , ..., ? |? | ) = BaseModel(?)</formula><p>For a graphical visualization, please see figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hierarchical model for the pretraining task</head><p>For the pretraining task, we used Hibert (Zhang et al. <ref type="bibr" target="#b34">[35]</ref>) with the adaptation to long documents by using Reformer (Kitaev et al. <ref type="bibr" target="#b14">[15]</ref>) instead of Transformer (Vaswani et al. <ref type="bibr" target="#b30">[31]</ref>). Let ? represents the set of indices of the masked sentences.</p><p>For the masked sentences with indices ? ? ?, and for each subword tokens with indices ? ? [1; |? ? |], we replace all original tokens ? ? ? by the mask token &lt;MASK&gt;. We then use a base model to get the contextual embeddings {? ? |? ? ? } at sentence level for these masked sentences.</p><p>As the context, represented by ? ? , is a fixed vector for each subword prediction, this context is injected by adding this sentence  embedding as well as ? ? position embedding for the ?th indice for each timestep. We then use a single Reformer layer as follow:</p><formula xml:id="formula_3">? ? ? = EmbeddingLayer(? ? ? ) + ? ? + ? ? ? ?-1 ? = ? ?-1 ? , ? ?-1 ? = (? 1 ? , ? 2 ? , ..., ? ?-1 ? ), ? ?-1 ? = (? 1 ? , ? 2 ? , ..., ? ?-1 ? ) ? ?-1 ? = ReformerLayer(? ?-1 ? , ? ?-1 ? , ? ?-1 ? )</formula><p>From this, we predict the next word given the masked sentence embedding ? ? and the previous words ?</p><formula xml:id="formula_4">1:?-1 ? with ? ? ? ? R ?????_???? * ??????_???? and ? ? ? ? R: ? (? ? ? |? 1:?-1 ? , ? ? ) = softmax(? ? ? ? ?-1 ? + ? ? ? )</formula><p>We then compute the cross entropy loss to be minimized for this task and discard the additional Reformer layer for downstream tasks.</p><formula xml:id="formula_5">Loss ???????????? (?) = - 1 |? | ?? ? ?? |? ? | ?? ?=1 ??? ? (? ? ? |? 1:?-1 ? , ? ? )</formula><p>For a graphical visualization, please see figure <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hierarchical model for the surrogate downstream task</head><p>The classification task aims to incorporate knowledge from market signals into the hierarchical model. For this task, for a document ?, the label ? ? is is constructed as per equation 1.</p><p>On top of the base model, we add a single Reformer layer with global attention mechanism to retrieve a latent representation of the sequence of sentences (? 1 , ? 2 , ..., ? |? | ) for the document ?. The attention weights ? ? used by this Reformer layer will be used later to retrieve the focus of the model.</p><formula xml:id="formula_6">(? 1 , ? 2 , ..., ? |? | ) = ReformerLayer(BaseModel(D))</formula><p>We then treat the first latent embedding ? 1 for the document ? as the document embedding and use a classification head. This head outputs the predicted probability of increase or decrease of daily volume, used later for the binary cross entropy loss:</p><formula xml:id="formula_7">?? = ? (daily volume change ? 0|?) = sigmoid (ClassficationHead(? 1 )) Loss ??? (?) = -? ? * ???( ?? ) + (1 -? ? ) * ???(1 -?? )</formula><p>Finally, to encourage sparsity in the attention weights in additional Reformer layer for this task, in order to increase the focus on the most important sentences, we add a ? 1 regularization term for the attention weights ? ? .</p><formula xml:id="formula_8">Loss ?????? ? ??????? (?) = Loss ??? (?) + ?? ? |? ? |</formula><p>For a graphical visualization, please see figure <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>5.1.1 SEDAR. The System for Electronic Document Analysis and Retrieval (SEDAR)<ref type="foot" target="#foot_1">2</ref> is a filing system developed for the Canadian Securities Administrators<ref type="foot" target="#foot_2">3</ref> (CSA), umbrella organization of provincial and territorial regulators in Canada, to: facilitate the electronic filing of securities information as required by CSA; allow for the public dissemination of Canadian securities information collected in the securities filing process; and provide electronic communication between electronic filers, agents and the CSA. It can be viewed as the Canadian equivalent of EDGAR, the filing system managed by the SEC <ref type="foot" target="#foot_3">4</ref> .</p><p>For the purpose of this work, we collected 3.8M documents in English from 1997 to October 2018. These documents were originally in PDF and have been converted into raw text format. This conversion has resulted in a certain amount of noise into the raw text.</p><p>For the pretraining task, we used 2M documents randomly selected from the overall dataset. All selected documents were before 2018 to prevent information leakage.</p><p>For the downstream task, we only used News releases and Management, Discussion and Analysis (MD&amp;A) documents for firms part of the TSX S&amp;P 60 index as of 30th July 2020. The selected document types are among the most known ones to broadcast financial information that are not part of standardised accounting metrics (e.g.: net income, cash flow from operations, ...). They are therefore highly susceptible to contain new material information.</p><p>The training set contained 14,241 documents before 2018. The validation and test set are constituted by all documents in 2018, from which a random allocation has been performed to have 50% for validation set and 50% for test set. This results into 612 documents for validation set and 613 documents for test set. 89.5% of documents contain less than 512 sentences and 94% sentences contain less than 128 subwords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Market data.</head><p>For the downstream task, we collected daily trade volumes from 2004 from the Bloomberg Terminal for all components from the TSX S&amp;P 60 Index. We then computed the daily change as per equation 1 and joined this dataset with SEDAR using filing dates and ticker codes. Labels are fairly imbalanced with 59%, 63.6% and 57.8% up movements for train, validation and test sets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation details</head><p>5.2.1 Tokenization. We used BBPE (Sennrich et al. <ref type="bibr" target="#b27">[28]</ref>) implementation from HuggingFace<ref type="foot" target="#foot_4">5</ref> with a vocabulary size 8,000. Similar to common practices in natural language processing, for each sentence ? ? , &lt;BOS&gt; and &lt;EOS&gt; are added to represent the beginning of sentence and the end of sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Base model.</head><p>The base model used for following hyperparameters: 8 attention heads, 8 layers (4 for the Reformer model at word level and 4 for the Reformer model at sentence level), intermediary size 2048, maximum sentence length 128 and maximum number of sentences 512. The number of parameters for the base model was 67M. We used the Reformer implementation from HuggingFace<ref type="foot" target="#foot_5">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.3</head><p>Pretraining. We trained our model with 2M documents for 1 epoch with a learning rate of 2e-4 with a linear learning rate schedule and with a batch size 32 using gradient accumulation. The duration of the training was 20 days using a single GPU (11GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Downstream task.</head><p>After hyperparameter search, we trained our best model for 2 epochs with a learning rate of 3e-6 for models with frozen base model encoder and 2e-5 otherwise, with a ? 1 factor 0.1, with a cosine annealing learning rate schedule and with a batch size 32 using gradient accumulation. We used 2 layers MLP for classification head. Each training run lasted less than 1 day using a single GPU (11GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation methodology</head><p>We evaluated our model performance on the downstream task using the test set with ROC-AUC, MCC and F1 metrics. For a better results reporting, we used bootstrapping to obtain confidence intervals by repeating 100 times the evaluation on 300 observations. Classifications thresholds were calibrated to maximize MCC and F1 on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results and discussion</head><p>5.4.1 Predicting market signals from regulatory filings. Our results, summarized in the table 1, confirm the predictive power from only using regulatory filings for trade volume change prediction.</p><p>Our model, without pretraining, outperforms significantly "simple" baselines with the exception of F1 score. F1 score doesn't take into account true negatives, which penalizes our model compared to the majority class prediction model. Despite the former, our model still achieves similar performance statistically. We only keep F1 score for comparison as this metric is commonly reported in related studies.</p><p>As a conclusion, our results support the importance of information from regulatory reporting, at least for News releases and MD&amp;A, for investors in their investment decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Pretraining on document dataset.</head><p>In this experiment, we compare pretrained models with frozen base model with randomly initialized model with frozen base model and a fully trainable model, also randomly initialized.</p><p>As indicated by the table 2, the model pretrained with 2M documents outperforms the fully trainable model with statistical significance on ROC-AUC metric. For MCC metric, the model pretrained with 600K documents is statistically similar to the fully trainable model. For the F1 score, all models are statistically similar, making this metric less useful for ranking. Therefore, models ranking depends on the selected metric, but pretrained models outperform the fully trainable model on all metrics, except MCC for which results are statistically similar.</p><p>Moreover, we observed that the best performing model, in term of validation loss, on the pretraining task was the one pretrained on 600K documents. This model is also the best on the MCC metric. For other metrics, it still remains statistically similar to the one pretrained on 2M documents. Whereas the common knowledge assumes that further pretraining helps, see Liu et al. <ref type="bibr" target="#b17">[18]</ref>, we didn't find it in our case. As model capacity limited by the single GPU memory constraint, our models were smaller in number of parameters than Zhang et al. <ref type="bibr" target="#b34">[35]</ref>. Thus, we believe bigger models may provide better results for this task.</p><p>Furthermore, an interesting observation is that, we noticed that pretrained models have less variance on training set and validation set losses during training than the one with frozen weight and random initialization. This suggests that initialization from pretraining does help to have a more robust training.</p><p>We conclude that the Hibert (Zhang et al. <ref type="bibr" target="#b34">[35]</ref>) pretraining task is slightly beneficial for learning good representation from financial text corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Qualitative analysis.</head><p>For this experiment, we analyzed the best predictions on validation set from our model, meaning the most confident predictions for increase and decrease trade volumes. We reviewed the documents qualitatively and we also looked at the attention weights at the Reformer layer, on top of the base model, which is specialized for the downstream task, see section 4.3. We applied two attention patterns analysis, one using raw attention weights values similar to the study of BERT attention weights from Clark et al. <ref type="bibr" target="#b5">[6]</ref> and one using attention vector norms (Kobayashi et al. <ref type="bibr" target="#b15">[16]</ref>). For both analyses, results were fairly consistent, even if some orders slightly changed.</p><p>Despite our use of ? 1 regularizer to encourage sparsity, attention patterns are quite broad among attention heads. This indicates that there is no strong focus on a single sentence, but rather a broad focus on all sentences. While the focus appears to contain some noise, we noticed the model tends to focus more on: the second sentence of the document; sentences containing information about the firm (ex: ticker, firm name, web site url); statements related to disclosures of risks (ex: "Forward-looking statements in this document include, but are not limited to, statements relating to our financial performance objectives, vision and strategic goals, and include our President and Chief Executive Officers statements."); and also sometimes on mentions to non "Generally Accepted Accounting Principles" (GAAP) measures. While focus on firm name information is quite intuitive, the focus on information about non GAAP measures is quite surprising. Indeed, non GAAP measures have received some attentions from literature and regulators for decades as they may mislead investors in some cases, Bradshaw and Sloan <ref type="bibr" target="#b3">[4]</ref>, Entwistle et al. <ref type="bibr" target="#b9">[10]</ref>, Marques <ref type="bibr" target="#b22">[23]</ref>. The level of noise in the attention pattern remains an open question. Some reasons may be due to the model capacity, our assumption of using only one document to predict market event without further context, or also to the intrinsic evolving nature of markets.</p><p>At the document level, the most confident predictions of decrease of trade volumes appear to be on documents containing no clear new information: such as date confirmation for a result announcement; termination of already announced third party share repurchase program; or third party recognition related to "Environmental, Social and Corporate Governance" (ESG). All of these documents contained no accounting or new business development information. For the most confident predictions of increase of trade volumes, documents contain new accounting information, such as increase of net income or debt refinancing. See figures in appendix for examples. The previously mentioned patterns are consistent with the common knowledge for fundamental analysis.</p><p>To conclude, attention weights patterns appear to be too noisy to be used as a clear extractive summary for documents with our approach. At the document level, our model predictions are inline with accepted knowledge about the focus of the market, despite no prior knowledge about markets and no strong labels. Thus, without the need of labelling documents, we believe that our model can be relevant for finding documents containing potential material information, which could help investors for their risk management and also regulators for their market surveillance mandate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>By leveraging recent progress in natural language processing to process document level large dataset for the financial context, we have proposed a new approach to attempt to discover material information. The key ideas are: efficient deep learning models for long sequences, such as Reformer (Kitaev et al. <ref type="bibr" target="#b14">[15]</ref>) in our work; a We also show the benefits of the HiBERT (Zhang et al. <ref type="bibr" target="#b34">[35]</ref>) pretraining task to improve the quality of sentence level embeddings by using a large unlabelled financial corpus. Finally, while attention patterns learnt by our model are still noisy, we were able to demonstrate the ability to discover material information without prior knowledge, which is relevant to regulators for their market surveillance mandate. With this work, we hope to encourage research between deep learning and finance communities as benefits could deserve all actors in the financial industry, including regulators, and ultimately users. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Base hierarchical model to obtain contextualized sentences embeddings.</figDesc><graphic url="image-1.png" coords="2,104.24,83.68,403.53,171.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>use the following notation: ? = (? 1 , ? 2 , ..., ? |? | ) for the sequence of sentences in a document, ? ? = (? 1 ? , ? 2 ? , ..., ? |? ? | ? ) for the sequence of subword tokens for the ?th sentence and ? ? ? for the ?th subword token for the ?th sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchical model for the pretraining task.</figDesc><graphic url="image-3.png" coords="3,342.17,277.64,193.69,201.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hierarchical model for the surrogate downstream task (classification task).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of documents within most confident predictions of decreased volumes with attentions focus based on attention vector norms. Darker color indicates stronger focus.</figDesc><graphic url="image-7.png" coords="9,53.80,120.77,504.41,54.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-4.png" coords="8,53.80,109.24,504.40,209.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-5.png" coords="8,53.80,319.39,504.41,235.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test set results for volume direction classification with 95% CI. Majority baseline uses the majority class from training set. Random init.: random weights initialization (no pretraining). Ours -random init. [56.9%, 57.8%] [12.8%, 14.5%] [72.5%, 73.2%]</figDesc><table><row><cell>Model</cell><cell>ROC-AUC</cell><cell>MCC</cell><cell>F1</cell></row><row><cell>Random baseline</cell><cell>[49.9%, 51.0%]</cell><cell>[-0.1%, 2.1%]</cell><cell>[53.2%, 54.5%]</cell></row><row><cell>Majority baseline</cell><cell>[50.0%, 50.0%]</cell><cell>[ 0.0%, 0.0%]</cell><cell>[72.7%, 73.4%]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test set results for volume direction classification with 95% CI. Random init.: random weights initialization (no pretraining). Frozen: frozen base model. Ours -frozen+pretrained 600K docs [57.5%, 58.4%] [13.6%, 15.2%] [72.2%, 73.0%] Ours -frozen+pretrained 2M docs [58.0%, 59.0%] [11.6%, 13.1%] [72.2%, 73.0%] hierarchical model for capturing sentence level and document level contextualized embeddings; and a surrogate downstream task to align market signals, volume prediction in our work, with financial filings text dataset.</figDesc><table><row><cell>Model</cell><cell>ROC-AUC</cell><cell>MCC</cell><cell>F1</cell></row><row><cell>Ours -random init.</cell><cell cols="3">[56.9%, 57.8%] [12.8%, 14.5%] [72.5%, 73.2%]</cell></row><row><cell>Ours -frozen+random init.</cell><cell>[55.1%, 56.2%]</cell><cell>[ 3.5%, 5.6%]</cell><cell>[72.7%, 73.4%]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://en.wikipedia.org/wiki/Finance</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.sedar.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.securities-administrators.ca</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.sec.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Tokenizers library: https://github.com/huggingface/tokenizers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Transformers library: https://github.com/huggingface/transformers</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>S?hnke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Bartram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrshad</forename><surname>Branke</surname></persName>
		</author>
		<author>
			<persName><surname>Motahari</surname></persName>
		</author>
		<title level="m">Artificial intelligence in asset management. Centre for Economic Policy Research (CEPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The longdocument transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stock trade volume prediction with yahoo finance user browsing behavior</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kourtellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Billawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 30th International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1168" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gaap versus the street: An empirical assessment of two alternative definitions of earnings</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard E G</forename><surname>Sloan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Accounting Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="41" to="66" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning networks for stock market analysis and prediction: Methodology, data representations, and case studies</title>
		<author>
			<persName><forename type="first">Eunsuk</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chulwoo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="187" to="205" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What does BERT look at? an analysis of BERT&apos;s attention</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4828</idno>
		<ptr target="https://www.aclweb.org/anthology/W19-4828" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08">August 2019</date>
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=r1xMH1BtvB" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using structured events to predict stock price movement: An empirical investigation</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwen</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The voluntary disclosure of pro forma earnings: a us-canada comparison</title>
		<author>
			<persName><forename type="first">Glenn</forename><forename type="middle">D</forename><surname>Gary M Entwistle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chima</forename><surname>Feltham</surname></persName>
		</author>
		<author>
			<persName><surname>Mbagwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of International Accounting Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Market efficiency, long-term returns, and behavioral finance</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Fama</surname></persName>
		</author>
		<idno>eee:jfinec:v:49:y:1998:i:3</idno>
		<ptr target="https://EconPapers.repec.org/RePEc" />
	</analytic>
	<monogr>
		<title level="j">Journal of Financial Economics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="306" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient capital markets: Ii</title>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">F</forename><surname>Fama</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1540-6261.1991.tb04636.x</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1991.tb04636.x" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1575" to="1617" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Applications of deep learning in stock market prediction: recent progress</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Jiang</surname></persName>
		</author>
		<idno>ArXiv, abs/2003.01859</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Attention module is not only a weight: Analyzing transformers with vector norms</title>
		<author>
			<persName><forename type="first">Goro</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuki</forename><surname>Kuribayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sho</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">When is a liability not a liability? textual analysis, dictionaries, and 10-ks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Loughran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1540-6261.2010.01625.x</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2010.01625.x" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="65" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The efficient market hypothesis and its critics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><surname>Malkiel</surname></persName>
		</author>
		<idno type="DOI">https://www.aeaweb.org/articles?id=10.1257/089533003321164958</idno>
		<ptr target="https://www.aeaweb.org/articles?id=10.1257/089533003321164958" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="82" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient capital markets: A review of theory and empirical work</title>
		<author>
			<persName><forename type="first">G</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">F</forename><surname>Malkiel</surname></persName>
		</author>
		<author>
			<persName><surname>Fama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of Finance</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="383" to="417" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sec interventions and the frequency and usefulness of non-gaap financial measures</title>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Accounting Studies</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="549" to="574" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distributed representations of words and phrases and their compositionality. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">10 2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoper</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The efficient market hypothesis and calendar anomalies: a literature review</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Managerial and Financial Accounting</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="285" to="296" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno>CoRR, abs/1508.07909</idno>
		<ptr target="http://arxiv.org/abs/1508.07909" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predictive sentiment analysis of tweets: A stock market application</title>
		<author>
			<persName><forename type="first">Jasmina</forename><surname>Smailovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miha</forename><surname>Gr?ar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nada</forename><surname>Lavra?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>?nidar?i?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human-Computer Interaction and Knowledge Discovery in Complex, Unstructured, Big Data</title>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gabriella</forename><surname>Pasi</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word embeddings-based uncertainty detection in financial disclosures</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Kilian Theil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>?tajner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-3104</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-3104" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Economics and Natural Language Processing</title>
		<meeting>the First Workshop on Economics and Natural Language Processing<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="32" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">HIBERT: document level pre-training of hierarchical bidirectional transformers for document summarization</title>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1905.06566</idno>
		<ptr target="http://arxiv.org/abs/1905.06566" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">HIBERT: document level pre-training of hierarchical bidirectional transformers for document summarization</title>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1905.06566</idno>
		<ptr target="http://arxiv.org/abs/1905.06566" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
