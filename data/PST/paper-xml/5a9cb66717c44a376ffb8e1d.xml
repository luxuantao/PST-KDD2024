<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GLA: Global-local Attention for Image Description</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">L</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">GLA: Global-local Attention for Image Description</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6475F683CE3B7AABD5386CE0D48F4618</idno>
					<idno type="DOI">10.1109/TMM.2017.2751140</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2751140, IEEE Transactions on Multimedia SUBMITTED TO IEEE TRANSACTIONS This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2751140, IEEE Transactions on Multimedia 2 SUBMITTED TO IEEE TRANSACTIONS</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Network</term>
					<term>Recurrent Neural Network</term>
					<term>image description</term>
					<term>Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, the task of automatically generating image description has attracted a lot of attention in the field of artificial intelligence. Benefitting from the development of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), many approaches based on the CNN-RNN framework have been proposed to solve this task and achieved remarkable process. However, there remain two problems to be tackled in that most of existing methods only use imagelevel representation. One problem is object missing that there may miss some important objects when generating the image description and the other is misprediction that it may recognize one object to a wrong category. In this paper, to address the two problems, we propose a new method called global-local attention (GLA) for generating image description. The proposed GLA model utilizes attention mechanism to integrate objectlevel features with image-level feature. Through this manner, our model can selectively pay attention to objects and context information concurrently. Therefore, our proposed GLA method can generate more relevant image description sentences, and achieves the state-of-the-art performance on the well-known Microsoft COCO caption dataset with several popular evaluation metrics -CIDEr, METEOR, ROUGE-L and BLEU-1,2,3,4.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R ECENTLY, image description has become more and more important and received much attention in the field of computer vision. It is a higher-level and more complicated cognitive task than fundamental perceptual tasks, such as image classification task <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, object localization <ref type="bibr" target="#b4">[5]</ref> and detection task <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, image retrieval task <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and so on. The goal of image description is to make computer understand what is happening in a given ⋆⋆ This paper is an expanded version of our work which has been published at the 31st AAAI Conference on Artificial Intelligence (AAAI-17).</p><p>Manuscript received April 24, 2017; revised July 18, 2017; accepted August 16, 2017. This work was supported in part by National Natural Science Foundation of China (61525206, 61572472, 61429201), in part by Beijing Natural Science Foundation (4152050), in part by Beijing Advanced Innovation Center for Imaging Technology (BAICIT-2016009), and in part to Dr. Qi Tian by ARO grant W911NF-15-1-0290 and Faculty Research Gift Awards by NEC Laboratories of America and Blippar. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Chang-Su Kim. (Corresponding author: Sheng Tang) Linghui Li, Sheng Tang, Lixi Deng and Yongdong Zhang are with the Key lab of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China, and also with the University of the Chinese Academy of Sciences, Beijing 100049, China (email: lilinghui@ict.ac.cn, ts@ict.ac.cn, denglixi@ict.ac.cn, zhyd@ict.ac.cn).</p><p>Qi Tian is with Department of Computer Science, University of Texas at San Antonio, One UTSA Circle San Antonio, TX 78249-1604, USA Tel: 1-210-458-5165, Fax: 1-210-458-4437 URL: http://www.cs.utsa.edu/ qitian (e-mail: qitian@cs.utsa.edu).</p><p>Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.</p><p>Baseline: A group of people standing on top of a snow covered slope. Ours: A group of people on skis standing on a snow covered slope.</p><p>Baseline: A group of young men playing a game of soccer. Ours: A boy and a child fly kites in a field.</p><p>Baseline: A person is holding a hot dog in a box. Ours: A picnic table with a plate of food on it.</p><p>Baseline: A woman is eating a doughnut with a fork. Ours: A woman holding a cell phone and smiling. Fig. <ref type="figure">1</ref>. Illustration of problems of the object missing (top left: missing "skis", bottom left: missing "picnic table") and misprediction (top right: mispredicting "kite" as "soccer", bottom right: mispredicting "cell phone" as "doughnut"). The baseline caption is generated by using the LRCN <ref type="bibr" target="#b12">[13]</ref> method. Our caption is generated by utilizing the proposed GLA method.</p><p>image and establish the bridge of image and natural language. So it not only involves computer vision technologies, but also requires natural language processing technologies. Automatically generating image caption is very meaningful for image understanding and can be used to many applications, such as, helping to improve the performance of multi-modal image retrieval, helping the blind navigation and so on.</p><p>For humans, it is very easy to vividly and accurately describe the content of an image. However, for computer, it remains a challenging task to automatically generate image caption in that the model need address the following key points in one model: <ref type="bibr" target="#b0">(1)</ref> Object: the model should be able to accurately recognize objects as much as possible. ( <ref type="formula" target="#formula_3">2</ref>  <ref type="formula" target="#formula_8">4</ref>) Description: the model can express the above components by using grammatically correct natural language sentences. So far, many pioneering approaches have been proposed for solving the task of image description and achieved significant progress. These methods can be broadly divided into three categories according to the way of sentence generation <ref type="bibr" target="#b13">[14]</ref>:</p><p>(1) template-based method; (2) transfer-based method; (3) neural network-based method.</p><p>Template-based method <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> firstly recognizes that what kind of objects there are in an image by using object category classifiers, then recognizes attributes of these objects by using attribute classifiers and relationships among these objects by using relationship classifiers, and finally uses rigid sentence templates to form these components to complete sentences. The kind of method is intuitive, but needs complex data processing and is not enough flexible to generate meaningful sentence due to the limitation of sentence template.</p><p>Transfer-based method <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> firstly retrieves a similar image from annotated image caption dataset, and subsequently directly transfers the description of the retrieved image to the query image. This kind of method can generate more grammatically correct and natural sentences. Nevertheless, the generated sentences may not correctly express the visual content of query image due to the differences between query image and retrieved image.</p><p>Inspired by the recent advance of neural network in image recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref> and natural language processing <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, neural network-based method has been rapidly applied to automatically generate image/video caption <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> and has been made great success. This kind of method is primarily based on the encoder-decoder framework which is proposed for sequenceto-sequence learning via utilizing Recurrent Neural Network (RNN) <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. These methods based on this framework firstly utilize an encoder to transform the input to a fixed length vector representation, and then use a decoder to decode the vector to the corresponding output.</p><p>Although the encoder-decoder framework is very efficient, there is a big limitation of it. Since the encoder needs to encode all the input sequence information to a fix length vector, the vector can not represent the whole sequence information and the former information may be diluted by the later information along the sequence becoming longer. Therefore, the decoder can not obtain enough input information and lead to inaccurate decoding. Furthermore, an attention-based encoder-decoder framework <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b39">[40]</ref> is proposed to solve the problem by introducing attention weights. The attention weights make the encoder dynamically focus on more important parts of input sequence. Compared with the previous encoder-decoder framework, the attention-based encoder-decoder framework just encodes a subset of input sequence to a fixed length vector instead of encoding all input sequence. Thus, the decoder can selectively make the best of input information.</p><p>For image caption methods, the encoder is always based on Convolutional Neural Network (CNN) and the decoder is always based on RNN. These methods extract the image feature using a CNN encoder, and then utilize a language model, RNN or its variants, such as Long Short-Term Memory (LSTM) <ref type="bibr" target="#b40">[41]</ref>, Gated Recurrent Unit (GRU) <ref type="bibr" target="#b27">[28]</ref>, Bi-directional Recurrent Neural Network (BRNN) <ref type="bibr" target="#b41">[42]</ref> et al., to decode the image feature to meaningful sentence. Compared with the previous two kind of methods, neural network-based method can generate more coherent and relevant sentences thanks to the ability of capturing dynamic temporal information of RNN and the good representation ability of CNN.</p><p>However, there are some limitations of most existing neural network-based methods due to the mere use of global representation in image-level. As shown in Fig. <ref type="figure">1</ref>, these limitations can result in the problem of object missing and the problem of object misprediction. Some objects may not be recognized with only utilizing global features. As shown in the top left and bottom left pictures of Fig. <ref type="figure">1</ref>, the "skis" and "picnic table" are missed. Besides, global features are extracted at a coarse level which may result in incorrect recognition and cause the problem of object misprediction during the process of description generation. As shown in the top right and bottom right pictures of Fig. <ref type="figure">1</ref>, the "kite" is mispredicted as "soccer" and the "cell phone" is mispredicted as "doughnut". In order to tackle these issues and obtain more accurate description, we take advantage of local features at object-level to address the problem of object missing. Moreover, inspired by attentionbased encoder-decoder framework, we integrate local features with global features via attention mechanism to reserve context information to address the problem of misprediction.</p><p>Therefore, we propose an global-local attention (GLA) method for image caption, which is our major contribution. The proposed GLA method can selectively focus on semantically more important regions at different time while keeping global context information through integrating local features with global features via attention mechanism. We compare our GLA with several baselines over the well-known MS COCO image caption dataset. Our results show that the proposed GLA method achieves the state-of-the-art performance with different evaluation metrics. A preliminary version of this paper has appeared as a full paper in the Thirty-First AAAI Conference on Artificial Intelligence 2017 <ref type="bibr" target="#b42">[43]</ref>. Compared to our previous conference paper, in this paper, we provide technical details about our previous image caption framework, present extended results with more comparisons and datasets, and offer an indepth analysis of the effect of different number of objects and the property of attention mechanism. Besides, we also apply our method to multi-model retrieval applications, image-tocaption retrieval and caption-to-image retrieval and the experiments show that our model achieves significant improvement compared with exiting methods.</p><p>The remainder of this paper is organized as follows. We first review the existing related works in Section II. Secondly, we present the overview of our framework in Section III. Thirdly, we elaborate on the details of the proposed GLA method in Section IV. Then, we evaluate our GLA on different datasets and analyze the performance in Section V. Finally, we draw our conclusive remarks with discussions for future work in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we mainly review the related work from the following three aspects. First, we review recent image caption methods which are based on deep neural networks and the limitations of these methods. Second, we briefly introduce the attention-based image caption approaches. Finally, we introduce some works on object detection which are related with our proposed methods.</p><p>Deep neural network-based image caption. With the successful application of deep neural network in the task of image recognition and machine translation, the task of automatically generating image description also makes significant progress. There exist several effective methods <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b12">[13]</ref> based on deep neural networks.</p><p>As mentioned above, these approaches consider generating image description as a translation process. They directly translate an image to a sentence via utilizing the encoderdecoder framework <ref type="bibr" target="#b27">[28]</ref> which is originally introduced in machine translation task. In general, this paradigm firstly uses a deep CNN as the encoder which encodes an image to a static representation, and then uses a RNN as the decoder which decodes this static representation to a meaningful sentence. The generated sentence should be grammatically correct and well describe the content of the image as much as possible.</p><p>To address the task of image description, in <ref type="bibr" target="#b31">[32]</ref>, Mao et al. propose a multimodal RNN (m-RNN) model which can also be used for image and sentence retrieval. The proposed m-RNN additionally utilizes a multimodal layer to connect the language model and the CNN together. Similarly, Karpathy et al. <ref type="bibr" target="#b32">[33]</ref> propose an alignment model via a multimodal embedding layer. This alignment model can align segments of sentence with the regions of the corresponding image that they describe. Replacing the basic RNN by LSTM, a more powerful RNN model, Vinyals et al. <ref type="bibr" target="#b30">[31]</ref> propose an endto-end model named NIC by combining deep CNN with LSTM for the problem. Furthermore, to address the problem of "drift away" or "lose track" of the image content, Jia et al. <ref type="bibr" target="#b13">[14]</ref> propose gLSTM model, an alternative extension of LSTM. This model utilizes semantic information extracted from image as input along with the whole image to generate image descriptions. Donahue et al. <ref type="bibr" target="#b12">[13]</ref> propose Long-term Recurrent Convolutional Network (LRCN) which combines convolutional layers and long-range temporal recursion for visual recognition and description.</p><p>However, as shown in Fig. <ref type="figure">1</ref>, we notice that the above mentioned approaches may suffer from the problems of object missing and misprediction in that those methods encode the whole image to a static global feature vector. To overcome these problems, in this paper, we propose to integrate objectlevel features with image-level features for generating image caption via the widely used attention mechanism. In the next section, we brief some related works based on attention mechanism.</p><p>Attention mechanism in image caption and machine translation. Recently, attention mechanism has been widely used and proved to be important and effective in the field of natural language processing <ref type="bibr" target="#b26">[27]</ref> and computer vision <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b44">[45]</ref>. In fact, the essence of attention mechanism is to assign positive weights to different parts to indicate the importance of these parts.</p><p>Attention mechanism is originally introduced in machine translation task <ref type="bibr" target="#b26">[27]</ref>. In <ref type="bibr" target="#b26">[27]</ref>, Bahdanau et al. exploit BRNN with attention mechanism for machine translation. This approach is able to automatically search the part of the source sentence which is most relevant to a target word. Then, attention mechanism is introduced into image/video understanding task. Xu et al. <ref type="bibr" target="#b29">[30]</ref> explore two kinds of attention mechanism for image caption, i.e, soft-attention and hardattention, and analyze how the attention mechanism works in the process of generating image caption via visualization manner. In <ref type="bibr" target="#b36">[37]</ref>, Yao et al. address video caption task through capturing global temporal structure among video frames with a temporal attention mechanism which is based on softalignment method. This temporal attention mechanism makes the model dynamically focus on key frames which are more relevant with the predicted word. ATT <ref type="bibr" target="#b43">[44]</ref> proposes to utilize semantic concept to improve the performance. This method firstly obtains semantic concept proposals by utilizing different approaches, such as, k-NN, multi-label ranking and so on, and then integrates these concept proposals into one vector via the attention mechanism. The integrated vector is finally used to guide language model to generate description. Different from soft/hard attention method <ref type="bibr" target="#b29">[30]</ref> and ATT method <ref type="bibr" target="#b36">[37]</ref>, our proposed GLA method integrates local representation at object-level with global representation at imagelevel through attention mechanism, whose aim is to address aforementioned problems of object missing and misprediction. Due to these methods which use only global frame-level features which cannot avoid problems of object missing and misprediction. Instead of considering semantic concepts or attributes used in ATT <ref type="bibr" target="#b43">[44]</ref>, we directly apply image visual feature with attention mechanism to image caption. RA <ref type="bibr" target="#b44">[45]</ref> proposes a complicated pipeline to obtain important regions from selective search region proposals <ref type="bibr" target="#b45">[46]</ref> and combines them with scene-specific contexts to generate image caption. Compared with ATT and RA methods, our GLA method is simpler and the performance is much better than RA method.</p><p>Object Detection. With the great success achieved by deep learning technology, object detection has also made significant progress. R-CNN <ref type="bibr" target="#b46">[47]</ref> stands out as one of the notable landmarks in the process of object detection tasks. It takes advantage of high quality region proposals (selective search method <ref type="bibr" target="#b45">[46]</ref>) and CNN features. This pipeline mainly contains four procedures: (1) Extracting region proposals which likely contain objects via region proposal methods; (2) Extracting CNN features of these region proposals via CNNs;</p><p>(3) Classifying these proposals through classifier trained with CNN features; (4) Localizing these objects via bounding box regression methods.</p><p>However, this kind of framework is time consuming due to the four distinct steps. To reduce the computing time and improve the accuracy of detection, SPP-Net <ref type="bibr" target="#b47">[48]</ref>, Fast R-CNN <ref type="bibr" target="#b48">[49]</ref> and DeepID-Net <ref type="bibr" target="#b7">[8]</ref> et al. are developed. These methods integrate the last three steps into one end-to-end framework which can simultaneously complete classification and bounding box regression.</p><p>Although these improved methods have improved the performance of object detection via the end-to-end part, the region proposal generated process also isolates with it and this process is the most time-cosuming. Thus, some real-time methods, such as, YOLO <ref type="bibr" target="#b8">[9]</ref>, Faster R-CNN <ref type="bibr" target="#b5">[6]</ref> and SSD <ref type="bibr" target="#b49">[50]</ref> et al., are proposed. These methods consider to generate region proposals along with the classification in one forward pass of CNN which can make the object detection more efficient. They can directly generate the region proposal bounding boxes which makes them possible to detect objects in near real-time.</p><p>Among these methods, the speed of YOLO is faster than Faster R-CNN, but the accuracy is much lower than Faster R-CNN. Therefore, we choose to use Faster R-CNN for detecting objects in our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FRAMEWORK OVERVIEW</head><p>As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, we propose a novel image caption framework for automatically describing the content of an image based on object attention. The proposed framework mainly consists of three important processes.</p><p>First of all, we propose to make full use of global feature and local features for automatically generating image description. The global feature which contains the context information of an image is extracted from deep CNN. The local features which keep more precise object information are extracted through Faster R-CNN method.</p><p>Secondly, we integrate global feature with local features via attention mechanism. In this step, the attention mechanism can choose more important objects and assign them greater weights. This makes the model selectively focus on some important entities as well as considering the image's context information.</p><p>Finally, we utilize a language model to generate a description sentence with the dynamic integrated feature. In this paper, we use a stacked two-layer LSTM as the language model which can effectively capture the long-term sequence information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. GLOBAL-LOCAL ATTENTION MODEL</head><p>In this section, we introduce our proposed GLA method for automatically generating image description in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Global and Local Features Extraction</head><p>To make the computer understand an image, we firstly need to extract image features. There are mainly two kinds of features, global feature and local feature. Global feature reserves the comprehensive information of an image which is a kind of coarse-grained feature. Local feature usually contains the fine-grained information of objects. In our model, we explore the effect of the two types of feature in the task of image caption. Benefitting from the good performance of CNNs in the task of image classification and object detection, we consider extracting global feature with VGG16 model <ref type="bibr" target="#b2">[3]</ref> and local features with Faster R-CNN <ref type="bibr" target="#b5">[6]</ref>.</p><p>For global feature denoted as Gf , we represent it as a 4096dimension vector, the fc7 layer feature extracted from VGG16 net. This VGG16 net is trained on ImageNet classification dataset. For local features, we select k objects in an image and their features are denoted as {Lf 1 , ..., Lf k } which is a set of 4096-dimension vectors extracted from fc7 layer for each object bounding box. These k objects are the top-k objects which are chosen according to the classification confidence scores obtained from Faster R-CNN. The Faster R-CNN model is pre-trained on ImageNet classification dataset and then finetuned on the MS COCO detection dataset.</p><p>Therefore, each image is finally represented by a set of 4096-dimension vectors I={Gf ,Lf 1 ,...,Lf k }. In our experiments, we set k to 10 since the number of object contained in an image is usually below 10 and the experiments verify the effectiveness of this conjecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Global-local Attention Mechanism</head><p>Obviously, for a generated sentence, each word usually corresponds to different entity of the image. Therefore, when generating an image description, the model should be able to focus on different entities in each time step. For this purpose, we adopt attention mechanism to integrate the local features with the global features according to the following Eq. 1:</p><formula xml:id="formula_0">Ψ (t) (I) = α (t) 0 Gf + k ∑ i=1 α (t) i Lf i , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where Ψ t (I) is the final integrated image representation at time t. α (t) i denotes the attention weight of each feature i at time t and satisfies the constraint ∑ k i=0 α (t) =1. During sentence generation procedure, this attention mechanism dynamically weights the importance of each entity by assigning it with one positive weight α (t) i . Through this manner, the model can selectively focus on the salient objects and keep the scene information at the same time.</p><p>The weight α</p><formula xml:id="formula_2">(t)</formula><p>i measures the importance of each feature i and relevancy to the history information at each time step t. Thus, it can be determined by the previous output h t-1 and the feature f i ∈ {Gf, Lf 0 , ..., Lf n } via the following equations:</p><formula xml:id="formula_3">β (t) i = w T φ(W h h (t-1) + W o f i + b), (<label>2</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">α (t) i = β (t) i ∑ n j=0 β (t) j , (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where</p><formula xml:id="formula_7">β (t)</formula><p>i denotes the relevance score of each feature f i with the generated word.</p><p>The weight α (t)</p><p>i is computed through normalizing the relevance score β (t) i with softmax regression. h (t-1) contains the history information which is output by the previous hidden state of RNN. W, W h , W o and b are the parameters shared by all features at all time steps. φ is the element-wise Hyperbolic Tangent activation function which is defined as the Eq. 4:</p><formula xml:id="formula_8">tanh(x) = e x -e -x e x + e -x .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Description Generation</head><p>For language model, we choose to use a stacked twolayer LSTM to generate sentence due to the outstanding performance of RNN in the task of neural machine translation <ref type="bibr" target="#b26">[27]</ref>. LSTM is one kind of advanced RNN which can effectively capture dynamic long-term temporal information with a distinctive unit.</p><p>We first introduce the basic RNN structure as shown in the top row of Fig. <ref type="figure">3</ref>. A basic RNN essentially maintains a hidden state h t in each time step t and overwrites this hidden state with input x t and the previous hidden state h t-1 . Usually, we use Back-Propagation Through Time (BPTT) algorithm to train RNNs. However, the gradients of RNNs tend to vanish with this BPTT algorithm due to the chain rule of derivative. Thus, the basic RNNs suffer from the problem of long-range dependency caused by vanshing and exploding gradients in the training process. Then, to solve this issue, the LSTM is designed to combat them via a gating mechanism. The bottom figure of Fig. <ref type="figure">3</ref> shows the structure of a LSTM unit. A LSTM unit mainly consists of four part, a memory cell, input gate, output gate and forget gate. The forget gate determines how much the history information will be retained. The input gate decides what new information should be reserved and the output gate determines how much of the hidden state should be exposed to next process. All the three gates receive the input information x t (word or input image) and the previous hidden state h t-1 and input these information into activation functions. The memory cell also has the same input with these gates, but with different activation functions. In our model, the LSTM is trained for predicting each word s t with which the model composes a complete description sentence. To predict each word s t , the model should know the image visual information I as well as the predicted words {s 0 , s 1 , ..., s t-1 }. This process can be defined by predicting the word probability on the condition that I and {s 0 , s 1 , ..., s t-1 }, i.e. p(s t |I, s 0 , s 1 , ..., s t1 ), are known. Specifically, we only input the image information into the second LSTM layer. The detailed operations of the first LSTM layer is listed as following Eq. 5:</p><formula xml:id="formula_9">x t = w x s t i t 1 = σ(w 1 is x t + w 1 ih h (t-1) 1 + b 1 i ) f t 1 = σ(w 1 f s x t + w 1 f h h (t-1) 1 + b 1 f ) o t 1 = σ(w 1 os x t + +w 1 oh h (t-1) 1 + b 1 o ) c t 1 = f t 1 ⊗ c t-1 1 ⊕ i t 1 ⊗ ϕ(w 1 cs x t 1 + w 1 ch h t-1 1 ) h t 1 = o t 1 ⊗ c t 1<label>(5)</label></formula><p>where each word is represented as an one-hot vector s t whose dimension is equal to the vocabulary size. The second LSTM layer is computed as the following Eq. 6:</p><formula xml:id="formula_10">I t = Ψ t (I) i t 2 = σ(w 2 iI I t + w 12 ih h (t-1) 1 + w 2 ih h (t-1) 2 + +b 2 i ) f t 2 = σ(w 2 f I I t + w 12 f h h (t-1) 1 + w 2 f h h (t-1) 2 + b 2 f ) o t 2 = σ(w 2 oI I t + w 12 oh h (t-1) 1 + w 2 oh h (t-1) 2 + b 2 o ) c t 2 = f t 2 ⊗ c t-1 2 ⊕ i t 2 ⊗ ϕ(w 2 cs x t 2 + w 2 ch h t-1 2 ) h t 2 = o t 2 ⊗ c t 2 P (s t |I, s 0 , s 1 , ..., s t-1 ) = Sof tmax(w p h t 2 )<label>(6)</label></formula><p>where w * and b * are parameters that the model should learn and shared by all time steps in our model.</p><p>Our final goal is to obtain an optimal description sentence of the input image. This can be generated by computing the probability of all the observing words. As the Eq. 7 shows, the probability of a sentence is the product of probability of each word given an image and all the words before current time step.</p><formula xml:id="formula_11">p(s 0 , s 1 , ..., s m ) = m ∏ i=0 p(s i |I, s 0 , .., s i-1 )<label>(7)</label></formula><p>Thus, the loss function final can be defined as Eq. 8. It is the sum of the probability log likelihood of each word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L(I, S)</head><formula xml:id="formula_12">= m ∑ i=0 log(p(s t |I, s 0 , ..., s i ))<label>(8)</label></formula><p>We use stochastic gradient descent to optimize the above objective function over the whole training set. The configuration of hyperparameters is introduced in the next section. In test procedure, there are two strategies for generating description sentence of a given image. The first strategy is essentially a greedy method. At each time step, we sample the next word which has the maximum probability from the probability distribution until we sample the sign word or the sentence reaches to the maximum length. The second strategy is beam search method. At each time step, we select the top-m best sentences and then sample new best top-m sentences based on the previous top-m sentences. In this paper, we sample sentences by using these two strategies to evaluate our method. Particularly, in beam search strategy, we can obtain the best sentence when the value of m is set to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we first introduce our implement details. Second, we brief several popular metrics used in our experiments. Third, we introduce the datasets used for validating our proposed method. Finally, we introduce our experiments configuration and analyze the results in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Setup</head><p>LSTM Hyperparameter Configuration. We implement our global-local attention model based on LRCN framework <ref type="bibr" target="#b12">[13]</ref>, an open-source implementation of RNN. For hyperparameters of training the LSTM, we set the momentum of stochastic gradient descent to 0.9. The learning rate is initially set to 0.01 and then is decreased every 20000 iterations. The final training iteration is set to 120, 000. The clip gradient used in LSTM is set to 10. The node number of LSTM's hidden layer is set to 1000.</p><p>Selection of Top-k Object. In our method, one of the important part is how to select the top-k objects. In order to obtain better objects, in our experiments, we firstly use Faster R-CNN to obtain some region proposals and then use Non-Maximum Suppress (NMS) algorithm to pick out better region proposals. After executing NMS, we sort the remained region proposals according to these regions confidences in descending order. Finally, we choose the top-k region proposals as the final object bounding boxes. If there are no more than k region proposals after NMS, we randomly sample some region proposals from the remainders to make enough k objects.</p><p>Finetuning of Faster R-CNN. In order to extract better object features, we firstly finetune Faster R-CNN model over MS COCO object detection dataset. The base model is pretrained on ImageNet object detection dataset. MS COCO object detection task shares the same images with image caption task. Therefore, we keep the same splits with the image caption dateset for training which will be introduced in the following introduction of dataset. The finetuning process is almost the same with the pre-training process <ref type="bibr" target="#b5">[6]</ref>. The initial learning rate for finetuning is set to 0.001. The momentum of stochastic gradient descent is set to 0.9 and the weight decay is set to 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>There have been proposed various criteria for evaluating the generated sentences. However, how to evaluate the quality of the descriptions remains to be a challenging problem. Therefore, in order to accurately verify the performance of our model, we use multiple metrics to evaluate the GLA method, i.e. METEOR <ref type="bibr" target="#b50">[51]</ref>, ROUGE-L <ref type="bibr" target="#b51">[52]</ref>, CIDEr <ref type="bibr" target="#b52">[53]</ref> and BLEU-1,2,3,4 <ref type="bibr" target="#b53">[54]</ref>.</p><p>BLEU is the most popular metric for evaluating the generated sentence in machine translation task. This metric is only based on the n-gram precision. In our experiments, we validate the performance with 1,2,3,4-gram individually. To fix some of the problems of BLEU metric, METEOR is designed which is based on the harmonic mean of unigram precision and recall. Different with the BLEU metric, the METEOR seeks correlation at the corpus level. ROUGE-L is designed for measuring the common subsequence with maximum length between target sentence and source sentence. CIDEr is used to evaluate the generated descriptions using human consensus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Datasets</head><p>In order to prove the effectiveness of our proposed method, we conduct several experiments on the following three popular datasets.</p><p>Flickr 8k Dataset. The first dataset is the popular Flickr 8k dataset <ref type="bibr" target="#b54">[55]</ref> which consists of 8,000 images in total and each image is annotated with 5 English sentences. In order to fairly compare with existing methods, we keep 6,000 images for training, 1,000 images for validation and 1,000 images for testing which is the same splits with the existing methods.</p><p>Flickr 30k Dataset. The second dataset is the well-known Flickr 30k dataset <ref type="bibr" target="#b55">[56]</ref>, an extension of the previous Flickr 8k Dateset. This dataset has more images which totally consists of 31,014 images and also is annotated with 5 sentences. With the same goal with the Flickr 8k datasets, we keep 29,000 images for training, 1,000 images for validation and 1,014 images for testing, also same with other existing methods.</p><p>MS COCO Dataset. The third dataset is the MS COCO caption dataset <ref type="bibr" target="#b56">[57]</ref>, a well-known large scale dataset. This dataset consists of 82,783 training images and 40,504 validation images. Each image is annotated with 5-sentences in English which are written by AMT workers. Compared with other existing image caption dataset, such as, flickr8k and flickr30k, MS COCO dataset has much more annotated images. Hence, we choose the MS COCO dataset as the main dataset in our experiments. To fairly compare with state-ofthe-art methods, we keep the same splits as the previous work <ref type="bibr" target="#b32">[33]</ref>, i.e, 5,000 images sampled from validation dataset for validation, another 5,000 images sampled from validation for testing, and the left 113, 287 images for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Image Caption Experiments and Results</head><p>The determination of the number of top-k objects. How many objects to choose is also important for generating description. In order to obtain proper number of objects, in the first section, we analyze the effect of different number of objects for image caption. Here, we choose 5, 10, 15 and 20 objects of each image to verify the performance respectively. From this figure, we observe that it achieves best performance when we choose top-10 objects in each image. This may because that the average number of objects in the dataset is blew 10. When the number of objects is smaller, there may miss some important object which causes incomplete description. On the contrary, when the number of objects is larger, there may have some non-objects to influence the prediction. According to this experiment, we finally choose that k is equal to 10 to conduct our following experiments.</p><p>Evaluation on Image-level and Object-level Information for Image Description on MS COCO Dataset. In the task of image recognition, global features and local features have been proved to be important. Therefore, in this section, we first conduct three experiments to explore the effect of local features, global feature and the fusion form of these two features for automatically describing image. The detail configuration of the three experiments are listed as follows:</p><p>• GloFeat: Only employing the image-level feature Gf which is extracted from VGG16. • LocAtt: Only employing object-level features {Lf 1 , ..., Lf k } which are extracted from Faster R-CNN, and then integrating them with attention mechanism. • GloLocAtt: Employing these two kinds of features, and then integrating them with attention mechanism to generate description. Tab. I shows the results of these experiments. From the comparison result of the GloFeat experiment with LocATT experiment, we observe that it has better performance when using global feature than only using local features. Our conjecture is that global feature can provide the relationship of objects which is important for describing the content of an Baseline: A group of birds are standing in the water.</p><p>Ours: A group of ducks swimming in a lake.</p><p>Baseline: A group of people playing a game of frisbee.</p><p>Ours: A group of people playing soccer on a field.</p><p>Baseline: A man is riding a elephant with a trunk.</p><p>Ours: A man is standing next to a large elephant.</p><p>Baseline: A bear is walking through a tree in the woods.</p><p>Ours: A bear is sitting on a tree branch.</p><p>Baseline: A sheep standing in a field with a green and white sheep.</p><p>Ours: A sheep standing in a field with a sheep.</p><p>Baseline: A brown cow standing in a field of grass.</p><p>Ours: A group of cows standing next to each other.</p><p>Baseline: A man riding a wave on top of a surfboard.</p><p>Ours: A man is standing on a mountain overlooking a mountain.</p><p>Baseline: A table with a plate of food and a cup of coffee.</p><p>Ours: A table with a bunch of fruit and a glass of wine.</p><p>Fig. <ref type="figure">5</ref>. Illustrating the comparison of some sampled image caption results with existing method results. The baseline caption is generated by LRCN method which only exploits image-level features. The second caption is generated with our proposed GLA method.</p><p>image. However, the relationship can not be captured by only using local features. By comparing GloLocAtt experiment with the other two experiments, we find that the fusion form with the proposed approach can achieve the best performance. The reason may be that the GloLocAtt model can capture both local object information and global context information at the same time.</p><p>Evaluation on Dropout Mechanism for Our Proposed Method on MS COCO Dataset. In our GloLocAtt experiment, we note that there would be overfitting which does not appear in the GloFeat experiment. This is because that our "GloLocAtt" model needs to compute attention weights which introduces an additional two million parameters. Therefore, compared with "GloFeat" model, "GloLocAtt" model is more complex and prone to overfitting. Hence, we add some dropout layers to address this issue. Dropout <ref type="bibr" target="#b57">[58]</ref> is proved to be an import mechanism for regularizing deep network which is used to overcome overfitting. As introduced in <ref type="bibr" target="#b58">[59]</ref>, we explore the effect of dropout in our model with different forms. Additionally, to keep consistent with the dimension of LSTM hidden layer, we reduce the integrated 4096-d features to 1000-d via one linear transform layer. Therefore, we also conduct three experiments whose configurations are shown in the following:</p><p>• GloLocAttEmb: Employing integrated feature and adding one linear transform layer to reduce the feature dimension.   experiment, adding one dropout layer after the first LSTM layer. Tab. II shows the comparison results of the three experiments. Compared with GloLocAtt experiment, the performance of GloLocAttEmb experiment has some improvement in that the linear transform layer makes the feature more distinctive. Furthermore, the dropout experiments improve the performance since dropout can reduce overfitting in some degree. The performance is better by adding two dropout layers.</p><p>Comparison with the State-of-the-art Methods. In the final, we compare our GLA method with several state-ofthe-art methods, such as, g-LSTM <ref type="bibr" target="#b13">[14]</ref>, NIC <ref type="bibr" target="#b30">[31]</ref>, m-RNN <ref type="bibr" target="#b31">[32]</ref>, LRCN <ref type="bibr" target="#b12">[13]</ref>, DeepVS <ref type="bibr" target="#b32">[33]</ref>, soft/hard attention <ref type="bibr" target="#b29">[30]</ref> and ATT <ref type="bibr" target="#b43">[44]</ref>, on all the above three datasets. In the following tables, we use "GLA" to represent our proposed method whose configuration is the same with "GloLocAttEmb+TwoDrop" experiment. We sample sentences with greedy strategy in all the above experiments. Since beam search can approximately obtain sentence with the maximum probability, we try beam search in our experiment. When the m is set to 3, we can get the best performance. The best result is denoted as "GLA+BEAM3".</p><p>Among the above state-of-the-art methods, there are some differences with each other. The first difference is the encoder used for representing images. LRCN exploits AlexNet to extract image-level features. ATT, g-LSTM, and NIC use GoogLeNet to extract image-level features. DeepVS, m-RNN and soft/hard attention use the same encoder with our to obtain image-level representation. To make fair comparison, we first compare our approach with methods which use the same encoder. The results show that the performance is improved significantly on different metrics.</p><p>The second difference is the decoder used for generating sentence. Same as our method, LRCN employs a stacked twolayer LSTM to generate image description. NIC, g-LSTM, and soft/hard attention use one layer LSTM network as language model. ATT and m-RNN use the basic RNN as decoder. DeepVS employs the BRNN to generate image caption. Here, compared with the same decoder, the performance of our GLA method also has better performance.</p><p>We show our results over MS COCO dataset in Tab. III. By comparing our GLA model with the existing methods on MS COCO dataset, we note that our approach achieves best performance in that our method can capture more detailed object information. From Fig. <ref type="figure">5</ref> which illustrates the sampled images and their descriptions generated by LRCN <ref type="bibr" target="#b12">[13]</ref> model and GLA model, we can see that GLA model can generate more relevant descriptions. The results show our method can solve the problems of objects missing and misprediction in some degree.</p><p>Then we show our results over Flickr 8k dataset in Tab. IV and over Flickr 30k dataset in Tab. V. However, from those two tables, we can find that the performance is decreased compared with the existing method. There are two reasons to account for this phenomenon. On the one hand, our model can not jointly train the feature extractor and the language model. On the other hand, the flickr datasets lack object information which causes that we can not finetune the Faster R-CNN on flickr dataset. This two reasons cause that our model can not be well generalized for flickr datasets.</p><p>Visualization of the Attention Weights. As we have known, attention mechanism can dynamically weight each feature by assigning it with one positive value α (t) i along with the sentence generation. This property makes the model selectively focus on some more important objects at different time and consider the context information at the same time. In order to conveniently observe this property, we visualize the attention weight of each object along with the description generation.</p><p>As shown in Fig. <ref type="figure" target="#fig_6">6</ref>, we represent the description generated by our proposed GLA method and some object boxes as local information used in the method. The line charts illustrate the  attention weight of each entity when the model generates a certain word. For example, in the first row, the blue line shows the weights of all the 11 entities (10 objects and the whole image) when generating the word "sheep". The red line shows the weights of these entities when generating the word "field". Each point of horizontal axis corresponds one entity of the right image according to the number.</p><p>Through these subfigures, we can clearly observe that the attention mechanism assigns the objects with greater weights which are relevant to the predicted word. For example, in the first row, when generating the word "sheep", the entities with number two, three and four have greater weights. This proves that our model can dynamically focus on more relevant objects when describing the context of an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Retrieval Experiments and Results over MS COCO Dataset</head><p>The improvement of image caption can promote the multimodal retrieval task. In addition to our previous work <ref type="bibr" target="#b42">[43]</ref>, we also conduct two kinds of retrieval experiments, image-tocaption retrieval task and caption-to-image retrieval task, for further verifying the performance of proposed method over MS COCO dataset. Here, we use two configurations of test set. One test is the same with caption task which consists of 5,000 images. The other consists of 1,000 images sampled from the original 5,000 images. We also use R@K (k=1, 5, 10) and Med r for evaluation of the two tasks. R@K is the recall rate with top K return results. The higher the R@K is, the better the performance of retrieval is. Med r is the median rank of the first retrieved groundtruth results. In contrast with R@K, the lower the Med r is, the better the performance of retrieval is.</p><p>Tab. VI shows the comparison results of our method with previous methods. Image-to-caption retrieval is to find the most relevant caption given an image. The results show that our method outperforms these methods over two test datasets. For 1k validation dataset, our method improves over 15% of R@1. For 5k validation dataset, our method improves about 10% of R@1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>R@1 R@5 R@10 Med r DeepVS 1k <ref type="bibr" target="#b32">[33]</ref> 38.4 69.9 80.5 1.0 DeepVS 5k <ref type="bibr" target="#b32">[33]</ref> 16.5 39.2 52.0 9.0 m-RNN 1k <ref type="bibr" target="#b31">[32]</ref> 41.0 73.0 83.5 2.0 GLA (ours) 1k 55.0 81.5 89.9 1.0 GLA (ours) 5k 27.8 56.5 69.9 4.0</p><p>Tab. VII shows the results of caption-to-image retrieval. Caption-to-image retrieval is to find the most relevant image for a given image description. Our model also performs well in this task. Compared with the existing methods, our method improves over 10% of R@1 in 1k test dataset and improves about 8% of R@1 in 5k test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We propose a novel method for generating image description which achieves better performance on the MS COCO benchmark compared with the previous approaches. Our proposed method combines image-level and object-level information via attention mechanism. Compared with state-of-theart approaches, our method not only can capture the global </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>R@1 R@5 R@10 Med r DeepVS 1k <ref type="bibr" target="#b32">[33]</ref> 27.4 60.2 74.8 3.0 DeepVS 5k <ref type="bibr" target="#b32">[33]</ref> 10.7 29.6 42.2 14.0 m-RNN 1k <ref type="bibr" target="#b31">[32]</ref> 29.0 42.2 77.0 3.0 GLA (ours) 1k 40.9 75.0 85.9 2.0 GLA (ours) 5k 18.9 46.2 60.5 6.5</p><p>information, but also obtain local object information. We prove this by doing quantitative analysis of the attention weights along with the sentence generation procedure. Consequently, our method generates more relevant and coherent natural language sentences which can describe the context of images. However, our current GLA method is not end-to-end which can not jointly train the CNN part and the language model. Thus, we will try how to integrate the object detector with image feature extractor so as to train and test our model endto-end.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) Relationship: the model needs to correctly identify what relationships among objects have. (3) Scene: the model can determine what scenes those objects are in. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our proposed GLA image caption framework based on objects attention mechanism.</figDesc><graphic coords="4,48.96,52.43,514.56,175.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 3. Illustration of an un-fold basic RNN and a LSTM unit. σ represents logic sigmoid function. ϕ represents hyperbolic tangent function. ⊙ represents the multiplication operation and ⊕ represents sum operation.</figDesc><graphic coords="5,324.52,172.15,225.97,100.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the performance with different top-k objects on six wellknown metrics -BLEU1, BLEU2, BLEU3, BLEU4, CIDEr and METEOR. The abscissa represents the top-k type. The ordinate denotes the accuracy of image caption. Each subfigure represents the performance of different k on one of these metrics. Each color denotes the performance of different metric on one kind of k. We explore the performance by using four different k -5,10,15,20.</figDesc><graphic coords="7,54.93,531.88,120.64,87.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 illustrates the results of MS COCO image caption dataset with different top-k objects via several metrics. Each subfigure shows the accuracy of different k with the same metric and each color shows the accuracy of the same k with different metrics. These experiments use greedy strategy to generate description sentences.From this figure, we observe that it achieves best performance when we choose top-10 objects in each image. This may because that the average number of objects in the dataset is blew 10. When the number of objects is smaller, there may miss some important object which causes incomplete description. On the contrary, when the number of objects is larger, there may have some non-objects to influence the prediction. According to this experiment, we finally choose that k is equal to 10 to conduct our following experiments.Evaluation on Image-level and Object-level Information for Image Description on MS COCO Dataset. In the task of image recognition, global features and local features have been proved to be important. Therefore, in this section, we first conduct three experiments to explore the effect of local features, global feature and the fusion form of these two features for automatically describing image. The detail configuration of the three experiments are listed as follows:</figDesc><graphic coords="7,178.93,347.01,120.64,87.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>SUBMITTED TO IEEE TRANSACTIONS ON MULTIMEDIA, VOL. **, NO. **, 2017 Caption: A group of sheep grazing in the field. Caption: A pizza on a plate with a glass of beer. Caption: A couple of young girls playing a game of baseball. Caption: A woman holding an umbrella in a market.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Illustration of the attention weights of objects. In each row, the left subfigure shows the sampled image and its description generated by our method. Each box corresponds to one object and the soft blue box denotes the whole image. These boxes in each image are a subset of the top-k objects which are generated by Faster R-CNN. The right subfigure is a line chart which shows the weight of each entity when generating a certain word.</figDesc><graphic coords="10,158.72,460.72,140.98,103.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>EXPERIMENTS FOR EXPLORING THE EFFECT OF IMAGE DESCRIPTION WITH GLOBAL FEATURE, LOCAL FEATURES AND FUSION OF THE TWO FEATURES OVER MS COCO DATASET RESPECTIVELY .</figDesc><table><row><cell>Method</cell><cell>Bleu1</cell><cell cols="2">Bleu2 Bleu3</cell><cell cols="2">Bleu4 METEOR</cell><cell>CIDEr</cell><cell cols="2">ROUGE-L</cell></row><row><cell>GlobFeat</cell><cell>67.3</cell><cell>49.1</cell><cell>34.4</cell><cell>24.0</cell><cell>22.2</cell><cell>76.9</cell><cell>49.2</cell></row><row><cell>LocAtt</cell><cell>66.3</cell><cell>47.5</cell><cell>33.1</cell><cell>22.9</cell><cell>21.6</cell><cell>73.6</cell><cell>47.8</cell></row><row><cell>GloLocAtt</cell><cell>69.7</cell><cell>51.7</cell><cell>37.1</cell><cell>26.3</cell><cell>23.8</cell><cell>85.7</cell><cell>51.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">COMPARISON EXPERIMENTS FOR EXPLORING THE EFFECT OF OUR PROPOSED IMAGE DESCRIPTION WITH DROPOUT MECHANISM OVER MS COCO</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">DATASET.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>Bleu1</cell><cell cols="2">Bleu2 Bleu3</cell><cell cols="2">Bleu4 METEOR</cell><cell>CIDEr</cell><cell>ROUGE-L</cell></row><row><cell>GloLocAttEmb</cell><cell></cell><cell>70.1</cell><cell>52.4</cell><cell>37.7</cell><cell>26.6</cell><cell>23.7</cell><cell>87.3</cell><cell>51.4</cell></row><row><cell cols="2">GloLocAttEmb+OneDrop</cell><cell>71.0</cell><cell>53.4</cell><cell>38.6</cell><cell>27.6</cell><cell>23.8</cell><cell>89.2</cell><cell>51.5</cell></row><row><cell cols="2">GloLocAttEmb+TwoDrop</cell><cell>71.8</cell><cell>54.3</cell><cell>39.5</cell><cell>28.6</cell><cell>24.2</cell><cell>91.2</cell><cell>52.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III COMPARISON</head><label>III</label><figDesc>WITH SEVERAL STATE-OF-THE-ART MODELS IN TERMS OF BLEU-1,2,3,4, METEOR, CIDER, ROUGE-L AND METEOR OVER MS COCO DATASET. -INDICATES UNKNOWN SCORES. † INDICATES THAT THE MODEL HAS THE SAME DECODER WITH OURS, THAT IS, THE SAME CNN MODEL FOR IMAGE REPRESENTATION. * INDICATE THAT THE MODEL HAS THE SAME ENCODER -THE LANGUAGE MODEL FOR GENERATING SENTENCE DESCRIPTION WITH OURS.</figDesc><table><row><cell>Method</cell><cell>Bleu1</cell><cell>Bleu2</cell><cell cols="2">Bleu3 Bleu4</cell><cell>METEOR</cell><cell cols="2">CIDEr ROUGH-L</cell></row><row><cell>NIC [31]</cell><cell>66.6</cell><cell>46.1</cell><cell>32.9</cell><cell>24.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LRCN  *  [13]</cell><cell>62.79</cell><cell>44.19</cell><cell>30.41</cell><cell>21</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepVS  † [33]</cell><cell>62.5</cell><cell>45</cell><cell>32.1</cell><cell>23</cell><cell>19.5</cell><cell>66</cell><cell>-</cell></row><row><cell>m-RNN  † [32]</cell><cell>67</cell><cell>49</cell><cell>35</cell><cell>25</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>soft attention  † [30]</cell><cell>70.7</cell><cell>49.2</cell><cell>34.4</cell><cell>24.3</cell><cell>23.9</cell><cell>-</cell><cell>-</cell></row><row><cell>g-LSTM Gaussian [14]</cell><cell>67</cell><cell>49.1</cell><cell>35.8</cell><cell>26.4</cell><cell>22.74</cell><cell>81.25</cell><cell>-</cell></row><row><cell>(RA+SF)-GREEDY  *  † [45]</cell><cell>69.1</cell><cell>50.4</cell><cell>35.7</cell><cell>24.6</cell><cell>22.1</cell><cell>78.3</cell><cell>50.1</cell></row><row><cell>(RA+SF)-BEAM10  *  † [45]</cell><cell>69.7</cell><cell>51.9</cell><cell>38.1</cell><cell>28.2</cell><cell>23.5</cell><cell>83.8</cell><cell>50.9</cell></row><row><cell>ATT [44]</cell><cell>70.9</cell><cell>53.7</cell><cell>40.2</cell><cell>30.4</cell><cell>24.3</cell><cell>-</cell><cell>-</cell></row><row><cell>GLA (ours)  *  †</cell><cell>71.8</cell><cell>54.3</cell><cell>39.5</cell><cell>28.5</cell><cell>24.2</cell><cell>91.2</cell><cell>52.3</cell></row><row><cell>GLA-BEAM3 (ours)  *  †</cell><cell>72.5</cell><cell>55.6</cell><cell>41.7</cell><cell>31.2</cell><cell>24.9</cell><cell>96.4</cell><cell>53.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>On the basis of the first experiment, adding one dropout layer after the second LSTM layer. On the basis of the second 1520-9210 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2751140, IEEE Transactions on Multimedia L. LI, et al.: IMAGE CAPTION WITH GLOBAL-LOCAL ATTENTION</figDesc><table /><note><p>• GloLocAttEmb+OneDrop: • GloLocAttEmb+TwoDrop:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>WITH SEVERAL STATE-OF-THE-ART MODELS IN TERMS OF BLEU-1,2,3,4, METEOR, CIDER, ROUGE-L AND METEOR OVER FLICKR 8K DATASET.THE MEANING OF SYMBOL IS THE SAME WITH THE ABOVE TAB.III.</figDesc><table><row><cell>Method</cell><cell cols="2">Bleu1 Bleu2</cell><cell cols="2">Bleu3 Bleu4</cell><cell>METEOR</cell><cell cols="2">CIDEr ROUGH-L</cell></row><row><cell>NIC [31]</cell><cell>63.-</cell><cell>41.-</cell><cell>27.-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>soft attention  † [30]</cell><cell>67.-</cell><cell>44.8</cell><cell>29.9</cell><cell>19.5</cell><cell>18.93</cell><cell>-</cell><cell>-</cell></row><row><cell>hard attention  † [30]</cell><cell>67.-</cell><cell>45.7</cell><cell>31.4</cell><cell>21.3</cell><cell>20.3</cell><cell>-</cell><cell>-</cell></row><row><cell>g-LSTM Gaussian [14]</cell><cell>64.7</cell><cell>45.9</cell><cell>31.8</cell><cell>21.6</cell><cell>20.19</cell><cell>-</cell><cell>-</cell></row><row><cell>GLA (ours)  *  †</cell><cell>57.2</cell><cell>37.9</cell><cell>23.9</cell><cell>14.8</cell><cell>16.6</cell><cell>36.2</cell><cell>41.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE COMPARISON WITH</head><label>COMPARISON</label><figDesc>SEVERAL STATE-OF-THE-ART MODELS IN TERMS OF BLEU-1,2,3,4, METEOR, CIDER, ROUGE-L AND METEOR OVER FLICKR 30K DATASET. THE MEANING OF THOSE TAGS IS THE SAME WITH THOSE LISTED AT THE ABOVE TAB.III.</figDesc><table><row><cell>Method</cell><cell cols="2">Bleu1 Bleu2</cell><cell cols="2">Bleu3 Bleu4</cell><cell>METEOR</cell><cell cols="2">CIDEr ROUGH-L</cell></row><row><cell>NIC [31]</cell><cell>66.3</cell><cell>42.3</cell><cell>27.7</cell><cell>18.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LRCN  *  [13]</cell><cell>58.7</cell><cell>39.1</cell><cell>25.1</cell><cell>16.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>m-RNN  † [32]</cell><cell>54.-</cell><cell>36.-</cell><cell>23.-</cell><cell>15.-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>soft attention  † [30]</cell><cell>66.7</cell><cell>43.4</cell><cell>28.8</cell><cell>19.1</cell><cell>18.49</cell><cell>-</cell><cell>-</cell></row><row><cell>g-LSTM Gaussian [14]</cell><cell>64.6</cell><cell>44.6</cell><cell>30.5</cell><cell>20.6</cell><cell>17.91</cell><cell>-</cell><cell>-</cell></row><row><cell>ATT [44]</cell><cell>64.7</cell><cell>46.0</cell><cell>32.4</cell><cell>23.0</cell><cell>18.9</cell><cell>-</cell><cell>-</cell></row><row><cell>GLA (ours)  *  †</cell><cell>56.8</cell><cell>37.2</cell><cell>23.2</cell><cell>14.6</cell><cell>16.6</cell><cell>36.2</cell><cell>41.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>WITH EXISTING MODELS IN TERMS OF R@1, R@5, R@10 AND MED R OVER MS COCO DATASET. 1K DENOTES THAT WE SAMPLE 1K IMAGES FROM VALIDATION DATASET FOR TESTING THE PERFORMANCE. 5K DENOTES THAT WE SAMPLE 5K IMAGES FROM VALIDATION DATASET.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>WITH EXISTING MODELS IN TERMS OF R@1, R@5, R@10 AND MED R OVER MS COCO DATASET. 1K DENOTES THAT WE SAMPLE 1K IMAGES FROM VALIDATION DATASET FOR TESTING THE PERFORMANCE. 5K DENOTES THAT WE SAMPLE 5K IMAGES FROM VALIDATION DATASET.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>SUBMITTED TO IEEE TRANSACTIONS ON MULTIMEDIA, VOL. **, NO. **, 2017</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>L. LI, et al.: IMAGE CAPTION WITH GLOBAL-LOCAL ATTENTION</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse ensemble learning for concept detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="54" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object localization based on proposal fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2015" to="2116" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Object detection using deep neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Toshev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01">Mar. 1 2016</date>
			<biblScope unit="page">308</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning consistent feature representation for cross-modal multimedia retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="381" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning high-level feature by deep belief networks for 3-d model retrieval and recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2154" to="2167" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Guiding longshort term memory for image caption generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04942</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname><genName>III</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="444" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Midge: Generating image descriptions from computer vision detections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter</title>
		<meeting>the 13th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="747" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collective generation of natural image descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonparametric method for data-driven image captioning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="592" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large scale retrieval and generation of image descriptions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SUBMITTED TO IEEE TRANSACTIONS ON MULTIMEDIA</title>
		<editor>
			<persName><forename type="first">*</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">O</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2017. 2015</date>
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bidirectional longshort term memory for video description</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="436" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Coling</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1886" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image caption with global-local attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4133" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Aligning where to see and what to tell: image caption with region-based attention and scene factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06272</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23">2014. June 23-28, 2014, 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cider: Consensusbased image description evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Her current research interests include image caption and object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<imprint>
			<date type="published" when="2009">2014. 2009</date>
			<pubPlace>Changchun, China; Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Chinese Academy of Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Linghui Li received her B.S. degree from Jilin university</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Tian&apos;s research interests include multimedia information retrieval, computer vision, pattern recognition and bioinformatics and published over 350 refereed journal and conference papers. He was the co-author of a Best Paper</title>
	</analytic>
	<monogr>
		<title level="m">Top 10% Paper Award in MMSP 2011, a Best Student Paper in ICASSP 2006, and co-author of a Best Student Paper Candidate in ICME 2015, and a Best Paper Candidate in PCM 2007. Dr. Tian research projects are funded by ARO, NSF, DHS, Google, FXPAL, NEC, SALSI, CIAS, Akiira Media Systems, HP, Blippar and UTSA. He received 2014 Research Achievement Awards from College of Science, UTSA. He received 2016 UTSA Innovation Award and 2010 ACM Service Award. He is the associate editor of IEEE Transactions on Multimedia (TMM), IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), Multimedia System Journal (MMSJ), and in the Editorial Board of Journal of Multimedia (JMM) and Journal of Machine Vision and Applications (MVA)</title>
		<imprint>
			<date type="published" when="2002">2002-2008. During 2008-2009</date>
		</imprint>
	</monogr>
	<note>Tian is a Fellow of IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
