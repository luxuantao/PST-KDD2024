<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-training on Large-Scale Heterogeneous Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianrui</forename><surname>Jia</surname></persName>
							<email>jiatianrui@bupt.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
							<email>yfang@smu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<email>shichuan@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-training on Large-Scale Heterogeneous Graph</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3447548.3467396</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Heterogeneous graph</term>
					<term>self-supervised learning</term>
					<term>pre-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) emerge as the state-of-the-art representation learning methods on graphs and often rely on a large amount of labeled data to achieve satisfactory performance. Recently, in order to relieve the label scarcity issues, some works propose to pre-train GNNs in a self-supervised manner by distilling transferable knowledge from the unlabeled graph structures. Unfortunately, these pre-training frameworks mainly target at homogeneous graphs, while real interaction systems usually constitute large-scale heterogeneous graphs, containing different types of nodes and edges, which leads to new challenges on structure heterogeneity and scalability for graph pre-training. In this paper, we first study the problem of pre-training on a large-scale heterogeneous graph and propose a novel pre-training GNN framework, named PT-HGNN. The proposed PT-HGNN designs both the node-and schema-level pre-training tasks to contrastively preserve heterogeneous semantic and structural properties as a form of transferable knowledge for various downstream tasks. In addition, a relation-based personalized PageRank is proposed to sparsify a large-scale heterogeneous graph for efficient pre-training. Extensive experiments on one of the largest public heterogeneous graphs demonstrate that our PT-HGNN significantly outperforms various state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Data mining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, as an emerging tool for learning on graph-structured data <ref type="bibr" target="#b32">[33]</ref>, graph neural networks (GNNs) learn powerful graph representations by recursively aggregating messages (i.e., features) from neighboring nodes. They have been demonstrated to benefit a wide variety of graph mining tasks from node classification and link prediction <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref> to graph generation and graph classification <ref type="bibr" target="#b17">[18]</ref>. However, the training of GNNs generally requires abundant taskspecific labeled data in order to achieve competitive performance for each downstream task. On one hand, unfortunately, labeled data for many tasks are usually expensive or infeasible to obtain. On the other hand, a large amount of unlabeled graph structures are often readily available in various domains amid the digitization trend.</p><p>To alleviate the reliance on task-specific labeled data, inspired by pre-training techniques from computer version <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> and natural language processing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>, recent works propose to pre-train GNNs in a self-supervised manner by distilling transferable knowledge from the unlabeled graph structures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref>. In particular, the transferable knowledge aims to capture the inherent structural properties of the unlabeled graphs, which can be flexibly applied across different tasks to complement task-specific labeled data. The key differences between existing strategies of GNN pre-training boil down to the design of the self-supervised pre-training tasks and corpus. For example, Hu et al. <ref type="bibr" target="#b10">[11]</ref> present various strategies which utilize node-and graph-level self-supervision on multiple graphs for pre-training the GNNs, while GPT-GNN <ref type="bibr" target="#b11">[12]</ref> introduces a selfsupervised attributed graph generation task on one large graph. Although the existing pre-training methods for GNNs achieve promising results, they are mainly designed for homogeneous graphs, lacking the capacity to capture the rich, heterogeneous semantic and structural properties of a heterogeneous graph during pre-training.</p><p>Heterogeneous graphs <ref type="bibr" target="#b25">[26]</ref> have been commonly utilized for modeling complex systems, in which objects of different types interact with each other via various relations. For example, in an academic graph, authors can publish papers that appear in various conferences, and co-author with others; in an e-commerce graph, users can click or buy products in various shops, and shops can promote products. To handle the heterogeneous objects and interactions, various complex semantic patterns (e.g., meta-path <ref type="bibr" target="#b26">[27]</ref> and meta-graph <ref type="bibr" target="#b6">[7]</ref>) have been proposed to capture the heterogeneous semantic and structural properties that are rich, inherent and vital on heterogeneous graphs. Thus, recent GNNs (without pre-training) start to utilize such semantic patterns to model heterogeneous graphs and achieve promising performance, implying that self-supervised pre-training tasks should also be designed to preserve the heterogeneous properties on a heterogeneous graph as part of the transferable knowledge. However, existing GNN pre-training approaches have not attempted to encode such heterogeneity. Furthermore, real-world heterogeneous graphs are often large with million-or billion-scale nodes or edges. Hence, it becomes critical and timely to study the pre-training of GNNs on a large-scale heterogeneous graph.</p><p>Challenges and present work. In this paper, we take the first attempt to pre-train GNNs on a large heterogeneous graph, utilizing the rich semantic and structural properties as self-supervised information. However, the problem is non-trivial, presenting us with two key challenges, as follows.</p><p>(1) How to capture the semantic and structural properties on a heterogeneous graph during pre-training? Existing pre-training strategies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref> are devised only for homogeneous graphs, treating all nodes and edges uniformly. However, a heterogeneous graph entails a variety of rich semantic and structural properties, which defines the varying characteristics of different types of objects. Taking an academic graph as example, a paper is characterized by not only its keywords but also its authors and venue, whereas an author is characterized by his/her institutes and papers. Moreover, different types of objects often manifest heterogeneous structural properties, e.g., conference nodes generally have much higher degrees than author nodes. Thus, it is important to encapsulate such diverse semantic and structural characteristics in self-supervised pre-training tasks, in order to learn more precise and expressive transferable knowledge that can be tailored to different downstream tasks on a heterogeneous graph.</p><p>(2) How to efficiently pre-train GNNs on a large-scale heterogeneous graph? Real-word heterogeneous graphs can be enormous with billions of nodes and edges <ref type="bibr" target="#b35">[36]</ref>. To ensure scalability to large graphs, existing work often considers two lines of approach: sampling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> and sparsification <ref type="bibr" target="#b2">[3]</ref>. Sampling of neighboring nodes is often conducted online during training, whereas sparsification can be regarded as a form of graph compression to retain the most useful edges in an offline step. On a very large graph, online sampling still incurs a significant overhead in computing the distribution of nodes <ref type="bibr" target="#b2">[3]</ref>. Thus, it is less desirable than sparsification, which does not introduce any overhead during training. In this paper, we resort to sparsification. However, existing sparsification methods, such as personalized PageRank and other centrality analysis, ignore the differences between various node and edge types, which could cause unwanted biases toward certain type of nodes-e.g., the type of nodes with higher degrees will be indiscriminately deemed more important. Thus, it is vital to propose an edge sparsification process for large-scale heterogeneous graphs for efficient pre-training.</p><p>To tackle the above challenges, we present a novel framework of Pre-Training GNNs on Heterogeneous Graph, named PT-HGNN. It aims to preserve the inherent semantic and structural properties efficiently on a large-scale heterogeneous graph. For the first challenge, inspired by contrastive learning <ref type="bibr" target="#b33">[34]</ref>, we design a contrastive pre-training strategy to model the heterogeneity w.r.t. both semantics and structures. To be more specific, we introduce two contrastive strategies at the node and schema levels. At the node level, to enhance the subtle semantic difference between nodes, we generate relation-wise negative node samples-two nodes can form a negative sample if they satisfy the types involved in a specific relation and do not constitute a positive example under this relation, e.g., author 𝑎 1 and paper 𝑝 2 with the "write" relation in Figure <ref type="figure" target="#fig_0">1</ref>(a). At the schema-level, we generate negative subgraph samples guided by network schema, which effectively preserves high-order structures on heterogeneous graphs. Although complex semantic patterns (i.e., meta-graph) are often used to capture higher-order graph heterogeneity, computation of their instances can be time-consuming. In contrast, as a unique high-order structure that defines a heterogeneous graph, network schema <ref type="bibr" target="#b26">[27]</ref> is convenient to sample its instances, and schema instances naturally contain all types of nodes and relations in the graph. To address the second challenge, inspired by the previous graph sparsification studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>, we propose a relation-based personalized PageRank (PPR) to sparsify a large heterogeneous graph, as the original personalized PageRank <ref type="bibr" target="#b13">[14]</ref> does not deal with graph heterogeneity. Particularly, in our relation-based sparsification, we distinguish various relations in the computation of personalized PageRank, thereby alleviating the bias introduced by certain types of nodes (e.g., those with high degrees). The less biased sparsification can accelerate the pre-training procedure while preserving the heterogeneous nature of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>To summarize, we make the following major contributions in this work.</p><p>• This is the first attempt to pre-train GNNs on a large-scale heterogeneous graph, which is an important and practical problem with numerous application scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph neural networks have received significant research interests due to the prevalence of graph-structure data <ref type="bibr" target="#b32">[33]</ref>. They utilize neural networks to learn node representations on graphs, and belong to two major categories: spectral methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref> and message passing architectures to aggregate neighbors' features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>. Besides, some studies have attempted to deploy the GNNs on heterogeneous graphs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>. Wang et al. <ref type="bibr" target="#b31">[32]</ref>   To enable more effective learning, researchers have explored how to utilize the abundant unlabeled data for pre-training a GNN. Inspired by pre-training techniques in natural language processing <ref type="bibr" target="#b5">[6]</ref> and computer vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, recent studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref> have been proposed to pre-train GNNs with self-supervised information. Boardly, these work are classified into two categories: contrastive method and generative method. For instances, Qiu et al. <ref type="bibr" target="#b24">[25]</ref> and You et al. <ref type="bibr" target="#b34">[35]</ref> propose various graph data augmentations to construct positive/negative samples for conducting contrastive learning, while GPT-GNN <ref type="bibr" target="#b11">[12]</ref> introduces a self-supervised attributed graph generation task to pre-train a GNN. However, these methods usually focus on homogeneous graphs, which can not be directly applied to heterogeneous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED PT-HGNN FRAMEWORK</head><p>In this section, we introduce our pre-training framework PT-HGNN on a large-scale heterogeneous graph. More specifically, we first elaborate on the design of the pre-training tasks for a heterogeneous graph. To preserve the heterogeneity, we propose both nodeand schema-level pre-training tasks to respectively utilize node relations and the network schema, which encourages the GNN to capture heterogeneous semantic and structural properties. Second, we present our edge sparsification strategy on a large-scale heterogeneous graph for pre-processing. To avoid unwanted biases toward certain types of node, we propose a relation-based personalized PageRank to retain the most useful graph structures, in order to accelerate the pre-training procedure. Figure <ref type="figure" target="#fig_0">1</ref> shows the overall framework of the proposed PT-HGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-training Tasks on Heterogeneous Graph</head><p>A heterogeneous graph <ref type="bibr" target="#b25">[26]</ref>, denoted by G = {V, E, A, R, 𝜙, 𝜑 }, is a form of graph, where V and E denote the sets of nodes and edges, respectively. It is also associated with a node-type mapping function 𝜙 : V → A and an edge-type mapping function 𝜑 : E → R, where A and R denote the sets of node and edge types such that</p><formula xml:id="formula_0">|A| + |R| &gt; 2.</formula><p>Moreover, the network schema 𝑇 G = (A, R) of a heterogeneous graph specifies the type constraints on the nodes and their relations, which can guide the exploration of heterogeneous structural contexts on the graph. Figure <ref type="figure" target="#fig_0">1</ref>(a) shows an example of heterogeneous graph and its network schema with four types of node and edge.</p><p>For the design of pre-training tasks, our goal is to encode the inherent heterogeneity of nodes and edges in the transferable knowledge. In contrast to previous pre-training strategies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>, we need to consider the subtle semantic differences and complex heterogeneous structures. Thus, we employ both node instances and network schema instances as self-supervision, to differentiate the rich semantic relations and high-order heterogeneous structural contexts, respectively. Inspired by contrastive learning <ref type="bibr" target="#b24">[25]</ref>, we generate the positive and negative samples from two levels, to learn transferable knowledge for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Node-level Pre-training Task.</head><p>In existing methods, positive and negative samples are differentiated only by network structures, such as through a perturbation of the original graph structures and node features. However, a heterogeneous graph embodies rich semantics manifested in terms of multiple types of node and their relation (i.e., edge), and thus it is crucial for the transferable knowledge to encode such semantics. Here, we design a node-level pretraining task to encode the semantics, which allows us to model pairwise relations between different types of node.</p><p>A relation between two nodes conveys important semantic information about them. On one hand, a positive triple ⟨𝑢, 𝑅, 𝑣⟩ in a heterogeneous graph G means that nodes 𝑢 ∈ V and 𝑣 ∈ V are linked via a specific relation 𝑅 ∈ R on G. On the other hand, negative samples are obtained by replacing the nodes in a corresponding positive sample, as follows.</p><p>Negative Samples Selection. In previous contrastive learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35]</ref>, the negative samples are only selected based on homogeneous graph structures (i.e., two unlinked nodes), ignoring two important aspects. First, they do not constrain node types for a specific relation. For instance, for the "write" relation, the negative samples should only be sampled from author-paper node pairs. Second, they do not consider the similarity of nodes themselves, in contradiction to the alignment principle <ref type="bibr" target="#b30">[31]</ref>. Specifically, two nodes that are too similar should not simultaneously appear in the corresponding positive and negative samples under the same relation. For instance, consider a positive sample ⟨𝑎 1 , "write", 𝑝 1 ⟩ shown in Figure <ref type="figure" target="#fig_2">1(c)</ref>. 𝑎 2 , though not linked to 𝑝 2 , should not form a negative example with 𝑝 2 under the "write" relation, as 𝑎 2 and 𝑎 1 are too similar in their representations to serve as a good contrast. Instead, 𝑎 3 , which is not linked to 𝑝 2 and is dissimilar to 𝑎 1 , can form a more reasonable negative sample ⟨𝑎 3 , "write", 𝑝 1 ⟩. Accounting for both of the above aspects, we construct the negative samples in a relation-specific manner and consistent with the alignment principle. In specific, for a given positive triplet ⟨𝑢, 𝑅, 𝑣⟩, we define the negative samples for a relation 𝑅 as:</p><formula xml:id="formula_1">N 𝑛𝑜𝑑𝑒 ⟨𝑢,𝑅,𝑣 ⟩ = {⟨𝑢, 𝑅, 𝑣 − ⟩ | 𝜙 (𝑣) = 𝜙 (𝑣 − ), (𝑢, 𝑣 − ) ∉ E, 𝑆𝑖𝑚(𝑣, 𝑣 − ) ≤ 𝛿 } , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where 𝑆𝑖𝑚 is a function to measure the similarity between the node representations, and 𝛿 is a threshold for filtering out too similar nodes in violation of the alignment principle. Note that, to avoid getting only very easy negative samples, 𝛿 is set to a relatively large number.</p><p>Node-level Loss. Given a positive triple ⟨𝑢, 𝑅, 𝑣⟩ and its corresponding negative samples that replace node 𝑣, we optimize the following InfoNCE loss <ref type="bibr" target="#b27">[28]</ref> for node 𝑢:</p><formula xml:id="formula_3">L 𝑛𝑜𝑑𝑒 𝑢,𝑅 = − log exp h ⊤ 𝑢 W 𝑅 h 𝑣 /𝜏 𝑖 ∈ {𝑣 }∪{𝑤 | ⟨𝑢,𝑅,𝑤 ⟩ ∈N 𝑛𝑜𝑑𝑒 ⟨𝑢,𝑅,𝑣⟩ } exp h ⊤ 𝑢 W 𝑅 h 𝑖 /𝜏 ,<label>(2)</label></formula><p>where W 𝑅 ∈ R 𝑑×𝑑 is a learnable relation matrix for relation 𝑅 and 𝜏 is a temperature hyper-parameter. Here, h 𝑢 indicates the representation vector of node 𝑢, which can be generated by any existing GNN architecture.</p><p>The above node-level pre-training tasks only capture the firstorder semantics involving direct relations between nodes. To capture high-order semantic and structural properties, we resort to the schema-level pre-training, as follows.</p><p>3.1.2 Schema-level Pre-training Task. In order to incorporate the high-order heterogeneous semantic and structural contexts in a heterogeneous graph, a natural idea is to utilize high-order semantic patterns such as meta-structures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> and motifs. However, there are three major weakness of these semantic patterns for pre-training GNNs on a heterogeneous graph. (1) Meta-paths are relatively limited in the ability to express more complex high-order structures, due to its path-only structure, whereas motifs only capture highorder structures but limited in semantics. (2) For meta-graph or motif, it is intractable to find their instances especially when the meta-graph or motif is large. (3) The choice of meta-path and metagraph relies on domain knowledge. However, for a heterogeneous graph, its network schema <ref type="bibr" target="#b26">[27]</ref> is a unique defining structure that captures both high-order semantic and structural properties, and does not require any domain knowledge. Moreover, as network schema is essentially a template for the heterogeneous graph, its instances can be sampled easily. Therefore, we utilize the network schema to strike a balance between capturing the heterogeneity and achieving efficiency on a large-scale heterogeneous graph. We generate schema instances to construct the positive and negative samples, to complement the first-order node-level samples.</p><p>Positive Network Schema Samples. Given a network schema 𝑇 𝐺 = (A, R), we can generate its instances. However, if we directly generate instances randomly following the network schema, the instances will be extremely imbalanced w.r.t. different node types, due to the fact that the degree of nodes for each type can vary dramatically (e.g., the degree of a conference is much larger than a paper). To address the imbalance problem, we control the number of sampled schema instances of each type of node.</p><p>We consider the association between a node and a network schema instance. Formally, given a schema instance containing node 𝑢, let 𝑢 be the target node and the other nodes in the instance be the context nodes of 𝑢. For example, for a schema instance s = {𝑝 1 , 𝑎 1 , 𝑓 1 , 𝑣 1 , 𝑝 3 } as shown in Figure <ref type="figure" target="#fig_0">1</ref>(a), we say that node 𝑝 1 is a target node with context nodes {𝑎 1 , 𝑓 1 , 𝑣 1 , 𝑝 3 }, or 𝑎 1 is a target node with context nodes {𝑝 1 , 𝑓 1 , 𝑣 1 , 𝑝 3 }, and so on. Based on this definition, we consider the context nodes of target node 𝑢 as the schema-level positive samples for 𝑢, denoted P 𝑠𝑐ℎ𝑒 𝑢 , given by</p><formula xml:id="formula_4">P 𝑠𝑐ℎ𝑒 𝑢 = s∈𝐼 (𝑢) s\{𝑢},<label>(3)</label></formula><p>where 𝐼 (𝑢) denotes the set of all schema instances containing node 𝑢. Intuitively, the context nodes capture not only the structural contexts of the target node, but also semantic properties of different types of node and edge.</p><p>Negative Network Schema Samples. To construct the schemalevel negative samples for target node 𝑢, we follow two approaches: 1) if two network schema instances are generated from two different target nodes of the same type, we treat them as negative schema samples of each other; and 2) we design a dynamic queue <ref type="bibr" target="#b9">[10]</ref> for storing the negative network schema samples.</p><p>In the first approach, for the nodes in a batch denoted as V 𝐵 , we obtain the negative schema instances N 1 𝑢 for target node 𝑢 as</p><formula xml:id="formula_5">N 1 𝑢 = {P 𝑠𝑐ℎ𝑒 𝑢 − | 𝑢 − ∈ V 𝐵 , 𝑢 ≠ 𝑢 − , 𝜙 (𝑢) = 𝜙 (𝑢 − )}.<label>(4)</label></formula><p>In the second approach, for the sake of getting more negative schema samples, a direct idea is to randomly select the nodes for each type to construct the network schema instances. However, negative instances generated from this method will be easily distinguishable from the positive schema samples, in which nodes in such negative samples are more likely to be unrelated. Thus, to avoid randomly generated negative samples, we resort to actual schema instances from the previous batch by designing a dynamic queue to store the network schema instances for more representative instances. Then, we have the negative schema instances N 2 𝑢 from the dynamic queue as:</p><formula xml:id="formula_6">N 2 𝑢 = {P 𝑠𝑐ℎ𝑒 𝑣 | 𝜙 (𝑢) = 𝜙 (𝑣), 𝑣 ∈ V 𝑡 −1 𝐵 },<label>(5)</label></formula><p>where V 𝑡 −1 𝐵 is the node set of the previous batch. It is worth noticing that the queue is initialized as an empty set in the beginning and updated during the training procedure. Therefore, we obtain the overall schema-level negative samples as follows:</p><formula xml:id="formula_7">N 𝑠𝑐ℎ𝑒 𝑢 = N 1 𝑢 ∪ N 2 𝑢 .<label>(6)</label></formula><p>Encoding Context Nodes and Schema-Level Loss. Directly predicting the proximity between target node 𝑢 and its context nodes {𝑣 1 , 𝑣 2 , • • • } cannot capture the heterogeneity of nodes. To account for nodes of different types, we devise one encoder for each type of nodes to learn node representations. Specifically, we learn the node embedding of target node 𝑣 𝑖 with an encoder Enc 𝜙 (𝑣 𝑖 ) for node type 𝜙 (𝑣 𝑖 ):</p><formula xml:id="formula_8">e 𝑣 𝑖 = Enc 𝜙 (𝑣 𝑖 ) (h 𝑣 𝑖 ),<label>(7)</label></formula><p>where each encoder Enc(•) represents an MLP. Then, for the target node 𝑢, we generate its context embedding via a pooling function over the context nodes</p><formula xml:id="formula_9">{𝑣 1 , 𝑣 2 , • • • }, denoted as c 𝑠 𝑢 : c 𝑠 𝑢 = Pool(e 𝑣 1 , e 𝑣 2 , • • • ),<label>(8)</label></formula><p>where Pool(•) averages the embeddings of context nodes.</p><p>Given the schema-level samples, we optimize the likelihood that the target node 𝑢 associates with context nodes in the positive samples and does not related to those in the negative samples:</p><formula xml:id="formula_10">L 𝑠𝑐ℎ𝑒 𝑢 = s + ∈ P 𝑠𝑐ℎ𝑒 𝑢 log exp h ⊤ 𝑢 c s + /𝜏 s∈ {s + }∪N 𝑠𝑐ℎ𝑒 𝑢 exp h ⊤ 𝑢 c s /𝜏 ,<label>(9)</label></formula><p>where 𝜏 is a temperature hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparsification of Large-Scale Heterogeneous Graph</head><p>To pre-train a better GNN, the key is to leverage a large-scale heterogeneous graph. Existing scalable GNNs utilize sampling and sparsification in the online and offline steps, respectively. However, on large-scale graphs, online sampling still incurs a significant overhead in computing the distribution of nodes <ref type="bibr" target="#b2">[3]</ref>. Thus, we resort to offline sparsification to retain the most important edges in the graph. Inspired by previous studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29]</ref>, personalized PageRank can be utilized to preserve more effective neighborhood. However, due to the heterogeneity of graph, existing personalized PageRank on homogeneous graphs is not applicable. Thus, we propose a relationbased personalized PageRank for edge sparsification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Personalized PageRank on Heterogeneous</head><p>Graph. As we mentioned above, personalized PageRank (PPR) is a good tool to obtain more meaningful neighbors for a node. To evaluate the personalized PageRank for a certain node, it relies on the node degree and adjacency matrix to calculate the transition probability. However, on a heterogeneous graph, the transition probability can be highly skewed toward certain types of node. For example, the transition probability from conference node (high out-degree) and paper node (low out-degree), will be extremely different due to their distinct structural role in an academic graph, which leads to unwanted biases in the PageRank scores.</p><p>To alleviate the biases caused by the heterogeneous structures, we design a power-iteration method to compute the personalized PageRank score for a relation 𝑅. For the sake of simplicity, we take 𝐴 1 𝑅 −→ 𝐴 2 for illustration, which defines a relation 𝑅 between nodes with types 𝐴 1 and 𝐴 2 . We obtain personalized PageRank Π 𝑅 for a relation 𝑅 by iteratively updating the following:</p><formula xml:id="formula_11">Π 𝑅 = 𝛼I + (1 − 𝛼)𝑆Π 𝑅 −1 Π 𝑅 −1 = 𝛽I + (1 − 𝛽)𝑆 𝑇 Π 𝑅<label>(10)</label></formula><p>where 𝑆 = 𝐷</p><formula xml:id="formula_12">− 1 2 𝐴 1 𝐴 𝑅 𝐷 − 1 2 𝐴 2</formula><p>, 𝐷 𝐴 𝑖 denotes a diagonal matrix whose diagonal contains the degrees of all nodes of 𝐴 𝑖 type, 𝐴 𝑅 represents the adjacency matrix for nodes with type 𝐴 1 and 𝐴 2 under the relation 𝑅, Π 𝑅 denotes a matrix for pairwise personalized PageRank scores under the relation 𝑅, 𝑅 −1 is the inverse relation of 𝑅, and 𝛼, 𝛽 are hyper-parameters for controlling the convergence. Thus, the relation-based personalized PageRank can handle different node and edge types without skewing to a particular type. However, the computation of Eq. 10 depends on matrix multiplication, which is infeasible for a large-scale graph. Inspired by previous studies <ref type="bibr" target="#b0">[1]</ref>, we obtain the personalized PageRank for a relation 𝑅 using a equivalent random walk formulation. Further details are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Edge Sparsification.</head><p>Through the above process, we can obtain a dense matrix Π 𝑅 with pairwise personalized PageRank scores for each relation 𝑅, in contrast to the sparse adjacency matrix 𝐴 𝑅 under the same relation. Note that the values in Π 𝑅 represent the pairwise influence between all pairs of nodes in relation 𝑅, which typically are highly localized <ref type="bibr" target="#b22">[23]</ref>. Spatial localization allows us to simply truncate small values of Π 𝑅 and recover sparsity. Similar to previous studies <ref type="bibr" target="#b0">[1]</ref>, we use the top-k entries with the highest mass per column and set all other entries to zero in Π 𝑅 . Subsequently, as shown in Figure <ref type="figure" target="#fig_0">1</ref> </p><p>The edge sparsification for the heterogeneous graph G will be done as a pre-processing step. Subsequently, we obtain the sparsified graph G ′ for pre-training, which can accelerate the aggregation operation of GNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-training Pipeline</head><formula xml:id="formula_14">L = L 𝑛𝑜𝑑𝑒 + 𝜆L 𝑠𝑐ℎ𝑒 , (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>where 𝜆 is a balancing coefficient. At last, we optimize the model via the AdamW optimizer <ref type="bibr" target="#b20">[21]</ref> with Cosine Annealing Learning Rate Scheduler <ref type="bibr" target="#b19">[20]</ref>. The detailed algorithm and time complexity of the pre-training pipeline are provided in Appendix B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first conduct experiments on large heterogeneous graphs to evaluate model performance, and then investigate the knowledge transferability of PT-HGNN on different graphs and the underlying mechanism with two ablated models. Lastly, we explore the impact of different model settings on task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>Datasets and Tasks. We conduct experiments on Open Academic Graph (OAG) <ref type="bibr" target="#b35">[36]</ref>, which is consitituted by papers (P), authors (A), venues (V), institutes (I), fields (F) and their relations with 178 million nodes and 2.236 billion edges. As far as we know, this is the largest publicly available heterogeneous graph. To test the generalization ability and transferability of the proposed pre-training framework, we also construct four representative domain-specific subgraphs from OAG: Computer Science (CS), Material Science (Mater), Engineering (Engin) and Chemistry (Chem). These graphs have diverse statistics listed in Table <ref type="table" target="#tab_2">1</ref>, which also exhibit large differences in graph properties as shown in Appendix C. We consider the prediction of Paper-Field, Paper-Venue, and Author Name Disambiguation (Author ND) as three downstream tasks used in prior works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. The model performance is evaluated by NDCG and MRR, which are widely adopted performance metrics <ref type="bibr" target="#b18">[19]</ref>.</p><p>Pre-Training and Fine-Tuning Setting. We pre-train a GNN model and use its output node embeddings as input features for the downstream tasks. Then we fine-tune the GNN models according to the specific downstream tasks on an unseen fine-tuning dataset and evaluate the model performance. In OAG dataset, the publication data of papers ranges from 1900 to 2019. Accordingly, we use the data with publication information before 2014 for pre-training. In contrast, the data since 2014 are used for fine-tuning on different downstream tasks. To be more specific, in the fine-tuning stage, we use 10% data from 2014 to 2016 for training and 10% data in 2017 for validation. The data after 2018 are used for testing.</p><p>Baselines. We compare our proposed PT-HGNN with a series of state-of-the-art baselines, as follows.</p><p>• No pre-train method trains a GNN model for the downstream tasks on the fine-tuning graph directly. • EdgePred <ref type="bibr" target="#b8">[9]</ref> predicts whether there exists a link between two nodes, which is an unsupervised method that forces linked nodes to have similar node embedding.</p><p>• DGI <ref type="bibr" target="#b29">[30]</ref> maximizes local mutual information across the graph's patch representations.</p><p>• ContextPred <ref type="bibr" target="#b10">[11]</ref> maps nodes appearing in similar structural contexts to nearby embeddings. • GraphCL <ref type="bibr" target="#b34">[35]</ref> proposes four graph data augmentation method to conduct contrastive learning, in which node dropping and subgraphs are employed as the pre-training strategies in the experiment. • GPT-GNN <ref type="bibr" target="#b11">[12]</ref> is generative pre-training model for GNNs, which reconstructs the attributes and the structure of the input graph to learn the transferable knowledge from the input graph.</p><p>Implementation details. We employ the state-of-the-art heterogeneous GNN method HGT <ref type="bibr" target="#b12">[13]</ref> as the base model for our method PT-HGNN and other baselines. We implement the base model with PyTorch Geometric (PyG) package <ref type="bibr" target="#b7">[8]</ref>. We set the hidden dimension as 400, the number of heads as 8, and the number of layers as 3.</p><p>Besides, we optimize the model via AdamW optimizer <ref type="bibr" target="#b20">[21]</ref> with Cosine Annealing Learning Rate Scheduler <ref type="bibr" target="#b19">[20]</ref> with 200 epochs and choose the model parameters with the lowest validation loss as the pre-trained model in the pre-training procedure. For fair comparison, the above parameters follow the setting of previous study <ref type="bibr" target="#b11">[12]</ref>. For the implementation of our pre-training framework PT-HGNN, we set 𝛿 in Eq.(1) as 0.99. The maximum number of negative samples in Eq. ( <ref type="formula" target="#formula_1">1</ref>) and Eq. ( <ref type="formula" target="#formula_5">4</ref>) are set to 256 and 512, respectively. In the pre-training procedure, we will remove some negative samples randomly if the number of negative samples exceeds the maximum setting. For the balancing coefficient 𝜆 in Eq. ( <ref type="formula" target="#formula_14">12</ref>), we set it as 0.1 in the pre-training procedure. The temperature hyperparameter 𝜏 in Eq.( <ref type="formula" target="#formula_3">2</ref>) and Eq.( <ref type="formula" target="#formula_10">9</ref>) are set to 0.2. In the fine-tuning stage, for fair comparison, we fine-tune the model using the same optimization setting on the downstream tasks for ten times and report the mean and standard deviation of the test performance. More experiment details can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison on Downstream Tasks</head><p>In this experiment, we compare PT-HGNN to baseline methods in the downstream tasks on the whole OAG graph and the four domain-specific subgraphs, respectively. We evaluate the model performance by pre-training and fine-tuning on the same graphs but with different time periods as described in Section 4.1. Table <ref type="table" target="#tab_3">2</ref> demonstrates the link prediction and node classification performance on the five datasets. Overall, PT-HGNN achieves relative performance gains of 22.02% over the base model (i.e., HGT) without pre-training on OAG. Moreover, our PT-HGNN consistently yields the best performance among all methods, leading to an average improvement of 4.98% compared to the second best baseline method. These improvements indicate that our proposed pre-training strategy is capable of exploiting transferable information and graph properties on heterogeneous graphs, which are beneficial to the downstream tasks. We also observe that GPT-GNN achieves the second best performance due to the fact that its edge generation pre-training task can also be adopted for heterogeneous graphs, while its performance is significantly worse than our PT-HGNN. This demonstrates the superiority of our proposed pre-training tasks in capturing the heterogeneity on graphs. In addition, among the baselines, different pre-training methods work well on different datasets, e.g., EdgePred for Material Science and ContextPred for Chemistry. This difference implies that those baselines may only capture a portion of local information and structural contexts which make their performance vary dramatically in different datasets. In all, the proposed node-and schema-level pre-training tasks enable our PT-HGNN to make full use of heterogeneous semantic and structural properties on heterogeneous graphs, which accounts for the performance gain of PT-HGNN over the other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Knowledge Transfer on Different Graphs</head><p>We investigate how the network structures affect the ability of knowledge transfer from pre-training to fine-tuning, and examine the applicability of our proposed pre-training strategies. In order to exploit diverse network structures, in this experiment, we add the Art graph with unique network structures from the art field of OAG, whose details can be found in Appendix C. We compute the correlation between graphs using a series of commonly used graph property metrics <ref type="bibr" target="#b23">[24]</ref> listed in Table <ref type="table">6</ref> in Appendix C, so as to quantify the structural difference between graphs. In all, a higher correlation value between two graphs means the higher similarity of their network structures. Figure <ref type="figure" target="#fig_5">2</ref>(a) shows a heatmap of the pairwise correlation among five representative graphs, from which we observe that the four graphs out of a total number of five, namely, CS, Chemistry, Engineering and Materials, have similar network structures, whereas the Art graph is significantly different in its network structures compared to the other graphs. This difference in network structures is also verified by the citation coefficient <ref type="bibr" target="#b35">[36]</ref>, which measures the percentage of publications in graph Y that have citations in graph X, as shown in Figure <ref type="figure" target="#fig_5">2(b)</ref>.    Next, we evaluate the model performance by pre-training with one graph and fine-tuning with another for the Author ND task. The MRR improvement of our proposed method over the one without pre-training is shown in Figure <ref type="figure" target="#fig_5">2</ref>(c), which leads to the following findings. Knowledge transferring from pre-training to fine-tuning does not guarantee a gain in performance, and generally speaking, a positive correlation value between graphs results in positive transferring and vice versa. Moreover, a higher similarity of network structures between graphs tend to give rise to larger performance improvement. From the above observations, we can come to the conclusion that the proposed pre-training strategy is applicable if the two graphs for pre-training and fine-tuning, respectively, have similar network structures with a positive correlation value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We experimentally evaluate the effect of node-and schema-level pre-training tasks on heterogeneous graphs. Hence, we conduct an ablation study and consider two ablated variants of PT-HGNN named PT-HGNN 𝑛𝑜𝑑𝑒 and PT-HGNN 𝑠𝑐ℎ𝑒 . Here, PT-HGNN 𝑛𝑜𝑑𝑒 only includes the node-level pre-training task, while PT-HGNN 𝑠𝑐ℎ𝑒 only incorporates the schema-level pre-training task. Specifically, we consider the prediction of Paper-Field, Paper-Venue, and Author ND as three downstream tasks on the CS graph.</p><p>In Table <ref type="table" target="#tab_4">3</ref>, the two ablated variants achieve significant improvement over the no pre-train model, while they still perform worse than the complete pre-training strategy, i.e., PT-HGNN. These results illustrate the benefits of the node-and schema-level pretraining tasks. In particular, PT-HGNN 𝑛𝑜𝑑𝑒 models the pairwise node interaction with semantic information, which obtains better performance than PT-HGNN 𝑠𝑐ℎ𝑒 in the link prediction experiments (i.e., Paper-Field and Paper-Venue). Compared to PT-HGNN 𝑛𝑜𝑑𝑒 , the PT-HGNN 𝑠𝑐ℎ𝑒 significantly improves the node classification performance by focusing on modeling the structure context in a heterogeneous graph. Putting it all together, the combination of PT-HGNN 𝑛𝑜𝑑𝑒 and PT-HGNN 𝑠𝑐ℎ𝑒 offers strong capability in learning both the link prediction and node classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Analysis</head><p>Lastly, we investigate the performance of PT-HGNN with different fine-tuning strategies, as well as the impact of edge sparsification.</p><p>Freezing vs. Full Fine-tuning. The main goal of pre-training is to learn transferable weights. Thus, we evaluate two different training modes for downstream tasks, named freezing mode and full fine-tuning mode. In the freezing mode denoted by PT-HGNN(FE), we freeze the parameters of the pre-trained model during finetuning stage, and treat it as a feature extractor (FE) only. Then, we just train the downstream classifier with the output embeddings from the base GNN. In the full fine-tuning mode (i.e., our PT-HGNN), we train the base GNN with the downstream classifier in an end-to-end manner. We compare the performance of freezing mode, full fine-tuning mode and the no pre-train mode. As shown in Table <ref type="table" target="#tab_5">4</ref>, PT-HGNN(FE) achieves better performance than the no pre-train model, which demonstrates that the proposed pretraining strategies are able to capture the transferable knowledge. Moreover, the performance of PT-HGNN in freezing mode exhibits competitive performance to that of the full fine-tuning mode. In all, the experimental results indicate that our pre-training strategies can capture transferable knowledge.</p><p>Impact of edge sparsification. We conduct further experiments to examine the efficiency of the PPR-based edge sparsificaiton operation. Specifically, we evaluate the three downstream tasks on the CS graph with and without the sparsification method. Table <ref type="table" target="#tab_6">5</ref> demonstrates that pre-training on the pre-processed heterogeneous graph G ′ achieve competitive performance to that on the original input heterogeneous graph G. This is mainly because the PPR strategy in PT-HGNN can preserve more meaningful edges and reduce some noisy edges in the pre-processing stage. Moreover, with the edges sparsification based on personalized PageRank, the training efficiency is increased by 41.97% on average, showing that this strategy effectively expedites the pre-training process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we take the first attempt to pre-train GNNs on a largescale heterogeneous graph, and introduce a pre-training framework named PT-HGNN. First, to preserve the heterogeneity, we propose both node-and schema-level pre-training tasks to utilize node relations and the network schema, respectively, which enables the GNN to capture heterogeneous semantics and structural properties. Second, to pre-train on large-scale heterogeneous graphs, we present an edge sparsification strategy via relation-based personalized PageRank, which retains meaningful graph structures while accelerating the pre-training procedure. Extensive experiments on one of the largest heterogeneous graphs, OAG, demonstrate the superior ability of our PT-HGNN to transfer knowledge to various downstream tasks via pre-training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework of PT-HGNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b), for each relation 𝑅, we define a new adjacency matrix Ã𝑅 as follows: Ã𝑅 𝑢 𝑗 = 1, if Π 𝑅 𝑢 𝑗 &gt; 0 and (𝑢, 𝑗) ∈ E 0, otherwise .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1</head><label>1</label><figDesc>Figure 1 shows the overall pipeline of the pre-training framework (PT-HGNN). Given a large-scale heterogeneous graph G, PT-HGNN first conducts the edge sparsification via the relation-based personalized PageRank to obtain a more representative and sparsified graph G ′ , as show in Figure 1(b). Then, PT-HGNN employs the pre-training tasks at both the node and relation levels on the sparsified G ′ , in which the two pre-training tasks are jointly optimized to capture the heterogeneous semantic and structural properties as the transferable knowledge. In other words, we minimize the following loss on G ′ :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>MRR gain (%) over no pre-training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of pairwise correlation and knowledge transfer among five graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of Open Academic Graph Dataset</figDesc><table><row><cell>Dataset</cell><cell>#nodes</cell><cell>#edges</cell><cell>#venues</cell><cell>#papers</cell><cell>#fields</cell><cell>#authors #institutes</cell><cell>#P-V</cell><cell>#P-P</cell><cell>#P-F</cell><cell>#P-A</cell><cell>#A-I</cell></row><row><cell>CS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison between our method and baselines (best result in bold, and second best underlined).</figDesc><table><row><cell>Dataset</cell><cell cols="2">Downstream Task</cell><cell>No pre-train EdgePred</cell><cell>DGI</cell><cell>ContextPred GraphCL</cell><cell>GPT-GNN</cell><cell>PT-HGNN</cell><cell>Improv.</cell></row><row><cell></cell><cell>Paper-Field</cell><cell>NDCG MRR</cell><cell cols="4">27.42±0.42 31.37±0.32 32.82±0.67 33.15±0.71 32.64±0.65 35.24±0.47 23.17±0.45 32.13±0.52 33.43±0.81 33.24±0.57 33.24±0.67 33.57±0.71</cell><cell>36.04±0.37 37.76±0.42</cell><cell>2.27% 12.48%</cell></row><row><cell>CS</cell><cell>Paper-Venue</cell><cell>NDCG MRR</cell><cell cols="4">27.76±0.56 35.77±0.59 34.23±0.71 34.30±0.92 32.11±0.69 36.15±0.53 11.39±0.37 16.34±0.47 16.21±0.62 17.66±0.81 16.29±0.49 19.13±0.65</cell><cell>38.81±0.51 21.19±0.45</cell><cell>7.35% 10.76%</cell></row><row><cell></cell><cell>Author ND</cell><cell>NDCG MRR</cell><cell cols="4">76.27±0.53 79.41±0.68 81.38±0.93 79.22±0.72 79.95±0.89 80.20±0.51 54.82±0.49 59.06±0.74 58.98±0.79 60.23±0.83 60.55±0.74 60.94±0.52</cell><cell>82.19±0.60 63.38±0.38</cell><cell>2.48% 4.00%</cell></row><row><cell></cell><cell>Paper-Field</cell><cell>NDCG MRR</cell><cell cols="4">37.44±0.77 44.47±0.67 43.32±0.66 41.94±0.32 43.89±0.54 44.77±0.47 35.56±0.54 47.69±0.75 46.21±0.81 44.11±0.69 46.24±0.80 48.24±0.35</cell><cell>46.61±0.64 51.11±0.51</cell><cell>4.01% 5.94%</cell></row><row><cell>Mater</cell><cell>Paper-Venue</cell><cell>NDCG MRR</cell><cell cols="4">37.94±0.56 43.73±0.59 43.32±0.68 43.26±0.49 42.76±0.44 44.23±0.63 18.78±0.49 23.23±0.33 26.77±0.42 27.32±0.44 24.68±0.34 28.39±0.52</cell><cell>47.88±0.52 29.54±0.41</cell><cell>8.25% 4.05%</cell></row><row><cell></cell><cell>Author ND</cell><cell>NDCG MRR</cell><cell cols="4">71.45±0.84 71.52±0.82 70.49±0.71 71.44±0.89 71.52±0.66 72.75±0.58 47.46±0.63 48.49±0.56 47.38±0.42 46.32±0.49 48.22±0.59 49.92±0.40</cell><cell>71.99±0.72 51.57±0.47</cell><cell>-1.04% 3.31%</cell></row><row><cell></cell><cell>Paper-Field</cell><cell>NDCG MRR</cell><cell cols="4">28.44±0.32 34.33±0.46 34.40±0.56 34.88±0.60 35.20±0.60 35.94±0.44 23.90±0.42 36.86±0.45 36.43±0.55 37.42±0.46 35.74±0.49 38.28±0.39</cell><cell>37.78±0.53 40.24±0.44</cell><cell>5.12% 5.12%</cell></row><row><cell>Engin</cell><cell>Paper-Venue</cell><cell>NDCG MRR</cell><cell cols="4">41.44±0.67 47.25±0.88 47.53±0.53 46.81±0.69 45.29±0.93 47.96±0.71 23.67±0.76 27.65±0.59 29.29±0.78 30.14±0.88 29.13±0.92 29.79±0.65</cell><cell>50.27±0.65 31.12±0.64</cell><cell>4.81% 4.46%</cell></row><row><cell></cell><cell>Author ND</cell><cell>NDCG MRR</cell><cell cols="4">73.77±1.02 73.92±0.96 74.78±0.74 74.32±0.89 74.56±0.75 75.22±0.82 49.37±0.52 50.87±0.57 51.29±0.63 50.23±0.72 50.37±0.51 52.66±0.64</cell><cell>76.71±0.70 54.47±0.59</cell><cell>1.94% 3.44%</cell></row><row><cell></cell><cell>Paper-Field</cell><cell>NDCG MRR</cell><cell cols="4">31.74±0.46 36.53±0.49 38.45±0.66 39.27±0.49 39.90±0.42 41.76±0.39 23.90±0.37 44.66±0.39 46.25±0.47 47.02±0.42 45.39±0.58 46.70±0.49</cell><cell>42.85±0.50 48.70±0.62</cell><cell>2.61% 4.11%</cell></row><row><cell>Chem</cell><cell>Paper-Venue</cell><cell>NDCG MRR</cell><cell cols="4">30.39±0.60 42.23±0.57 43.56±0.53 44.11±0.51 42.73±0.59 43.05±0.62 13.68±0.32 22.13±0.33 23.27±0.35 22.85±0.44 21.84±0.56 24.19±0.62</cell><cell>46.81±0.43 27.64±0.49</cell><cell>6.12% 14.26%</cell></row><row><cell></cell><cell>Author ND</cell><cell>NDCG MRR</cell><cell cols="4">75.19±0.94 76.71±0.82 75.69±0.69 77.60±0.76 77.65±0.86 78.31±0.85 55.61±0.62 58.33±0.59 57.00±0.44 59.20±0.59 57.57±0.38 60.30±0.55</cell><cell>80.09±0.87 62.91±0.50</cell><cell>2.27% 6.48%</cell></row><row><cell></cell><cell>Paper-Field</cell><cell>NDCG MRR</cell><cell cols="4">32.33±0.36 38.03±0.33 37.12±0.42 38.40±0.49 39.32±0.30 40.76±0.40 28.15±0.48 44.23±0.56 42.96±0.43 43.15±0.55 45.65±0.60 45.70±0.41</cell><cell>42.33±0.62 47.29±0.49</cell><cell>3.85% 3.48%</cell></row><row><cell>OAG</cell><cell>Paper-Venue</cell><cell>NDCG MRR</cell><cell cols="4">42.28±0.50 43.25±0.61 44.23±0.53 43.07±0.74 42.66±0.66 44.05±0.75 22.76±0.37 23.40±0.35 24.38±0.35 24.12±0.42 25.03±0.48 25.19±0.45</cell><cell>47.13±0.68 26.75±0.57</cell><cell>6.56% 6.19%</cell></row><row><cell></cell><cell>Author ND</cell><cell>NDCG MRR</cell><cell cols="4">76.52±1.13 78.01±0.86 77.98±0.93 77.88±0.72 78.11±0.93 79.33±0.87 54.65±0.53 58.00±0.63 58.30±0.48 57.49±0.60 58.22±0.53 59.08±0.52</cell><cell>79.99±0.92 61.32±0.55</cell><cell>0.83% 3.79%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Analysis of different ablated models on various downstream tasks on the CS graph. Task No pre-train PT-HGNN 𝑛𝑜𝑑𝑒 PT-HGNN 𝑠𝑐ℎ𝑒 PT-HGNN</figDesc><table><row><cell cols="2">NDCG Downstream Paper-Field MRR</cell><cell>27.42 23.17</cell><cell>35.80 36.82</cell><cell>35.16 36.21</cell><cell>36.04 37.76</cell></row><row><cell>Paper-Venue</cell><cell>NDCG MRR</cell><cell>27.76 11.39</cell><cell>36.23 20.42</cell><cell>35.24 18.92</cell><cell>38.81 21.19</cell></row><row><cell>Author ND</cell><cell>NDCG MRR</cell><cell>76.27 54.82</cell><cell>80.41 60.57</cell><cell>81.25 62.02</cell><cell>82.19 63.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance of PT-HGNN in freezing and full finetuning modes on the CS graph.</figDesc><table><row><cell cols="2">Downstream Task</cell><cell cols="3">No pre-train PT-HGNN (FE) PT-HGNN</cell></row><row><cell>Paper-Field</cell><cell>NDCG MRR</cell><cell>27.42 23.17</cell><cell>32.81 32.50</cell><cell>36.04 37.76</cell></row><row><cell>Paper-Venue</cell><cell>NDCG MRR</cell><cell>27.76 11.39</cell><cell>35.93 18.45</cell><cell>38.81 21.19</cell></row><row><cell>Author ND</cell><cell>NDCG MRR</cell><cell>76.27 54.82</cell><cell>81.41 62.15</cell><cell>82.19 63.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance and time efficiency of our model with PPR and without PPR-based sparsification on the CS graph.</figDesc><table><row><cell cols="2">Downstream Task</cell><cell cols="3">No PPR PPR Improv.</cell></row><row><cell>Paper-Field</cell><cell>NDCG MRR</cell><cell>36.54 38.12</cell><cell cols="2">36.04 -1.38% 37.76 -0.95%</cell></row><row><cell>Paper-Venue</cell><cell>NDCG MRR</cell><cell>37.82 20.42</cell><cell>38.81 21.19</cell><cell>2.62% 3.77%</cell></row><row><cell>Author ND</cell><cell>NDCG MRR</cell><cell>80.87 60.09</cell><cell>82.19 63.38</cell><cell>1.63% 5.48%</cell></row><row><cell>Efficiency</cell><cell>Time per batch (s)</cell><cell>64.2</cell><cell>37.9</cell><cell>41.97%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://github.com/acbull/GPT-GNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>This work is supported in part by the National Natural Science Foundation of China (No. U20B2045, 61772082, 61702296, 62002029), Key fields R&amp;D project Of Guangdong Province (No. 2020B0101380001). All opinions, findings, conclusions and recommendations are those of the authors and do not reflect the views of the funding agencies.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>11,918,983 107,263,811 27,433 5,597,605 289,930 5,985,759 18,256 5,597,606 31,441,552 47,462,559 15,571,614 7,190,480 Mater 4,552,941 42,161,581 15,141 2,442,235 79,305 2,005,362 10,898 2,442,235 13,011,272 19,119,947 5,582,765 2,005,362 Engin 5,191,920 36,146,719 19,867 3,239,504 99,444 1,819,100 14,005 3,239,504 4,848,158 22,498,822 3,741,135 1,819,100 Chem 12,158,967 159,537,437 19,142 7,193,321 183,782 4,748,812 13,910 7,193,321 74,018,600 57,162,528 16,414,176 4,748,812 OAG 178,663,987 2,236,196,802 53,073 89,606,257 615,288 88,364,081 25,288 89,606,258 1,021,237,518 657,049,405 300,853,688 167,449,933</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A RELATION-BASED PERSONALIZED PAGERANK WITH RANDOM WALK</head><p>The algorithm of relation-based personalized PageRank (PPR) with random walk is described in Algorithm 1. The core idea of this algorithm is to compute an approximate personalized PageRank vector per node per relation type using random walk, and then assemble the resulted vectors into personalized PageRank matrices according to the relation types. Note that in Algorithm 1, we distinguish the relations by both their types and directions, e.g., a relation denoting that an author writes a paper is different from the relation showing that a paper is written by an author. </p><p>end for 7:</p><p>end for 11:</p><p>end if 13: end while 14: return 𝝅 𝑅 for node 𝑡 in relation 𝑅</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PRE-TRAINING PIPELINE AND TIME COMPLEXITY</head><p>The training algorithm for PT-HGNN is outlined in Algorithm 2. We analyze the time complexity of our pre-training pipeline from the following two aspects. Prepare the negative samples with Eq. ( <ref type="formula">1</ref>) 5:</p><p>Prepare the negative schema samples with Eq. ( <ref type="formula">4</ref>) 6:</p><p>for each node 𝑢 in V 𝐵 do 7:</p><p>Prepare the negative samples N 2 𝑢 with Eq. ( <ref type="formula">5</ref>)</p><p>8:</p><p>for each relation 𝑅 ∈ R do Calculate L by Eq. ( <ref type="formula">12</ref>)</p><p>16:</p><p>Update parameters 𝜃 17: end for 18: return Pre-trained GNN model 𝑓 𝜃 * for downstream tasks tasks, 𝑘 1 and 𝑘 2 denote the number of negative samples per positive sample at the node-and schema-level, and 𝑑 means the embedding dimension. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C STRUCTURAL PROPERTY OF GRAPHS</head><p>We extract eight commonly used graph property metrics <ref type="bibr" target="#b23">[24]</ref> as shown in Table <ref type="table">6</ref>, each of which quantifies a specific structural property of graphs. Regarding the five graphs we used in the experiments, the Art graph has a large difference in most of the graph properties from the others. We compute the correlation matrix of different graphs using these graph properties, as shown in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D IMPLEMENTATION OF BASE GNN MODEL AND BASELINES</head><p>We utilize Intel(R) Xeon(R) Platinum 8268 CPU and Tesla V100 to run experiments with both the pre-training and downstream tasks.</p><p>In the sampling stage, we follow the settings of the HGSampling sampler in previous studies <ref type="bibr" target="#b11">[12]</ref> for fair comparison. The parameter settings of the baseline models are as follows: (1) EdgePred <ref type="bibr" target="#b8">[9]</ref> is simply used to predict whether there exists a link between two nodes; (2) DGI <ref type="bibr" target="#b29">[30]</ref>: we apply mean pooling to the whole graphs for the generation of the global graph summary embeddings, and </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>Rózemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PushNet: Efficient and Adaptive Neural Message Passing</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1039" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic proximity search on graphs with metagraphbased learning</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">Wenchen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="277" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TheWebConf</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling personalized web search</title>
		<author>
			<persName><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sampling from large graphs</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="631" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Deep Generative Models of Graphs</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">SGDR: Stochastic gradient descent with warm restarts</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to Pre-train Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4276" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Strong localization in personalized PageRank vectors</title>
		<author>
			<persName><forename type="first">Huda</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Kloster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Gleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Algorithms and Models for the Web-Graph</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="190" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The structure and function of complex networks</title>
		<author>
			<persName><surname>Mark Ej Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="167" to="256" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>KDD. 1150-1160</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A Survey of Heterogeneous Information Network Analysis</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="17" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Path-Sim: Meta Path-Based Top-K Similarity Search in Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Preserving Personalized Pagerank in Subgraphs</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vattani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Gurevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="793" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Attention Network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>WWW. 2022-2032</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">OAG: Toward linking large-scale heterogeneous entity graphs</title>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2585" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
