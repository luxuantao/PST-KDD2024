<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNDERSTANDING AND QUANTIFYING ADVERSARIAL EXAMPLES EXISTENCE IN LINEAR CLASSIFICATION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-29">October 29, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xupeng</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">A</forename><forename type="middle">Adam</forename><surname>Ding</surname></persName>
							<email>a.ding@northeastern.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UNDERSTANDING AND QUANTIFYING ADVERSARIAL EXAMPLES EXISTENCE IN LINEAR CLASSIFICATION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-29">October 29, 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1910.12163v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-art deep neural networks (DNN) are vulnerable to attacks by adversarial examples: a carefully designed small perturbation to the input, that is imperceptible to human, can mislead DNN. To understand the root cause of adversarial examples, we quantify the probability of adversarial example existence for linear classifiers. Previous mathematical definition of adversarial examples only involves the overall perturbation amount, and we propose a more practical relevant definition of strong adversarial examples that separately limits the perturbation along the signal direction also. We show that linear classifiers can be made robust to strong adversarial examples attack in cases where no adversarial robust linear classifiers exist under the previous definition. The quantitative formulas are confirmed by numerical experiments using a linear support vector machine (SVM) classifier. The results suggest that designing general strong-adversarial-robust learning systems is feasible but only through incorporating human knowledge of the underlying classification problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The deep neural networks (DNN) are widely used as the state-of-art machining learning classification systems due to its great performance gains in recent years. Meanwhile adversarial examples, first pointed out by <ref type="bibr" target="#b0">Szegedy et al. (2014)</ref>, emerges as a novel peculiar security threat against such systems: a small perturbation that is unnoticeable to human eyes can cause the DNNs to misclassify. Various adversarial algorithms have since been developed to efficiently find adversarial examples <ref type="bibr" target="#b1">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b2">Moosavi-Dezfooli et al., 2016;</ref><ref type="bibr" target="#b3">Carlini and Wagner, 2017;</ref><ref type="bibr" target="#b4">Madry et al., 2018)</ref>. The adversarial examples have also been demonstrated to misled DNN based classification systems in physical world applications <ref type="bibr" target="#b5">(Sharif et al., 2016;</ref><ref type="bibr" target="#b6">Brown et al., 2017;</ref><ref type="bibr" target="#b7">Kurakin et al., 2018;</ref><ref type="bibr">Athalye et al., 2018a)</ref>. Various defense methods have also been proposed to prevent adversarial example attacks: Adversarial training <ref type="bibr" target="#b0">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b1">Goodfellow et al., 2015)</ref>; Defensive distillation <ref type="bibr" target="#b9">Papernot et al. (2016)</ref>; Minmax robust training <ref type="bibr" target="#b4">(Madry et al., 2018;</ref><ref type="bibr" target="#b10">Sinha et al., 2018)</ref>; Input transformation <ref type="bibr" target="#b11">Xu et al. (2017)</ref>. However, many of the defenses are shown to be vulnerable to attacks taking such defense strategies into consideration <ref type="bibr">(Athalye et al., 2018b)</ref>.</p><p>Recently, <ref type="bibr" target="#b13">Shafahi et al. (2019)</ref> showed that, for two classes of data distributed with bounded probability densities on a compact region of a high dimensional space, no classifier can both have low misclassification rate and be robust to adversarial examples attack. So are we left hopeless against such threat? Theoretical analysis for understanding adversarial examples is needed to address this issue. <ref type="bibr" target="#b1">Goodfellow et al. (2015)</ref>; <ref type="bibr" target="#b14">Fawzi et al. (2018)</ref> pointed out that susceptibility of DNN classifiers to adversarial attacks could be related to their locally linear behaviours. The existence of adversarial examples is not unique to DNN, traditional linear classifiers also have adversarial examples. In this paper, we extend the understanding of adversarial examples by quantifying the probability of their existence for a simple case of linear classifiers that performs binary classification on Gaussian mixture data.</p><p>In previous literature, a data point x is mathematically defined as having an adversarial example x = x + v when the perturbation amount v is small and x is classified differently from x. This definition does not exclude genuine signal perturbation. For example, if a dog image x is perturbed to an image x that is classified as a cat by both human and the machine classifier, then x should not be an adversarial example even if v = x − x is small. The proper definition needs to capture the novelty of adversarial examples attack: while a human would consider two images x and x very similar and consider both clearly as dogs, a machine classifier misclassifies x as a cat. While defining genuine signal perturbation for general learning problems is difficult mathematically, the signal perturbation is clear in the binary linear classification for Gaussian mixture data. We therefore propose a new definition of strong-adversarial examples that limits the perturbation amount in the signal direction separately from the limit on overall perturbation amount.</p><p>In this paper, we derive quantitative formulas for the probabilities of adversarial and strong-adversarial examples existence in the binary linear classification problem. Our quantitative analysis shows that an adversarial-robust linear classifier requires much higher signal-to-noise ratio (SNR) in data than a good performing classifier does. Therefore, in many practical applications, adversarial-robust classifiers may not be available nor are such classifiers desirable. On the contrary, useful strong-adversarial-robust linear classifiers exists at the SNR similar to that required by the existence of any useful linear classifiers, however, they require better designed training algorithms.</p><p>The paper is organized as follows. Section 2 presents the notations and definitions of (strong-)adversarial examples and derive explicit formulas for the probability of their existence. Section 3 presents numerical experiments. The formulas are confirmed experimentally, and then are used to illustrate their implication on the vulnerability against (strong-)adversarial example attacks. Section 4 discusses how our results relate to some works in literature and summarize their implication on general adversarial attack defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Adversarial Rates Analysis of Linear Binary Classifier on Gaussian Mixture Data</head><p>We first introduce our definitions of adversarial and strong-adversarial examples, and then we characterize their existence through defining sets. Using the defining sets, we derive explicit probability rates of (strong-)adversarial examples existence for linear classifiers on Gaussian mixture data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition of Adversarial and Strong-Adversarial Examples</head><p>The classical adversarial examples are defined as follows:</p><p>Definition 1.<ref type="foot" target="#foot_0">1</ref> Given a classifier C, an ε-adversarial example of a data vector x is another data vector x such that</p><formula xml:id="formula_0">x − x ≤ ε but C(x) = C(x ).</formula><p>Without loss of generality, in this paper we focus on 2 norm perturbations. If not specified, • in the following refers to the 2 norm. The general p norm (p ≥ 1) perturbation is studied in the Appendix 5, and the results will be stated in the discussion section.</p><p>For a general machine classification problem, it is reasonable to only consider adversarial examples since the signal direction is often not easily definable mathematically. Here we consider the simple binary linear classification of Gaussian mixture data where the signal direction can be clearly distinguished. For two classes labeled '+' and '−' respectively, a linear classifier is C(x; w, b) = {w • x + b &gt; 0} where '•' denotes the inner product of two vectors. Here the parameters w and b are respectively the weight vector and the bias term. For the classical Gaussian mixture data problem, for each of the two classes, the d-dimensional data vector x comes from a multivariate Gaussian distribution N (µ i , σ<ref type="foot" target="#foot_1">2</ref> i I d ), i = '+' or '−'. Notice the optimal ideal classifier here is the Bayes classifier</p><formula xml:id="formula_1">C(x; µ, μ) = {µ • (x − μ) &gt; 0} 2 where µ = 1 2 (µ + − µ − ), μ = 1 2 (µ + + µ − ).</formula><p>For this problem, the data distributions of the two classes only differ in their means µ + and µ − . Thus the signal direction is µ 0 = µ/ µ . Adding 2 µ amount of perturbation along the signal direction changes the '−' class data distribution to the '+' class data distribution exactly, rending all classifiers unable to defend against such a perturbation.</p><p>In previous literature, the adversarial examples definition does not limit perturbation along the signal direction, therefore we propose a new definition that limits the perturbation along the signal direction separately by an amount δ, we will refer these examples as strong-adversarial examples .</p><p>Definition 2. Given a classifier C, an (ε, δ)-strong-adversarial example of a data vector x is another data vector 2 (tanh 2x 3 + 1) and then displayed in grey scale as a 19 × 19 image <ref type="bibr" target="#b3">(Carlini and Wagner, 2017)</ref>. The two means µ + and µ − are chosen to be zero at every component of the vector except the component corresponding to center grid cell (shown with red boundary in Figure <ref type="figure" target="#fig_1">1</ref>). Hence the optimal Bayes classifier identifies the image as from '+' (or '−') class when the center grid cell within the red boundary appears to be white (or black). With a perturbation amount of ε = 0.3 × 19 = 5. </p><formula xml:id="formula_2">x such that x − x ≤ ε and |(x − x ) • µ 0 | ≤ δ but C(x) = C(x ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Defining Sets</head><p>Here we characterize the defining sets where the (strong-)adversarial examples exist. Then we quantify the probability of data falling into these defining sets in the next subsection 2.3.</p><p>We denote Ω ε = {x : x has an ε-adversarial example} and Ω ε,δ = {x : x has an (ε, δ)-strong-adversarial example}. Furthermore, for a fixed perturbation v, we denote the set where v changes classification as</p><formula xml:id="formula_3">Ω(v) = {x ∈ R d : C(x + v) = C(x)}.</formula><p>For any data point x in Ω ε , there exists a v with v ≤ ε such that x + v is classified differently from x. In other words, the distance of x from the classifier's decision boundary is less than ε. For a linear classifier C(x; w, b) = {w • x + b &gt; 0}, the normal direction of its decision boundary is v 0 = w/ w . Thus, perturbing x by ε amount along one of the two directions v 0 or −v 0 will cross the linear decision boundary. That is, Ω ε ⊆ Ω(εv 0 ) ∪ Ω(−εv 0 ). Since it is obvious from the definition that Ω ε = v ≤ε Ω(v) ⊇ Ω(εv 0 ) ∪ Ω(−εv 0 ), we have Ω ε = Ω(εv 0 ) ∪ Ω(−εv 0 ). In summary, to judge if x ∈ Ω ε , we only need to check the perturbation along the normal direction v 0 .</p><p>In contrast, our definition of strong-adversarial examples only allows δ amount of perturbation along the signal notation µ 0 , hence it is not sufficient to only check perturbations εv 0 and −εv 0 for judging if x ∈ Ω ε,δ . Let θ denote the deflected angle between µ 0 and v 0 . (Without loss of generality, we choose the θ value such that 0 ≤ θ ≤ π/2.) Then we can decompose v 0 into two components along and orthogonal to the signal direction µ 0 respectively. That is, v 0 = cos θµ 0 + sin θn 0 where n = v 0 − (v 0 • µ 0 )µ 0 and n 0 = n/ n . When ε cos θ ≤ δ, the adversarial example resulting from the εv 0 perturbation is also strong-adversarial by definition. When ε cos θ &gt; δ, however, εv 0 is no longer an allowable perturbation in the strong-adversarial example definition. Then we need to check whether classification change is caused by a perturbation of δ amount along µ 0 direction and √ ε 2 − δ 2 amount along n 0 direction. That is, , to judge if x ∈ Ω ε,δ , we need to check perturbations u 2 = δµ 0 + √ ε 2 − δ 2 n 0 and −u 2 . We summarize the defining sets characterization in the following lemma whose detailed proof is in the Appendix 5.1. Lemma 1. The defining sets for ε-adversarial and (ε, δ)-strong-adversarial examples are given by:</p><formula xml:id="formula_4">Ω ε = Ω(εv 0 ) ∪ Ω(−εv 0 ); Ω ε,δ = Ω(u 2 ) ∪ Ω(−u 2 ) (1)</formula><p>where</p><formula xml:id="formula_5">u 2 = βµ 0 + ε 2 − β 2 n 0 , β = min(ε cos θ, δ).</formula><p>Next, we use these defining sets to quantify the probabilities of (strong-)adversarial example existence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adversarial and Strong-Adversarial Rates</head><p>For the binary classification problem, a random data vector comes from the Gaussian mixture distribution p(x</p><formula xml:id="formula_6">) = λ + ϕ + (x) + λ − ϕ − (x)</formula><p>, where ϕ i (x) is the probability density function of the multivariate Gaussian N (µ i , σ 2 i I d ) and λ i is the probability that the data vector belongs to the class of i = '+' or '−'. For simplicity, we focus on the balanced classes case of λ + = λ − = 0.5 and also σ</p><formula xml:id="formula_7">+ = σ − = σ.</formula><p>Adversarial Rate For a random data vector x from the '+' class, it has an ε-adversarial example x if it is classified correctly by w • x + b &gt; 0 and x ∈ Ω(−εv 0 ). Thus the adversarial rate from the '+' class is</p><formula xml:id="formula_8">λ + pr[w • x + b &gt; 0, w • (x − εv 0 ) + b &lt; 0 |ϕ + (x)] = 0.5pr[0 &lt; w • x + b &lt; ε w |ϕ + (x)].</formula><p>(2)</p><p>Since under the multivariate Gaussian</p><formula xml:id="formula_9">N (µ + , σ 2 I d ) distribution ϕ + (x), w • x + b is a univariate Gaussian random variable with mean w • µ + + b and variance w 2 σ 2 , the above quantity becomes 0.5 Φ ε w − (w • µ + + b) w σ − Φ −(w • µ + + b) w σ .<label>(3)</label></formula><p>Here Φ(•) denotes the cumulative distribution function (CDF) of the standard Gaussian distribution N (0, 1). Similarly, the adversarial rate from the '−' class is</p><formula xml:id="formula_10">λ − pr[−ε w &lt; w • x + b &lt; 0|ϕ − (x)] = 0.5 Φ −(w • µ − + b) w σ − Φ −ε w − (w • µ − + b) w σ . (4) Recall µ = 1 2 (µ + − µ − ), μ = 1 2 (µ + + µ − ).</formula><p>If we denote b = w • μ + b, then we can rewritten the expressions as <ref type="formula" target="#formula_9">3</ref>) and (4), we have the overall adversarial rate as</p><formula xml:id="formula_11">w • µ ± + b = ±w • µ + b . Combining equations (</formula><formula xml:id="formula_12">p adv = 0.5 Φ ε σ − w•µ+b w σ − Φ − w•µ+b w σ + Φ w•µ−b w σ − Φ − ε σ + w•µ−b w σ = 0.5 Φ w•µ+b w σ − Φ w•µ+b w σ − ε σ + Φ w•µ−b w σ − Φ w•µ−b w σ − ε σ (5)</formula><p>Also notice that the misclassification rates from the two classes are respectively</p><formula xml:id="formula_13">λ + Φ[−(w • µ + + b)/( w σ)] = 0.5{1 − Φ[(w • µ + b )/( w σ)]} and λ − {1 − Φ[−(w • µ − + b)/( w σ)]} = 0.5{1 − Φ[(w • µ − b )/( w σ)]}.</formula><p>Thus the overall misclassification rate is</p><formula xml:id="formula_14">p m = 1 − 0.5 Φ w•µ+b w σ + Φ w•µ−b w σ . (<label>6</label></formula><formula xml:id="formula_15">)</formula><p>We combine equations ( <ref type="formula">5</ref>) and ( <ref type="formula" target="#formula_14">6</ref>) into the following Theorem.</p><p>Theorem 1. The overall adversarial rate of a linear classifier for the balanced Gaussian mixture data is</p><formula xml:id="formula_16">p adv = 1 − p m − 0.5 Φ w•µ+b w σ − ε σ + Φ w•µ−b w σ − ε σ .<label>(7)</label></formula><p>To be robust against adversarial attacks, a linear classifier needs a low adversarial rate. For the classifier to be useful, it also needs a low misclassification rate. Hence we should look at the sum of misclassification rate and adversarial rate, which we call the adversarial-error rate:</p><formula xml:id="formula_17">p err = p adv + p m = 1 − 0.5 Φ w•µ+b w σ − ε σ + Φ w•µ−b w σ − ε σ (8)</formula><p>Comparing equation ( <ref type="formula">8</ref>) with ( <ref type="formula" target="#formula_14">6</ref>), we can see why adversarial-robustness is hard to achieve.</p><p>First, the misclassification rate p m in ( <ref type="formula" target="#formula_14">6</ref>) is minimized by the Bayes classifier with b = 0 and w • µ = w µ . Hence the best p m value is 1 − Φ( µ /σ). There exists useful classifiers when µ /σ is big enough to make 1 − Φ( µ /σ) small. This is achieved for µ /σ = O(1). For example, when µ /σ = 3, the misclassification rate of the Bayes classifier is around 0.1%.</p><p>However, to achieve a low adversarial-error rate in (8), the required SNR µ /σ can be much bigger. When w • µ &gt; ε w , a lower bound for the adversarial-error rate is</p><formula xml:id="formula_18">p err ≥ 1 − Φ w • µ w σ − ε σ ≥ 1 − Φ µ σ − ε σ .<label>(9)</label></formula><p>Therefore, the existence of a useful adversarial-robust linear classifier requires µ /σ − ε/σ = O(1) instead. Notice that, for this Gaussian mixture data setup, the noise in each class follows the N (0, σ 2 I d ) distribution with an expected square of 2 norm of dσ 2 . Therefore, for a positive constant value η a &lt; 1, the perturbation amount of ε = η a √ dσ is smaller than the average noise in data and generally is hard to detect. Hence, for the typical high-dimensional data applications, an adversarial-robust linear classifier needs to protect against perturbation amount of ε = O(</p><formula xml:id="formula_19">√ d) which implies that µ /σ = O( √ d</formula><p>) is needed from equation ( <ref type="formula" target="#formula_18">9</ref>). Next, we show that this high SNR requirement is not needed for a strong-adversarial-robust linear classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strong-Adversarial Rate</head><p>The derivation of the strong-adversarial rate is very similar to that of the adversarial rate. From equation ( <ref type="formula">1</ref>), the difference between the adversarial defining set and the strong-adversarial defining set is only that εv 0 is replaced by u 2 = βµ 0 + ε 2 − β 2 n 0 . Hence the strong-adversarial rate from the '+' class is</p><formula xml:id="formula_20">0.5pr[0 &lt; w • x + b &lt; w • u 2 |ϕ + (x)]. Since w • µ 0 = w cos θ and w • n 0 = w sin θ, we have w • u 2 = (β cos θ + ε 2 − β 2 sin θ) w where β = min(ε cos θ, δ). We denote g(ε, δ, θ) = β cos θ + ε 2 − β 2 sin θ. (<label>10</label></formula><formula xml:id="formula_21">)</formula><p>Thus replacing ε w by g(ε, δ, θ) w in equations from (3) to (8), we have the following Theorem.</p><p>Theorem 2. The overall strong adversarial rate and strong-adversarial-error rate of a linear classifier are</p><formula xml:id="formula_22">p s−adv = 1 − p m − 0.5 Φ w • µ + b w σ − g(ε, δ, θ) σ + Φ w • µ − b w σ − g(ε, δ, θ) σ (11) p s−err = p s−adv + p m = 1 − 0.5 Φ w • µ + b w σ − g(ε, δ, θ) σ + Φ w • µ − b w σ − g(ε, δ, θ) σ (12)</formula><p>Compared to the analysis above, the existence of a useful strong-adversarial-robust linear classifier requires µ /σ − g(ε, δ, θ)/σ = O(1) instead. Besides the overall perturbation amount ε, the function g(ε, δ, θ) in equation ( <ref type="formula" target="#formula_20">10</ref>) is also affected by two other factors: the signal direction perturbation amount δ and the angle θ between the classifier and the ideal Bayes classifier. What is the practical relevant amount δ we should study? Let δ = η s µ = η s µ . When η s &gt; 1, a δ amount perturbation along the signal direction to all '+' class data points will make more than half of them be classified as '−' by the Bayes classifier (also to human eye, e.g., Figure <ref type="figure" target="#fig_3">1(c)</ref>). Therefore, when studying real strong-adversarial perturbations (imperceptible to human but confuses machine) mathematically, we need to focus on η s &lt; 1. That is, δ = O(1). Compared to the overall perturbation amount ε = O( √ d) discussed earlier, we see that δ ε for typical high-dimensional data applications. When δ ε, g(ε, δ, θ) ≈ δ cos θ + ε sin θ. Hence if the linear classifier is well-trained to have small θ and small bias b (i.e., very close to the Bayes classifier), then its strong-adversarial-error rate is approximately 1 − Φ[(1 − η s ) µ /σ], which can be made small when SNR µ /σ is of order O(1). That is, with good training, we can find a useful strong-adversarial-robust linear classifier when µ /σ = O(1). In contrast, no training can make the linear classifier to be useful and adversarial-robust unless the SNR µ /σ is much bigger, at the order of O(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>√ d).</head><p>The conclusion for the analysis using p norm (see Appendix 5 for details) is similar. There exists a useful strongadversarial-robust linear classifier for constant order SNR µ /σ = O(1), but a useful p -adversarial-robust linear classifier only exists when SNR is much bigger, at the order of O(d min(1/p,1/2) ).</p><p>3 Numerical Studies and Analysis of Adversarial Examples  <ref type="formula" target="#formula_14">6</ref>), ( <ref type="formula">8</ref>) and ( <ref type="formula">12</ref>). Figure <ref type="figure" target="#fig_5">2</ref>  In our simulation, µ = µ /1 = µ /σ is the SNR. Figure <ref type="figure" target="#fig_5">2</ref> shows that SVM have pretty good performance in terms of misclassification rate once the SNR exceeds 2. However, it is not robust to (strong-)adversarial attacks when µ = 2, and will only become robust for much larger SNR. The part of curves for µ &lt; 2 have some fluctuations due to the fact that the bias term b varies a lot when SNR is small. When µ ≥ 2, the SVM has b ≈ 0, and we can approximate the (strong-)adversarial-error rate by dropping the bias term in ( <ref type="formula">8</ref>) and ( <ref type="formula">12</ref>) and replace θ with its asymptotic limit as given by solving (θ, t) from the equations <ref type="bibr" target="#b15">(Huang, 2017)</ref>:</p><formula xml:id="formula_23">sin 2 θ = N d t −∞ (t − z) 2 ϕ(z)dz, cos θ = N d • µ σ t −∞ (t − z)ϕ(z)dz<label>(13)</label></formula><p>where ϕ(z) is the density function of standard normal distribution. The rates plotted with these approximate formulas overlap the curves on Figure <ref type="figure" target="#fig_5">2</ref> very well for the part µ ≥ 2. We use these formulas to study the robustness of SVM against (strong-)adversarial examples.</p><p>Figure <ref type="figure" target="#fig_6">3</ref>(a) plots the three error rates formulas of SVM when η = 0.3. Figure <ref type="figure" target="#fig_6">3</ref>(b) plots the same rates for the Bayes classifier. These two classifiers are similar in misclassification rates and adversarial-error rates, but are very different in strong-adversarial-error rates. For a linear classier with small bias b ≈ 0, equations ( <ref type="formula" target="#formula_14">6</ref>), ( <ref type="formula">8</ref>) and ( <ref type="formula">12</ref>) become:</p><formula xml:id="formula_24">p m ≈ 1 − Φ µ σ cos θ , p err ≈ 1 − Φ µ σ − ε σ cos θ , p s−err ≈ 1 − Φ µ σ − δ σ cos θ − ε σ sin θ<label>(14)</label></formula><p>Setting θ = 0, we get the theoretical optimal rates achieved by the ideal Bayes classifier:</p><formula xml:id="formula_25">p id m = 1 − Φ µ σ , p id err = 1 − Φ µ σ − ε σ , p id s−err = 1 − Φ µ σ − δ σ . (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>Comparing equations ( <ref type="formula" target="#formula_24">14</ref>) and ( <ref type="formula" target="#formula_25">15</ref>), between the Bayes classifier and a linear classifier with small bias, both the misclassification rate and adversarial-error rate differ by a factor of cos θ inside the Φ(•) function. However, comparing their strong-adversarial-error rates, besides the multiplicative factor cos θ, there is also an extra bias term</p><formula xml:id="formula_27">− ε σ sin θ inside the Φ(•) function. Since ε σ is of order O( √ d), θ = o(1/ √ d)</formula><p>is needed for the linear classifier to approach the optimal strong-adversarial-error rate. In contrast, for the misclassification rate and adversarial-error rate, only θ = o(1) is needed to approach the optimal rates. Figure <ref type="figure" target="#fig_6">3(c</ref>) plots these three rates versus cos θ when η = 0.3 and µ = 2. We can see that misclassification rate is low for a wide range of cos θ values while the strong-adversarial-error rate only becomes low when cos θ is very close to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Defending Against Strong-Adversarial Example Attacks</head><p>We have just seen that training a strong-adversarial-robust classifier needs stricter training requirements than those for a classifier with low misclassification rate:</p><formula xml:id="formula_28">θ = o(1/ √ d) versus θ = o(1)</formula><p>. This is doable by incorporating some extra knowledge about the classification setting into the training. As an illustration, we show the results of using a naive method to find a sparse SVM in this case: for the SVM trained using standard method, takes ten non-zero components of w with largest absolute coefficients and set rest of components zero. The left panel of Figure <ref type="figure" target="#fig_7">4</ref> plots the strong-adversarial-error rates of this sparse SVM versus original SVM. We can see that the sparse SVM achieves a low strong-adversarial-error rate very close to the optimal rate of the ideal Bayes classifier. However, the same way of finding a sparse SVM does not produce strong-adversarial-robust classifier, shown in the right panel of Figure <ref type="figure" target="#fig_7">4</ref>, when the data are generated with µ + = (µ, µ, ..., µ)/ √ d instead of µ + = (µ, 0, ..., 0). The data distributions in these two cases are equivalent with a change of coordinate systems. The sparse SVM fails in the second case since the extra knowledge incorporated into training is incorrect (sparseness only happens in the first coordinate system but not in the second coordinate system).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions and Conclusions</head><p>In this paper, we provide clear definitions of adversarial and strong adversarial examples in the linear classification setting. Quantitative analysis shows that adversarial examples are hard to avoid but also should not be of concern in practice. Rather, we should focus on finding strong-adversarial-robust classifiers. We now consider the implications of these results on studying adversarial examples for general classifiers, and their relationship to some recent works in literature.</p><p>Recently, <ref type="bibr" target="#b13">Shafahi et al. (2019)</ref> shows that no classifier can achieve low misclassification rate and also be adversarialrobust for data distributions with bounded density on a compact region in a high-dimensional space. Our analysis does not match exactly with their impossibility statement because we are studying the Gaussian mixture case, which has positive density on the whole space. However, in spirit our results have similar implications: for the usual SNR O(1) that allows low misclassification rate, generally it is impossible to be also adversarial-robust (for which a much bigger SNR O(</p><formula xml:id="formula_29">√ d) is required).</formula><p>Our results, however, do show that there can be adversarial-robust classifiers under the traditional definition when the SNR is very big. Schmidt et al. ( <ref type="formula">2018</ref>) has also shown that, for Gaussian mixture classification problem and a particular training method, the adversarial-robustness is achievable but requires more training data than simply achieving the low misclassification rate only. Our formula indicates that useful adversarial-robust classifier do exist at the SNR level they assumed. Our study is more focused on the fundamental issue of when useful adversarial-robust classifiers exist, not which training method and what data complexity will find such a classifier. However, our formulas do indicate that an adversarial-robust classifier has to satisfy a stricter requirement than a good performing classifier. Thus either a better training method or a higher data complexity is needed for finding a useful adversarial-robust classifier, agreeing with the general theme of <ref type="bibr" target="#b4">Schmidt et al. (2018)</ref>.</p><p>Our results on the existence of adversarial examples do not change qualitatively when using other p norm to measure the perturbation: under traditional definition, useful adversarial-robust classifier exists only when the data distribution has a very big SNR of O(d min(1/p,1/2) ) as shown in the Appendix 5. For many applications where good classifiers exists (SNR of only O(1) ensures this), we can not pursue adversarial-robust classifier under the traditional adversarial example definition 1. The current defense strategies based on such adversarial example definition likely will still be suspect to more sophisticated adversarial attacks. For certifiable adversarial-robust classifiers <ref type="bibr" target="#b4">(Madry et al., 2018;</ref><ref type="bibr" target="#b10">Sinha et al., 2018)</ref>, the robustness is achieved only for the perturbation amount ε high enough so that they differ from human in classifying images like those in Figure <ref type="figure" target="#fig_1">1</ref>(c) and Figure <ref type="figure" target="#fig_8">5(c</ref>). Thus a paradigm change is needed: we should train a classifier to be strong-adversarial-robust rather than adversarial-robust.</p><p>While the signal direction is obvious in the linear classification, the signal direction and the definition of strongadversarial examples in general classification warrants further study. The signal direction in the linear classification here is the direction where the likelihood ratio of the two classes changes most rapidly. One reasonable extension is to define the signal direction at any data vector x as the gradient direction of the likelihood ratio at x. Then similar to definition 2, the strong-adversarial example for general classifier also restrict the change along this signal direction to the amount δ. The strong-adversarial-robust classifiers therefore are likely to be very close to the Bayes classifier. G. O'Brien, "Limit theorems for the maximum term of a stationary process," The Annals of Probability, pp. 540-545, 1974.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Proof of Lemma 1</head><p>Lemma 2. The defining sets for ε-adversarial and (ε, δ)-strong adversarial examples are given by:</p><formula xml:id="formula_30">Ω ε = Ω(εv 0 ) ∪ Ω(−εv 0 ); Ω ε,δ = Ω(u 2 ) ∪ Ω(−u 2 ) (<label>16</label></formula><formula xml:id="formula_31">)</formula><p>where</p><formula xml:id="formula_32">u 2 = βµ 0 + ε 2 − β 2 n 0 , β = min(ε cos θ, δ).</formula><p>Proof. Proof of the adversarial defining set formula. Since it is obvious from the definition that</p><formula xml:id="formula_33">Ω ε = v ≤ε Ω(v) ⊇ Ω(εv 0 ) ∪ Ω(−εv 0 ), we only need to show that Ω ε ⊆ Ω(εv 0 ) ∪ Ω(−εv 0 ).</formula><p>That is, for any data point x ∈ Ω ε , either x + εv 0 or x − εv 0 changes its classification.</p><p>We now claim that the last statement is equivalent to that εv 0 is the solution to the optimization problem:</p><formula xml:id="formula_34">max w • v, v ∈ D ε = {v ∈ R d : v ≤ ε}.<label>(17)</label></formula><p>To see this, if εv 0 is the solution, then w • v ≤ w • (εv 0 ) = ε w for all v ∈ D 1 . Now for a x classified into the '−' class and x ∈ Ω ε , then w • x + b &lt; 0 and w • (x + v) + b &gt; 0. Hence</p><formula xml:id="formula_35">w • (x + εv 0 ) + b ≥ w • x + w • v + b &gt; 0,</formula><p>that is, x + εv 0 is misclassified into the '+' class thus x ∈ Ω(εv 0 ). By symmetry, −εv 0 is the solution to min w • v when v ∈ D 1 , and hence −ε w ≤ w • v also for all v ∈ D 1 . Hence for a x classified into the '+' class and x ∈ Ω ε , similarly we have that x − εv 0 is misclassified into the '−' class thus x ∈ Ω(−εv 0 ).</p><p>Finally, εv 0 is indeed the solution to (17) due to the Cauchy-Schwartz inequality w • v ≤ w v ≤ w ε. The first equality holds if and only if v is along the same direction of w, thus v = cv 0 . The second equality holds if and only if v = ε, thus v = εv 0 . This finishes the proof for Ω ε = Ω(εv 0 ) ∪ Ω(−εv 0 ).</p><p>Proof of the strong adversarial defining set formula. The proof follows exactly the outline of the adversarial case proof above. Only now we need to prove that u 2 is the solution to the optimization problem</p><formula xml:id="formula_36">max w • v, v ∈ D ε,δ = {v ∈ R d : v ≤ ε, |v • µ 0 | ≤ δ}. (<label>18</label></formula><formula xml:id="formula_37">)</formula><p>We can decompose w as w = (w</p><formula xml:id="formula_38">• µ 0 )µ 0 + (w • n 0 )n 0 , accordingly, v can be decomposed as v = (v • µ 0 )µ 0 + (v • n 0 )n 0 + (v • m 0 )m 0</formula><p>, where m 0 is the unit normal vector of the plane spanned by µ 0 and w, therefore</p><formula xml:id="formula_39">w • v = (w • µ 0 )(v • µ 0 ) + (w • n 0 )(v • n 0 ) = cos θ(v • µ 0 ) + sin θ(v • n 0 ) := x cos θ + y sin θ. (<label>19</label></formula><formula xml:id="formula_40">)</formula><p>The optimization problems becomes to maximize x cos θ + y sin θ in (19) under the constraints</p><formula xml:id="formula_41">x 2 + y 2 = ε 2 − (v • m 0 ) 2 , |x| ≤ δ.</formula><p>This is a linear programming setup, it is easy to see that first we must have v •m 0 = 0 to reach maximum. Then the solution is either at the corner (x, y) = (δ, √ ε 2 − δ 2 ) or at the tangent point (x, y) = ε(cos θ, sin θ) as in semi-adversarial case. If ε cos θ &lt; δ, the solution is at the tangent point (x, y) = ε(cos θ, sin θ). Otherwise, the solution is at the corner (x, y) = (δ, √ ε 2 − δ 2 ). Combining the two cases, we arrive at the formula for u 2 under equation ( <ref type="formula" target="#formula_30">16</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">p -Adversarial and p -Strong-Adversarial Rates</head><p>In literature, the adversarial examples have been studied under different norms. Here we extend the analysis in main text to the general p norms with p ∈ [1, ∞]<ref type="foot" target="#foot_2">3</ref> . That is, we use the distance metric d p (x, y) = x − y p . Also, we denote q as the dual of p , i.e., 1/p + 1/q = 1.</p><p>Therefore the classical adversarial examples definition becomes the following. Definition 3. Given a classifier C, an εp -adversarial example of a data vector x is another data vector x such that</p><formula xml:id="formula_42">d p (x, x ) ≤ ε but C(x) = C(x ).</formula><p>As before, we restrict the perturbation amount along the signal direction µ 0 to δ for strong-adversarial examples. Definition 4. Given a classifier C, an (ε, δ)p -strong-adversarial example of a data vector x is another data vector</p><formula xml:id="formula_43">x such that d p (x, x ) ≤ ε and |(x − x ) • µ 0 | ≤ δ but C(x) = C(x ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">p -Adversarial Rate and Existence of p -Adversarial-Robust Classifiers</head><p>The analysis follows the same outline as the analysis for the 2 norm case. We first characterize the defining set Ω ε|p = {x : x has an ε − p -adversarial example}. Lemma 3. The defining sets for εp -adversarial examples is given by:</p><formula xml:id="formula_44">Ω ε|p = Ω(εv 0|p ) ∪ Ω(−εv 0|p ) (20)</formula><p>where v 0|p is the d-dimensional vector with component</p><formula xml:id="formula_45">(v 0|p ) i = sgn(w i ) • (|w i |/ w q ) q−1 .</formula><p>Here sgn denotes the sign function. That is, sgn(x) = 1 for x &gt; 0; sgn(x) = −1 for x &lt; 0 and sgn(0) = 0.</p><p>Furthermore, we denote the p-th power of a vector v = (v 1 , ..., v d ) as taking the power component-wise. That is,</p><formula xml:id="formula_46">(v p ) i = sgn(v i ) • |v i | p .</formula><p>Then the above v 0|p can be rewritten as v 0|p = (w/ w q ) q−1 .</p><p>Proof. The proof is similar to the proof of Lemma 2. Following the derivations there, we only need to show that v 0 = (w/ w q ) q−1 is the solution to the optimization problem:</p><formula xml:id="formula_47">max |w • v|, v ∈ D ε|p = {v ∈ R d : v p ≤ ε}.<label>(21)</label></formula><p>By Holder's inequality, we have |w • v| ≤ w q v p ≤ ε w q .</p><p>For the first "≤" to be "=", v p has to be proportional to w q . That is, for some constant c, v = cw q/p = cw q−1 . For the second "≤" to be "=", we need ε = v p . That is,</p><formula xml:id="formula_48">ε p = v p p = c p d i=1 |v i | p = c p d i=1 (|w i | q/p ) p = c p d i=1 (|w i | q ) = c p w q q .</formula><p>Hence we have ε = c w q/p q = c w q−1 q</p><p>, and thus c = ε w 1−q q</p><p>. Plug c into v = cw q−1 , we get v = ε(w/ w q ) q−1 = εv 0|p . This is the solution to the optimization problem. Hence arguments similar to those for the proof of Lemma 2 above show that the equation (20) gives the defining set here.</p><p>With the characterization lemma 3, we can then compute the adversarial rate as before. Note that the misclassification rate has nothing to do with the perturbation for adversarial examples. Thus regardless of which p norm is used to measure the perturbation, the misclassification rate is still given by the same formula as before.</p><formula xml:id="formula_49">p m = 1 − 0.5 Φ w • µ + b w σ + Φ w • µ − b w σ . (<label>22</label></formula><formula xml:id="formula_50">)</formula><p>The calculation of p -adversarial rate follows 2 -adversarial rate calculation exactly, except that the term ε w 2 is replaced by w • εv 0|p = ε w q . Therefore, we have the following result.</p><p>Theorem 3. The overall p -adversarial rate of a linear classifier for the balanced Gaussian mixture data is</p><formula xml:id="formula_51">p adv|p = 1 − p m − 0.5 Φ w • µ + b w 2 σ − w q w 2 ε σ + Φ w • µ − b w 2 σ − w q w 2 ε σ .<label>(23)</label></formula><p>Now we have the p -adversarial error formula as the following.</p><formula xml:id="formula_52">p err|p = p adv|p + p m = 1 − 0.5 Φ w • µ + b w 2 σ − w q w 2 ε σ + Φ w • µ − b w 2 σ − w q w 2 ε σ ≥ 1 − Φ w • µ w 2 σ − w q w 2 ε σ = 1 − Φ µ 2 σ cos θ − w q w 2 ε σ<label>(24)</label></formula><p>Corresponding to the discussions in the main text, a useful classifier only requires a signal-noise ratio (SNR) of µ 2 /σ = O(1) due to equation ( <ref type="formula" target="#formula_49">22</ref>).</p><p>In contrast, due to equation ( <ref type="formula" target="#formula_52">24</ref>), a necessary condition for the existence of a p -adversarial-robust classifier is</p><formula xml:id="formula_53">µ 2 σ − w q w 2 ε σ = O(1).<label>(25)</label></formula><p>We now investigate what order of SNR µ 2 /σ is needed to make (25) hold.</p><p>First, we have to find the practical relevant order of ε needs to be studied. The following lemma about the average p -norm of Gaussian noise will provide the guideline. Lemma 4. Let the random vector x = (x 1 , ..., x d ) follows the d-dimensional Gaussian distribution N (0,</p><formula xml:id="formula_54">σ 2 I d ). Then E[ x p p ] = m p dσ p , p ∈ [1, ∞), E[ x p ] = O( √ log dσ), p = ∞,<label>(26)</label></formula><p>where m p denotes the p-th moment of the standard Gaussian distribution.</p><p>Proof.</p><formula xml:id="formula_55">For p ∈ [1, ∞), E[ x p p ] = E[ d i=1 |x i | p ] = d i=1 E[|x i | p ] = dE[|x 1 | p ] = dσ p m p .<label>(27)</label></formula><p>The ∞ result follows directly from the large deviation formula obtained by <ref type="bibr">O'Brien (1974)</ref>.</p><p>In the Gaussian mixture data, Lemma 4 states that the average p noise is d 1/p σm 1/p p . Therefore, for an η &lt; 1, a perturbation amount of ε = ηd 1/p σm 1/p p will be smaller than the average noise thus hard to distinguish from noise (unless it concentrates in the signal direction). Thus any practical relevant defense needs to be robust at the minimum against perturbations of order ε = O(d 1/p σ). For the ∞ , the defense needs to be robust at the minimum against perturbations of order ε = O( √ log dσ).</p><p>Plug-in these ε orders into the necessary condition (25), the existence of a p -adversarial-robust classifier requires at least µ 2 σ = O( w q w 2 d 1/p ) for p ∈ [1, ∞); and it requires at least</p><formula xml:id="formula_56">µ 2 σ = O( w q w 2 √ log d) for p = ∞.</formula><p>Next we use the norm comparison inequality to find these orders. For any w ∈ R d and any 0 &lt; r &lt; s &lt; ∞, we have</p><formula xml:id="formula_57">w s ≤ w r ≤ d 1/r−1/s w s .<label>(28)</label></formula><p>(A) For 1 ≤ p &lt; 2, we have 2 &lt; q ≤ ∞. Using r = 2 and s = q in (28), we get</p><formula xml:id="formula_58">d 1/q−1/2 ≤ w q w 2 ≤ 1.</formula><p>Plug this lower bound into the required order µ 2 σ = O( w q w 2 d 1/p ), the existence of a p -adversarial-robust classifier requires SNR of at least µ 2 σ = O(d 1/q−1/2 d 1/p ) = O(d 1/p+1/q−1/2 ) = O(d 1/2 ).</p><p>(B) For 2 &lt; p &lt; ∞, then q &lt; 2 &lt; ∞. Using r = q and s = 2 in (28), we get</p><formula xml:id="formula_59">1 ≤ w q w 2 ≤ d 1/q−1/2 .</formula><p>Thus the existence of a p -adversarial-robust classifier requires SNR of at least</p><formula xml:id="formula_60">µ 2 σ = O(1 • d 1/p ) = O(d 1/p ).</formula><p>(C) When p = ∞, then q = 1. Using r = q and s = 2 in (28), we get</p><formula xml:id="formula_61">1 ≤ w 1 w 2 ≤ d 1/q−1/2 .</formula><p>Thus the existence of a p -adversarial-robust classifier requires SNR of at least</p><formula xml:id="formula_62">µ 2 σ = O(1 • log d) = O( log d).</formula><p>We summarize the results for cases (A), (B) and (C) into the following theorem. Theorem 4. For linear classification of balanced Gaussian mixture data, the existence of a p -adversarial-robust classifier requires SNR of at least</p><formula xml:id="formula_63">µ 2 σ = O(d min(1/p,1/2) ) p ∈ [1, ∞), O( √ log d) p = ∞. (<label>29</label></formula><formula xml:id="formula_64">)</formula><p>Theorem 4 shows that the required SNR magnitude for p -adversarial-robustness differs for different p. where u p is the solution to the optimization problem:</p><formula xml:id="formula_65">max |w • v|, v ∈ D ε,δ|p = {v ∈ R d : v p ≤ ε, |v • µ 0 | ≤ δ} ⊂ D ε|p .<label>(31)</label></formula><p>Notice the optimization problem of max |w • v| is a linear programming problem, and the feasible region D ε,δ|p is a convex region. Therefore the solution u p does exist. Now replace the term ε w 2 by w • u p in the previous derivations of (ε, δ)-strong-adversarial rate using 2 norm, we get the following Theorem. Theorem 5. The overall (ε, δ) − p -strong-adversarial rate of a linear classifier for the balanced Gaussian mixture data is</p><formula xml:id="formula_66">p s−adv|p = 1 − p m − 0.5 Φ w • µ + b w 2 σ − w • u p w 2 σ + Φ w • µ − b w 2 σ − w • u p w 2 σ<label>(32)</label></formula><p>We now try to find an SNR order that allows p -strong-adversarial-robustness by applying formula (32) to the Bayes classifier whose w = µ 0 and b = 0. In this case, the solution to the optimization problem (31) becomes u p = δµ 0 . Thus using formula (32) we can get the (ε, δ) − p -strong-adversarial-error rate for the Bayes classifier as</p><formula xml:id="formula_67">p s−err|p = 1 − Φ µ 2 − δ σ . (<label>33</label></formula><formula xml:id="formula_68">)</formula><p>Since practical relevant δ can not exceed σ (in that case, no classifier can work as at least half of all data vectors will be perturbed into another class), SNR µ 2 σ of order O(1) can result in a useful p -strong-adversarial-robust classifier. This agrees with the conclusions in the main text about the existence of 2 -strong-adversarial-robust classifiers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>To illustrate the difference between the adversarial examples and the strong-adversarial examples, we consider the following examples visualized in Figure 1. Here, Figure 1(a) shows a data vector x of dimension d = 19 × 19 = 361 from the '+' class. To visualize, each component of the data vector is mapped onto [0, 1] via function 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) a data point x from the '+' class; (b) a randomly perturbed x ; (c) an adversarial x but not strongadversarial; (d) a strong-adversarial x . All three perturbations are of the same amount ε = 5.7 and µ = 4. The center grid cell within the red boundary contains the real class signal.</figDesc><graphic url="image-1.png" coords="3,72.00,129.18,468.01,118.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>7, Figure 1(b) shows a randomly perturbed x which is hardly distinguishable from the first image x to the human eye. This confirms that, in defending against realistic threats, ε of magnitude O( √ d) needs to be studied. (Detailed discussion of ε order is in subsection 2.3.) For a trained support vector machine (SVM) classifier, Figure 1(c) and (d) shows two adversarial examples with the same ε = 5.7, but only the last one in (d) is strong-adversarial for δ = 1.2. (Section 3 provides detailed setup of this experiment.) The adversarial attacks present a novel threat: a machine classifier misclassifies the perturbed data points that a human would not have noted the difference. We can see that our strong-adversarial example definition focus attention on this novel threat. In contrast, under the traditional definition, the adversarial examples include examples similar to Figure 1(c) that would indeed be classified by human into another class. We now quantitatively analyze the existence of adversarial and strong-adversarial examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3. 1 (</head><label>1</label><figDesc>Strong-)Adversarial Rates for the Linear SVM Settings We first conduct numerical experiments of a support vector machine (SVM) classifier on the Gaussian mixture data. We randomly generate 5000 data points from the balanced mixture distribution 0.5N (µ + , σ 2 I d ) + 0.5N (µ − , σ 2 I d ), and randomly splits them into 4000 train data and 1000 test data. We set µ + = −µ − = [µ, 0, • • • , 0], σ = 1 and d = 19 × 19. A linear SVM is trained on the training data using the python scikit-learn package and its default setting. Then for each test data vector, we check if it has any adversarial and strong-adversarial example, for ε = η a √ dσ = 19η a and δ = η s µ. Figure 1 earlier visualizes one such test data vector and its adversarial and strong-adversarial examples for η a = η s = 0.3. We conduct this experiment for various values of η = η a = η s and µ, and for each parameter combination, the simulation is repeated 1000 times. Figure 2 plots three empirical rates (misclassification, adversarial-error and strong-adversarial-error), each averaged over the 1000 simulations, against µ values, together with corresponding quantitative formulas from equations (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a)-(c) shows the results for three perturbation levels of η = 0.05, 0.1, 0.3, with the empirical quantities shown with symbols and the quantitative formulas shown in curves. The plots show very good agreement between the formulas with actual empirical proportions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Empirical probabilities and their theoretical values calculated from equations (6), (8) and (12), plotted versus µ. (a) η = 0.05, (b) η = 0.1, (c) η = 0.3</figDesc><graphic url="image-2.png" coords="6,94.15,271.85,421.20,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: When η = 0.3, the three error rates (a) of SVM versus SNR µ; (b) of Bayes classifier versus SNR µ; (c) of an unbiased linear classifier versus cos θ when µ = 2.</figDesc><graphic url="image-3.png" coords="7,94.15,72.00,421.20,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The strong-adversarial-error rates of standard SVM (SV M − s − err), the sparse SVM (sp − s − err) and the ideal Bayes classifier (id − s − err). Left: µ + = (µ, 0, ..., 0); Right: µ + = (µ, µ, ..., µ)/ √ d. η = 0.3.</figDesc><graphic url="image-4.png" coords="7,117.55,500.69,374.42,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: MNIST images of '1': (a) the original image, (b) an adversarial example with ε = 2.34, (c) a hand-made example with ε = 2.28</figDesc><graphic url="image-5.png" coords="8,117.55,107.38,374.41,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The padversarial-robustness is hardest to achieve for 1 ≤ p ≤ 2 since the required SNR O( √ d) is highest in these cases. The ∞ -adversarial-robustness has the smallest SNR requirement, thus easiest to achieve. This agrees with the observation by Schott et al. (2019): the ∞ robust classifier in Madry et al. (2018) is still highly susceptible to 2 attack.5.4 p -Strong-Adversarial Rate and Existence of p -Strong-Adversarial-Robust ClassifiersFollowing the same derivations before, we have the following lemma for the defining set Ω ε,δ|p = {x : x has an (ε, δ)− p -strong-adversarial example}. Lemma 5. The defining set for (ε, δ) − p -strong-adversarial examples is given by: Ω ε,δ|p = Ω(u p ) ∪ Ω(−u p ) (30)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Some recent works have attempted training DNN to be close to the Bayes classifier: Wang et al. (2018) uses a nearest neighbors method, and Schott et al. (2019) applies the generative model techniques. In particular, Schott al. (2019) applied their method on MNIST dataset, and when applying a specifically designed attack on such a trained DNN, the adversarial examples found are semantically meaningful for humans. That is, these adversarial examples are adversarial in traditional definition but likely not strong-adversarial. The new strong-adversarial examples framework can allow theoretical quantification of the robustness for these training methods. The analysis of strong-adversarial-robustness for general classifiers such as DNN can provide a new research direction on how to defend against realistic adversarial attacks. L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar, and A.Madry, "Adversarially robust generalization requires more data," in Advances in Neural Information Processing Systems, 2018, pp. 5014-5026. Y. Wang, S. Jha, and K. Chaudhuri, "Analyzing the robustness of nearest neighbors to adversarial examples," in International Conference on Machine Learning, 2018, pp. 5120-5129. L. Schott, J. Rauber, M. Bethge, and W. Brendel, "Towards the first adversarially robust neural network model on mnist," in Seventh International Conference on Learning Representations (ICLR 2019), 2019, pp. 1-16.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We don't distinguish the targeted and untargeted adversarial examples here because for binary classification they are the same.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Here we just use the optimal Bayes classfier for balanced case since we are focusing on the balanced case in the following text.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We did not consider p ∈ [0, 1) because in this case, dp is not a metric, although practically, 0 is considered.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page">6199</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6572" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepfool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<date type="published" when="2017-05">May 2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJzIBfZAb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<idno type="DOI">10.1145/2976749.2978392</idno>
		<ptr target="http://doi.acm.org/10.1145/2976749.2978392" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS &apos;16</title>
				<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1528" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial patch</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09665</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence Safety and Security</title>
				<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="99" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Symposium on Security and Privacy (SP</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Certifiable distributional robustness with principled adversarial training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hk6kPgZA-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01155</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="274" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are adversarial examples inevitable</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1lWUoA9FQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analysis of classifiers&apos; robustness to adversarial perturbations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-017-5663-3</idno>
		<ptr target="https://doi.org/10.1007/s10994-017-5663-3" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2018-03">Mar 2018</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="481" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Asymptotic behavior of support vector machine for spiked population model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1472" to="1492" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distance-weighted discrimination</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/27639976" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">480</biblScope>
			<biblScope unit="page" from="1267" to="1271" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
