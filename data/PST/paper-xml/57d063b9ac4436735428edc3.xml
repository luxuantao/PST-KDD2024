<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-Driven Precondition Inference with Learned Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Saswat</forename><surname>Padhi</surname></persName>
							<email>padhi@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Univ. of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Sharma</surname></persName>
							<email>sharmar@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Todd</forename><surname>Millstein</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Univ. of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data-Driven Precondition Inference with Learned Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E27F6A6BCBC9E9164A06FD7361D94CE8</idno>
					<idno type="DOI">10.1145/2908080.2908099</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.2.1 [Software Engineering]: Requirements/Specifications-Tools</term>
					<term>D.2.4 [Software Engineering]: Software/Program Verification-Validation</term>
					<term>F.3.1 [Theory of Computation]: Specifying and Verifying and Reasoning about Programs-Invariants, Mechanical verification, Specification techniques Precondition Inference, Loop Invariant Inference, Data-driven Invariant Inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We extend the data-driven approach to inferring preconditions for code from a set of test executions. Prior work requires a fixed set of features, atomic predicates that define the search space of possible preconditions, to be specified in advance. In contrast, we introduce a technique for ondemand feature learning, which automatically expands the search space of candidate preconditions in a targeted manner as necessary. We have instantiated our approach in a tool called PIE. In addition to making precondition inference more expressive, we show how to apply our featurelearning technique to the setting of data-driven loop invariant inference. We evaluate our approach by using PIE to infer rich preconditions for black-box OCaml library functions and using our loop-invariant inference algorithm as part of an automatic program verifier for C++ programs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this work we extend the data-driven paradigm for precondition inference: given a piece of code C along with a predicate Q, the goal is to produce a predicate P whose satisfaction on entry to C is sufficient to ensure that Q holds after C is executed. Data-driven approaches to precondition inference <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref> employ a machine learning algorithm to separate a set of "good" test inputs (which cause Q to be satisfied) from a set of "bad" ones (which cause Q to be falsified). Therefore, these techniques are quite general: they can infer candidate preconditions regardless of the complexity of C and Q, which must simply be executable.</p><p>A key limitation of data-driven precondition inference, however, is the need to provide the learning algorithm with a set of features, which are predicates over the inputs to C (e.g., x &gt; 0). The learner then searches for a boolean combination of these features that separates the set G of "good" inputs from the set B of "bad" inputs. Existing data-driven precondition inference approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref> require a fixed set of features to be specified in advance. If these features are not sufficient to separate G and B, the approaches must either fail to produce a precondition, produce a precondition that is known to be insufficient (satisfying some "bad" inputs), or produce a precondition that is known to be overly strong (falsifying some "good" inputs).</p><p>In contrast, we show how to iteratively learn useful features on demand as part of the precondition inference process, thereby eliminating the problem of feature selection. We have implemented our approach in a tool called PIE (Precondition Inference Engine). Suppose that at some point PIE has produced a set F of features that is not sufficient to separate G and B. We observe that in this case there must be at least one pair of tests that conflict: the tests have identical valuations to the features in F but one test is in G and the other is in B. Therefore we have a clear criterion for feature learning: the goal is to learn a new feature to add to F that resolves a given set of conflicts. PIE employs a form of search-based program synthesis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> for this purpose, since it can automatically synthesize rich expressions over arbitrary data types. Once all conflicts are resolved in this manner, the boolean learner is guaranteed to produce a precondition that is both sufficient and necessary for the given set of tests.</p><p>In addition to making data-driven precondition inference less onerous and more expressive, our approach to feature learning naturally applies to other forms of data-driven invariant inference that employ positive and negative examples. To demonstrate this, we have built a novel data-driven algorithm for inferring provably correct loop invariants. Our algorithm uses PIE as a subroutine to generate candidate invariants, thereby learning features on demand through conflict resolution. In contrast, all prior data-driven loop invariant inference techniques require a fixed set or template of features to be specified in advance <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>We have implemented PIE for OCaml as well as the loop invariant inference engine based on PIE for C++. We use these implementations to demonstrate and evaluate two distinct uses cases for PIE. 1  First, PIE can be used in the "black box" setting to aid programmer understanding of third-party code. For example, suppose a programmer wants to understand the conditions under which a given library function throws an exception. PIE can automatically produce a likely precondition for an exception to be thrown, which is guaranteed to be both sufficient and necessary over the set of test inputs that were considered. We evaluate this use case by inferring likely preconditions for the functions in several widely used OCaml libraries. The inferred preconditions match the English documentation in the vast majority of cases and in two cases identify behaviors that are absent from the documentation.</p><p>Second, PIE-based loop invariant inference can be used in the "white box" setting, in conjunction with the standard weakest precondition computation <ref type="bibr" target="#b10">[11]</ref>, to automatically verify that a program meets its specification. We have used our C++ implementation to verify benchmark programs used in the evaluation of three recent approaches to loop invariant inference <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46]</ref>. These programs require loop invariants involving both linear and non-linear arithmetic as well as operations on strings. The only prior techniques that have demonstrated such generality require a fixed set or template of features to be specified in advance.</p><p>The rest of the paper is structured as follows. Section 2 overviews PIE and our loop invariant inference engine informally by example, and Section 3 describes these algorithms precisely. Section 4 presents our experimental evaluation. Section 5 compares with related work, and Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview</head><p>This section describes PIE through a running example. The sub function in the String module of the OCaml standard library takes a string s and two integers i1 and i2 and returns a substring of the original one. A caller of sub must provide appropriate arguments, or else an Invalid _ argument exception is raised. PIE can be used to automatically infer a predicate that characterizes the set of valid arguments.</p><p>Our OCaml implementation of precondition inference using PIE takes three inputs: a function f of type 'a -&gt; 'b; a set T of test inputs of type 'a, which can be generated using any desired method; and a postcondition Q, which is sim- 1 Our code and full experimental results are available at https://github.com/SaswatPadhi/PIE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tests</head><p>Features Set T is partitioned into a set G of "good" inputs that cause Q to be satisfied and a set B of "bad" inputs that cause Q to be falsified. Finally, PIE is given the sets G and B, with the goal to produce a predicate that separates them. In our running example, the function f is String.sub and the postcondition Q is the following function:</p><formula xml:id="formula_0">i1&lt;0 i1&gt;0 i2&lt;0 i2&gt;0 ("pie", 0, 0) F F F F G ("pie", 0, 1) F F F T G ("pie", 1, 0) F T F F G ("pie", 1, 1) F T F T G ("pie", -1, 0) T F F F B ("pie", 1, -1) F T T F B ("pie", 1, 3) F T F T B ("pie", 2<label>, 2)</label></formula><formula xml:id="formula_1">fun arg res -&gt; match res with Exn (Invalid _ argument _ ) -&gt; false | _ -&gt; true</formula><p>As we show in Section 4, when given many random inputs generated by the qcheck library<ref type="foot" target="#foot_0">2</ref> , PIE-based precondition inference can automatically produce the following precondition for String.sub to terminate normally:</p><formula xml:id="formula_2">i1 &gt;= 0 &amp;&amp; i2 &gt;= 0 &amp;&amp; i1 + i2 &lt;= (length s)</formula><p>Though in this running example the precondition is conjunctive, PIE infers arbitrary conjunctive normal form (CNF) formulas. For example, if the postcondition above is negated, then PIE will produce this complementary condition for when an Invalid _ argument exception is raised:</p><formula xml:id="formula_3">i1 &lt; 0 || i2 &lt; 0 || i1 + i2 &gt; (length s)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data-Driven Precondition Inference</head><p>This subsection reviews the data-driven approach to precondition inference <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref> in the context of PIE. For purposes of our running example, assume that we are given only the eight test inputs for sub that are listed in the first column of Figure <ref type="figure" target="#fig_0">1</ref>. The induced set G of "good" inputs that cause String.sub to terminate normally and set B of "bad" inputs that cause sub to raise an exception are shown in the last column of the figure.</p><p>Like prior data-driven approaches, PIE separates G and B by reduction to the problem of learning a boolean formula from examples <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref>. This reduction requires a set of features, which are predicates on the program inputs that will be used as building blocks for the inferred precondition. As we will see later, PIE's key innovation is the ability to automatically learn features on demand, but PIE also accepts an optional initial set of features to use.</p><p>Suppose that PIE is given the four features shown along the top of Figure <ref type="figure" target="#fig_0">1</ref>. Then each test input induces a feature vector of boolean values that results from evaluating each feature on that input. For example, the first test induces the feature vector &lt;F,F,F,F&gt;. Each feature vector is now interpreted as an assignment to a set of four boolean variables, and the goal is to learn a propositional formula over these variables that satisfies all feature vectors from G and falsifies all feature vectors from B.</p><p>There are many algorithms for learning boolean formulas by example. PIE uses a simple but effective probably approximately correct (PAC) algorithm that can learn an arbitrary conjunctive normal form (CNF) formula and is biased toward small formulas <ref type="bibr" target="#b30">[31]</ref>. The resulting precondition is guaranteed to be both sufficient and necessary for the given test inputs, but there are no guarantees for other inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Learning via Program Synthesis</head><p>At this point in our running example, we have a problem: there is no boolean function on the current set of features that is consistent with the given examples! This situation occurs exactly when two test inputs conflict: they induce identical feature vectors, but one test is in G while the other is in B. For example, in Figure <ref type="figure" target="#fig_0">1</ref> the tests ("pie",1,1) and ("pie",1,3) conflict; therefore no boolean function over the given features can distinguish between them.</p><p>Prior data-driven approaches to precondition inference require a fixed set of features to be specified in advance. Therefore, whenever two tests conflict they must produce a precondition that violates at least one test. The approach of Sankaranarayanan et al. <ref type="bibr" target="#b41">[42]</ref> learns a decision tree using the ID3 algorithm <ref type="bibr" target="#b39">[40]</ref>, which minimizes the total number of misclassified tests. The approach of Gehr et al. <ref type="bibr" target="#b20">[21]</ref> strives to produce sufficient preconditions and so returns a precondition that falsifies all of the "bad" tests while minimizing the total number of misclassified "good" tests.</p><p>In our running example, both prior approaches will produce a predicate equivalent to the following one, which misclassifies one "good" test:</p><formula xml:id="formula_4">!(i1 &lt; 0) &amp;&amp; !(i2 &lt; 0) &amp;&amp; !((i1 &gt; 0) &amp;&amp; (i2 &gt; 0))</formula><p>This precondition captures the actual lower-bound requirements on i1 and i2. However, it includes an upper-bound requirement that is both overly restrictive, requiring at least one of i1 and i2 to be zero, and insufficient (for some unobserved inputs), since it is satisfied by erroneous inputs such as ("pie",0,5). Further, using more tests does not help. On a test suite with full coverage of the possible "good" and "bad" feature vectors, an approach that falsifies all "bad" tests must require both i1 and i2 to be zero, obtaining sufficiency but ruling out almost all "good" inputs. The ID3 algorithm will produce a decision tree that is larger than the original one, due to the need for more case splits over the features, and this tree will be either overly restrictive, insufficient, or both. In contrast to these approaches, we have developed a form of automatic feature learning, which augments the set of features in a targeted manner on demand. The key idea is to leverage the fact that we have a clear criterion for selecting new features -they must resolve conflicts. Therefore, PIE first generates new features to resolve any conflicts, and it then uses the approach described in Section 2.1 to produce a precondition that is consistent with all tests.</p><p>Let a conflict group be a set of tests that induce the same feature vector and that participate in a conflict (i.e., at least one test is in G and one is in B). PIE's feature learner uses a form of search-based program synthesis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> to generate a feature that resolves all conflicts in a given conflict group. Given a set of constants and operations for each type of data in the tests, the feature learner enumerates candidate boolean expressions in order of increasing size until it finds one that separates the "good" and "bad" tests in the given conflict group. The feature learner is invoked repeatedly until all conflicts are resolved.</p><p>In Figure <ref type="figure" target="#fig_0">1</ref>, three tests induce the same feature vector and participate in a conflict. Therefore, the feature learner is given these three input-output examples: (("pie",1,1), T), (("pie",1,3), F), and (("pie",2,2), F). Various predicates are consistent with these examples, including the "right" one i1 + i2 &lt;= (length s) and less useful ones like i1 + i2 != 4. However, overly specific predicates are less likely to resolve a conflict group that is sufficiently large; the small conflict group in our example is due to the use of only eight test inputs. Further, existing synthesis engines bias against such predicates by assigning constants a larger "size" than variables <ref type="bibr" target="#b0">[1]</ref>.</p><p>PIE with feature learning is strongly convergent: if there exists a predicate that separates G and B and is expressible in terms of the constants and operations given to the feature learner, then PIE will eventually (ignoring resource limitations) find such a predicate. PIE's search space is limited to predicates that are expressible in the "grammar" given to the feature learner. However, each type typically has a standard set of associated operations, which can be provided once and reused across many invocations of PIE. For each such invocation, feature learning automatically searches an unbounded space of expressions in order to produce targeted features. For example, the feature i1 + i2 &lt;= (length s) for String.sub in our running example is automatically constructed from the operations + and &lt;= on integers and length on strings, obviating the need for users to manually craft this feature in advance.</p><p>string sub(string s, int i1, int i2) { assume(i1 &gt;= 0 &amp;&amp; i2 &gt;= 0 &amp;&amp; i1+i2 &lt;= s.length()); int i = i1; string r = ""; while (i &lt; i1+i2) { assert(i &gt;= 0 &amp;&amp; i &lt; s.length()); r = r + s.at(i); i = i + 1; } return r; } Our approach to feature learning could itself be used to perform precondition inference in place of PIE, given all tests rather than only those that participate in a conflict. However, we demonstrate in Section 4 that our separation of feature learning and boolean learning is critical for scalability. The search space for feature learning is exponential in the maximum feature size, so attempting to synthesize entire preconditions can quickly hit resource limitations. PIE avoids this problem by decomposing precondition inference into two subproblems: generating rich features over arbitrary data types and generating a rich boolean structure over a fixed set of black-box features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Learning for Loop Invariant Inference</head><p>Our approach to feature learning also applies to other forms of data-driven invariant inference that employ positive and negative examples, and hence can have conflicts. To illustrate this, we have built a novel algorithm called LOOP-INVGEN for inferring loop invariants that are sufficient to prove that a program meets its specification. The algorithm employs PIE as a subroutine, thereby learning features on demand as described above. In contrast, all prior data-driven loop invariant inference techniques require a fixed set or template of features to be specified in advance <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>. To continue our running example, suppose that we have inferred a likely precondition for the sub function to execute without error and want to verify its correctness for the C++ implementation of sub shown in Figure <ref type="figure" target="#fig_1">2</ref>. 3 As is standard, we use the function assume(P ) to encode the precondition; executions that do not satisfy P are silently ignored. We would like to automatically prove that the assertion inside the while loop never fails (which implies that the subsequent access s.at(i) is within bounds). However, doing so requires an appropriate loop invariant to be inferred, which involves both integer and string operations. To our knowledge, the only previous technique that has been demon- 3 Note that + is overloaded as both addition and string concatenation in C++.</p><p>strated to infer such invariants employs a random search over a fixed set of features <ref type="bibr" target="#b45">[46]</ref>.</p><p>In contrast, our algorithm LOOPINVGEN can infer an appropriate loop invariant without being given any features as input. The algorithm is inspired by the HOLA loop invariant inference engine, a purely static analysis that employs logical abduction via quantifier elimination to generate candidate invariants <ref type="bibr" target="#b12">[13]</ref>. Our approach is similar but does not require the logic of invariants to support quantifier elimination and instead leverages PIE to generate candidates. HOLA's abduction engine generates multiple candidates, and HOLA performs a backtracking search over them. PIE instead generates a single precondition, but we show how to iteratively augment the set of tests given to PIE in order to refine its result. We have implemented LOOPINVGEN for C++ programs.</p><p>The LOOPINVGEN algorithm has three main components. First, we build a program verifier V for loop-free programs in the standard way: given a piece of code C along with a precondition P and postcondition Q, V generates the formula P ⇒ WP(C, Q), where WP denotes the weakest precondition <ref type="bibr" target="#b10">[11]</ref>. The verifier then checks validity of this formula by querying an SMT solver that supports the necessary logical theories, which either indicates validity or provides a counterexample.</p><p>Second, we use PIE and the verifier V to build an algorithm VPREGEN for generating provably sufficient preconditions for loop-free programs, via counterexample-driven refinement <ref type="bibr" target="#b4">[5]</ref>. Given code C, a postcondition Q, and test sets G and B, VPREGEN invokes PIE on G and B to generate a candidate precondition P . If the verifier V can prove the sufficiency of P for C and Q, then we are done. Otherwise, the counterexample from the verifier is incorporated as a new test in the set B, and the process iterates. PIE's feature learning automatically expands the search space of preconditions whenever a new test creates a conflict.</p><p>Finally, the LOOPINVGEN algorithm iteratively invokes VPREGEN to produce candidate loop invariants until it finds one that is sufficient to verify the given program. We illustrate LOOPINVGEN in our running example, where the inferred loop invariant I(i, i 1 , i 2 , r, s) must satisfy the following three properties:</p><p>1. The invariant should hold when the loop is first entered:</p><formula xml:id="formula_5">(i 1 ≥ 0 ∧ i 2 ≥ 0 ∧ i 1 + i 2 ≤ s.length() ∧ i = i 1 ∧ r = "" ⇒ I(i, i 1 , i 2 , r, s)</formula><p>2. The invariant should be inductive:</p><formula xml:id="formula_6">I(i, i 1 , i 2 , r, s)∧i &lt; i 1 +i 2 ⇒ I(i+1, i 1 , i 2 , r+s.at(i), s)</formula><p>3. The invariant should be strong enough to prove the assertion:</p><formula xml:id="formula_7">I(i, i 1 , i 2 , r, s) ∧ i &lt; i 1 + i 2 ⇒ 0 ≤ i &lt; s.length()</formula><p>Our example involves both linear arithmetic and string operations, so the program verifier V must use an SMT solver that supports both theories, such as Z3-Str2 <ref type="bibr" target="#b51">[52]</ref> or CVC4 <ref type="bibr" target="#b34">[35]</ref>.</p><p>To generate an invariant satisfying the above properties, LOOPINVGEN first asks VPREGEN to find a precondition to ensure that the assertion will not fail in the following program, which represents the third constraint above:</p><formula xml:id="formula_8">assume(i &lt; i1 + i2); assert(0 &lt;= i &amp;&amp; i &lt; s.length());</formula><p>Given a sufficiently large set of test inputs, VPREGEN generates the following precondition, which is simply a restatement of the assertion itself:</p><formula xml:id="formula_9">0 &lt;= i &amp;&amp; i &lt; s.length()</formula><p>While this candidate invariant is guaranteed to satisfy the third constraint, an SMT solver can show that it is not inductive. We therefore use VPREGEN again to iteratively strengthen the candidate invariant until it is inductive. For example, in the first iteration, we ask VPREGEN to infer a precondition to ensure that the assertion will not fail in the following program:</p><formula xml:id="formula_10">assume(0 &lt;= i &amp;&amp; i &lt; s.length()); assume(i &lt; i1 + i2); r = r + s.at(i); i = i+1; assert(0 &lt;= i &amp;&amp; i &lt; s.length());</formula><p>This program corresponds to the second constraint above, but with I replaced by our current candidate invariant. VPREGEN generates the precondition i1+i2 &lt;= s.length() for this program, which we conjoin to the current candidate invariant to obtain a new candidate invariant:</p><formula xml:id="formula_11">0 &lt;= i &amp;&amp; i &lt; s.length() &amp;&amp; i1+i2 &lt;= s.length()</formula><p>This candidate is inductive, so the iteration stops.</p><p>Finally, we ask the verifier if our candidate satisfies the first constraint above. In this case it does, so we have found a valid loop invariant and thereby proven that the code's assertion will never fail. If instead the verifier provides a counterexample, then we incorporate this as a new test input and restart the entire process of finding a loop invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithms</head><p>In this section we describe our data-driven precondition inference and loop invariant inference algorithms in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Precondition Inference</head><p>Figure <ref type="figure" target="#fig_2">3</ref> presents the algorithm for precondition generation using PIE, which we call PREGEN. We are given a code snippet C , which is assumed not to make any internal non-deterministic choices, and a postcondition Q, such as an assertion. We are also given a set of test inputs T for C , </p><formula xml:id="formula_12">FeatureVectors V + := CREATEFV(F ,G) 4: FeatureVectors V -:= CREATEFV(F ,B) 5: Conflict X := GETCONFLICT(V + , V -, G, B ) 6:</formula><p>if X = None then 7: which can be generated by any means, for example a fuzzer, a symbolic execution engine, or manually written unit tests. The goal is to infer a precondition P such that the execution of C results in a state satisfying Q if and only if it begins from a state satisfying P . In other words, we would like to infer the weakest predicate P that satisfies the Hoare triple {P }C{Q}. Our algorithm guarantees that P will be both sufficient and necessary on the given set of tests T but makes no guarantees for other inputs.</p><formula xml:id="formula_13">F := F ∪ FEATURELEARN(X) 8: end if 9: until X = None 10: φ := BOOLLEARN(V + , V -) 11: return SUBSTITUTE(F , φ)</formula><p>The function PARTITIONTESTS in Figure <ref type="figure" target="#fig_2">3</ref> executes the tests in T in order to partition them into a sequence G of "good" tests, which cause C to terminate in a state that satisfies Q, and a sequence B of "bad" tests, which cause C to terminate in a state that falsifies Q (line 1). The precondition is then obtained by invoking PIE, which is discussed next.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> describes the overall structure of PIE, which returns a predicate that is consistent with the given set of tests. The initial set F of features is empty, though our implementation optionally accepts an initial set of features from the user (not shown in the figure). For example, such features could be generated based on the types of the input data, the branch conditions in the code, or by leveraging some knowledge of the domain.</p><p>Regardless, PIE then iteratively performs the loop on lines 2-9. First it creates a feature vector for each test in G and B (lines 3 and 4). The i th element of the sequence V + is a sequence that stores the valuation of the features on the i th test in G. More formally,</p><formula xml:id="formula_14">V + = CREATEFV(F, G) ⇐⇒ ∀i, j.(V + i ) j = F j (G i )</formula><p>Here we use the notation S k to denote the k th element of the sequence S, and F j (G i ) denotes the boolean result of evaluating feature F j on test G i . V -is created in an analogous manner given the set B.</p><p>We say that a feature vector v is a conflict if it appears in both V + and V -, i.e. ∃i, j.</p><formula xml:id="formula_15">V + i = V - j = v.</formula><p>The function GETCONFLICT returns None if there are no conflicts. Otherwise it selects one conflicting feature vector v and returns a pair of sets X = (X + , X -), where X + is a subset of G whose associated feature vector is v and X -is a subset of B whose associated feature vector is v. Next PIE invokes the feature learner on X, which uses a form of program synthesis to produce a new feature f such that ∀t ∈ X + .f (t) and ∀t ∈ X -.¬f (t). This new feature is added to the set F of features, thus resolving the conflict.</p><p>The above process iterates, identifying and resolving conflicts until there are no more. PIE then invokes the function BOOLLEARN, which learns a propositional formula φ over |F | variables such that ∀v ∈ V + .φ(v) and ∀v ∈ V -.¬φ(v). Finally, the precondition is created by substituting each feature for its corresponding boolean variable in φ.</p><p>Discussion Before describing the algorithms for feature learning and boolean learning, we note some important aspects of the overall algorithm. First, like prior data-driven approaches, PREGEN and PIE are very general. The only requirement on the code C in Figure <ref type="figure" target="#fig_2">3</ref> is that it be executable, in order to partition T into the sets G and B. The code itself is not even an argument to the function PIE. Therefore, PREGEN can infer preconditions for any code, regardless of how complex it is. For example, the code can use idioms that are hard for automated constraint solvers to analyze, such as non-linear arithmetic, intricate heap structures with complex sharing patterns, reflection, and native code. Indeed, the source code itself need not even be available. The postcondition Q similarly must simply be executable and so can be arbitrarily complex.</p><p>Second, PIE can be viewed as a hybrid of two forms of precondition inference. Prior data-driven approaches to precondition inference <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref> perform boolean learning but lack feature learning, which limits their expressiveness and accuracy. On the other hand, a feature learner based on program synthesis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> can itself be used as a precondition inference engine without boolean learning, but the search space grows exponentially with the size of the required precondition. PIE uses feature learning only to resolve conflicts, leveraging the ability of program synthesis to generate expressive features over arbitrary data types, and then uses boolean learning to scalably infer a concise boolean structure over these features.</p><p>Due to this hybrid nature of PIE, a key parameter in the algorithm is the maximum number c of conflicting tests to allow in the conflict group X at line 5 in Figure <ref type="figure" target="#fig_3">4</ref>. If the conflict groups are too large, then too much burden is placed on the feature learner, which limits scalability. For example, FEATURELEARN(X + : Tests, X -: Tests) : Predicate Returns: A feature f such that f (t) for all t ∈ X + and ¬f (t) for all t ∈ X - 1: Operations O := GETOPERATIONS() 2: Integer i := 1 3: loop 4:</p><p>Features F := FEATURESOFSIZE(i, O)  i := i + 1 9: end loop a degenerate case is when the set of features is empty, in which case all tests induce the empty feature vector and are in conflict. Therefore, if the set of conflicting tests that induce the same feature vector has a size greater than c, we choose a random subset of size c to provide to the feature learner. We empirically evaluate different values for c in our experiments in Section 4.</p><formula xml:id="formula_16">5: if ∃f ∈ F.(∀t ∈ X + .f (t) ∧ ∀t ∈ X -.¬f (t))</formula><p>Feature Learning Figure <ref type="figure" target="#fig_6">5</ref> describes our approach to feature learning. The algorithm is a simplified version of the Escher program synthesis tool <ref type="bibr" target="#b0">[1]</ref>, which produces functional programs from examples. Like Escher, we require a set of operations for each type of input data, which are used as building blocks for synthesized features. By default, FEA-TURELEARN includes operations for primitive types as well as for lists. For example, integer operations include 0 (a nullary operation), +, and &lt;=, while list operations include [], ::, and length. Users can easily add their own operations, for these as well as other types of data.</p><p>Given this set of operations, FEATURELEARN simply enumerates all possible features in order of the size of their abstract syntax trees. Before generating features of size i+1, it checks whether any feature of size i completely separates the tests in X + and X -; if so, that feature is returned. The process can fail to find an appropriate feature, either because no such feature over the given operations exists or because resource limitations are reached; either way, this causes the PIE algorithm to fail.</p><p>Despite the simplicity of this algorithm, it works well in practice, as we show in Section 4. Enumerative synthesis is a good match for learning features, since it biases toward small features, which are likely to be more general than large features and so helps to prevent against overfitting. Further, the search space is significantly smaller than that of traditional program synthesis tasks, since features are simple expressions rather than arbitrary programs. For example, our algorithm does not attempt to infer control structures such as conditionals, loops, and recursion, which is a technical focus of much program-synthesis research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>. BOOLLEARN(V + : Feature Vectors, V -: Feature Vectors) : Boolean Formula Returns: A formula φ such that φ(v) for all v ∈ V + and ¬φ(v) for all v ∈ V - 1: Integer n := size of each feature vector in V + and V - 2: Integer k := 1 3: loop Boolean Function Learning We employ a standard algorithm for learning a small CNF formula that is consistent with a given set of boolean feature vectors <ref type="bibr" target="#b30">[31]</ref>; it is described in Figure <ref type="figure" target="#fig_7">6</ref>. Recall that a CNF formula is a conjunction of clauses, each of which is a disjunction of literals. A literal is either a propositional variable or its negation. Our algorithm returns a CNF formula over a set x 1 , . . . , x n of propositional variables, where n is the size of each feature vector (line 1). The algorithm first attempts to produce a 1-CNF formula (i.e., a conjunction), and it increments the maximum clause size k iteratively until a formula is found that is consistent with all feature vectors. Since BOOLLEARN is only invoked once all conflicts have been removed (see Figure <ref type="figure" target="#fig_3">4</ref>), this process is guaranteed to succeed eventually.</p><p>Given a particular value of k, the learning algorithm first generates a set C of all clauses of size k or smaller over x 1 , . . . , x n (line 4), implicitly representing the conjunction of these clauses. In line 5, all clauses that are inconsistent with at least one of the "good" feature vectors (i.e., the vectors in V + ) are removed from C. A clause c is inconsistent with a "good" feature vector v if v falsifies c:</p><formula xml:id="formula_17">∀1 ≤ i ≤ n.(x i ∈ c ⇒ v i = false)∧(¬x i ∈ c ⇒ v i = true)</formula><p>After line 5, C represents the strongest k-CNF formula that is consistent with all "good" feature vectors.</p><p>Finally, line 6 weakens C while still falsifying all of the "bad" feature vectors (i.e., the vectors in V -). In particular, the goal is to identify a minimal subset C of C where for each v ∈ V -, there exists c ∈ C such that v falsifies c. This problem is equivalent to the classic minimum set cover problem, which is NP-complete. Therefore, our GREEDYSET-COVER function on line 6 uses a standard heuristic for that problem, iteratively selecting the clause that is falsified by the most "bad" feature vectors that remain, until all such feature vectors are "covered." This process will fail to cover all VPREGEN(C: Code, Q: Predicate, G: Tests) : Predicate Returns: A precondition P such that P (t) for all t in G and {P }C{Q} holds "bad" feature vectors if there is no k-CNF formula consistent with V + and V -, in which case k is incremented; otherwise the resulting set C is returned as our CNF formula.</p><p>Because the boolean learner treats features as black boxes, this algorithm is unaffected by their sizes. Rather, the search space is O(n k ), where n is the number of features and k is the maximum clause size, and in practice k is a small constant. Though we have found this algorithm to work well in practice, there are many other algorithms for learning boolean functions from examples. As long as they can learn arbitrary boolean formulas, then we expect that they would also suffice for our purposes.</p><p>Properties As described above, the precondition returned by PIE is guaranteed to be both necessary and sufficient for the given set of test inputs. Furthermore, PIE is strongly convergent: if there exists a predicate that separates G and B and is expressible in terms of the constants and operations given to the feature learner, then PIE will eventually (ignoring resource limitations) find and return such a predicate.</p><p>To see why PIE is strongly convergent, note that FEA-TURELEARN (Figure <ref type="figure" target="#fig_6">5</ref>) performs an exhaustive enumeration of possible features. By assumption a predicate that separates G and B is expressible in the language of the feature learner, and that predicate also separates any sets X + and X -of conflicting tests, since they are respectively subsets of G and B. Therefore each call to FEATURELEARN on line 7 in Figure <ref type="figure" target="#fig_3">4</ref> will eventually succeed, reducing the number of conflicting tests and ensuring that the loop at line 2 eventually terminates. At that point, there are no more conflicts, so there is some CNF formula over the features in F that separates G and B, and the boolean learner will eventually find it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loop Invariant Inference</head><p>As described in Section 2.3, our loop invariant inference engine relies on an algorithm VPREGEN that generates provably sufficient preconditions for loop-free code. The VPREGEN algorithm is shown in Figure <ref type="figure">7</ref>. In the context of loop invariant inference (see below), VPREGEN will always be passed a set of "good" tests to use and will start LOOPINVGEN(C: Code, T : Tests) : Predicate Returns: A loop invariant that is sufficient to verify that C's assertion never fails.</p><p>Require: C = assume P ; while E {C 1 }; assert Q 1: G := LOOPHEADSTATES(C , T ) 2: loop 3:</p><formula xml:id="formula_18">I := VPREGEN( [assume ¬E], Q, G) 4:</formula><p>while not {I ∧ E}C 1 {I} do G := G ∪ LOOPHEADSTATES(C , {t})</p><formula xml:id="formula_19">13:</formula><p>end if 14: end loop with no "bad" tests, so we specialize the algorithm to that setting. The VPREGEN algorithm assumes the existence of a verifier for loop-free programs. If the verifier can prove the sufficiency of a candidate precondition P generated by PIE (lines 3-4), it returns None and we are done. Otherwise the verifier returns a counterexample t, which has the property that P (t) is true but executing C on t ends in a state that falsifies Q. Therefore we add t to the set B of "bad" tests and iterate.</p><p>The LOOPINVGEN algorithm for loop invariant inference is shown in Figure <ref type="figure" target="#fig_10">8</ref>. For simplicity, we restrict the presentation to code snippets of the form C = assume P ; while E {C 1 }; assert Q where C 1 is loop-free. Our implementation also handles code with multiple and nested loops, by iteratively inferring invariants for each loop encountered in a backward traversal of the program's control-flow graph.</p><p>The goal of LOOPINVGEN is to infer a loop invariant I which is sufficient to prove that the Hoare triple {P }(while E{C 1 }){Q} is valid. In other words, we must find an invariant I that satisfies the following three constraints: With this new set G of tests, LOOPINVGEN first generates a candidate invariant that meets the third constraint above by invoking VPREGEN on line 3. The inner loop (lines 4-7) then strengthens I until the second constraint is met. If the generated candidate also satisfies the first constraint (line 8), then we have found an invariant. Otherwise we obtain a counterexample t satisfying P ∧ ¬I, which we use to collect new program states as additional tests (line 12), and the process iterates. The verifier for loop-free code is used on lines 3 (inside VPREGEN), 4 (to check the Hoare triple), and 5 (inside VPREGEN), and the underlying SMT solver is used on line 8 (the validity check).</p><formula xml:id="formula_20">P ⇒ I {I ∧ E} C 1 {I} I ∧ ¬E ⇒ Q Given a</formula><p>We note the interplay of strengthening and weakening in the LOOPINVGEN algorithm. Each iteration of the inner loop strengthens the candidate invariant until it is inductive. However, each iteration of the outer loop uses a larger set G of passing tests. Because PIE is guaranteed to return a precondition that is consistent with all tests, the larger set G has the effect of weakening the candidate invariant. In other words, candidates get strengthened, but if they become stronger than P in the process then they will be weakened in the next iteration of the outer loop.</p><p>Properties Both the VPREGEN and LOOPINVGEN algorithms are sound: VPREGEN(C, Q, G) returns a precondition P such that {P }C{Q} holds, and LOOPIN-VGEN(C, T ) returns a loop invariant I that is sufficient to prove that {P }(while E {C 1 }){Q} holds, where C = assume P ; while E {C 1 }; assert Q. However, neither algorithm is guaranteed to return the weakest such predicate.</p><p>VPREGEN(C, Q, G) is strongly convergent: if there exists a precondition P that is expressible in the language of the feature learner such that {P }C{Q} holds and P (t) holds for each t ∈ G, then VPREGEN will eventually find such a precondition.</p><p>To see why, first note that by assumption each test in G satisfies P , and since {P }C{Q} holds, each test that will be put in B at line 5 in Figure <ref type="figure">7</ref> falsifies P (since each such test causes Q to be falsified). Therefore P is a separator for G and B, so each call to PIE at line 3 terminates due to the strong convergence result described earlier. Suppose P has size s. Then each call to PIE from VPREGEN will generate features of size at most s, since P itself is a valid separator for any set of conflicts. Further, each call to PIE produces a logically distinct precondition candidate, since each call includes a new test in B that is inconsistent with the previous candidate. Since the feature learner has a finite number of operations for each type of data, there are a finite number of features of size at most s and so also a finite number of logically distinct boolean functions in terms of such features.</p><p>Hence eventually P or another sufficient precondition will be found.</p><p>LOOPINVGEN is not strongly convergent: it can fail to terminate even when an expressible loop invariant exists. First, the iterative strengthening loop (lines 4-7 of Figure <ref type="figure" target="#fig_10">8</ref>) can generate a VPREGEN query that has no expressible solution, causing VPREGEN to diverge. Second, an adversarial sequence of counterexamples from the SMT solver (line 9 of Figure <ref type="figure" target="#fig_10">8</ref>) can cause LOOPINVGEN's outer loop to diverge. Nonetheless, our experimental results below indicate that the algorithm performs well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>We have evaluated PIE's ability to infer preconditions for black-box OCaml functions and LOOPINVGEN's ability to infer sufficient loop invariants for verifying C++ programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Precondition Inference</head><p>Experimental Setup We have implemented the PREGEN algorithm described in Figure <ref type="figure" target="#fig_2">3</ref> in OCaml. We use PREGEN to infer preconditions for all of the first-order functions in three OCaml modules: List and String from the standard library, and BatAvlTree from the widely used batteries library <ref type="foot" target="#foot_1">4</ref> . Our test generator and feature learner do not handle higher-order functions. For each function, we generate preconditions under which it raises an exception. Further, for functions that return a list, string, or tree, we generate preconditions under which the result value is empty when it returns normally. Similarly, for functions that return an integer (boolean) we generate preconditions under which the result value is 0 (false) when the function returns normally. A recent study finds that roughly 75% of manually written specifications are predicates like these, which relate to the presence or absence of data <ref type="bibr" target="#b42">[43]</ref>.</p><p>For feature learning we use a simplified version of the Escher program synthesis tool <ref type="bibr" target="#b0">[1]</ref> that follows the algorithm described in Figure <ref type="figure" target="#fig_6">5</ref>. Escher already supports operations on primitive types and lists; we augment it with operations for strings (e.g., get, has, sub) and AVL trees (e.g., left _ branch, right _ branch, height). For the set T of tests, we generate random inputs of the right type using the qcheck OCaml library. Analogous to the small scope hypothesis <ref type="bibr" target="#b27">[28]</ref>, which says that "small inputs" can expose a high proportion of program errors, we find that generating many random tests over a small domain exposes a wide range of program behaviors. For our tests we generate random integers in the range [-4, 4], lists of length at most 5, trees of height at most 5 and strings of length at most 12.</p><p>In total we attempt to infer preconditions for 101 functionpostcondition pairs. Each attempt starts with no initial features and is allowed to run for at most one hour and use up to 8GB of memory. Two key parameters to our algorithm are the number of tests to use and the maximum size of conflict </p><formula xml:id="formula_21">throws exception 3 (i &lt; 0) ∨ (len(s) ≤ i) sub(s,i1,i2) throws exception 3 (i1 &lt; 0) ∨ (i2 &lt; 0) ∨ (i1 &gt; len(s) -i2) index(s,c) result = 0 2 has(get(s, 0), c) index _ from(s,i,c) throws exception 4 (i &lt; 0) ∨ (i &gt; len(s)) ∨ ¬ has(sub(s, i, len(s) -i), c) List module functions nth(l, n) throws exception 2 (0 &gt; n) ∨ (n ≥ len(l)) append(l1, l2) empty(result) 2 empty(l1) ∧ empty(l2) BatAvlTree module functions create (t1, v, t2) throws exception 6 height(t1) &gt; (height(t2) + 1) ∨ height(t2) &gt; (height(t1) + 1) concat(t1, t2) empty(result) 2 empty(t1) ∧ empty(t2)</formula><p>groups to provide the feature learner. Empirically we have found 6400 tests and conflict groups of maximum size 16 to provide good results (see below for an evaluation of other values of these parameters).</p><p>Results Under the configuration described above, PRE-GEN generates correct preconditions in 87 out of 101 cases. By "correct" we mean that the precondition fully matches the English documentation, and possibly captures actual behaviors not reflected in that documentation. The latter happens for two BatAvlTree functions: the documentation does not mention that split _ leftmost and split _ rightmost will raise an exception when passed an empty tree.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows some of the more interesting preconditions that PREGEN inferred, along with the number of synthesized features for each. For example, it infers an accurate precondition for String.index _ from(s,i,c), which returns the index of the first occurrence of character c in string s after position i, through a rich boolean combination of arithmetic and string functions. As another example, PREGEN automatically discovers the definition of a balanced tree, since BatAvlTree.create throws an exception if the resulting tree would not be balanced. Prior approaches to precondition inference <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref> can only capture these preconditions if they are provided with exactly the right features (e.g.,  height(t 1 ) &gt; (height(t 2 ) + 1)) in advance, while PRE-GEN learns the necessary features on demand.</p><p>The 14 cases that either failed due to time or memory limits or that produce an incorrect or incomplete precondition were of three main types. The majority (10 out of 14) require universally quantified features, which are not supported by our feature learner. For example, List.flatten(l) returns an empty list when each of the inner lists of l is empty. In a few cases the inferred precondition is incomplete due to our use of small integers as test inputs. For example, we do not infer that String.make(i,c) throws an exception if i is greater than Sys.max _ string _ length. Finally, a few cases produce erroneous specifications for list functions that employ physical equality, such as List.memq. Our tests for lists only use primitives as elements, so they cannot distinguish physical from structural equality.</p><p>Configuration Parameter Sensitivity We also evaluated PIE's sensitivity to the number of tests and the maximum conflict group size. The top plot in Figure <ref type="figure" target="#fig_12">9</ref> shows the results with varied numbers of tests (and conflict group size of 16). In general, the more tests we use, the more correct our results. However, with 12,800 tests we incur one additional case that hits resource limits due to the extra overhead involved.</p><p>Table <ref type="table">2</ref>: Comparison of PIE with an approach that uses eager feature learning. The size of a feature is the number of nodes in its abstract syntax tree. Each Q i indicates the i th quartile, computed independently for each column. The bottom plot in Figure <ref type="figure" target="#fig_12">9</ref> shows the results with varied conflict group sizes (and 6400 tests). On the one hand, we can give the feature learner only a single pair of conflicting tests at a time. As the figure shows, this leads to more cases hitting resource limits and producing incorrect results versus a conflict group size of 16, due to the higher likelihood of synthesizing overly specific features. On the other hand, we can give the feature learner all conflicting tests at once. When starting with no initial features, all tests are in conflict, so this strategy requires the feature learner to synthesize the entire precondition. As the figure shows, this approach hits resource limitations more often versus a conflict group size of 16. For example, this approach fails to generate the preconditions for String.index _ from and BatAvlTree.create shown in Table <ref type="table" target="#tab_1">1</ref>. Further, in the cases that do succeed, the average running time and memory consumption are 11.7 second and 309 MB, as compared to only 1.8 seconds and 66 MB when the conflict group size is 16.</p><p>Comparison With Eager Feature Learning PIE generates features lazily as necessary to resolve conflicts. An alternative approach is to use Escher up front to eagerly generate every feature for a given program up to some maximum feature size s. These features can then simply all be passed to the boolean learner. To evaluate this approach, we instrumented PIE to count the number of candidate features that were generated by Escher each time it was called. <ref type="foot" target="#foot_2">5</ref> For each call to PIE, the maximum such number across all calls to Escher is a lower bound, and therefore a best-case scenario, for the number of features that would need to be passed to the boolean learner in an eager approach. It's a lower bound for two reasons. First, we are assuming that the user can correctly guess the maximum size s of features to generate in order to produce a precondition that separates the "good" and "bad" tests. Second, Escher stops generating features as soon as it finds one that resolves the given conflicts, so in general there will be many features of size s that are not counted.</p><p>Table <ref type="table">2</ref> shows the results for the 52 cases in our experiment above where PIE produces a correct answer and at least one feature is generated. Notably, the minimum number of features generated in the eager approach ( <ref type="formula">13</ref>) is more than double the maximum number of features selected in our approach <ref type="bibr" target="#b4">(5)</ref>. Nonetheless, for functions that require only simple preconditions, eager feature learning is reasonably practical. For example, 25% of the preconditions (Min to Q 1 in the table) require 29 or fewer features. However, the number of features generated by eager feature learning grows exponentially with their maximum size. For example, the top 25% of preconditions (from Q 3 to Max) require a minimum of 541 features to be generated and a maximum of more than 18, 000. Since boolean learning is in general super-linear in the number n of features (the algorithm we use is</p><formula xml:id="formula_22">O(n k )</formula><p>where k is the maximum clause size), we expect an eager approach to hit resource limits as the preconditions become more complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loop Invariants for C++ Code</head><p>We have implemented the loop invariant inference procedure described in Figure <ref type="figure" target="#fig_10">8</ref> for C++ code as a Clang tool 6 . As mentioned earlier, our implementation supports multiple and nested loops. We have also implemented a verifier for loop-free programs using the CVC4 <ref type="bibr" target="#b34">[35]</ref> and Z3-Str2 <ref type="bibr" target="#b51">[52]</ref> SMT solvers, which support several logical theories including both linear and non-linear arithmetic and strings. We employ both solvers because their support for both non-linear arithmetic and strings is incomplete, causing some queries to fail to terminate. We therefore run both solvers in parallel for two minutes and fail if neither returns a result in that time.</p><p>We use the same implementation and configuration for PIE as in the previous experiment. To generate the tests we employ an initial set of 256 random inputs of the right type. As described in Section 3.2, the algorithm then captures the values of all variables whenever control reaches the loop head, and we retain at most 6400 of these states.</p><p>We evaluate our loop invariant inference engine on multiple sets of benchmarks; the results are shown in Table <ref type="table" target="#tab_3">3</ref> and a sample of the inferred invariants is shown in Table <ref type="table" target="#tab_4">4</ref>. First, we have used LOOPINVGEN on all 46 of the benchmarks that were used to evaluate the HOLA loop invariant engine <ref type="bibr" target="#b12">[13]</ref>. These benchmarks require loop invariants that involve only the theory of linear arithmetic. Table <ref type="table" target="#tab_3">3</ref> shows each benchmark's name from the original benchmark set, the number of calls to the SMT solvers, the number of calls to the feature learner, the size of the generated invariant, and the running time of LOOPINVGEN in seconds. LOOPINVGEN succeeds in inferring invariants for 43 out of 46 HOLA benchmarks, including three benchmarks which 6 http://clang.llvm.org/docs/LibTooling.html HOLA's technique cannot handle (cases <ref type="bibr">15, 19, and 34)</ref>. By construction, these invariants are sufficient to ensure the correctness of the assertions in these benchmarks. The three cases on which LOOPINVGEN fails run out of memory during PIE's CNF learning phase.</p><p>Second, we have used LOOPINVGEN on 39 of the benchmarks that were used to evaluate the ICE loop invariant engine <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The remaining 19 of their benchmarks cannot be evaluated with LOOPINVGEN because they use language features that our program verifier does not support, notably arrays and recursion. As shown in Table <ref type="table" target="#tab_3">3</ref>, we succeed in inferring invariants for 35 out of the 36 ICE benchmarks that require linear arithmetic. LOOPINVGEN infers the invariants fully automatically and with no initial features, while ICE requires a fixed template of features to be specified in advance. The one failing case is due to a limitation of the current implementation -we treat boolean values as integers, which causes PIE to consider many irrelevant features for such values.</p><p>We also evaluated LOOPINVGEN on the three ICE benchmarks whose invariants require non-linear arithmetic. Doing so simply required us to allow the feature learner to generate non-linear features; such features were disabled for the above tests due to the SMT solvers' limited abilities to reason about non-linear arithmetic. LOOPINVGEN was able to generate sufficient loop invariants to verify two out of the three benchmarks. Our approach fails on the third benchmark because both SMT solvers fail to terminate on a particular query. However, this is a limitation of the solvers rather than of our approach; indeed, if we vary the conflict-group size, which leads to different SMT queries, then our tool can succeed on this benchmark.</p><p>Third, we have evaluated our approach on the four benchmarks whose invariants require both arithmetic and string operations that were used to evaluate another recent loop invariant inference engine <ref type="bibr" target="#b45">[46]</ref>. As shown in Table <ref type="table" target="#tab_3">3</ref>, our approach infers loop invariants for all of these benchmarks. The prior approach <ref type="bibr" target="#b45">[46]</ref> requires both a fixed set of features and a fixed boolean structure for the desired invariants, neither of which is required by our approach.</p><p>Finally, we ran all of the above experiments again, but with PIE replaced by our program-synthesis-based feature learner. This version succeeds for only 61 out of the 89 benchmarks. Further, for the successful cases, the average running time is 567 seconds and 1895 MB of memory, versus 28 seconds and 128 MB (with 573 MB peak memory usage) for our PIE-based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>We compare our work against three forms of specification inference in the literature. First, there are several prior approaches to inferring preconditions given a piece of code and a postcondition. Closest to PIE are the prior data-driven approaches. Sankaranarayanan et al. <ref type="bibr" target="#b41">[42]</ref> uses a decision-tree  (HOLA) 07</p><formula xml:id="formula_23">I : (b = 3i -a) ∧ (n &gt; i ∨ b = 3n -a) (HOLA) 22 I : (k = 3y) ∧ (x = y) ∧ (x = z) (ICE linear) dillig12 I1 : (a = b) ∧ (t = 2s ∨ flag = 0) I2 : (x ≤ 2) ∧ (y &lt; 5) (ICE linear) sum1 I : (i = sn + 1) ∧ (sn = 0 ∨ sn = n ∨ n ≥ i)<label>(Strings) c</label></formula><p>I : has(r, "a") ∧ (len(r) &gt; i)</p><p>(ICE non-linear) multiply</p><formula xml:id="formula_24">I : (s = y * j) ∧ (x &gt; j ∨ s = x * y)</formula><p>learner to infer preconditions from good and bad examples. Gehr et al. also uses a form of boolean learning from examples, in order to infer conditions under which two functions commute <ref type="bibr" target="#b20">[21]</ref>. As discussed in Section 2, the key innovation of PIE over these works is its support for on-demand feature learning, instead of requiring a fixed set of features to be specified in advance. In addition to eliminating the problem of feature selection, PIE's feature learning ensures that the produced precondition is both sufficient and necessary for the given set of tests, which is not guaranteed by the prior approaches.</p><p>There are also several static approaches to precondition inference. These techniques can provide provably sufficient (or provably necessary <ref type="bibr" target="#b9">[10]</ref>) preconditions. However, unlike data-driven approaches, they all require the source code to be available and statically analyzable. The standard weakest precondition computation infers preconditions for loopfree programs <ref type="bibr" target="#b10">[11]</ref>. For programs with loops, a backward symbolic analysis with search heuristics can yield preconditions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. Other approaches leverage properties of particular language paradigms <ref type="bibr" target="#b22">[23]</ref>, require logical theories that support quantifier elimination <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref>, and employ counterexample-guided abstraction refinement (CEGAR) with domain-specific refinement heuristics <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. Finally, some static approaches to precondition inference target specific program properties, such as predicates about the heap structure <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref> or about function equivalence <ref type="bibr" target="#b29">[30]</ref>.</p><p>Second, we have shown how PIE can be used to build a novel data-driven algorithm LOOPINVGEN for inferring loop invariants that are sufficient to prove that a program meets its specification. Several prior data-driven approaches exist for this problem <ref type="bibr">[18-20, 29, 32, 33, 46-49]</ref>. As above, the key distinguishing feature of LOOPINVGEN relative to this work is its support for feature learning. Other than one exception <ref type="bibr" target="#b46">[47]</ref>, which uses support vector machines (SVMs) <ref type="bibr" target="#b6">[7]</ref> to learn new numerical features, all prior works employ a fixed set or template of features. In addition, some prior approaches can only infer restricted forms of boolean formulas <ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref>, while LOOPINVGEN learns arbitrary CNF formulas. Finally, the ICE approach <ref type="bibr" target="#b18">[19]</ref> requires a set of "implication counterexamples" in addition to good and bad examples, which necessitates new algorithms for learning boolean formulas <ref type="bibr" target="#b19">[20]</ref>. In contrast, LOOPINVGEN can employ any off-the-shelf boolean learner. Unlike LOOP-INVGEN, ICE is strongly convergent <ref type="bibr" target="#b18">[19]</ref>: it restricts invariant inference to a finite set of candidate invariants that is iteratively enlarged using a dovetailing strategy that eventually covers the entire search space.</p><p>There are also many static approaches to invariant inference. The HOLA <ref type="bibr" target="#b12">[13]</ref> loop invariant generator is based on an algorithm for logical abduction <ref type="bibr" target="#b11">[12]</ref>; we employed a similar technique to turn PIE into a loop invariant generator. HOLA requires the underlying logic of invariants to support quantifier elimination, while LOOPINVGEN has no such restriction. Standard invariant generation tools that are based on abstract interpretation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, constraint solving <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>, or probabilistic inference <ref type="bibr" target="#b24">[25]</ref> require the number of disjunctions to be specified manually. Other approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref> can handle disjunctions but restrict their number via trace-based heuristics, custom built abstract domains, or widening. In contrast, LOOPINVGEN places no a priori bound on the number of disjunctions.</p><p>Third, there has been prior work on data-driven inference of specifications given only a piece of code as input. For example, Daikon <ref type="bibr" target="#b13">[14]</ref> generates likely invariants at various points within a given program. Other work leverages Daikon to generate candidate specifications and then uses an automatic program verifier to validate them, eliminating the ones that are not provable <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. As above, these approaches employ a fixed set or template of features. Unlike precondition inference and loop invariant inference, which require more information from the programmer (e.g., a postcondition), general invariant inference has no particular goal and so no notion of "good" and "bad" examples. Hence these approaches cannot obtain counterexamples to refine candidate invariants and cannot use our conflict-based approach to learn features.</p><p>Finally, the work of Cheung et al. <ref type="bibr" target="#b3">[4]</ref>, like PIE, combines machine learning and program synthesis, but for a very different purpose: to provide event recommendations to users of social media. They use the SKETCH system <ref type="bibr" target="#b49">[50]</ref> to generate a set of recommendation functions that each classify all test inputs, and then they employ SVMs to produce a linear combination of these functions. PIE instead uses program synthesis for feature learning, and only as necessary to resolve conflicts, and then it uses machine learning to infer boolean combinations of these features that classify all test inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have described PIE, which extends the data-driven paradigm for precondition inference to automatically learn features on demand. The key idea is to employ a form of program synthesis to produce new features whenever the current set of features cannot exactly separate the "good" and "bad" tests. Feature learning removes the need for users to manually select features in advance, and it ensures that PIE produces preconditions that are both sufficient and necessary for the given set of tests. We also described LOOP-INVGEN, which leverages PIE to provide automatic feature learning for data-driven loop invariant inference. Our experimental results indicate that PIE can infer high-quality preconditions for black-box code and LOOPINVGEN can infer sufficient loop invariants for program verification across a range of logical theories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Data-driven precondition inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A C++ implementation of sub.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precondition generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The PIE algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The feature learning algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The boolean function learning algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>6 :Figure 7 :</head><label>67</label><figDesc>Figure 7: Verified precondition generation for loop-free code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>5 :I</head><label>5</label><figDesc>:= VPREGEN( [assume I ∧ E; C 1 ], I, G)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Loop invariant inference using PIE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of PIE configurations. The top plot shows the effect of different numbers of tests. The bottom plot shows the effect of different conflict group sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>test suite T for C , LOOPINVGEN first generates a set of tests for the loop by logging the program state every time the loop head is reached (line 1). In other words, if x denotes the set of program variables then we execute the following instrumented version of C on each test in T :If the Hoare triple {P }(while E{C 1 }){Q} is valid, then all test executions are guaranteed to pass the assertion, so all logged program states will belong to the set G of passing tests. If a test fails the assertion then no valid loop invariant exists so we abort (not shown in the figure).</figDesc><table /><note><p>assume P ; log x; while E {C 1 ; log x}; assert Q</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A sample of inferred preconditions for OCaml library functions.</figDesc><table><row><cell>Case</cell><cell>Postcondition</cell><cell>Learned Features</cell></row><row><cell></cell><cell cols="2">String module functions</cell></row><row><cell>set(s,i,c)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental results for LOOPINVGEN. An invariant's size is the number of nodes in its abstract syntax tree. The analysis time is in seconds.</figDesc><table><row><cell></cell><cell>Calls to</cell><cell>Calls to</cell><cell>Sizes of</cell><cell>Analysis</cell><cell></cell><cell>Calls to</cell><cell>Calls to</cell><cell>Sizes of</cell><cell>Analysis</cell></row><row><cell>Case</cell><cell>Solvers</cell><cell>Escher</cell><cell>Invariants</cell><cell>Time</cell><cell>Case</cell><cell>Solvers</cell><cell>Escher</cell><cell>Invariants</cell><cell>Time</cell></row><row><cell></cell><cell></cell><cell cols="2">HOLA benchmarks [13]</cell><cell></cell><cell cols="5">Linear arithmetic benchmarks from ICE [20]</cell></row><row><cell>01</cell><cell>7</cell><cell>3</cell><cell>11</cell><cell>21</cell><cell>afnp</cell><cell>12</cell><cell>7</cell><cell>11</cell><cell>22</cell></row><row><cell>02</cell><cell>43</cell><cell>17</cell><cell>15</cell><cell>27</cell><cell>cegar1</cell><cell>16</cell><cell>11</cell><cell>12</cell><cell>30</cell></row><row><cell>03</cell><cell>31</cell><cell>22</cell><cell>3,7,15</cell><cell>46</cell><cell>cegar2</cell><cell>13</cell><cell>11</cell><cell>19</cell><cell>23</cell></row><row><cell>04</cell><cell>4</cell><cell>2</cell><cell>7</cell><cell>18</cell><cell>cggmp</cell><cell>34</cell><cell>22</cell><cell>143</cell><cell>32</cell></row><row><cell>05</cell><cell>7</cell><cell>3</cell><cell>11</cell><cell>23</cell><cell>countud</cell><cell>6</cell><cell>4</cell><cell>9</cell><cell>17</cell></row><row><cell>06</cell><cell>51</cell><cell>26</cell><cell>9,9</cell><cell>54</cell><cell>dec</cell><cell>4</cell><cell>2</cell><cell>3</cell><cell>17</cell></row><row><cell>07</cell><cell>111</cell><cell>45</cell><cell>19</cell><cell>116</cell><cell>dillig01</cell><cell>7</cell><cell>3</cell><cell>11</cell><cell>19</cell></row><row><cell>08</cell><cell>4</cell><cell>2</cell><cell>7</cell><cell>18</cell><cell>dillig03</cell><cell>14</cell><cell>7</cell><cell>15</cell><cell>29</cell></row><row><cell>09</cell><cell>27</cell><cell>18</cell><cell>3,15,7,22</cell><cell>40</cell><cell>dillig05</cell><cell>7</cell><cell>3</cell><cell>11</cell><cell>21</cell></row><row><cell>10</cell><cell>14</cell><cell>7</cell><cell>28</cell><cell>21</cell><cell>dillig07</cell><cell>8</cell><cell>4</cell><cell>11</cell><cell>21</cell></row><row><cell>11</cell><cell>21</cell><cell>18</cell><cell>15</cell><cell>22</cell><cell>dillig12</cell><cell>32</cell><cell>18</cell><cell>13,11</cell><cell>44</cell></row><row><cell>12</cell><cell>33</cell><cell>19</cell><cell>13,15</cell><cell>45</cell><cell>dillig15</cell><cell>31</cell><cell>35</cell><cell>55</cell><cell>42</cell></row><row><cell>13</cell><cell>46</cell><cell>30</cell><cell>33</cell><cell>54</cell><cell>dillig17</cell><cell>24</cell><cell>22</cell><cell>19,11</cell><cell>31</cell></row><row><cell>14</cell><cell>9</cell><cell>9</cell><cell>31</cell><cell>22</cell><cell>dillig19</cell><cell>19</cell><cell>13</cell><cell>31</cell><cell>32</cell></row><row><cell>15</cell><cell>26</cell><cell>27</cell><cell>33</cell><cell>39</cell><cell>dillig24</cell><cell>29</cell><cell>18</cell><cell>1,3,11</cell><cell>40</cell></row><row><cell>16</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>22</cell><cell>dillig25</cell><cell>57</cell><cell>31</cell><cell>11,19</cell><cell>74</cell></row><row><cell>17</cell><cell>22</cell><cell>17</cell><cell>15,7</cell><cell>31</cell><cell>dillig28</cell><cell>7</cell><cell>2</cell><cell>3,3</cell><cell>19</cell></row><row><cell>18</cell><cell>6</cell><cell>5</cell><cell>15</cell><cell>20</cell><cell>dtuc</cell><cell>9</cell><cell>2</cell><cell>3,3</cell><cell>22</cell></row><row><cell>19</cell><cell>18</cell><cell>14</cell><cell>27</cell><cell>32</cell><cell>fig1</cell><cell>4</cell><cell>2</cell><cell>7</cell><cell>17</cell></row><row><cell>20</cell><cell>61</cell><cell>24</cell><cell>33</cell><cell>115</cell><cell>fig3</cell><cell>7</cell><cell>5</cell><cell>7</cell><cell>17</cell></row><row><cell>21</cell><cell>23</cell><cell>10</cell><cell>19</cell><cell>23</cell><cell>fig9</cell><cell>7</cell><cell>2</cell><cell>7</cell><cell>19</cell></row><row><cell>22</cell><cell>16</cell><cell>11</cell><cell>13</cell><cell>22</cell><cell>formula22</cell><cell>12</cell><cell>11</cell><cell>16</cell><cell>25</cell></row><row><cell>23</cell><cell>10</cell><cell>7</cell><cell>11</cell><cell>21</cell><cell>formula25</cell><cell>11</cell><cell>5</cell><cell>15</cell><cell>21</cell></row><row><cell>24</cell><cell>29</cell><cell>19</cell><cell>1,7,11</cell><cell>40</cell><cell>formula27</cell><cell>23</cell><cell>5</cell><cell>19</cell><cell>25</cell></row><row><cell>25</cell><cell>83</cell><cell>47</cell><cell>11,19</cell><cell>142</cell><cell>inc2</cell><cell>4</cell><cell>2</cell><cell>7</cell><cell>18</cell></row><row><cell>26</cell><cell>90</cell><cell>32</cell><cell>9,9,9</cell><cell>71</cell><cell>inc</cell><cell>4</cell><cell>2</cell><cell>7</cell><cell>17</cell></row><row><cell>27</cell><cell>32</cell><cell>20</cell><cell>7,3,7</cell><cell>44</cell><cell>loops</cell><cell>19</cell><cell>12</cell><cell>7,7</cell><cell>28</cell></row><row><cell>28</cell><cell>7</cell><cell>2</cell><cell>3,3</cell><cell>20</cell><cell>sum1</cell><cell>16</cell><cell>14</cell><cell>21</cell><cell>22</cell></row><row><cell>29</cell><cell>66</cell><cell>19</cell><cell>11,11</cell><cell>47</cell><cell>sum3</cell><cell>6</cell><cell>1</cell><cell>3</cell><cell>20</cell></row><row><cell>30</cell><cell>18</cell><cell>12</cell><cell>35</cell><cell>29</cell><cell>sum4c</cell><cell>41</cell><cell>21</cell><cell>38</cell><cell>32</cell></row><row><cell>31</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>sum4</cell><cell>6</cell><cell>3</cell><cell>9</cell><cell>17</cell></row><row><cell>32</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>tacas6</cell><cell>9</cell><cell>8</cell><cell>11</cell><cell>22</cell></row><row><cell>33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>trex1</cell><cell>6</cell><cell>2</cell><cell>7,1</cell><cell>19</cell></row><row><cell>34</cell><cell>30</cell><cell>20</cell><cell>37</cell><cell>25</cell><cell>trex3</cell><cell>9</cell><cell>7</cell><cell>7</cell><cell>23</cell></row><row><cell>35</cell><cell>5</cell><cell>4</cell><cell>11</cell><cell>18</cell><cell>w1</cell><cell>4</cell><cell>2</cell><cell>7</cell><cell>17</cell></row><row><cell>36</cell><cell>128</cell><cell>36</cell><cell>11,15,19,11</cell><cell>113</cell><cell>w2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>37 38 39 40 41</cell><cell>13 44 10 30 17</cell><cell>11 38 5 24 11</cell><cell>19 29 11 19,17 15</cell><cell>22 36 20 40 27</cell><cell cols="5">Non-linear arithmetic benchmarks from ICE [20] multiply 25 19 15 41 sqrt ----square 11 7 5 24</cell></row><row><cell>42</cell><cell>25</cell><cell>15</cell><cell>50</cell><cell>37</cell><cell></cell><cell cols="3">String benchmarks [46]</cell><cell></cell></row><row><cell>43</cell><cell>4</cell><cell>2</cell><cell>7</cell><cell>19</cell><cell>a</cell><cell>66</cell><cell>22</cell><cell>110</cell><cell>45</cell></row><row><cell>44</cell><cell>14</cell><cell>14</cell><cell>20</cell><cell>26</cell><cell>b</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>10</cell></row><row><cell>45</cell><cell>60</cell><cell>33</cell><cell>11,9,9</cell><cell>64</cell><cell>c</cell><cell>7</cell><cell>4</cell><cell>8</cell><cell>11</cell></row><row><cell>46</cell><cell>12</cell><cell>5</cell><cell>21</cell><cell>24</cell><cell>d</cell><cell>7</cell><cell>3</cell><cell>9</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>A sample of inferred invariants for C++ benchmarks.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/c-cube/qcheck</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>http://batteries.forge.ocamlcore.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>All candidates generated by Escher are both type-correct and exceptionfree, i.e. they do not throw exceptions on any test inputs.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to Miryung Kim, Sorin Lerner, Madan Musuvathi, Guy Van den Broeck, and the anonymous reviewers for helpful feedback on this paper and research; Sumit Gulwani and Zachary Kincaid for access to the Escher program synthesis tool; Isil Dillig for access to the HOLA benchmarks; and Yang He for extensions to the PIE implementation. This research was supported by the National Science Foundation under award CCF-1527923 and by a Microsoft fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recursive program synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Albarghouthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kincaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification -25th International Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="934" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Compositional shape analysis by means of bi-abduction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Calcagno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Distefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>O'hearn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Snugglebug: A powerful approach to weakest preconditions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="363" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using program synthesis for social recommendations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st ACM International Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1732" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Counterexample-guided abstraction refinement</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Veith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification -12th International Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Linear invariant generation using non-linear constraint solving</title>
		<author>
			<persName><forename type="first">M</forename><surname>Colón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sipma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification -15th International Conference</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="420" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cousot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cousot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth ACM Symposium on Principles of Programming Languages</title>
		<imprint>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="238" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic discovery of linear restraints among variables of a program</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cousot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Halbwachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth Annual ACM Symposium on Principles of Programming Languages</title>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="84" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic inference of necessary preconditions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cousot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cousot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fähndrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Logozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Verification, Model Checking, and Abstract Interpretation -14th International Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="128" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Discipline of Programming</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Dijkstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explain: A tool for performing abductive inference</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dillig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dillig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification -25th International Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="684" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive invariant generation via abductive inference</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dillig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dillig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM SIGPLAN International Conference on Object-Oriented Programming Systems Languages &amp; Applications</title>
		<meeting>the 2013 ACM SIGPLAN International Conference on Object-Oriented Programming Systems Languages &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="443" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The daikon system for dynamic detection of likely invariants</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccamant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Tschantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Comput. Program</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Static contract checking with abstract interpretation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fähndrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Logozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Formal Verification of Object-Oriented Software -International Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="10" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Synthesizing data structure transformations from input-output examples</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Feser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dillig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="229" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving abstract interpretations by systematic lifting to the powerset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Filé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Logic Programming, Proceedings of the 1994 International Symposium</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="655" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamate: Dynamically inferring loop invariants for automatic full functional verification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Galeotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Furia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hardware and Software: Verification and Testing -10th International Haifa Verification Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ICE: A robust framework for learning invariants</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Löding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Madhusudan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification -26th International Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="69" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning invariants using decision trees and implication counterexamples</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Madhusudan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages</title>
		<meeting>the 43rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="499" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning commutativity specifications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification -27th International Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="307" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Donut domains: Efficient non-convex domains for abstract interpretation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ghorbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ivancic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Verification, Model Checking, and Abstract Interpretation -13th International Conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="235" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Abductive analysis of modular logic programs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Giacobazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="457" to="483" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Guided static analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gopan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Reps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Static Analysis, 14th International Symposium</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="349" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Program verification as probabilistic inference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages</title>
		<meeting>the 34th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="277" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Program analysis as constraint solving</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIG-PLAN 2008 Conference on Programming Language Design and Implementation</title>
		<meeting>the ACM SIG-PLAN 2008 Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="281" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From tests to proofs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rybalchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools and Algorithms for the Construction and Analysis of Systems, 15th International Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="262" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Jackson</surname></persName>
		</author>
		<title level="m">Software Abstractions: Logic, Language, and Analysis</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deriving invariants by algorithmic learning, decision procedures, and predicate abstraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Verification, Model Checking, and Abstract Interpretation, 11th International Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="180" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Conditional equivalence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rebelo</surname></persName>
		</author>
		<idno>MSR-TR-2010-119</idno>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An Introduction to Computational Learning Theory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatically inferring quantified loop invariants by algorithmic learning from simple templates</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programming Languages and Systems -8th Asian Symposium</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="328" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning invariants using decision trees</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wies</surname></persName>
		</author>
		<idno>CoRR, abs/1501.04725</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Backward analysis for inferring quantified preconditions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lev-Ami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sagiv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<idno>TR-2007-12-01</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Tel Aviv University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A DPLL(T) theory solver for a theory of strings and regular expressions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification -26th International Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="646" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Trace partitioning in abstract interpretation based static analyzers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mauborgne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Programming</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sufficient preconditions for modular assertion checking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Verification, Model Checking, and Abstract Interpretation, 9th International Conference</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="188" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic generation of program specifications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Nimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Software Testing and Analysis</title>
		<meeting>the International Symposium on Software Testing and Analysis</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="229" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Invariant inference for static checking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Nimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGSOFT Symposium on Foundations of Software Engineering</title>
		<meeting>the 10th ACM SIGSOFT Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Static analysis in disjunctive numerical domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ivancic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shlyakhter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Static Analysis Symposium</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dynamic inference of likely data preconditions over predicates by tree learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ivancic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/SIGSOFT International Symposium on Software Testing and Analysis</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="295" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Case studies and tools for contract specifications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Donohue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Coward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Software Engineering</title>
		<meeting>the 36th International Conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="596" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Counterexample-guided precondition inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Seghir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kroening</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd European Symposium on Programming</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="451" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Necessary and sufficient preconditions via eager abstraction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Seghir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schrammel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programming Languages and Systems -12th Asian Symposium</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="236" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From invariant checking to invariant inference using randomized search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification -26th International Conference, CAV 2014, Held as Part of the Vienna Summer of Logic</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07-18">2014. July 18-22, 2014. 2014</date>
			<biblScope unit="page" from="88" to="105" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Interpolants as classifiers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification -24th International Conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="71" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Verification as learning geometric concepts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Static Analysis -20th International Symposium</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="388" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Conditionally correct superoptimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schkufza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Churchill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM SIGPLAN International Conference on Object-Oriented Programming Systems Languages &amp; Applications</title>
		<meeting>the 2013 ACM SIGPLAN International Conference on Object-Oriented Programming Systems Languages &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="147" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Combinatorial sketching for finite programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tancau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saraswat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 12th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="404" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">From program verification to program synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Z3-str: a z3-based string solver for web application analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="114" to="124" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
