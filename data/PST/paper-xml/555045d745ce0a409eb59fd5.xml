<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Error-Driven Incremental Learning in Deep Convolutional Neural Network for Large-Scale Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
							<email>xiaotianjun@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
							<email>kuyang@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
							<email>pengyuxin@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<email>zheng.zhang@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>November 3-7</addrLine>
									<postCode>2014</postCode>
									<settlement>Orlando</settlement>
									<region>Florida</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Error-Driven Incremental Learning in Deep Convolutional Neural Network for Large-Scale Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2647868.2654926</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.5.1 [Pattern Recognition]: Models-Neural nets Algorithms, Experimentation, Performance Incremental Learning</term>
					<term>Deep Convolutional Neural Network</term>
					<term>Large-scale Image Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised learning using deep convolutional neural network has shown its promise in large-scale image classification task. As a building block, it is now well positioned to be part of a larger system that tackles real-life multimedia tasks. An unresolved issue is that such model is trained on a static snapshot of data. Instead, this paper positions the training as a continuous learning process as new classes of data arrive.</p><p>A system with such capability is useful in practical scenarios, as it gradually expands its capacity to predict increasing number of new classes. It is also our attempt to address the more fundamental issue: a good learning system must deal with new knowledge that it is exposed to, much as how human do.</p><p>We developed a training algorithm that grows a network not only incrementally but also hierarchically. Classes are grouped according to similarities, and self-organized into levels. The newly added capacities are divided into component models that predict coarse-grained superclasses and those return final prediction within a superclass. Importantly, all models are cloned from existing ones and can be trained in parallel. These models inherit features from existing ones and thus further speed up the learning. Our experiment points out advantages of this approach, and also yields a few important open questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>This paper focuses on incremental learning of deep convolutional neural network (DCNN) <ref type="bibr" target="#b13">[14]</ref> in image classification task. By incremental, we mean that batches of labeled data of new classes are made available gradually. Our objective is to train a deep neural network that performs well at each of such steps. Figure <ref type="figure" target="#fig_0">1</ref> illustrates incremental learning in a multiclass classification model.</p><p>Three aspects motivate this study. The first is from the perspective of end applications. The net trained by image classification task can perform as a critical building block in many multimedia tasks. A DCNN model can map images to text, in tasks such as image tagging and annotation, the tags can be further used in image retrieval tasks <ref type="bibr" target="#b17">[18]</ref>. Other tasks applies DCNN indirectly. For example, researchers have used a fine-tuned DCNN trained by image classification using ImageNet data to extract features in the detection task of TRECVID <ref type="bibr" target="#b19">[20]</ref>. In all such real-world scenarios, classes and their associated labeled data are always collected in an incremental manner. As such, incremental learning plays a critical role.</p><p>The second is a performance one. Applying deep learning to image classification has made rapid progress. In a short span of one year, DCNN has improved the top-5 error rate on the challenge of ImageNet 1K-category classification from 26.2% to 15.3% <ref type="bibr" target="#b13">[14]</ref>. Yet, the same network performs poorly on the more general 22K-category classification. One obvious culprit is the relatively limited network capacity. Thus, one option is to substantially increase the network capacity. The difficulties are two-folds. First, large models are inherently difficult to train, probably exponentially so. Second, it is not clear at all how and where new capacities should be allocated. It is more prudent to incrementally evolve the network capacity onwards, which is one principle behind the model proposed in this study.</p><p>The third motivation is more fundamental: we believe that this is a more general pattern of learning. As we are exposed to new and more data, we don't start learning from a blank slate. Rather, we leverage what's been learned and absorb new knowledge in a continuous process. Unlike other transfer-learning tasks where the features extracted in one domain is applied to a different but similar one (e.g. applying ImageNet feature sets to PASCAL VOL data <ref type="bibr" target="#b5">[6]</ref>), the problem here is to transfer the existing features and learn new one in the same task with an ever expanding scope. This process can be emulated with the setting we outlined above, and the goal is to understand what alternatives would work well. There have been relatively little study in putting learning in such a dynamic and evolving context. One reason is that large amount of labeled data is only available more recently. Second, doing incremental learning using deep neural network is a new problem and faces inherent technical challenges. Unlike SVM-based approaches, neural networks embed feature extraction and classification in one coherent architecture and within the same model. A consequence is that the modification of parts of the parameter space immediately affects the rest of the model globally. For instance, the so-called "catastrophic forgetting" problem <ref type="bibr" target="#b9">[10]</ref> refers to the destruction of existing features learned from earlier data, when the model is exclusively trained with data of new classes. A related difficulty is the allocation of new parameters, as mentioned earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incremental Learning</head><p>We draw intuition from our everyday experience of learning. For natural objects, there is an inherent ontology hierarchy due to the process of evolution. Artifacts, too, borrow elements from existing ones as one key factor of innovation. As such, it is highly unlikely that there is one flat space where we assign probability to predictions (i.e. with one softmax output layer for the entire 22K ImageNet classes). Rather, we first make a reasonable guess, and zoom into a sub-space where classes are similar. This process is inherently iterative and may involve multiple levels. If that's the case, the model should similarly have a hierarchical structure and the inference should allow iterative refinement.</p><p>In this paper, we propose a model that grows organically and hierarchically, as new classes become available. We make the following contributions. First, the model is hierarchical, and the total classes are split gradually towards the leaf of the tree, where similar classes are grouped. Importantly, this process is guided by a pragmatic, error-driven preview process. Second, we carefully control the growth of the network capacity, allocating them according to the responsibility of the components (i.e. coarse-grained routing versus fine-grained prediction). Third, we initialize the new parameters using cloning, thereby maximally retain the learned features. Finally, we perform a detailed study and reveal how learning propagates, with and without cloning.</p><p>Our preliminary results show that this approach is promising. Cloning, rather than starting from scratch, can learn faster or better, and often both. Comparing with the existing approaches that always train from scratch, we can reach similar performance by starting from an existing model, but with up to 25% few samples. The results of hierarchical model are mixed, in the sense that the performance gain is limited, due to the errors of superclass prediction. We have gained a few important lessons and insights to guide our future work.</p><p>The rest of the paper is organized as follows. We cover related work in Section 2 and then describe our overall architecture in Section 3. We then explain the training algorithm in Section 4, which also depicts how the network grows. Detailed performance study is covered in Section 5.</p><p>We discuss what we learned, future work and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Our work attempts to bring the promise of deep learning to the practical and more general paradigm of incremental learning. We organize the discussion of related work according to researches in these areas.</p><p>Multi-class image classification and deep learning . The last decade has witnessed a great progress in multi-class image categorization, both in terms of the number of categories and accuracy. For instance, ImageNet has about 22K categories <ref type="bibr" target="#b3">[4]</ref>, compared to hundreds in the Caltech series <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>, and the collection is growing. When the dataset grows bigger, hierarchy pattern in the class set becomes more obvious. Jia et al. <ref type="bibr" target="#b2">[3]</ref> researched on the classification result in Im-ageNet 10K dataset and report the phenomenon where misclassification information has some correlation with the semantic hierarchy of ImageNet. Griffin et al. <ref type="bibr" target="#b11">[12]</ref> utilized the taxonomy hierarchy in the class set to do hierarchical classification to pursue greater speedup. Besides those two works, others mainly focused on the classification accuracy on different dataset. The accuracy has been improving steadily with new developed image features over the years <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19]</ref>, and achieved a great leap with the renewed Deep Convolutional Neural Network <ref type="bibr" target="#b13">[14]</ref>. The superiority of DCNN comes from its ability in simultaneously learning the feature extractor and classifier via the network with many layers. Besides image classification task, the DCNN model has been applied to some multimedia tasks directly or indirectly <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Incremental learning. A crude definition of incremental learning is that learning is a continuous process with new data. Prior work has addressed two major scenarios, out of which the second one is relevant to this study. The first is concept drift in the training dataflow, and therefore the classifier learns in a non-stationary environment <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>. The second is when there are existing classifiers that are related to the new classes to be learned <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Thrun <ref type="bibr" target="#b20">[21]</ref> proposed the question as whether leaning the n-th classifier is easier (than learning the first), and since then researchers began to tackle this problem using transfer learning intuition, with techniques using fewer samples to get new models or better generalization. Two of the hot research topics derived from this problem is one-shot learning and zero-shot learning. Fei-Fei et al. <ref type="bibr" target="#b7">[8]</ref> proposed a Bayesian transfer learning method to avoid learning new categories from scratch and instead using very few training samples. Tommasi et al. <ref type="bibr" target="#b21">[22]</ref> proposed a multi model knowledge transfer method where source classifiers were weighted by learned coefficient. Lampert et al. <ref type="bibr" target="#b15">[16]</ref> achieved zeroshot learning by introducing attribute-based classification. Kuzborskij et al. <ref type="bibr" target="#b14">[15]</ref> pointed out that prior work mostly focused on binary classification problem (object detection), and proposed a discriminative method in the One-Versus-All multi-class classification task by transferring knowledge to a new class while preserving what has already been learned. Those works rely on shallow models instead of DCNN, and the category size is small in comparison.</p><p>The particular challenge with DCNN in the context of incremental learning (and in same sense transfer learning as well) is that it mingles feature extractor and classifier in one architecture. Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref> investigated the catastrophic forgetting problem in gradient-based neural networks. The study is however on the small MNIST dataset with small network, and the proposal of using dropout is already in the default DCNN configuration. Nevertheless it qualitatively re-affirms the difficulty of achieving good performance on old and new tasks simultaneously. Many works have focused on performing domain adaptation, one such example is the one-shot learning by adapting features learned form ImageNet 1K dataset to other datasets <ref type="bibr" target="#b12">[13]</ref>. The recent work of zero-shot learning assumes the availability of a semantic embedding space to which outputs of DCNN are projected. Our work differs in the goal, as we want to transfer the learning withinthe same task with larger dataset.</p><p>While many incremental learning works paid more attention on efficiency than accuracy, Bengio <ref type="bibr" target="#b1">[2]</ref> offered a deeper insight that speed and quality can be obtained simultaneously if increments are made properly, noted as curriculum learning. Its key idea is to start learning on easier aspects of tasks and then gradually increase the difficulty. This setting may have positive effect on both the convergence speed and the quality of local minimum obtained. This work can help analyzing our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INCREMENTAL LEARNING MODEL</head><p>In this section, we will introduce our proposal of incremental learning model and its inference process. Without loss of generality, we assume there is a model M0 that is already trained on N0 classes. The goal of incremental learning is to evolve from Mi−1 to Mi, to train Ni classes, in which Ni − Ni−1 are new classes. For example, we might have a DCNN classifier for 500 animal classes out of the 22K categories in ImageNet, we want to grow the network to classify 1000 and then the complete collection of animal classes.</p><p>Obviously, the model must increase its capacity to accommodate more classes. The simplest way to grow is widening the top softmax layer to output the extra probabilities on new classes. In other words, Mi share the same structure as Mi−1 except it has Ni softmax units at the output. One obvious drawback is that the capacity increment is small: if the width of the layer before softmax is H then the additional parameters amounts to H × (Ni − Ni−1). To put it more concretely, if we use the default configuration of DCNN <ref type="bibr" target="#b13">[14]</ref> for the 500-class model, when we increase it to 1000 classes, the number of parameter increment is merely 1%.</p><p>We can make the model bigger by injecting units in the fully-connected layers or having more feature maps in the convolutional layers. However, it is not clear how this should be done. We could end up putting many untrained new parameters into the model everywhere, and their initialization becomes a black magic: too big or too small a random value will either ruin the existing model or making training tediously long.</p><p>A better approach must maximize the transfer of learned features, and still conservatively grow the capacity. The architecture we explore in this paper is a hierarchy of models, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. In our default setting, all the models (in each blue box) share the same topology and the amount of parameters, except the top output layers. All of them receive the same input sample, and have a softmax output. They logically make up a tree during the inference process. Models at different positions play different roles. Each leaf model performs the final prediction among a nonoverlapping subset of the total classes. The branch models, on the other hand, have each of their output unit points to a child model, directing the prediction (eventually) to the correct leaf model. More specifically, the total classes are partitioned into superclasses, and each superclass is assigned to a leaf model (the red units in Figure <ref type="figure" target="#fig_1">2</ref>). An input sample triggers the root model which outputs the probabilities of which branch the sample belongs to, and then the child model (can be a leaf or branch model) with the highest probability is chosen. This recursion continues until a leaf model is selected, and we take the output in this superclass as the final prediction.</p><p>The intuition in this tree-style prediction is that each branch model, being constrained by capacity, is only optimized for predicting a coarse collection of classes. A leaf model, on the other hand, concentrates on a more accurate fine-grained classification on a smaller set of classes. This architecture mirrors the hierarchical and iterative inference process that human brain seems to be adept at, as we do a crude prediction first, and then refine it further by discriminating against close competitors. It also allows the total model capacities to be divided according to responsibilities (coarse vs fine-grained classification). In such a hierarchy of models, the capacity growth is done by cloning new models from the exiting ones and expanding the tree. The cloning reduces the difficulty of initializing the added capacity. We will discuss the details of this incremental learning in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ALGORITHM</head><p>For ease of disposition, we will describe the algorithm when there is only one single model, and then generalize the procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Starting from a Single Superclass</head><p>In the starting point of the training, all N0 classes are in one single superclass and predicted by one model L0. Thus, L0 is a leaf model by itself. When new classes come and the superclass size increases to N1, we have two choices to make the model bigger. One choice is simply extending L0 to L 0 by inserting more output units, which conservatively increases a small amount of capacity. The second choice that substantially scales up the capacity is partitioning the superclass into K superclasses, and clone L0 into several new leaf models L1, L2, . . . , LK to predict within each of these new superclasses. A branch model B with K final output units is also cloned from L0 to direct the prediction to the correct leaf model on a given input sample. These two choices are illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. We call the former flat increment, and the latter clone increment. There are three problems we need to address: 1) how to partition a superclass; 2) how to re-train these models with changed data and objectives; 3) most importantly, how to decide the best strategy out of the two alternatives.</p><p>In clone increment, the network grows by one level, and the total N1 classes are first clustered to superclasses by similarity. This allows the branch model B and the leaf models to focus on inter-and intra-superclass discriminative features, respectively. This is done using error-driven preview, which essentially error-driven preview observes the error distribution using current model L0 on samples drawn from all the data, including both new and old. A validation set of N0 are tested through L0, and calculating a confusion matrix C ∈ R N 0 ×N 0 from the output. The entry Cij denotes the probability that the i-th class is predicted to j-th class, which also measures the similarity between class i and j. We then use spectral clustering partition to split N0 classes into K clusters based on the confusion matrix. The classes that are easy to confuse with each other are grouped into the same cluster as a superclass, with the desired side effects to minimize the confusion between superclasses. Next, N1 − N0 new classes are assigned to superclasses based on their confusion rates among the superclasses.</p><p>After the superclass partitioning, there are a total of K leaf models to train. Each of them has the same topology as L0 except the output layer. Compared with L0 which is only trained to predict N0 old classes, each new leaf model Li inherits a portion of old classes plus with some new classes. This change of data and objective requires each leaf model to be retrained. Instead of training from scratch, we train these new leaf models incrementally by copying L0's parameters as initialization and using random initialization on the remaining new parameters (i.e. the weights connecting the units of the last hidden layer to the newly added output layer units). This training is much more efficient than from scratch.</p><p>We also evolve a new leaf model L 0 to have N1 output units, trained with flat incremental with data of N1 classes. Finally, we clone the branch model B by copying parameters from L 0 . As illustrated in Figure <ref type="figure" target="#fig_3">4</ref>, and simply sum up the softmax units belonging to each superclass as the predicted probability of that superclass.</p><p>At this point, we have built two separate models, the first is L 0 , and the second is the branch model B and its leaf models L1 to LK . Strictly speaking, choosing one over the other depends on the estimation of extra model capacity demanded by the new classes. We adopt a simpler strategy to simply let the two compete. This is illustrated in Algorithm 1. Note that all these models can be trained in parallel. More importantly, the process of cloning and incremental leaf model training automatically transfers features learned from the old model L0 to the new leaf and branch models.</p><p>Comparing with training such new models from scratch, the hope is that we can reach better accuracy with the same amount of time budget, or retain the accuracy with shorter training time, or both. Therefore, this is a building block whose performance is critical, and is the subject of much deeper analysis in later part of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Incremental Learning</head><p>Incremental learning from a single superclass can be generalized to a deeper hierarchy, at each step receiving new batches of classes (Algorithm 2). The first step is to distribute all the new classes into the existing superclasses by error-driven preview in a coarse granularity of superclass (line 1 to 6). Next, for each leaf model l, we grow it using flat increment or clone increment by Algorithm 1, discussed previously. Note that if the clone increment is selected, l is itself expanded to a subtree.</p><p>This algorithm can be applied repeatedly with new batches of classes, and naturally support more levels of hierarchy, independent of how many leaf models and how many levels of hierarchy exists in the current model. As the hierarchy deepens, the original trained branch models, especially those near the root, might lose accuracy due to update in their subtrees. We can choose to retrain each branch model by simple flat increment training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we will evaluate our framework on two datasets, using convergence speed and classification accuracy as criterion to show how to learn faster and better incrementally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset.</head><p>We prepare two datasets for our study. The first smaller dataset include all the 398 animal classes in ImageNet 1K, in which 501K and 18K images are used as the training set and validation set, respectively. We call it ImageNet 1K Animal. It is used primarily to understand the performance of initialization with existing model parameters. We perform a fuller experiment growing a model incrementally with the second larger dataset selected from the 3998 animal synsets in Im-ageNet 22K. Samples of these synsets have a few problems:</p><p>• Not all of them have image samples. For those that do, there are severe unbalance in the number of samples; some of them contain fewer than 10 images.</p><p>• The image samples of an inner node in the ImageNet hierarchy can also belong to a leaf node. For such an image, there are multiple labels.</p><p>We therefore take all the leaf nodes of animals in the Im-ageNet 22K hierarchy. We also remove classes with fewer than 100 images, results in 2282 classes. The training, validation and testing set contains 85%, 5%, 10% of the data respectively, with 1.6M, 91K and 183K images each. We call it ImageNet 22K Animal.</p><p>To create an incremental training process, the small dataset is incremented from 195 randomly drawn classes to 398 classes, whereas the big dataset is incremented from 1000 to the full 2282 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model.</head><p>The baseline model is the one 8-layer convolutional network as described in <ref type="bibr" target="#b13">[14]</ref>, with configuration shown in Table 1. As discussed in Section 4.1, we have two ways to extend the capacity, flat increment by increasing the softmax output layer size, and clone increment by duplicating more models. In either case, the configurations stay the same except the top output layer, and network parameters are initialized by copying from an existing model, which we call incremental learning. We also compare the results against a baseline where the training starts from scratch, which is called from-scratch learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details.</head><p>One of the "black magics" in training a deep network is the scheduling of learning rate adjustment. As we are interested in possible training time savings, we apply an adaptive learning rate tuning strategy to avoid tuning by hand. After training every 30K samples, we test with 1K samples randomly drawn from the whole validation set. We compute and monitor the average validation error for each epoch, and if it doesn't drop by more than 0.1% for three consecutive epochs, we stop the training or cut the learning rate by 10-folds. For one complete training, the learning rate is adjusted twice, starting from 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Result Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on ImageNet_1K_Animal.</head><p>With the model well trained on 195 animals, we can train it for 398 animals incrementally or from scratch. Table <ref type="table" target="#tab_0">2</ref> summarizes the comparison between these two choices, by the error rates trained for different epochs. We hand-tuned the learning rate of the incremental runs for we want them The scenario we consider here is one where a model of smaller net has already been trained. From-scratch training simply throws that model away, whereas ours leverages it. The simple flat-incremental method actually creates a curriculum learning <ref type="bibr" target="#b1">[2]</ref> scenario. Training a net classifying 195 animals is an easier task compared with a net for 398 animals. We observed positive effects both on convergence speed and the quality of local minimum obtained. More analysis is needed because curriculum learning study is currently empirical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on ImageNet_22K_Animal.</head><p>When class number grows from 1000 to 2282, we split the superclass and do a clone increment from the 1000-animalmodel. The branch model and the two leaf models are all incrementally trained by copying exiting parameters as initialization. Figure <ref type="figure" target="#fig_6">5</ref> shows the confusion matrices of the existing 1000 and the new 1282 classes before clone increment. All these classes are partitioned into two superclasses with 1350 and 932 classes, respectively (shown in red and blue boxes in Figure <ref type="figure" target="#fig_6">5</ref>). One can observe the similar error distribution on the old and new classes.  baseline) but with substantially more data training the leaf models. The total amount of time needed won't increase if trained in parallel.</p><p>To understand the benefit of clone increment, we do a breakdown analysis in Table <ref type="table" target="#tab_4">4</ref> and Table <ref type="table" target="#tab_5">5</ref>. Flat-incremental model achieved 49.15% overall error rate, in which the superclass 1 and 2 have 45.79% and 56.02% error rates, respectively. If the branch model is perfect, the two leaf models obtain 44.43% and 54.79% error rates, more than 1.2% accuracy each. Accounting the prediction error of the branch model, the 2-level model achieves 45.03% and 55.63%. The end performance on a given superclass can not be derived from the average accuracy of a leaf model (i.e. 55.57% superclass 1) and the branch accuracy (i.e. 97.80%). For superclass 1, this will yield the error rate to be 1 − 0.5557 • 0.9780, which is 45.65%. This is higher than we actually get (45.03%). Why is that?</p><p>The reason is that the images misclassified by the branch are also more difficult to be classified correctly by the leaf  <ref type="table" target="#tab_5">5</ref> shows the error rate if we divide the data according to the branch decision. It is clear that there exists such a strong correlation. This is both subtle and intuitive, and explains the apparent discrepancy described earlier. The correct way to compute the performance on a given superclass is:</p><formula xml:id="formula_0">E whole = 1 − (1 − E L|B Correct ) * (1 − EB)<label>(1)</label></formula><p>The conditional probability that an image is correctly classified given that it is correctly predicted by the branch (1 − E L|B Correct ) is not equal to and higher than its average (1 − EL). Improving the branch model will help, but with a diminishing return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Leaf Model Learning Analysis</head><p>In our architecture, each leaf model is cloned to deal with one superclass, which includes part of the old classes and some extra new classes. In this section we perform a detailed analysis, showing why initialization with existing parameters is advantageous, and furthermore how new features fight a continuous "tug-of-war" with the old ones. The study is on the 1350 superclass partition, which includes 602 old classes and 748 new ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Advantage of Incremental Learning</head><p>In Figure <ref type="figure" target="#fig_7">6</ref>, we compare the error curves of incremental learning versus one that is trained from scratch. It is obvious that the error rate of incremental learning drops much faster. Such speedup benefits are the results of bigger gradients on weights, which subsequently relate to activations in the forward pass, and derivatives in the backward pass.</p><p>Figure <ref type="figure" target="#fig_8">7</ref> shows the 2 norms of the weight gradients between different layers as a function of training epochs. To save space, only the gradient between layer 7 and 8, and that between layer 1 and 2 are shown, as representatives of classification layers and low feature extraction layers, respectively. The gradient curves between other layers have similar trends. In the first 150 batches, the incremental learning produces much bigger gradient than the from-scratch learning in both high and low layers. This explains why error rate of the incremental learning drops quickly in the same period, as it is able to descend the error surface faster. As training progresses to later epochs, the differences between incremental and from-scratch gradually disappears.  What leads to the bigger gradients? In back-propagation, the gradient of the weights between layer i−1 and i, gi−1,i ≡ ∂E ∂W i−1,i , is the outer product of xi−1, the activation of layer i − 1 computed in the forward pass, and ei, the error derivatives layer i computed in the back propagation pass. In other words, gi−1,i = ei • x T i−1 . For example, g7,8 = e8 • x T 7 . First, we dive into the activations. In Figure <ref type="figure" target="#fig_9">8</ref>, we plot the 1 norm of the activations in layer 7 and layer 1. We can see that in the feature extraction layers (e.g. layer 1), the activations all rise, signaling the learning and activation of new features. The one with incremental learning starts with bigger values (roughly five times bigger) and increase slowly, the reason is that incremental learning is able to reuse existing features and is therefore less motivated to learn new ones. In the classification layer, activations of incremental learning are also much higher, since from-scratch are gradually moving forward with random (and therefore lower) activations.</p><p>Next, we look at error derivatives to check how the error signals are propagated down by the back-propagation algorithm. The last layer's error derivative e8 is simply the gap between label and output. While from-scratch is essentially making random guesses and suffering from high errors, incremental learning at least are getting most of the predictions of the old classes correct. As such, higher classification layers in from-scratch learning has bigger errors to start the propagation, as shown in Figure <ref type="figure" target="#fig_10">9</ref>. Things are different for lower layers. The back-propagation is governed by  The above observations can be summarized as the followings. Incremental learning has higher activation due to learned features being reused, which also allows errors to propagate more smoothly downwards. Neither is true for from-scratch learning, whose only drive is the bigger error at the output layer. As such, incremental learning has bigger gradients and descent faster.</p><formula xml:id="formula_1">ei = ∂xi ∂zi W T i,i+1 • ei+1<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Interference between new and old classes</head><p>By the virtue of the error-driven preview and the subsequent partitioning, classes within a superclass is similar. Since the cloned leaf model was originally trained on old classes (some of them have been moved to another superclass), at the beginning of the training all new classes are misclassified as old ones (see Figure <ref type="figure" target="#fig_11">10</ref>). Figure <ref type="figure" target="#fig_13">12</ref> is an illustration of some new classes being classified into similar old classes.</p><p>The leaf model slowly learn the new classes. Meanwhile, the prediction accuracy of old classes recovers from some damages. As it turns out, such interference is the driving force for low layers to learn new features that helps separate new classes from old ones. We offer a detailed analysis below.</p><p>To make things simple, we will only concentrate on the interference between a pair of similar classes, one old and one new. As illustrated in Figure <ref type="figure" target="#fig_12">11</ref>, y1, t1 and w1 are the  output, target, output weight vector for the old class unit respectively, as are y2, t2 and w2 for the new class unit. At the beginning of incremental training, w1 is copied from the previous training result, and w2 is just randomly initialized. Given an input sample vector x1 in old class, the last hidden layer outputs its feature vector h1. Since the model is well trained on old classes, y1 = t1 = 1. Because softmax output y1 = e w 1 ,h 1 k e w k ,h 1 , this correct prediction is due to a big inner product w1, h1 . Without any error on this sample, backpropagation doesn't make any update on weight w1 and w2.</p><p>Instead, x2, being similar to x1, is wrongly predicted as the old class because of the big inner product w1, h2 . That means that, unsurprisingly, h1 and h2 are highly correlated, with a big inner product h1, h2 . Back-propagation starting from the mismatch between output (y1 = 1, y2 = 0) and the target (t1 = 0, t2 = 1) makes update on both w1 and w2,</p><formula xml:id="formula_2">w 1 = w1 − • h2(y1 − t1) = w1 − • h2 w 2 = w2 − • h2(y2 − t2) = w2 + • h2<label>(3)</label></formula><p>where is the learning rate. It effectively "shaves" some weights off w1 and moves them to w2, and damages w1 in the sense that if the model sees x1 again, y1 is a little less to be turned on. That's formulated by</p><formula xml:id="formula_3">w 1 , h1 = w1, h1 − • h2, h1<label>(4)</label></formula><p>So the stronger the correlation between h1 and h2, the more damage it is. As the training proceeds by alternating between these two samples, the net result is a compromising accuracy for either one of the class. To see why this is so, consider a state when both classes are correctly predicted. This state cannot be a stable state as the correlation between h1 and h2 will cause interferences that will weaken their weights in turn.  That means, however, that the back-propagation, being optimum-pursuing, will eventually separate h1 and h2 by forcing the error propagates downwards. As shown in Figure <ref type="figure" target="#fig_15">13</ref>, old class 'yorkshire terrior' and new class 'silky terrier' have very close features at the beginning of training, with the latter misclassfied as the former. Quickly, their feature distance widens, driven by the interference in classification layer to improve the prediction accuracy. The growth of the capacity for this fine-grained feature learning is hopefully stolen from classes that are moved away into other superclass (and not seen by the model any more).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUTION</head><p>We believe that tackling large-scale image classification task can and should take a divide and conquer approach, and do so incrementally. This study establishes a framework to do so with a training procedure that grows the network capacity hierarchically. The overall performance numbers are positive, but not overwhelmingly so. We take this as a steppingstone and the experiences point out a few lessons and future directions, which we summarize as the followings:</p><p>• Cloning naturally inherits leaned features. However, there is a tug-of-war between old and new features, in addition to removing old features that were acquired on removed classes. The current way of training with vanilla back-propagation does not seem to be efficient.</p><p>• At the time of performing superclass partition, we have a fair amount of knowledge of errors within each partition. Cloning with the exact same configuration simplifies parameter initialization, at the expense of missing needed capacity increase.</p><p>• The branch model should focus on differentiating superclasses, our current training model deprives it from such opportunities.</p><p>We are actively pursuing the above directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Incremental Learning in Multiclass Classification: the model needs to evolve with arrival of data of new classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A hierarchy of models: branch models predict superclasses, leaf models return final predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two choices of capacity increment. (a) Flat increment. The output units is increased to hold more classes. (b) Clone increment. A single leaf model is duplicated to clones for more classes. A branch model is also cloned to direct the correct prediction of superclass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Cloning a branch model. We first do the flat increment and train the new leaf model L 0 . The units in L 0 for each superclass are summed up for a branch model output unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>ExtendLeafModel input (L0, S): leaf model L0, superclass S output (L 0 , B, L1, L2, . . . , L K ): leaf model L 0 by flat increment, leaf models {L1, L2, . . . , LK } and branch model B by clone increment /* flat increment*/ incrementally train L0 to L 0 /* clone increment */ error-driven preview of S partition S into {S1, S2, . . . , SK } clone L0 into new leaf models {L1, L2, . . . , LK } for i = 1 to K do incrementally train Li end for clone branch model B ← L 0 /* use clones or not */ if prediction accuracy {B, L1, L2, . . . , LK } &gt; L 0 then L 0 ← ∅ else B ← ∅ end if return (L 0 , B, L1, L2, . . . , LK )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 2 :</head><label>2</label><figDesc>IncrementalLearning input (S, L, B, Snew): superclass set S, leaf model set L (each l ∈ L is corresponding to a s ∈ S), branch model set B, new class set Snew output (S, L, B): updated superclass set S, leaf model set L, branch model set B /* ditribute new classes to superclasses* / calculate the confusion matrix Φ with entry Φ(c, s) for probability of predicting c ∈ Snew to s ∈ S for all c ∈ Snew do select s ∈ S with maximum Φ(c, s) s = s ∪ {c} end for /* incremental training */ for all s ∈ S and the corresponding l ∈ L do (l , b, l1, l2, . . . , lK ) = ExtendLeafModel(s, l) if b = ∅ then insert b to B, replace l by {l1, l2, . . . , lK } in L else replace l by l in L end if end for /* refine brach models (optional) */ for all b ∈ B do incrementally train b according to updated subtrees end for return (S, L, B)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Confusion matrix among 1000 original classes (left), and the confusion matrix between new 1282 classes and original 1000 classes (right). The partitioning has been done (in red and blue boxes, respectively).</figDesc><graphic url="image-31.png" coords="7,193.94,178.85,92.37,118.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Learning curves of incremental run and scratch run in the first 500 batches</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: l 2 norm of the gradient in layer 2 and layer 8 in the incremental run and the scratch run</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: l 1 norm of the activation in layer 1 and layer 7 in the incremental run and the scratch run</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: l 1 norm of the error derivative in layer 2 and layer 8 in the incremental run and the scratch run</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Learning curves of old and new classes.</figDesc><graphic url="image-47.png" coords="9,343.94,150.19,68.56,68.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Inference between new and old classes. Input x1 with feature h1 belongs to old classes (target t1 = 1); x2 with h2 belongs to new classes (target t2 = 1).</figDesc><graphic url="image-40.png" coords="9,335.50,54.20,85.87,317.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Examples of some new classes being misclassified into similar old classes at the beginning of training.</figDesc><graphic url="image-32.png" coords="9,451.36,54.20,85.87,317.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Changing of feature distance between new class 'silky terrier' and old class 'yorkshire terrior'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Incremental/from-scratch learning result-</cell></row><row><cell cols="3">s in ImageNet 1K Animal from 195 animals to 398</cell></row><row><cell>animals</cell><cell></cell><cell></cell></row><row><cell>Training</cell><cell cols="2">Epochs Error Rate</cell></row><row><cell>from-scratch</cell><cell>41</cell><cell>38.6%</cell></row><row><cell>incremental</cell><cell>10</cell><cell>41.6%</cell></row><row><cell>incremental</cell><cell>20</cell><cell>39.2%</cell></row><row><cell>incremental</cell><cell>30</cell><cell>37.9%</cell></row><row><cell>incremental</cell><cell>40</cell><cell>36.8%</cell></row><row><cell cols="3">to train for certain epochs. We observe that we can achieve</cell></row><row><cell cols="3">better performance if we incrementally train from the exist-</cell></row><row><cell cols="3">ing 195-animal-model (36.8% versus 38.6%), taking slightly</cell></row><row><cell cols="3">less time than the scratch run. On the other hand, with 25%</cell></row><row><cell cols="3">few data (stopping with 30 epochs), incremental learning al-</cell></row><row><cell cols="3">ready beating the from-scratch one (37.9% versus 38.6%).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>:</cell><cell cols="3">Incremental training result in Ima-</cell></row><row><cell cols="2">geNet 22K Animal dataset</cell><cell></cell><cell></cell></row><row><cell>Increment</cell><cell>Training</cell><cell cols="2">Error Rate Examples</cell></row><row><cell>flat</cell><cell>from-scratch</cell><cell>49.46%</cell><cell>74.74M</cell></row><row><cell>flat</cell><cell>incremental</cell><cell>49.15%</cell><cell>61.45M</cell></row><row><cell>clone</cell><cell>incremental</cell><cell>48.52%</cell><cell>117.79M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>shows advantage of the incremental learning over other training strategies. Column Increment shows how the model capacity are extended, flat increment or clone increment. Column Training tells how the new models are trained, from-scratch or incremental. Column Examples accounts the training examples sweeped to reach that accuracy. Note that if the model has several parts, the examples of each part are summed up to fill the entry. Compared with the baseline (the first row) with flat increment and from-scratch learning, the flat increment combining incremental learning (the second row) performs slightly better, with 0.3% accuracy gain and 18% fewer training examples. Finally, our increment (the last row), with more extra capacity from clone, gains some more (48.52% versus 49.46% of</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Architecture of the baseline model</figDesc><table><row><cell>Layer</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>Type</cell><cell cols="6">conv+max+norm conv+max+norm conv conv conv+max full</cell><cell>full</cell><cell>full</cell></row><row><cell>Channels</cell><cell>96</cell><cell>256</cell><cell>384</cell><cell>384</cell><cell>256</cell><cell cols="3">4096 4096 Output Dim</cell></row><row><cell>Filter Size</cell><cell>11*11</cell><cell>5*5</cell><cell>3*3</cell><cell>3*3</cell><cell>3*3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Convolution Stride</cell><cell>4*4</cell><cell>1*1</cell><cell>1*1</cell><cell>1*1</cell><cell>1*1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pooling Size</cell><cell>3*3</cell><cell>3*3</cell><cell>-</cell><cell>-</cell><cell>3*3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pooling Stride</cell><cell>2*2</cell><cell>2*2</cell><cell>-</cell><cell>-</cell><cell>2*2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Padding Size</cell><cell>2*2</cell><cell>1*1</cell><cell>1*1</cell><cell>1*1</cell><cell>1*1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>1000 old classes</cell><cell>1282 new classes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1000 old classes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1000 old classes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Breakdown comparison between cloned models and the flat-incremented model</figDesc><table><row><cell>Model</cell><cell cols="2">Superclass 1 Superclass 2</cell></row><row><cell>cloned leaf1</cell><cell>44.43%</cell><cell>N/A</cell></row><row><cell>cloned leaf2</cell><cell>N/A</cell><cell>54.79%</cell></row><row><cell>cloned branch</cell><cell>* 2.20%</cell><cell>* 4.11%</cell></row><row><cell>cloned leafs+branch</cell><cell>45.03%</cell><cell>55.63%</cell></row><row><cell>flat-incremented model</cell><cell>45.79%</cell><cell>56.02%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Conditional Leaf Model Error</figDesc><table><row><cell cols="3">SuperClass Branch Correct Branch Miss</cell></row><row><cell>SuperClass1</cell><cell>43.79%</cell><cell>72.89%</cell></row><row><cell>SuperClass2</cell><cell>53.72%</cell><cell>79.76%</cell></row><row><cell>model. Table</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>This work was supported by National Hi-Tech Research and Development Program (863 Program) of China under Grants 2014AA015102 and 2012AA012503, National Natural Science Foundation of China under Grant 61371128, and Ph.D. Programs Foundation of Ministry of Education of China under Grant 20120001110097.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tabula rasa: Model transfer for object category detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2252" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
				<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What does classifying more than 10,000 image categories tell us?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental learning of concept drift in nonstationary environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Elwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1517" to="1531" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A bayesian approach to unsupervised one-shot learning of object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fe-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgeting in gradient-based neural networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning and using taxonomies for fast visual categorization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">One-shot adaptation of supervised deep convolutional models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6204</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From n to n+ 1: Multiclass transfer incremental learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kuzborskij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3358" to="3365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image annotation by semantic sparse recoding of visual content</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Multimedia</title>
				<meeting>the 20th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mediamill at trecvid 2013: Searching concepts, objects, instances and events in video</title>
		<author>
			<persName><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fontijne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kordumova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Workshop</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Is learning the n-th thing any easier than learning the first? Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="640" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Safety in numbers: Learning categories from few examples with multi model knowledge transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3081" to="3088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning in the presence of concept drift and hidden contexts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kubat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="101" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
