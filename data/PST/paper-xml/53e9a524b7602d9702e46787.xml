<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning mixtures of product distributions over discrete domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jon</forename><surname>Feldman</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ryan</forename><surname>O'donnell</surname></persName>
							<email>odonnell@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Rocco</forename><forename type="middle">A</forename><surname>Servedio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Industrial Engineering and Operations Research Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning mixtures of product distributions over discrete domains</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">664C5EE2E8E597721AC498D52BB92870</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of learning mixtures of product distributions over discrete domains in the distribution learning framework introduced by Kearns et al. <ref type="bibr" target="#b17">[18]</ref>. We give a poly(n/ ) time algorithm for learning a mixture of k arbitrary product distributions over the n-dimensional Boolean cube {0, 1} n to accuracy , for any constant k. Previous polynomial time algorithms could only achieve this for k = 2 product distributions; our result answers an open question stated independently in [8] and [14]. We further give evidence that no polynomial time algorithm can succeed when k is superconstant, by reduction from a notorious open problem in PAC learning. Finally, we generalize our poly(n/ ) time algorithm to learn any mixture of k = O(1) product distributions over {0, 1, . . . , b} n , for any b = O(1).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Framework and motivation.</head><p>In this paper we study mixture distributions. Given distributions X<ref type="foot" target="#foot_0">1</ref> , . . . , X k over R n and mixing weights π 1 , . . . , π k that sum to 1, a draw from the mixture distribution Z is obtained by first selecting i with probability π i and then making a draw from X i . Mixture distributions arise in many practical scientific situations as diverse as medicine, geology, and artificial intelligence; indeed, there are several textbooks devoted to the subject <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Assuming that data arises as a mixture of some distributions from a class of distributions C, it is natural to try to learn the parameters of the mixture components. Our work addresses the learning problem in the PAC-style model introduced by Kearns et al. <ref type="bibr" target="#b17">[18]</ref>. In this framework we are given a class C of probability distributions over R n and access to random data sampled from an unknown mixture Z of k unknown distributions from C. The goal is to output a hypothesis mixture Z of k distributions from C which (with high confidence), is -close to the unknown mixture. The learning algorithm should run in time poly(n/ ). The standard notion of "closeness" between distributions Z and Z , proposed by <ref type="bibr">Kearns et al. and</ref> used in this work, is the Kullback-Leibler (KL) divergence (or relative entropy), defined as KL(Z||Z ) := x Z(x) ln(Z(x)/Z (x)). 1  In this paper we learn mixtures of product distributions over the Boolean cube {0, 1} n , and more generally over the b-ary cube {0, . . . , b -1} n ; i.e., the classes C will consist of distributions X i whose n coordinates are mutually independent distributions over {0, 1} and {0, . . . , b -1}, respectively. 2 Such learning problems have been well studied in the past, as we now describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related work.</head><p>In <ref type="bibr" target="#b17">[18]</ref> Kearns et al. gave efficient algorithms for learning mixtures of Hamming balls; these are product distributions over {0, 1} n in which all the coordinate means E[X i j ] must be either p or 1p for some unknown p which is fixed over all mixture components. Although these algorithms can handle mixtures with k = O(1) many components, the fact that the components are Hamming balls rather than general product distributions is a very strong restriction. (The algorithms also have some additional restrictions: p has to be bounded away from 1/2, and a more generous learning scenario is assumed in which the learner is in addition given oracle access to the target distribution Z -i.e. she can submit an input x and get back the probability mass Z assigns to x.)</p><p>More recently, Freund and Mansour <ref type="bibr" target="#b13">[14]</ref> gave an efficient algorithm for learning a mixture of two general product distributions over {0, 1} n . Around the same time Cryan et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> gave an efficient algorithm for learning phylogenetic trees in the two-state general Markov model; for the special case in which the tree topology is a star, this gives an algorithm for learning an arbitrary mixture of two product distributions over {0, 1} n . Both <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b7">[8]</ref> stated as an open question the problem of obtaining a polynomial-time algorithm for learning a mixture of k &gt; 2 product distributions. Indeed, recent work of Mossel and Roch <ref type="bibr" target="#b19">[20]</ref> on learning phylogenetic trees argues that the rank-deficiency of transition matrices is a major source of difficulty, and this may indicate why k = 2 has historically been a barrier -a two-row matrix can be rank-deficient only if one row is a multiple of the other, whereas the general case of k &gt; 2 is much more complex.</p><p>In other related work, there is a vast literature in statistics on the general problem of analyzing mixture data -see <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> for surveys. To a large degree this work centers on trying to find the exact best mixture model (in terms of likelihood) which explains a given data sample; this is computationally intractable in general. In contrast, our main goal (and the goal of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>) is to obtain efficient algorithms that produce -close hypotheses.</p><p>We also note that there has also been recent interest in learning mixtures of n-dimensional Gaussians from the point of view of clustering <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b24">25]</ref>. In this framework one is given samples from a a mixture of "well-separated" Gaussians, and the goal is to classify each point in the sample according to which Gaussian it came from. We discuss the relationship between our scenario and this recent literature on Gaussians in Section 6; here we emphasize that throughout this paper we make no "separation" assumptions (indeed, no assumptions at all) on the component product distributions in the mixture.</p><p>Finally, the problem of learning discrete mixture distributions may have applications to other areas of theoretical computer science, such as database privacy <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6]</ref> and quantum complexity <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Our results.</head><p>In this paper we give an efficient algorithm for learning a mixture of k = O(1) many product distributions over {0, 1} n . Our main theorem is the following: <ref type="bibr" target="#b0">(1)</ref>, and let Z be any unknown mixture of k product distributions over {0, 1} n . Then there is an algorithm that, given samples from Z and any , δ &gt; 0 as inputs, runs in time poly(n/ ) • log(1/δ) and with probability 1δ outputs a mixture Z of k product distributions over {0, 1} n satisfying KL(Z||Z ) ≤ .</p><formula xml:id="formula_0">Theorem 1 Fix any k = O</formula><p>We emphasize that our algorithm requires none of the additional assumptions -such as minimum mixing weights or coordinate means bounded away from 0, 1/2, or 1 -that appear in some work on learning mixture distributions.</p><p>Our algorithm runs in time (n/ ) k 3 , which is polynomial only if k is constant; however, this dependence may be unavoidable. In Theorem 7 we give a reduction from a notorious open question in computational learning theory (the problem of learning decision trees of superconstant size) to the problem of learning a mixture of any superconstant number of product distributions over {0, 1} n . This implies that solving the mixture learning problem for any k = ω(1) would require a major breakthrough in learning theory, and suggests that Theorem 1 may be essentially the best possible.</p><p>We also generalize our result to learn a mixture of product distributions over {0, . . . , b -1} n for any constant b: <ref type="formula">1</ref>) and b = O(1), and let Z be any unknown mixture of k product distributions over {0, . . . , b -1} n . Then there is an algorithm that, given samples from Z and any , δ &gt; 0 as inputs, runs in time poly(n/ ) • log(1/δ) and with probability 1δ outputs a mixture Z of k product distributions over {0, . . . , b -1} n satisfying KL(Z||Z ) ≤ .</p><formula xml:id="formula_1">Theorem 2 Fix any k = O(</formula><p>Taking b = k, this gives a polynomial time algorithm for learning k-state Markov Evolutionary Trees with a star topology. (Note that the main result of Cryan et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> is an algorithm for learning two-state METs with an arbitrary topology; hence our result is incomparable to theirs.)</p><p>2 Overview of our approach 2.1 The WAM algorithm. The cornerstone of our overall learning algorithms is an algorithm we call WAM (for Weights And Means). WAM is a general algorithm taking as input a parameter &gt; 0 and having access to samples from an unknown mixture Z of k product distributions X 1 , . . . , X k . Here each X i = (X i 1 , . . . , X i n ) is an R n -valued random vector with independent coordinates. The goal of WAM is to output accurate estimates for all of the mixing weights π i and coordinate means µ i j := E[X i j ]. Note that a product distribution over {0, 1} n is completely specified by its coordinate means.</p><p>More precisely, WAM outputs a list of poly(n/ ) many candidates ( π1 , . . . , πk , μ1</p><p>1 , μ1 2 , . . . , μk n ); each candidate may be viewed as a possible estimate for the correct mixing weights and coordinate means. We will show that with high probability at least one of the candidates output by WAM is parametrically accurate; roughly speaking this means that the candidate is a good estimate in the sense that in the sense that |π iπ i | ≤ for each i and that |μ i jµ i j | ≤ for each i and j. However there is a slight twist: if a mixing weight π i is very low then WAM may not receive any samples from X i , and thus it is not reasonable to require WAM to get an accurate estimate for µ i 1 , . . . , µ i n . On the other hand, if π i is so low then it is not very important to get an accurate estimate for µ i 1 , . . . , µ i n because X i has only a tiny effect on Z. We thus make the following formal definition:</p><formula xml:id="formula_2">Definition 1 A candidate ( π1 , . . . , πk , μ1 1 , μ1 2 , . . . , μk n ) is said to be parametrically -accurate if: 1. |π i -π i | ≤ for all 1 ≤ i ≤ k; 2. |μ i j -µ i j | ≤ for all 1 ≤ i ≤ k and 1 ≤ j ≤ n such that π i ≥ .</formula><p>The main technical theorem in this paper, Theorem 4, shows that so long as the X i 's take values in a bounded range, WAM will with high probability output at least one candidate that is parametrically accurate. The proof of this theorem uses tools from linear algebra (singular value theory) along with a very careful error analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3</head><p>As will be clear from the proof of Theorem 4, WAM will succeed even if the mixture distributions X i are only pairwise independent, not fully independent. This may be of independent interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">From WAM to PAC learning (binary case).</head><p>As we noted already, in the binary case a product distribution on {0, 1} n is completely specified by its n coordinate means; thus a candidate can essentially be viewed as a hypothesis mixture of product distributions. (This is not precisely correct, as the candidate mixing weights may not precisely sum to 1 and the candidate means might be outside the range [0, 1] by as much as .) To complete the learning algorithm described in Theorem 1 we must give an efficient procedure that takes the list output by WAM and identifies a candidate distribution that is close to Z in KL divergence, as required by Theorem 1. We do this in two steps:</p><p>1. We first give an efficient procedure that converts a parametrically accurate candidate into a proper hypothesis distribution that is close to Z in KL divergence. We apply this procedure to each candidate in the list output by WAM, and thus obtain a list of mixtures (hypotheses), at least one of which is close to Z in KL divergence.</p><p>2. We then show that a maximum-likelihood procedure can take a list of hypotheses, at least one of which is good (close to Z in KL divergence), and identify a single hypothesis which is good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Larger alphabets.</head><p>In the larger alphabet setting, Z is a mixture of k product distributions X 1 , . . . , X k over {0, . . . , b -1} n . Now each mixture component X i is defined by bn parameters p i j,</p><p>(with j = 1, . . . , n and = 0, . . . , b -1) where p i j, is the probability that a draw from X i j yields . The simple but useful observation that underlies our extension to {0, . . . , b -1} n is the following: just as any distribution over {0, 1} is completely specified by its mean, any distribution X i j over {0, . . . , b -1} is completely specified by its first b -</p><formula xml:id="formula_3">1 moments E[X i j ], E[(X i j ) 2 ], . . . , E[(X i j ) b-1</formula><p>]. Our approach is thus to run WAM b -1 times; for = 1, . . . , b -1 the th run will sample from the mixture distribution given by converting each sample (z 1 , . . . , z n ) to the sample (z 1 , . . . , z n ). We then carefully combine the lists output by the runs of WAM, and follow similar steps to (1) and (2) above to find a good hypothesis in the combined list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Outline.</head><p>Most of the main body of this paper, Section 3, is dedicated to explaining the ideas behind the WAM algorithm and its proof of correctness. (The detailed algorithm and proof appear in Appendices A through C.) We discuss the application of WAM to the b-ary case in Section 4, and in Section 5 we detail our reduction from a notorious open question in computational learning theory. We conclude in Section 6 with a discussion of applications and future work.</p><p>The two steps outlined in Section 2.2 are conceptually straightforward, but the details are quite technical, and are given in Appendices E through G. The pieces are all put together to prove Theorems 1 and 2 in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The WAM Algorithm</head><p>In this section we describe our main algorithm, WAM. We assume a general mixture setting: WAM has access to samples from Z, a mixture of k product distributions X 1 , . . . , X k with mixing weights π 1 , . . . , π k . Each X i = (X i 1 , . . . , X i n ) is an n-dimensional vector-valued random variable. We will further assume that all components' coordinates are bounded in the range [-1, 1]; i.e., X i ∈ [-1, 1] n with probability 1. We have chosen [-1, 1] for mathematical convenience; by scaling and translating samples we can get a theorem about any interval such as [0, 1] or [0, (b -1) b-1 ], with an appropriate scaling of . We write <ref type="bibr" target="#b0">1]</ref> for the mean of the jth coordinate of X i . Our main theorem is the following: Theorem 4 There is an algorithm WAM with the following property: for any k = O(1) and any , δ &gt; 0, WAM runs in time poly(n/ ) • log(1/δ) and outputs a list of poly(n/ ) many candidates, at least one which (with probability at least 1δ) is parametrically -accurate.</p><formula xml:id="formula_4">µ i j := E[X i j ] ∈ [-1,</formula><p>We give the full proof of correctness in Appendix C. The remainder of this section is devoted to explaining the main ideas behind the algorithm and its analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of WAM.</head><p>There is of course a brute-force way to come up with a list of candidates ( π1 , . . . , πk , μ1 1 , μ1 2 , . . . , μk n ), at least one of which is parametrically -accurate: simply "try all possible values" for the parameters up to additive accuracy . In other words, try all values 0, , 2 , 3 , . . . , 1 for the mixing weights and all values -1, -1 + , . . . , 1 -, 1 for the means. We call this approach "gridding". Unfortunately there are Θ(n) parameters in a candidate so this naive gridding strategy requires time (and produces a list of length) (1/ ) Θ(n) , i.e. exponential in n, which is clearly unacceptable.</p><p>The basic idea behind WAM is as follows: given all pairwise correlations between the coordinates of Z, it can be shown that there are a constant number of "key" parameters that suffice to determine all others. Hence in polynomial time we can empirically estimate all the correlations, try all possibilities for the constantly many key parameters, and then determine the remaining Θ(n) parameters.</p><p>The main challenge in implementing this idea is that it is not at all a priori clear that the error incurred from gridding the key parameters does not "blow up" when these are used to determine the remaining parameters. The heart of our analysis involves showing that it suffices to grid the key parameters to granularity poly( /n) in order to get final error .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head><p>The algorithm, and intuition for the analysis. We will now go over the steps of the algorithm WAM and at the same time provide an "intuitive" discussion of the analysis. A concise description of the steps of WAM is given in Appendix A for the reader's convenience. Throughout this section we will assume for the sake of discussion that the steps we take incur no error; a sketch of the actual error analysis appears in Section 3.3.</p><p>The first step of WAM is to "grid" the values of the mixing weights {π i } to granularity wts := 3 . Since there are only constantly many mixing weights, this costs just a multiplicative factor of poly(1/ ) in the running time. The remainder of the algorithm "assumes" that the values currently being gridded for the mixing weights are the nearly-correct values of the mixing weights. In fact, for the purposes of this intuitive description of WAM, we will simply assume we have exactly correct values.</p><p>The next step is simple: Suppose some s of the k mixing weights we have are smaller than . By the definition of being " -parametrically accurate", we are not obliged to worry about coordinates with such small mixing weights; hence we will simply forget about these mixture components completely and treat k as ks in what follows. (We assign arbitrary values for the candidate means of the forgotten components.) We may henceforth assume that π i ≥ &gt; 0 for all i.</p><p>The next step of algorithm WAM is to use samples from Z to estimate the pairwise correlations between the coordinates of Z. Specifically, for all pairs of coordinates 1 ≤ j &lt; j ≤ n, the algorithm WAM empirically estimates corr(j, j</p><formula xml:id="formula_5">) = E[Z j Z j ].</formula><p>The estimation will be done to within additive accuracy matrix = poly( /n); specifically, matrix := τ k+1 , where τ := 2 /n 2 . With high (i.e. 1δ) confidence we will get good such estimates in time poly(n/ ). Again, for the purposes of this intuitive description of WAM we will henceforth assume we have exactly correct values for each value corr(j, j ). (As an aside, this is the only part of the algorithm that uses samples from Z; as we will shortly see, this justifies Remark 3.) Observe that since X i j and X i j are (pairwise) independent we have</p><formula xml:id="formula_6">corr(j, j ) = E[Z j Z j ] = k i=1 π i E[X i j X i j ] = k i=1 π i E[X i j ]E[X i j ] = k i=1 π i µ i j µ i j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us define μi</head><formula xml:id="formula_7">j = √ π i µ i j and write μj = (μ 1 j , μ2 j , . . . , μk j ) ∈ [-1, 1] k for 1 ≤ j ≤ n. We thus have corr(j, j ) = μj • μj ,</formula><p>where • denotes the dot product in R k . The remaining task for WAM is to determine all the values µ i j . Since WAM already has values for each π i and each π i ≥ &gt; 0, it suffices for WAM to determine all the values μi j and then divide by √ π i . At this point WAM has empirically estimated values for all the pairwise dot products μj • μj , j = j , and as mentioned, for intuitive purposes we are assuming all of these estimates are exactly correct. Let M denote the k × n matrix whose (i, j) entry is the unknown μi j ; i.e., the jth column of M is μj . The statement that WAM has all the dot products μj • μj for j = j is equivalent to saying that WAM has all the off-diagonal entries of the Gram matrix M M . We are thus led to what is essentially the central problem WAM solves:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Central Task: Given (estimates) for the off-diagonal entries of the n × n Gram matrix M M , generate (estimates of ) all possible candidates for the entries of the</head><formula xml:id="formula_8">k × n matrix M . k k k k n M J M J M J Matrix M of μi j 's M J n -k = B</formula><p>solved for gridded estimated Figure <ref type="figure" target="#fig_1">1</ref>: The full rank case. We solve for the unknown μi j 's in M J .</p><p>(A remark: The diagonal entries of M M are the quantities μj</p><formula xml:id="formula_9">• μj = k i=1 π i (µ i j ) 2</formula><p>and there is no obvious way to estimate these quantities using samples from Z. Also there are n such quantities, which is too many to "grid over". Nevertheless, the fact that we are missing the diagonal entries of M M will not play an important role for WAM.)</p><p>In general, a complete n × n Gram matrix determines the original k × n matrix matrix up to isometries on R k . Such isometries can be described by k × k orthonormal matrices, and these k 2 "degrees of freedom" roughly correspond to the constantly many key parameters that we grid over in the end. A geometric intuition for the Central Task is the following: there are n unknown vectors in R k and we have all the "angles" between them (more precisely, the dot products) between them. Thus fixing k of the vectors (hence k 2 unknown coordinates) is enough to completely determine the remainder of the vectors.</p><p>The full rank case. We proceed with our intuitive description of WAM and show how to solve the Central Task when M has full rank. Having done this, we will give the actual steps of the algorithm that show how the full rank assumption can be removed.</p><p>So suppose for now that M has full rank. Then there exists some set of k columns of M that are linearly independent, say J = {j 1 , . . . , j k } ⊂ [n]. Algorithm WAM tries all n k = poly(n) possibilities for the set J and then grids over the vectors μj 1 , . . . , μj k with granularity matrix = poly( /n) in each coordinate. As usual for the purposes of intuition, we assume that we now have μj 1 , . . . , μj k exactly correct.</p><p>Let M J be the k × k matrix given by the J-columns of M , and let M J be the k × (nk) matrix given by deleting the J-columns of M . WAM now has the entries of M J and must compute the remaining unknowns, M J . Since WAM has all of the off-diagonal entries of M M , it has all of the values of B = M J M J . (See Figure <ref type="figure" target="#fig_1">1</ref>.) But the columns of M J are linearly independent, so M J is invertible and hence WAM can compute M J = BM -1 J in poly(n) time. Having done this, WAM has all the entries of M and so the Central Task is complete, as is the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The general case.</head><p>Of course in general, M does not have full rank. This represents the main conceptual problem we faced in rigorously solving the Central Task. Indeed, we believe that handling rank-deficiency is the chief conceptual problem for the whole learning mixtures question, and that our linear algebraic methods for overcoming it (the description of which occupies the remainder of Section 3) are the main technical contribution of this paper.</p><p>Suppose rank(M ) = r &lt; k. By trying all possible values (only constantly many), algorithm WAM can be assumed to know r. Now by definition of rank(M ) = r there must exist kr orthonormal vectors u r+1 , . . . , u k ∈ [-1, 1] k which are orthogonal to all columns of M . WAM grids over these vectors with granularity matrix , incurring another multiplicative poly(n/ ) time factor. As usual, assume for the intuitive discussion that we now have the u j 's exactly. Let these vectors be adjoined as columns to M , forming M . But now the matrix M has full rank; furthermore, WAM knows all the off-diagonal elements of (M ) M , i.e. all the pairwise dot products of M 's columns, since all of the new dot products which involve the u j 's are simply 0! Thus we now have an instance of the Central Task with a full-rank matrix, a case we already solved. (Technically, n may now be as large as n + (k -1), but this is still O(n) and hence no time bounds are affected.) Given all entries of M we certainly have all entries of M , and so we have solved the Central Task and completed the algorithm WAM in the rank-deficient case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sketch of the actual analysis of WAM.</head><p>The preceding intuitive discussion of algorithm WAM neglected all error analysis. Correctly handling the error analysis is the somewhat subtle issue we discuss in this section. As mentioned, the full proof is given in Appendix C.</p><p>The main issue in the error analysis comes in understanding the right notion of the rank of M -since of all our gridding inevitably yields only approximations of the entries of M , the actual notion of rank is far too fragile to be of use. Recall the outline of the algorithm in our idealized intuition (rank-deficient case):</p><formula xml:id="formula_10">r = dimension of subspace in which μj 's lie ⇒ augment M by k -r orthogonal u i 's, forming M ⇒ M now full rank ⇒ find nonsingular k × k submatrix M J ⇒ solve linear system M J M J = B</formula><p>For the purposes of the error analysis, we reinterpret the operation of WAM as follows:</p><p>r * = dimension of subspace in which the μj 's "essentially" lie ⇒ augment M by kr "essentially" orthogonal u i 's, forming M ⇒ M now "strongly" full rank</p><formula xml:id="formula_11">⇒ find "strongly" nonsingular k × k submatrix M J ⇒ solve linear system M J M J = B (1)</formula><p>The real difficulty of the error analysis comes in the last step: controlling the error incurred from the solution of the linear system. Since we will only have approximately correct values for the entries of M J and B, we need to analyze the additive error arising from solving a perturbed linear system. Standard results from numerical analysis (see Corollary 5 in Appendix B) let us bound this error by a function of: (i) the error in M J and B, and (ii) the smallest singular value of M J , denoted by σ k (M ).</p><p>Let us briefly recall some notions related to singular values: Given any k × n matrix M , the first (largest) singular value of M is σ 1 (M ) = max u 1 2 =1 u 1 M 2 , and a u 1 achieving this maximum is taken as the first (left) singular vector of M . The second singular value of M is σ 2 (M ) = max u 2 2 =1,u 2 ⊥u 1 u 2 M 2 , and u 2 is the second left singular vector of M . In general, the ith singular value and vector are given by maximizing over all u i 2 = 1 orthogonal to all u 1 , . . . , u i-1 . In a well-defined sense (the Frobenius norm), the smallest singular value σ k (M ) measures the distance of M from being singular.</p><p>WAM's final error bounds arise from dividing the error in its estimates for M J and B by the smallest singular value of M J . The error in the estimates for the entries of M J come from gridding, and thus can essentially be made as small as desired; WAM makes them smaller than matrix . The errors in B come from two sources: some of the entries of B are estimates of quantities μj • μj = corr(j, j ), and again these errors can be made essentially as small as desired, smaller than matrix . However the other errors in B come from approximating the quantities μj • u i by 0; i.e, assuming the augmenting vectors are orthogonal to the columns of M .</p><p>As the reader may by now have guessed, the vectors with which WAM attempts to augment M will be the last k-r * singular vectors of M , u r * +1 , . . . , u k . The hope is that for an appropriate choice of r * , these singular vectors will be "essentially" orthogonal to the columns of M , and that the resulting M will be "strongly" full rank, in the sense that σ k (M ) will be somewhat large (cf. ( <ref type="formula">1</ref>)). One can show (see Proposition 9 of Appendix B) that the extent to which the u i 's are orthogonal to the columns of M is controlled by the (r * + 1)th singular value of M ; i.e., |μ j • u i | ≤ σ r * +1 (M ) for all i ≥ r * + 1; this is precisely the error we incur for the zero entries in B. On the other hand, one can also show that the augmented M has smallest singular value at least σ r * (M ). Thus we are motivated to choose r * so as to get a large multiplicative gap between σ r * (M ) and σ r * +1 (M ):</p><formula xml:id="formula_12">Definition 2 Given τ &gt; 0, the τ -essential rank of M is r * (M ) = r * τ (M ) = min{0 ≤ r ≤ k : σ r+1 (M )/σ r (M ) ≤ τ },</formula><p>where we take σ 0 (M ) = 1 and σ k+1 (M ) = 0.</p><p>One might think that if the additive error incurred from solving the linear system were to be roughly σ r * (M )/σ r * +1 (M ) then it should suffice to select τ on the order of poly( ). However, there is still a missing piece of the analysis: Although the smallest singular value of M becomes at least σ r * (M ) after adjoining the u j 's, we only use a k × k submatrix M J to solve the linear system. Is it the case that if M has a large smallest singular value then its "best" k × k submatrix also has a somewhat large smallest singular value? We need a quantitative version of the fact that a nonsingular k × n matrix has a k × k nonsingular submatrix (again, cf. ( <ref type="formula">1</ref>)). This does not seem to be a well-studied problem, and indeed there are some open questions in linear algebra surrounding the issue. It is possible to derive an extremely weak quantitative result of the required nature using the Cauchy-Binet formula. We instead give the following quantitatively strong version:</p><formula xml:id="formula_13">Corollary 5 Let A be a k × n real matrix with σ k (A) ≥ . Then there exists a subset of columns J ⊆ [n] with |J| = k such that σ k (A J ) ≥ / k(n -k) + 1.</formula><p>(We call the result a corollary because our proof in Appendix B is derived from a 1997 linear algebraic result of Goreinov, Tyrtyshnikov, and Zamarashkin <ref type="bibr" target="#b14">[15]</ref>. Incidentally, it is conjectured in their paper, and we also conjecture, that k(nk) + 1 can be replaced by √ n.) With this result in hand it becomes sufficient to take τ = 2 /n 2 , as described in the previous section. Now the error analysis can be completed:</p><p>• If M has a singular value gap of τ and so has essential rank r * &lt; k, then when WAM tries out the appropriate r * and singular vectors, the error it incurs from solving the linear system is roughly at most O( √ nτ ) = O( 2 /n 3/2 ); and as we show at the end of Appendix C, having this level of control over errors in solving the linear system for the unknown μi j 's lets us obtain the final µ i j values to the required -accuracy.</p><p>• On the other hand, if M has no singular value gap smaller than τ then its smallest singular value is at least τ k to begin with; thus it suffices to take matrix = τ k+1 = poly( /n) to control the errors in the full-rank case.</p><p>See Appendix C for the detailed proof of correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Estimating Higher Moments</head><p>In this section we explain our remarks from Section 2.3 more thoroughly; specifically, how to use WAM to learn a mixture Z of k product distributions X 1 , . . . , X k over {0, . . . , b -1} n . Such a distribution can be "parametrically" described by mixing weights {π i } i∈ <ref type="bibr">[k]</ref> and probabilities {p i j, }, where p i j, = Pr[X i j = ]. Running WAM on samples from Z gives a list of estimates of mixing weights and coordinate means E[X i j ], but these coordinate means are insufficient to completely describe the distributions X i j . However, suppose that we run WAM on samples from Z (i.e. each time we obtain a draw (z 1 , . . . , z n ) from Z, we actually give (z 1 , . . . , z n ) to WAM). It is easy to see that by doing this, we are running WAM on the π-weighted mixture of distributions (X 1 ) , . . . , (X k ) ; we will thus get as output a list of candidates for the mixing weights and the coordinate th moments E[(X i j ) ] for Z. Our algorithm for distributions over {0, . . . , b -1} n uses this approach to obtain a list of candidate descriptions of each of the first b -1 coordinate moments of Z. The algorithm then essentially takes the cross-product of these b -1 lists to obtain a list of overall candidates, each of which is an estimate of the mixing weights and all b -1 moments. Since WAM guarantees that each list contains an accurate estimate, the overall list will also contain an accurate estimate of the mixing weights and of all moments. For each candidate the estimate of the moments is then easily converted to "parametric form" {p i j, }, and as we show, any candidate with accurate estimates of the moments yields an accurate estimate of the probabilities p i j, . We now give the main theorem of the section, the proof of which (in Appendix D) contains the details of the algorithm: <ref type="bibr" target="#b0">(1)</ref>. Let Z be a mixture of k product distributions X 1 , . . . , X k over {0, . . . , b -1} n , so Z is described by mixing weights π 1 , . . . , π k and probabilities {p i j, } i∈[k],j∈[n], ∈{0,...,b-1} . There is an algorithm with the following property: for any , δ &gt; 0, the algorithm runs in poly(n/ ) • log 1 δ time and with probability 1δ outputs a list of candidates {π i }, {p i j, } such that for at least one candidate in the list, the following holds:</p><formula xml:id="formula_14">Theorem 6 Fix k = O(1), b = O</formula><formula xml:id="formula_15">1. |π i -π i | ≤ for all i ∈ [k]; and 2. |p i j, -p i j,</formula><p>| ≤ for all i, j, such that π i ≥ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Hardness of Learning Mixtures of Product Distributions</head><p>In this section we give evidence that the class of mixtures of k(n) product distributions over the Boolean cube may be hard to learn in polynomial time for any k(n) = ω(1). Before describing our results, we recall some standard terminology about Boolean decision trees. A decision tree is a rooted binary tree in which each internal node has two children and is labeled with a variable and each leaf is labeled with a bit b ∈ {0, 1}. A decision tree T computes a Boolean function f : {0, 1} n → {0, 1} in the obvious way: on input x ∈ {0, 1} n , if variable x i is at the root of T we go to either the left or right subtree depending on whether x i is 0 or 1. Continue in this fashion until reaching a bit leaf; the value of this bit is f (x).</p><p>Our main result in this section is the following theorem:</p><p>Theorem 7 For any function k(n), if there is a poly(n/ ) time algorithm which learns a mixture of k(n) many product distributions over {0, 1} n , then there is a poly(n/ ) time uniform distribution PAC learning algorithm which learns the class of all k(n)-leaf decision trees.</p><p>The basic idea behind this theorem is quite simple. Given any k(n)-leaf decision tree T , the set of all positive examples for T is a union of at most k(n) many disjoint subcubes of {0, 1} n , and thus the uniform distribution over the positive examples is a mixture of at most k(n) product distributions over {0, 1} n . If we can obtain a high-accuracy hypothesis mixture D for this mixture of product distributions, then roughly speaking D must put "large" weight on the positive examples and "small" weight on the negative examples. We can thus use D to make accurate predictions of T 's value on new examples very simply as follows: given a new example x to classify, we simply compute the probability weight that the hypothesis mixture D puts on x, and output 1 or 0 depending on whether this weight is large or small. We give the formal proof of Theorem 7 in Appendix I.</p><p>We note that after years of intensive research, no poly(n) time uniform distribution PAC learning algorithm is known which can learn k(n)-leaf decision trees for any k(n) = ω(1); indeed, such an algorithm would be a major breakthrough in computational learning theory. <ref type="foot" target="#foot_2">3</ref> The fastest algorithms to date <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">3]</ref> can learn k(n)-leaf decision trees under the uniform distribution in time n log k(n) . This suggests that it may be impossible to learn mixtures of a superconstant number of product distributions over {0, 1} n in polynomial time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have shown how to learn mixtures of any constant number of product distributions over {0, 1} n , and more generally over {0, . . . , b -1} n , in polynomial time.</p><p>The methods we use are quite general and can be adapted to learn mixtures of other types of multivariate product distributions which are definable in terms of their moments. Along these lines, we have used the approach in this paper to give a PAC-style algorithm for learning mixtures of k = O(1) axis-aligned Gaussians in polynomial time <ref type="bibr" target="#b12">[13]</ref>. (We note that while some previous work on learning mixtures of Gaussians from a clustering perspective can handle k = ω(1) many component Gaussians, all such work assumes that there is some minimum separation between the centers of the component Gaussians, since otherwise clustering is clearly impossible. In contrast, our result in <ref type="bibr" target="#b12">[13]</ref> -in which we do not attempt to do clustering but instead find a hypothesis distribution with small KL-divergence from the target mixture -does not require us to assume that the component Gaussians are separated.) We expect that our techniques can also be adapted to learn mixtures of other distributions such as products of exponential distributions or beta distributions.</p><p>It is natural to ask if our approach can be extended to learn mixtures of distributions which are not necessarily product distributions; this is an interesting direction for future work. Note that our main algorithmic ingredient, algorithm WAM, only requires that that the coordinate distributions be pairwise independent.</p><p>Finally, one may also ask if it is possible to improve the efficiency of our learning algorithmscan the running times be reduced to n O(k 2 ) , to n O(k) , or even n O(log k) ?</p><p>6. Try all possible subsets of exactly k column indices of M ; write these indices as J = J ∪ J , where J corresponds to columns from the original matrix M and J corresponds to augmented columns. Grid over [-1, 1] for the entries of M in columns J to within ± matrix , yielding { μi j : i ∈ [k], j ∈ J}. Let M J denote the matrix of estimates for all the columns in J . (See Figure <ref type="figure">2</ref>.) 7. Let J denote the columns of M other than J, and let M J denote the matrix of remaining unknowns formed by these columns. Let B be the matrix with rows indexed by J and columns indexed by J whose (j, j ) entry is the estimate corr(j, j ) of μj • μj if j ∈ J, or is 0 if j ∈ J . Using the entries of B and M J (all of which are known), solve the system M J M J = B to obtain estimates μi j for the entries of M J (which are the unknown μi j 's), thus producing estimates μi j for all entries of M . (If the matrix M J is singular, simply abandon the current gridding.)</p><formula xml:id="formula_16">J J k columns of J t columns ûr * +1 , . . . , ûk which augment M n columns of M columns of J                    k rows of M μ1 1 • • • • • • . . . . . . . . . . . . μ1 k • • • • • • • • • • • • μ1 n . . . . . . . . . . . . n + (k -r * ) columns of M Figure 2:</formula><p>A depiction of the matrix used by WAM. For ease of illustration the columns J of M are depicted as being the rightmost columns of M, and the columns J from the augmenting columns ûk-t+1 , . . . , ûk are depicted as being the leftmost of those augmenting columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8.</head><p>From the estimated values μi j , compute the estimates μi j = μi j / √ πi for all i, j. (Note that πi is never 0 since each is at leastwts &gt; 0.) 9. Output the candidate ( π1 , . . . , πk , μ1</p><p>1 , μ1 2 , . . . , μk n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Linear algebra necessities</head><p>In this section we give the results from linear algebra and numerical analysis necessary for the analysis of WAM.</p><p>Let A = (a ij ) be any k × n real matrix and write its singular value decomposition as A = U ΣV . We let σ 1 ≥ • • • ≥ σ k ≥ 0 denote the singular values of A, and let u 1 , . . . , u k denote the corresponding left singular vectors of A, i.e., the columns of U . Recall that • the vectors u 1 , . . . , u k form an orthonormal basis for R k ;</p><formula xml:id="formula_17">• σ 1 = max x 2 =1 x A 2 and σ k = min x 2 =1 x A 2 . The Frobenius norm A F of a k × n matrix A is defined as A F = i,j (A i,j ) 2 .</formula><p>Recall that σ k (A) equals the Frobenius norm distance from the k × n matrix A to the nearest rank-deficient matrix Ã, i.e.</p><formula xml:id="formula_18">σ k (A) = min rank( Ã)&lt;k A -Ã F . The spectral norm A 2 of a k × n matrix A is A 2 = max x 2 =1 Ax . It is well known that A 2 = σ 1 and A F = σ 2 1 + • • • + σ 2 k ; note that this implies A 2 ≤ A F .</formula><p>Our first necessary result is a quantitative version of the elementary fact that a full-rank k × n matrix has a full-rank k×k submatrix. We will use the following theorem of Goreinov, Tyrtyshnikov, and Zamarashkin <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_19">Theorem 8 [15] Let V be a k × n real matrix with orthonormal rows. Then there is a k × k submatrix V J which has σ k (V J ) ≥ 1/ k(n -k) + 1.</formula><p>The result we need is an easy corollary: Corollary 5 Let A be a k × n real matrix with σ k (A) ≥ . Then there exists a subset of columns</p><formula xml:id="formula_20">J ⊆ [n] with |J| = k such that σ k (A J ) ≥ / k(n -k) + 1.</formula><p>Proof: By the singular value decomposition we have A = U ΣV where U is a k × k matrix with orthonormal columns, Σ is a k ×k diagonal matrix with diagonal entries σ 1 , . . . , σ k , and V is a k ×n matrix with orthonormal rows. Let V J be the k × k submatrix of V whose existence is asserted by Theorem 8, so</p><formula xml:id="formula_21">σ k (V J ) ≥ 1/ k(n -k) + 1. We have σ k (U ) = 1 (since U is an orthogonal matrix) and σ k (Σ) ≥ , so σ k (U ΣV J ) ≥ σ k (U )σ k (Σ)σ k (V J ) ≥ / k(n -k) + 1</formula><p>where the inequality holds since σ k (P Q) ≥ σ k (P )σ k (Q) for any k × k matrices P, Q (this is easily seen from the variational characterization σ k (P ) = min x 2 =1 x P 2 .) The corollary follows by observing that U ΣV J is the k × k submatrix of A whose columns are in J.</p><p>The next result we will need is the characterization of what happens when the last kr * left singular vectors of a matrix are adjoined to it: Proposition 9 Let A be a k × n matrix with columns a 1 , . . . , a n . Fix any r * and let u r * +1 , . . . , u k be the left singular vectors corresponding to the smallest singular values σ r * +1 , . . . , σ k of A. Let A be A with the vectors u r * +1 , . . . , u k adjoined as columns. Then</p><formula xml:id="formula_22">σ k (A ) ≥ min{1, σ r * (A)},</formula><p>and for all r * + 1 ≤ ≤ k and for all columns a j of A we have</p><formula xml:id="formula_23">|a j • u | ≤ σ r * +1 (A).</formula><p>Proof: Write the singular value decomposition A = U ΣV where U is a k × k matrix with orthonormal columns u 1 , . . . , u k , Σ is a k × k diagonal matrix with σ 1 ≥ • • • ≥ σ k ≥ 0 on the diagonal, and V is a k × n matrix with orthonormal rows. It follows that for any vector x ∈ R k we have</p><formula xml:id="formula_24">x A 2 2 = σ 2 1 (x u 1 ) 2 + • • • + σ 2 k (x u k ) 2 .</formula><p>Let R denote the k × (kr * ) matrix whose columns are u r * +1 , . . . , u k , so we have</p><formula xml:id="formula_25">A = [A R].</formula><p>It is easily verified that the left singular vectors of R are simply u r * +1 , . . . , u k , while the singular values of R are all 1. Consequently we have</p><formula xml:id="formula_26">x R 2 2 = (x u r * +1 ) 2 + • • • + (x u k ) 2 for any x ∈ R k . Now recall the variational characterization of σ k (A ), namely σ k (A ) = min x 2 =1 x A 2 . Since x A 2 = x A 2 2 + x R 2 2 , we have σ k (A ) = min x 2 =1 σ 2 1 (x u 1 ) 2 + • • • + σ 2 k (x u k ) 2 + (x u r * +1 ) 2 + • • • + (x u k ) 2 . (<label>2</label></formula><formula xml:id="formula_27">)</formula><p>Since u 1 , . . . , u k form an orthonormal basis for R k we have that (x u 1 )</p><formula xml:id="formula_28">2 + • • • + (x u k ) 2 = 1 for all x 2 = 1. If we let α x = (x u r * +1 ) 2 + • • • + (x u k ) 2 then the quantity inside the square root of (2) is at least σ 2 r * (1 -α x ) + α x ≥ min{σ 2 r * , 1}</formula><p>. This proves the first inequality of the proposition. For the second inequality, we observe that a j • u = u U Σv j where v j is the jth column of V . Since U is orthonormal and Σ , = σ we thus have</p><formula xml:id="formula_29">|u U Σv j | = |σ v ,j | ≤ σ ≤ σ r * +1 ,</formula><p>where the first inequality holds since the rows of V are orthonormal and hence each entry of V must be at most 1 in magnitude.</p><p>The final result we will need is a very basic fact from numerical analysis controlling the error in a perturbed linear system: Theorem 10 Let A be a nonsingular k × k matrix, b be a k-dimensional vector, and</p><formula xml:id="formula_30">x the solution to Ax = b. Suppose A is a k×k matrix satisfying A-A F ≤ 1 &lt; σ k (A). Let b be a k-dimensional vector satisfying b -b 2 ≤ 2 and let x be the solution to A x = b . Then x -x 2 ≤ 1 x 2 + 2 σ k (A) -1 .</formula><p>The proof of a result like this can be found in most textbooks on numerical analysis (although it is more common to use the condition number of A rather than its smallest singular value). Since we are more interested in the • ∞ measure of distance, we give the following simple corollary: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Theorem 4</head><p>We go through the algorithm step by step, as it appears in Appendix A. In Step 1 of WAM, we define constants wts = 3 , τ = 2 /n 2 , and matrix = τ k+1 , which we use throughout the proof. In Step 2 of WAM the algorithm will grid over estimates πi that satisfy |π iπ i | for all i. In this case, any mixing component X i whose mixing weight π i is at least will not be eliminated. Since we need not be concerned with accuracy for the means of the other mixing components, we can ignore them and assume for the rest of the proof that π i ≥ for all i. Now we come to the main work in the proof of correctness of Theorem 4: namely, showing that in Steps 3-7 of algorithm WAM, accurate estimates for the μi j 's are produced. Our goal for most of the remainder of the proof will be to show we obtain estimates μi j satisfying</p><formula xml:id="formula_31">| μi j -μi j | ≤ ˜ := 2</formula><p>for all i.</p><p>To that end, let r * = r * τ (M ), the τ -essential rank of M . We will quickly dismiss the two easy cases, r * = 0 and r * = k; we then treat the general case 0 &lt; r * &lt; k. r * = 0 case. By definition, in this case σ 1 (M ) ≤ τ ≤ ˜ . Since σ 1 (M ) is at least as large as the magnitude of M 's largest entry we must therefore have |μ i j | ≤ ˜ for all i, j. Now when WAM tries r * = 0 in Step 4, tries the k standard basis vectors for û1 , . . . , ûk in Step 5, and chooses all of these vectors for J in Step 6, it will set B = 0 in Step 7 and get μi j = 0 for all i, j when it solves the linear system. But this is indeed within an additive τ ≤ ˜ of the true values, as desired. r * = k case. By definition, it's not hard to see that in this case we must have σ k (M ) ≥ τ k . Now consider when WAM tries r * = k in Step 4. Step 5 becomes vacuous. By Corollary 5 there is some set of k columns J = J such that σ k (M J ) ≥ σ k (M )/ k(nk) + 1 ≥ τ k /n. In Step 6 WAM will try out this J and grid the associated entries to within ± matrix . In Step 7 the algorithm will use only corr's in forming B and these will also be correct to within an additive ± matrix . We can now use Corollary 11 -note that matrix = τ k+1 ≤ (τ k /n)/2k ≤ σ k (M J )/2k, as necessary. This gives estimates in Step 7 satisfying</p><formula xml:id="formula_32">| μi j -μi j | ≤ O(k) 2 matrix τ k /n = O(knτ ) ≤ ˜ , as desired. 0 &lt; r * &lt; k case.</formula><p>In this case, by definition of the essential rank, we have</p><formula xml:id="formula_33">τ σ r * (M ) ≥ σ r * +1 (M ) ≥ τ k .<label>(3)</label></formula><p>In Step 4 WAM will try out the correct value for r * and in Step 5 WAM will grid over vectors ûr * +1 , . . . , ûk that are within ± matrix in each coordinate of the actual last left singular vectors of M , u r * +1 , . . . , u k . Let M denote the matrix M with these true singular vectors adjoined. By Proposition 9 we have</p><formula xml:id="formula_34">σ k (M ) ≥ min{1, σ r * (M )}.<label>(4)</label></formula><p>From the crude upper bound σ r * (M ) ≤ M F = i,j (μ i j ) 2 ≤ √ kn, we can restate (4) as simply</p><formula xml:id="formula_35">σ k (M ) ≥ σ r * (M )/ √ kn. Now applying Corollary 5 we conclude there is a subset J of M 's columns with |J | = k such that σ k (M J ) ≥ σ k (M )/ k(n -k) + 1 ≥ σ r * (M )/kn.<label>(5)</label></formula><p>In Step 6, WAM will try this set of columns J = J ∪ J ; it will also grid estimates for the entries in this column that are correct up to an additive ± matrix . Note that WAM now has an M J that has all entries correct up to an additive ± matrix . Now consider the matrix B WAM forms in Step 7. For the columns corresponding to J the entries are given by corr's, which are correct to within ± matrix . For the columns corresponding to J the entries are 0's; by the second part of Proposition 9 these are correct up to an additive σ r * +1 (M ). We now use Corollary 5 to bound the error resulting from solving the system M J M J = B in Step 7. To check that the necessary hypothesis is satisfied we combine (3) and ( <ref type="formula" target="#formula_35">5</ref>):</p><formula xml:id="formula_36">σ k (M J )/2k ≥ σ r * (M )/2k 2 n ≥ τ k-1 /2k 2 n ≥ τ k+1 = matrix .</formula><p>Now Corollary 11 tells us that the μi j produced satisfy</p><formula xml:id="formula_37">| μi j -μi j | ≤ O(k) matrix + max{ matrix , σ r * +1 (M )} σ k (M J ) ≤ O(k 2 n) matrix + σ r * +1 (M ) σ r * (M ) ,</formula><p>where in the last step we used <ref type="bibr">(5)</ref>. But by (3) we have matrix /σ r * (M ) ≤ matrix /τ k-1 = τ 2 and also σ r * +1 (M )/σ r * (M ) ≤ τ . Thus we have</p><formula xml:id="formula_38">| μi j -μi j | ≤ O(k 2 n)τ ≤ ˜ , as desired.</formula><p>It remains to bound the error blowup in Step 8. By this point we have values for the π i 's that are accurate to within ± wts , and further, all π i 's are at least . We also have values for all μi j 's that are accurate to within ±˜ . Since the function g(x, y)</p><formula xml:id="formula_39">= y/ √ x satisfies sup x∈[ ,1] y∈[-1,1] ∂ ∂x g(x, y) = 2 -3/2 and sup x∈[ ,1] y∈[-1,1] ∂ ∂y g(x, y) &lt; -1/2 ,</formula><p>the Mean Value Theorem implies that in Step 8 our resulting estimates μi j are accurate to within additive error</p><formula xml:id="formula_40">wts • 2 -3/2 + ˜ • -1/2 ≤ , as necessary.</formula><p>This completes the proof of WAM's correctness. As for the running time, it is easy to see that the dominating factor comes from gridding over the entries of M J and u r * +1 , . . . , u k . Since there are k 2 entries and we grid to granularity matrix = τ k+1 = poly(n/ ) k , the overall running time is poly(n/ ) k 3 ; i.e., poly(n/ ) for constant k.</p><p>(p i j,0 , . . . , pi j,b-1 ) = (μ i j,0 , . . . , μi j,b-1 )V -1 for each i, j to obtain parametric estimates {p i j, } for the probabilities {p i j, }. Now applying Corollary 11, we have that for all i, j, , we have</p><formula xml:id="formula_41">|p i j, -p i j, | ≤ • O(b)/σ b =</formula><p>, where σ b is set equal to σ b (V ), the smallest singular value of V . (Since the Vandermonde matrix is nonsingular, even without specifying σ b we have that it is a positive constant that depends only on b; it can be shown to be at least b -poly(b) ) The running time is dominated by the time to take the cross-product of the lists. This concludes the proof of Theorem 6. We remark that the running time dependence on b is of the form (n/ ) poly(b) ; since a b in the exponent is inevitable in our cross-product approach, we have refrained from excessive optimization of the dependence on b (by doing things such as representing the alphabet by bth roots of unity rather than equally spaced reals, which would have given a better Vandermonde singular value bound).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E The road ahead</head><p>Since the binary domain {0, 1} n corresponds to the b = 2 case of the general {0, . . . , b -1} n domain, here we shall deal only with the latter.</p><p>Recall that p i j, is the probability that under the ith product distribution over {0, . . . , b -1} n in the target mixture Z, the jth coordinate takes value . From Theorem 6, we have a list L of M candidates {π i }, {p i j, } such that at least one candidate is parametrically accurate -i.e., satisfies the following:</p><formula xml:id="formula_42">1. |π i -π i | ≤ for all i = 1 . . . k; and 2. |p i j, -p i j, | ≤ for all i ∈ [k], j ∈ [n] and ∈ {0, . . . , b -1} such that π i ≥ .</formula><p>In Section F, we show how to convert candidate into a true mixture of product distributions, in such a way that any parametrically accurate candidate becomes a mixture distribution with small KL divergence from the target distribution (see <ref type="bibr">Theorem 12)</ref>. Applying this conversion procedure to the list from Theorem 6, we get a list of M hypothesis mixture distributions such that at least one hypothesis in the list has small KL divergence from the target Z (see <ref type="bibr">Theorem 16)</ref>.</p><p>Then in Section G we show how a maximum-likelihood procedure can find a KL-accurate hypothesis (one with small KL divergence from Z) from among a list of hypothesis, one of which is guaranteed to have good KL divergence (see <ref type="bibr">Theorem 17)</ref>.</p><p>In Section H we combine Theorem 17 with Theorem 16 to obtain Theorem 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F From candidates to hypothesis mixture distributions</head><p>The following theorem defines a process that converts a single candidate for the π i 's and p i j, 's of Z to a true mixture of product distributions over {0, . . . , b -1} n that has at least some minimum mass on every point in {0, . . . , b -1} n (as we will see in Section G, this minimum mass condition is required by the maximum-likelihood procedure). More importantly, the theorem guarantees that if the candidate is parametrically accurate then the process outputs a mixture distribution with small KL divergence relative to Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 12</head><p>1. There is an efficient procedure A which takes values bprobs , wts &gt; 0 and πi ,p i j, as inputs and outputs a mixture Ż of k product distributions over {0, . . . , b -1} n with mixing weights πi &gt; 0 and probabilities ṗi j, &gt; 0 satisfying  Then for sufficiently small bprobs and wts , the mixture Ż will satisfy</p><formula xml:id="formula_43">KL(Z|| Ż) ≤ η( bprobs , wts , minwt ),<label>(6)</label></formula><p>where η( bprobs , wts , minwt ) := n • (12b We prove Theorem 12 in Section F.2 after setting up the required machinery in Section F.1.</p><p>F.1 Some tools. Here we give some propositions which will be used in the proof of Theorem 12.</p><p>The following simple proposition bounds the KL divergence between two product distributions in terms of the KL divergences between their coordinates.</p><p>Proposition 13 Suppose P 1 , . . . , P n and Q 1 , . . . , Q n are distributions satisfying KL(P i ||Q i ) ≤ i for all i. Then KL(P</p><formula xml:id="formula_44">1 × • • • × P n ||Q 1 × • • • × Q n ) ≤ n i=1 i .</formula><p>Proof: We prove the case n = 2:</p><formula xml:id="formula_45">KL(P 1 × P 2 ||Q 1 × Q 2 ) =</formula><p>x y P 1 (x)P 2 (y) ln P 1 (x)P 2 (y)</p><formula xml:id="formula_46">Q 1 (x)Q 2 (y) =</formula><p>x y P 1 (x)P 2 (y) ln P 1 (x) Q 1 (x) +</p><p>x y P 1 (x)P 2 (y) ln</p><formula xml:id="formula_47">P 2 (y) Q 2 (y) = P 2 (y)KL(P 1 ||Q 1 ) + P 1 (x)KL(P 2 ||Q 2 ) ≤ 1 + 2 .</formula><p>The general case follows by induction.</p><p>Very roughly speaking, the following proposition states that if P is a π-weighted mixture of distributions P 1 , . . . , P k and Q is a γ-weighted mixture of distributions Q 1 , . . . , Q k , then if each Q i is "close" to the corresponding P i and the π-weighting is "close" to the γ-weighting, then Q is "close" to P. To make this precise we need several technical conditions as stated in the proposition. Proposition 14 Let π 1 , . . . , π k , γ 1 , . . . , γ k ≥ 0 be mixing weights satisfying</p><formula xml:id="formula_48">π i = γ i = 1. Let I = {i : π i ≥ 3 }. Let P 1 , . . . , P k and Q 1 , . . . , Q k be distributions. Suppose that 1. |π i -γ i | ≤ 1 for all i ∈ [k]; 2. γ i ≥ 2 for all i ∈ [k]; 3. KL(P i ||Q i ) ≤ I for all i ∈ I; 4. KL(P i ||Q i ) ≤ all for all i ∈ [k].</formula><p>Then, letting P denote the π-mixture of the P i 's and Q the γ-mixture of the Q i 's, for any 4 &gt; 1 we have KL(P||Q) ≤ I + k 3 all + k 4 ln Proof:</p><formula xml:id="formula_49">KL(P||Q) = i π i P i ln i π i P i i γ i Q i ≤ i π i P i ln π i P i γ i Q i (by the log-sum inequality [7]) = i π i P i ln P i Q i + P i ln π i γ i = i π i KL(P i ||Q i ) + i π i ln π i γ i = i∈I π i KL(P i ||Q i ) + i ∈I π i KL(P i ||Q i ) + i π i ln π i γ i . (<label>7</label></formula><formula xml:id="formula_50">)</formula><p>For the first term of ( <ref type="formula" target="#formula_49">7</ref>), we have</p><formula xml:id="formula_51">i∈I π i KL(P i ||Q i ) ≤ I .</formula><p>For the second term of ( <ref type="formula" target="#formula_49">7</ref>), we have i ∈I</p><formula xml:id="formula_52">π i KL(P i ||Q i ) ≤ k 3 • max i∈[k] {KL(P i ||Q i )} ≤ k 3 all .</formula><p>For the third term of <ref type="bibr" target="#b6">(7)</ref>, letting I = {i ∈ I : π i ≥ 4 }, we have</p><formula xml:id="formula_53">i π i ln π i γ i = i / ∈I π i ln π i γ i + i∈I π i ln π i γ i . (<label>8</label></formula><formula xml:id="formula_54">)</formula><p>For the first sum in <ref type="bibr" target="#b7">(8)</ref> we have</p><formula xml:id="formula_55">i / ∈I π i ln π i γ i ≤ k 4 ln 4 2</formula><p>.</p><p>Since γ i ≥ π i -1 for all i, we have that for all i ∈ I</p><formula xml:id="formula_56">π i γ i ≥ 4 4 -1 = 1 + 1 4 -1 .</formula><p>Hence for the second sum in (8), we have</p><formula xml:id="formula_57">i∈I π i ln π i γ i ≤ i∈I π i ln 1 + 1 4 -1 ≤ 1 4 -1 .</formula><p>Putting all the bounds together the proof is done.</p><p>Finally, we will also need the following elementary proposition: On the other hand, if πi &lt; wts then it must be the case that π i ≤ 2 wts so we again have</p><formula xml:id="formula_58">|π i -πi | ≤ wts . Since k i=1 π i = 1 it follows that k i=1</formula><p>πi -1 ≤ k wts <ref type="bibr" target="#b9">(10)</ref> and thus</p><formula xml:id="formula_59">k i=1 πi ∈ [1 -k wts , 1 + k wts ].</formula><p>By definition of s this gives</p><formula xml:id="formula_60">s ∈ 1 1 + k wts , 1 1 -k wts<label>(11)</label></formula><p>Multiplying inequality <ref type="bibr" target="#b9">(10)</ref>   where the first inequality follows from <ref type="bibr" target="#b10">(11)</ref>.</p><formula xml:id="formula_61">|π i -πi | ≤ |π i -πi | + |π i -πi | ≤ wts + |π i -πi | = wts + |(1 -s)π i | ≤ wts +</formula><p>(3) Upper bounding KL(Z i || Żi ) for all i such that π i ≥ minwt . Fix an i such that π i ≥ minwt and fix any j ∈ [n]. Let P denote the distribution over {0, . . . , b -1} with probabilities p i j,0 , . . . , p i j,b-1 and let Q denote the distribution over {0, . . . , b -1} with probabilities ṗi j,0 , . . . , ṗi j,b-1 . We first show that each ṗi j, is close to pi j, and thus also to p i j, . This is done much as in (1) above. If pi j, ≥ bprobs then we have pi j, = pi j, so |p i j,pi j, | ≤ bprobs (by condition 2(b) in the theorem statement). On the other hand, if pi j, &lt; bprobs then it must be the case that p i j, ≤ 2 bprobs so we again have |p</p><formula xml:id="formula_62">i j, -pi j, | ≤ bprobs . Since b-1 =0 p i j, = 1 it follows that b-1 =0 pi j, -1 ≤ b bprobs<label>(12)</label></formula><p>and thus</p><formula xml:id="formula_63">b-1 =0 pi j, ∈ [1 -b bprobs , 1 + b bprobs ].</formula><p>By definition of t this gives</p><formula xml:id="formula_64">t ∈ 1 1 + b bprobs , 1 1 -b bprobs<label>(13)</label></formula><p>Multiplying inequality <ref type="bibr" target="#b11">(12)</ref>   <ref type="formula" target="#formula_64">13</ref>) and bprobs ≤ 1/b), we also have ṗi j, ≥ bprobs /2. We may thus apply Proposition 15 to P and Q (taking τ = bprobs /2 and ξ = 3b bprobs ), and we obtain KL(P||Q) ≤ 2(3b bprobs ) 1/2 + b(3b bprobs ) 3/2 /( bprobs /2). Routine simplification gives that this is at most 12b </p><formula xml:id="formula_65">(Z i || Żi ) ≤ -H(Z i ) + ln(1/( bprobs ) n ) ≤ n ln(b/ bprobs ),</formula><p>where H(X) := x X(x) ln(1/X(x)) denotes the"entropy in nats" of the random variable X.</p><p>We can now apply Proposition 14 with the parameter settings given by <ref type="bibr" target="#b8">(9)</ref>. Proposition 14 implies:  δ time algorithm which, given samples from Z, outputs a list of poly(n/ ) many mixtures of product distributions over {0, . . . , b -1} n with the property that:</p><formula xml:id="formula_66">KL(Z|| Ż) ≤ n • (12b</formula><p>• every distribution Z in the list satisfies ( 36nb 3 ) 2n ≤ Z (x) ≤ 1 for all x ∈ {0, . . . , b -1} n ; and</p><p>• with probability 1δ, some distribution Z in the list satisfies KL(Z||Z ) ≤ .</p><p>Proof: We will use a specialization of Theorem 6 in which we have different parameters for the different roles that plays:</p><formula xml:id="formula_67">Theorem 6 : Fix k = O(1), b = O(1). Let Z be a mixture of k product distributions X 1 , . . . , X k over {0, . . . , b -1} n , so Z is described by mixing weights π 1 , . . . , π k and probabilities {p i j, } i∈[k],j∈[n], ∈{0,...,b-1} .</formula><p>There is an algorithm with the following property: for any wts , bprobs , minwt , δ &gt; 0, with probability 1-δ the algorithm outputs a list of candidates {π i }, {p i j, } such that for at least one candidate in the list, the following holds:</p><formula xml:id="formula_68">1. |π i -π i | ≤ wts for all i ∈ [k]; and 2. |p i j, -p i j, | ≤ bprobs for all i, j, such that π i ≥ minwt .</formula><p>The algorithm runs in time poly(n/ ) • log(1/δ), where = min{ wts , bprobs , minwt }.</p><p>Let , δ &gt; 0 be given. We run the algorithm of Theorem 6 with parameters bprobs = ( 36nb 3 ) 2 , minwt = 3kn ln(1296b 7 n 2 / 2 ) , and wts = 3 27 . With these parameters the algorithm runs in time poly(n/ ) • log 1 δ . By Theorem 6 , we get as output a list of poly(n/ ) many candidate parameter settings {π i }, {μ i j } with the guarantee that with probability 1δ at least one of the settings satisfies</p><formula xml:id="formula_69">• |π i -πi | ≤ wts for all i ∈ [k],</formula><p>and</p><formula xml:id="formula_70">• |p i j, -p i j, | ≤ bprobs for all i, j, such that π i ≥ minwt .</formula><p>We now pass each of these candidate parameter settings through Theorem 12. It follows that the resulting distributions each satisfy n bprobs = ( 36nb 3 ) 2n ≤ Z (x) ≤ 1 for all x ∈ {0, 1} n . A routine verification shows that with our choice of bprobs , minwt and wts we have n • (12b </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Finding a good hypothesis using maximum likelihood</head><p>Theorem 16 gives us a list of distributions at least one of which is close to the target mixture distribution Z that we are trying to learn. Now we must identify some distribution in the list which is close to the target. In this section we give a simple maximum likelihood algorithm which helps us accomplish this. This is a standard situation (see e.g. Section 4.6 of <ref type="bibr" target="#b13">[14]</ref>) and we emphasize that the ideas behind Theorem 17 below are not new. However, we were unable to find in the literature a clear statement of the exact result which we need, so for completeness we give our own statement and proof below.</p><p>Let P be a target distribution over some space X. Let Q be a set of hypothesis distributions such that at least one Q * ∈ Q has KL(P||Q * ) ≤ . The following algorithm will be used to find a distribution Q ML ∈ Q which is close to P: Draw a set S of samples from the distribution P. For each Q ∈ Q, compute the log-likelihood Λ(Q) = x∈S (-ln Q(x)). Now output the distribution Q ML ∈ Q such that Λ(Q) is minimum. This is known as the Maximum Likelihood (ML) Algorithm since it outputs the distribution in Q which maximizes arg max Q∈Q x∈S Q(x). Theorem 17 Let β, α, &gt; 0 be such that α &lt; β. Let Q be a set of hypothesis distributions for some distribution P over the space X such that at least one Q * ∈ Q has KL(P||Q * ) ≤ . Suppose also that α ≤ Q(x) ≤ β for all Q ∈ Q and all x such that P(x) &gt; 0.</p><p>Run the ML algorithm on Q using a set S of independent samples from P, where S = m. Then, with probability 1δ, where δ ≤ (|Q| + 1) • exp -2m 2 log 2 (β/α) , the algorithm outputs some distribution Q ML ∈ Q which has KL(P||Q ML ) ≤ 4 .</p><p>Before proving Theorem 17 we give some preliminaries. Let P and Q be arbitrary distributions over some space X. We can rewrite the KL divergence between P and Q as KL(P||Q) = -H(P) -x∈X P(x) ln Q(x), <ref type="bibr" target="#b13">(14)</ref> where H(P) = -x∈X P(x) ln P(x) is the "entropy in nats" of P.</p><p>Consider the random variableln Q(x), where x is a sample from the distribution P. Using <ref type="bibr" target="#b13">(14)</ref>, we can express the expectation of this variable in terms of the KL-divergence: E x∈P [-ln Q(x)] = KL(P||Q) + H(P). <ref type="bibr" target="#b14">(15)</ref> Recall that when the ML algorithm runs on a list Q of distributions, it uses a set S of independent samples from P, where m = |S|. For each distribution Q ∈ Q, the algorithm computes</p><formula xml:id="formula_71">Λ(Q) = x∈S (-ln Q(x)).</formula><p>So, by <ref type="bibr" target="#b14">(15)</ref>, we have that the expected "score" of distribution Q is the following: E S [Λ(Q)] = m(H(P) + KL(P||Q)). <ref type="bibr" target="#b15">(16)</ref> We recall the theorem of Hoeffding <ref type="bibr" target="#b15">[16]</ref>: Proof of Theorem 17: Call a distribution Q ∈ Q good if KL(P||Q ML ) ≤ 4 , and bad otherwise. Note that by assumption, we have at least one good distribution in Q.</p><formula xml:id="formula_72">Theorem</formula><p>The probability δ that the algorithm fails to output some good distribution is at most the probability that either some bad distribution Q has Λ(Q) ≤ m(H(P) + 3 ) or the good distribution Q * has Λ(Q * ) ≥ m(H(P) + 2 ). Thus, by a union bound, we have ≤ exp -2m</p><formula xml:id="formula_73">2 log 2 (β/α) . (<label>20</label></formula><formula xml:id="formula_74">)</formula><p>Equation ( <ref type="formula">18</ref>) follows from the bound on the KL-divergence, equation <ref type="bibr" target="#b18">(19)</ref> follows from ( <ref type="formula">16</ref>), and equation ( <ref type="formula" target="#formula_73">20</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Putting it all together</head><p>All the pieces are now in place for us to prove our main learning result, Theorem 2, for learning mixtures of product distributions over {0, . . . , b -1} n .</p><p>Proof of Theorem 2: Run the algorithm described in Theorem 16. With probability 1δ this produces a list of T =poly(n/ ) many hypothesis distributions, one of which has KL divergence at most from Z and each of which puts weight at least ( 36nb 3 ) 2n on every point in {0, . . . , b -1} n . Now run the ML algorithm with α = ( 36nb 3 ) 2n , β = 1, and m = poly(n, 1/ ) ln(T /δ). By Theorem 17, with probability at least 1δ the ML algorithm outputs a hypothesis with KL divergence at most 4 from Z. Thus with overall probability 1 -2δ we get a hypothesis with KL divergence at most 4 from Z, and the total running time is poly(n/ ) • log(1/δ). Replacing by /4 and δ by δ/2 we are done.</p><p>Tracing through the proofs, it is easy to check that the running time dependence on k is (n/ ) O(k 3 ) • log 1 δ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Proof of Theorem 7</head><p>The following claim is used in the proof of Theorem 7:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Corollary 11</head><label>11</label><figDesc>Let A be a nonsingular k × k matrix, b be a k-dimensional vector, and x the solution to Ax = b. Assume that x ∞ ≤ 1. Suppose A is a k × k matrix such that each entry of A -A is at most matrix in magnitude, and assume that matrix &lt; σ k (A)/2k. Let b be a k-dimensional vector satisfying bb ∞ ≤ rhs . Let x be the solution to A x = b . Then we have xx ∞ ≤ O(k) matrix + rhs σ k (A) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>πi = 1 ,</head><label>1</label><figDesc>and for each i ∈ [k] and j ∈ [n], b-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 )</head><label>1</label><figDesc>Upper bounding |π iπi |. Fix any i ∈ [k]. If πi ≥ wts then we have πi = πi so |π iπi | ≤ wts .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F. 3</head><label>3</label><figDesc>Some candidate distribution is good. Here we establish the following:Theorem 16 Let b = O(1) and let Z be any unknown mixture of k product distributions over {0, . . . , b -1} n . There is a poly(n/ ) • log1  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc><ref type="bibr" target="#b17">18</ref> (Hoeffding)  Let x 1 , . . . , x n be independent bounded random variables such that each x i falls into the interval [a, b] with probability one. Let X = n i=1 x i . Then for any t &gt; 0 we havePr[X -E[X] ≥ t] ≤ e -2t 2 /n(b-a) 2 and Pr[X -E[X] ≤ -t] ≤ e -2t 2 /n(b-a) 2 .Now we can prove Theorem 17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>δ</head><label></label><figDesc>≤ |Q| • Pr Λ(Q) ≤ m(H(P) + 3 ) | KL(P||Q) ≥ 4 + Pr Λ(Q * ) ≥ m(H(P) + 2 ) (17) For each bad Q ∈ Q which has KL(P||Q) &gt; 4 , we have Pr[Λ(Q) ≤ m(H(P) + 3 )] = Pr[Λ(Q) ≤ m(H(P) + 4 )m)] ≤ Pr[Λ(Q) ≤ m(H(P) + KL(P||Q))m)] (18) = Pr[Λ(Q) ≤ E S [Λ(Q)]m](19)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>≤ exp -2m 2 log 2 21 )</head><label>2221</label><figDesc>) follows from the Hoeffding bound (Theorem 18). Following the same logic for Q * where KL(P||Q * ) ≤ , we getPr[Λ(Q * ) ≥ m(H(P) + 2 )] = Pr[Λ(Q * ) ≥ m(H(P) + ) + m ] ≤ Pr[Λ(Q * ) ≥ m(H(P) + KL(P||Q * )) + m ] = Pr[Λ(Q * ) ≥ E S [Λ(Q * )] + m ] (β/α) . (Theorem 17 follows from plugging equations (20) and (21) into equation<ref type="bibr" target="#b16">(17)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>≥ ( bprobs ) n for all x ∈ {0, . . . , b -1} n . 2. Furthermore, suppose Z is a mixture of k product distributions on {0, . . . , b -1} n with mixing weights π 1 , . . . , π k and probabilities p i j, , and that the following are satisfied: (a) for i = 1 . . . k we have |π iπi | ≤ wts , and (b) for all i, j, such that π i ≥ minwt we have |p i j,pi j, | ≤ bprobs .</figDesc><table><row><cell>(b) Ż(x)</cell></row><row><cell>1 =0 p i j, = 1;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>3 1/2 bprobs ) + k minwt n ln(b/ bprobs ) +</figDesc><table><row><cell>1/3 wts .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>by t, recalling that t b-1</figDesc><table><row><cell></cell><cell></cell><cell>=0</cell><cell cols="2">pi j, = 1, and assuming bprobs ≤ 1/(2b), we</cell></row><row><cell>obtain</cell><cell>|1 -t| ≤ tb bprobs ≤</cell><cell cols="2">b bprobs 1 -b bprobs</cell><cell>≤ 2b bprobs .</cell></row><row><cell>Thus, we have</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">|p i j, -ṗi j, | ≤ |p i j, -pi j, | + |p i j, -ṗi j, |</cell></row><row><cell></cell><cell cols="4">≤ bprobs + |p i j, -ṗi j, |</cell></row><row><cell></cell><cell cols="4">= bprobs + |(1 -t)p i j, |</cell></row><row><cell></cell><cell cols="4">≤ bprobs + 2b bprobs |p i j, |</cell></row><row><cell></cell><cell cols="4">≤ bprobs + 2b bprobs ;</cell></row><row><cell cols="2">certainly, this gives |p i j, -ṗi j, | ≤ 3b bprobs .</cell><cell></cell><cell></cell></row><row><cell cols="5">Moreover, since pi j, ≥ bprobs for all and ṗi j, = tp i j, where t &gt; 1 2 (by (</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>3 1/2 bprobs . Each Z i ( Żi respectively) is the product of n such distributions P (distributions Q respectively) over {0, . . . , b -1}. Therefore, by Proposition 13, we have KL(Z i || Żi ) ≤ n • (12b3 1/2  bprobs ) for all i with π i ≥ minwt .</figDesc><table /><note><p>(4) Upper bounding KL(Z i || Żi ) for all i ∈ [k]. This is simple: fix any i ∈ [k]. Since we know that Żi (x) ≥ n bprobs for all x ∈ {0, . . . , b -1} n , we immediately have KL</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>3 1/2 bprobs ) + k minwt n ln(b/ bprobs ) + k</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1/2 wts ln</cell><cell>1/2 wts wts /2</cell><cell>+</cell><cell>3k wts wts -3k wts 1/2</cell><cell>.</cell></row><row><cell cols="10">Considering the terms of the expression in brackets above, we have that</cell></row><row><cell>k</cell><cell>1/2 wts ln</cell><cell>1/2 wts wts /2</cell><cell cols="2">= k</cell><cell cols="2">1/2 wts ln</cell><cell>2 1/2 wts</cell><cell cols="2">≤</cell><cell>1 2</cell><cell>1/3 wts</cell></row><row><cell>and</cell><cell cols="3">3k wts 1/2 wts -3k wts</cell><cell cols="2">≤ 6k</cell><cell cols="2">1/2 wts ≤</cell><cell>1 2</cell><cell>1/3 wts</cell></row><row><cell cols="10">(note that these inequalities only require that 1/3 wts .</cell></row><row><cell cols="3">This concludes the proof of Theorem 12.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>wts is at most a sufficiently small constant depending only on k, roughly 1/k 6 ).</p>Hence KL(Z|| Ż) ≤ n • (12b 3 1/2 bprobs ) + k minwt n ln(b/ bprobs ) +</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Thus η( bprobs , wts , minwt ) ≤ , and we have that at least one of the resulting distributions Z satisfies KL(Z||Z ) ≤ .</figDesc><table><row><cell>3 1/2 bprobs ) ≤ 3</cell><cell>, k minwt n ln</cell><cell>b bprobs</cell><cell>≤ 3</cell><cell>, and</cell><cell>1/3 wts ≤ 3</cell><cell>.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We remind the reader (see e.g.<ref type="bibr" target="#b6">[7]</ref>) that Z -Z 1 ≤ (2 ln</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2) KL(Z||Z ) where • 1 denotes total variation distance; hence if the KL divergence is small, then the total variation distance is also small.2  Of course, the algorithm works for product distributions over Σ n for any alphabet Σ with |Σ| = b; i.e., the names of the characters in the alphabet do not matter.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Avrim Blum has offered a $1000 prize for solving a subproblem of the k(n) = n case and a $500 prize for a subproblem of the k(n) = log n case; see[4].</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Supported by an NSF Mathematical Sciences Postdoctoral Research Fellowship † Some of this work was done while at the Institute for Advanced Study, supported in part by the National Science Foundation under agreement No. CCR-0324906. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</p><p>‡ Supported in part by NSF CAREER award CCF-0347282.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of Theorem 6</head><p>For each = 1, . . . , b-1, the algorithm runs WAM on the random variable Z . In each such run, the " " parameter of WAM is set to := σ b /(O(b) • (b -1) b-1 ), where σ b is a constant we define later, and the "δ" parameter is set to δ := δ/(b -1). From these runs we obtain (b -1) lists L 1 , . . . , L b-1 of candidates {π i }, {μ i j, } i,j , where μi j, is an estimate of µ i j, = E[(X i j ) ]. The algorithm then uses these (b -1) lists to construct one larger list L of candidates {π i }, {µ i j, } i,j, , where each candidate estimates the mixing weights and all b -1 moments. This is done by taking all possible combinations of one candidate from each of the b -1 lists L 1 , . . . , L b-1 , and combining them as follows: take the mixing weights {π i } from the candidate from list L 1 , and for = 1, . . . , b -1, take {µ i j, } i,j from the candidate from list L . The list L will have size |L| = b-1 =1 |L | = poly(n, 1/ ). Theorem 4 on the WAM algorithm guarantees that with probability at least 1-(b-1)δ = 1-δ, each list L contains a candidate whose {μ i j, } are accurate estimates of the th moments. When we choose the accurate candidate from each list, we will obtain an overall candidate in L that is accurate on all b -1 moments. Define :</p><p>(The extra factor of (b -1) b-1 /2 comes from the need to scale the distributions for WAM so that the means fall into the range [-1, 1].)</p><p>To complete the proof of the theorem, we must show how the algorithm converts each candidate {π i }, {μ i j, } in the list L into "parametric" form {π i }, {p i j, } so that the "good" candidate satisfying (i) and (ii) above does not incur much error. It is easy to see that for a given i ∈ [k], j ∈ [n], we have (µ i j,0 , . . . , µ i j,b-1 ) = (p i j,0 , . . . , p i j,b-1 )V , where V is a b × b Vandermonde matrix (more precisely, V α,β = (α -1) β-1 , with V 1,1 = 1.) Following this characterization, the algorithm computes Proposition 15 Let P and Q denote distributions over {0, . . . , b -1} where P has probabilities p 0 , . . . , p b-1 and Q has probabilities q 0 , . . . , q b-1 . Suppose that |pq | &lt; ξ ≤ 1  4 for all ∈ {0, . . . , b -1}, and that also q ≥ τ for all ∈ {0, . . . , b -1}, where τ &lt; ξ. Then KL(P||Q) ≤ 2ξ 1/2 + bξ 3/2 /τ. Proof: Let L small = { ∈ {0, . . . , b -1} : p ≤ ξ 1/2 } and L big = {0, . . . , b -1} \ L small . We bound the contribution to KL(P||Q) from L small and L big separately. Now for the L small case. For all , it is easy to see that ln p q ≤ ln ξ+τ τ = ln(1</p><p>where the last inequality holds since ξ 1/2 ≤ ξ 1/2 /2 (since ξ ≤ 1 4 ). We thus have that the total contribution to KL(P||Q) from ∈ L big is at most ln(1+2ξ 1/2 ) ≤ 2ξ 1/2 . This proves the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Proof of Theorem 12.</head><p>We construct a mixture Ż of product distributions Ż1 , . . . , Żk by defining new mixing weights πi and probabilities ṗi j, . The procedure A is defined as follows:</p><p>Now let s be such that s k i=1 πi = 1, and take πi = sπ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">For all</head><p>pi j, = 1, and take ṗi j, = tp i j, .</p><p>It is clear from construction that this yields πi , ṗi j, that satisfy condition 1(a) of the theorem. It is also clear that for each i ∈ [k] we have that the distribution Żi satisfies Żi (x) ≥ n bprobs for all x ∈ {0, . . . , b -1} n , and thus the mixture Ż must satisfy Ż(x) ≥ n bprobs for all x. This gives part 1(b) of the theorem.</p><p>We now turn to part 2, and henceforth assume that the conditions on π i , πi , p i j, , pi j, from part 2 are indeed all satisfied. Roughly speaking, these conditions tell us that πi , pi j, are "good" (in the sense that they are parametrically accurate); we will show that the resulting πi , ṗi j, are "good" (in the sense of giving rise to a mixture Ż that satisfies (6)).</p><p>Our goal is to apply Proposition 14 with parameter settings</p><p>wts ; I = 12nb Proof: We show that U S is a mixture of product distributions, where is the number of leaves in T which are labeled with bit b. To see this, observe that the k leaves of T partition {0, 1} n into k disjoint subsets, each consisting of those x ∈ {0, 1} n which reach the corresponding leaf. For a leaf at depth d the corresponding subset is of size 2 n-d and consists of those x ∈ {0, 1} n which satisfy the length-d conjunction defined by the path from the root to that leaf. Thus, choosing a uniform element of S can be performed by the following process: (i) choose a leaf whose label is b, where each leaf at depth d is chosen with probability proportional to 1/2 d ; and then (ii) choose a uniform random example from the set of examples which satisfy the conjunction corresponding to that leaf. The uniform distribution over examples which satisfy a given conjunction is easily seen to be a product distribution X over {0, 1} n in which E[X i ] ∈ {0, 1  2 , 1} for all i = 1, . . . , n. It follows that the uniform distribution over S is a mixture of product distributions of this sort.</p><p>Theorem 7: For any function k(n), if there is a poly(n/ ) time algorithm which learns a mixture of k(n) product distributions over {0, 1} n , then there is a poly(n/ ) time uniform distribution PAC learning algorithm which learns the class of all k(n)-leaf decision trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>We suppose that we are given access to an oracle EX(T, U) which, at each invocation, supplies a labeled example (x, T (x)) ∈ {0, 1} n × {0, 1} where x is chosen from the uniform distribution U over {0, 1} n and T is the unknown k(n)-leaf decision tree to be learned. We describe an efficient algorithm A which with probability 1δ outputs a hypothesis h : {0, 1} n → {0, 1} which satisfies Pr U [h(x) = T (x)] ≤ . The algorithm A uses as a subroutine an algorithm A which learns a mixture of k(n) product distributions. Let M be the number of examples required by algorithm A to learn an unknown mixture of k(n) product distributions to L 1 -norm accuracy 1 -2 and confidence 1 -δ 3 . Recall from Section 1.1 that to learn to L 1 -norm error it suffices to learn to KL-divergence 2 , and thus we have that M =poly(n/ ) by our assumption on the running time of A.</p><p>Algorithm A works as follows:</p><p>1. Determine b ∈ {-1, 1} such that with probability 1 -δ 3 tree T outputs b on at least 1/3 of the inputs in {0, 1} n . Let S denote {x ∈ {0, 1} n : T (x) = b}, and let U S denote the uniform distribution over S. Let D be the hypothesis which is the output of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Run algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Output the hypothesis</head><p>We now verify the algorithm's correctness. Note first that Step 1 can easily be performed by making O(log </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now observe that the hypothesis h in</head><p>Step 3 disagrees with T on precisely those x which either (i) belong to S but have D (x) &lt; 1 2•2 n ; or (ii) do not belong to S but have D (x) ≥ 1 2•2 n . Each x of type (i) contributes at least 1 2•2 n toward D -U S 1 since U S (x) ≥ 1 2 n for each x ∈ S. Each x of type (ii) also incurs at least 1 2•2 n toward D -U S 1 . Consequently, since D -U S 1 ≤ 2 , there are at most 2 n points x ∈ {0, 1} n on which h is wrong. Thus, we have shown that with probability at least 1δ, the hypothesis h is an -accurate hypothesis for T with respect to the uniform distribution as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 1:</head><p>We note that our reduction to decision tree learning in fact only uses quite restricted mixtures of product distributions in which (i) the mixture coefficients are proportional to powers of 2, (ii) the supports of the product distributions in the mixture are mutually disjoint, and (iii) each product distribution is a uniform distribution over some subcube of {0, 1} n (equivalently, each product distribution has each E[X i ] ∈ {-1, 0, 1}). Thus, even this restricted class of mixtures of k(n) product distributions is as hard to learn as k(n)-leaf decision trees.</p><p>Remark 2: Known results of Blum et al. <ref type="bibr">[5]</ref> imply that the class of k(n)-leaf decision trees unconditionally cannot be learned under the uniform distribution in time less than n log k(n) in the model of learning from statistical queries.</p><p>A "Statistical Query" learning algorithm is only allowed to obtain statistical estimates (accurate to within some specified error tolerance) of properties of the distribution over pairs (x, T (x)), and does not have access to actual labeled examples (x, T (x)). The algorithm is "charged" more time for estimates with a higher precision guarantee; this is motivated by the fact that such highprecision estimates would normally be obtained, given access to random examples, by drawing a large sample and making an empirical estimate. (See <ref type="bibr" target="#b16">[17]</ref> for a detailed description of the Statistical Query model.)</p><p>Note that our algorithm for learning mixtures of product distributions interacts with the data solely by constructing empirical estimates of probabilities; thus, when this algorithm is used in the reduction of Theorem 7, the resulting algorithm for learning decision trees is easily seen to have an equivalent Statistical Query algorithm. Thus the results of Blum et al. unconditionally imply that no algorithm with the same basic approach as our algorithm can learn mixtures of k(n) product distributions in time less than n log k(n) .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multilinear formulas and skepticism of quantum computation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aaronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Symposium on Theory of Computing (STOC)</title>
		<meeting>the 36th Annual Symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning mixtures of arbitrary Gaussians</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Symposium on Theory of Computing</title>
		<meeting>the 33rd Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rank-r decision trees are a subclass of r-decision lists</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="183" to="185" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a function of r relevant variables (open problem)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference on Learning Theory and 7th Kernel Workshop</title>
		<meeting>the 16th Annual Conference on Learning Theory and 7th Kernel Workshop</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="731" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly learning DNF and characterizing statistical query learning using Fourier analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Furst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rudich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth Annual Symposium on Theory of Computing</title>
		<meeting>the Twenty-Sixth Annual Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards privacy in public databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory of Cryptography</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning and approximation algorithms for problems motivated by evolutionary trees</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cryan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Warwick</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evolutionary trees can be learned in polynomial time in the two state general Markov model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="375" to="397" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning mixtures of gaussians</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Symposium on Foundations of Computer Science</title>
		<meeting>the 40th Annual Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="634" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Two-round Variant of EM for Gaussian Mixtures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 16th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="143" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning decision trees from random examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ehrenfeucht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Computation</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="246" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">PAC Learning mixtures of axis-aligned Gaussians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>O'donnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Servedio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>manuscript</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating a mixture of two product distributions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Annual Conference on Computational Learning Theory</title>
		<meeting>the Twelfth Annual Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A theory of pseudoskeleton approximations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goreinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tyrtyshnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zamarashkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient noise-tolerant learning from statistical queries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="983" to="1006" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the learnability of discrete distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rubinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sellie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth Symposium on Theory of Computing</title>
		<meeting>the Twenty-Sixth Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mixture models: theory, geometry and applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lindsay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Institute for Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning nonsingular phylogenies and hidden markov models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mossel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">To appear in Proceedings of the 37th Annual Symposium on Theory of Computing (STOC)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Personal communication</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mixture densities, maximum likelihood and the EM algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Redner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="195" to="202" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Personal communication</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Statistical analysis of finite mixture distributions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">E</forename><surname>Makov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning mixtures of distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Symposium on Foundations of Computer Science</title>
		<meeting>the 43rd Annual Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m">A Algorithm WAM Algorithm WAM has access to samples from the mixture Z and takes as input parameters , δ &gt; 0. Algorithm WAM: 1. Let wts = 3</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Grid over the mixing weights, producing values π1 , . . . , πk ∈ [0, 1] accurate to within ± wts . If s of these weights are smaller than -wts , eliminate them and treat k as k -s in what follows</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Make empirical estimates corr(j, j ) for all correlations corr(j, j ) = E</title>
	</analytic>
	<monogr>
		<title level="j">Z j Z j ] = μj • μj for j = j to within ± matrix</title>
		<imprint/>
	</monogr>
	<note>with confidence 1 -δ</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m">Let M be the k × n matrix of unknowns (M ij ) = (μ i j ), and try all possible integers 0 ≤ r * ≤ k for the essential rank of M</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Grid over k -r * vectors ûr * +1 , . . . , ûk ∈ [-1, 1] k to within ± matrix in each coordinate and augment M with these as columns</title>
		<imprint/>
	</monogr>
	<note>forming M</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
